id;text;label
JVtwC9RzlI;"REVIEW 
Summary:
This paper proposes a method to integrate the processing and generation of both text and graph data using a single transformer-based model. Structure Token encodes graphs with text labels into a sequence of tokens, enabling the handling of both data types interchangeably. This approach leverages a unified representation within a Transformer Encoder-Decoder model, enhanced to incorporate structure tokens. They evaluate TextGraphBART on text-to-graph (T2G) and graph-to-text (G2T) tasks, demonstrating comparable results to baseline models with fewer parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The introduction of structure tokens is a significant advancement, providing a unified method for processing and generating both text and graph data.
2. The method integrates seamlessly with existing Transformer architectures, requiring only minor modifications, and avoids the need for specialized loss functions or additional modules.
3. Empirical results show that TextGraphBART achieves comparable performance on both T2G and G2T tasks with baselines but with fewer parameters.

Weaknesses:
1. The baseline settings are weak. It lacks of comparing with some strong baselines with advanced graph-structured-aware methods [1][2].
2. The model settings are limited. This paper only try one size of model, and further experiments with larger model sizes are needed.
3. The provided ablation study, though useful, could be more detailed, exploring the interactions and contributions of various components more comprehensively.

Reference: 
[1] Stage-wise Fine-tuning for Graph-to-Text Generation
[2] Self-supervised Graph Masking Pre-training for Graph-to-Text Generation

Limitations:
Yes. The limitations are discussed in Discussion section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces TextGraphBART. Its a new method of encoding graph/text-input by using a structure token. This new token should preserve graph structure as opposed graph linearization or cycle training. This should also allow for the generation of graphs, with accompying text tokens, without making architectural changes to the transformer. It is then verified on both Graph2Text and Text2Graph tasks for commonly used benchmarks such as WebNLG and EventNarrative. It is followed by results that show comparable results to previous works, and a short discussion and conclusion.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Paper is generally nice to read
2. The work considers a lot of relevant related work, which they both refer to or base decisions on
3. The build-up of the structure token is well explained

Weaknesses:
1. The work proposes a seamless integration of both text and graph inputs, but only verifies on strictly one-directional modal evaluation: Either text-to-graph or graph-to-text. However, as the authors claim that this is the ""first method that can autoregressively generate sub-graphs with multi-token labels without modifying transformers."", a single text-to-graph benchmark feels like a very shallow evaluation.
2.  Models are evaluated on datasets that already work well. This makes it hard to distinguish the added value of such a structure token. Scenarios are sketched where e.g. graph linearization is treated simply as a sequence of tokens, but then it this work should methodologically be verified on datasets where preserving graph structure should be required. This is now not the case.
3.  This is reflected in the results: they are comparable, so the structure token might aswell not be there. The proclaimed difference that this is done with less parameters I would argue is a very weak claim as the difference is not that big.
4.  Finally, I am not convinced that the proposed ""structure token"" is a truely lossless way of preserving graph structure through-out a transformer. The structure token and its generation is explained in Figure 2, and is claimed to be lossless. However, this is only true in the operations before the embedding leveraging the orthogonal property. After the embedding into a structure embedding, information will (seemingly) be lost about structure in this embedding. From this structure embedding, it cannot be determined what the original structure of the graph was?
5.  In the discussion the authors mention scaling, but there is no clear evidence for this as the authors only tested one model size. This is essentially an unsubstantiated claim.
 6. Why is the domain token needed? This is not further addressed or experimented with.

Limitations:
The authors partially address this limitations by motivating the paper in the introduction. However, I dont feel the discussion (or the rest of the work) addresses the actual limitations of this method. What are pros or cons compared to transformer modifications?

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new unified graph-text generation framework, TextGraphBART, for the large language model. The paper tries to address both the generation and representation of text and graphs. The paper proposes a new structure token to encode text graphs into a set of tokens. The structured token can encode the text graphs and be decoded into text graphs. Specifically, it consists of seven different embeddings, including position, domain, and text information. The paper pretrained the proposed framework over four different tasks, including text2text, graph2text, text2graph, and graph2graph. The model is pretrained on TEKGEN and GenWIKI and tests on the event narrative and WebNLG. The performance is evaluated on BLEU, METEOR< and BERTScore. The paper compares the model with T5, BART, GAP, CYcleGT, ZBT5, and Grapher. The paper also includes an ablation study and analyzes the future directions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The motivation of the paper is clear and the topic of the paper is interesting. The paper investigates an important unified graph-text generation framework for the language model. The idea of structure tokens is interesting and can contribute to future research. Additionally, the model seems to achieve good results while using fewer parameters.
2. The paper tests on both graph-to-text and text-to-graph datasets. The proposed method achieves better results compared to other baselines. The paper also conducts an ablation study to investigate the contribution of each component. It includes a discussion session to explore the paper's current limitations.
3. The paper provides code and implementation details.

Weaknesses:
1. The proposed framework is incremental. Multiple previous papers have used the idea of using different position embeddings to represent the structure information [1,2,3]. The idea of joint text-to-graph and graph-to-text pretraining/generation is also not new [4]. 
2. The experiment is not comprehensive. The paper used eventnarrative for the graph-to-text generation. However, compared to WebNLG  (2020), the EventNarrative has fewer baselines and most of its baselines are outdated. The paper must also add WebNLG(2020) as an additional graph-to-text generation tool to show its contributions. The pretraining datasets and the testing datasets are all Wiki-style datasets. The performance gain may come from pretraining (data leakage) instead of the actual model architecture. The paper needs to include an additional ablation study to show the gain of the eventnarrative/webnlg comes from the model architecture, or also pretraining the proposed baselines on TEKGEN/GenWiki. Otherwise, the scores are not comparable. Additionally, the paper needs to include some of the latest frameworks like [4] since all of the baselines used in the paper are old. The paper also needs to include some human evaluation or qualitative analysis to help readers understand the generation results better. Furthermore, the comparison in Table 4 is not fair, since domain-specific pretraining is more useful than general pretraining [5].
3. The paper puts important information in the Appendix while not reaching the page limit (9 pages). 

[1] Herzig, J., Nowak, P. K., Müller, T., Piccinno, F., & Eisenschlos, J. M. (2020). TaPas: Weakly supervised table parsing via pre-training. ACL 2020.

[2] Wang, Q., Yavuz, S., Lin, V., Ji, H., & Rajani, N. (2021). Stage-wise fine-tuning for graph-to-text generation. arXiv preprint ACL 2021 SRW.

[3] Chen, W., Su, Y., Yan, X., & Wang, W. Y. (2020). KGPT: Knowledge-grounded pre-training for data-to-text generation. EMNLP 2020.

[4] Wang, Z., Collins, M., Vedula, N., Filice, S., Malmasi, S., & Rokhlenko, O. (2023). Faithful low-resource data-to-text generation through cycle training. ACL 2023

[5] Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., & Smith, N. A. (2020). Don't stop pretraining: Adapt language models to domains and tasks. ACL 2020.

Limitations:
The paper includes a limitation section in the Appendix and a Discussion section in the main paper.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
- The paper highlights the limitations of two existing methods for generating text graphs: 1. The multi-stage approach does not consider multi-hop relations and cannot handle the case where two concepts have more than one relation. 2. The graph linearization approach introduces extra complexity to the Language Model (LM) and the predictions are altered if the generated triples are shuffled.
- Building on this, the paper proposes the Structure Token method. Specifically, it identifies a graph element (node or edge) using seven parts. Then, it transforms Structure Tokens into embeddings using OneHot and orthonormal-like vectors, which are then input into a Transformer Encoder-Decoder model.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The method proposed in the paper can avoid extra computation, such as the duplication of concepts.
- The experimental results presented in the paper show that the model performs comparably to models with a larger number of parameters while using fewer parameters, suggesting that the model might perform even better with an increased number of parameters.

Weaknesses:
- The paper's layout is not aesthetically pleasing, particularly the formatting of Tables 1-3.
- The experiments lack error bars, which diminishes the credibility of the results.
- The experimental results are unsatisfactory and fail to demonstrate the superiority of the model, and the experiments are incomplete.
  - The model's results did not achieve state-of-the-art (SOTA) performance in the G2T/T2G tasks.
  - Lines 257-259 state, ""In conclusion, our structure token approach can achieve comparable performance on text-to-graph generation under similar model size without using special training methods or loss functions."" The results in Table 3 are nearly identical to those of Grapher-small (Text) T2G. Combined with the absence of error bars, it is challenging to determine whether the similarities are due to errors or model efficacy, making it difficult to convince.
  - Existing results only may demonstrate that the model performs comparably to models with larger parameters under fewer parameters. This does not prove that increasing the number of parameters will enhance performance. A more direct comparison of experimental results is necessary to substantiate this claim. The explanations in Section 5, ""Scaling Up,"" are overly subjective and unconvincing, making the paper seem like a work in progress.

Limitations:
Author addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a method to integrate the processing and generation of both text and graph data using a single transformer-based model. Structure Token encodes graphs with text labels into a sequence of tokens, enabling the handling of both data types interchangeably. This approach leverages a unified representation within a Transformer Encoder-Decoder model, enhanced to incorporate structure tokens. They evaluate TextGraphBART on text-to-graph (T2G) and graph-to-text (G2T) tasks, demonstrating comparable results to baseline models with fewer parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The introduction of structure tokens is a significant advancement, providing a unified method for processing and generating both text and graph data.
2. The method integrates seamlessly with existing Transformer architectures, requiring only minor modifications, and avoids the need for specialized loss functions or additional modules.
3. Empirical results show that TextGraphBART achieves comparable performance on both T2G and G2T tasks with baselines but with fewer parameters.

Weaknesses:
1. The baseline settings are weak. It lacks of comparing with some strong baselines with advanced graph-structured-aware methods [1][2].
2. The model settings are limited. This paper only try one size of model, and further experiments with larger model sizes are needed.
3. The provided ablation study, though useful, could be more detailed, exploring the interactions and contributions of various components more comprehensively.

Reference: 
[1] Stage-wise Fine-tuning for Graph-to-Text Generation
[2] Self-supervised Graph Masking Pre-training for Graph-to-Text Generation

Limitations:
Yes. The limitations are discussed in Discussion section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces TextGraphBART. Its a new method of encoding graph/text-input by using a structure token. This new token should preserve graph structure as opposed graph linearization or cycle training. This should also allow for the generation of graphs, with accompying text tokens, without making architectural changes to the transformer. It is then verified on both Graph2Text and Text2Graph tasks for commonly used benchmarks such as WebNLG and EventNarrative. It is followed by results that show comparable results to previous works, and a short discussion and conclusion.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Paper is generally nice to read
2. The work considers a lot of relevant related work, which they both refer to or base decisions on
3. The build-up of the structure token is well explained

Weaknesses:
1. The work proposes a seamless integration of both text and graph inputs, but only verifies on strictly one-directional modal evaluation: Either text-to-graph or graph-to-text. However, as the authors claim that this is the ""first method that can autoregressively generate sub-graphs with multi-token labels without modifying transformers."", a single text-to-graph benchmark feels like a very shallow evaluation.
2.  Models are evaluated on datasets that already work well. This makes it hard to distinguish the added value of such a structure token. Scenarios are sketched where e.g. graph linearization is treated simply as a sequence of tokens, but then it this work should methodologically be verified on datasets where preserving graph structure should be required. This is now not the case.
3.  This is reflected in the results: they are comparable, so the structure token might aswell not be there. The proclaimed difference that this is done with less parameters I would argue is a very weak claim as the difference is not that big.
4.  Finally, I am not convinced that the proposed ""structure token"" is a truely lossless way of preserving graph structure through-out a transformer. The structure token and its generation is explained in Figure 2, and is claimed to be lossless. However, this is only true in the operations before the embedding leveraging the orthogonal property. After the embedding into a structure embedding, information will (seemingly) be lost about structure in this embedding. From this structure embedding, it cannot be determined what the original structure of the graph was?
5.  In the discussion the authors mention scaling, but there is no clear evidence for this as the authors only tested one model size. This is essentially an unsubstantiated claim.
 6. Why is the domain token needed? This is not further addressed or experimented with.

Limitations:
The authors partially address this limitations by motivating the paper in the introduction. However, I dont feel the discussion (or the rest of the work) addresses the actual limitations of this method. What are pros or cons compared to transformer modifications?

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new unified graph-text generation framework, TextGraphBART, for the large language model. The paper tries to address both the generation and representation of text and graphs. The paper proposes a new structure token to encode text graphs into a set of tokens. The structured token can encode the text graphs and be decoded into text graphs. Specifically, it consists of seven different embeddings, including position, domain, and text information. The paper pretrained the proposed framework over four different tasks, including text2text, graph2text, text2graph, and graph2graph. The model is pretrained on TEKGEN and GenWIKI and tests on the event narrative and WebNLG. The performance is evaluated on BLEU, METEOR< and BERTScore. The paper compares the model with T5, BART, GAP, CYcleGT, ZBT5, and Grapher. The paper also includes an ablation study and analyzes the future directions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The motivation of the paper is clear and the topic of the paper is interesting. The paper investigates an important unified graph-text generation framework for the language model. The idea of structure tokens is interesting and can contribute to future research. Additionally, the model seems to achieve good results while using fewer parameters.
2. The paper tests on both graph-to-text and text-to-graph datasets. The proposed method achieves better results compared to other baselines. The paper also conducts an ablation study to investigate the contribution of each component. It includes a discussion session to explore the paper's current limitations.
3. The paper provides code and implementation details.

Weaknesses:
1. The proposed framework is incremental. Multiple previous papers have used the idea of using different position embeddings to represent the structure information [1,2,3]. The idea of joint text-to-graph and graph-to-text pretraining/generation is also not new [4]. 
2. The experiment is not comprehensive. The paper used eventnarrative for the graph-to-text generation. However, compared to WebNLG  (2020), the EventNarrative has fewer baselines and most of its baselines are outdated. The paper must also add WebNLG(2020) as an additional graph-to-text generation tool to show its contributions. The pretraining datasets and the testing datasets are all Wiki-style datasets. The performance gain may come from pretraining (data leakage) instead of the actual model architecture. The paper needs to include an additional ablation study to show the gain of the eventnarrative/webnlg comes from the model architecture, or also pretraining the proposed baselines on TEKGEN/GenWiki. Otherwise, the scores are not comparable. Additionally, the paper needs to include some of the latest frameworks like [4] since all of the baselines used in the paper are old. The paper also needs to include some human evaluation or qualitative analysis to help readers understand the generation results better. Furthermore, the comparison in Table 4 is not fair, since domain-specific pretraining is more useful than general pretraining [5].
3. The paper puts important information in the Appendix while not reaching the page limit (9 pages). 

[1] Herzig, J., Nowak, P. K., Müller, T., Piccinno, F., & Eisenschlos, J. M. (2020). TaPas: Weakly supervised table parsing via pre-training. ACL 2020.

[2] Wang, Q., Yavuz, S., Lin, V., Ji, H., & Rajani, N. (2021). Stage-wise fine-tuning for graph-to-text generation. arXiv preprint ACL 2021 SRW.

[3] Chen, W., Su, Y., Yan, X., & Wang, W. Y. (2020). KGPT: Knowledge-grounded pre-training for data-to-text generation. EMNLP 2020.

[4] Wang, Z., Collins, M., Vedula, N., Filice, S., Malmasi, S., & Rokhlenko, O. (2023). Faithful low-resource data-to-text generation through cycle training. ACL 2023

[5] Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., & Smith, N. A. (2020). Don't stop pretraining: Adapt language models to domains and tasks. ACL 2020.

Limitations:
The paper includes a limitation section in the Appendix and a Discussion section in the main paper.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
- The paper highlights the limitations of two existing methods for generating text graphs: 1. The multi-stage approach does not consider multi-hop relations and cannot handle the case where two concepts have more than one relation. 2. The graph linearization approach introduces extra complexity to the Language Model (LM) and the predictions are altered if the generated triples are shuffled.
- Building on this, the paper proposes the Structure Token method. Specifically, it identifies a graph element (node or edge) using seven parts. Then, it transforms Structure Tokens into embeddings using OneHot and orthonormal-like vectors, which are then input into a Transformer Encoder-Decoder model.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The method proposed in the paper can avoid extra computation, such as the duplication of concepts.
- The experimental results presented in the paper show that the model performs comparably to models with a larger number of parameters while using fewer parameters, suggesting that the model might perform even better with an increased number of parameters.

Weaknesses:
- The paper's layout is not aesthetically pleasing, particularly the formatting of Tables 1-3.
- The experiments lack error bars, which diminishes the credibility of the results.
- The experimental results are unsatisfactory and fail to demonstrate the superiority of the model, and the experiments are incomplete.
  - The model's results did not achieve state-of-the-art (SOTA) performance in the G2T/T2G tasks.
  - Lines 257-259 state, ""In conclusion, our structure token approach can achieve comparable performance on text-to-graph generation under similar model size without using special training methods or loss functions."" The results in Table 3 are nearly identical to those of Grapher-small (Text) T2G. Combined with the absence of error bars, it is challenging to determine whether the similarities are due to errors or model efficacy, making it difficult to convince.
  - Existing results only may demonstrate that the model performs comparably to models with larger parameters under fewer parameters. This does not prove that increasing the number of parameters will enhance performance. A more direct comparison of experimental results is necessary to substantiate this claim. The explanations in Section 5, ""Scaling Up,"" are overly subjective and unconvincing, making the paper seem like a work in progress.

Limitations:
Author addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
IfpNsorodK;"REVIEW 
Summary:
This work proposes a training-free time step-skipping method that can be used with existing ODE solvers for reduced NFE. The method was motivated by two observations: 1) a significant similarity in the model's outputs at time step size during the denoising process and 2) a high resemblance between the denoising process and SGD. The proposed method employed gradient replacement from past time steps and rapidly updated intermediate states inspired by Nesterov momentum. The proposed method yielded promising results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- Experimental results look promising with multiple diffusion models on diverse datasets.
- Accelerating diffusion models for sampling is an important issue and this work tried to address it.

Weaknesses:
- There have been a lot of prior works on accelerating diffusion models for sampling. While this manuscript cited many, it still missed important prior works - some of them look quite similar to the proposed method. Thus, the novelty of the proposed method is unclear in the current form of this manuscript. For example, using Nesterov acceleration for fast diffusion models is not really new (e.g., R Li et al., Hessian-Free High-Resolution Nesterov Acceleration For Sampling, ICML 2022). Eq (15) of this work can be seen as a special case of the following prior works such as [R1], DeepCache [28] (using past), [R3] (using three moments or future) or [R2] (using all). Some recent work like [R4] even used partial caching instead of using the whole results. A more theoretically grounded work on using Nesterov momentum for sampling can be found in [R5]. 
[R1] M Xia et al., Towards More Accurate Diffusion Model Acceleration with A Timestep Tuner, CVPR 2024.
[R2] A Pokle et al., Deep Equilibrium Approaches to Diffusion Models, NeurIPS 2022.
[R3] H Guo et al., Gaussian Mixture Solvers for Diffusion Models, NeurIPS 2023.
[R4] F Wimbauer et al., Cache Me if You Can: Accelerating Diffusion Models through Block Caching, CVPR 2023.
[R5] R Li et al., Hessian-Free High-Resolution Nesterov Acceleration For Sampling, ICML 2022.
- A number of acceleration works for diffusion models also investigated the feasibility of the parallel computation. Will the proposed method be parallelized for computation? 
- It is unclear if the proposed method was compared with other methods in terms of computation. Will 1 NFE of the proposed method take the same computation time as 1 NFE of other methods since the proposed method contains multiple evaluations of the neural network as in Eq. (15).
- The notation and explanation are quite confusing, so it is not easy to understand the whole idea as well as the algorithm itself.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new training-free acceleration method for the inference of diffusion probabilistic models. The key components of the presented time-skipping strategy are the use of past and future gradients to eliminate redundant neural function evaluations (NFE). The proposed method is shown effective compared to other training-free acceleration methods, leading to solid performance improvements especially for ODE solvers with less than 10 NFEs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
*    The method is training-free and can complement existing fast ODE solvers
*    The paper is well structured and puts the presented method in the proper context with respect to existing methods
*    The experimental results cover conditional and unconditional settings, showing performance improvements across the board.

Weaknesses:
*    The performance gap compared to training-based methods is still apparent, especially considering the latest distillation techniques resulting in one-step models.
*    The mathematical notations are a bit hard to follow up. I would advise the authors to add a schematic clarifying for a given setting of hyperparameters, which timepoints are being evaluated and which are being skipped.
*    The optimal setting of hyperparameters *k,l* is model/dataset dependent and it is not clear apriori how to set these. Therefore, this requires empirical experimentation which makes it time-consuming to get optimal performance when using the method out-of-the-box.

Limitations:
The authors were upfront about the limitations of their method.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
To accelerate the sampling speed in diffusion models, this paper proposes a training-free denoising method, dubbed PFDiff. 
Concretely, PFDiff employs the gradient from past time steps to update intermediate states, aiming to reduce unnecessary NFEs while correcting for discretization errors.
In this manner, PFDiff enables to improve classic samplers without any training computation.
Importantly, experimental results demonstrate the effectiveness of the proposed PFDiff.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. Reducing discretization errors in diffusion models in a training-free manner is attractive and practical.

2. The motivation of using previous gradients to guide the current sampling direction is intuitively plausible, and the proposed method is technically sound.

3. The presentation is excellent and the figures all are readable.

Weaknesses:
1. In my humble opinion, the theoretical analysis part is naive. Can you provide more explanation about why previous gradients is helpful to guide current sampling direction? Since different noise levels correspond to different gradients, is there any harm in denoising images with the proposed method?

2. Many works investigate using previous gradients to improve sampling speed, so the contribution is limited.

Limitations:
Please see in Weaknesses and Questions. If all of my concerns are addressed, I will improve my score.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes PFDiff, a fast training-free sampler for diffusion models. PFDiff updates the current state with both the past score network evaluation and the future score network evaluation. It can achieve good sample quality with less than 10 NFE. The authors showcase the effectiveness of PFDiff on various pre-trained diffusion models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. With proper tuning, the proposed PFDiff can outperform existing ODE solvers in the low-NFE regime on various datasets. 
2. The authors provide comprehensive technical details about the proposed algorithm.

Weaknesses:
1. Flawed justification for future gradient: The authors' claim that using future gradient information is better than using current gradient information is based on the mean value theorem (lines 164-167 and Appendix B.2). However, this theorem only guarantees the existence of an optimal point within an interval, not its specific location. Therefore, the mean value theorem itself doesn't justify the preference for future gradients. 
2. Missing justification for the approximation: While the authors claim that their approximation is better, there is no theoretical justification for it. The proof in Appendix B.2 assumes that the optimal point is already known, which is not informative. The manuscript will benefit from a further approximation error analysis. 
3. Expensive and case-specific tuning: the proposed method essentially defines a set of candidate points and searches for the optimal point by tuning parameters $k$ and $l$. This tuning process can be computationally expensive and needs to be done for each specific case, limiting its practicality.

Limitations:
The algorithm performance depends heavily on parameters $k$ and $l$ as shown in Table 7.  Optimal values for $k$ and $l$ vary based on the pre-trained model and the number of function evaluations. This necessitates extensive parameter tuning when applying the proposed method in practice.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes PFDiff, a training-free approach for accelerating diffusion models. Motivated by the high similarity of the diffusion network outputs at adjacent timesteps on the sampling trajectory, PFDiff utilizes past and future information for sampling with time-skipping, 
and decreases the number of function evaluations (NFEs) significantly. Experiments on various settings show significant acceleration, especially in the low NFE regime.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- The method is training-free and can be plug into existing solvers.
- The motivation and overall method seem reasonable.
- The improvements is significant especially in the low NFE regime. State-of-the-art diffusion solvers like UniPC and DPM-Solver-v3 are compared.
- The finding that first-order solver (DDIM), along with PFDiff, can outperform high-order solvers, is intriguing.

Weaknesses:
- The highly concise writing and complex notations might be a bit confusing. Additional illustrations for certain local algorithm procedures can be helpful for understanding the overall idea.
- There are fundamental mistakes in the writing. Eqn. (8) (9) are represented as Euler discretizations of the original PF-ODE. However, both DDIM and the series of DPM-Solvers rely on exponential integrators to transform the PF-ODE into other forms, so that the linear term $x_t$ is cancelled. Though this does not mean the method is wrong, such simplified writing can be misleading. The authors are obligated to correct this, or I will be forced to reject this paper.
- It will be more convincing to include experiments on EDM, the SOTA diffusion model on CIFAR-10 and ImageNet 64x64.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a training-free time step-skipping method that can be used with existing ODE solvers for reduced NFE. The method was motivated by two observations: 1) a significant similarity in the model's outputs at time step size during the denoising process and 2) a high resemblance between the denoising process and SGD. The proposed method employed gradient replacement from past time steps and rapidly updated intermediate states inspired by Nesterov momentum. The proposed method yielded promising results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- Experimental results look promising with multiple diffusion models on diverse datasets.
- Accelerating diffusion models for sampling is an important issue and this work tried to address it.

Weaknesses:
- There have been a lot of prior works on accelerating diffusion models for sampling. While this manuscript cited many, it still missed important prior works - some of them look quite similar to the proposed method. Thus, the novelty of the proposed method is unclear in the current form of this manuscript. For example, using Nesterov acceleration for fast diffusion models is not really new (e.g., R Li et al., Hessian-Free High-Resolution Nesterov Acceleration For Sampling, ICML 2022). Eq (15) of this work can be seen as a special case of the following prior works such as [R1], DeepCache [28] (using past), [R3] (using three moments or future) or [R2] (using all). Some recent work like [R4] even used partial caching instead of using the whole results. A more theoretically grounded work on using Nesterov momentum for sampling can be found in [R5]. 
[R1] M Xia et al., Towards More Accurate Diffusion Model Acceleration with A Timestep Tuner, CVPR 2024.
[R2] A Pokle et al., Deep Equilibrium Approaches to Diffusion Models, NeurIPS 2022.
[R3] H Guo et al., Gaussian Mixture Solvers for Diffusion Models, NeurIPS 2023.
[R4] F Wimbauer et al., Cache Me if You Can: Accelerating Diffusion Models through Block Caching, CVPR 2023.
[R5] R Li et al., Hessian-Free High-Resolution Nesterov Acceleration For Sampling, ICML 2022.
- A number of acceleration works for diffusion models also investigated the feasibility of the parallel computation. Will the proposed method be parallelized for computation? 
- It is unclear if the proposed method was compared with other methods in terms of computation. Will 1 NFE of the proposed method take the same computation time as 1 NFE of other methods since the proposed method contains multiple evaluations of the neural network as in Eq. (15).
- The notation and explanation are quite confusing, so it is not easy to understand the whole idea as well as the algorithm itself.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new training-free acceleration method for the inference of diffusion probabilistic models. The key components of the presented time-skipping strategy are the use of past and future gradients to eliminate redundant neural function evaluations (NFE). The proposed method is shown effective compared to other training-free acceleration methods, leading to solid performance improvements especially for ODE solvers with less than 10 NFEs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
*    The method is training-free and can complement existing fast ODE solvers
*    The paper is well structured and puts the presented method in the proper context with respect to existing methods
*    The experimental results cover conditional and unconditional settings, showing performance improvements across the board.

Weaknesses:
*    The performance gap compared to training-based methods is still apparent, especially considering the latest distillation techniques resulting in one-step models.
*    The mathematical notations are a bit hard to follow up. I would advise the authors to add a schematic clarifying for a given setting of hyperparameters, which timepoints are being evaluated and which are being skipped.
*    The optimal setting of hyperparameters *k,l* is model/dataset dependent and it is not clear apriori how to set these. Therefore, this requires empirical experimentation which makes it time-consuming to get optimal performance when using the method out-of-the-box.

Limitations:
The authors were upfront about the limitations of their method.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
To accelerate the sampling speed in diffusion models, this paper proposes a training-free denoising method, dubbed PFDiff. 
Concretely, PFDiff employs the gradient from past time steps to update intermediate states, aiming to reduce unnecessary NFEs while correcting for discretization errors.
In this manner, PFDiff enables to improve classic samplers without any training computation.
Importantly, experimental results demonstrate the effectiveness of the proposed PFDiff.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. Reducing discretization errors in diffusion models in a training-free manner is attractive and practical.

2. The motivation of using previous gradients to guide the current sampling direction is intuitively plausible, and the proposed method is technically sound.

3. The presentation is excellent and the figures all are readable.

Weaknesses:
1. In my humble opinion, the theoretical analysis part is naive. Can you provide more explanation about why previous gradients is helpful to guide current sampling direction? Since different noise levels correspond to different gradients, is there any harm in denoising images with the proposed method?

2. Many works investigate using previous gradients to improve sampling speed, so the contribution is limited.

Limitations:
Please see in Weaknesses and Questions. If all of my concerns are addressed, I will improve my score.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes PFDiff, a fast training-free sampler for diffusion models. PFDiff updates the current state with both the past score network evaluation and the future score network evaluation. It can achieve good sample quality with less than 10 NFE. The authors showcase the effectiveness of PFDiff on various pre-trained diffusion models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. With proper tuning, the proposed PFDiff can outperform existing ODE solvers in the low-NFE regime on various datasets. 
2. The authors provide comprehensive technical details about the proposed algorithm.

Weaknesses:
1. Flawed justification for future gradient: The authors' claim that using future gradient information is better than using current gradient information is based on the mean value theorem (lines 164-167 and Appendix B.2). However, this theorem only guarantees the existence of an optimal point within an interval, not its specific location. Therefore, the mean value theorem itself doesn't justify the preference for future gradients. 
2. Missing justification for the approximation: While the authors claim that their approximation is better, there is no theoretical justification for it. The proof in Appendix B.2 assumes that the optimal point is already known, which is not informative. The manuscript will benefit from a further approximation error analysis. 
3. Expensive and case-specific tuning: the proposed method essentially defines a set of candidate points and searches for the optimal point by tuning parameters $k$ and $l$. This tuning process can be computationally expensive and needs to be done for each specific case, limiting its practicality.

Limitations:
The algorithm performance depends heavily on parameters $k$ and $l$ as shown in Table 7.  Optimal values for $k$ and $l$ vary based on the pre-trained model and the number of function evaluations. This necessitates extensive parameter tuning when applying the proposed method in practice.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes PFDiff, a training-free approach for accelerating diffusion models. Motivated by the high similarity of the diffusion network outputs at adjacent timesteps on the sampling trajectory, PFDiff utilizes past and future information for sampling with time-skipping, 
and decreases the number of function evaluations (NFEs) significantly. Experiments on various settings show significant acceleration, especially in the low NFE regime.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- The method is training-free and can be plug into existing solvers.
- The motivation and overall method seem reasonable.
- The improvements is significant especially in the low NFE regime. State-of-the-art diffusion solvers like UniPC and DPM-Solver-v3 are compared.
- The finding that first-order solver (DDIM), along with PFDiff, can outperform high-order solvers, is intriguing.

Weaknesses:
- The highly concise writing and complex notations might be a bit confusing. Additional illustrations for certain local algorithm procedures can be helpful for understanding the overall idea.
- There are fundamental mistakes in the writing. Eqn. (8) (9) are represented as Euler discretizations of the original PF-ODE. However, both DDIM and the series of DPM-Solvers rely on exponential integrators to transform the PF-ODE into other forms, so that the linear term $x_t$ is cancelled. Though this does not mean the method is wrong, such simplified writing can be misleading. The authors are obligated to correct this, or I will be forced to reject this paper.
- It will be more convincing to include experiments on EDM, the SOTA diffusion model on CIFAR-10 and ImageNet 64x64.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
IdEaeCbhUW;"REVIEW 
Summary:
This paper addresses the problem that commonly in RL, small reaction times and high action frequencies are required, which is not the case for computations in the brain. As a more accurate model, the authors propose an RL method that learns an internal model to improve performance in high-latency applications.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper rightfully describes the issue that, usually, RL methods operate under different conditions than the human brain (higher latencies/lack of information). Developing a biologically plausible model that can deal with higher latencies and lower perceptual frequencies would be beneficial to get a better understanding of the human brain and for realizing certain applications in robotics. The approach of learning an internal model guiding the actions, if no observations are present, seems to follow intuition and could be promising. I would therefore rate the problem as relevant and interesting, and the general model (at least on a computational level - this could be stated more clearly in the paper) as ""plausible"".

The problem and approach (besides my points in the Weaknesses/Questions sections) are clearly described. The method is evaluated on a wide range of six control problems, showing that the model can produce reasonable control signals.

Weaknesses:
I found a few parts of the paper difficult to follow (mainly the policy and evaluation section, see also my questions for this), and the paper could benefit from improvements in these parts. In particular, I am uncertain whether the approach introduced in the paper fits the problem description. It seems that the approach targets the setting of high action frequency but not delays of perception.

Furthermore, I had problems understanding how the policy of the proposed approach works. The authors seem to define a probabilistic policy ($\pi_\omega$) defined by parameters $\omega$ with the past previous action as input (line 177). In Eq. 5, which describes the loss for learning the policy, the previous action does not seem to appear. For this loss, they need state-action pairs from a deterministic policy $\pi_\psi$. For a final evaluation, I would need clarification on this (see questions).

Also, the experiment section was not easy to follow. I believe it would be valuable to give an overview at the beginning of the section about how the evaluation is structured and state the goals of each conducted experiment (linked to the motivation of the paper). As an example, I think (please correct me if I am mistaken) that the comparison to SAC was conducted to show that the proposed method can learn a controller that, even with the limitation of a lower frequency, does not compromise much performance. I am a bit unclear as to why the proposed method outperforms SAC (what it does in 4/6 tasks), as to my understanding the strength of the method should be in the specific scenario where hardware is more similar to the human brain. That the proposed method achieves higher performance even in this standard scenario suggests to me rather a lack of appropriate parameter tuning. Also, the setting of the ASL subsection and Online planning section should be stated more clearly (see questions).

In lines 104-105, it is confusing to me to imply that the proposed method in comparison to model-based RL has the advantage that a model is not needed after training. The main purpose of applying model-based RL is to learn a model with the option to replan after training. Model-free RL which could be more similar for this application is not mentioned.

Section 3.2. is about macro actions but it does not even provide in a single sentence an overview of what it is. The authors claim that an advantage of their approach is that it uses the principles of RL. Based on my understanding the concepts of hierarchical reinforcement learning and movement primitives would be very relevant here but are not discussed.


## Minor:
- Line 17 ""not""
- Line 163 $\psi$ should be subscript
- Line 202 Typo in ""Experiemental""
- I find it confusing that parentheses are used for both equations and citations. In most papers, therefore, for citations square brackets are used.
- The plots do not use well the space in the paper (large whitespaces between subplots, the graphs could be made wider), and font sizes between plots differ significantly. Legends are usually integrated into the first subplot only but could be put, e.g., next to the plots to make this information more obvious.
- The Readme of the provided code seems to be incomplete. The code cannot be directly used to reproduce the figures of the paper.

Limitations:
I think so.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the Hindsight-Sequence-Planner (HSP), a reinforcement learning (RL) model inspired by the brain's ability to achieve precise control using slow neurons. The model aims to mimic human-like sensory and reaction times by leveraging an environmental model for sequence learning. HSP demonstrates competitive performance with fewer observations and actor calls compared to faster RL models. The model is evaluated on various continuous control tasks, showing robust performance even with longer action sequences.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The concept of mimicking brain-like conditions in RL models is innovative and offers a fresh perspective on sequence learning. The integration of temporal recall and sequence learning inspired by neural mechanisms is novel.
2. The use of a temporal recall mechanism allows for fine-tuned action sequence learning despite operating on slower hardware.
3. HSP demonstrates competitive performance across various continuous control tasks, showcasing its robustness and adaptability.
4. The experiments cover a range of continuous control tasks and provide comparisons with state-of-the-art models like Soft Actor-Critic (SAC), highlighting HSP’s efficiency.

Weaknesses:
1. The paper lacks specific numerical performance comparisons to quantify improvements over baseline models.
2. Sections like ""Learning the Model"" and ""Learning Critic"" need further elaboration to highlight their specific contributions and novelty.
3. The broader implications and potential real-world applications of HSP are not fully discussed.
4. There is insufficient discussion on how HSP handles situations with highly inaccurate model predictions.
5. The related work section lacks details on ""Macro-Actions,"" the scalability issues of current methods, and the meaning of ""principles.""

Limitations:
1. The authors mention the reliance on an inaccurate model but could provide more in-depth analysis on how this affects different types of tasks?
2. The scalability of HSP to very large action spaces and high-dimensional state spaces is not thoroughly discussed.
3. Typo Issue: In Section Abstract: demonstrating that it not can achieve comparable performance at ‘human-like’ frequencies by relying on significantly fewer observations and actor calls. -->demonstrating that it can achieve comparable performance at ‘human-like’ frequencies by relying on significantly fewer observations and actor calls.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed HSP, a bio-mimic framework for learning-based control. Motivated by human brains, HSP can deal with observation and computation in different frequencies by making the ""actor"" produce action sequences, similar to the functioning pattern of ganglia and the prefrontal cortex in human brains. HSP employs a model-based training approach to achieve model-free control, resulting in precise behavior despite running on slow hardware. The authors demonstrate the performance of HSP on various continuous control tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well structured with clear figures, and the motivation and introduction are interesting, bridging the gap between biological and artificial reinforcement learning systems. The empirical results across various continuous control tasks are impressive as well. Overall, the work presents a promising direction for developing more efficient and adaptable RL algorithms that could have broad implications for robotics and other real-world applications.

Weaknesses:
* One major concern is the performance of the proposed framework. The experiment results do not show a significantly better performance than traditional RL methods, and the performances are worse when $J=16$ in most cases. The latent space variant of HSP shows improvements only in one environment (Walker2d). Further investigation into why this approach doesn't generalize well to other environments would be valuable.
* The novelty of the proposed framework is also questionable. In control literature, especially trajectory optimization control, producing action sequences is commonly used, and a similar technique could be viewed as a variant of model predictive control (MPC). It would be better if the authors could provide a detailed comparison with existing control algorithms (like MPC) in the experiment section. 
* The comparison with model-based online planning is somewhat limited. A more comprehensive comparison with state-of-the-art model-based RL methods would provide better context for HSP's contributions.
* Just a small suggestion - The title of the paper mentions ""hardware,"" so I expect to see some real-world control experiments in the paper. It would be nice if the author could really demonstrate the control performance using ""slow hardware"" like Raspberry Pi or even slower platform.

Limitations:
Limitations are discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem that commonly in RL, small reaction times and high action frequencies are required, which is not the case for computations in the brain. As a more accurate model, the authors propose an RL method that learns an internal model to improve performance in high-latency applications.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper rightfully describes the issue that, usually, RL methods operate under different conditions than the human brain (higher latencies/lack of information). Developing a biologically plausible model that can deal with higher latencies and lower perceptual frequencies would be beneficial to get a better understanding of the human brain and for realizing certain applications in robotics. The approach of learning an internal model guiding the actions, if no observations are present, seems to follow intuition and could be promising. I would therefore rate the problem as relevant and interesting, and the general model (at least on a computational level - this could be stated more clearly in the paper) as ""plausible"".

The problem and approach (besides my points in the Weaknesses/Questions sections) are clearly described. The method is evaluated on a wide range of six control problems, showing that the model can produce reasonable control signals.

Weaknesses:
I found a few parts of the paper difficult to follow (mainly the policy and evaluation section, see also my questions for this), and the paper could benefit from improvements in these parts. In particular, I am uncertain whether the approach introduced in the paper fits the problem description. It seems that the approach targets the setting of high action frequency but not delays of perception.

Furthermore, I had problems understanding how the policy of the proposed approach works. The authors seem to define a probabilistic policy ($\pi_\omega$) defined by parameters $\omega$ with the past previous action as input (line 177). In Eq. 5, which describes the loss for learning the policy, the previous action does not seem to appear. For this loss, they need state-action pairs from a deterministic policy $\pi_\psi$. For a final evaluation, I would need clarification on this (see questions).

Also, the experiment section was not easy to follow. I believe it would be valuable to give an overview at the beginning of the section about how the evaluation is structured and state the goals of each conducted experiment (linked to the motivation of the paper). As an example, I think (please correct me if I am mistaken) that the comparison to SAC was conducted to show that the proposed method can learn a controller that, even with the limitation of a lower frequency, does not compromise much performance. I am a bit unclear as to why the proposed method outperforms SAC (what it does in 4/6 tasks), as to my understanding the strength of the method should be in the specific scenario where hardware is more similar to the human brain. That the proposed method achieves higher performance even in this standard scenario suggests to me rather a lack of appropriate parameter tuning. Also, the setting of the ASL subsection and Online planning section should be stated more clearly (see questions).

In lines 104-105, it is confusing to me to imply that the proposed method in comparison to model-based RL has the advantage that a model is not needed after training. The main purpose of applying model-based RL is to learn a model with the option to replan after training. Model-free RL which could be more similar for this application is not mentioned.

Section 3.2. is about macro actions but it does not even provide in a single sentence an overview of what it is. The authors claim that an advantage of their approach is that it uses the principles of RL. Based on my understanding the concepts of hierarchical reinforcement learning and movement primitives would be very relevant here but are not discussed.


## Minor:
- Line 17 ""not""
- Line 163 $\psi$ should be subscript
- Line 202 Typo in ""Experiemental""
- I find it confusing that parentheses are used for both equations and citations. In most papers, therefore, for citations square brackets are used.
- The plots do not use well the space in the paper (large whitespaces between subplots, the graphs could be made wider), and font sizes between plots differ significantly. Legends are usually integrated into the first subplot only but could be put, e.g., next to the plots to make this information more obvious.
- The Readme of the provided code seems to be incomplete. The code cannot be directly used to reproduce the figures of the paper.

Limitations:
I think so.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the Hindsight-Sequence-Planner (HSP), a reinforcement learning (RL) model inspired by the brain's ability to achieve precise control using slow neurons. The model aims to mimic human-like sensory and reaction times by leveraging an environmental model for sequence learning. HSP demonstrates competitive performance with fewer observations and actor calls compared to faster RL models. The model is evaluated on various continuous control tasks, showing robust performance even with longer action sequences.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The concept of mimicking brain-like conditions in RL models is innovative and offers a fresh perspective on sequence learning. The integration of temporal recall and sequence learning inspired by neural mechanisms is novel.
2. The use of a temporal recall mechanism allows for fine-tuned action sequence learning despite operating on slower hardware.
3. HSP demonstrates competitive performance across various continuous control tasks, showcasing its robustness and adaptability.
4. The experiments cover a range of continuous control tasks and provide comparisons with state-of-the-art models like Soft Actor-Critic (SAC), highlighting HSP’s efficiency.

Weaknesses:
1. The paper lacks specific numerical performance comparisons to quantify improvements over baseline models.
2. Sections like ""Learning the Model"" and ""Learning Critic"" need further elaboration to highlight their specific contributions and novelty.
3. The broader implications and potential real-world applications of HSP are not fully discussed.
4. There is insufficient discussion on how HSP handles situations with highly inaccurate model predictions.
5. The related work section lacks details on ""Macro-Actions,"" the scalability issues of current methods, and the meaning of ""principles.""

Limitations:
1. The authors mention the reliance on an inaccurate model but could provide more in-depth analysis on how this affects different types of tasks?
2. The scalability of HSP to very large action spaces and high-dimensional state spaces is not thoroughly discussed.
3. Typo Issue: In Section Abstract: demonstrating that it not can achieve comparable performance at ‘human-like’ frequencies by relying on significantly fewer observations and actor calls. -->demonstrating that it can achieve comparable performance at ‘human-like’ frequencies by relying on significantly fewer observations and actor calls.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed HSP, a bio-mimic framework for learning-based control. Motivated by human brains, HSP can deal with observation and computation in different frequencies by making the ""actor"" produce action sequences, similar to the functioning pattern of ganglia and the prefrontal cortex in human brains. HSP employs a model-based training approach to achieve model-free control, resulting in precise behavior despite running on slow hardware. The authors demonstrate the performance of HSP on various continuous control tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well structured with clear figures, and the motivation and introduction are interesting, bridging the gap between biological and artificial reinforcement learning systems. The empirical results across various continuous control tasks are impressive as well. Overall, the work presents a promising direction for developing more efficient and adaptable RL algorithms that could have broad implications for robotics and other real-world applications.

Weaknesses:
* One major concern is the performance of the proposed framework. The experiment results do not show a significantly better performance than traditional RL methods, and the performances are worse when $J=16$ in most cases. The latent space variant of HSP shows improvements only in one environment (Walker2d). Further investigation into why this approach doesn't generalize well to other environments would be valuable.
* The novelty of the proposed framework is also questionable. In control literature, especially trajectory optimization control, producing action sequences is commonly used, and a similar technique could be viewed as a variant of model predictive control (MPC). It would be better if the authors could provide a detailed comparison with existing control algorithms (like MPC) in the experiment section. 
* The comparison with model-based online planning is somewhat limited. A more comprehensive comparison with state-of-the-art model-based RL methods would provide better context for HSP's contributions.
* Just a small suggestion - The title of the paper mentions ""hardware,"" so I expect to see some real-world control experiments in the paper. It would be nice if the author could really demonstrate the control performance using ""slow hardware"" like Raspberry Pi or even slower platform.

Limitations:
Limitations are discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
GxpkSJjbQR;"REVIEW 
Summary:
This work introduces a novel approach to diagnosing Alzheimer's Disease using a decentralized expert system. This system leverages blockchain technology and Federated Learning to enhance data privacy and manage large volumes of MRI data effectively. The key innovation lies in integrating these technologies to address the challenges of traditional diagnostic methods, which often suffer from delays and inaccuracies, especially in the early stages of the disease.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. This work presents a pioneering integration of blockchain technology and Federated Learning to enhance Alzheimer's Disease (AD) diagnostics, addressing privacy concerns and data management challenges.
2. The proposed decentralized expert system architecture, which includes anomaly detection for patient-submitted data, showcases a comprehensive approach to AD diagnostics, emphasizing AI-driven MRI analysis.

Weaknesses:
1. While the system shows promising results, the article does not provide extensive comparative data against traditional centralized systems or other decentralized approaches, which could validate its superiority more robustly. This work lacks of comparative performance data.
2. The complexity of the blockchain and Federated Learning components might pose usability challenges for less technically adept users, potentially affecting the system's adoption.
3. There are no more details of the algorithms this work used, maybe give out more meaningful algorithm design for the specific model you are using.

Limitations:
1. The accuracy of the AI model heavily depends on the quality and consistency of input data, which might vary significantly across different healthcare settings.
2. The use of advanced technologies such as blockchain might limit the accessibility of the system for users not familiar with such technology, potentially restricting its applicability.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assume that applying blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training can improve diagnostic performance. However, the manuscript lacks technical details and experimental evidence. All descriptions are conceptual, making the manuscript a proposal rather than a technical paper.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It is interesting to apply blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training.

Weaknesses:
There are no technical details and no experiments. Details can be found in Questions and Limitations.

Limitations:
1.	No technical details and experiments.
2.	The literature review of Alzheimer’s Disease diagnosis is not complete, especially regarding AI-based approaches.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a decentralized expert system designed to predict early-stage Alzheimer's Disease using AI-driven MRI analysis. The system leverages blockchain technology and Federated Learning to ensure data privacy and security while performing anomaly detection on patient-submitted data. The architecture includes a Web3 application for patients to upload biological information and MRI images securely. The decentralized approach aims to improve early detection and intervention for Alzheimer's Disease, providing a more comprehensive representation of AD patterns and enhancing model performance through data diversity.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper encapsulates a few novel ideas. They can be summarized as follows:

1. Handling the security and sensitivity of patient medical information is of paramount importance. The authors were motivated by a very relevant problem and presented an approach to blockchain technology with stated aim of providing robust data privacy and security. By building on decades on research on this topic, this approach has the potential to be extended in future with the general updates in this domain.
2. While there are some confusion around their use case (see weakness below), the authors leveraging Federated Learning and a decentralized system to mitigate the challenges associated with model training on centralized data repositories, such as data bottlenecks and privacy concerns
3. The system aims to provide early-stage prediction of Alzheimer's Disease, which is crucial for timely intervention and improved patient outcomes.

Weaknesses:
However given the commendable motivations there are several challenges with the current paper,

1. First and perhaps the most important aspect is that the paper fails to present the real-world challenges associated with the adoption of such decentralized approaches, especially as it pertains to patients engaging with blockchain wallets and data submission interfaces. Also, the primary use-case for the decentralized approach is not evident - is model training the prime use-case or is the main use case patients being able to generate inferences on their own medical records. Overall, the usage scenario around the setup needs to be better motivated and established
2. The paper also lacks formalism around the presentation. For example, if the primary contribution is the architecture around the decentralized AI approach, the design principles needs to be better justified and articulated. A system architecture diagrams needs to be established as well. Similarly, the ""proof"" around the decentralized approach is not a rigorous mathematical proof. Rather the logic is derived from a hypothesis that more diverse data should lead to a better model. This is a hypothesis at the best and needs to be experimentally validates
3. Finally, the paper is lacking in experimental validation. For example, the proof needs to be backed by real world experiments. Also, this is not the first paper to posit a federated learning approach to medical AI prediction. Some of the SOTA methods in this space needs to be compared against

Limitations:
Please see the weakness above

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces an innovative decentralized expert system designed for early prediction of Alzheimer's Disease (AD), leveraging blockchain technology and Federated Learning. Traditional diagnostic methods often result in delays and imprecision, particularly in early-stage AD detection, while centralized data repositories face challenges in managing vast volumes of MRI data and maintaining patient privacy. The proposed system addresses these issues by combining blockchain for secure, decentralized data management and Federated Learning for collaborative AI model training across multiple institutions. The system includes robust anomaly detection mechanisms to ensure data quality and integrity, enabling precise early-stage AD predictions. This comprehensive approach aims to revolutionize disease diagnostics by enhancing data privacy, security, and collaborative efforts in the medical community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a novel integration of blockchain technology and Federated Learning for early AD prediction, which is innovative in addressing data privacy, security, and collaborative AI model training.
- The proposed system is well-conceived, with a detailed architecture and implementation strategy. The inclusion of anomaly detection mechanisms to ensure data quality adds robustness to the system.
- The approach has significant potential to improve early-stage AD detection, which is crucial for timely intervention and better patient outcomes. The decentralized nature of the system promotes data privacy and security, addressing major concerns in medical data management.

Weaknesses:
- The integration of blockchain and Federated Learning introduces significant computational complexity and potential delays due to off-chain processing and communication overhead.
- The system's scalability is a concern as the volume of data and the number of users increase, necessitating ongoing optimization to ensure efficient performance.

Limitations:
The authors have addressed several limitations, including data quality and consistency, computational complexity, and model generalizability. However, further discussion may be needed on:
- Ensuring that the AI model is unbiased and fair across different demographic groups can be difficult, especially if the training data is not representative.
- Real-time processing and predictions might be challenging due to the decentralized nature and the need for off-chain processing.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work introduces a novel approach to diagnosing Alzheimer's Disease using a decentralized expert system. This system leverages blockchain technology and Federated Learning to enhance data privacy and manage large volumes of MRI data effectively. The key innovation lies in integrating these technologies to address the challenges of traditional diagnostic methods, which often suffer from delays and inaccuracies, especially in the early stages of the disease.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. This work presents a pioneering integration of blockchain technology and Federated Learning to enhance Alzheimer's Disease (AD) diagnostics, addressing privacy concerns and data management challenges.
2. The proposed decentralized expert system architecture, which includes anomaly detection for patient-submitted data, showcases a comprehensive approach to AD diagnostics, emphasizing AI-driven MRI analysis.

Weaknesses:
1. While the system shows promising results, the article does not provide extensive comparative data against traditional centralized systems or other decentralized approaches, which could validate its superiority more robustly. This work lacks of comparative performance data.
2. The complexity of the blockchain and Federated Learning components might pose usability challenges for less technically adept users, potentially affecting the system's adoption.
3. There are no more details of the algorithms this work used, maybe give out more meaningful algorithm design for the specific model you are using.

Limitations:
1. The accuracy of the AI model heavily depends on the quality and consistency of input data, which might vary significantly across different healthcare settings.
2. The use of advanced technologies such as blockchain might limit the accessibility of the system for users not familiar with such technology, potentially restricting its applicability.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assume that applying blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training can improve diagnostic performance. However, the manuscript lacks technical details and experimental evidence. All descriptions are conceptual, making the manuscript a proposal rather than a technical paper.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It is interesting to apply blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training.

Weaknesses:
There are no technical details and no experiments. Details can be found in Questions and Limitations.

Limitations:
1.	No technical details and experiments.
2.	The literature review of Alzheimer’s Disease diagnosis is not complete, especially regarding AI-based approaches.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a decentralized expert system designed to predict early-stage Alzheimer's Disease using AI-driven MRI analysis. The system leverages blockchain technology and Federated Learning to ensure data privacy and security while performing anomaly detection on patient-submitted data. The architecture includes a Web3 application for patients to upload biological information and MRI images securely. The decentralized approach aims to improve early detection and intervention for Alzheimer's Disease, providing a more comprehensive representation of AD patterns and enhancing model performance through data diversity.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper encapsulates a few novel ideas. They can be summarized as follows:

1. Handling the security and sensitivity of patient medical information is of paramount importance. The authors were motivated by a very relevant problem and presented an approach to blockchain technology with stated aim of providing robust data privacy and security. By building on decades on research on this topic, this approach has the potential to be extended in future with the general updates in this domain.
2. While there are some confusion around their use case (see weakness below), the authors leveraging Federated Learning and a decentralized system to mitigate the challenges associated with model training on centralized data repositories, such as data bottlenecks and privacy concerns
3. The system aims to provide early-stage prediction of Alzheimer's Disease, which is crucial for timely intervention and improved patient outcomes.

Weaknesses:
However given the commendable motivations there are several challenges with the current paper,

1. First and perhaps the most important aspect is that the paper fails to present the real-world challenges associated with the adoption of such decentralized approaches, especially as it pertains to patients engaging with blockchain wallets and data submission interfaces. Also, the primary use-case for the decentralized approach is not evident - is model training the prime use-case or is the main use case patients being able to generate inferences on their own medical records. Overall, the usage scenario around the setup needs to be better motivated and established
2. The paper also lacks formalism around the presentation. For example, if the primary contribution is the architecture around the decentralized AI approach, the design principles needs to be better justified and articulated. A system architecture diagrams needs to be established as well. Similarly, the ""proof"" around the decentralized approach is not a rigorous mathematical proof. Rather the logic is derived from a hypothesis that more diverse data should lead to a better model. This is a hypothesis at the best and needs to be experimentally validates
3. Finally, the paper is lacking in experimental validation. For example, the proof needs to be backed by real world experiments. Also, this is not the first paper to posit a federated learning approach to medical AI prediction. Some of the SOTA methods in this space needs to be compared against

Limitations:
Please see the weakness above

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces an innovative decentralized expert system designed for early prediction of Alzheimer's Disease (AD), leveraging blockchain technology and Federated Learning. Traditional diagnostic methods often result in delays and imprecision, particularly in early-stage AD detection, while centralized data repositories face challenges in managing vast volumes of MRI data and maintaining patient privacy. The proposed system addresses these issues by combining blockchain for secure, decentralized data management and Federated Learning for collaborative AI model training across multiple institutions. The system includes robust anomaly detection mechanisms to ensure data quality and integrity, enabling precise early-stage AD predictions. This comprehensive approach aims to revolutionize disease diagnostics by enhancing data privacy, security, and collaborative efforts in the medical community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a novel integration of blockchain technology and Federated Learning for early AD prediction, which is innovative in addressing data privacy, security, and collaborative AI model training.
- The proposed system is well-conceived, with a detailed architecture and implementation strategy. The inclusion of anomaly detection mechanisms to ensure data quality adds robustness to the system.
- The approach has significant potential to improve early-stage AD detection, which is crucial for timely intervention and better patient outcomes. The decentralized nature of the system promotes data privacy and security, addressing major concerns in medical data management.

Weaknesses:
- The integration of blockchain and Federated Learning introduces significant computational complexity and potential delays due to off-chain processing and communication overhead.
- The system's scalability is a concern as the volume of data and the number of users increase, necessitating ongoing optimization to ensure efficient performance.

Limitations:
The authors have addressed several limitations, including data quality and consistency, computational complexity, and model generalizability. However, further discussion may be needed on:
- Ensuring that the AI model is unbiased and fair across different demographic groups can be difficult, especially if the training data is not representative.
- Real-time processing and predictions might be challenging due to the decentralized nature and the need for off-chain processing.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
DSVGACQ3sO;"REVIEW 
Summary:
The paper studies the behaviour of amortised (supervised) causal discovery methods based on different training data distributions and its relation to more traditional causal discovery and the related identifiability theory. The authors empirically validate the intuitions about supervised causal discovery and generalisation of supervised learning methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper studies the behaviour of amortised causal discovery methods which have previously been unstudied.
- The empirical insights generally validate the intuition about identifiability and generalisation. Some examples give interesting insights into the identifiability and performance in the case of mixed assumptions.

Weaknesses:
- The paper is a purely empirical study of the generalisation behaviour of supervised causal discovery methods, validating general intuition without thorough novel insights.
- Given the empirical nature of this paper, I'd have expected to see a more thorough comparison, e.g. setting up a leave-one-out generalisation study or more in-depth analyses of the prediction on interesting individual SCMs such as the non-identifiable example or the performance of the prediction from new samples from a training set SCM.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores why causal discovery from observational data, particularly with CSIvA, a transformer-based model, can achieve competitive performance despite seemingly avoiding the explicit assumptions that traditional methods make for identifiability. The authors demonstrate that constraints on the training data distribution implicitly define a prior on the test observations. When this prior is well-suited, the underlying model can be identifiable. In other words, prior knowledge of the test distribution is encoded in the training data through constraints on the structural causal model governing data generation.

Additionally, they provide a theoretical basis for training on observations sampled from multiple classes of identifiable SCMs, a strategy that enhances test generalization to a wide range of causal models. They show that training on mixtures of causal models offers an alternative approach that is less reliant on assumptions about the mechanisms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper bridges the gap between existing theoretical results on identifiability and practical observations. More importantly, it moves away from classical causality settings and quite restricted models, shifting towards more mainstream and modern models like transformers. This opens a pathway for causality research to integrate with large language models (LLMs), which represent the state-of-the-art in a wide range of applications.

Weaknesses:
The presentation can be significantly improved. Since the paper aims to offer novel insights, it is crucial to organize the arguments, theoretical results, and experimental findings effectively to support these insights.

Limitations:
n.a.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the level of generalization achievable when training a predictor to classify “X causes Y” vs. “Y causes X” from observational data. Motivated by recent works performing causal discovery using a pretrained transformer model, the works explores which cases result in predictors that generalize to graph-dataset pairs generated from unseen types of SCM models. This is mostly achieved through a set of empirical experiments on synthetic 2-node SCM data. The work derives a corollary of Hoyer et al [7] to argue why training on multiple identifiable classes of synthetic SCM instances may help generalization amortized causal discovery methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper takes a first step towards analysing why amortized causal discovery performs well in practice and often significantly better than classical approaches. This is generally an important direction and of interest to the field.

Weaknesses:
While the motivation of this work is generally well-grounded, the contribution and argument of the work itself have several weaknesses that, in my opinion, do not justify many of the claims made in the abstract, introduction, and throughout the paper.

First, a major aspect of amortized causal discovery with transformers (referenced in the title) is that of solving structure learning tasks in *high dimensions*. Lopez [10] already provide theoretical and empirical analyses of the bivariate case. Recent work showed that this idea can generalize to (very) large systems -- the literature of works on causal discovery with transformers cited in the paper all study significant large-dimensional problems (ranging from 20-100 variables). Despite this, the present paper limits its entire analysis to the bivariate case. Thus, it is misleading to claim the paper “demystifies amortized causal discovery with transformers”. No part of the analysis concerns multivariate causal discovery or transformers. The paper should be upfront and highlight much more clearly what its contributions are beyond Lopez et al [10], which already study the bivariate amortized causal discovery case.

The paper repeatedly states it analyses CSIvA. However, none of the algorithmic components of CSIvA, such as e.g. the auxiliary loss it is trained on or the architecture of the predictive model, are part of the analysis. The loss function studied here (p.3, l. 133) is the same as e.g. used by [13]. Hence, it would be more truthful to claim the analysis concerns general predictors trained on the classification task of X->Y vs X<-V, as in [10].

A major component of causal discovery performance is not only identifiability of the graph from the observational distribution, but also the intractably large search problem incurred by classical score- and constrained-based methods. The question is: do transformers outperform classical methods in large problem sizes because 1) (parts of) the graphs are identifiable to it, or 2) a prediction-based approach is better at finding the identifiable edges in a large system (as opposed to doing a search)? This question motivates amortizing causal discovery in the first place, but the two-variable special case studied here is ill-suited for answering it. Since the work only studies the bivariate case, the title and claims throughout the paper, as well as their ties to the (large-scale) transformer literature have to be recalibrated.

Section 3.2 seems unnecessary. The section only studies the generalization ability of CSIvA, which is no contribution. The takeaways (lines 195-) that “CSIvA generalizes well to test data generated by the same class of SCMs used for training” and that “it struggles when the test data are [from different SCMs]” are obvious and well-studied by CSIva or related works with the same approach. The same applies to the insight that “training […] exclusively on LiNGAM-generated data is equivalent to learning the distribution p(.|D, LiNGAM)”, implying identifiability.

Limitations:
-	The “theoretical result” (Proposition 1) is a simple corollary of Hoyer et al [7]. The paper makes otherwise no theoretical contribution to the problem underlying amortized causal discovery itself.

-	It is unclear whether “randomly initialized MLPs” are sensible nonlinear functions to use for constructing nonlinear mechanisms and non-Gaussian noise distributions. The fact that a few prior works used it is not a good reason. The shape and scale of randomly initialized neural network functions depends heavily on the activation function and weights distribution. The functions in these experiments could be anything from approximately constant or linear to very jumpy. Please provide additional motivation or evidence for why this is a good choice, and what hyperparameters are used, or consider as an alternative, e.g., samples from a GP, which are smooth and have an interpretable length-scale parameter, also in high dimensions.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper conducts an empirical study of the performance of supervised causal discovery methods, its generality, and the learnability vs. causal structure identifiability. The scope is the bivariate case, and with controlled mechanism and noise to establish the SCM for training and testing data. 

In my opinion, this paper gives two findings:

1) a previous claim (Lopez-Paz et al. [10]) said that, by using the supervised learning based approach, the performance of causal discovery can exceed the boundary of identifiability. which is not true

2) by using diverse training data (diverse = diverse mechanisms + diverse noise), supervised based causal discovery can achieve better OOD performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1 - the study of supervised causal learning, especially the DNN-based approach, is timely and important. 

2 - the experiment setup, is a good starting point. To my knowledge, this is the first paper study the performance, boundary of supervised-based causal discovery methods, setting the bivariate case, with the configuration in terms of mechanism + noise is valid.

3 - some findings are interesting, which can potentially benefit the community for further algorithm design.

Weaknesses:
1 - part of the study can be summarized as learnability vs. identifiability, or in my opinion, one question within this category is ""when and how can learnability exceeds the boundary of identifiability?"". in this regard, the current findings are still very limited, need to be further consolidated.
in this regard, a related work [1] is missing, I think it is helpful for this work.

2 - although not explicitly claimed, this paper suggests that ""CSIvA is capable of in-distribution generalization', is this true? or is this true just for bivariate case or generaly applicable?

3 - I suggest to use the term supervised-based approach, or supervised causal learning (SCL), rather than amortized causal discovery, which is more to the point.

4 -  one claim ""we conclude that the post-ANM is generally identifiable, which suggests that the setting of Example 2 is rather artificial""
I disagree. Although the space of all continuous distributions such that the bivariate post-ANM is non-identifiable is contained in a 2-dimensional space, thus it is a submanifold of the entire distribution space, thus its measure is 0. This is only a mathematical claim but lacks real-world relevance. I would argue that the setting of example 2 is quite valid in real-world setting, or the linear gaussian setting, is also commonly adoped in real-world, but had not been discussed in this work.

5 - potential conflict between section 3.3 and 3.4:
3.3 shows that when mixed two training dataset (different setting) together, would significantly compromise the SCL's performance; however, section 3.4 shows that the more diverse of the training data, the more gain on OOD setting.


[1] Dai, H., Ding, R., Jiang, Y., Han, S., & Zhang, D. (2023). Ml4c: Seeing causality through latent vicinity. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM) (pp. 226-234). Society for Industrial and Applied Mathematics.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the behaviour of amortised (supervised) causal discovery methods based on different training data distributions and its relation to more traditional causal discovery and the related identifiability theory. The authors empirically validate the intuitions about supervised causal discovery and generalisation of supervised learning methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper studies the behaviour of amortised causal discovery methods which have previously been unstudied.
- The empirical insights generally validate the intuition about identifiability and generalisation. Some examples give interesting insights into the identifiability and performance in the case of mixed assumptions.

Weaknesses:
- The paper is a purely empirical study of the generalisation behaviour of supervised causal discovery methods, validating general intuition without thorough novel insights.
- Given the empirical nature of this paper, I'd have expected to see a more thorough comparison, e.g. setting up a leave-one-out generalisation study or more in-depth analyses of the prediction on interesting individual SCMs such as the non-identifiable example or the performance of the prediction from new samples from a training set SCM.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores why causal discovery from observational data, particularly with CSIvA, a transformer-based model, can achieve competitive performance despite seemingly avoiding the explicit assumptions that traditional methods make for identifiability. The authors demonstrate that constraints on the training data distribution implicitly define a prior on the test observations. When this prior is well-suited, the underlying model can be identifiable. In other words, prior knowledge of the test distribution is encoded in the training data through constraints on the structural causal model governing data generation.

Additionally, they provide a theoretical basis for training on observations sampled from multiple classes of identifiable SCMs, a strategy that enhances test generalization to a wide range of causal models. They show that training on mixtures of causal models offers an alternative approach that is less reliant on assumptions about the mechanisms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper bridges the gap between existing theoretical results on identifiability and practical observations. More importantly, it moves away from classical causality settings and quite restricted models, shifting towards more mainstream and modern models like transformers. This opens a pathway for causality research to integrate with large language models (LLMs), which represent the state-of-the-art in a wide range of applications.

Weaknesses:
The presentation can be significantly improved. Since the paper aims to offer novel insights, it is crucial to organize the arguments, theoretical results, and experimental findings effectively to support these insights.

Limitations:
n.a.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the level of generalization achievable when training a predictor to classify “X causes Y” vs. “Y causes X” from observational data. Motivated by recent works performing causal discovery using a pretrained transformer model, the works explores which cases result in predictors that generalize to graph-dataset pairs generated from unseen types of SCM models. This is mostly achieved through a set of empirical experiments on synthetic 2-node SCM data. The work derives a corollary of Hoyer et al [7] to argue why training on multiple identifiable classes of synthetic SCM instances may help generalization amortized causal discovery methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper takes a first step towards analysing why amortized causal discovery performs well in practice and often significantly better than classical approaches. This is generally an important direction and of interest to the field.

Weaknesses:
While the motivation of this work is generally well-grounded, the contribution and argument of the work itself have several weaknesses that, in my opinion, do not justify many of the claims made in the abstract, introduction, and throughout the paper.

First, a major aspect of amortized causal discovery with transformers (referenced in the title) is that of solving structure learning tasks in *high dimensions*. Lopez [10] already provide theoretical and empirical analyses of the bivariate case. Recent work showed that this idea can generalize to (very) large systems -- the literature of works on causal discovery with transformers cited in the paper all study significant large-dimensional problems (ranging from 20-100 variables). Despite this, the present paper limits its entire analysis to the bivariate case. Thus, it is misleading to claim the paper “demystifies amortized causal discovery with transformers”. No part of the analysis concerns multivariate causal discovery or transformers. The paper should be upfront and highlight much more clearly what its contributions are beyond Lopez et al [10], which already study the bivariate amortized causal discovery case.

The paper repeatedly states it analyses CSIvA. However, none of the algorithmic components of CSIvA, such as e.g. the auxiliary loss it is trained on or the architecture of the predictive model, are part of the analysis. The loss function studied here (p.3, l. 133) is the same as e.g. used by [13]. Hence, it would be more truthful to claim the analysis concerns general predictors trained on the classification task of X->Y vs X<-V, as in [10].

A major component of causal discovery performance is not only identifiability of the graph from the observational distribution, but also the intractably large search problem incurred by classical score- and constrained-based methods. The question is: do transformers outperform classical methods in large problem sizes because 1) (parts of) the graphs are identifiable to it, or 2) a prediction-based approach is better at finding the identifiable edges in a large system (as opposed to doing a search)? This question motivates amortizing causal discovery in the first place, but the two-variable special case studied here is ill-suited for answering it. Since the work only studies the bivariate case, the title and claims throughout the paper, as well as their ties to the (large-scale) transformer literature have to be recalibrated.

Section 3.2 seems unnecessary. The section only studies the generalization ability of CSIvA, which is no contribution. The takeaways (lines 195-) that “CSIvA generalizes well to test data generated by the same class of SCMs used for training” and that “it struggles when the test data are [from different SCMs]” are obvious and well-studied by CSIva or related works with the same approach. The same applies to the insight that “training […] exclusively on LiNGAM-generated data is equivalent to learning the distribution p(.|D, LiNGAM)”, implying identifiability.

Limitations:
-	The “theoretical result” (Proposition 1) is a simple corollary of Hoyer et al [7]. The paper makes otherwise no theoretical contribution to the problem underlying amortized causal discovery itself.

-	It is unclear whether “randomly initialized MLPs” are sensible nonlinear functions to use for constructing nonlinear mechanisms and non-Gaussian noise distributions. The fact that a few prior works used it is not a good reason. The shape and scale of randomly initialized neural network functions depends heavily on the activation function and weights distribution. The functions in these experiments could be anything from approximately constant or linear to very jumpy. Please provide additional motivation or evidence for why this is a good choice, and what hyperparameters are used, or consider as an alternative, e.g., samples from a GP, which are smooth and have an interpretable length-scale parameter, also in high dimensions.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper conducts an empirical study of the performance of supervised causal discovery methods, its generality, and the learnability vs. causal structure identifiability. The scope is the bivariate case, and with controlled mechanism and noise to establish the SCM for training and testing data. 

In my opinion, this paper gives two findings:

1) a previous claim (Lopez-Paz et al. [10]) said that, by using the supervised learning based approach, the performance of causal discovery can exceed the boundary of identifiability. which is not true

2) by using diverse training data (diverse = diverse mechanisms + diverse noise), supervised based causal discovery can achieve better OOD performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1 - the study of supervised causal learning, especially the DNN-based approach, is timely and important. 

2 - the experiment setup, is a good starting point. To my knowledge, this is the first paper study the performance, boundary of supervised-based causal discovery methods, setting the bivariate case, with the configuration in terms of mechanism + noise is valid.

3 - some findings are interesting, which can potentially benefit the community for further algorithm design.

Weaknesses:
1 - part of the study can be summarized as learnability vs. identifiability, or in my opinion, one question within this category is ""when and how can learnability exceeds the boundary of identifiability?"". in this regard, the current findings are still very limited, need to be further consolidated.
in this regard, a related work [1] is missing, I think it is helpful for this work.

2 - although not explicitly claimed, this paper suggests that ""CSIvA is capable of in-distribution generalization', is this true? or is this true just for bivariate case or generaly applicable?

3 - I suggest to use the term supervised-based approach, or supervised causal learning (SCL), rather than amortized causal discovery, which is more to the point.

4 -  one claim ""we conclude that the post-ANM is generally identifiable, which suggests that the setting of Example 2 is rather artificial""
I disagree. Although the space of all continuous distributions such that the bivariate post-ANM is non-identifiable is contained in a 2-dimensional space, thus it is a submanifold of the entire distribution space, thus its measure is 0. This is only a mathematical claim but lacks real-world relevance. I would argue that the setting of example 2 is quite valid in real-world setting, or the linear gaussian setting, is also commonly adoped in real-world, but had not been discussed in this work.

5 - potential conflict between section 3.3 and 3.4:
3.3 shows that when mixed two training dataset (different setting) together, would significantly compromise the SCL's performance; however, section 3.4 shows that the more diverse of the training data, the more gain on OOD setting.


[1] Dai, H., Ding, R., Jiang, Y., Han, S., & Zhang, D. (2023). Ml4c: Seeing causality through latent vicinity. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM) (pp. 226-234). Society for Industrial and Applied Mathematics.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
DHVqAQ9DHy;"REVIEW 
Summary:
This paper introduces Posterior Label smoothing (PosteL), an innovative approach to enhance node classification on graph-structured data. PosteL integrates local neighborhood information with global label statistics to generate soft labels, aiming to improve model generalization and mitigate overfitting. The authors demonstrate the effectiveness of PosteL through extensive experiments on various datasets and models, showing significant performance improvements over baseline methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The paper is well-written and easy to follow.
2.	The authors provide a comprehensive set of experiments across different datasets and models, which substantiates the effectiveness of the proposed method.
3.	The figures and tables are well-organized, clear and easy to understand.
4.	The method is relatively lightweight and easy to implement at the technical level.

Weaknesses:
1.	While the paper mentions the computational complexity, a deeper analysis or comparison with existing methods could provide more insight. For example, maybe you could provide some compared experiments with existing methods on time/resource consumption.
2.	The reliance on global label statistics might introduce bias in cases where the dataset has inherent class imbalance or label noise.
3.	The article ""Rethinking the inception architecture for computer vision"" appears twice in your reference list; please consolidate these entries. Carefully review your references to maintain standardization.
4.	The author compares two soft label methods that were proposed quite some time ago (from 2015 and 2016, respectively). Are there any experimental results comparing with more recent methods? Otherwise, the persuasiveness of the experiments might not be so strong.
5.	Please maintain consistent terminology throughout the text. The term ""over-fitting"" in line 46 should be changed to ""overfitting"" to be consistent with the rest of the context.
6.	Authors could provide more details on the sensitivity analysis of the hyperparameters α and β, which are crucial for the method's performance.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed label-smoothing to improve the transductive node classification in GNN.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Label-smoothing and knowledge distillation are applied for node classification performance.

Weaknesses:
1.	The paper could benefit from discussing related works that combine label-smoothing with Graph Neural Networks (GNNs), such as [1] and [2]. Including these would provide a more comprehensive context for the current research.

2.	The proposed method lacks a theoretical motivation or analysis. Providing this would strengthen the paper's scientific rigor and help readers better understand the underlying principles.

3.	The proposed method bears similarities to the approach in [1]. A direct comparison with this work would clarify the novel contributions of the current study and situate it within the existing research landscape.

4.	Iterative pseudo-labeling is a well-established technique in the field. The paper should address this, explaining how the current application differs from or builds upon previous uses of this method.
Addressing these points could significantly enhance the paper's depth and impact. 

---
[1]: Wang, Y., Cai, Y., Liang, Y., Wang, W., Ding, H., Chen, M., ... & Hooi, B. (2021). Structure-aware label smoothing for graph neural networks. arXiv preprint arXiv:2112.00499.

[2]: Zhang, Wentao, et al. ""Node dependent local smoothing for scalable graph learning."" Advances in Neural Information Processing Systems 34 (2021): 20321-20332.

Limitations:
The limitation should be clarified in the paper. The current limitations are not clear.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes PosteL, a label smoothing method utilizing posterior distribution for node classification in graph-structured data. It is basically a preprocessing method for GNNs, generating soft labels based on neighborhood context and global label statistics before the training phase.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is generally well-written and easy to follow. For example, Fig. 1 is clear and intuitive.
2. The method is simple yet effective. PosteL can be combined seamlessly with existing methods.
3. The results are significant. PosteL is tested on seven neural network models across ten datasets, demonstrating significant improvements in classification accuracy.

Weaknesses:
1. The authors select $\alpha$ and $\beta$ from a wide range but did not explore the parameter sensitivity of PosteL. The sensitivity to hyperparameters could be a potential limitation, necessitating careful tuning, which may reduce the credibility of the experiments.
2. The authors do not seem to clarify the difference between PosteL and other label smoothing methods for node classification (or methods that can be adapted to node classification), which makes the novelty of the method unclear. The paper could explore other smoothing techniques or baselines in more depth for a comprehensive comparison.

Limitations:
The authors discuss the case when the prior likely dominates the posterior, which limits the effectiveness of the proposed PosteL.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a preprocessing step to refine labels of nodes in a structured graph that can benefit different graph-related transductive classification tasks. Inspired by the success of label smoothing in other machine learning tasks, the authors propose a label smoothing procedure based on a Bayesian inference that aggregates local and global information to estimate the soft labels. The procedure consists of mixing the soft and hard labels and an iterative regime akin to the Bayes update, which makes the method adaptive to different regularities present in different datasets. Authors conduct experiments applied to various models and datasets to support the efficacy of their methodology. They also provide an ablation study and further analyses of the results that shed light on different aspects of their proposal.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The identified gap is relevant, and applying label smoothing to the context of graph node classification bears novelty in terms of its application in this context. 
2. The empirical results suggest that the proposed solution addresses the research question successfully and merits the attention of the community. 
3. Moreover, the core ideas are communicated clearly and coupled with intuitive illustrations demonstrating the proposed method, which is very well appreciated. 
4. And lastly, the results and analyses are communicated well.

Weaknesses:
1. **Related work**: Currently, the related works seem to provide references to earlier studies that, for the most part, motivate this work and are not methodologically close to it. For example, there is no reference to closely related works that either adopted label smoothing or conducted a very similar procedure in the context of graph data. Most notable is ""[Adaptive Label Smoothing To Regularize Large-Scale Graph Training](https://epubs.siam.org/doi/abs/10.1137/1.9781611977653.ch7)"" which appears to have a very similar procedure but a different approach to obtain the soft labels.

Moreover, the current statements imply that the current work is the first to suggest label smoothing for the graph data. To be more concrete, line 81 needs to be expanded, and more closely related works need to be discussed. For example, to compare the current approach and highlight similarities and key distinctions with earlier works that are closely related to it.

Some other related works could be the following: 
- [Structure-Aware Label Smoothing for Graph Neural Networks](https://arxiv.org/abs/2112.00499)
- [Label Efficient Regularization and Propagation for Graph Node Classification](https://ieeexplore.ieee.org/abstract/document/10234505)
- [Node Dependent Local Smoothing for Scalable Graph Learning](https://proceedings.neurips.cc/paper_files/paper/2021/hash/a9eb812238f753132652ae09963a05e9-Abstract.html)


2. **Design decisions and theory**: besides complexity analysis, the study could have been accompanied by convergence analysis and more theoretically founded justification. However, this does not reduce the value of the work as its empirical results provide a strong signal for the effectiveness of the method, which merits future work toward theoretical assessment and explanation of its success.

3. **Background information**: the classification task that uses the preprocessed smooth labels is not defined explicitly, which makes the work less accessible for the readers without prior knowledge.

4. **The IPL step** is proposed to address the presence of ""unlabeled nodes""; however, it is hard to follow how the varying training size experiment reported in Table 4 is analogous to the unlabeled node scenario. Perhaps it is due to a lack of background information mentioned in point 3. 

5. **Suggestions to rephrase**:
   line 236: ""mitigate the importance"", perhaps some rephrasing is needed.
   line 189: ""learning curve"" -> ""loss curve"" 
   line 209: ""when"", some rephrasing might be needed

Limitations:
Limitations are addressed in the body of the text. It is perhaps preferable to have the important limitations mentioned in a separate section or in the conclusion as well.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Posterior Label smoothing (PosteL), an innovative approach to enhance node classification on graph-structured data. PosteL integrates local neighborhood information with global label statistics to generate soft labels, aiming to improve model generalization and mitigate overfitting. The authors demonstrate the effectiveness of PosteL through extensive experiments on various datasets and models, showing significant performance improvements over baseline methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The paper is well-written and easy to follow.
2.	The authors provide a comprehensive set of experiments across different datasets and models, which substantiates the effectiveness of the proposed method.
3.	The figures and tables are well-organized, clear and easy to understand.
4.	The method is relatively lightweight and easy to implement at the technical level.

Weaknesses:
1.	While the paper mentions the computational complexity, a deeper analysis or comparison with existing methods could provide more insight. For example, maybe you could provide some compared experiments with existing methods on time/resource consumption.
2.	The reliance on global label statistics might introduce bias in cases where the dataset has inherent class imbalance or label noise.
3.	The article ""Rethinking the inception architecture for computer vision"" appears twice in your reference list; please consolidate these entries. Carefully review your references to maintain standardization.
4.	The author compares two soft label methods that were proposed quite some time ago (from 2015 and 2016, respectively). Are there any experimental results comparing with more recent methods? Otherwise, the persuasiveness of the experiments might not be so strong.
5.	Please maintain consistent terminology throughout the text. The term ""over-fitting"" in line 46 should be changed to ""overfitting"" to be consistent with the rest of the context.
6.	Authors could provide more details on the sensitivity analysis of the hyperparameters α and β, which are crucial for the method's performance.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed label-smoothing to improve the transductive node classification in GNN.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Label-smoothing and knowledge distillation are applied for node classification performance.

Weaknesses:
1.	The paper could benefit from discussing related works that combine label-smoothing with Graph Neural Networks (GNNs), such as [1] and [2]. Including these would provide a more comprehensive context for the current research.

2.	The proposed method lacks a theoretical motivation or analysis. Providing this would strengthen the paper's scientific rigor and help readers better understand the underlying principles.

3.	The proposed method bears similarities to the approach in [1]. A direct comparison with this work would clarify the novel contributions of the current study and situate it within the existing research landscape.

4.	Iterative pseudo-labeling is a well-established technique in the field. The paper should address this, explaining how the current application differs from or builds upon previous uses of this method.
Addressing these points could significantly enhance the paper's depth and impact. 

---
[1]: Wang, Y., Cai, Y., Liang, Y., Wang, W., Ding, H., Chen, M., ... & Hooi, B. (2021). Structure-aware label smoothing for graph neural networks. arXiv preprint arXiv:2112.00499.

[2]: Zhang, Wentao, et al. ""Node dependent local smoothing for scalable graph learning."" Advances in Neural Information Processing Systems 34 (2021): 20321-20332.

Limitations:
The limitation should be clarified in the paper. The current limitations are not clear.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes PosteL, a label smoothing method utilizing posterior distribution for node classification in graph-structured data. It is basically a preprocessing method for GNNs, generating soft labels based on neighborhood context and global label statistics before the training phase.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is generally well-written and easy to follow. For example, Fig. 1 is clear and intuitive.
2. The method is simple yet effective. PosteL can be combined seamlessly with existing methods.
3. The results are significant. PosteL is tested on seven neural network models across ten datasets, demonstrating significant improvements in classification accuracy.

Weaknesses:
1. The authors select $\alpha$ and $\beta$ from a wide range but did not explore the parameter sensitivity of PosteL. The sensitivity to hyperparameters could be a potential limitation, necessitating careful tuning, which may reduce the credibility of the experiments.
2. The authors do not seem to clarify the difference between PosteL and other label smoothing methods for node classification (or methods that can be adapted to node classification), which makes the novelty of the method unclear. The paper could explore other smoothing techniques or baselines in more depth for a comprehensive comparison.

Limitations:
The authors discuss the case when the prior likely dominates the posterior, which limits the effectiveness of the proposed PosteL.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a preprocessing step to refine labels of nodes in a structured graph that can benefit different graph-related transductive classification tasks. Inspired by the success of label smoothing in other machine learning tasks, the authors propose a label smoothing procedure based on a Bayesian inference that aggregates local and global information to estimate the soft labels. The procedure consists of mixing the soft and hard labels and an iterative regime akin to the Bayes update, which makes the method adaptive to different regularities present in different datasets. Authors conduct experiments applied to various models and datasets to support the efficacy of their methodology. They also provide an ablation study and further analyses of the results that shed light on different aspects of their proposal.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The identified gap is relevant, and applying label smoothing to the context of graph node classification bears novelty in terms of its application in this context. 
2. The empirical results suggest that the proposed solution addresses the research question successfully and merits the attention of the community. 
3. Moreover, the core ideas are communicated clearly and coupled with intuitive illustrations demonstrating the proposed method, which is very well appreciated. 
4. And lastly, the results and analyses are communicated well.

Weaknesses:
1. **Related work**: Currently, the related works seem to provide references to earlier studies that, for the most part, motivate this work and are not methodologically close to it. For example, there is no reference to closely related works that either adopted label smoothing or conducted a very similar procedure in the context of graph data. Most notable is ""[Adaptive Label Smoothing To Regularize Large-Scale Graph Training](https://epubs.siam.org/doi/abs/10.1137/1.9781611977653.ch7)"" which appears to have a very similar procedure but a different approach to obtain the soft labels.

Moreover, the current statements imply that the current work is the first to suggest label smoothing for the graph data. To be more concrete, line 81 needs to be expanded, and more closely related works need to be discussed. For example, to compare the current approach and highlight similarities and key distinctions with earlier works that are closely related to it.

Some other related works could be the following: 
- [Structure-Aware Label Smoothing for Graph Neural Networks](https://arxiv.org/abs/2112.00499)
- [Label Efficient Regularization and Propagation for Graph Node Classification](https://ieeexplore.ieee.org/abstract/document/10234505)
- [Node Dependent Local Smoothing for Scalable Graph Learning](https://proceedings.neurips.cc/paper_files/paper/2021/hash/a9eb812238f753132652ae09963a05e9-Abstract.html)


2. **Design decisions and theory**: besides complexity analysis, the study could have been accompanied by convergence analysis and more theoretically founded justification. However, this does not reduce the value of the work as its empirical results provide a strong signal for the effectiveness of the method, which merits future work toward theoretical assessment and explanation of its success.

3. **Background information**: the classification task that uses the preprocessed smooth labels is not defined explicitly, which makes the work less accessible for the readers without prior knowledge.

4. **The IPL step** is proposed to address the presence of ""unlabeled nodes""; however, it is hard to follow how the varying training size experiment reported in Table 4 is analogous to the unlabeled node scenario. Perhaps it is due to a lack of background information mentioned in point 3. 

5. **Suggestions to rephrase**:
   line 236: ""mitigate the importance"", perhaps some rephrasing is needed.
   line 189: ""learning curve"" -> ""loss curve"" 
   line 209: ""when"", some rephrasing might be needed

Limitations:
Limitations are addressed in the body of the text. It is perhaps preferable to have the important limitations mentioned in a separate section or in the conclusion as well.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
CECVgSZbLW;"REVIEW 
Summary:
The authors explore the use of distributional reinforcement learning within Monte Carlo Tree search. They propose two algorithms CATS and PATS a categorical distribution and particle distribution based approach respectfully. They perform a theoretical analysis of the methods and show analysis of regret. They then evaluate on a synthetic planning tasks and evaluate it in combination with a pre-trained network on the atari benchmark.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- Thorough theoretical analysis
- The authors address distributional RL applied to planning which is a clear important direction of research

Weaknesses:
- Lack of referencing of existing work and novelty relative to existing work
    - Hayes, C.F., Reymond, M., Roijers, D.M., Howley, E. and Mannion, P., 2023. Monte Carlo tree search algorithms for risk-aware and multi-objective reinforcement learning. *Autonomous Agents and Multi-Agent Systems*, *37*(2), p.26.
- Unjustifiable statement: “For example, CATS is significantly better than other methods in Breakout, Enduro” There is no significance testing performed so this statement cannot be made and in fact the Confidence intervals overlap
- Key results in appendix and lack of empirical results in the main paper
- CATS never outperforms fixed depth MCTS on the synthetic tree task
- Unable to find code despite checklist saying it is provided

- Small issues
    - Figure 2 algorithms alignment off
    - Indication of Atari results in section 5 which are not there
    - Adding bold to best performing method in the Atari table would be useful for readability

Limitations:
- Limitations are not included in the main body of the paper which they should be especially considering there is space. The limitations are also not thoroughly discussed for example
    - “faces challenges in managing computational demand” : this does not say anything meaningful
    - “Our approach’s performance is slightly influenced by hyperparameters”, this can be said for essentially any method
- It seems that the distributional approach has an additional memory cost which if correct should be added to the limitations
- Given CATS and PATS do not massively outperform all baselines on the synthetic task I think the limitations should be where this is addressed and perhaps some insight given into why this is and why performance on Atari is also not particular strong relative to methods such as MENTS.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper propose two algorithms, Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS). These algorithms extend Distributional Reinforcement Learning (RL) to Monte-Carlo Tree Search (MCTS) by modeling value functions as categorical and particle distributions, respectively to improve the performance of MCTS in highly stochastic settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- **Originality:** The integration of Distributional RL into MCTS using categorical and particle distributions is innovative and addresses a critical need in handling stochastic environments (Sections 3.1-3.3).
- **Quality:** The theoretical analysis is rigorous, with well-defined proofs and clear methodology (Sections 4.1 and 4.2).

Weaknesses:
1. **Empirical Validation**: While the paper presents a comprehensive set of experiments demonstrating the efficacy of the proposed methods (CATS and PATS) in synthetic scenarios, there is an evident lack of diversity in the benchmarks used. 

2. **Algorithm Complexity and Overhead**: Both CATS and PATS introduce additional complexity by incorporating distributional approaches and Thompson Sampling into MCTS. The paper does not sufficiently address the computational overhead or the scalability of these methods when applied to environments with larger state or action spaces. This could be crucial for understanding the practical deployment of these algorithms in real-world applications.

Limitations:
### Computational Demands
The authors recognize that the Categorical Thompson Sampling (CATS) distributional Monte Carlo Tree Search (MCTS) involves increased complexity due to the management and updating of probability distributions. This acknowledgment is crucial as it highlights a potential scalability issue, especially in environments where computational resources are limited or real-time responses are required.

### Fixed Precision
The approach used in the Particle Thompson Sampling (PATS) to manage the growth in the number of particles by fixing the float precision is a practical solution to prevent computational overload. However, this method may introduce limitations in the precision and adaptiveness of the model, potentially affecting the accuracy of value estimations in environments with high variability.

### Number of Atoms
The performance sensitivity to the number of atoms indicates a hyperparameter dependency, which could impact the effectiveness and robustness of the model. The authors mention that suboptimal choices in this hyperparameter may affect performance, suggesting a need for careful tuning and validation to optimize the model's accuracy and efficiency.

### Addressing Limitations
While the authors have outlined these limitations, the discussion could be expanded to include more detailed strategies for mitigating these issues, particularly the computational demands and fixed precision aspects. For instance, strategies to optimize computational efficiency or adaptive techniques to dynamically adjust precision based on the context could further strengthen the approach.

### Societal Impact
The paper does not explicitly address the potential negative societal impacts of the research. In the realm of reinforcement learning and AI planning, concerns such as the deployment in sensitive or critical environments, where errors may have significant consequences, should be considered. Discussions around ethical implications, misuse, and long-term effects would be beneficial.

### Suggestions for Improvement
1. **Enhanced Computational Strategies**: The authors could explore methods to reduce computational overhead, such as parallel processing or optimizing algorithmic efficiency, to make the model more practical for real-time applications.
   
2. **Dynamic Precision Adjustment**: Introducing mechanisms to adjust the precision of particle distributions dynamically based on the observed variability in the environment could help maintain balance between computational efficiency and model accuracy.

Overall, the authors should be commended for their upfront discussion of the limitations, but there is room for deeper analysis and additional strategies to address these limitations comprehensively.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces distributional return estimates to MCTS-based planning. For this the authors borrow from work on distributional Q-Learning and show how to adapt the MCTS value back-up and action selection steps to compute and utilise these distributions. They formulate two approaches based on different distribution representations (quantile and particle based) for which they provide some theoretical convergence analysis as well as first experimental results.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
The paper combines two well-established ideas in MCTS and distributional value approximation resulting in a new algorithm with better theoretical guarantees. The overall approach and implementation of this combination makes sense and should at least in theory overcome limitations in stochastic environments.

Though I was unable to verify all proofs in detail, the theoretical analysis seems to make sense and establish the advantages of the proposed methods.

Weaknesses:
Despite the soundness of the overall proposed method, I found the paper very hard to follow and felt details were missing due to an overall lack of focus. Contributing to this were the following issues:

1. Empirical Evaluation
Experiments are limited to a toy domain and results on the Atari benchmark reported in the appendix.
The toy domain is a tabular environment that is being generated randomly and contains stochasticity in both the final reward and transitions.
For an illustrating example this makes it hard to judge the combined effect on the overall return distribution to be approximated. 

How the combinations of branching factor and depths that were plotted were chosen is unclear to me. Beyond this I am not sure how meaningful these plots are. In the right most plot it appears as if the PATS approximates the root value almost correctly in under 100 simulations - at which point it could not even have tried all k = 200 actions available to it.

Also CATS appears to be doing consistently worse than some of the other methods despite having the same theoretical properties as PATS.

For the Atari baseline the authors make use of Q-networks and point to a related paper. However, the exact implementation details and hyperparameters are not discussed making it hard to reproduce this work based on the paper alone.

While stochasiticity and the exploration challenges this causes form one of the main motivations for this paper, no further ablations how the proposed methods improve here are presented.

2. Content division
The author devote a significant amount of space to the summarisation of MCTS and distributional RL. While the theoretical analysis is arguably the strongest part of the presented work only the main theorems are found in the main body of the paper with very little contextualization.

3. Overall presentation
There are several presentation issues in overall formatting, grammar and spelling. The former includes, but is not limited to overlapping lines, inconsistent / in-text section headers and wrong section references.

Limitations:
The discussion of limitations is restricted to a short paragraph in the appendix listing generic points such as increased computational demand and sensitivity to hyperparameter choices. However, no further investigation or explanation as to their severity is provided.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS) algorithms, which incorporate distributional reinforcement learning into Monte Carlo Tree Search (MCTS) to handle value estimation in stochastic settings. By modeling value functions as categorical and particle-based distributions and applying Thompson Sampling for action selection, the proposed algorithms aim to improve the robustness and accuracy of value estimates. The paper proves the theoretical effectiveness of these methods by achieving a non-asymptotic problem-dependent upper bound on simple regret of $O(n^{−1})$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea is interesting and original and the non-asymptotic problem-dependent upper bound on simple regret of $O(n^{−1})$ significantly advances the state-of-the-art from the previous $O(n^{−1/2})$.

Weaknesses:
1- While using distributional RL in MCTS to do Thompson sampling is interesting, it introduces much computation complexity hindering the applicability of the proposed algorithms.

2- The numerical experiments for the stochastic environments that are the main motivation of this work are done on a toy problem.


Minor comments

1- The presentation of the paper can be improved, specifically the parentheses () citation style can be confused with equations reference. 

2- Line 42, the authors mention V node for the first time without properly defining what is a V node.

Limitations:
1- The added high computational complexity from maintaining a distribution for each node in the MCTS.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors explore the use of distributional reinforcement learning within Monte Carlo Tree search. They propose two algorithms CATS and PATS a categorical distribution and particle distribution based approach respectfully. They perform a theoretical analysis of the methods and show analysis of regret. They then evaluate on a synthetic planning tasks and evaluate it in combination with a pre-trained network on the atari benchmark.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- Thorough theoretical analysis
- The authors address distributional RL applied to planning which is a clear important direction of research

Weaknesses:
- Lack of referencing of existing work and novelty relative to existing work
    - Hayes, C.F., Reymond, M., Roijers, D.M., Howley, E. and Mannion, P., 2023. Monte Carlo tree search algorithms for risk-aware and multi-objective reinforcement learning. *Autonomous Agents and Multi-Agent Systems*, *37*(2), p.26.
- Unjustifiable statement: “For example, CATS is significantly better than other methods in Breakout, Enduro” There is no significance testing performed so this statement cannot be made and in fact the Confidence intervals overlap
- Key results in appendix and lack of empirical results in the main paper
- CATS never outperforms fixed depth MCTS on the synthetic tree task
- Unable to find code despite checklist saying it is provided

- Small issues
    - Figure 2 algorithms alignment off
    - Indication of Atari results in section 5 which are not there
    - Adding bold to best performing method in the Atari table would be useful for readability

Limitations:
- Limitations are not included in the main body of the paper which they should be especially considering there is space. The limitations are also not thoroughly discussed for example
    - “faces challenges in managing computational demand” : this does not say anything meaningful
    - “Our approach’s performance is slightly influenced by hyperparameters”, this can be said for essentially any method
- It seems that the distributional approach has an additional memory cost which if correct should be added to the limitations
- Given CATS and PATS do not massively outperform all baselines on the synthetic task I think the limitations should be where this is addressed and perhaps some insight given into why this is and why performance on Atari is also not particular strong relative to methods such as MENTS.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper propose two algorithms, Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS). These algorithms extend Distributional Reinforcement Learning (RL) to Monte-Carlo Tree Search (MCTS) by modeling value functions as categorical and particle distributions, respectively to improve the performance of MCTS in highly stochastic settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- **Originality:** The integration of Distributional RL into MCTS using categorical and particle distributions is innovative and addresses a critical need in handling stochastic environments (Sections 3.1-3.3).
- **Quality:** The theoretical analysis is rigorous, with well-defined proofs and clear methodology (Sections 4.1 and 4.2).

Weaknesses:
1. **Empirical Validation**: While the paper presents a comprehensive set of experiments demonstrating the efficacy of the proposed methods (CATS and PATS) in synthetic scenarios, there is an evident lack of diversity in the benchmarks used. 

2. **Algorithm Complexity and Overhead**: Both CATS and PATS introduce additional complexity by incorporating distributional approaches and Thompson Sampling into MCTS. The paper does not sufficiently address the computational overhead or the scalability of these methods when applied to environments with larger state or action spaces. This could be crucial for understanding the practical deployment of these algorithms in real-world applications.

Limitations:
### Computational Demands
The authors recognize that the Categorical Thompson Sampling (CATS) distributional Monte Carlo Tree Search (MCTS) involves increased complexity due to the management and updating of probability distributions. This acknowledgment is crucial as it highlights a potential scalability issue, especially in environments where computational resources are limited or real-time responses are required.

### Fixed Precision
The approach used in the Particle Thompson Sampling (PATS) to manage the growth in the number of particles by fixing the float precision is a practical solution to prevent computational overload. However, this method may introduce limitations in the precision and adaptiveness of the model, potentially affecting the accuracy of value estimations in environments with high variability.

### Number of Atoms
The performance sensitivity to the number of atoms indicates a hyperparameter dependency, which could impact the effectiveness and robustness of the model. The authors mention that suboptimal choices in this hyperparameter may affect performance, suggesting a need for careful tuning and validation to optimize the model's accuracy and efficiency.

### Addressing Limitations
While the authors have outlined these limitations, the discussion could be expanded to include more detailed strategies for mitigating these issues, particularly the computational demands and fixed precision aspects. For instance, strategies to optimize computational efficiency or adaptive techniques to dynamically adjust precision based on the context could further strengthen the approach.

### Societal Impact
The paper does not explicitly address the potential negative societal impacts of the research. In the realm of reinforcement learning and AI planning, concerns such as the deployment in sensitive or critical environments, where errors may have significant consequences, should be considered. Discussions around ethical implications, misuse, and long-term effects would be beneficial.

### Suggestions for Improvement
1. **Enhanced Computational Strategies**: The authors could explore methods to reduce computational overhead, such as parallel processing or optimizing algorithmic efficiency, to make the model more practical for real-time applications.
   
2. **Dynamic Precision Adjustment**: Introducing mechanisms to adjust the precision of particle distributions dynamically based on the observed variability in the environment could help maintain balance between computational efficiency and model accuracy.

Overall, the authors should be commended for their upfront discussion of the limitations, but there is room for deeper analysis and additional strategies to address these limitations comprehensively.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces distributional return estimates to MCTS-based planning. For this the authors borrow from work on distributional Q-Learning and show how to adapt the MCTS value back-up and action selection steps to compute and utilise these distributions. They formulate two approaches based on different distribution representations (quantile and particle based) for which they provide some theoretical convergence analysis as well as first experimental results.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
The paper combines two well-established ideas in MCTS and distributional value approximation resulting in a new algorithm with better theoretical guarantees. The overall approach and implementation of this combination makes sense and should at least in theory overcome limitations in stochastic environments.

Though I was unable to verify all proofs in detail, the theoretical analysis seems to make sense and establish the advantages of the proposed methods.

Weaknesses:
Despite the soundness of the overall proposed method, I found the paper very hard to follow and felt details were missing due to an overall lack of focus. Contributing to this were the following issues:

1. Empirical Evaluation
Experiments are limited to a toy domain and results on the Atari benchmark reported in the appendix.
The toy domain is a tabular environment that is being generated randomly and contains stochasticity in both the final reward and transitions.
For an illustrating example this makes it hard to judge the combined effect on the overall return distribution to be approximated. 

How the combinations of branching factor and depths that were plotted were chosen is unclear to me. Beyond this I am not sure how meaningful these plots are. In the right most plot it appears as if the PATS approximates the root value almost correctly in under 100 simulations - at which point it could not even have tried all k = 200 actions available to it.

Also CATS appears to be doing consistently worse than some of the other methods despite having the same theoretical properties as PATS.

For the Atari baseline the authors make use of Q-networks and point to a related paper. However, the exact implementation details and hyperparameters are not discussed making it hard to reproduce this work based on the paper alone.

While stochasiticity and the exploration challenges this causes form one of the main motivations for this paper, no further ablations how the proposed methods improve here are presented.

2. Content division
The author devote a significant amount of space to the summarisation of MCTS and distributional RL. While the theoretical analysis is arguably the strongest part of the presented work only the main theorems are found in the main body of the paper with very little contextualization.

3. Overall presentation
There are several presentation issues in overall formatting, grammar and spelling. The former includes, but is not limited to overlapping lines, inconsistent / in-text section headers and wrong section references.

Limitations:
The discussion of limitations is restricted to a short paragraph in the appendix listing generic points such as increased computational demand and sensitivity to hyperparameter choices. However, no further investigation or explanation as to their severity is provided.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS) algorithms, which incorporate distributional reinforcement learning into Monte Carlo Tree Search (MCTS) to handle value estimation in stochastic settings. By modeling value functions as categorical and particle-based distributions and applying Thompson Sampling for action selection, the proposed algorithms aim to improve the robustness and accuracy of value estimates. The paper proves the theoretical effectiveness of these methods by achieving a non-asymptotic problem-dependent upper bound on simple regret of $O(n^{−1})$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea is interesting and original and the non-asymptotic problem-dependent upper bound on simple regret of $O(n^{−1})$ significantly advances the state-of-the-art from the previous $O(n^{−1/2})$.

Weaknesses:
1- While using distributional RL in MCTS to do Thompson sampling is interesting, it introduces much computation complexity hindering the applicability of the proposed algorithms.

2- The numerical experiments for the stochastic environments that are the main motivation of this work are done on a toy problem.


Minor comments

1- The presentation of the paper can be improved, specifically the parentheses () citation style can be confused with equations reference. 

2- Line 42, the authors mention V node for the first time without properly defining what is a V node.

Limitations:
1- The added high computational complexity from maintaining a distribution for each node in the MCTS.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
AthZ2g6VE2;"REVIEW 
Summary:
This paper proposed LoCoDL, an algorithm than combines communication compression with local training. The authors proved the convergence results under regular assumptions, achieving comparable rate with existing SOTA algorithms. The experimental results also show that LoCoDL behaves best among tested algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The combination of communication compression and local training is novel.
2. The convergence results achieves SOTA for large $n$ and nearly SOTA for small $n$.
3. The algorithm behaves empirically better than ADIANA, an existing theoretically SOTA algorithm.
4. The algorithm is simple.
5. The target problem setting is novel and general.

Weaknesses:
1. Throughout the four experimental settings, the number of nodes, $n$, is comparable to the feature dimension $d$. As the convergence rate of LoCoDL is suboptimal when $n$ is small, I believe it important to compare LoCoDL with ADIANA when $n$ is at least 10$\times$ or 100$\times$ smaller than $d$ to see whether LoCoDL beats ADIANA in these scenarios.
2. The experimental datasets are relatively small. It's recommended to conduct experiments on MNIST or larger datasets.
3. It is not easy to capture the intuition behind each algorithm line. It's recommended to give more detailed explanations on how the algorithm is developed.

Limitations:
As stated in the conclusion part, the algorithm is limited to single-directional, deterministic setting without partial participation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes LoCoDL, a new GD-based distributed training algorithm that employs both communication compression (CC) and local training (LT). It achieves double acceleration and a SOTA convergence rate for strongly convex problems.

A crux of the algorithmic improvement is maintaining two local estimates, intuitively enabling efficient LT (similarly to SCAFFOLD) and efficient CC (i.e., compressing values' differences instead of values themselves).

The paper offers a thorough theoretical analysis that proves the main claim and conducts some experiments that show LoCoDL's benefits compared to previous algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is timely and important, and I enjoyed reading it. It appears to set a new bar for distributed communication complexity in the strongly convex case (and with full participation?).

While some works assume similarity between local client functions, this work allows these functions to be arbitrarily different.
Also, interpreting the added term $g$ is intuitive and compelling (viewpoints 1-4). 

The theoretical claims are rigorously proven, and some experiments demonstrate the efficiency of LoCoDL compared to previous techniques.

Weaknesses:
The practical applicability of LoCoDL is unclear. Namely, it does not apply to NNs and possibly is less efficient for partial participation use cases (this part is unclear). Either providing concrete evidence of why this contribution is important for modern practical use cases or slightly rephrasing the paper as a theoretical (and important) contribution would strengthen the claim.   

Strengthening the evaluation section is also advised. The submission would be strengthened if the author provided an experiment other than logistic regression to demonstrate the efficiency of LoCoDL in another task.

Additional points: 

1.	“are smooth,  so their gradients will be called. “ This sentence is unclear.

2.	“is slower than  broadcasting the same message to an arbitrary number of clients.” Are there any real FL systems that employ broadcasting? Or do you mean sending the same message? (The term “broadcast” may be confusing here.)

3.	“In this work, we focus on the  uplink communication complexity, which is the bottleneck in practice.“ The second part of the sentence should be softened or extended with real evidence that this is the case. 

4.	“No other compressor can be used, which notably rules out any type of quantization.” Why is this the case? Why quantization cannot be applied according to the selected pattern? 

5.	“Instead of the cumbersome permutation-based compressor of the latter.” is there a specific challenge in implementing permutation-based compressors? Maybe it's worth specifying specific setups where this is insufficient or cannot be applied.

6.	“Thus, LoCoDL sets new standards in terms of communication efficiency. “ Do you mean: our experiments indicate that...?

7.	What is the communication complexity with partial participation (PP) (i.e., $\rho=1$)? When considering PP, is LoCoDL the current SOTA, or are there better alternatives? 

8. It seems that some elements of LoCoDL have some similarities to DoCoFL [1], which also uses an anchor for the model that allows clients to obtain only a compressed correction to that anchor. Can the authors shed light on this similarity?

[1] Dorfman, Ron, et al. ""DoCoFL: Downlink compression for cross-device federated learning."" International Conference on Machine Learning. PMLR, 2023.

Limitations:
The paper does not have a dedicated limitation section. The conclusions section discusses potential future extensions. Outlining the limitations clearly is advised. For example, is the PP use case relevant here?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an algorithm (LoCoDL) that leverages two well-known methods of local training. It reduces the communication load in distributed learning.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper addresses the interesting problem of distributed learning.

Weaknesses:
1. Generally, compressing the error and feeding it back to the updates is a well-known technique to reduce the variance in distributed learning. The idea of the algorithm is marginal with respect to the previous known algorithms (feeding back the error and aggregating with proper coefficients). 
2. Also, the experiments should include the accuracy versus iteration (or time) to see after how many iterations (or how much time), the performance shown in Figure 1 is achieved. So, there are lots of work to improve the experimental part.

Limitations:
justification, experiments

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed LoCoDL, an algorithm than combines communication compression with local training. The authors proved the convergence results under regular assumptions, achieving comparable rate with existing SOTA algorithms. The experimental results also show that LoCoDL behaves best among tested algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The combination of communication compression and local training is novel.
2. The convergence results achieves SOTA for large $n$ and nearly SOTA for small $n$.
3. The algorithm behaves empirically better than ADIANA, an existing theoretically SOTA algorithm.
4. The algorithm is simple.
5. The target problem setting is novel and general.

Weaknesses:
1. Throughout the four experimental settings, the number of nodes, $n$, is comparable to the feature dimension $d$. As the convergence rate of LoCoDL is suboptimal when $n$ is small, I believe it important to compare LoCoDL with ADIANA when $n$ is at least 10$\times$ or 100$\times$ smaller than $d$ to see whether LoCoDL beats ADIANA in these scenarios.
2. The experimental datasets are relatively small. It's recommended to conduct experiments on MNIST or larger datasets.
3. It is not easy to capture the intuition behind each algorithm line. It's recommended to give more detailed explanations on how the algorithm is developed.

Limitations:
As stated in the conclusion part, the algorithm is limited to single-directional, deterministic setting without partial participation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes LoCoDL, a new GD-based distributed training algorithm that employs both communication compression (CC) and local training (LT). It achieves double acceleration and a SOTA convergence rate for strongly convex problems.

A crux of the algorithmic improvement is maintaining two local estimates, intuitively enabling efficient LT (similarly to SCAFFOLD) and efficient CC (i.e., compressing values' differences instead of values themselves).

The paper offers a thorough theoretical analysis that proves the main claim and conducts some experiments that show LoCoDL's benefits compared to previous algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is timely and important, and I enjoyed reading it. It appears to set a new bar for distributed communication complexity in the strongly convex case (and with full participation?).

While some works assume similarity between local client functions, this work allows these functions to be arbitrarily different.
Also, interpreting the added term $g$ is intuitive and compelling (viewpoints 1-4). 

The theoretical claims are rigorously proven, and some experiments demonstrate the efficiency of LoCoDL compared to previous techniques.

Weaknesses:
The practical applicability of LoCoDL is unclear. Namely, it does not apply to NNs and possibly is less efficient for partial participation use cases (this part is unclear). Either providing concrete evidence of why this contribution is important for modern practical use cases or slightly rephrasing the paper as a theoretical (and important) contribution would strengthen the claim.   

Strengthening the evaluation section is also advised. The submission would be strengthened if the author provided an experiment other than logistic regression to demonstrate the efficiency of LoCoDL in another task.

Additional points: 

1.	“are smooth,  so their gradients will be called. “ This sentence is unclear.

2.	“is slower than  broadcasting the same message to an arbitrary number of clients.” Are there any real FL systems that employ broadcasting? Or do you mean sending the same message? (The term “broadcast” may be confusing here.)

3.	“In this work, we focus on the  uplink communication complexity, which is the bottleneck in practice.“ The second part of the sentence should be softened or extended with real evidence that this is the case. 

4.	“No other compressor can be used, which notably rules out any type of quantization.” Why is this the case? Why quantization cannot be applied according to the selected pattern? 

5.	“Instead of the cumbersome permutation-based compressor of the latter.” is there a specific challenge in implementing permutation-based compressors? Maybe it's worth specifying specific setups where this is insufficient or cannot be applied.

6.	“Thus, LoCoDL sets new standards in terms of communication efficiency. “ Do you mean: our experiments indicate that...?

7.	What is the communication complexity with partial participation (PP) (i.e., $\rho=1$)? When considering PP, is LoCoDL the current SOTA, or are there better alternatives? 

8. It seems that some elements of LoCoDL have some similarities to DoCoFL [1], which also uses an anchor for the model that allows clients to obtain only a compressed correction to that anchor. Can the authors shed light on this similarity?

[1] Dorfman, Ron, et al. ""DoCoFL: Downlink compression for cross-device federated learning."" International Conference on Machine Learning. PMLR, 2023.

Limitations:
The paper does not have a dedicated limitation section. The conclusions section discusses potential future extensions. Outlining the limitations clearly is advised. For example, is the PP use case relevant here?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an algorithm (LoCoDL) that leverages two well-known methods of local training. It reduces the communication load in distributed learning.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper addresses the interesting problem of distributed learning.

Weaknesses:
1. Generally, compressing the error and feeding it back to the updates is a well-known technique to reduce the variance in distributed learning. The idea of the algorithm is marginal with respect to the previous known algorithms (feeding back the error and aggregating with proper coefficients). 
2. Also, the experiments should include the accuracy versus iteration (or time) to see after how many iterations (or how much time), the performance shown in Figure 1 is achieved. So, there are lots of work to improve the experimental part.

Limitations:
justification, experiments

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ASA2jdKtf3;"REVIEW 
Summary:
This paper extends the framework of multi-agent influence diagrams (MAIDs) to explicitly capture complex forms of reasoning corresponding to Theory of Mind (ToM) as required for the interaction of Multi-Agent Systems with human users.  It introduces the framework of incomplete information MAIDs (II-MAIDs) for explicitly modeling higher-order beliefs in multi-agent interactions alongside probabilistic and causal dependencies between variables. Using results connecting EFGs to MAIDs, the authors demonstrate a natural mapping between strategies in the two frameworks that preserves expected utilities according to the agents’ subjective models.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The approach is well situated within the state-of-the-art of related work in agent models with a game theory component and, as far as one can judge, appears technically sound within the broad remit of causal and influence diagrams (IDs). 
The paper is very well structured and the authors did their utmost to keep it relatively accessible by alternating formal sections with intuitive descriptive summaries. It remains somehow tedious to read, owing to the large number of definitions, whose numbering alternate with that of theorems. 
The rationale for building a framework on top of MAIDs rather than EFGs is well introduced, together with the mapping between strategies in MAIDs and EFGs and the choice of working at the interim stage. This culminates with Theorem 20, until section 5.1 raises some issues around the relevance of Nash Equilibria.

Weaknesses:
The major issue I would raise for this paper is one of relevance to NeurIPS, even in the extended sense. While a major rationale for the paper appears to be its potential application to AI Safety, in the NeurIPS context there does not seem to be enough outreach to current AI models, at least in a way in which they could be interfaced to the proposed ID model. This means some consideration of how current models may form ‘beliefs’, and this was not entirely obvious from the paper’s Title and Abstract. Perhaps my expectation was unrealistic, but I had imagined an attempt to unify formal ToM issues with ToM properties that are known to be associated to LLM, under a framework where this approach would federate or wrap formal agentic methods around, say Agentic LLM. With this comment I am not criticising the authors for not having written another sort of paper, I am simply pointing the perceived gap that may exist between this approach and the NeurIPS constituency. Further evidence would be the absence of references to NeurIPS paper and the relative dearth of mainstream AI venues in the references (to the notable exception of AIJ). Overall, it appears that AAMAS might be a better venue to host this type of paper. 

The paper does not really clarify its ToM framework which references both “multi-agent interactions” as well as “higher-order intentional states” but these aspects are not part of further formal developments. It also mentions “belief hierarchies of arbitrary and infinite depth” and this raises the issue of whether such a formal approach is realistic when it comes to ToM, in particular in the interactions between agents and human users. 

Despite an early reference to AI Safety and a mention in the paper’s abstract, there is little in the paper that actually progresses the discussion on AI Safety, which is only used marginally through ID examples, such as the one of Figure 2.

Limitations:
The limitation section begins with a number of upbeat statements that would better be placed in the conclusion or parts of the abstract. The main identified limitation, which echoes the discussion of section 5.1 is verbatim: “The main limitation of our work is the lack of a useful solution concept.” appears a quite severe restriction. While not affecting the solid grounding of the approach it considerably restricts its impact at its current stage of development.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework Incomplete Information Multi-Agent Influence Diagrams (II-MAIDs) for modeling complex multi-agent interactions involving theory of mind (ToM) and higher-order beliefs. The authors prove the equivalence between II-MAIDs and Incomplete Information Extensive Form Games (II-EFGs) at the interim stage. The paper also shows the existence of Nash equilibria in II-MAIDs under certain conditions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The II-MAID framework fills a gap in existing game-theoretic models by allowing for inconsistent beliefs and higher-order reasoning. The paper is built on solid mathematical foundation with formal definitions and proofs.

Weaknesses:
* From my perspective, the proposed II-MAID framework appears overly complicated for modeling Theory of Mind (ToM), which is fundamentally a straightforward psychological mechanism observed in daily human interactions. The paper's approach may overcomplicate a concept that should be more intuitively represented.
* The paper introduces numerous assumptions and definitions without clear explanations which hinders the readability. As a non-expert in the field, some details in the paper are difficult to read. 
* It is unclear whether the model can be scaled and applied to larger, more realistic scenarios, where ToM takes place more frequently.
* The paper lacks experiments that validates the model.

Limitations:
N/A, see weaknesses.

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work extends the theoretical framework of multi-agent influence diagrams (MAIDs) with incomplete information (II-MAIDs) to explicitly capture this complex form of reasoning. The primary theoretical contribution is the proof of the existence of Nash equilibria, although, in general, these equilibria are impossible for agents to identify.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This work is game-theoretic in nature, and overall, the presentation quality is good and smooth to the best of my knowledge.

2. Although I think the assumption made in this work generally makes sense to me: agents have consistent beliefs as part of our commonsense, which can be derived from a common prior distribution, I agree that there are settings with no common prior available. The setup is a less constrained setup.

Weaknesses:
1. One of my major concerns is the audience of this work. Given that this work is submitted to the safe ML track of NeurIPS, I expect more discussion on the relevance of this framework to AI safety. The author should elaborate on what they imply by “safety” rather than making a very brief claim about its relevance in the related work and conclusions sections.

2. The discussion of theory of mind is also lacking, given that this is well-motivated. There have been extensive studies on machine theory of mind, ranging from early studies [1-2] to recent studies on LLMs [3-4]. There has also been research connecting Theory of Mind to Game theory [5] and Interactive POMDP [6]. See the survey [7] for details. Overall, this work needs significant improvement in discussing related work for readers to evaluate its contribution and relevance to NeurIPS.

[1] Rabinowitz, Neil, et al. ""Machine theory of mind."" International conference on machine learning. PMLR, 2018.

[2] Jara-Ettinger, Julian. ""Theory of mind as inverse reinforcement learning."" Current Opinion in Behavioral Sciences 29 (2019): 105-110.

[3] Sap, Maarten, et al. ""Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs."" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.

[4] Ma, Ziqiao, et al. ""Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models."" Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.

[5] Yoshida, Wako, Ray J. Dolan, and Karl J. Friston. ""Game theory of mind."" PLoS computational biology 4.12 (2008): e1000254.

[6] Çelikok, Mustafa Mert, et al. ""Interactive AI with a Theory of Mind."" Computational Modeling in Human-Computer Interaction. 2019.

[7] Albrecht, Stefano V., and Peter Stone. ""Autonomous agents modelling other agents: A comprehensive survey and open problems."" Artificial Intelligence 258 (2018): 66-95.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper extends the framework of multi-agent influence diagrams (MAIDs) to explicitly capture complex forms of reasoning corresponding to Theory of Mind (ToM) as required for the interaction of Multi-Agent Systems with human users.  It introduces the framework of incomplete information MAIDs (II-MAIDs) for explicitly modeling higher-order beliefs in multi-agent interactions alongside probabilistic and causal dependencies between variables. Using results connecting EFGs to MAIDs, the authors demonstrate a natural mapping between strategies in the two frameworks that preserves expected utilities according to the agents’ subjective models.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The approach is well situated within the state-of-the-art of related work in agent models with a game theory component and, as far as one can judge, appears technically sound within the broad remit of causal and influence diagrams (IDs). 
The paper is very well structured and the authors did their utmost to keep it relatively accessible by alternating formal sections with intuitive descriptive summaries. It remains somehow tedious to read, owing to the large number of definitions, whose numbering alternate with that of theorems. 
The rationale for building a framework on top of MAIDs rather than EFGs is well introduced, together with the mapping between strategies in MAIDs and EFGs and the choice of working at the interim stage. This culminates with Theorem 20, until section 5.1 raises some issues around the relevance of Nash Equilibria.

Weaknesses:
The major issue I would raise for this paper is one of relevance to NeurIPS, even in the extended sense. While a major rationale for the paper appears to be its potential application to AI Safety, in the NeurIPS context there does not seem to be enough outreach to current AI models, at least in a way in which they could be interfaced to the proposed ID model. This means some consideration of how current models may form ‘beliefs’, and this was not entirely obvious from the paper’s Title and Abstract. Perhaps my expectation was unrealistic, but I had imagined an attempt to unify formal ToM issues with ToM properties that are known to be associated to LLM, under a framework where this approach would federate or wrap formal agentic methods around, say Agentic LLM. With this comment I am not criticising the authors for not having written another sort of paper, I am simply pointing the perceived gap that may exist between this approach and the NeurIPS constituency. Further evidence would be the absence of references to NeurIPS paper and the relative dearth of mainstream AI venues in the references (to the notable exception of AIJ). Overall, it appears that AAMAS might be a better venue to host this type of paper. 

The paper does not really clarify its ToM framework which references both “multi-agent interactions” as well as “higher-order intentional states” but these aspects are not part of further formal developments. It also mentions “belief hierarchies of arbitrary and infinite depth” and this raises the issue of whether such a formal approach is realistic when it comes to ToM, in particular in the interactions between agents and human users. 

Despite an early reference to AI Safety and a mention in the paper’s abstract, there is little in the paper that actually progresses the discussion on AI Safety, which is only used marginally through ID examples, such as the one of Figure 2.

Limitations:
The limitation section begins with a number of upbeat statements that would better be placed in the conclusion or parts of the abstract. The main identified limitation, which echoes the discussion of section 5.1 is verbatim: “The main limitation of our work is the lack of a useful solution concept.” appears a quite severe restriction. While not affecting the solid grounding of the approach it considerably restricts its impact at its current stage of development.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework Incomplete Information Multi-Agent Influence Diagrams (II-MAIDs) for modeling complex multi-agent interactions involving theory of mind (ToM) and higher-order beliefs. The authors prove the equivalence between II-MAIDs and Incomplete Information Extensive Form Games (II-EFGs) at the interim stage. The paper also shows the existence of Nash equilibria in II-MAIDs under certain conditions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The II-MAID framework fills a gap in existing game-theoretic models by allowing for inconsistent beliefs and higher-order reasoning. The paper is built on solid mathematical foundation with formal definitions and proofs.

Weaknesses:
* From my perspective, the proposed II-MAID framework appears overly complicated for modeling Theory of Mind (ToM), which is fundamentally a straightforward psychological mechanism observed in daily human interactions. The paper's approach may overcomplicate a concept that should be more intuitively represented.
* The paper introduces numerous assumptions and definitions without clear explanations which hinders the readability. As a non-expert in the field, some details in the paper are difficult to read. 
* It is unclear whether the model can be scaled and applied to larger, more realistic scenarios, where ToM takes place more frequently.
* The paper lacks experiments that validates the model.

Limitations:
N/A, see weaknesses.

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work extends the theoretical framework of multi-agent influence diagrams (MAIDs) with incomplete information (II-MAIDs) to explicitly capture this complex form of reasoning. The primary theoretical contribution is the proof of the existence of Nash equilibria, although, in general, these equilibria are impossible for agents to identify.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This work is game-theoretic in nature, and overall, the presentation quality is good and smooth to the best of my knowledge.

2. Although I think the assumption made in this work generally makes sense to me: agents have consistent beliefs as part of our commonsense, which can be derived from a common prior distribution, I agree that there are settings with no common prior available. The setup is a less constrained setup.

Weaknesses:
1. One of my major concerns is the audience of this work. Given that this work is submitted to the safe ML track of NeurIPS, I expect more discussion on the relevance of this framework to AI safety. The author should elaborate on what they imply by “safety” rather than making a very brief claim about its relevance in the related work and conclusions sections.

2. The discussion of theory of mind is also lacking, given that this is well-motivated. There have been extensive studies on machine theory of mind, ranging from early studies [1-2] to recent studies on LLMs [3-4]. There has also been research connecting Theory of Mind to Game theory [5] and Interactive POMDP [6]. See the survey [7] for details. Overall, this work needs significant improvement in discussing related work for readers to evaluate its contribution and relevance to NeurIPS.

[1] Rabinowitz, Neil, et al. ""Machine theory of mind."" International conference on machine learning. PMLR, 2018.

[2] Jara-Ettinger, Julian. ""Theory of mind as inverse reinforcement learning."" Current Opinion in Behavioral Sciences 29 (2019): 105-110.

[3] Sap, Maarten, et al. ""Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs."" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.

[4] Ma, Ziqiao, et al. ""Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models."" Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.

[5] Yoshida, Wako, Ray J. Dolan, and Karl J. Friston. ""Game theory of mind."" PLoS computational biology 4.12 (2008): e1000254.

[6] Çelikok, Mustafa Mert, et al. ""Interactive AI with a Theory of Mind."" Computational Modeling in Human-Computer Interaction. 2019.

[7] Albrecht, Stefano V., and Peter Stone. ""Autonomous agents modelling other agents: A comprehensive survey and open problems."" Artificial Intelligence 258 (2018): 66-95.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
AO5MjDuHpr;"REVIEW 
Summary:
This paper proposes Tree of Attributes Prompt learning (TAP). Unlike previous works that rely on unstructured class descriptions, this approach distillates structured knowledge graphs associated with class names from LLMs. Text/vision prompts and vision-conditional pooling module are designed to extract instance-specific text features. Extensive experimental results demosntrate its improved performances.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Overall, the idea of distillating structured knowledge from LLMs in the task of prompt learning is new and interesting.

- The paper designed an effective prompt learning framework to capture fine-grained attributes, using vision expert tokens and vision-conditional pooling layer.

- The illustrated way to generate structure tree of attribute from LLMs can also be used in other tasks.

- From the experiments, using structured knowledge leads to better performances than unstructured descriptions in base-to-novel and few-shot classification tasks.

- The visualization of class activation maps and attention weights look good. The paper is well written and easy to follow.

Weaknesses:
- Apart from the new framework, the method highly relies on the quality of tree of attribute generated with GPT-3.5-turbo. There is no study on the robustness aganist different LLMs, different generation prompts, or varying attribute sets.

- The loss includes a model regularization and its effectiveness is not discussed.

- In Figure 2, it is not too clear to me about $I_1 T_1$, $I_2 T_2$,etc. They seem not be discussed in the text parts.

Limitations:
One limitation is its reliance on LLMs (GPT) to generate the tree of attribute. When generating more complex responses, it is challening to ensure the quality and variances. How to keep a balance between the diversity of attribute sets and relevancy of attributes to classification is important.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new method called ""Attribute Prompt Learning Tree (TAP)"" to improve the performance of CLIP on zero-shot and few-shot classification tasks. The authors leverage large language models (LLMs) to generate more descriptive text prompts and introduce a hierarchical tree-like structure to systematically generate and integrate these descriptions, ensuring a layered and comprehensive understanding of the visual content. The method also learns specialized ""domain expert"" prompt tokens that focus on different visual attributes and uses a vision-based pooling module to extract text features for specific instances. Extensive experiments show that TAP outperforms state-of-the-art methods on zero-shot and few-shot classification tasks across multiple datasets

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1), The idea that utilizing LLM to generate tree-like prompts makes sense. This structured description approach is significantly different from the existing simple text prompt methods and provides an efficient way to improve VLMs.

2), The image-conditional pooling module looks like good for capturing instance-specific features.

3), Experiments and visualization demonstrate the effectiveness of the proposed model.

Weaknesses:
1), TAP introduces many textual and visual prompts, which leads to high computing and time costs. This may limit its applications.

2), TAP first generates hierarchical token prompts, while it seems like TAP does not use such a hierarchical structure to integrate the output of the text encoder. It only uses a pooling strategy to update the text encoder output with the visual feature. That is, TAP also does not utilize these relationships in the prompt graph.

3), TAP can be viewed as a multimodal prompt tuning method. What is the main difference between TAP and MAPLE,  ALIGN.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a method that aiming to align the vision modality with not only the category name but also the whole concept subgraph the noun represents in the knowledge graph. This is achieved by adding a bunch of attributes branches attached to this concept. The authors argue that this integration of attribute knowledge will make the alignment more transferrable and thus result in a good performance boost in terms of zero/few shot results.
Basically, this work focusing on the topic of textual prompt enrichment task that is investigated before but implement in a different manner. Additionally, the proposed method use seperate tokens to learn different aspectrs of attributes of given images, working as 'domain expert'.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Might be the first work trying to align the vision image with structured data. It is quite interesting considering that most text prompts now are less organized and noisy. And structured data, as pointed out in the recent research of LLM, may lead to better reasoning skill for a foundation model.
2. The proposed vision-conditional pooling can help the model filter out descriptions that are not direct appeared in the image.
3. Recieve good results on different classification datasets with the model trained with this method.

Weaknesses:
1. The attributes description is generated by the LLM, which could contain hallucinated content. While there are many reliable sources of knowledge such as wikipedia or conceptNet, this paper seems skip these sources to obtain some accurate attributes.
2. Though this paper decide to use a tree structure to represent the concept. The built tree is not encoded in a structure-awared manner. They are still feed as langauge tokens to the LLMs. 
3.  in equation (5), what is $v_y^a$ stands for? 
4. The author argued that the vision-conditional pooling, which is bascially a cross attention layer between the visual and language modal. The authors believe this this design will make the model filter out non-exisiting material in the text description. However, we know that due to the quirk of softmax function. You can never make some tokens attention to be '0'. Thus, the model is learning some spurious correlation aftertall.

Limitations:
Not applicable.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The TAP method structures textual descriptions in a hierarchical “concept-attribute-description” format, effectively creating a knowledge graph from large language models (LLMs) for each category name. This structure allows for a more comprehensive and detailed understanding of the visual content. The paper reimagines learnable prompt tokens as ""domain experts,"" each specializing in different aspects of the image, supplemented by a global perspective provided by the CLS token. To address potential misalignment between general descriptions and specific image content, the paper introduces a vision-conditional pooling module. This module extracts instance-specific text features, ensuring optimal image-text alignment.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method incorporates structured tree of attribute into prompt tuning that provide richer supervisory information compared to unstructured attribute information. A set of experiments has been conducted, and the results look promising.

Weaknesses:
One major limitation of the method is that it requires human review to ""ensure the quality of the example"" (L175). Recall that one major advantage of prompt tuning is that it can adapt large models quickly to specific tasks. However, the requirement of human reviewing in the proposed method is not consistent with this goal. In addition, it is not clear how many human efforts are needed here, and how to handle the potential human bias in quality evaluation. 

The paper lacks cross-dataset experiments, which is typically provided in existing PT papers. The results are important to examine the domain generalization capability of the method. 

For training details, different learning rates were used for different datasets, however, existing methods typically use a same LR for all datasets. From this point, the comparison is somewhat unfair.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new prompt tuning method for adapting the vision-language model.  The authors design the tree of attribute prompt learning to substitute the categorical description for adapting the vision-language model. A vision-conditional pooling module is proposed to extract instance-specific text features. Extensive experimental results demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. A tree of attribute prompt learning method is proposed to guide the adatpation of VLM with the hierarchical semantic information.

2. This paper is well-written and easy to follow.

Weaknesses:
1. According to the experiment, the performance improvement of TAP is marginal, e.g., the few-shot performance on most of datasets.  Although the visualization results of VCP layer are impressive, the improvement of this module is also very slight compared to average pooling. 

2. The core motivation of this method is learning fine-grained attributes to adapt VLMs. However, similar ideas have been explored in previous works , e.g., APPL[1], MAP[2].  Please discuss the differences.

[1] AAPL: Adding Attributes to Prompt Learning for Vision-Language Models

[2] Multi-modal Attribute Prompting for Vision-Language Models

3. The construction of ToA depends heavily on the prior information on the category of attributes suitable for the dataset. However, one of the most capability of VLM is its zero-shot ability in the open-vocabulary context. What's the performance of the proposed method in the domain generalization setting?


4. The model details in Figure 2 are not presented very clear, especially the input & output streams. This figure should be refined for better clarity. 


5. The mechanism behind Equation (5) and the function of VCP needs more clarification. Why conduct constrastive learning between expert token P_a^v and attribute embedding v_c^a generated from P_a^v itself, instead of P_a^v and the embedding of attribute descriptions D?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes Tree of Attributes Prompt learning (TAP). Unlike previous works that rely on unstructured class descriptions, this approach distillates structured knowledge graphs associated with class names from LLMs. Text/vision prompts and vision-conditional pooling module are designed to extract instance-specific text features. Extensive experimental results demosntrate its improved performances.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Overall, the idea of distillating structured knowledge from LLMs in the task of prompt learning is new and interesting.

- The paper designed an effective prompt learning framework to capture fine-grained attributes, using vision expert tokens and vision-conditional pooling layer.

- The illustrated way to generate structure tree of attribute from LLMs can also be used in other tasks.

- From the experiments, using structured knowledge leads to better performances than unstructured descriptions in base-to-novel and few-shot classification tasks.

- The visualization of class activation maps and attention weights look good. The paper is well written and easy to follow.

Weaknesses:
- Apart from the new framework, the method highly relies on the quality of tree of attribute generated with GPT-3.5-turbo. There is no study on the robustness aganist different LLMs, different generation prompts, or varying attribute sets.

- The loss includes a model regularization and its effectiveness is not discussed.

- In Figure 2, it is not too clear to me about $I_1 T_1$, $I_2 T_2$,etc. They seem not be discussed in the text parts.

Limitations:
One limitation is its reliance on LLMs (GPT) to generate the tree of attribute. When generating more complex responses, it is challening to ensure the quality and variances. How to keep a balance between the diversity of attribute sets and relevancy of attributes to classification is important.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new method called ""Attribute Prompt Learning Tree (TAP)"" to improve the performance of CLIP on zero-shot and few-shot classification tasks. The authors leverage large language models (LLMs) to generate more descriptive text prompts and introduce a hierarchical tree-like structure to systematically generate and integrate these descriptions, ensuring a layered and comprehensive understanding of the visual content. The method also learns specialized ""domain expert"" prompt tokens that focus on different visual attributes and uses a vision-based pooling module to extract text features for specific instances. Extensive experiments show that TAP outperforms state-of-the-art methods on zero-shot and few-shot classification tasks across multiple datasets

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1), The idea that utilizing LLM to generate tree-like prompts makes sense. This structured description approach is significantly different from the existing simple text prompt methods and provides an efficient way to improve VLMs.

2), The image-conditional pooling module looks like good for capturing instance-specific features.

3), Experiments and visualization demonstrate the effectiveness of the proposed model.

Weaknesses:
1), TAP introduces many textual and visual prompts, which leads to high computing and time costs. This may limit its applications.

2), TAP first generates hierarchical token prompts, while it seems like TAP does not use such a hierarchical structure to integrate the output of the text encoder. It only uses a pooling strategy to update the text encoder output with the visual feature. That is, TAP also does not utilize these relationships in the prompt graph.

3), TAP can be viewed as a multimodal prompt tuning method. What is the main difference between TAP and MAPLE,  ALIGN.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a method that aiming to align the vision modality with not only the category name but also the whole concept subgraph the noun represents in the knowledge graph. This is achieved by adding a bunch of attributes branches attached to this concept. The authors argue that this integration of attribute knowledge will make the alignment more transferrable and thus result in a good performance boost in terms of zero/few shot results.
Basically, this work focusing on the topic of textual prompt enrichment task that is investigated before but implement in a different manner. Additionally, the proposed method use seperate tokens to learn different aspectrs of attributes of given images, working as 'domain expert'.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Might be the first work trying to align the vision image with structured data. It is quite interesting considering that most text prompts now are less organized and noisy. And structured data, as pointed out in the recent research of LLM, may lead to better reasoning skill for a foundation model.
2. The proposed vision-conditional pooling can help the model filter out descriptions that are not direct appeared in the image.
3. Recieve good results on different classification datasets with the model trained with this method.

Weaknesses:
1. The attributes description is generated by the LLM, which could contain hallucinated content. While there are many reliable sources of knowledge such as wikipedia or conceptNet, this paper seems skip these sources to obtain some accurate attributes.
2. Though this paper decide to use a tree structure to represent the concept. The built tree is not encoded in a structure-awared manner. They are still feed as langauge tokens to the LLMs. 
3.  in equation (5), what is $v_y^a$ stands for? 
4. The author argued that the vision-conditional pooling, which is bascially a cross attention layer between the visual and language modal. The authors believe this this design will make the model filter out non-exisiting material in the text description. However, we know that due to the quirk of softmax function. You can never make some tokens attention to be '0'. Thus, the model is learning some spurious correlation aftertall.

Limitations:
Not applicable.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The TAP method structures textual descriptions in a hierarchical “concept-attribute-description” format, effectively creating a knowledge graph from large language models (LLMs) for each category name. This structure allows for a more comprehensive and detailed understanding of the visual content. The paper reimagines learnable prompt tokens as ""domain experts,"" each specializing in different aspects of the image, supplemented by a global perspective provided by the CLS token. To address potential misalignment between general descriptions and specific image content, the paper introduces a vision-conditional pooling module. This module extracts instance-specific text features, ensuring optimal image-text alignment.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method incorporates structured tree of attribute into prompt tuning that provide richer supervisory information compared to unstructured attribute information. A set of experiments has been conducted, and the results look promising.

Weaknesses:
One major limitation of the method is that it requires human review to ""ensure the quality of the example"" (L175). Recall that one major advantage of prompt tuning is that it can adapt large models quickly to specific tasks. However, the requirement of human reviewing in the proposed method is not consistent with this goal. In addition, it is not clear how many human efforts are needed here, and how to handle the potential human bias in quality evaluation. 

The paper lacks cross-dataset experiments, which is typically provided in existing PT papers. The results are important to examine the domain generalization capability of the method. 

For training details, different learning rates were used for different datasets, however, existing methods typically use a same LR for all datasets. From this point, the comparison is somewhat unfair.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new prompt tuning method for adapting the vision-language model.  The authors design the tree of attribute prompt learning to substitute the categorical description for adapting the vision-language model. A vision-conditional pooling module is proposed to extract instance-specific text features. Extensive experimental results demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. A tree of attribute prompt learning method is proposed to guide the adatpation of VLM with the hierarchical semantic information.

2. This paper is well-written and easy to follow.

Weaknesses:
1. According to the experiment, the performance improvement of TAP is marginal, e.g., the few-shot performance on most of datasets.  Although the visualization results of VCP layer are impressive, the improvement of this module is also very slight compared to average pooling. 

2. The core motivation of this method is learning fine-grained attributes to adapt VLMs. However, similar ideas have been explored in previous works , e.g., APPL[1], MAP[2].  Please discuss the differences.

[1] AAPL: Adding Attributes to Prompt Learning for Vision-Language Models

[2] Multi-modal Attribute Prompting for Vision-Language Models

3. The construction of ToA depends heavily on the prior information on the category of attributes suitable for the dataset. However, one of the most capability of VLM is its zero-shot ability in the open-vocabulary context. What's the performance of the proposed method in the domain generalization setting?


4. The model details in Figure 2 are not presented very clear, especially the input & output streams. This figure should be refined for better clarity. 


5. The mechanism behind Equation (5) and the function of VCP needs more clarification. Why conduct constrastive learning between expert token P_a^v and attribute embedding v_c^a generated from P_a^v itself, instead of P_a^v and the embedding of attribute descriptions D?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
AM1znjeo9l;"REVIEW 
Summary:
The paper studies the effect of rescaling symmetry in SGD and shows SGD tends to favor solutions with balanced gradient noises. The authors then derive an exact solution of the stationary distribution of a toy model trained by SGD.  The derived solution shed lights on problems observed in deep learning such as fluctuation inversion and edge of stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper contributes to the understanding of SGD properties. The noise balance theorem is novel and important. The analytical solution as well as the interpretation is interesting and insightful.

Weaknesses:
The results of the paper are interesting and important, the writing needs refinement to improve clarity and precision. The conditions under which the results hold is sometimes omitted, leading to confusion. The language should also be made more precise.

Minor points:
1.	The first paragraph in related works appears to overstate the novelty of the results. Specifically, our result is the first to derive an exact solution to the stationary distribution of SGD without any approximation. (Line 55-56) This is a strong claim, but it seems inaccurate. There are previous results showing exact solution of stationary distribution of SGD (e. g. Liu Ziyin 2021). Corollary I.1 in arXiv:2306.04251 (2023) also states the stationary distribution on a deep learning setup similar to the D=1 model discussed in this paper. Also, the solution given in the paper is for a specific model. These should be made clear.
2.	It seems that eq. 15 takes D=1, which has not been stated and thus is confusing.
3.	It is unclear why the left figure of Fig. 5 has only two theory lines instead of three.

Major points:
1.	The related works on symmetry and SGD dynamics are insufficient. There are a few related works that are missing, e. g. arXiv:2309.16932 (2023).
2.	The paper has not discussed convergence to the stationary distribution. The authors seem to assume convergence to stationary distribution and use interchangeably the SGD properties and the stationary solution properties (e. g. line 97-98). However, the properties of SGD can be very different from the properties of stationary solutions unless convergence to the stationary solutions is guaranteed. The authors should clarify this.
3.	The authors fail to discuss uniqueness of the stationary solutions. For example, it is unclear to me why eq (3) is a necessary and efficient condition for stationarity. Eq (3) is a critical result in the paper, and it would be better to make it a theorem or corollary. However, since eq (2) cannot be interpreted as a deterministic ODE. The unique condition for a stationary distribution should be justified, especially considering that C1 and C2 are not constant but depend on u and w.
4.	The equivalence of SGD bias and weight decay is not rigorous. (line 155-158) The C0 term is not constant but depends on u and w, while the weight decay rate is constant.

Limitations:
The authors have listed limitations at the end. The major limitations are the simplicity of the model and lack of experiments on deep neural networks.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
For ReLU networks trained by gradient flows, it is classical that a type of Minkowski inner product between the coefficients of consecutive layers is preserved. The authors demonstrate a monotonicity of the same quantity for stochastic gradient descent in continuous time. They use this to study the invariant distribution of parameters trained by (continuous time) SGD.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The topic is well-chosen and the results - if correct - are very interesting.

Weaknesses:
* In its current form, I find the article a bit unpolished and the results not easy to access. Many questions remained unanswered when I tried reading the article (see questions).

* Important quantities are defined throughout the plain text. I understand that reading as a reviewer under time pressure is different from normal reading, but for instance in Theorem 3.1, I would have hoped for a more self-contained statement on relations and properties $L, C, \ell$ and the distribution of $x$ have to satisfy. As far as I can tell, the statement is fairly general and not specific to machine learning.

* I have serious doubts about Theorem 3.1. It is derived in Appendix A from Itô's Lemma without the diffusion term. This is valid *in expectation over $\theta$*, but not pointwise in $\theta$. Pointwise in $\theta$, there should be white noise in the 'time derivatives', i.e. the ODE identity should be written as an SDE. In the proof, equations (27) and (28) appear to be wrong.

* The authors do not pay any attention to whether solutions to the evolution equations exist (or are unique). Problems with regularity can sometimes be alleviated if the distribution in $x$ is sufficiently regular, but I would appreciate a short discussion.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to analyze the specific features that carry the noise of SGD (through a continuous model). The authors show that there is a certain 'law of balance' across the layers when some invariance is assumed. Going further, they derive a toy model to push their study, showing that there is an analytic stationary solution to it. They finally propose a phenomenology related to the role of the noise of SGD when analyzing this precise stationary distribution.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea that a conservation law for the gradient flow implies an asymptotic balancedness condition for the stochastic flow is a good and striking idea.

The one-dimensional examples that are given in the text are very pleasant to follow and they are good exercices to display the ability of the stochastic flow to diverge from the gradient flow.

The example given in Eq.(13) is thoroughly analyzed.

Weaknesses:
The paper present the following weaknesses:

- The law of balance is an interesting phenomenon, yet considering it with a closer look, it seems that not much can be said generally and that one has to understand it case by case. In one dimension, sure, it is possible to conclude that balancedness will occur at exponential speed, yet in dimension more than $2$, it seems impossible to predict it surely.

- I have to say that I was a bit bothered by the general overselling of the paper : 
     - As said before the law of balance is truly valid asymptotically in one-dimension
     - The stationary distribution that the authors claim to be the first to derive is for a very specific model, which is not standard and does not resemble a diagonal network! 
     - The fact that the stationary distribution can be computed is also very inherent to $1d$ calculation and is simply a recognition of a Pearson diffusion that already made in way in ML (at least in https://arxiv.org/pdf/2402.01382 and https://arxiv.org/pdf/2407.02322).

Minor typos/flaws:

- l.41: Fokker Planck is not inherently high-dimensional
- l.44: Go to the line for new paragraph 
- l.165: The law of balance is not strictly applicable here since $\ell$ is not scale invariant because of the regularization.
- Section **4.1 Depth - 0**: I think that $\Delta > 0$ is not currently the ""most practical example"" since it corresponds to a underparametrized model.

Limitations:
As said before, all conclusion are drawn for models that live intrinsically in one dimension.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the effect of rescaling symmetry in SGD and shows SGD tends to favor solutions with balanced gradient noises. The authors then derive an exact solution of the stationary distribution of a toy model trained by SGD.  The derived solution shed lights on problems observed in deep learning such as fluctuation inversion and edge of stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper contributes to the understanding of SGD properties. The noise balance theorem is novel and important. The analytical solution as well as the interpretation is interesting and insightful.

Weaknesses:
The results of the paper are interesting and important, the writing needs refinement to improve clarity and precision. The conditions under which the results hold is sometimes omitted, leading to confusion. The language should also be made more precise.

Minor points:
1.	The first paragraph in related works appears to overstate the novelty of the results. Specifically, our result is the first to derive an exact solution to the stationary distribution of SGD without any approximation. (Line 55-56) This is a strong claim, but it seems inaccurate. There are previous results showing exact solution of stationary distribution of SGD (e. g. Liu Ziyin 2021). Corollary I.1 in arXiv:2306.04251 (2023) also states the stationary distribution on a deep learning setup similar to the D=1 model discussed in this paper. Also, the solution given in the paper is for a specific model. These should be made clear.
2.	It seems that eq. 15 takes D=1, which has not been stated and thus is confusing.
3.	It is unclear why the left figure of Fig. 5 has only two theory lines instead of three.

Major points:
1.	The related works on symmetry and SGD dynamics are insufficient. There are a few related works that are missing, e. g. arXiv:2309.16932 (2023).
2.	The paper has not discussed convergence to the stationary distribution. The authors seem to assume convergence to stationary distribution and use interchangeably the SGD properties and the stationary solution properties (e. g. line 97-98). However, the properties of SGD can be very different from the properties of stationary solutions unless convergence to the stationary solutions is guaranteed. The authors should clarify this.
3.	The authors fail to discuss uniqueness of the stationary solutions. For example, it is unclear to me why eq (3) is a necessary and efficient condition for stationarity. Eq (3) is a critical result in the paper, and it would be better to make it a theorem or corollary. However, since eq (2) cannot be interpreted as a deterministic ODE. The unique condition for a stationary distribution should be justified, especially considering that C1 and C2 are not constant but depend on u and w.
4.	The equivalence of SGD bias and weight decay is not rigorous. (line 155-158) The C0 term is not constant but depends on u and w, while the weight decay rate is constant.

Limitations:
The authors have listed limitations at the end. The major limitations are the simplicity of the model and lack of experiments on deep neural networks.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
For ReLU networks trained by gradient flows, it is classical that a type of Minkowski inner product between the coefficients of consecutive layers is preserved. The authors demonstrate a monotonicity of the same quantity for stochastic gradient descent in continuous time. They use this to study the invariant distribution of parameters trained by (continuous time) SGD.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The topic is well-chosen and the results - if correct - are very interesting.

Weaknesses:
* In its current form, I find the article a bit unpolished and the results not easy to access. Many questions remained unanswered when I tried reading the article (see questions).

* Important quantities are defined throughout the plain text. I understand that reading as a reviewer under time pressure is different from normal reading, but for instance in Theorem 3.1, I would have hoped for a more self-contained statement on relations and properties $L, C, \ell$ and the distribution of $x$ have to satisfy. As far as I can tell, the statement is fairly general and not specific to machine learning.

* I have serious doubts about Theorem 3.1. It is derived in Appendix A from Itô's Lemma without the diffusion term. This is valid *in expectation over $\theta$*, but not pointwise in $\theta$. Pointwise in $\theta$, there should be white noise in the 'time derivatives', i.e. the ODE identity should be written as an SDE. In the proof, equations (27) and (28) appear to be wrong.

* The authors do not pay any attention to whether solutions to the evolution equations exist (or are unique). Problems with regularity can sometimes be alleviated if the distribution in $x$ is sufficiently regular, but I would appreciate a short discussion.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to analyze the specific features that carry the noise of SGD (through a continuous model). The authors show that there is a certain 'law of balance' across the layers when some invariance is assumed. Going further, they derive a toy model to push their study, showing that there is an analytic stationary solution to it. They finally propose a phenomenology related to the role of the noise of SGD when analyzing this precise stationary distribution.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea that a conservation law for the gradient flow implies an asymptotic balancedness condition for the stochastic flow is a good and striking idea.

The one-dimensional examples that are given in the text are very pleasant to follow and they are good exercices to display the ability of the stochastic flow to diverge from the gradient flow.

The example given in Eq.(13) is thoroughly analyzed.

Weaknesses:
The paper present the following weaknesses:

- The law of balance is an interesting phenomenon, yet considering it with a closer look, it seems that not much can be said generally and that one has to understand it case by case. In one dimension, sure, it is possible to conclude that balancedness will occur at exponential speed, yet in dimension more than $2$, it seems impossible to predict it surely.

- I have to say that I was a bit bothered by the general overselling of the paper : 
     - As said before the law of balance is truly valid asymptotically in one-dimension
     - The stationary distribution that the authors claim to be the first to derive is for a very specific model, which is not standard and does not resemble a diagonal network! 
     - The fact that the stationary distribution can be computed is also very inherent to $1d$ calculation and is simply a recognition of a Pearson diffusion that already made in way in ML (at least in https://arxiv.org/pdf/2402.01382 and https://arxiv.org/pdf/2407.02322).

Minor typos/flaws:

- l.41: Fokker Planck is not inherently high-dimensional
- l.44: Go to the line for new paragraph 
- l.165: The law of balance is not strictly applicable here since $\ell$ is not scale invariant because of the regularization.
- Section **4.1 Depth - 0**: I think that $\Delta > 0$ is not currently the ""most practical example"" since it corresponds to a underparametrized model.

Limitations:
As said before, all conclusion are drawn for models that live intrinsically in one dimension.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
AAN46kUPXM;"REVIEW 
Summary:
In this paper, the author proposes ""Expressiveness,"" a metric that measures the dissimilarity of feature maps produced by different filters. Subsequently, the author introduces NEXP, a technique to prune filters based on their expressiveness. The proposed method is tested on tasks such as image classification and object detection.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The overall structure of the paper is clean and easy to follow.
2. Although I am not very familiar with the structured pruning literature, the application of the concept of representation power of neural networks in this field appears novel.
3. The experiments are comprehensive, including pruning at initialization (PaI), pruning after training, and other tasks like object detection.

Weaknesses:
1. The notation is very confusing. For example, in the section 'Generalization of concepts at a structural level,' $\ell$ is the index of a certain layer, but the upper-case K is the total number of layers, and the lower-case k is the index of a filter/channel in a certain layer.
2. The concept of expressiveness is not new in the context of pruning [1] and neural architecture search [2,3,4].
3. The performance improvement by NEXP is not prominent. For example, in Tables 1 and 2, NEXP often shows a higher compression ratio but lower accuracy. This makes it unclear if NEXP offers a significant advantage over other methods.

***
**Minor Mistakes:**

Line 163: ""where k the is"" should be corrected.

[1] Tanaka, Hidenori, et al. ""Pruning neural networks without any data by iteratively conserving synaptic flow."" Advances in Neural Information Processing Systems 33 (2020): 6377-6389.

[2] Lin, Ming, et al. ""Zen-NAS: A zero-shot NAS for high-performance image recognition."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.

[3] Wang, Haoxiang, et al. ""Global convergence of MAML and theory-inspired neural architecture search for few-shot learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.

[4] Chen, Wuyang, Xinyu Gong, and Zhangyang Wang. ""Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective."" arXiv preprint arXiv:2102.11535 (2021).

Limitations:
The authors have discussed limitation in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper works on weight pruning for CNNs. It proposes an evaluation metric, i.e., ""expressiveness"", to evaluate whether a neuron/groups of neurons should be pruned or not. The metric focuses on the neurons' ability to redistribute informational resources. As the evaluation of expressiveness requires data samples, the paper includes studies on arbitrary data or limited dataset’s representative samples. The experiments are conducted for image classification tasks on ResNet architectures, and the object detection task on YOLOv8m.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Weight pruning is an effective manner in reducing the redundancies in DNNs. Instead of focusing on weight importance, this paper considers the expressiveness of neurons in the information flow within a network. The proposed evaluation metric can also be combined with existing strategies with importance evaluation metrics as a hybrid pruning approach.

Weaknesses:
1. Limited practicality of the approach. The method is mainly focused on removing redundant filters from CNNs. Though CNN is one category of DNNs, recent works have shifted to more advanced model architectures such as transformers and Mamba, which are mainly composed of FC layers instead of CNNs. The practicality of the approach is highly restricted to SOTA model architectures for image classification tasks.   

2. Limited performance improvements. This is a major concern. The performance gain of the proposed method is not obvious compared with baselines. For instance. on CIFAR-10 VGG-16, SCP and reduce the parameters 15.28$\times$ with a 93.85\% accuracy while the proposed method can only reduce the parameters 5.62$\times$ with a slightly better accuracy 93.87\%. HRank also provides better performance than the proposed method with higher accuracy 93.96\% (0.09\% higher than NEXP) and higher reductions of FLOPs (4.26$\times$ (HRank) v.s. 4.01$\times$ (NEXP) ). On DenseNet-40, Hrank also shows better performance across all metrics. Hrank v.s. NEXP: Accuracy 95.05\% v.s. 94.64\%, parameter reduction 3.31$\times$ v.s.3.12$\times$, FLOPs reduction 3.38$\times$ v.s. 2.51$\times$.

Limitations:
Please refer to weaknesses and questions. The major concern is that the method does not show better performance than baselines, and is also limited in model architectures.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a new structured pruning approach NEXP. It works by computing the dissimilarity score of the feature activations across samples and removing those filters with smaller variances. Experimental results on several models and datasets demonstrate the effectiveness of the proposed approaches.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written with clear motivation and many of the important technical details are included.  
2. The authors have presented the experimental results well and the additional discussion provides deeper insights on the effectiveness of the proposed methods.  
3. The authors evaluated models beyond image classification, i.e., the proposed methods work well on YOLOv8 object detectors.

Weaknesses:
1. The conclusion section is too short and fails to characterize the main contribution of this paper. What does it mean as to “when” and “how” to prune? The authors should elaborate on these further.   

2. I think that this is not a scalable approach for computing the pruning metrics. The pruning metric proposed in the paper requires computing a N by N matrix for each filter in the network, where N is the number of samples. This could grow quite computationally infeasible for large networks and batch sizes. The authors also fails to discuss this aspect on the pruning efficiency in the paper.  

3. In terms of experiments, I am not sure why the authors compare each methods under different compression ratio? If the pruned models in each method have different parameters, it can be hard to compare the accuracy numbers.  

4. I feel a lot of the content in section 3 is not necessary and they can could go into a separate preliminary section. Section 3.1 and early parts of Section 3.2 takes up a lot of space and in the meantime do not provides us with the motivation and insights for the later introduced methods.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to handle network pruning problem. Specifically, it proposes to use a new importance measurement, called expressiveness, to decide the pruning process. It jointly considers the model state to leverage on the proposed measurement. In addition, it can also combined with typical importance based pruning methods to improve the model efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Network pruning is a valuable research direction to study on, espeicailly for current large-scale era where efficiency matters a lot.
2. The proposed new metric to measure the network importance is interesting.
3.  Empirical results show the method superiority.

Weaknesses:
1. Adding more discussion in the conclusion part helps to improve the paper readability.
2. Since this paper proposes a new pruning metric, it is better to show more visualization and network behavior analysis to illustrate the intuition.
3. The compared baseline models are relatively old, adding more recent publications helps to support this paper.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the author proposes ""Expressiveness,"" a metric that measures the dissimilarity of feature maps produced by different filters. Subsequently, the author introduces NEXP, a technique to prune filters based on their expressiveness. The proposed method is tested on tasks such as image classification and object detection.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The overall structure of the paper is clean and easy to follow.
2. Although I am not very familiar with the structured pruning literature, the application of the concept of representation power of neural networks in this field appears novel.
3. The experiments are comprehensive, including pruning at initialization (PaI), pruning after training, and other tasks like object detection.

Weaknesses:
1. The notation is very confusing. For example, in the section 'Generalization of concepts at a structural level,' $\ell$ is the index of a certain layer, but the upper-case K is the total number of layers, and the lower-case k is the index of a filter/channel in a certain layer.
2. The concept of expressiveness is not new in the context of pruning [1] and neural architecture search [2,3,4].
3. The performance improvement by NEXP is not prominent. For example, in Tables 1 and 2, NEXP often shows a higher compression ratio but lower accuracy. This makes it unclear if NEXP offers a significant advantage over other methods.

***
**Minor Mistakes:**

Line 163: ""where k the is"" should be corrected.

[1] Tanaka, Hidenori, et al. ""Pruning neural networks without any data by iteratively conserving synaptic flow."" Advances in Neural Information Processing Systems 33 (2020): 6377-6389.

[2] Lin, Ming, et al. ""Zen-NAS: A zero-shot NAS for high-performance image recognition."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.

[3] Wang, Haoxiang, et al. ""Global convergence of MAML and theory-inspired neural architecture search for few-shot learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.

[4] Chen, Wuyang, Xinyu Gong, and Zhangyang Wang. ""Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective."" arXiv preprint arXiv:2102.11535 (2021).

Limitations:
The authors have discussed limitation in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper works on weight pruning for CNNs. It proposes an evaluation metric, i.e., ""expressiveness"", to evaluate whether a neuron/groups of neurons should be pruned or not. The metric focuses on the neurons' ability to redistribute informational resources. As the evaluation of expressiveness requires data samples, the paper includes studies on arbitrary data or limited dataset’s representative samples. The experiments are conducted for image classification tasks on ResNet architectures, and the object detection task on YOLOv8m.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Weight pruning is an effective manner in reducing the redundancies in DNNs. Instead of focusing on weight importance, this paper considers the expressiveness of neurons in the information flow within a network. The proposed evaluation metric can also be combined with existing strategies with importance evaluation metrics as a hybrid pruning approach.

Weaknesses:
1. Limited practicality of the approach. The method is mainly focused on removing redundant filters from CNNs. Though CNN is one category of DNNs, recent works have shifted to more advanced model architectures such as transformers and Mamba, which are mainly composed of FC layers instead of CNNs. The practicality of the approach is highly restricted to SOTA model architectures for image classification tasks.   

2. Limited performance improvements. This is a major concern. The performance gain of the proposed method is not obvious compared with baselines. For instance. on CIFAR-10 VGG-16, SCP and reduce the parameters 15.28$\times$ with a 93.85\% accuracy while the proposed method can only reduce the parameters 5.62$\times$ with a slightly better accuracy 93.87\%. HRank also provides better performance than the proposed method with higher accuracy 93.96\% (0.09\% higher than NEXP) and higher reductions of FLOPs (4.26$\times$ (HRank) v.s. 4.01$\times$ (NEXP) ). On DenseNet-40, Hrank also shows better performance across all metrics. Hrank v.s. NEXP: Accuracy 95.05\% v.s. 94.64\%, parameter reduction 3.31$\times$ v.s.3.12$\times$, FLOPs reduction 3.38$\times$ v.s. 2.51$\times$.

Limitations:
Please refer to weaknesses and questions. The major concern is that the method does not show better performance than baselines, and is also limited in model architectures.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a new structured pruning approach NEXP. It works by computing the dissimilarity score of the feature activations across samples and removing those filters with smaller variances. Experimental results on several models and datasets demonstrate the effectiveness of the proposed approaches.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written with clear motivation and many of the important technical details are included.  
2. The authors have presented the experimental results well and the additional discussion provides deeper insights on the effectiveness of the proposed methods.  
3. The authors evaluated models beyond image classification, i.e., the proposed methods work well on YOLOv8 object detectors.

Weaknesses:
1. The conclusion section is too short and fails to characterize the main contribution of this paper. What does it mean as to “when” and “how” to prune? The authors should elaborate on these further.   

2. I think that this is not a scalable approach for computing the pruning metrics. The pruning metric proposed in the paper requires computing a N by N matrix for each filter in the network, where N is the number of samples. This could grow quite computationally infeasible for large networks and batch sizes. The authors also fails to discuss this aspect on the pruning efficiency in the paper.  

3. In terms of experiments, I am not sure why the authors compare each methods under different compression ratio? If the pruned models in each method have different parameters, it can be hard to compare the accuracy numbers.  

4. I feel a lot of the content in section 3 is not necessary and they can could go into a separate preliminary section. Section 3.1 and early parts of Section 3.2 takes up a lot of space and in the meantime do not provides us with the motivation and insights for the later introduced methods.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to handle network pruning problem. Specifically, it proposes to use a new importance measurement, called expressiveness, to decide the pruning process. It jointly considers the model state to leverage on the proposed measurement. In addition, it can also combined with typical importance based pruning methods to improve the model efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Network pruning is a valuable research direction to study on, espeicailly for current large-scale era where efficiency matters a lot.
2. The proposed new metric to measure the network importance is interesting.
3.  Empirical results show the method superiority.

Weaknesses:
1. Adding more discussion in the conclusion part helps to improve the paper readability.
2. Since this paper proposes a new pruning metric, it is better to show more visualization and network behavior analysis to illustrate the intuition.
3. The compared baseline models are relatively old, adding more recent publications helps to support this paper.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
93qSRpucpN;"REVIEW 
Summary:
The paper proposed RGD, a novel method for integrating classifier guidance into classifier-free guidance diffusion models for solving offline MBO problems. Experiment results and ablation studies validate that the method outperforms state-of-the-art baselines and each proposed component is resonable.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Idea is intuitive and easy to follow

- Motivating example in the introduction makes the reader easy to understand the limitations of the prior method and the advantages of the proposed method

- Strong experiment results and detailed ablation studies make the proposed method more convincing

Weaknesses:
- For the diffusion-based proxy refinement part, it seems that there are several estimations to compute the distance between $p_{\phi}(y\vert \hat{x})$ and $p_{\theta}(y\vert \hat{x})$. Furthermore, it incurs additional hyperparameter $\alpha$, which should be carefully tuned.

Limitations:
There are a few minor comments on the manuscript.

- For figure 2, it seems that $\tilde{s}(x_T, y, \omega)$ should be written as $\tilde{s}(x_T, y, \hat{\omega})$. Furthermore, at first, it makes me confusion that RGD conducts classifer-guidance. However, that misleading part has been resolved after reading the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper,  the authors proposed to combine both classifier guidance and classifier-free guidance for offline black-box optimization.  In addition, the authors propose a Proxy Refinement procedure by minimizing KL divergence between the Proxy distribution and diffusion distribution regarding $y$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.  The paper is well-written and well-organized. 



2.  The paper introduces several refinement procedures to boost the offline optimization performance.  The proposed Diffusion-based Proxy Refinement procedure is interesting.

Weaknesses:
1.  **Technical contribution seems to be incremental**

Employing diffusion models for offline black-box optimization is not new.  The technical contribution of this paper seems to be incremental. The draft extends the paper ""Diffusion Models for Black-Box Optimization"" [1]. However,   detailed discussions about the relationship between the proposed method and the paper [1] are missing.

[1] Siddarth Krishnamoorthy, Satvik Mashkaria, and Aditya Grover. ""Diffusion Models for Black-Box Optimization."" ICML 2023. 


2.  **Part of the technical details are not clear.**

(a) In Equation (12),  the concrete computation procedure of $p_\theta (\hat{\boldsymbol{x}} | y)$ and $p_\theta (\hat{\boldsymbol{x}})$  via diffusion model is not clear. 

(b) The derivation of Equation (10) is not given.   It seems that Equation (10) is from the forward pass of the diffusion model.  However,  the forward pass  (Eq.32-32 in [10]) is regarding the distribution.  And the concrete  $\boldsymbol{x} _ t $ is constructed via the backward pass with  $s_\theta(\boldsymbol{x}_k,k)$ for $k \in T,\cdots, t+1$. 
 In addition,  how to choose $\mu(t)$ and $\sigma(t)$ in Equation (10)  is not clear.  

3.  **The additional proxy training, sample refinement procedure and proxy refinement procedure  increase the computation cost**

The additional proxy training, sample refinement procedure and proxy refinement procedure increase the computation cost. However, the time comparison with baselines is missing. 

4.  **The additional proxy training, sample refinement procedure and proxy refinement procedure bring many additional hyperparameters, which may overfit the offline BBO task**

In the offline BBO tasks,  the offline dataset is provided.  The evaluation is the black-box function value at the generated query at one time. 
The long-term convergence properties and exploration/exploitation balance are not considered.  As a result, there are risks that overfit the evaluation metric for the offline tasks.  The paper Introduces lots of additional hyperparameters, which increases the overfitting risks.

Limitations:
Additional computation cost and overfitting risk may be additional limitations besides the limitations discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework called Robust Guided Diffusion for the problem of Offline Black-box Optimization. The key idea is to formulate the solution as conditional generation of high-performance designs using a diffusion model which has explicit guidance from a proxy (surrogate) model. This proxy model is also refined/updated via a proxy-free diffusion procedure. Experimental analysis is shown on multiple tasks from design-bench benchmark.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Overall, I like the paper because it includes two simple changes to an existing approach (DDOM) that shows improved performance and the changes are validated by ablation choices.

Weaknesses:
- One major premise (repeated multiple times in the paper) in the paper is that proxy guidance conditional generation is more robust than updating the design with standard gradient ascent on the proxy. However, it is not immediately clear why this should be true and the justification for this key point is somewhat limited. If true, this will be much bigger insight going beyond black-box optimization. If it is only about the exploration/exploitation balance driven by w, we could also make standard gradient have this property by optimizing a upper/lower confidence bound on the objective. Please describe why this is the case either via some empirical experiment or theoretical insight. Also, in equation 11, we might evaluate the proxy far away from the training data depending on the values of s_\theta(x_t), \sigma(t), \mu(t).

- The related work coverage and corresponding experimental analysis of the paper can be improved. This problem has seen an extensive body of work recently. Please see the references below and discuss/compare them appropriately. Some of them are included in references but not compared in the experiments ([1], [2], [3]):

- [1] Yuan, Ye, et al. ""Importance-aware co-teaching for offline model-based optimization."" Advances in Neural Information Processing Systems 36 (2023).
- [2] Kim, Minsu, et al. ""Bootstrapped training of score-conditioned generator for offline design of biological sequences."" Advances in Neural Information Processing Systems 36 (2023).
- [3] Nguyen, Tung, Sudhanshu Agrawal, and Aditya Grover. ""ExPT: Synthetic pretraining for few-shot experimental design."" Advances in Neural Information Processing Systems 36 (2023).
- [4] Chemingui, Yassine, et al. ""Offline model-based optimization via policy-guided gradient search."" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 38. No. 10. 2024.
- [5] Yao, Michael S., et al. ""Generative Adversarial Bayesian Optimization for Surrogate Objectives."" arXiv preprint arXiv:2402.06532 (2024).

Limitations:
Please see weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a robust guided diffusion framework for offline black-box optimization, combining proxy and proxy-free diffusion for conditional generation. Key improvements include proxy-enhanced sampling and diffusion-based proxy refinement to address out-of-distribution issues. Experiments on the Design-Bench benchmark show the method outperforms existing techniques, validated by ablation studies.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The regularization of the proxy using the diffusion model is interesting. Additionally, optimizing the alpha parameter in an offline manner aligns well with the offline setup, enhancing the method's consistency and applicability.
- Experiments and ablations on four continuous and three discrete tasks validate the effectiveness of the proposed RGD method, showing improved performance and robustness.

Weaknesses:
- The paper lacks comparison with relevant approaches like ICT [1] and TRI-mentoring [2]. Despite referencing the latter in the related work section, it’s overlooked in the results.
- It is unclear why the results without proxy-enhanced sampling still achieve competitive outcomes, surpassing the dataset y_max. This contradicts the claims in lines 40-46. Where does the out-of-distribution (OOD) problem arise then? What is the distribution of the generated 128 candidates with and without the sampling? 
- The BDI reported results are significantly lower than in the original paper, especially for the ANT and TFBIND8 tasks. This also seems to be the case for BONET results. Did the authors change the evaluation setup?


[1]: Importance-aware Co-teaching for Offline Model-based Optimization, https://arxiv.org/abs/2309.11600

[2]: Parallel-mentoring for Offline Model-based Optimization, https://arxiv.org/abs/2309.11592

Limitations:
The authors address the limitations and potential negative impacts in their paper.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method, named RGD, for Offline Black-box Optimization (BBO). RGD incorporates an improved proxy to guide the previous proxy-free method (i.e. DDOM[4]). Key technical innovations includes (1) improving the robustness of the proxy function against adversarial samples by consistency regularization with the diffusion process; (2) dynamic per-sample reweighting between proxy-guided and proxy-free sampling. Compared to previous approaches, RGD demonstrates superior performance on Design-Bench [3].

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Methodology: RGD integrates forward and reverse approaches for BBO, in a way that they can help with each other (e.g. using forward proxy to guide the reverse sampling and using the diffusion process to improve the forward proxy), which is technically sound and interesting.

Experiment: RGD demonstrates superior performance on Design-Bench, compared to the baselines.

Ablation: Ablations on different components of RGD are provided.

Weaknesses:
The reviewer would prefer some clarifications on the method and the experiments

i) Algorithm 1, Line 4, how to identify the adversarial examples? From Line 187-188, it looks like gradient ascent is utilized to find the x that maximize y, it is unclear to the reviewer that how to determine if the obtained x is an adversarial example

ii) Algorithm 1, Line 7, refine the proxy function via eq 15. It would be best if the author could provide further details on how to optimize eq (15), e.g. number of validation and adversarial samples, number of iterations for the bi-level optimization discussed in Appendix B.

iii) Algorithm 1 Line 13, optimizing \omega. Again, it would be best if the author could provide extra info on how to optimize \omega. From Algorithm 1, it looks like \omega is time dependent and optimized for each time step. How many training iterations are required for each time step. The reviewer also wonder if the obtained \omega are dramatically different between different time steps. 

iv) From Line 257-258, it looks like the baselines shown in Table 1 & 2 were re-implemented. If this is the case, the authors are encouraged to include more implementation details, e.g. the model architecture for the score function, etc. This could help follow-up works to reproduce the reported results. The reviewer also wonders if the source code will be made public.

Limitations:
Limitations have been discussed in the appendix

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed RGD, a novel method for integrating classifier guidance into classifier-free guidance diffusion models for solving offline MBO problems. Experiment results and ablation studies validate that the method outperforms state-of-the-art baselines and each proposed component is resonable.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Idea is intuitive and easy to follow

- Motivating example in the introduction makes the reader easy to understand the limitations of the prior method and the advantages of the proposed method

- Strong experiment results and detailed ablation studies make the proposed method more convincing

Weaknesses:
- For the diffusion-based proxy refinement part, it seems that there are several estimations to compute the distance between $p_{\phi}(y\vert \hat{x})$ and $p_{\theta}(y\vert \hat{x})$. Furthermore, it incurs additional hyperparameter $\alpha$, which should be carefully tuned.

Limitations:
There are a few minor comments on the manuscript.

- For figure 2, it seems that $\tilde{s}(x_T, y, \omega)$ should be written as $\tilde{s}(x_T, y, \hat{\omega})$. Furthermore, at first, it makes me confusion that RGD conducts classifer-guidance. However, that misleading part has been resolved after reading the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper,  the authors proposed to combine both classifier guidance and classifier-free guidance for offline black-box optimization.  In addition, the authors propose a Proxy Refinement procedure by minimizing KL divergence between the Proxy distribution and diffusion distribution regarding $y$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.  The paper is well-written and well-organized. 



2.  The paper introduces several refinement procedures to boost the offline optimization performance.  The proposed Diffusion-based Proxy Refinement procedure is interesting.

Weaknesses:
1.  **Technical contribution seems to be incremental**

Employing diffusion models for offline black-box optimization is not new.  The technical contribution of this paper seems to be incremental. The draft extends the paper ""Diffusion Models for Black-Box Optimization"" [1]. However,   detailed discussions about the relationship between the proposed method and the paper [1] are missing.

[1] Siddarth Krishnamoorthy, Satvik Mashkaria, and Aditya Grover. ""Diffusion Models for Black-Box Optimization."" ICML 2023. 


2.  **Part of the technical details are not clear.**

(a) In Equation (12),  the concrete computation procedure of $p_\theta (\hat{\boldsymbol{x}} | y)$ and $p_\theta (\hat{\boldsymbol{x}})$  via diffusion model is not clear. 

(b) The derivation of Equation (10) is not given.   It seems that Equation (10) is from the forward pass of the diffusion model.  However,  the forward pass  (Eq.32-32 in [10]) is regarding the distribution.  And the concrete  $\boldsymbol{x} _ t $ is constructed via the backward pass with  $s_\theta(\boldsymbol{x}_k,k)$ for $k \in T,\cdots, t+1$. 
 In addition,  how to choose $\mu(t)$ and $\sigma(t)$ in Equation (10)  is not clear.  

3.  **The additional proxy training, sample refinement procedure and proxy refinement procedure  increase the computation cost**

The additional proxy training, sample refinement procedure and proxy refinement procedure increase the computation cost. However, the time comparison with baselines is missing. 

4.  **The additional proxy training, sample refinement procedure and proxy refinement procedure bring many additional hyperparameters, which may overfit the offline BBO task**

In the offline BBO tasks,  the offline dataset is provided.  The evaluation is the black-box function value at the generated query at one time. 
The long-term convergence properties and exploration/exploitation balance are not considered.  As a result, there are risks that overfit the evaluation metric for the offline tasks.  The paper Introduces lots of additional hyperparameters, which increases the overfitting risks.

Limitations:
Additional computation cost and overfitting risk may be additional limitations besides the limitations discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework called Robust Guided Diffusion for the problem of Offline Black-box Optimization. The key idea is to formulate the solution as conditional generation of high-performance designs using a diffusion model which has explicit guidance from a proxy (surrogate) model. This proxy model is also refined/updated via a proxy-free diffusion procedure. Experimental analysis is shown on multiple tasks from design-bench benchmark.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Overall, I like the paper because it includes two simple changes to an existing approach (DDOM) that shows improved performance and the changes are validated by ablation choices.

Weaknesses:
- One major premise (repeated multiple times in the paper) in the paper is that proxy guidance conditional generation is more robust than updating the design with standard gradient ascent on the proxy. However, it is not immediately clear why this should be true and the justification for this key point is somewhat limited. If true, this will be much bigger insight going beyond black-box optimization. If it is only about the exploration/exploitation balance driven by w, we could also make standard gradient have this property by optimizing a upper/lower confidence bound on the objective. Please describe why this is the case either via some empirical experiment or theoretical insight. Also, in equation 11, we might evaluate the proxy far away from the training data depending on the values of s_\theta(x_t), \sigma(t), \mu(t).

- The related work coverage and corresponding experimental analysis of the paper can be improved. This problem has seen an extensive body of work recently. Please see the references below and discuss/compare them appropriately. Some of them are included in references but not compared in the experiments ([1], [2], [3]):

- [1] Yuan, Ye, et al. ""Importance-aware co-teaching for offline model-based optimization."" Advances in Neural Information Processing Systems 36 (2023).
- [2] Kim, Minsu, et al. ""Bootstrapped training of score-conditioned generator for offline design of biological sequences."" Advances in Neural Information Processing Systems 36 (2023).
- [3] Nguyen, Tung, Sudhanshu Agrawal, and Aditya Grover. ""ExPT: Synthetic pretraining for few-shot experimental design."" Advances in Neural Information Processing Systems 36 (2023).
- [4] Chemingui, Yassine, et al. ""Offline model-based optimization via policy-guided gradient search."" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 38. No. 10. 2024.
- [5] Yao, Michael S., et al. ""Generative Adversarial Bayesian Optimization for Surrogate Objectives."" arXiv preprint arXiv:2402.06532 (2024).

Limitations:
Please see weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a robust guided diffusion framework for offline black-box optimization, combining proxy and proxy-free diffusion for conditional generation. Key improvements include proxy-enhanced sampling and diffusion-based proxy refinement to address out-of-distribution issues. Experiments on the Design-Bench benchmark show the method outperforms existing techniques, validated by ablation studies.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The regularization of the proxy using the diffusion model is interesting. Additionally, optimizing the alpha parameter in an offline manner aligns well with the offline setup, enhancing the method's consistency and applicability.
- Experiments and ablations on four continuous and three discrete tasks validate the effectiveness of the proposed RGD method, showing improved performance and robustness.

Weaknesses:
- The paper lacks comparison with relevant approaches like ICT [1] and TRI-mentoring [2]. Despite referencing the latter in the related work section, it’s overlooked in the results.
- It is unclear why the results without proxy-enhanced sampling still achieve competitive outcomes, surpassing the dataset y_max. This contradicts the claims in lines 40-46. Where does the out-of-distribution (OOD) problem arise then? What is the distribution of the generated 128 candidates with and without the sampling? 
- The BDI reported results are significantly lower than in the original paper, especially for the ANT and TFBIND8 tasks. This also seems to be the case for BONET results. Did the authors change the evaluation setup?


[1]: Importance-aware Co-teaching for Offline Model-based Optimization, https://arxiv.org/abs/2309.11600

[2]: Parallel-mentoring for Offline Model-based Optimization, https://arxiv.org/abs/2309.11592

Limitations:
The authors address the limitations and potential negative impacts in their paper.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method, named RGD, for Offline Black-box Optimization (BBO). RGD incorporates an improved proxy to guide the previous proxy-free method (i.e. DDOM[4]). Key technical innovations includes (1) improving the robustness of the proxy function against adversarial samples by consistency regularization with the diffusion process; (2) dynamic per-sample reweighting between proxy-guided and proxy-free sampling. Compared to previous approaches, RGD demonstrates superior performance on Design-Bench [3].

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Methodology: RGD integrates forward and reverse approaches for BBO, in a way that they can help with each other (e.g. using forward proxy to guide the reverse sampling and using the diffusion process to improve the forward proxy), which is technically sound and interesting.

Experiment: RGD demonstrates superior performance on Design-Bench, compared to the baselines.

Ablation: Ablations on different components of RGD are provided.

Weaknesses:
The reviewer would prefer some clarifications on the method and the experiments

i) Algorithm 1, Line 4, how to identify the adversarial examples? From Line 187-188, it looks like gradient ascent is utilized to find the x that maximize y, it is unclear to the reviewer that how to determine if the obtained x is an adversarial example

ii) Algorithm 1, Line 7, refine the proxy function via eq 15. It would be best if the author could provide further details on how to optimize eq (15), e.g. number of validation and adversarial samples, number of iterations for the bi-level optimization discussed in Appendix B.

iii) Algorithm 1 Line 13, optimizing \omega. Again, it would be best if the author could provide extra info on how to optimize \omega. From Algorithm 1, it looks like \omega is time dependent and optimized for each time step. How many training iterations are required for each time step. The reviewer also wonder if the obtained \omega are dramatically different between different time steps. 

iv) From Line 257-258, it looks like the baselines shown in Table 1 & 2 were re-implemented. If this is the case, the authors are encouraged to include more implementation details, e.g. the model architecture for the score function, etc. This could help follow-up works to reproduce the reported results. The reviewer also wonders if the source code will be made public.

Limitations:
Limitations have been discussed in the appendix

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
8Hy3KMZTL5;"REVIEW 
Summary:
The paper presents H-CLIP, a novel framework for open-vocabulary semantic segmentation using the CLIP model. The framework addresses three key challenges: high computational cost, misalignment between CLIP's image and text modalities, and degraded generalization ability on unseen categories when fine-tuning for pixel-level predictions. H-CLIP employs a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both CLIP modalities. This strategy uses efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module to mitigate misalignment issues. Additionally, an orthogonality constraint based on the hyperspherical energy principle is applied to the text encoder to preserve the generalization ability of the pre-trained model.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The introduction of the H-CLIP framework for open-vocabulary semantic segmentation represents a significant innovation. The use of a symmetrical parameter-efficient fine-tuning (PEFT) strategy in hyperspherical space is a unique approach to addressing the challenges associated with fine-tuning vision-language models.

The paper provides extensive experimental results across multiple benchmarks, including ADE20K, PASCAL VOC, and PASCAL-Context. These experiments validate the effectiveness of H-CLIP, showing its superior performance compared to state-of-the-art methods.

Weaknesses:
1. Consider that the expression in formula 5 does not specify how to interact with the \boldsymbol{R} matrix.

2. The paper states that current fine-tuning strategies are usually asymmetrical, but it does not provide enough evidence or references to support this claim. The authors should provide empirical evidence or references to support the claim of asymmetry.

3. While the paper extensively discusses the orthogonality constraint in the CLIP image encoder, it lacks an in-depth analysis of how the misalignment problem impacts segmentation performance. The authors should discuss the specific effects of misalignment on segmentation.

4. The paper should mention SAM (Segment Anything) and how the current work is still significant

Limitations:
The work is on semantic segmentation and there is no qualitative comparison shown in the main paper. There are some visuals in the supplementary, but most of those are from test set and there is no comparison shown with the baselines and existing methods, so it is not clear where the improvement is coming from. It will be good to see how the results improve with and without alignment.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents H-CLIP, a novel approach for parameter-efficient fine-tuning of the CLIP model in hyperspherical space, specifically for open-vocabulary semantic segmentation. H-CLIP includes the introduction of a symmetrical parameter-efficient fine-tuning strategy, leveraging hyperspherical energy principles. And a dual cross-relation communication module is utilized to enhance cross-modal and cross-layer alignment.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper is well-motivated. The proposed H-CLIP effectively addresses common issues in fine-tuning CLIP.
- The paper effectively argues that maintaining the hyperspherical energy helps preserve the model's generalization ability, a critical factor in multi-modal tasks.
- The ablation experiments are thorough and effectively support the arguments.

Weaknesses:
- The writing needs improvement. The introduction lacks transitions from existing problems to the approach of this paper, such as introducing the advantages of Hyperspherical Space. 
- Some formula descriptions can be optimized, for example, explaining the meaning of * in Formula 9. 
- Details about comparison methods are needed. In Table 1, the compared method SAN includes an additional backbone.

Limitations:
The authors provide no analysis of the limitations and broader impact. The author can analyze the limitations of this fine-tuning strategy in the field of OVS.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes H-CLIP, a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both of the two CLIP modalities. The PEFT strategy is achieved by a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. Extensive evaluations across various benchmarks show that H-CLIP achieves new SOTA open-vocabulary semantic segmentation results while only requiring updating approximately 4% of the total parameters of CLIP.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper achieves SOTA performance.
The Parameter-efficient Fine-tuning is explained by tensor computation.

Weaknesses:
1.The novelty is limited.  Partial orthogonal fine-tuning (POF) doesn't directly address the challenges of OVSS but rather offers a generic PEFT approach, so what is the difference between POF and OFT[1]? In Equ.5, which module's weights are used by the pre-trained weight matrix, Q(K or V)’s projection layer or FFN? Method details need detailed explanation.
2.Some concerns about DCRC. In this section, the author discusses the use of two k layers deep neural network to update the fourth-order tensor in Equ. 7, and provides some mathematical proof. However, these proofs only show that reversible transformations S(·) can be replaced by reversible matrices S (as shown in Equ. 11,12,14,15), and the authors use k layers deep neural network to replace such reversible matrices S, which cannot explain the meaning of reversible transformations. In other words, why adopting reversible transformations to update the fourth-order tensor in Equ. 7, and what is the role of reversible transformations? Is this approach also work in other fields other than semantic segmentation tasks? In addition, If the block diagonal structure is not adopted, Equ. 16 seems to require only one reversible matrices S4 for mapping. Does this reduce the number of parameters?
3.Insufficient experimental analysis. 
1)The decoder of HCLIP seems to be learnable as well. Does the param in Table 2 calculate the decoder part? And, Is the proposed PEFT method applicable to various decoders? If it is replaced with linear probe, is the proposed method still effective? Need further exploration.
2)If a different VFM is adopted (not CLIP), is the proposed method still valid?
3)The proposed method should be compared with more PEFT methods such as VPT, Adapter, LST, SSF [1-5] .

[1] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information Processing Systems, 36:79320–79362, 2023.
[2] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709–727. Springer, 2022.
[3] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790–2799. PMLR, 2019.
[4] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:12991–13005, 2022.
[5] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. Advances in Neural Information Processing Systems, 35:109–123, 2022.

Limitations:
The limitation of the proposed method should be discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel method called Parameter-Efficient Fine-Tuning in Hyperspherical Space for efficiently solving the open-vocabulary semantic segmentation problem. The method introduces a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. To maintain the generalization ability offered by the CLIP text encoder, the authors designed a constraint to PEFT based on Hyperspherical Energy. Comprehensive results on open-vocabulary semantic segmentation benchmarks demonstrate the strong performance of this PEFT method by training only 4% of the total parameters of CLIP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The idea of introducing hyperspherical space to achieve parameter-efficient training is interesting. This approach attains state-of-the-art performance on current open-vocabulary semantic segmentation benchmarks with fewer learnable parameters. Additionally, it demonstrates better parameter efficiency than LORA on open-vocabulary semantic segmentation tasks, as shown in Table 3.

Weaknesses:
I do not see any clear weaknesses. However, I acknowledge that I am not familiar with hyperspherical theorems.

Limitations:
See in questions. No other clear limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents H-CLIP, a novel framework for open-vocabulary semantic segmentation using the CLIP model. The framework addresses three key challenges: high computational cost, misalignment between CLIP's image and text modalities, and degraded generalization ability on unseen categories when fine-tuning for pixel-level predictions. H-CLIP employs a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both CLIP modalities. This strategy uses efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module to mitigate misalignment issues. Additionally, an orthogonality constraint based on the hyperspherical energy principle is applied to the text encoder to preserve the generalization ability of the pre-trained model.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The introduction of the H-CLIP framework for open-vocabulary semantic segmentation represents a significant innovation. The use of a symmetrical parameter-efficient fine-tuning (PEFT) strategy in hyperspherical space is a unique approach to addressing the challenges associated with fine-tuning vision-language models.

The paper provides extensive experimental results across multiple benchmarks, including ADE20K, PASCAL VOC, and PASCAL-Context. These experiments validate the effectiveness of H-CLIP, showing its superior performance compared to state-of-the-art methods.

Weaknesses:
1. Consider that the expression in formula 5 does not specify how to interact with the \boldsymbol{R} matrix.

2. The paper states that current fine-tuning strategies are usually asymmetrical, but it does not provide enough evidence or references to support this claim. The authors should provide empirical evidence or references to support the claim of asymmetry.

3. While the paper extensively discusses the orthogonality constraint in the CLIP image encoder, it lacks an in-depth analysis of how the misalignment problem impacts segmentation performance. The authors should discuss the specific effects of misalignment on segmentation.

4. The paper should mention SAM (Segment Anything) and how the current work is still significant

Limitations:
The work is on semantic segmentation and there is no qualitative comparison shown in the main paper. There are some visuals in the supplementary, but most of those are from test set and there is no comparison shown with the baselines and existing methods, so it is not clear where the improvement is coming from. It will be good to see how the results improve with and without alignment.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents H-CLIP, a novel approach for parameter-efficient fine-tuning of the CLIP model in hyperspherical space, specifically for open-vocabulary semantic segmentation. H-CLIP includes the introduction of a symmetrical parameter-efficient fine-tuning strategy, leveraging hyperspherical energy principles. And a dual cross-relation communication module is utilized to enhance cross-modal and cross-layer alignment.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper is well-motivated. The proposed H-CLIP effectively addresses common issues in fine-tuning CLIP.
- The paper effectively argues that maintaining the hyperspherical energy helps preserve the model's generalization ability, a critical factor in multi-modal tasks.
- The ablation experiments are thorough and effectively support the arguments.

Weaknesses:
- The writing needs improvement. The introduction lacks transitions from existing problems to the approach of this paper, such as introducing the advantages of Hyperspherical Space. 
- Some formula descriptions can be optimized, for example, explaining the meaning of * in Formula 9. 
- Details about comparison methods are needed. In Table 1, the compared method SAN includes an additional backbone.

Limitations:
The authors provide no analysis of the limitations and broader impact. The author can analyze the limitations of this fine-tuning strategy in the field of OVS.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes H-CLIP, a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both of the two CLIP modalities. The PEFT strategy is achieved by a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. Extensive evaluations across various benchmarks show that H-CLIP achieves new SOTA open-vocabulary semantic segmentation results while only requiring updating approximately 4% of the total parameters of CLIP.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper achieves SOTA performance.
The Parameter-efficient Fine-tuning is explained by tensor computation.

Weaknesses:
1.The novelty is limited.  Partial orthogonal fine-tuning (POF) doesn't directly address the challenges of OVSS but rather offers a generic PEFT approach, so what is the difference between POF and OFT[1]? In Equ.5, which module's weights are used by the pre-trained weight matrix, Q(K or V)’s projection layer or FFN? Method details need detailed explanation.
2.Some concerns about DCRC. In this section, the author discusses the use of two k layers deep neural network to update the fourth-order tensor in Equ. 7, and provides some mathematical proof. However, these proofs only show that reversible transformations S(·) can be replaced by reversible matrices S (as shown in Equ. 11,12,14,15), and the authors use k layers deep neural network to replace such reversible matrices S, which cannot explain the meaning of reversible transformations. In other words, why adopting reversible transformations to update the fourth-order tensor in Equ. 7, and what is the role of reversible transformations? Is this approach also work in other fields other than semantic segmentation tasks? In addition, If the block diagonal structure is not adopted, Equ. 16 seems to require only one reversible matrices S4 for mapping. Does this reduce the number of parameters?
3.Insufficient experimental analysis. 
1)The decoder of HCLIP seems to be learnable as well. Does the param in Table 2 calculate the decoder part? And, Is the proposed PEFT method applicable to various decoders? If it is replaced with linear probe, is the proposed method still effective? Need further exploration.
2)If a different VFM is adopted (not CLIP), is the proposed method still valid?
3)The proposed method should be compared with more PEFT methods such as VPT, Adapter, LST, SSF [1-5] .

[1] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information Processing Systems, 36:79320–79362, 2023.
[2] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709–727. Springer, 2022.
[3] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790–2799. PMLR, 2019.
[4] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:12991–13005, 2022.
[5] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. Advances in Neural Information Processing Systems, 35:109–123, 2022.

Limitations:
The limitation of the proposed method should be discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel method called Parameter-Efficient Fine-Tuning in Hyperspherical Space for efficiently solving the open-vocabulary semantic segmentation problem. The method introduces a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. To maintain the generalization ability offered by the CLIP text encoder, the authors designed a constraint to PEFT based on Hyperspherical Energy. Comprehensive results on open-vocabulary semantic segmentation benchmarks demonstrate the strong performance of this PEFT method by training only 4% of the total parameters of CLIP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The idea of introducing hyperspherical space to achieve parameter-efficient training is interesting. This approach attains state-of-the-art performance on current open-vocabulary semantic segmentation benchmarks with fewer learnable parameters. Additionally, it demonstrates better parameter efficiency than LORA on open-vocabulary semantic segmentation tasks, as shown in Table 3.

Weaknesses:
I do not see any clear weaknesses. However, I acknowledge that I am not familiar with hyperspherical theorems.

Limitations:
See in questions. No other clear limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
7yqjVgWWxx;"REVIEW 
Summary:
This paper proves that the probability ratio that appears when computing the time reverse rate matrix for an absorbing state diffusion model has a simple form composed of the conditional distributions of clean data given partial masking scaled by an analytic time dependent weighting. They exploit this form to simplify the parametrization of absorbing state diffusion models and show this improves performance and sampling speed on text datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work is clearly written and I think theorem 1 will be genuinely useful for future work in absorbing state diffusion models. The fact that the time reversal of the rate matrix has a simple relation to the conditional distributions of clean data that is independent of time makes the target of optimization much clearer. This removes needless complexity when trying to condition models on time, even though the relationship with time is known analytically. This also helps model convergence since the scale factor of the target is known allowing the network to be targeting normalized quantities which is highly desirable for neural net training.

The removal of the time conditioning also has a significant benefit with respect to model speed ups. It is quite surprising that the absorbing state literature does not use this trick where at most L neural network evaluations are required for L length data. This paper should significantly help existing implementations in this regard by removing needless calls to the network.

Weaknesses:
Theorem 2 is wrong. Line C.14 in the proof is incorrect, it's not an equality but a lower bound. The correct version should read

$q\_\theta(x\_0) = \sum\_{\pi} U(\pi) q\_\theta(x\_0 | \pi)$

$q\_\theta(x_0) = \sum\_{\pi} U(\pi) \prod\_{l=1}^d q\_\theta(x\_0^{\pi(l)} | x\_0^{\pi( <l )} )$

$q\_\theta(x_0) = \mathbb{E}\_{\pi \sim U(S\_d) } [ \prod\_{l=1}^d q\_\theta (x\_0^{\pi(l)} | x\_0^{\pi (<l)}) ] $

$ \log q\_\theta(x\_0) = \log ( \mathbb{E}\_{\pi \sim U(S\_d)} [ \prod\_{l=1}^d q\_\theta(x\_0^{\pi(l)} | x\_0^{\pi(<l)}) ] )$

$ \log q\_\theta(x\_0) \geq \mathbb{E}\_{\pi \sim U(S\_d)} [ \sum\_{l=1}^d \log q\_\theta (x\_0^{\pi(l)} | x\_0^{\pi(<l)} ) ]$

When your generative model is a mixture of different generation paths (which an absorbing state diffusion model is), then you need to apply Jensen's inequality. See https://arxiv.org/pdf/2110.02037 equation (2).

Therefore, the authors should remove Section 3.3. I think this section should be replaced with discussion of Autoregressive Diffusion Models https://arxiv.org/pdf/2110.02037 which the author's model has basically reduced to. Autoregressive diffusion models randomly sample a generation order and gradually infill tokens with no dependence on time. The link between Autoregressive Diffusion Models and absorbing state diffusion should be clearly discussed in this paper and this reference is glaringly missing.

The authors should also remove line SEDD-S* in Table 2 since it is based on Theorem 2. Your models are then really not doing favourably compared to standard SEDD. What is your explanation for this and new narrative for Table 2?

In the paper's current state I cannot recommend acceptance since a large part of the narrative is based around Theorem 2. However, I believe the contributions surrounding Theorem 1 with regards to making models simpler and achieve good speed up stand alone as a worthy contribution. Therefore, if the authors clearly describe how they will adjust the narrative under this new information I will be happy to raise my score.

I think it would also be good to include a baseline against autoregressive diffusion models since they propose additional tricks relating to picking how many tokens to reveal. However, I appreciate this would be difficult in the limited time of the rebuttal period and is not required for an increase in score.

Limitations:
The authors adequately discuss the limitations in Section 6.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a simplified discrete diffusion model to improve upon prior language diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The method is simple and scalable. It is overall a nice insight, and the authors do a good job in extracting the relevant and impactful applications of this.

* The method seems to improve upon previous results, in particular resulting in better/faster sample quality.

* The presentation is pretty clear and direct.

Weaknesses:
* Although the method speeds up sampling, especially in the large sample step regime, this is a bit misleading/irrelevant. In particular, under more standard sampling practices, the gain is naturally not as big, so the claim of 3.5x improvement is a bit misleading. Furthermore, this does not really improve the sample quality at a smaller number of steps, which is the critical question. As a comparison, this would be like sampling from a standard diffusion model with 4096 timesteps, showing that you can speed it up in that regime, and then claiming a general improvement.

* The results are ultimately a bit marginal. The improvements on sample quality are nice, but I think there is a mistranslation between figure 2 and table 1 (there is no 15 generative perplexity for RADD in that table). Until this is clarified, I'm trusting the results of table 1 more. Table 2 also shows a slight improvement.

* The exact likelihood computation is never applied. I want table 2 to showcase this exact likelihood instead of just a bound.

* The model size is only small. I want to see a similar improvement for the medium quality.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work derives a new interesting connection between the concrete score and conditional target densities in absorbing diffusion models, which decomposes the time-dependent ratio between marginal probabilities (of two transitive states) as a conditional distribution on clean data scaled by an analytic time-dependent scalar, and hence inspires the commonly-used scaling trick and new re-parameterizations. In addition, it also simplifies the original complicated loss objective (denoising score entropy; DSE) as a more straightforward denoising cross-entropy loss (DCE) that enables the exact log-likelihood computation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is generally well-written and easy to follow.
2. This work proposes valuable insights of decoupled model parameterizations and simplified learning objectives, whose effectivenesses are theoretically grounded.
3. The proposed methods are also numerically verified, which advances the development of (absorbing) discrete diffusion models.

Weaknesses:
1. Despite that the overall presentation is good, it can be further improved by interpreting or illustrating more about the problem formulation. For example, what is the intuition behind the absorbing matrix $Q^{\text{absorb}}$ (eq. (2.4))? Why do we require a more complicated DSE loss instead of usual score-matching objectives (e.g. MSE)? Note that in eq. (2.6), the score network must be *additionally* positive. 
2. Although the proposed method (RADD) is reported to be superior for efficient sampling, the performance of RADD need further verifications on language modeling tasks (Table 2). The hyper-parameters should be fine-tuned to better demonstrate the capability of RADD.

Limitations:
As is stated by authors, future explorations include flexible variable-length texts generation and applications to models with larger scales.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proves that the probability ratio that appears when computing the time reverse rate matrix for an absorbing state diffusion model has a simple form composed of the conditional distributions of clean data given partial masking scaled by an analytic time dependent weighting. They exploit this form to simplify the parametrization of absorbing state diffusion models and show this improves performance and sampling speed on text datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work is clearly written and I think theorem 1 will be genuinely useful for future work in absorbing state diffusion models. The fact that the time reversal of the rate matrix has a simple relation to the conditional distributions of clean data that is independent of time makes the target of optimization much clearer. This removes needless complexity when trying to condition models on time, even though the relationship with time is known analytically. This also helps model convergence since the scale factor of the target is known allowing the network to be targeting normalized quantities which is highly desirable for neural net training.

The removal of the time conditioning also has a significant benefit with respect to model speed ups. It is quite surprising that the absorbing state literature does not use this trick where at most L neural network evaluations are required for L length data. This paper should significantly help existing implementations in this regard by removing needless calls to the network.

Weaknesses:
Theorem 2 is wrong. Line C.14 in the proof is incorrect, it's not an equality but a lower bound. The correct version should read

$q\_\theta(x\_0) = \sum\_{\pi} U(\pi) q\_\theta(x\_0 | \pi)$

$q\_\theta(x_0) = \sum\_{\pi} U(\pi) \prod\_{l=1}^d q\_\theta(x\_0^{\pi(l)} | x\_0^{\pi( <l )} )$

$q\_\theta(x_0) = \mathbb{E}\_{\pi \sim U(S\_d) } [ \prod\_{l=1}^d q\_\theta (x\_0^{\pi(l)} | x\_0^{\pi (<l)}) ] $

$ \log q\_\theta(x\_0) = \log ( \mathbb{E}\_{\pi \sim U(S\_d)} [ \prod\_{l=1}^d q\_\theta(x\_0^{\pi(l)} | x\_0^{\pi(<l)}) ] )$

$ \log q\_\theta(x\_0) \geq \mathbb{E}\_{\pi \sim U(S\_d)} [ \sum\_{l=1}^d \log q\_\theta (x\_0^{\pi(l)} | x\_0^{\pi(<l)} ) ]$

When your generative model is a mixture of different generation paths (which an absorbing state diffusion model is), then you need to apply Jensen's inequality. See https://arxiv.org/pdf/2110.02037 equation (2).

Therefore, the authors should remove Section 3.3. I think this section should be replaced with discussion of Autoregressive Diffusion Models https://arxiv.org/pdf/2110.02037 which the author's model has basically reduced to. Autoregressive diffusion models randomly sample a generation order and gradually infill tokens with no dependence on time. The link between Autoregressive Diffusion Models and absorbing state diffusion should be clearly discussed in this paper and this reference is glaringly missing.

The authors should also remove line SEDD-S* in Table 2 since it is based on Theorem 2. Your models are then really not doing favourably compared to standard SEDD. What is your explanation for this and new narrative for Table 2?

In the paper's current state I cannot recommend acceptance since a large part of the narrative is based around Theorem 2. However, I believe the contributions surrounding Theorem 1 with regards to making models simpler and achieve good speed up stand alone as a worthy contribution. Therefore, if the authors clearly describe how they will adjust the narrative under this new information I will be happy to raise my score.

I think it would also be good to include a baseline against autoregressive diffusion models since they propose additional tricks relating to picking how many tokens to reveal. However, I appreciate this would be difficult in the limited time of the rebuttal period and is not required for an increase in score.

Limitations:
The authors adequately discuss the limitations in Section 6.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a simplified discrete diffusion model to improve upon prior language diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The method is simple and scalable. It is overall a nice insight, and the authors do a good job in extracting the relevant and impactful applications of this.

* The method seems to improve upon previous results, in particular resulting in better/faster sample quality.

* The presentation is pretty clear and direct.

Weaknesses:
* Although the method speeds up sampling, especially in the large sample step regime, this is a bit misleading/irrelevant. In particular, under more standard sampling practices, the gain is naturally not as big, so the claim of 3.5x improvement is a bit misleading. Furthermore, this does not really improve the sample quality at a smaller number of steps, which is the critical question. As a comparison, this would be like sampling from a standard diffusion model with 4096 timesteps, showing that you can speed it up in that regime, and then claiming a general improvement.

* The results are ultimately a bit marginal. The improvements on sample quality are nice, but I think there is a mistranslation between figure 2 and table 1 (there is no 15 generative perplexity for RADD in that table). Until this is clarified, I'm trusting the results of table 1 more. Table 2 also shows a slight improvement.

* The exact likelihood computation is never applied. I want table 2 to showcase this exact likelihood instead of just a bound.

* The model size is only small. I want to see a similar improvement for the medium quality.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work derives a new interesting connection between the concrete score and conditional target densities in absorbing diffusion models, which decomposes the time-dependent ratio between marginal probabilities (of two transitive states) as a conditional distribution on clean data scaled by an analytic time-dependent scalar, and hence inspires the commonly-used scaling trick and new re-parameterizations. In addition, it also simplifies the original complicated loss objective (denoising score entropy; DSE) as a more straightforward denoising cross-entropy loss (DCE) that enables the exact log-likelihood computation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is generally well-written and easy to follow.
2. This work proposes valuable insights of decoupled model parameterizations and simplified learning objectives, whose effectivenesses are theoretically grounded.
3. The proposed methods are also numerically verified, which advances the development of (absorbing) discrete diffusion models.

Weaknesses:
1. Despite that the overall presentation is good, it can be further improved by interpreting or illustrating more about the problem formulation. For example, what is the intuition behind the absorbing matrix $Q^{\text{absorb}}$ (eq. (2.4))? Why do we require a more complicated DSE loss instead of usual score-matching objectives (e.g. MSE)? Note that in eq. (2.6), the score network must be *additionally* positive. 
2. Although the proposed method (RADD) is reported to be superior for efficient sampling, the performance of RADD need further verifications on language modeling tasks (Table 2). The hyper-parameters should be fine-tuned to better demonstrate the capability of RADD.

Limitations:
As is stated by authors, future explorations include flexible variable-length texts generation and applications to models with larger scales.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
7ug4oSmN7l;"REVIEW 
Summary:
This paper presents a novel neural network-based CARP solver that uses a direction-aware attention model to incorporate directionality into the embedding process. It then applies supervised reinforcement learning for subsequent fine-tuning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A learning-based CARP solver is proposed.  
2. The performance of the proposed solver on large-scale data is discussed.

Weaknesses:
1. The comparison algorithms were published five years ago, and there is no discussion of existing methods aimed at big data.  
2. The experiments only tested the self-constructed dataset and did not evaluate on public datasets.

Limitations:
The amount of data required for algorithm training needs to be discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a learning-based method to address the Capacitated Arc Routing Problem (CARP). It involves breaking undirected edges into directed arcs and utilizing a graph attention network to build a Direction-aware Attention Model. In the training process, supervised learning is used to create the initial policy, followed by reinforcement learning based on policy gradients using Proximal Policy Optimization (PPO) to refine strategies. Lastly, dynamic programming is applied to optimize depot placements for path enhancement. Experimental outcomes show notable benefits of this algorithm in evaluation criteria.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
In general, the paper exhibits a well-organized structure with detailed experimental outcomes showcase through graphs and tables, facilitating readers in comprehending and visualizing the results effortlessly. The dataset employed comprises real-world scenarios, thereby boosting its practical relevance.

Weaknesses:
Converting the graph G from arcs to nodes represents a common approach in many heuristics for addressing CARP. This process adds complexity to the problem and increases its scale. The proposed method appears to lack enough novelty, with most components bearing resemblance to neural models designed for CVRP.

Limitations:
It appears that the paper focuses on an unlimited number of vehicles. How would the approach adapt to a specific set of vehicles?

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors skillfully address challenges posed by non-Euclidean graphs, traversal direction, and capacity constraints with their novel NN-based solver in solving capacitated arc routing problem. The introduction of the direction-aware attention model and a supervised reinforcement learning scheme is particularly commendable. These innovations significantly narrow the gap with advanced metaheuristics, achieving superior efficiency and competitive decision quality.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The manuscript employs numerous innovative methods to solve the capacitated arc routing problem, achieving impressive results. 
2. It also shows promising performance in generalizing to larger problem instances.
3. The combination of supervised and reinforcement learning is quite interesting. Using supervised learning for pre-training followed by fine-tuning with reinforcement learning is a noteworthy approach.
4. The qualitative comparisons in real street scenes presented in Figure 4 are particularly interesting.

Weaknesses:
1. It's better to redraw the first part of Figure 1 to enhance its aesthetic quality.
2. The baseline is not very recent. After S2V-DQN and S2V-DQN, there are still some excellent works that can be used to address the CARP problem.
3. Some writing errors have been identified, such as in line 2 of Algorithm 1. Please review the entire manuscript to check.
4. The completeness of the manuscript still requires supplementation and refinement.

Limitations:
The approach of decomposing undirected edges into directed ones introduces additional decision elements, which complicates the problem. It's better that the authors can find a more efficient graph processing method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new learning-based constructive heuristic for capacitated arc routing problems. In contrast to node routing problems such as the TSP and VRP, arc routing problems received comparably little attentition. To address the specific
challenges in the capacitated arc routing problems, the authors propose a Neural Network-based approach that uses a graph attention
model considering arc directionality, a reinforcement learning approach with supervised pre-training and PPO-based fine-tuning. In
order to improve solutions obtained by an RL-based construction approach, they propose a beam search approach for path optimization which, after turning the set of routes into a giant tour, splits the tour into routes by adding returns to the depot. A set of experiments
shows that the proposed approach consistently yields better results than traditional hand-crafted constructive heuristics, and that their solutions almost match the quality of a time-consuming memetic algorithm that is only capable of solving small instance in a reaonable amount of time.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors propose one of the first learning-based approaches for the capacitated arc routing problem (CARP). Their approach, in particular their graph embedding, explicitly addresses one of the challenges of learning-based construction algorithms for this problem by explicitly replacing the undirected edges by directed arcs. This idea is original and turns out to be helpful to create a well-performing heuristic. 

The online performance of the approach surpasses hand-crafted constructive heuristics both in terms of runtime and efficiency. While for small instances, other metaheuristic approaches are better, it can be assumed that for large-scale instances, the proposed approach surpasses the state-of-the art of heuristic approaches. This is a significant result, since for node routing problems such as the CVRP, researcher have been struggling for years to design learning-based heuristics that achieve a performace that is comparable to hand-crafted heuristics. It should be mentioned, though, that in general, arc routing problems receive much less attention than node routing problems in the literature.

The paper comprises several insightful and, as far as I can tell, reasonably designed experiments, in particular showing the generalization capability to larger instances. 

The paper provides both code and instances.

Weaknesses:
The presentation of the CARP routing problem, the solutions approaches and related work lacks clarity in many places.
As an example, in the abstract, we find that the CARP consists in finding ""the minimum-cost tour""hat covers all required edges on a graph, while within capacity constraints"". This is a a bit misleading description since we look for a set of routes instead of a tour.

The paper distinguishes ""heuristics"" and ""metaheuristics"", while clearly metaheuristics are a type of heuristics. Actually, what the authors appear to have in mind is ""constructive heuristics"" which sequentially construct a solution by adding edges to form routes. I suggest to formulate more precisely here.

Similarly, it would enhance the understanding of the paper to introduce the notion of ""route-first, cluster second"" and the related
notion of a ""giant tour"" which is commonplace in routing applications, to characterize respective existing work. It would even facilitate the
presented path optimization which actually turns the presented approach into a route-first, cluster second approach. 


The computational results are convincing, but the discussion should emphasize that a fair comparison can only be made between their
approach wihout path optimization and the other constructive heuristics. It would indeed be interesting to see how the far the path
optimization is able to improve the results of the other constructive heuristics.

The claims ""NN-based approaches tend to lag behind advanced metaheuristics"" (abstract) and ""NN-based methods usually lags far
behind the traditional ones in solving CARP"", ""they still lag significantly behind traditional methods"" are not valid. Actually, (Rahmamoorty et. al 2024) (reference 20) report that on average, they improve upon the memetic algorithm by 11% on average.

When it comes to the evaluation of the path scanning approaches in the experiments, it is unclear how they are parameterized. From reading the paper (Aarakaki 2019) one sees that the parameter alpha and the number of iteration have a considerable impact both on solution time and solution quality, and (albeit on different instances), the average gaps for the path scanning approaches to the optimal (and to the memetic algrithm) reported in (Aarakaki 2019) are smaller than those found in the submission.

The description of the path improvement is not very clear; in particular the definition of the state used in the Dynamic Programming
algorithm. Is it a path? Is it the length of a path? Also, the statement ""f(*) denotes a state featuring dynamic programming"" is hard
to decipher.

Training time is not discussed at all.

Limitations:
Limitations are mostly addressed in a reasonable way. I suggest to add the following aspects:

I think that for small instances, the approach by (Rahmamoorty et. al 2024) may surpass the results reported here, which should be mentioned. 

Also, you should at least briefly mention the training time, since this makes it easier to assess the trade-off between offline effort and online performace.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel neural network-based CARP solver that uses a direction-aware attention model to incorporate directionality into the embedding process. It then applies supervised reinforcement learning for subsequent fine-tuning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A learning-based CARP solver is proposed.  
2. The performance of the proposed solver on large-scale data is discussed.

Weaknesses:
1. The comparison algorithms were published five years ago, and there is no discussion of existing methods aimed at big data.  
2. The experiments only tested the self-constructed dataset and did not evaluate on public datasets.

Limitations:
The amount of data required for algorithm training needs to be discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a learning-based method to address the Capacitated Arc Routing Problem (CARP). It involves breaking undirected edges into directed arcs and utilizing a graph attention network to build a Direction-aware Attention Model. In the training process, supervised learning is used to create the initial policy, followed by reinforcement learning based on policy gradients using Proximal Policy Optimization (PPO) to refine strategies. Lastly, dynamic programming is applied to optimize depot placements for path enhancement. Experimental outcomes show notable benefits of this algorithm in evaluation criteria.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
In general, the paper exhibits a well-organized structure with detailed experimental outcomes showcase through graphs and tables, facilitating readers in comprehending and visualizing the results effortlessly. The dataset employed comprises real-world scenarios, thereby boosting its practical relevance.

Weaknesses:
Converting the graph G from arcs to nodes represents a common approach in many heuristics for addressing CARP. This process adds complexity to the problem and increases its scale. The proposed method appears to lack enough novelty, with most components bearing resemblance to neural models designed for CVRP.

Limitations:
It appears that the paper focuses on an unlimited number of vehicles. How would the approach adapt to a specific set of vehicles?

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors skillfully address challenges posed by non-Euclidean graphs, traversal direction, and capacity constraints with their novel NN-based solver in solving capacitated arc routing problem. The introduction of the direction-aware attention model and a supervised reinforcement learning scheme is particularly commendable. These innovations significantly narrow the gap with advanced metaheuristics, achieving superior efficiency and competitive decision quality.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The manuscript employs numerous innovative methods to solve the capacitated arc routing problem, achieving impressive results. 
2. It also shows promising performance in generalizing to larger problem instances.
3. The combination of supervised and reinforcement learning is quite interesting. Using supervised learning for pre-training followed by fine-tuning with reinforcement learning is a noteworthy approach.
4. The qualitative comparisons in real street scenes presented in Figure 4 are particularly interesting.

Weaknesses:
1. It's better to redraw the first part of Figure 1 to enhance its aesthetic quality.
2. The baseline is not very recent. After S2V-DQN and S2V-DQN, there are still some excellent works that can be used to address the CARP problem.
3. Some writing errors have been identified, such as in line 2 of Algorithm 1. Please review the entire manuscript to check.
4. The completeness of the manuscript still requires supplementation and refinement.

Limitations:
The approach of decomposing undirected edges into directed ones introduces additional decision elements, which complicates the problem. It's better that the authors can find a more efficient graph processing method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new learning-based constructive heuristic for capacitated arc routing problems. In contrast to node routing problems such as the TSP and VRP, arc routing problems received comparably little attentition. To address the specific
challenges in the capacitated arc routing problems, the authors propose a Neural Network-based approach that uses a graph attention
model considering arc directionality, a reinforcement learning approach with supervised pre-training and PPO-based fine-tuning. In
order to improve solutions obtained by an RL-based construction approach, they propose a beam search approach for path optimization which, after turning the set of routes into a giant tour, splits the tour into routes by adding returns to the depot. A set of experiments
shows that the proposed approach consistently yields better results than traditional hand-crafted constructive heuristics, and that their solutions almost match the quality of a time-consuming memetic algorithm that is only capable of solving small instance in a reaonable amount of time.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors propose one of the first learning-based approaches for the capacitated arc routing problem (CARP). Their approach, in particular their graph embedding, explicitly addresses one of the challenges of learning-based construction algorithms for this problem by explicitly replacing the undirected edges by directed arcs. This idea is original and turns out to be helpful to create a well-performing heuristic. 

The online performance of the approach surpasses hand-crafted constructive heuristics both in terms of runtime and efficiency. While for small instances, other metaheuristic approaches are better, it can be assumed that for large-scale instances, the proposed approach surpasses the state-of-the art of heuristic approaches. This is a significant result, since for node routing problems such as the CVRP, researcher have been struggling for years to design learning-based heuristics that achieve a performace that is comparable to hand-crafted heuristics. It should be mentioned, though, that in general, arc routing problems receive much less attention than node routing problems in the literature.

The paper comprises several insightful and, as far as I can tell, reasonably designed experiments, in particular showing the generalization capability to larger instances. 

The paper provides both code and instances.

Weaknesses:
The presentation of the CARP routing problem, the solutions approaches and related work lacks clarity in many places.
As an example, in the abstract, we find that the CARP consists in finding ""the minimum-cost tour""hat covers all required edges on a graph, while within capacity constraints"". This is a a bit misleading description since we look for a set of routes instead of a tour.

The paper distinguishes ""heuristics"" and ""metaheuristics"", while clearly metaheuristics are a type of heuristics. Actually, what the authors appear to have in mind is ""constructive heuristics"" which sequentially construct a solution by adding edges to form routes. I suggest to formulate more precisely here.

Similarly, it would enhance the understanding of the paper to introduce the notion of ""route-first, cluster second"" and the related
notion of a ""giant tour"" which is commonplace in routing applications, to characterize respective existing work. It would even facilitate the
presented path optimization which actually turns the presented approach into a route-first, cluster second approach. 


The computational results are convincing, but the discussion should emphasize that a fair comparison can only be made between their
approach wihout path optimization and the other constructive heuristics. It would indeed be interesting to see how the far the path
optimization is able to improve the results of the other constructive heuristics.

The claims ""NN-based approaches tend to lag behind advanced metaheuristics"" (abstract) and ""NN-based methods usually lags far
behind the traditional ones in solving CARP"", ""they still lag significantly behind traditional methods"" are not valid. Actually, (Rahmamoorty et. al 2024) (reference 20) report that on average, they improve upon the memetic algorithm by 11% on average.

When it comes to the evaluation of the path scanning approaches in the experiments, it is unclear how they are parameterized. From reading the paper (Aarakaki 2019) one sees that the parameter alpha and the number of iteration have a considerable impact both on solution time and solution quality, and (albeit on different instances), the average gaps for the path scanning approaches to the optimal (and to the memetic algrithm) reported in (Aarakaki 2019) are smaller than those found in the submission.

The description of the path improvement is not very clear; in particular the definition of the state used in the Dynamic Programming
algorithm. Is it a path? Is it the length of a path? Also, the statement ""f(*) denotes a state featuring dynamic programming"" is hard
to decipher.

Training time is not discussed at all.

Limitations:
Limitations are mostly addressed in a reasonable way. I suggest to add the following aspects:

I think that for small instances, the approach by (Rahmamoorty et. al 2024) may surpass the results reported here, which should be mentioned. 

Also, you should at least briefly mention the training time, since this makes it easier to assess the trade-off between offline effort and online performace.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
7g8WSOHJtP;"REVIEW 
Summary:
1. The paper unifies existing heterophilous graph neural networks (HTGNNs) into a Heterophilous Message-Passing (HTMP) mechanism.
2. The authors reveal that the effectiveness of HTMP is due to increasing differences among node representations belonging to different classes.
3. Guided by this revelation, the paper then introduces Compatibility Matrix-aware Graph Neural Network (CMGNN) to further enhance HTGNNs.
4. The authors conduct fair evaluations and comparative analysis on multiple benchmark datasets, highlighting the superior performance of the HTMP mechanism and the proposed CMGNN method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The claims are supported empirically by a detailed comparison across multiple benchmark datasets.
2. The paper is well-written and clearly structured, with each section logically building on the previous ones.

Weaknesses:
1. Several research publications [1, 2, 3] have used compatible matrices to boost the effectiveness of GNNs on heterophilic graphs. In-depth qualitative and quantitative comparisons are missing from this submission. Including such analyses would significantly increase the importance of the contributions.
2. Some claims made in the paper, such as Observation 1 and Observation 2 in Section 4, would benefit from additional analysis. For instance, including theoretical analysis with formal notations would provide more rigorous support for these claims.
3. Existing survey articles have unified and categorised message passing on heterophilic graphs [4,5,6]. This submission should compare and position the proposed HTMP unification against these categorisations.
4. The experiments lack a comparison of training times with baseline models. Including an analysis of the tradeoff between accuracy and training time would greatly enhance the results.




References:
1. Simplifying Node Classification on Heterophilous Graphs with Compatible Label Propagation, In TMLR'22,
2. Explicit pairwise factorized graph neural network for semi-supervised node classification, In UAI'21,
3. Graph Neural Networks with Heterophily, In AAAI'21,
4. Graph Neural Networks for Graphs with Heterophily: A Survey,
5. Learning from Graphs with Heterophily: Progress and Future,
6. Heterophily and Graph Neural Networks: Past, Present and Future.



**Edit post Rebuttal:**
The authors have promised to include detailed comparisons in a future revised version. Since these details cannot be verified within the review period, I will lower my confidence from 4 to 3. However, given that other major concerns have been addressed, I will raise my rating by one point from 4 to 5.

Limitations:
The authors have provided some discussion on the limitations of their work (for instance, see section 7 on Page 9).

Potential negative societal impacts are not relevant to this study.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work revisits the message-passing mechanisms in existing HTGNNs and reformulates them into a unified heterophilous message-passing (HTMP) mechanism. Based on HTMP, the authors propose a new framework named CMGNN. Experiments on 10 datasets with 13 different baseline models demonstrate the effectiveness of the proposed framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This work proposes a unified heterophilous message-passing (HTMP) mechanism, which could be a guideline for further research on heterophilous GNN. 
2. Based on the HTMP mechanism, this work proposes a new framework named CMGNN, which is novel and has basic value.
3. The effectiveness of the HTMP mechanism and CMGNN framework is well supported by experiment results.

Weaknesses:
1. Paper presentation could be further improved. For example, the conception of ""good"" heterophily and ""bad"" homophily deserves further explanation. There are some spelling mistakes, such as ""heterophilious"" in line 12.
2. If space permits, I feel like moving experiments in Appendix C to the main body would be better for the introduction of *Observation 1*. 
3. It might be hard to follow as this paper has so many equations, especially those about CMGNN. So I suggest providing a flow chart or a pseudo algorithm for better understanding.
4. Conclusions in this paper are mainly based on experiment results. It would be better if corresponding theoretical analysis or proofs are provided.

Limitations:
As the authors mentioned, this work mainly focuses on semi-supervised settings, which could be further generalized.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to address the question of ""why does message passing remain effective on heterophilous graphs"" and proposes a unified framework called heterophilous message-passing (HTMP) mechanism. It extensively reviews the architecture of existing heterophilous GNNs under this framework. It then moves on to discuss the empirical observation that the success of message passing in existing heterophilous GNNs is attributed to their implicitly enhancement of the compatibility matrix among classes, and proposed a new GNN approach called CMGNN to further enhance the separability of the compatibility matrix for different classes in the message passing process. The paper includes an extensive empirical analysis involving 10 benchmark datasets and 13 well-established baseline GNNs, and show that the proposed CMGNN approach has the best overall performance against the baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The writing is clear and well-organized for most parts of the paper; 
- The paper gives an extensive survey of existing message-passing GNNs under the HTMP mechanism in Table 1 and Appendix A. 
- The experiments are well-thought and extensive: it addresses the drawbacks of the previous homophilous and heterophilous node classification benchmarks identified in previous works by using more recent benchmark datasets, and include 13 baselines for a comprehensive evaluation of the proposed method.
- The proposed approach, CMGNN, has the best overall performance against 13 baselines on 10 benchmark datasets.

Weaknesses:
- This work builds upon the findings of several previous works regarding the effective designs for GNNs under heterophily and when is heterophily challenging (or in other words, ""bad"") for GNNs. While the authors cited these works in some parts of the paper ([6,9,12,18] in References), I feel that **some of the observations in the paper overlapped with the findings in previous works, and their connections and differences are not clearly stated in the paper**. 
  - For example, Observation 1 seems to overlap with the previous observations made in [6] (""to ensure that the neighborhood patterns for nodes with different labels are distinguishable, the inter- class similarity should be low"") and [9] (""two key factors, low-degree nodes and complex compatibility matrices, deteriorate the distinguishability of the neighborhood label distributions when coupled with heterophily, thus making heterophily a unique challenge for GNNs in most cases""). 
  - Given this, I also think that the claim in the related work section (line 732-734) that ""these reviews ... not exploring the reason behind the effectiveness of message passing in heterophilous graphs"" is inaccurate, as this paper is in fact built upon these analyses regarding the effectiveness of message passing in heterophilous graphs. 

- Section 5 (method) is too condensed to present a clear picture of how the proposed Compatibility Matrix-Aware GNN (CMGNN) works for the readers. For example, it is unclear what ""topology structure"" that the authors are considering as ""additional available node features"", and the term in Eq. 7 is not well explained. The authors also didn't explain clearly in the main paper how is the ""soft pseudo labels"" being generated for the model. It will help with the understanding if the authors can include a figure showing the architecture of the proposed CMGNN model. I feel the ""method"" section is the most novel part in the paper and deserves more length in the paper. 

- It would be good to analyze the computational complexity and/or compare the empirical runtime of the model with the baselines. 

- As a minor point, the ""Norm"" term in Eq. 3 should be explained as ""L1 normalization for matrix row vectors"" to avoid the confusion that the normalization is done with the L1 norm for *matrix* (instead of for vectors).

Limitations:
The authors acknowledged the limitation that the proposed HTMP framework is only applicable to GNNs following the message-passing mechanism. One additional limitation is that the paper is mostly empirical and does not give theoretical underpinnings.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
1. The paper unifies existing heterophilous graph neural networks (HTGNNs) into a Heterophilous Message-Passing (HTMP) mechanism.
2. The authors reveal that the effectiveness of HTMP is due to increasing differences among node representations belonging to different classes.
3. Guided by this revelation, the paper then introduces Compatibility Matrix-aware Graph Neural Network (CMGNN) to further enhance HTGNNs.
4. The authors conduct fair evaluations and comparative analysis on multiple benchmark datasets, highlighting the superior performance of the HTMP mechanism and the proposed CMGNN method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The claims are supported empirically by a detailed comparison across multiple benchmark datasets.
2. The paper is well-written and clearly structured, with each section logically building on the previous ones.

Weaknesses:
1. Several research publications [1, 2, 3] have used compatible matrices to boost the effectiveness of GNNs on heterophilic graphs. In-depth qualitative and quantitative comparisons are missing from this submission. Including such analyses would significantly increase the importance of the contributions.
2. Some claims made in the paper, such as Observation 1 and Observation 2 in Section 4, would benefit from additional analysis. For instance, including theoretical analysis with formal notations would provide more rigorous support for these claims.
3. Existing survey articles have unified and categorised message passing on heterophilic graphs [4,5,6]. This submission should compare and position the proposed HTMP unification against these categorisations.
4. The experiments lack a comparison of training times with baseline models. Including an analysis of the tradeoff between accuracy and training time would greatly enhance the results.




References:
1. Simplifying Node Classification on Heterophilous Graphs with Compatible Label Propagation, In TMLR'22,
2. Explicit pairwise factorized graph neural network for semi-supervised node classification, In UAI'21,
3. Graph Neural Networks with Heterophily, In AAAI'21,
4. Graph Neural Networks for Graphs with Heterophily: A Survey,
5. Learning from Graphs with Heterophily: Progress and Future,
6. Heterophily and Graph Neural Networks: Past, Present and Future.



**Edit post Rebuttal:**
The authors have promised to include detailed comparisons in a future revised version. Since these details cannot be verified within the review period, I will lower my confidence from 4 to 3. However, given that other major concerns have been addressed, I will raise my rating by one point from 4 to 5.

Limitations:
The authors have provided some discussion on the limitations of their work (for instance, see section 7 on Page 9).

Potential negative societal impacts are not relevant to this study.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work revisits the message-passing mechanisms in existing HTGNNs and reformulates them into a unified heterophilous message-passing (HTMP) mechanism. Based on HTMP, the authors propose a new framework named CMGNN. Experiments on 10 datasets with 13 different baseline models demonstrate the effectiveness of the proposed framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This work proposes a unified heterophilous message-passing (HTMP) mechanism, which could be a guideline for further research on heterophilous GNN. 
2. Based on the HTMP mechanism, this work proposes a new framework named CMGNN, which is novel and has basic value.
3. The effectiveness of the HTMP mechanism and CMGNN framework is well supported by experiment results.

Weaknesses:
1. Paper presentation could be further improved. For example, the conception of ""good"" heterophily and ""bad"" homophily deserves further explanation. There are some spelling mistakes, such as ""heterophilious"" in line 12.
2. If space permits, I feel like moving experiments in Appendix C to the main body would be better for the introduction of *Observation 1*. 
3. It might be hard to follow as this paper has so many equations, especially those about CMGNN. So I suggest providing a flow chart or a pseudo algorithm for better understanding.
4. Conclusions in this paper are mainly based on experiment results. It would be better if corresponding theoretical analysis or proofs are provided.

Limitations:
As the authors mentioned, this work mainly focuses on semi-supervised settings, which could be further generalized.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to address the question of ""why does message passing remain effective on heterophilous graphs"" and proposes a unified framework called heterophilous message-passing (HTMP) mechanism. It extensively reviews the architecture of existing heterophilous GNNs under this framework. It then moves on to discuss the empirical observation that the success of message passing in existing heterophilous GNNs is attributed to their implicitly enhancement of the compatibility matrix among classes, and proposed a new GNN approach called CMGNN to further enhance the separability of the compatibility matrix for different classes in the message passing process. The paper includes an extensive empirical analysis involving 10 benchmark datasets and 13 well-established baseline GNNs, and show that the proposed CMGNN approach has the best overall performance against the baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The writing is clear and well-organized for most parts of the paper; 
- The paper gives an extensive survey of existing message-passing GNNs under the HTMP mechanism in Table 1 and Appendix A. 
- The experiments are well-thought and extensive: it addresses the drawbacks of the previous homophilous and heterophilous node classification benchmarks identified in previous works by using more recent benchmark datasets, and include 13 baselines for a comprehensive evaluation of the proposed method.
- The proposed approach, CMGNN, has the best overall performance against 13 baselines on 10 benchmark datasets.

Weaknesses:
- This work builds upon the findings of several previous works regarding the effective designs for GNNs under heterophily and when is heterophily challenging (or in other words, ""bad"") for GNNs. While the authors cited these works in some parts of the paper ([6,9,12,18] in References), I feel that **some of the observations in the paper overlapped with the findings in previous works, and their connections and differences are not clearly stated in the paper**. 
  - For example, Observation 1 seems to overlap with the previous observations made in [6] (""to ensure that the neighborhood patterns for nodes with different labels are distinguishable, the inter- class similarity should be low"") and [9] (""two key factors, low-degree nodes and complex compatibility matrices, deteriorate the distinguishability of the neighborhood label distributions when coupled with heterophily, thus making heterophily a unique challenge for GNNs in most cases""). 
  - Given this, I also think that the claim in the related work section (line 732-734) that ""these reviews ... not exploring the reason behind the effectiveness of message passing in heterophilous graphs"" is inaccurate, as this paper is in fact built upon these analyses regarding the effectiveness of message passing in heterophilous graphs. 

- Section 5 (method) is too condensed to present a clear picture of how the proposed Compatibility Matrix-Aware GNN (CMGNN) works for the readers. For example, it is unclear what ""topology structure"" that the authors are considering as ""additional available node features"", and the term in Eq. 7 is not well explained. The authors also didn't explain clearly in the main paper how is the ""soft pseudo labels"" being generated for the model. It will help with the understanding if the authors can include a figure showing the architecture of the proposed CMGNN model. I feel the ""method"" section is the most novel part in the paper and deserves more length in the paper. 

- It would be good to analyze the computational complexity and/or compare the empirical runtime of the model with the baselines. 

- As a minor point, the ""Norm"" term in Eq. 3 should be explained as ""L1 normalization for matrix row vectors"" to avoid the confusion that the normalization is done with the L1 norm for *matrix* (instead of for vectors).

Limitations:
The authors acknowledged the limitation that the proposed HTMP framework is only applicable to GNNs following the message-passing mechanism. One additional limitation is that the paper is mostly empirical and does not give theoretical underpinnings.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
7nbAots3f8;"REVIEW 
Summary:
This paper introduces a new method to learning unsigned distance functions. Specifically, the method leverages local shape priors which brings geometry priors and is also able to handle noises and outliers. The results demonstrate that the proposed method outperforms previous baselines, especially in the corrupted situations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The idea of learning local shape functions is widely explored in the SDF or occupancy based methods, but is not yet introduced to the field of UDF-based reconstruction. This paper combines the strengths of local functions and UDF which can represent shapes with arbitrary typologies.
2. The paper is overall easy to follow.
3. The comparison are comprehensive by introducing recent works as the baselines. 
4. The method can better handle the noises and outliers compared to the previous prior-free methods.

Weaknesses:
1. The idea is straightforward to learn local shape functions as the prior for global shape reconstruction. Similar ideas are proposed in SDF-based methods. I would like to see more insight in the differences to the previous methods and also the importance of introducing the local geometric priors for UDF learning. 
Local Implicit Grid Representations for 3D Scenes (CVPR 2020)
Surface Reconstruction from Point Clouds by Learning Predictive Context Priors (CVPR 2022)
Deep local shapes: Learning local sdf priors for detailed 3d reconstruction (ECCV 2020)
2. The visualization is not quite convincing in the geometry details. For example, the reconstructions of real-scans in Fig.6 seems worse than the results of other methods in their papers. The local shape functions is expected to produce reconstruction with more details and sharper edges, but the scene reconstructions lack details. 
3. Also, some reconstructions are too fat, which in my opinion, is caused by the inaccurate UDF near the zero-level set, since the method use DCUDF for UDF meshing.

Limitations:
Please refer to the strengths and weaknesses above.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an approach to reconstruct 3D surfaces from point clouds, using unsigned distance fields. The proposed approach consists in training a specific neural network architecture to predict UDF values from local point cloud patches, which can be triangulated using UDF meshing methods. The paper mathematically analyses the possible local patches that can appear, smooth and sharp, and trains the proposed architecture with them. Once trained, the neural network is queried for UDF values from a point cloud, and it computes them by extracting a local point cloud patch for each query point and applying the information learned during training. A denoising module is employed to reduce the impact of noise and outliers. Experiments are carried out on ShapeNet cars and DeepFashion3D, synthetically sampling point clouds from the dataset meshes and reconstructing surfaces using a number of baselines, an extracting meshes using DCUDF. Results show that the performance of the proposed method is not the strongest on clean data, but it is so in the presence of noise and/or outliers. Experiments on real-world point clouds are shown qualitatively.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
3: good

Strengths:
The analysis of the possible kinds of local surface patches is well thought out and significative, enabling the possibility to treat the problem of surface reconstruction locally instead of globally, with an extensive training set that covers virtually all possible cases. I believe this is a good contribution, and the main strength in the paper.
The performance of the method on noisy data, albeit the noise was introduced synthetically, is good; the performance with synthetic outliers looks impressive.
The writing quality is generally good, with some exceptions on clarity (see weaknesses).

Weaknesses:
I believe there are a few weaknesses in the paper, mostly regarding clarity, architecture validation and experiments. I list them in no particular order.
1) The paper proposes a complex architecture, consisting of two branches, a cross-attention module, fully connected layers and a denoising module. The latter is only qualitatively justified in Fig.8, but the rest of the architecture is not validated in any way. A (quantitative) experimental evaluation of why this architecture is suitable for the task would be needed, in my opinion, as well as the intuition that led the researchers to arrive to this final architecture.
2) The cross-attention mechanism is unclear to me: the two branches (points and vectors net) output a latent code. What is the meaning of a cross-attention module on a single input token (actually one as K-V, one as Q)?
3) In the experiments, a few details are not specified, for example the query grid resolution and the number of points used for the Chamfer distance.
4) A time evaluation is completely missing. The paper claims ""Our method is computationally efficient"" in the introduction, which is partially justified by the comparatively better training times and storage requirements with respect to GeoUDF, but there is no evaluation of the time required by the method to reconstruct a surface, compared to the other methods. Notice also that DCUDF is an extremely slow meshing algorithm, so the evaluation should be performed both with it and without it.
5) The paper claims ""superior performance in surface reconstruction from both synthetic point clouds and real scans, even in the presence of noise and outliers"". The experiments however show no superiority in the absence of noise and outliers. Moreover, there are no quantitative experiments on real scans, making it impossible to evaluate whether the noise robustness on synthetic noise also translates to real data. Additionally, the qualitative experiments shown on real data lack comparison with the baselines. Thus, in general, I find the experimental section incomplete and not fully convincing.

Limitations:
A limitation of the method on completing incomplete surfaces has been correctly addressed and disclosed. 
The limited performance on clean data is shown in the experiments, but not acknowledged in the claims in the introduction.
A quantitative assessment of the performance on real scans and of the time required to reconstruct the surface are missing, making it harder to fully evaluate the possible limitations of the method with respect to the claims.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel training strategy to learn Unsigned Distance Fields from local shapes. The idea is to train the model on a dataset of point cloud patches characterized by mathematical functions representing a continuum from smooth surfaces to sharp edges and corners. Although trained only on synthetic surfaces, it demonstrates a remarkable capability in predicting UDFs for a wide range of  surface types. The method is evaluated on “Car” category of ShapeNet dataset in addition to DeepFashion3D, and ScanNet datasets. Furthermore, the paper shows results  on scans from real range scan dataset , in addition to ablation studies highlighting the robustness of the method to noise and outliers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written and easy to follow.
- **Novelty**: While training on local patches is not new, the design of the local shapes and network architecture is novel and effective.
- **Performance**: The paper shows good generalization results while being only trained on local synthetic patches and more robustness to noise and outliers in the input pointcloud.

Weaknesses:
- **Inference time**: While the method is better in terms of data storage space, data preparation time and training time, no comparison regarding the  inference time is provided.
- **Patch radius**: The patch radius used at test time is a crucial hyper-parameter for the method. A discussion about how to set this parameter and how it depends on the point cloud density/size would strengthen the paper.

Limitations:
The authors adequately addressed the limitations of the method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method for open surface reconstruction from 3D point clouds. They train a network to predict unsigned distance functions (UDFs) from point cloud patches using only synthetic data of quadratic surfaces. Evaluation shows that the trained network generalizes well to other complex patterns and is more resilient to noise when reconstructing 3D surfaces from point clouds.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea of training a UDF regression network using only synthetic data of quadratic surfaces is quite intriguing. This approach allows for a controlled and systematic way to generate training data, which can be more consistent and free from the imperfections and variability found in real-world data. 

Using quadratic surfaces as a basis for synthetic data is quite interesting where the authors argue that quadratic surfaces can approximate various local geometries. I have some doubts about this but it is reasonable and novel to a certain extend.

Weaknesses:
I have three main concerns: potential biases with the synthetic training data, the evaluation scheme, and the applicability in practice due to the patch radius.


## Potential Biases with Synthetic Training Data: 

The use of primitive geometrical patches as training data for a UDF regressor might introduce biases. The observation that any local geometries can be approximated by quadratic surfaces is only valid at a very fine resolution, which requires dense point clouds to observe reliably. It is unclear how many patches have been synthesized and whether they provide a good approximation of universal geometrical primitives. Some analyses would be helpful here: for example, using the ShapeNet car dataset, cropping all local patches from each car, and finding the closest synthesized one to check for approximation errors. Are there any patterns with sufficiently high approximation errors? Can we perform these analyses at different resolutions and see how they correlate with surface reconstruction?

## Evaluation Scheme

The testing data is simulated to match the scenarios the method is designed for: the point cloud is quite dense, and artificial noise is added similarly to the training data. There is no quantitative evaluation for real-world scanned data. 

## How sensitive is the method to different patch radii?

The value of 0.018 is oddly specific. I suspect that with a larger value of r, the method will generate overly smooth surfaces (as shown in Figure 6 - right), and with a smaller value of r, it will generate holes due to the point cloud not being dense enough. Overall, there is an inherent issue with this trade-off that may not be resolvable with this approach. Detailed experiments varying the patch radius and analyzing the impact on reconstruction quality would be helpful.

## Additional Concerns

- Ablation studies on the network architecture are missing. I am unsure about the roles of the two branches and the cross-attention mechanism. There is a potential issue with the reliance on Point-Net for embedding computation. Is this network pre-trained on other datasets? If so, we should be careful with the claim of using only synthetic data, as pre-training on real data could influence the results.

- It also seems that the method could be quite slow. The authors should include a speed test to provide insights into the computational efficiency of the proposed approach. Evaluating the method's runtime on different hardware setups and for varying point cloud sizes would give a clearer picture of its practical applicability.

Limitations:
The authors acknowledged that the proposed method cannot handle incomplete point cloud. However, it is unclear what how resilient it is when dealing with this.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new method to learning unsigned distance functions. Specifically, the method leverages local shape priors which brings geometry priors and is also able to handle noises and outliers. The results demonstrate that the proposed method outperforms previous baselines, especially in the corrupted situations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The idea of learning local shape functions is widely explored in the SDF or occupancy based methods, but is not yet introduced to the field of UDF-based reconstruction. This paper combines the strengths of local functions and UDF which can represent shapes with arbitrary typologies.
2. The paper is overall easy to follow.
3. The comparison are comprehensive by introducing recent works as the baselines. 
4. The method can better handle the noises and outliers compared to the previous prior-free methods.

Weaknesses:
1. The idea is straightforward to learn local shape functions as the prior for global shape reconstruction. Similar ideas are proposed in SDF-based methods. I would like to see more insight in the differences to the previous methods and also the importance of introducing the local geometric priors for UDF learning. 
Local Implicit Grid Representations for 3D Scenes (CVPR 2020)
Surface Reconstruction from Point Clouds by Learning Predictive Context Priors (CVPR 2022)
Deep local shapes: Learning local sdf priors for detailed 3d reconstruction (ECCV 2020)
2. The visualization is not quite convincing in the geometry details. For example, the reconstructions of real-scans in Fig.6 seems worse than the results of other methods in their papers. The local shape functions is expected to produce reconstruction with more details and sharper edges, but the scene reconstructions lack details. 
3. Also, some reconstructions are too fat, which in my opinion, is caused by the inaccurate UDF near the zero-level set, since the method use DCUDF for UDF meshing.

Limitations:
Please refer to the strengths and weaknesses above.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an approach to reconstruct 3D surfaces from point clouds, using unsigned distance fields. The proposed approach consists in training a specific neural network architecture to predict UDF values from local point cloud patches, which can be triangulated using UDF meshing methods. The paper mathematically analyses the possible local patches that can appear, smooth and sharp, and trains the proposed architecture with them. Once trained, the neural network is queried for UDF values from a point cloud, and it computes them by extracting a local point cloud patch for each query point and applying the information learned during training. A denoising module is employed to reduce the impact of noise and outliers. Experiments are carried out on ShapeNet cars and DeepFashion3D, synthetically sampling point clouds from the dataset meshes and reconstructing surfaces using a number of baselines, an extracting meshes using DCUDF. Results show that the performance of the proposed method is not the strongest on clean data, but it is so in the presence of noise and/or outliers. Experiments on real-world point clouds are shown qualitatively.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
3: good

Strengths:
The analysis of the possible kinds of local surface patches is well thought out and significative, enabling the possibility to treat the problem of surface reconstruction locally instead of globally, with an extensive training set that covers virtually all possible cases. I believe this is a good contribution, and the main strength in the paper.
The performance of the method on noisy data, albeit the noise was introduced synthetically, is good; the performance with synthetic outliers looks impressive.
The writing quality is generally good, with some exceptions on clarity (see weaknesses).

Weaknesses:
I believe there are a few weaknesses in the paper, mostly regarding clarity, architecture validation and experiments. I list them in no particular order.
1) The paper proposes a complex architecture, consisting of two branches, a cross-attention module, fully connected layers and a denoising module. The latter is only qualitatively justified in Fig.8, but the rest of the architecture is not validated in any way. A (quantitative) experimental evaluation of why this architecture is suitable for the task would be needed, in my opinion, as well as the intuition that led the researchers to arrive to this final architecture.
2) The cross-attention mechanism is unclear to me: the two branches (points and vectors net) output a latent code. What is the meaning of a cross-attention module on a single input token (actually one as K-V, one as Q)?
3) In the experiments, a few details are not specified, for example the query grid resolution and the number of points used for the Chamfer distance.
4) A time evaluation is completely missing. The paper claims ""Our method is computationally efficient"" in the introduction, which is partially justified by the comparatively better training times and storage requirements with respect to GeoUDF, but there is no evaluation of the time required by the method to reconstruct a surface, compared to the other methods. Notice also that DCUDF is an extremely slow meshing algorithm, so the evaluation should be performed both with it and without it.
5) The paper claims ""superior performance in surface reconstruction from both synthetic point clouds and real scans, even in the presence of noise and outliers"". The experiments however show no superiority in the absence of noise and outliers. Moreover, there are no quantitative experiments on real scans, making it impossible to evaluate whether the noise robustness on synthetic noise also translates to real data. Additionally, the qualitative experiments shown on real data lack comparison with the baselines. Thus, in general, I find the experimental section incomplete and not fully convincing.

Limitations:
A limitation of the method on completing incomplete surfaces has been correctly addressed and disclosed. 
The limited performance on clean data is shown in the experiments, but not acknowledged in the claims in the introduction.
A quantitative assessment of the performance on real scans and of the time required to reconstruct the surface are missing, making it harder to fully evaluate the possible limitations of the method with respect to the claims.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel training strategy to learn Unsigned Distance Fields from local shapes. The idea is to train the model on a dataset of point cloud patches characterized by mathematical functions representing a continuum from smooth surfaces to sharp edges and corners. Although trained only on synthetic surfaces, it demonstrates a remarkable capability in predicting UDFs for a wide range of  surface types. The method is evaluated on “Car” category of ShapeNet dataset in addition to DeepFashion3D, and ScanNet datasets. Furthermore, the paper shows results  on scans from real range scan dataset , in addition to ablation studies highlighting the robustness of the method to noise and outliers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written and easy to follow.
- **Novelty**: While training on local patches is not new, the design of the local shapes and network architecture is novel and effective.
- **Performance**: The paper shows good generalization results while being only trained on local synthetic patches and more robustness to noise and outliers in the input pointcloud.

Weaknesses:
- **Inference time**: While the method is better in terms of data storage space, data preparation time and training time, no comparison regarding the  inference time is provided.
- **Patch radius**: The patch radius used at test time is a crucial hyper-parameter for the method. A discussion about how to set this parameter and how it depends on the point cloud density/size would strengthen the paper.

Limitations:
The authors adequately addressed the limitations of the method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method for open surface reconstruction from 3D point clouds. They train a network to predict unsigned distance functions (UDFs) from point cloud patches using only synthetic data of quadratic surfaces. Evaluation shows that the trained network generalizes well to other complex patterns and is more resilient to noise when reconstructing 3D surfaces from point clouds.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea of training a UDF regression network using only synthetic data of quadratic surfaces is quite intriguing. This approach allows for a controlled and systematic way to generate training data, which can be more consistent and free from the imperfections and variability found in real-world data. 

Using quadratic surfaces as a basis for synthetic data is quite interesting where the authors argue that quadratic surfaces can approximate various local geometries. I have some doubts about this but it is reasonable and novel to a certain extend.

Weaknesses:
I have three main concerns: potential biases with the synthetic training data, the evaluation scheme, and the applicability in practice due to the patch radius.


## Potential Biases with Synthetic Training Data: 

The use of primitive geometrical patches as training data for a UDF regressor might introduce biases. The observation that any local geometries can be approximated by quadratic surfaces is only valid at a very fine resolution, which requires dense point clouds to observe reliably. It is unclear how many patches have been synthesized and whether they provide a good approximation of universal geometrical primitives. Some analyses would be helpful here: for example, using the ShapeNet car dataset, cropping all local patches from each car, and finding the closest synthesized one to check for approximation errors. Are there any patterns with sufficiently high approximation errors? Can we perform these analyses at different resolutions and see how they correlate with surface reconstruction?

## Evaluation Scheme

The testing data is simulated to match the scenarios the method is designed for: the point cloud is quite dense, and artificial noise is added similarly to the training data. There is no quantitative evaluation for real-world scanned data. 

## How sensitive is the method to different patch radii?

The value of 0.018 is oddly specific. I suspect that with a larger value of r, the method will generate overly smooth surfaces (as shown in Figure 6 - right), and with a smaller value of r, it will generate holes due to the point cloud not being dense enough. Overall, there is an inherent issue with this trade-off that may not be resolvable with this approach. Detailed experiments varying the patch radius and analyzing the impact on reconstruction quality would be helpful.

## Additional Concerns

- Ablation studies on the network architecture are missing. I am unsure about the roles of the two branches and the cross-attention mechanism. There is a potential issue with the reliance on Point-Net for embedding computation. Is this network pre-trained on other datasets? If so, we should be careful with the claim of using only synthetic data, as pre-training on real data could influence the results.

- It also seems that the method could be quite slow. The authors should include a speed test to provide insights into the computational efficiency of the proposed approach. Evaluating the method's runtime on different hardware setups and for varying point cloud sizes would give a clearer picture of its practical applicability.

Limitations:
The authors acknowledged that the proposed method cannot handle incomplete point cloud. However, it is unclear what how resilient it is when dealing with this.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
7VNvM9SnRE;"REVIEW 
Summary:
This paper considers the Adaptive Optimal Assortment (AOA) problem a.k.a. Utility Maximization with Subset Choices. The goal of this problem is to find the optimal profit-maximizing subset of size up to m (Top-m-Objective) or its weighted variant (Wtd-Top-m-Objective). Given a selected subset, the feedback follows the Plackett-Luce (PL) choice model that returns an item from the subset or a ""no-choice"" option. The probability of choosing each item is proportional to their underlying score/utility values.

The paper proposes a new algorithm, AOA-RB, that is claimed to be practical, efficient, and optimal. Compared to previous works, this algorithm does not require sampling the same subset repeatedly nor assumes a strongest default item. Later, the authors extend this algorithm with adaptive pivots that further improves performance.

The theoretical analysis shows that AOA-RB obtains regret guarantees that build on a novel ""Rank-Breaking"" parameter estimation technique for the discrete choice model.

The performance of AOA-RB is further demonstrated in numerical experiments using synthetic datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Problem Statement
- Clear presentation and motivation. Easy-to-follow section.

Algorithm
- Clear strengths are the relaxation of previous assumptions, e.g., repeated sampling of the same subset or the assumption of a strong default item
- The algorithm is well-presented and easy-to-follow.
- The adaptive pivot extension of the AOA-RB is a clear improvement that provides significant improvements

Theoretical Analysis
- The new concentration lemmas in Section 3.2. are claimed to be novel by the authors.
- Regret guarantees are provided for both objectives. The main strength is Theorem 6 which analyses the regret of the adaptive pivot version of the algorithm and shows a regret bound that does not blow to $\infty$ in corner cases.

Experiments
- The numerical experiments section further demonstrates the performance improvement of AOA-RB over the state-of-the-art MNL-UCB algorithm. It highlights especially the benefits of the adaptive pivots.

Weaknesses:
Introduction, Related Works, and Contribution
- Certain claims are not supported, e.g., Line 21 ""Studies have shown that it is often easier..."" but it lacks citation which studies the authors refer to.
- I found some citations to be misplaced or non-supportive of the claims it is used for, e.g., [11] is used in Line 62 as a reference for Battling Bandits while it is a survey of dueling bandits. Similarly, citations [45, 46] are used for dueling bandits while they are only two examples from the literature. It would be great if authors could use consistent citations, e.g., surveys when they refer to broader literature and individual publications when specifics are important.
- Table 1 is provided for the comparison of regret guarantees but the authors do not describe it. It would be great if they could comment on the differences between the algorithms.

Problem Setting
- Limitations are not mentioned in the problem statement. For example, how restrictive is the Plackett-Luce model, and whether the approach could be extended to other models? I see that it is mentioned in Remark 1 but could be commented on in Section 2 as well.
- Both Top-m and Wtd-Top-m consider the (weighted) utility optimization problem. However, for most of the applications used as motivation, e.g., assortment optimization and recommender systems, the utility of the user which dictates the selected feedback, and the utility/profit of the subset selection (platform) are misaligned. Could the authors comment on how to formulate these problems in their setting?

Algorithm
- The $argmax_{S\subseteq [K], |S|\leq m}$ optimization is non-trivial and could be computationally expensive for large values for $K$.
- The authors claim that AOA-RB is practical, efficient, and optimal. While the theoretical analysis supports the last two claims, I struggle to find the intuition behind the algorithm. Could the authors elaborate further on this point?

Experiments
- Numerical experiments demonstrate performance only in synthetic data. Given the clear application and motivation of the paper, I would like to see experiments that reflect these problems.
- I recommend the authors to use larger figures. Axes and titles are hardly visible in the printed version.
- Only one baseline is considered. It would be appreciated if the authors could include the other algorithms mentioned in Table 1 for numerical comparison besides the theoretical one.

While the paper is easy to read and follow even for readers not familiar with all the works in the area, the inconsistent citations and unsupported claims have to be addressed before the paper would reach publication standards.

Limitations:
Limitations are mentioned in the paper, however, it is often not directly connected, e.g., the assumption of the PL model is only addressed in Remark 1. I would suggest the authors address limitations more clearly when they appear for easier readability.
The work is mainly theoretical without any immediate direct societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the online MNL assortment optimization problem, where the goal is to learn MNL parameters while suggesting assortments, with the goal of either learning the top-m highest utility items or learning the maximum revenue set with m items. They use a UCB-based approach on pairwise win rates to get a UCB for utilities, which can then be fed into a traditional assortment optimization algorithm. The authors show this approach achieves asymptotically optimal regret and does not require assumptions used by previous approached. The basic algorithm relies on comparisons between each item and the no-choice option, but they also introduce a more sophisticated adaptive pivot approach that works better when the no-choice option is rarely selected. In experiments on synthetic data, their assortment optimization approach performs significantly better than the previous state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem studied is natural and important. 
2. The presentation is generally clear.
3. The technical quality seems good, although I cannot attest to the correctness of all the proofs in the appendix.
4. The UCB approach on pairwise win rates is clever and appears original.

Weaknesses:
1. The algorithms and proofs could use some additional description/intuition. Some of the steps in the proofs take rather large leaps.

Limitations:
I think the limitations of the paper were adequately stated

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the problem of active online assortment optimization problem with preference feedback, which has been extensively studied. The paper argues that the previous studies have some unrealistic assumptions such as: there is a ‘strong reference’ which is always included in the choice sets; the same assortments can be repeatedly selected. Without these assumptions, they propose some efficient algorithms for the problem of regret minimization in assortment selection with Plackett Luce (PL) based user choices.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proves the regret bounds of the proposed online learning algorithms. The regret bounds are proved based on some concentration guarantee for estimating the score parameters of the PL model using ‘Pairwise Rank-Breaking’.

Weaknesses:
1.  I cannot fully understand the motivation of the paper. The paper says that two major drawbacks of the previous studies include: the existing algorithms assume that the ``no-choice’’ option is stronger than the other choices, and they may query the same set of items for multiple times. It seems that the focus of the paper is to address these drawbacks. However, I think that these ``drawbacks’’ may not be real. First, it is natural that most of the customers will not choose any product, so it is very reasonable to assume that no-choice option is stronger. Second, in the typical assortment optimization scenario where customers arrive online one by one, showing the same set of items to different customers for multiple times absolutely will not cause any problem.  So I think that addressing these ``drawbacks’’ has very limited value.
2.  The regret bounds proposed by the paper is actually K\sqrt{T}\log T. It seems that this regret bound is weaker than those of the previous studies such as [2] (at least by log factors on T). The authors may argue that their bounds are better when \theta_{max}\rightarrow \infty, but this depends on the assumptions made on specific application scenarios, which is questionable as explained in my last comment.
3.  The experiments are conducted using some specific values of \theta and hence are not very convincing. I think that more experiments on more applications are necessary to demonstrate the superiority of the paper.

Limitations:
see the above

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the problem of active assortment optimization in MNL model.

In the problem of assortment optimization, we have  a large universe of products i=1,2,\dots N, each of which generates a given revenue r_i for the seller.  In MNL model  each product i has a value \theta_i to the customers and when customers are offered a subset of products they choose each item (including the no-choice option) with a probability proportional to their value. We also assume there is a no-choice option with revenue 0. The seller’s objective is to identify the assortment of products which generates maximum expected revenue. 

In the active version of the problems, the values of items  \theta_1, \theta_2,\dots , \theta_N, are not known to the seller. Thus, the seller  shows a  subset of items from the universe to the customers at  rounds 1,2, \dots ,T  and estimates \theta_i s  based on the observations. After approximating these values, the seller may solve the problem in static setting and find the optimal assortment.  This strategy is known as exploration and exploitation. 

In active assortment optimization, the objective is to minimize the regret of the algorithm which is defined as the summation over rounds t=1,2,\dots T the difference of the expected revenue in each round from the optimal revenue.

Prior works for instance [2] provided an algorithm for this problem by estimating at each round a high probability upper bound for the values \theta_i,  and then solve the static problem using the upper-bounds. In [2] the authors assume that \theta_0 (the value assigned to no-choice option and thus its probability ) is the highest among all items. 

The submitted manuscript claims that they provide an algorithm with a similar regret bound to [2] which does not have the restriction of assuming the no-choice option has the highest value. Their suggested approach is similar to that of [2] (finding high probability upper bounds for the parameters) but it is hard to follow all details of obtaining the upper bound and how it removes the restriction imposed on the value of the no-choice option. 

The result, if true, is interesting but I found the paper hard to read and got lost in section 3.1. I think that the paper will benefit greatly from rewriting and improving the presentation. 

I will detail my confusions as follows: 

- In Equation (3) on line 173 there is a variable x which is not defined up to this point. I understand that x appears to bound the probability of error in Lemma 1. But you have to introduce it before you use it the first time. 
- Between line 176 and 177 what is the + sign on the denominator of the equation? you use this notation again in another equation between lines 252 and 253.
- In equation 3 you show an upper bound on \hat{p_ijt} which then turns to a bound on \theta_i s. But in Lemma 1 you have shown a different upper bound for \theta_i. Can you explain the connection of these two bounds. 

A few minor typos:

Lemma 1. atleast-> at least
^ucb is sometimes with roman font and sometimes normal font. 




[2] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5):1453–1485, 2019.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The problem of active assortment optimization is a fundamental problem in revenue management 
- The result is interesting if correct, as it removes an important restriction from prior algorithms.

Weaknesses:
- The results are poorly presented and it is hard to follow the paper. The paper lacks an explanation of main intuitions . 
- The technique seems to be similar to [2] as both papers obtain high probability upper bounds for the parameters and then solve it in an static setting. An intuitive explanation of how the given different upper bound is obtained, why it is correct, and how it removed the restriction on no-choice option is not provided.

Limitations:
limitations are not discussed but there are several future directions that have been discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the Adaptive Optimal Assortment (AOA) problem a.k.a. Utility Maximization with Subset Choices. The goal of this problem is to find the optimal profit-maximizing subset of size up to m (Top-m-Objective) or its weighted variant (Wtd-Top-m-Objective). Given a selected subset, the feedback follows the Plackett-Luce (PL) choice model that returns an item from the subset or a ""no-choice"" option. The probability of choosing each item is proportional to their underlying score/utility values.

The paper proposes a new algorithm, AOA-RB, that is claimed to be practical, efficient, and optimal. Compared to previous works, this algorithm does not require sampling the same subset repeatedly nor assumes a strongest default item. Later, the authors extend this algorithm with adaptive pivots that further improves performance.

The theoretical analysis shows that AOA-RB obtains regret guarantees that build on a novel ""Rank-Breaking"" parameter estimation technique for the discrete choice model.

The performance of AOA-RB is further demonstrated in numerical experiments using synthetic datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Problem Statement
- Clear presentation and motivation. Easy-to-follow section.

Algorithm
- Clear strengths are the relaxation of previous assumptions, e.g., repeated sampling of the same subset or the assumption of a strong default item
- The algorithm is well-presented and easy-to-follow.
- The adaptive pivot extension of the AOA-RB is a clear improvement that provides significant improvements

Theoretical Analysis
- The new concentration lemmas in Section 3.2. are claimed to be novel by the authors.
- Regret guarantees are provided for both objectives. The main strength is Theorem 6 which analyses the regret of the adaptive pivot version of the algorithm and shows a regret bound that does not blow to $\infty$ in corner cases.

Experiments
- The numerical experiments section further demonstrates the performance improvement of AOA-RB over the state-of-the-art MNL-UCB algorithm. It highlights especially the benefits of the adaptive pivots.

Weaknesses:
Introduction, Related Works, and Contribution
- Certain claims are not supported, e.g., Line 21 ""Studies have shown that it is often easier..."" but it lacks citation which studies the authors refer to.
- I found some citations to be misplaced or non-supportive of the claims it is used for, e.g., [11] is used in Line 62 as a reference for Battling Bandits while it is a survey of dueling bandits. Similarly, citations [45, 46] are used for dueling bandits while they are only two examples from the literature. It would be great if authors could use consistent citations, e.g., surveys when they refer to broader literature and individual publications when specifics are important.
- Table 1 is provided for the comparison of regret guarantees but the authors do not describe it. It would be great if they could comment on the differences between the algorithms.

Problem Setting
- Limitations are not mentioned in the problem statement. For example, how restrictive is the Plackett-Luce model, and whether the approach could be extended to other models? I see that it is mentioned in Remark 1 but could be commented on in Section 2 as well.
- Both Top-m and Wtd-Top-m consider the (weighted) utility optimization problem. However, for most of the applications used as motivation, e.g., assortment optimization and recommender systems, the utility of the user which dictates the selected feedback, and the utility/profit of the subset selection (platform) are misaligned. Could the authors comment on how to formulate these problems in their setting?

Algorithm
- The $argmax_{S\subseteq [K], |S|\leq m}$ optimization is non-trivial and could be computationally expensive for large values for $K$.
- The authors claim that AOA-RB is practical, efficient, and optimal. While the theoretical analysis supports the last two claims, I struggle to find the intuition behind the algorithm. Could the authors elaborate further on this point?

Experiments
- Numerical experiments demonstrate performance only in synthetic data. Given the clear application and motivation of the paper, I would like to see experiments that reflect these problems.
- I recommend the authors to use larger figures. Axes and titles are hardly visible in the printed version.
- Only one baseline is considered. It would be appreciated if the authors could include the other algorithms mentioned in Table 1 for numerical comparison besides the theoretical one.

While the paper is easy to read and follow even for readers not familiar with all the works in the area, the inconsistent citations and unsupported claims have to be addressed before the paper would reach publication standards.

Limitations:
Limitations are mentioned in the paper, however, it is often not directly connected, e.g., the assumption of the PL model is only addressed in Remark 1. I would suggest the authors address limitations more clearly when they appear for easier readability.
The work is mainly theoretical without any immediate direct societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the online MNL assortment optimization problem, where the goal is to learn MNL parameters while suggesting assortments, with the goal of either learning the top-m highest utility items or learning the maximum revenue set with m items. They use a UCB-based approach on pairwise win rates to get a UCB for utilities, which can then be fed into a traditional assortment optimization algorithm. The authors show this approach achieves asymptotically optimal regret and does not require assumptions used by previous approached. The basic algorithm relies on comparisons between each item and the no-choice option, but they also introduce a more sophisticated adaptive pivot approach that works better when the no-choice option is rarely selected. In experiments on synthetic data, their assortment optimization approach performs significantly better than the previous state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem studied is natural and important. 
2. The presentation is generally clear.
3. The technical quality seems good, although I cannot attest to the correctness of all the proofs in the appendix.
4. The UCB approach on pairwise win rates is clever and appears original.

Weaknesses:
1. The algorithms and proofs could use some additional description/intuition. Some of the steps in the proofs take rather large leaps.

Limitations:
I think the limitations of the paper were adequately stated

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the problem of active online assortment optimization problem with preference feedback, which has been extensively studied. The paper argues that the previous studies have some unrealistic assumptions such as: there is a ‘strong reference’ which is always included in the choice sets; the same assortments can be repeatedly selected. Without these assumptions, they propose some efficient algorithms for the problem of regret minimization in assortment selection with Plackett Luce (PL) based user choices.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proves the regret bounds of the proposed online learning algorithms. The regret bounds are proved based on some concentration guarantee for estimating the score parameters of the PL model using ‘Pairwise Rank-Breaking’.

Weaknesses:
1.  I cannot fully understand the motivation of the paper. The paper says that two major drawbacks of the previous studies include: the existing algorithms assume that the ``no-choice’’ option is stronger than the other choices, and they may query the same set of items for multiple times. It seems that the focus of the paper is to address these drawbacks. However, I think that these ``drawbacks’’ may not be real. First, it is natural that most of the customers will not choose any product, so it is very reasonable to assume that no-choice option is stronger. Second, in the typical assortment optimization scenario where customers arrive online one by one, showing the same set of items to different customers for multiple times absolutely will not cause any problem.  So I think that addressing these ``drawbacks’’ has very limited value.
2.  The regret bounds proposed by the paper is actually K\sqrt{T}\log T. It seems that this regret bound is weaker than those of the previous studies such as [2] (at least by log factors on T). The authors may argue that their bounds are better when \theta_{max}\rightarrow \infty, but this depends on the assumptions made on specific application scenarios, which is questionable as explained in my last comment.
3.  The experiments are conducted using some specific values of \theta and hence are not very convincing. I think that more experiments on more applications are necessary to demonstrate the superiority of the paper.

Limitations:
see the above

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the problem of active assortment optimization in MNL model.

In the problem of assortment optimization, we have  a large universe of products i=1,2,\dots N, each of which generates a given revenue r_i for the seller.  In MNL model  each product i has a value \theta_i to the customers and when customers are offered a subset of products they choose each item (including the no-choice option) with a probability proportional to their value. We also assume there is a no-choice option with revenue 0. The seller’s objective is to identify the assortment of products which generates maximum expected revenue. 

In the active version of the problems, the values of items  \theta_1, \theta_2,\dots , \theta_N, are not known to the seller. Thus, the seller  shows a  subset of items from the universe to the customers at  rounds 1,2, \dots ,T  and estimates \theta_i s  based on the observations. After approximating these values, the seller may solve the problem in static setting and find the optimal assortment.  This strategy is known as exploration and exploitation. 

In active assortment optimization, the objective is to minimize the regret of the algorithm which is defined as the summation over rounds t=1,2,\dots T the difference of the expected revenue in each round from the optimal revenue.

Prior works for instance [2] provided an algorithm for this problem by estimating at each round a high probability upper bound for the values \theta_i,  and then solve the static problem using the upper-bounds. In [2] the authors assume that \theta_0 (the value assigned to no-choice option and thus its probability ) is the highest among all items. 

The submitted manuscript claims that they provide an algorithm with a similar regret bound to [2] which does not have the restriction of assuming the no-choice option has the highest value. Their suggested approach is similar to that of [2] (finding high probability upper bounds for the parameters) but it is hard to follow all details of obtaining the upper bound and how it removes the restriction imposed on the value of the no-choice option. 

The result, if true, is interesting but I found the paper hard to read and got lost in section 3.1. I think that the paper will benefit greatly from rewriting and improving the presentation. 

I will detail my confusions as follows: 

- In Equation (3) on line 173 there is a variable x which is not defined up to this point. I understand that x appears to bound the probability of error in Lemma 1. But you have to introduce it before you use it the first time. 
- Between line 176 and 177 what is the + sign on the denominator of the equation? you use this notation again in another equation between lines 252 and 253.
- In equation 3 you show an upper bound on \hat{p_ijt} which then turns to a bound on \theta_i s. But in Lemma 1 you have shown a different upper bound for \theta_i. Can you explain the connection of these two bounds. 

A few minor typos:

Lemma 1. atleast-> at least
^ucb is sometimes with roman font and sometimes normal font. 




[2] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5):1453–1485, 2019.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The problem of active assortment optimization is a fundamental problem in revenue management 
- The result is interesting if correct, as it removes an important restriction from prior algorithms.

Weaknesses:
- The results are poorly presented and it is hard to follow the paper. The paper lacks an explanation of main intuitions . 
- The technique seems to be similar to [2] as both papers obtain high probability upper bounds for the parameters and then solve it in an static setting. An intuitive explanation of how the given different upper bound is obtained, why it is correct, and how it removed the restriction on no-choice option is not provided.

Limitations:
limitations are not discussed but there are several future directions that have been discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
7Su7gAei1l;"REVIEW 
Summary:
This paper introduces SymmetricDiffusers, a new approach to learning complex distributions. It works by breaking down the problem into simpler steps: learning how to reverse a transformation using deep neural networks. The authors identify a particularly effective method for this reversal step (the riffle shuffle) and provide guidance on choosing the right length for the process based on mathematical properties. Additionally, they propose a more powerful alternative to a common distribution (the generalized Plackett-Luce distribution) and a theoretically sound strategy for improving efficiency (the denoising schedule). Experiments show that SymmetricDiffusers performs extremely well on various tasks, including sorting images, solving puzzles, and optimizing routes.

Soundness:
1: poor

Presentation:
3: good

Contribution:
1: poor

Strengths:
The proposed method in general is interesting and seems effective in the examples studied in this paper.

Also, the problem studied is interesting.

Weaknesses:
My main concern is the proposed method is not the state-of-the-art. A clear approach is to learn invariant features and equivariant group actions using some existing methods like that proposed in Robin Winter, et al. Unsupervised Learning of Group Invariant and
Equivariant Representations, NeurIPS 2022. Such an approach. 

In Robin Winter et al.'s paper, the authors have studied the symmetric group and my understanding is disentangling invariant features and equivariant groups enables the design of flexible diffusion models since you only need to perform diffusion modeling in the invariant latent space. 

I want to see the comparison of the approach proposed in Robin Winter et al's paper. My understanding is that Robin Winter's approach enables building state-of-the-art diffusion models for molecular generation.

Limitations:
My main concern is the proposed method is not the state-of-the-art. A clear approach is to learn invariant features and equivariant group actions using some existing methods like that proposed in Robin Winter, et al. Unsupervised Learning of Group Invariant and
Equivariant Representations, NeurIPS 2022. Such an approach. 

In Robin Winter et al.'s paper, the authors have studied the symmetric group and my understanding is disentangling invariant features and equivariant groups enables the design of flexible diffusion models since you only need to perform diffusion modeling in the invariant latent space. 

I want to see the comparison of the approach proposed in Robin Winter et al's paper. My understanding is that Robin Winter's approach enables building state-of-the-art diffusion models for molecular generation.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
I am unable to review this paper as it lies outside my area of expertise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I am unable to review this paper as it lies outside my area of expertise.

Weaknesses:
I am unable to review this paper as it lies outside my area of expertise.

Limitations:
I am unable to review this paper as it lies outside my area of expertise.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors aim to create a discrete diffusion model that generates permutations. This model can then be used to solve combinatorial problems including jigsaws and travelling salesman problems. To formulate their model they cover a range of forward shuffling strategies and discuss how to parametrize the reverse transition. During sampling, they also use beam search to find high probability samples. They find their method performs competitively on computational experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea of using riffle shuffles to create a corruption process over permutations and then parametrizing the time reversal of this process is novel and interesting. I enjoyed reading the paper. I believe further work will build on this as there are many instances in machine learning where needing to learn permutations crops up.

The paper is quite well written and easy to understand. It is not overloaded with mathematical equations and intuition is given for some concepts.

The experimental results seem promising as it performs on par or better (especially in high dimensions) than previous methods for learning permutations. I appreciate the ablation studies into the greedy search vs beam search and types of shuffle (in the appendix).

Weaknesses:
I think some further clarification is required for the weaknesses of the random transposition and random insertion style of card shuffling. You later say that you can merge steps of the forward process if each individual step does not induce enough mixing and so the stated weakness that these styles of shuffle have slow mixing seems moot.

You mention that you do not have access to $q(X_t | X_0)$, and I think it should also be discussed that $q(X_{t-1} | X_t, X_0)$ is also unavailable since this distribution is used in standard diffusion models to re-write the variational bound in a lower variance form, see Appendix A in https://arxiv.org/pdf/2006.11239 .

You dedicate a lot of space to discussing the various forward noising processes with different shuffling methods, which is quite interesting. However, the ablations with these different styles of shuffle are in the appendix and I think it should be in the main since they have been given such prominence earlier on in the discussion of the method, it is strange they are not included in the main experiments.

I find it difficult to follow the description of the inverse transposition parametrization, there is no intuition given for the functions $\phi$ and $\psi$ nor the functional form of $p_{IT}(\sigma)$. Perhaps this is due to space limitations but since inverse transposition is not in the main experiments (see above point), I think you should either relegate a lot of this to the appendix if you only use the riffle shuffle in practice, or try and shift the wording to properly explain these types of forward and inverse process and have experiments for them in the main.

Limitations:
The authors do a good job of discussing the limitations of various parametrizations of their method.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a discrete diffusion model to learn distribution over the finite symmetric group $S_n$. The forward process is built off of random walks on finite groups (in this case, card shuffles), and the paper learns to reverse this diffusion process with standard discrete diffusion arguments.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, I really like the paper's contributions and presentation.

* The idea is a neat application of the discrete diffusion ideas to an important area. In particular, the structure of $S_n$ is sufficiently different from standard image/text datasets as to necessitate this paper.

* The presentation is very good and the contributions are numerous.

* For the experiments listed, the method seems to provide a very strong improvement over baseline methods. In particular, these other methods are based on fundamentally different technology, so this highlights that discrete diffusion can become a very promising direction here.

Weaknesses:
There are three primary weaknesses. These should all be addressable to some degree, and I'll take any response into consideration when recalibrating my final score.

1. The model proposes to directly learn the reverse transition densities $p_\theta(X_{t - 1}, X_t)$. The issue with doing this for standard diffusion models is that this seems to hurt model training since it increases the variance of training (as, in particular, one must sample the two $X_{t - 1}, X_t$ for training instead of just one $X_t$). As such, most works use the (ultimately equivalent) mean/score-parameterizations [1, 2]. I would want to hear a bit more about if this would be applicable in the $S_n$ case (and training with this parameterization might improve the model) or if this is not possible.

2. (Related to the above). Since most modern discrete diffusion methods are formulated in continuous time, I think the paper would benefit greatly with a discussion about potentially extending the current methods to this realm. In particular, works like [3, 4, 5] have established a working theory for discrete diffusion in continuous time, so it would be beneficial to discuss how the proposed framework might fit into the established theory.

3. The experiments, while showing good results, do not show that the method is particularly scalable, which seems to be a fundamental problem in prior work that was explicitly mentioned in this paper. In particular, it seems that the maximum value of $n$ in $S_n$ is 100. While some discussion is made here that talks about transformer layers, I think large values of $n$ aren't that big of an issue in transformers due to systems like Flash Attention. So, it should be made more clear if this is a fundamental problem with the existing method, or a larger scale example (even toy) should be presented.

[1] https://arxiv.org/abs/2006.11239

[2] https://arxiv.org/abs/2011.13456

[3] https://arxiv.org/abs/2205.14987

[4] https://arxiv.org/abs/2211.16750

[5] https://arxiv.org/abs/2310.16834

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces SymmetricDiffusers, a new approach to learning complex distributions. It works by breaking down the problem into simpler steps: learning how to reverse a transformation using deep neural networks. The authors identify a particularly effective method for this reversal step (the riffle shuffle) and provide guidance on choosing the right length for the process based on mathematical properties. Additionally, they propose a more powerful alternative to a common distribution (the generalized Plackett-Luce distribution) and a theoretically sound strategy for improving efficiency (the denoising schedule). Experiments show that SymmetricDiffusers performs extremely well on various tasks, including sorting images, solving puzzles, and optimizing routes.

Soundness:
1: poor

Presentation:
3: good

Contribution:
1: poor

Strengths:
The proposed method in general is interesting and seems effective in the examples studied in this paper.

Also, the problem studied is interesting.

Weaknesses:
My main concern is the proposed method is not the state-of-the-art. A clear approach is to learn invariant features and equivariant group actions using some existing methods like that proposed in Robin Winter, et al. Unsupervised Learning of Group Invariant and
Equivariant Representations, NeurIPS 2022. Such an approach. 

In Robin Winter et al.'s paper, the authors have studied the symmetric group and my understanding is disentangling invariant features and equivariant groups enables the design of flexible diffusion models since you only need to perform diffusion modeling in the invariant latent space. 

I want to see the comparison of the approach proposed in Robin Winter et al's paper. My understanding is that Robin Winter's approach enables building state-of-the-art diffusion models for molecular generation.

Limitations:
My main concern is the proposed method is not the state-of-the-art. A clear approach is to learn invariant features and equivariant group actions using some existing methods like that proposed in Robin Winter, et al. Unsupervised Learning of Group Invariant and
Equivariant Representations, NeurIPS 2022. Such an approach. 

In Robin Winter et al.'s paper, the authors have studied the symmetric group and my understanding is disentangling invariant features and equivariant groups enables the design of flexible diffusion models since you only need to perform diffusion modeling in the invariant latent space. 

I want to see the comparison of the approach proposed in Robin Winter et al's paper. My understanding is that Robin Winter's approach enables building state-of-the-art diffusion models for molecular generation.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
I am unable to review this paper as it lies outside my area of expertise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I am unable to review this paper as it lies outside my area of expertise.

Weaknesses:
I am unable to review this paper as it lies outside my area of expertise.

Limitations:
I am unable to review this paper as it lies outside my area of expertise.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors aim to create a discrete diffusion model that generates permutations. This model can then be used to solve combinatorial problems including jigsaws and travelling salesman problems. To formulate their model they cover a range of forward shuffling strategies and discuss how to parametrize the reverse transition. During sampling, they also use beam search to find high probability samples. They find their method performs competitively on computational experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea of using riffle shuffles to create a corruption process over permutations and then parametrizing the time reversal of this process is novel and interesting. I enjoyed reading the paper. I believe further work will build on this as there are many instances in machine learning where needing to learn permutations crops up.

The paper is quite well written and easy to understand. It is not overloaded with mathematical equations and intuition is given for some concepts.

The experimental results seem promising as it performs on par or better (especially in high dimensions) than previous methods for learning permutations. I appreciate the ablation studies into the greedy search vs beam search and types of shuffle (in the appendix).

Weaknesses:
I think some further clarification is required for the weaknesses of the random transposition and random insertion style of card shuffling. You later say that you can merge steps of the forward process if each individual step does not induce enough mixing and so the stated weakness that these styles of shuffle have slow mixing seems moot.

You mention that you do not have access to $q(X_t | X_0)$, and I think it should also be discussed that $q(X_{t-1} | X_t, X_0)$ is also unavailable since this distribution is used in standard diffusion models to re-write the variational bound in a lower variance form, see Appendix A in https://arxiv.org/pdf/2006.11239 .

You dedicate a lot of space to discussing the various forward noising processes with different shuffling methods, which is quite interesting. However, the ablations with these different styles of shuffle are in the appendix and I think it should be in the main since they have been given such prominence earlier on in the discussion of the method, it is strange they are not included in the main experiments.

I find it difficult to follow the description of the inverse transposition parametrization, there is no intuition given for the functions $\phi$ and $\psi$ nor the functional form of $p_{IT}(\sigma)$. Perhaps this is due to space limitations but since inverse transposition is not in the main experiments (see above point), I think you should either relegate a lot of this to the appendix if you only use the riffle shuffle in practice, or try and shift the wording to properly explain these types of forward and inverse process and have experiments for them in the main.

Limitations:
The authors do a good job of discussing the limitations of various parametrizations of their method.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a discrete diffusion model to learn distribution over the finite symmetric group $S_n$. The forward process is built off of random walks on finite groups (in this case, card shuffles), and the paper learns to reverse this diffusion process with standard discrete diffusion arguments.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, I really like the paper's contributions and presentation.

* The idea is a neat application of the discrete diffusion ideas to an important area. In particular, the structure of $S_n$ is sufficiently different from standard image/text datasets as to necessitate this paper.

* The presentation is very good and the contributions are numerous.

* For the experiments listed, the method seems to provide a very strong improvement over baseline methods. In particular, these other methods are based on fundamentally different technology, so this highlights that discrete diffusion can become a very promising direction here.

Weaknesses:
There are three primary weaknesses. These should all be addressable to some degree, and I'll take any response into consideration when recalibrating my final score.

1. The model proposes to directly learn the reverse transition densities $p_\theta(X_{t - 1}, X_t)$. The issue with doing this for standard diffusion models is that this seems to hurt model training since it increases the variance of training (as, in particular, one must sample the two $X_{t - 1}, X_t$ for training instead of just one $X_t$). As such, most works use the (ultimately equivalent) mean/score-parameterizations [1, 2]. I would want to hear a bit more about if this would be applicable in the $S_n$ case (and training with this parameterization might improve the model) or if this is not possible.

2. (Related to the above). Since most modern discrete diffusion methods are formulated in continuous time, I think the paper would benefit greatly with a discussion about potentially extending the current methods to this realm. In particular, works like [3, 4, 5] have established a working theory for discrete diffusion in continuous time, so it would be beneficial to discuss how the proposed framework might fit into the established theory.

3. The experiments, while showing good results, do not show that the method is particularly scalable, which seems to be a fundamental problem in prior work that was explicitly mentioned in this paper. In particular, it seems that the maximum value of $n$ in $S_n$ is 100. While some discussion is made here that talks about transformer layers, I think large values of $n$ aren't that big of an issue in transformers due to systems like Flash Attention. So, it should be made more clear if this is a fundamental problem with the existing method, or a larger scale example (even toy) should be presented.

[1] https://arxiv.org/abs/2006.11239

[2] https://arxiv.org/abs/2011.13456

[3] https://arxiv.org/abs/2205.14987

[4] https://arxiv.org/abs/2211.16750

[5] https://arxiv.org/abs/2310.16834

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
6Wm4202Wvc;"REVIEW 
Summary:
The paper revisited the problem of label leakage in split learning in the context of fine-tuning large models with parameter-efficient training. Based on modern use cases, they proposed two privacy-preserving protections for gradients and activations during split learning. The proposed methods are evaluated on several large models including Llama2-7B, fine-tuned using LoRA and full model fine-tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper moved forward a step on the urgent need to privacy-preserving split learning over large models and fine-tuning with LoRA.
2. The writing is generally good despite some minor issues. The flow of ideas is clear.
3. The proposed method is evaluated over different pre-trained large models, conforming to current real-world use cases of LLMs.

Weaknesses:
1. There exist several related works discussing the attacks and defense regarding the label leakage in split learning. The authors may need to compare the differences between the proposed methods and previous literature. The evaluation part lacks the comparison to existing privacy-preserving solutions over label leakage and some trivial solutions such as directly applying differential privacy, which is easy to implement.
2. In modern use cases of API fine-tuning, apart from the applications of classification, text generation with LLMs and image generation with multimodal models and diffusion are more common cases. And it is very critical to protect labels in these applications. For example, labels in text generation can contain answers to private questions in the private dataset. However, the leakage study and proposed privacy-preserving methods do not apply to these applications.
3. There are some minor writing issues that could be improved. For example, content introducing API fine-tuning and potential privacy concerns can be shortened in the introduction. The paragraph from line 53 to 58 can be reorganized so that it won't leave '[18]' for a whole line. Same thing for line 209. On line 28, write the full name of LoRA before using acronym. The authors are suggested to talk about split learning and no need to raise extra efforts for readers to understand what vertical federated learning is.

[1] Wan, Xinwei, Jiankai Sun, Shengjie Wang, Lei Chen, Zhenzhe Zheng, Fan Wu, and Guihai Chen. ""PSLF: Defending Against Label Leakage in Split Learning."" In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pp. 2492-2501. 2023.   
[2] Kariyappa, Sanjay, and Moinuddin K. Qureshi. ""ExPLoit: Extracting private labels in split learning."" In 2023 IEEE conference on secure and trustworthy machine learning (SaTML), pp. 165-175. IEEE, 2023.     
[3] Erdoğan, Ege, Alptekin Küpçü, and A. Ercüment Çiçek. ""Unsplit: Data-oblivious model inversion, model stealing, and label inference attacks against split learning."" In Proceedings of the 21st Workshop on Privacy in the Electronic Society, pp. 115-124. 2022.      
[4] Xu, Hengyuan, Liyao Xiang, Hangyu Ye, Dixi Yao, Pengzhi Chu, and Baochun Li. ""Permutation Equivariance of Transformers and Its Applications."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5987-5996. 2024.

Limitations:
Discussed in the last section.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This study addresses the privacy concerns associated with the fine-tuning of Large Language Models (LLMs), focusing on SplitNN. It explores how gradients and activations can leak data, potentially allowing attackers to reconstruct original data sets. In experiments, the proposed method reduces label leakage while maintaining minimal utility loss.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
S1. The manuscript highlights significant privacy issues in LLM fine-tuning, specifically the potential for data leakage through gradients and activations in SplitNN.

S2. Experimental results show that the proposed method significantly mitigates label leakage with minimal impact on utility.

Weaknesses:
W1. The claim that backpropagation is ""conditionally linear"" is not sufficiently rigorous. The manuscript suggests that $\text{backprop}(x, \theta, g_h+z)+\text{backprop}(x, \theta, g_h−z) = \text{backprop}(x, \theta, g_h)$ under the assumption that $\theta$ is constant. However, $\theta$ updates during each backpropagation, invalidating this assumption. Moreover, swapping the order of $\text{backprop}(x, \theta, g_h+z)$ and $\text{backprop}(x, \theta, g_h−z)$ could lead to different outcomes. Formal proof and a clearer statement of assumptions are needed to substantiate this claim.

W2. Section 3.4 describes a method to protect activations that resembles secure multi-party computation [1], lacking novelty. Its effectiveness is also questionable when only one adapter is present.

W3. The proposed protection mainly focuses on labels. In practice, data such as personal identifiers may pose a greater risk than labels. For example, knowing (a) Alice's salary (label) is included in the database is considered a more serious leakage than knowing (b) someone earns a salary of 3.2k. The manuscript should explore if the proposed method can also protect other sensitive features.

**References**

[1] Du, Wenliang, and Mikhail J. Atallah. ""Secure multi-party computation problems and their applications: a review and open problems."" Proceedings of the 2001 workshop on New security paradigms. 2001.

Limitations:
There elaboration on limitations is insufficient.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses privacy leakage during API-based Parameter Efficient Fine-Tuning (PEFT). Their designed P3EFT is a multi-party split learning algorithm that leverages PEFT adjustments to uphold privacy with minimal performance overhead. Their method proves competitive in both multi-party and two-party setups while achieving higher accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Researching API-based fine-tuning for large models is an intriguing topic, especially considering that many clients face challenges loading such large models due to size and computational constraints. In this scenario, privacy concerns regarding client data become paramount. This paper aims to mitigate potential privacy leakage by obfuscating gradients and parameters communicated during transmissions.

Weaknesses:
Their approach shows limited privacy improvement compared to the scenario  Without LoRAs, as indicated in Tables 1, 2, and 3, thereby restricting the overall benefits.

Limitations:
The paper addresses limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an algorithm to preserve the label privacy while achieve good accuracy in the split learning regime. The algorithm is used in parameter-efficient fine-tuning and empirically tested on some language models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper clearly presents its motivation and contribution. The modification on the back-propagation is reasonable and empirically effective across three models and different attacks that are tested

Weaknesses:
The main concern is the scalability of this method. For one iteration, the number of backpropagation is m (at least 2), which is too slow even for PEFT. The computation cost of PEFT is 2/3 of full training so if m=2, the total cost of this method is 4/3 of full training.

Looking at the code, there are 7 hyperparameters introduced by this method, which may be hard to use in practice. I would suggest the authors fix some hyperparamters that the algorithm is robust to, to reduce the number of tunable hyperparameters.

Also the experiment results on SST2 show a severe leakage around 10% compared to without LoRA (even though this is relatively weaker than other methods).

Limitations:
As in its current presentation, the method is limited to language models, split learning (two parties), and label privacy. The empirical evidence is limited to text classification (specifically, this method does not apply to natural language generation where LLAMA is originally trained for) and LoRA. Each limitation can be relaxed, e.g. extending to vision models, data reconstruction, additional PEFT, etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper revisited the problem of label leakage in split learning in the context of fine-tuning large models with parameter-efficient training. Based on modern use cases, they proposed two privacy-preserving protections for gradients and activations during split learning. The proposed methods are evaluated on several large models including Llama2-7B, fine-tuned using LoRA and full model fine-tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper moved forward a step on the urgent need to privacy-preserving split learning over large models and fine-tuning with LoRA.
2. The writing is generally good despite some minor issues. The flow of ideas is clear.
3. The proposed method is evaluated over different pre-trained large models, conforming to current real-world use cases of LLMs.

Weaknesses:
1. There exist several related works discussing the attacks and defense regarding the label leakage in split learning. The authors may need to compare the differences between the proposed methods and previous literature. The evaluation part lacks the comparison to existing privacy-preserving solutions over label leakage and some trivial solutions such as directly applying differential privacy, which is easy to implement.
2. In modern use cases of API fine-tuning, apart from the applications of classification, text generation with LLMs and image generation with multimodal models and diffusion are more common cases. And it is very critical to protect labels in these applications. For example, labels in text generation can contain answers to private questions in the private dataset. However, the leakage study and proposed privacy-preserving methods do not apply to these applications.
3. There are some minor writing issues that could be improved. For example, content introducing API fine-tuning and potential privacy concerns can be shortened in the introduction. The paragraph from line 53 to 58 can be reorganized so that it won't leave '[18]' for a whole line. Same thing for line 209. On line 28, write the full name of LoRA before using acronym. The authors are suggested to talk about split learning and no need to raise extra efforts for readers to understand what vertical federated learning is.

[1] Wan, Xinwei, Jiankai Sun, Shengjie Wang, Lei Chen, Zhenzhe Zheng, Fan Wu, and Guihai Chen. ""PSLF: Defending Against Label Leakage in Split Learning."" In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pp. 2492-2501. 2023.   
[2] Kariyappa, Sanjay, and Moinuddin K. Qureshi. ""ExPLoit: Extracting private labels in split learning."" In 2023 IEEE conference on secure and trustworthy machine learning (SaTML), pp. 165-175. IEEE, 2023.     
[3] Erdoğan, Ege, Alptekin Küpçü, and A. Ercüment Çiçek. ""Unsplit: Data-oblivious model inversion, model stealing, and label inference attacks against split learning."" In Proceedings of the 21st Workshop on Privacy in the Electronic Society, pp. 115-124. 2022.      
[4] Xu, Hengyuan, Liyao Xiang, Hangyu Ye, Dixi Yao, Pengzhi Chu, and Baochun Li. ""Permutation Equivariance of Transformers and Its Applications."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5987-5996. 2024.

Limitations:
Discussed in the last section.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This study addresses the privacy concerns associated with the fine-tuning of Large Language Models (LLMs), focusing on SplitNN. It explores how gradients and activations can leak data, potentially allowing attackers to reconstruct original data sets. In experiments, the proposed method reduces label leakage while maintaining minimal utility loss.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
S1. The manuscript highlights significant privacy issues in LLM fine-tuning, specifically the potential for data leakage through gradients and activations in SplitNN.

S2. Experimental results show that the proposed method significantly mitigates label leakage with minimal impact on utility.

Weaknesses:
W1. The claim that backpropagation is ""conditionally linear"" is not sufficiently rigorous. The manuscript suggests that $\text{backprop}(x, \theta, g_h+z)+\text{backprop}(x, \theta, g_h−z) = \text{backprop}(x, \theta, g_h)$ under the assumption that $\theta$ is constant. However, $\theta$ updates during each backpropagation, invalidating this assumption. Moreover, swapping the order of $\text{backprop}(x, \theta, g_h+z)$ and $\text{backprop}(x, \theta, g_h−z)$ could lead to different outcomes. Formal proof and a clearer statement of assumptions are needed to substantiate this claim.

W2. Section 3.4 describes a method to protect activations that resembles secure multi-party computation [1], lacking novelty. Its effectiveness is also questionable when only one adapter is present.

W3. The proposed protection mainly focuses on labels. In practice, data such as personal identifiers may pose a greater risk than labels. For example, knowing (a) Alice's salary (label) is included in the database is considered a more serious leakage than knowing (b) someone earns a salary of 3.2k. The manuscript should explore if the proposed method can also protect other sensitive features.

**References**

[1] Du, Wenliang, and Mikhail J. Atallah. ""Secure multi-party computation problems and their applications: a review and open problems."" Proceedings of the 2001 workshop on New security paradigms. 2001.

Limitations:
There elaboration on limitations is insufficient.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses privacy leakage during API-based Parameter Efficient Fine-Tuning (PEFT). Their designed P3EFT is a multi-party split learning algorithm that leverages PEFT adjustments to uphold privacy with minimal performance overhead. Their method proves competitive in both multi-party and two-party setups while achieving higher accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Researching API-based fine-tuning for large models is an intriguing topic, especially considering that many clients face challenges loading such large models due to size and computational constraints. In this scenario, privacy concerns regarding client data become paramount. This paper aims to mitigate potential privacy leakage by obfuscating gradients and parameters communicated during transmissions.

Weaknesses:
Their approach shows limited privacy improvement compared to the scenario  Without LoRAs, as indicated in Tables 1, 2, and 3, thereby restricting the overall benefits.

Limitations:
The paper addresses limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an algorithm to preserve the label privacy while achieve good accuracy in the split learning regime. The algorithm is used in parameter-efficient fine-tuning and empirically tested on some language models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper clearly presents its motivation and contribution. The modification on the back-propagation is reasonable and empirically effective across three models and different attacks that are tested

Weaknesses:
The main concern is the scalability of this method. For one iteration, the number of backpropagation is m (at least 2), which is too slow even for PEFT. The computation cost of PEFT is 2/3 of full training so if m=2, the total cost of this method is 4/3 of full training.

Looking at the code, there are 7 hyperparameters introduced by this method, which may be hard to use in practice. I would suggest the authors fix some hyperparamters that the algorithm is robust to, to reduce the number of tunable hyperparameters.

Also the experiment results on SST2 show a severe leakage around 10% compared to without LoRA (even though this is relatively weaker than other methods).

Limitations:
As in its current presentation, the method is limited to language models, split learning (two parties), and label privacy. The empirical evidence is limited to text classification (specifically, this method does not apply to natural language generation where LLAMA is originally trained for) and LoRA. Each limitation can be relaxed, e.g. extending to vision models, data reconstruction, additional PEFT, etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
5p57uCUH8k;"REVIEW 
Summary:
This paper formulates the higher-order curve estimation problem as a NODE problem, enabling effective and accurate solutions with standard ODE solvers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well-written and structurally organised.

Weaknesses:
Reference formats are not consistent.

Limitations:
As described above

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper mainly addressed the problem of insufficient data for low-light enhancement. Specifically, it proposed CLODE , which employs Neural Ordinary Differential Equations to learn the continuous dynamics of the latent image for the first time. The experiments demonstrate the CLODE performs better than other unsupervised learning methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ This is the first attempt to formulate the higher-order curve estimation problem as a NODE problem.
+ CLODE can offer user controllability, enabling users to manually adjust exposure.

Weaknesses:
- Details of User Controllable Design. Despite the better result with use control, detail of the users is missing. For example, the number of volunteers, and whether they are banned from the ground truth image before they adjust the output image. Also, involving human feedback bring much more time in the inference stage.
- In Sec. 3.3 Inference Process, the relationship between the output image IT and noise-free image is questioned. Each iteration includes a noise removal module, yet the output image still contains some noise, contradicting the expectation of a noise-free result in the model.
- Experimental Setup: The experimental setting described in [1] seems more suitable for unsupervised methods. Using only a single dataset for training in this study does not adequately reflect the advantages of the proposed method. A specific analysis comparing and justifying the differences in experimental setups is necessary.
- Model Iteration Selection in ""CLODE+"" (Table 2): The manual operation required to select the iteration step raises concerns. How is this value determined to ensure suitable results? This approach appears more suited to image retouching tasks than enhancement.
- Concern about the fair comparison with previous methods. This paper uses 5 different losses. I wonder whether only part of them is used in previous methods, are the proposed method align with previous methods? For example, some Retinex-based method does not explicitly consider the impact of noise, and they do not have Noise Removal process. Does CLODE still outperform other methods without noise removal? More ablation experiments are needed for thorough explanation.
- Effectiveness of Noise Removal Module: In the first toy scene in Figure 4, as well as Figures 7 and 8, there is noticeable noise residue and some degree of color distortion, which casts doubt on the effectiveness of CLODE and its noise removal module for low-light enhancement.
- More explanation of the superiority of CLODE. Can author provide clearer explanation of the mechanism? For example, in Figure 9 of Supp material, is the better results comes from the more iterations, or more iterations at the early stage, where the estimation is harder?

[1] Learning a Simple Low-light Image Enhancer from Paired Low-light Instances

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript introduces CLODE, which learns low-light image enhancement using neural ordinary differential equations (NODE). The key innovation lies in formulating the higher-order curve estimation problem as a NODE problem. Experimental results show that the proposed approach outperforms state-of-the-art unsupervised counterparts across several benchmarks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	The paper is easy to follow.
2.	Using neural ordinary differential equations to address the iterative curve-adjustment update process shows better performance.

Weaknesses:
1.	The novelty is limited, and the technical contribution is incremental. Apart from formulating the curve estimation as a NODE problem, the paper lacks innovation，which is the main reason why I gave this paper a lower score.
2.	More strong supervised baselines should be included for reference. Comparing only a few relatively weak baselines can lead to a misunderstanding of the current gap between supervised and unsupervised methods. 
3.	Additionally, the authors should report some perceptual metrics for better comparison.
4.	The writing and the presentation need improvement.

Limitations:
The authors have discussed the limitations of the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper This paper proposes an ODE-based  method to tackle low-light image enhancement problems. The motivation of the paper is inspired by the observation that the conventional discrete iterative approaches set fixed update steps. It does not only miss the optimal solution and also does not guarantee the convergence.  Hence, the proposed method takes the iterative curve-adjustment approach and formulates them into solving neural ordinary differential equations. This method is used to work with unsupervised learning to estimate the higher order curve parameters to reconstruct image structure details. Comprehensive experiments demonstrate that the proposed method outperforms the baseline methods on LOL and SICE  benchmarking datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths

1. This paper proposes a novel method that integrates the neural networks into an ODE optimization framework. The neural network is playing an adaptive set of updatable parameters. 
2. Comprehensive experiments show that the proposed method outperforms the baseline methods in the task of low-light image enhancement. 
3. The motivation of this paper is strong and solid. It is inspired by the drawbacks of the existing methods and tackle the problems directly in the proposed method.
4. This paper addresses the limitation of the proposed method.

Weaknesses:
Weakness
1. Based on the visual comparison in Figure 4, the proposed method tends to produce over-exposed areas for highlight regions.  
2. The processing speed of the proposed method is one of the limitations.

Limitations:
The limitation is included in the main manuscript. The processing speed (inference speed) of the proposed method is slow compared to dedicated supervised DL methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a Neural ODE method for curve-adjustment-based low light image enhancement methods to achieve better results which are often sub-optimal for fixed discrete step methods. Specifically, the proposed method reformulates the curve-adjustment-based from the discrete version into the ODE problem by introducing a continuous state. An ODE solver is adopted for the optimization to find the optimal step for the enhancement. Additionally, a simple denosier and a curve parameter estimation module are proposed for noise removal and parameter estimation, respectively. Extensive experiments are conducted to show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Turn the discrete curve-adjustment method into a NODE problem, benefited from the optimization to search for the optimal step.
2. User control support during inference is good for the application of the proposed method.
3. The proposed method seems to have good performance over other competitors.

Weaknesses:
1. The proposed method faces color casts, which is obvious in almost all qualitative results, even with a color constraint in loss functions.
2. The proposed method proposes to denoiser and curve parameter estimator in the NODE framework, however, generalize the method to existing curve-ajustment-based method seems to be a more attractive solution.
3. The denoiser seems to be weak since there is so much noise left for the qualitative results.

Limitations:
Yes, it is discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper formulates the higher-order curve estimation problem as a NODE problem, enabling effective and accurate solutions with standard ODE solvers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well-written and structurally organised.

Weaknesses:
Reference formats are not consistent.

Limitations:
As described above

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper mainly addressed the problem of insufficient data for low-light enhancement. Specifically, it proposed CLODE , which employs Neural Ordinary Differential Equations to learn the continuous dynamics of the latent image for the first time. The experiments demonstrate the CLODE performs better than other unsupervised learning methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ This is the first attempt to formulate the higher-order curve estimation problem as a NODE problem.
+ CLODE can offer user controllability, enabling users to manually adjust exposure.

Weaknesses:
- Details of User Controllable Design. Despite the better result with use control, detail of the users is missing. For example, the number of volunteers, and whether they are banned from the ground truth image before they adjust the output image. Also, involving human feedback bring much more time in the inference stage.
- In Sec. 3.3 Inference Process, the relationship between the output image IT and noise-free image is questioned. Each iteration includes a noise removal module, yet the output image still contains some noise, contradicting the expectation of a noise-free result in the model.
- Experimental Setup: The experimental setting described in [1] seems more suitable for unsupervised methods. Using only a single dataset for training in this study does not adequately reflect the advantages of the proposed method. A specific analysis comparing and justifying the differences in experimental setups is necessary.
- Model Iteration Selection in ""CLODE+"" (Table 2): The manual operation required to select the iteration step raises concerns. How is this value determined to ensure suitable results? This approach appears more suited to image retouching tasks than enhancement.
- Concern about the fair comparison with previous methods. This paper uses 5 different losses. I wonder whether only part of them is used in previous methods, are the proposed method align with previous methods? For example, some Retinex-based method does not explicitly consider the impact of noise, and they do not have Noise Removal process. Does CLODE still outperform other methods without noise removal? More ablation experiments are needed for thorough explanation.
- Effectiveness of Noise Removal Module: In the first toy scene in Figure 4, as well as Figures 7 and 8, there is noticeable noise residue and some degree of color distortion, which casts doubt on the effectiveness of CLODE and its noise removal module for low-light enhancement.
- More explanation of the superiority of CLODE. Can author provide clearer explanation of the mechanism? For example, in Figure 9 of Supp material, is the better results comes from the more iterations, or more iterations at the early stage, where the estimation is harder?

[1] Learning a Simple Low-light Image Enhancer from Paired Low-light Instances

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript introduces CLODE, which learns low-light image enhancement using neural ordinary differential equations (NODE). The key innovation lies in formulating the higher-order curve estimation problem as a NODE problem. Experimental results show that the proposed approach outperforms state-of-the-art unsupervised counterparts across several benchmarks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	The paper is easy to follow.
2.	Using neural ordinary differential equations to address the iterative curve-adjustment update process shows better performance.

Weaknesses:
1.	The novelty is limited, and the technical contribution is incremental. Apart from formulating the curve estimation as a NODE problem, the paper lacks innovation，which is the main reason why I gave this paper a lower score.
2.	More strong supervised baselines should be included for reference. Comparing only a few relatively weak baselines can lead to a misunderstanding of the current gap between supervised and unsupervised methods. 
3.	Additionally, the authors should report some perceptual metrics for better comparison.
4.	The writing and the presentation need improvement.

Limitations:
The authors have discussed the limitations of the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper This paper proposes an ODE-based  method to tackle low-light image enhancement problems. The motivation of the paper is inspired by the observation that the conventional discrete iterative approaches set fixed update steps. It does not only miss the optimal solution and also does not guarantee the convergence.  Hence, the proposed method takes the iterative curve-adjustment approach and formulates them into solving neural ordinary differential equations. This method is used to work with unsupervised learning to estimate the higher order curve parameters to reconstruct image structure details. Comprehensive experiments demonstrate that the proposed method outperforms the baseline methods on LOL and SICE  benchmarking datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths

1. This paper proposes a novel method that integrates the neural networks into an ODE optimization framework. The neural network is playing an adaptive set of updatable parameters. 
2. Comprehensive experiments show that the proposed method outperforms the baseline methods in the task of low-light image enhancement. 
3. The motivation of this paper is strong and solid. It is inspired by the drawbacks of the existing methods and tackle the problems directly in the proposed method.
4. This paper addresses the limitation of the proposed method.

Weaknesses:
Weakness
1. Based on the visual comparison in Figure 4, the proposed method tends to produce over-exposed areas for highlight regions.  
2. The processing speed of the proposed method is one of the limitations.

Limitations:
The limitation is included in the main manuscript. The processing speed (inference speed) of the proposed method is slow compared to dedicated supervised DL methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a Neural ODE method for curve-adjustment-based low light image enhancement methods to achieve better results which are often sub-optimal for fixed discrete step methods. Specifically, the proposed method reformulates the curve-adjustment-based from the discrete version into the ODE problem by introducing a continuous state. An ODE solver is adopted for the optimization to find the optimal step for the enhancement. Additionally, a simple denosier and a curve parameter estimation module are proposed for noise removal and parameter estimation, respectively. Extensive experiments are conducted to show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Turn the discrete curve-adjustment method into a NODE problem, benefited from the optimization to search for the optimal step.
2. User control support during inference is good for the application of the proposed method.
3. The proposed method seems to have good performance over other competitors.

Weaknesses:
1. The proposed method faces color casts, which is obvious in almost all qualitative results, even with a color constraint in loss functions.
2. The proposed method proposes to denoiser and curve parameter estimator in the NODE framework, however, generalize the method to existing curve-ajustment-based method seems to be a more attractive solution.
3. The denoiser seems to be weak since there is so much noise left for the qualitative results.

Limitations:
Yes, it is discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
5ClpGA0u9K;"REVIEW 
Summary:
The paper proposes a new method, called Energy Rank Alignment (ERA) to finetune large language models (LLMs) for molecular generation in a similar fashion to Reinforcement Learning from Human Feedback (RLHF). The paper first introduces how the alignment task  in LLMs is very similar to creating property-conditioned molecules from SMILES strings, which are token-based generation techniques. In the introduction, the paper distinguishes ERA from common RLHF methods, such as PPO and DPO, by stating that it has a minimization objectives and leverages a reward function. Next, the paper describes related work for using LLMs for molecular generation and RLHF for language models and reiterates the differences of ERA compared to PPO and DPO.

In Section 2, the paper outlines the definition of ERA which mostly center on the derivation of relevant loss functions that the algorithm aims to minimize. In its definition, the ERA loss makes use of the KL divergence to arrive at the final formulation at the end of Section 2 leading up to the on-policy loss formulation for ERA. Section 3 provides a theoretical analysis of the ERA loss and its gradients, as well as its connections to the regularized entropy objective.

Section 4 describes the experiments for molecular generation using ERA, including unprompted and prompted generation. The paper also includes a sub-section on general alignment settings of LLMs related to IMDB movie reviews. The results generally show a distribution shift between models finetuned with ERA and those that were not. The paper subsequently ends with a conclusion and discussion of limitations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The provides proposed an interesting method and finetuning objectives that is useful for conditioned molecular generation and LLM alignment. The strengths include:
* A novel method for designing property conditioned molecules that is also applicable to LLM alignment. [Originality, Significance]
* A detailed derivation of the ERA loss, as well as a theoretical analysis on relevant properties. [Quality, Clarity]
* Experiments that generally support the distribution shift induced by the ERA method.

Weaknesses:
The weaknesses of the paper mostly center on expanding relevant related work and baselines for experiments:
* The authors do not discussion related work to training of transformer models and LLMs using reinforcement learning to arrive at molecules with desired properties. Some examples include [1] [2] 
* The experiments do not include baseline evaluation of DPO and PPO, which would have provided relevant details for how ERA performs compared to established baselines. 
* The paper could be strengthened by providing additional details related to experimental settings (see questions)


[1] Ghugare, Raj, Santiago Miret, Adriana Hugessen, Mariano Phielipp, and Glen Berseth. ""Searching for High-Value Molecules Using Reinforcement Learning and Transformers."" In The Twelfth International Conference on Learning Representations.

[2] Blaschke, Thomas, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. ""REINVENT 2.0: an AI tool for de novo drug design."" Journal of chemical information and modeling 60, no. 12 (2020): 5918-5922.

Limitations:
The authors briefly discuss limitations at the end of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce “Energy Rank Alignment”, a novel alternative to PPO and DPO for policy optimization when an explicit reward model is available. ERA is shown to work for enriching chemical libraries for proxy objectives that are fast and easy to compute, and has clear benefits in the simplicity of tuning the strength of regularization to a reference and entropy of samples with two decoupled parameters. This controllability allows ERA to avoid greedy policies and the sort of mode collapse often observed using DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The ERA approach is interesting and clearly defined. It is well-suited for many preference optimization settings, where an explicit reward model is available and alternative methods do not take advantage of this. The authors show results on multi-objective optimization to illustrate that the approach is not limited to greedy optimization of single objectives.

Weaknesses:
The main weakness of the paper is the evaluation with respect to lead optimization of small molecules. This is a notoriously difficult kind of evaluation to make meaningful with purely in silico experiments. One clear opportunity for the authors to improve their evals, while respecting the constraints imposed by easily-computable reward functions, is to incorporate some kind of online evaluation. Comparing DPO and ERA in an online setting would be informative and more relevant for the chemistry community.

Limitations:
Partially

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study an important problem about searching through chemical space, where the number of possible molecules grows combinatorially with the number of atoms. They focus on aligning large autoregressive models trained on chemical compound databases to generate molecules. The energy rank alignment (ERA) algorithm is proposed to use an explicit reward function to produce a gradient-based objective for optimizing autoregressive policies.  The authors offer theoretical insights into the relationship between energy rank alignment (ERA) and proximal policy optimization (PPO), direct preference optimization (DPO). Their experiments show that ERA is scalable, does not require reinforcement learning, and performs well compared to DPO when preference observations per pairing are limited.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors study a significant problem about generating molecules with desired properties based on autoregressive models by proposing the energy rank alignment (ERA) algorithm. 

2. This paper is well written.

3. The proposed methods work reasonably well.

Weaknesses:
1. Diversity, novelty and uniqueness are all important properties for drug discovery as discussed in previous works. To verify whether the models can be used to improve the process of drug discovery, the paper may benefit from comparing the aligned models with the reference model based on these metrics.

2. Missing the discussion of the related works which also focus on molecule optimization and drug discovery for both traditional and state-of-the-art methods, such as [1] [2] and so on.

3. The authors propose using reinforcement learning for drug optimization, a well-established method frequently employed in prior works, such as [3,4]. Additionally, advantage-based and multi-objective policy optimization are well-known in the reinforcement learning literature. A more comprehensive analysis of the limitations of this approach, along with a comparison to other existing methods, would have been beneficial.

[1] Drugassist: A large language model for molecule optimization.

[2] Automatic chemical design using a data-driven continuous representation of molecules.

[3] Optimization of molecules via deep reinforcement learning. Scientific Reports. 2019. 

[4] Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Nature Machine Intelligence. 2021.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new method, called Energy Rank Alignment (ERA) to finetune large language models (LLMs) for molecular generation in a similar fashion to Reinforcement Learning from Human Feedback (RLHF). The paper first introduces how the alignment task  in LLMs is very similar to creating property-conditioned molecules from SMILES strings, which are token-based generation techniques. In the introduction, the paper distinguishes ERA from common RLHF methods, such as PPO and DPO, by stating that it has a minimization objectives and leverages a reward function. Next, the paper describes related work for using LLMs for molecular generation and RLHF for language models and reiterates the differences of ERA compared to PPO and DPO.

In Section 2, the paper outlines the definition of ERA which mostly center on the derivation of relevant loss functions that the algorithm aims to minimize. In its definition, the ERA loss makes use of the KL divergence to arrive at the final formulation at the end of Section 2 leading up to the on-policy loss formulation for ERA. Section 3 provides a theoretical analysis of the ERA loss and its gradients, as well as its connections to the regularized entropy objective.

Section 4 describes the experiments for molecular generation using ERA, including unprompted and prompted generation. The paper also includes a sub-section on general alignment settings of LLMs related to IMDB movie reviews. The results generally show a distribution shift between models finetuned with ERA and those that were not. The paper subsequently ends with a conclusion and discussion of limitations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The provides proposed an interesting method and finetuning objectives that is useful for conditioned molecular generation and LLM alignment. The strengths include:
* A novel method for designing property conditioned molecules that is also applicable to LLM alignment. [Originality, Significance]
* A detailed derivation of the ERA loss, as well as a theoretical analysis on relevant properties. [Quality, Clarity]
* Experiments that generally support the distribution shift induced by the ERA method.

Weaknesses:
The weaknesses of the paper mostly center on expanding relevant related work and baselines for experiments:
* The authors do not discussion related work to training of transformer models and LLMs using reinforcement learning to arrive at molecules with desired properties. Some examples include [1] [2] 
* The experiments do not include baseline evaluation of DPO and PPO, which would have provided relevant details for how ERA performs compared to established baselines. 
* The paper could be strengthened by providing additional details related to experimental settings (see questions)


[1] Ghugare, Raj, Santiago Miret, Adriana Hugessen, Mariano Phielipp, and Glen Berseth. ""Searching for High-Value Molecules Using Reinforcement Learning and Transformers."" In The Twelfth International Conference on Learning Representations.

[2] Blaschke, Thomas, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. ""REINVENT 2.0: an AI tool for de novo drug design."" Journal of chemical information and modeling 60, no. 12 (2020): 5918-5922.

Limitations:
The authors briefly discuss limitations at the end of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce “Energy Rank Alignment”, a novel alternative to PPO and DPO for policy optimization when an explicit reward model is available. ERA is shown to work for enriching chemical libraries for proxy objectives that are fast and easy to compute, and has clear benefits in the simplicity of tuning the strength of regularization to a reference and entropy of samples with two decoupled parameters. This controllability allows ERA to avoid greedy policies and the sort of mode collapse often observed using DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The ERA approach is interesting and clearly defined. It is well-suited for many preference optimization settings, where an explicit reward model is available and alternative methods do not take advantage of this. The authors show results on multi-objective optimization to illustrate that the approach is not limited to greedy optimization of single objectives.

Weaknesses:
The main weakness of the paper is the evaluation with respect to lead optimization of small molecules. This is a notoriously difficult kind of evaluation to make meaningful with purely in silico experiments. One clear opportunity for the authors to improve their evals, while respecting the constraints imposed by easily-computable reward functions, is to incorporate some kind of online evaluation. Comparing DPO and ERA in an online setting would be informative and more relevant for the chemistry community.

Limitations:
Partially

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study an important problem about searching through chemical space, where the number of possible molecules grows combinatorially with the number of atoms. They focus on aligning large autoregressive models trained on chemical compound databases to generate molecules. The energy rank alignment (ERA) algorithm is proposed to use an explicit reward function to produce a gradient-based objective for optimizing autoregressive policies.  The authors offer theoretical insights into the relationship between energy rank alignment (ERA) and proximal policy optimization (PPO), direct preference optimization (DPO). Their experiments show that ERA is scalable, does not require reinforcement learning, and performs well compared to DPO when preference observations per pairing are limited.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors study a significant problem about generating molecules with desired properties based on autoregressive models by proposing the energy rank alignment (ERA) algorithm. 

2. This paper is well written.

3. The proposed methods work reasonably well.

Weaknesses:
1. Diversity, novelty and uniqueness are all important properties for drug discovery as discussed in previous works. To verify whether the models can be used to improve the process of drug discovery, the paper may benefit from comparing the aligned models with the reference model based on these metrics.

2. Missing the discussion of the related works which also focus on molecule optimization and drug discovery for both traditional and state-of-the-art methods, such as [1] [2] and so on.

3. The authors propose using reinforcement learning for drug optimization, a well-established method frequently employed in prior works, such as [3,4]. Additionally, advantage-based and multi-objective policy optimization are well-known in the reinforcement learning literature. A more comprehensive analysis of the limitations of this approach, along with a comparison to other existing methods, would have been beneficial.

[1] Drugassist: A large language model for molecule optimization.

[2] Automatic chemical design using a data-driven continuous representation of molecules.

[3] Optimization of molecules via deep reinforcement learning. Scientific Reports. 2019. 

[4] Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Nature Machine Intelligence. 2021.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
4kCr61XYQJ;"REVIEW 
Summary:
This work extends Poisson-Gamma Dynamical Systems (PGDSs) by considering non-stationary transition dynamics to effectively capture the evolving dynamics of observed count sequences.

The authors propose a model where the underlying transition matrices evolve over time, based on three (gradually more complex and flexible) Dirichlet Markov chains.

For inference of the model, the authors make use of the Dirichlet-Multinomial-Beta data augmentation to derive a fully-conjugate Gibbs sampler.

Experiments showcase improved data-smoothing and forecasting performance of the proposed method across several real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Extending PGDS models to accommodate time-varying transition dynamics is of interest and significant

- The proposed variations of Dirichlet-Markov chains provide flexibility in capturing different modeling assumptions

- Devising a closed-form Gibbs sampler for posterior inference of this model is significant.
    - The attained expressions seem correct to the best of my knowledge, although I did not carefully double-check the mathematical details of the derivation.

Weaknesses:
- The main limitation of this work is the assumption that the transition kernel is static within each sub-interval: i.e., the authors consider that the kernel can only change at discrete instants, while is constant within each sub-interval.

Limitations:
The authors address the main limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Existing PGDS models struggle with capturing the time-varying transition dynamics seen in real-world data. To address this, the submission proposed a non-stationary PGDS, allowing the transition matrices to evolve over time, modeled by Dirichlet Markov chains. Using Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed for posterior simulation. Experiments demonstrate that the proposed non-stationary PGDS achieves improved predictive performance compared to related models.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed non-stationary Poisson-Gamma Dynamical System offers several notable advantages. 

Firstly, its ability to allow transition matrices to evolve over time addresses the limitation of state-of-the-art PGDS models in capturing time-varying transition dynamics, making it more suitable for real-world count time series. 

Secondly, the use of specifically-designed Dirichlet Markov chains to model the evolving transition matrices enhances the model’s capacity to learn non-stationary dependency structures. 

Thirdly, the application of Dirichlet-Multinomial-Beta data augmentation techniques facilitates the development of a fully-conjugate and efficient Gibbs sampler for posterior simulation.

Weaknesses:
I did not find any obvious weaknesses.

Limitations:
The authors discussed some future work directions in the conclusion.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work extends  Poisson-Gamma Dynamical systems (PGDS) to model non-stationary dynamics by replacing the constant transition matrix $\Pi$ with a time dependent one $\Pi^{(t)}$ and the original Dirichlet prior on the columns with three different Dirichlet Markov chain constructions. The manuscript describes am efficient Gibbs-sampler for inference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The work addresses a relevant problem of modeling non-stationary dynamics in count time series. The provided extension relative to the original PGDS is sufficiently novel. I lack deep enough understanding of some parts related to the sampler, therefore I cannot assess if the construction of the sampler required new ideas or was a mechanical extension of the sampler for PGDS  (this being the main reason for my lower confidence score.). I tend to assume new ideas were necessary.

Weaknesses:
My major problem is the experiment evaluation. In Table 1 in the NIPS dataset we can see results like $14.014 \pm 4.387$ bolded, over values like $14.706 \pm 4.414$, or $17.105 \pm 6.449$. 
In ICEWS values like $0.214 \pm 0.008$ over $0.215 \pm 0.007$ , in USEI $4.596 \pm 0.562$ over $4.703  \pm 0.538$, in COVID $6.969 \pm 1.107$ over $7.566 \pm 1.095$. These are mainly smoothing results. In the light of this I am not confident in the statement “As the experiment results shown in Table 1, the NS-PGDS exhibits improved performance in both data smoothing and forecasting tasks.”.  We do not know how the confidence interval was computed, or how many repeats were made. The lack of statistical rigor in the evaluation stands in striking contrast with the sophisticated Bayesian model presented. 

Besides this, other possible problem with the evaluation is that the manuscript states that default paramerters were used for the benchmark methods “GP-DPFA, PGDS, GMC-RATE, GMC-HIER, BGAR”  while the present method used specific K based on the dataset. It is very hard to tell if this is a fair comparison or not.

Limitations:
No specific limitation section was provided. The part on future work in the Conclusion can be interpreted as pointing out some limitations of the current model, but a specific limitation statement would be preferable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces non-stationary Poisson-Gamma dynamical systems, an extension of Poisson Gamma dynamical systems with a dynamic transition matrix. Decomposing the time steps into equally spaced subintervals, the transition matrices evolve between sub-intervals, remaining static within sub-intervals. The authors introduce three options for transitions to occur. The authors derive a Gibbs sampling scheme for exact posterior inference using data augmentation techniques and showcase the effectiveness of their method through a series of predictive and qualitative results.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This is a well-written, organized paper that is easy to read. The proposed method allows for exact posterior inference through Gibbs sampling. The authors exhibit extensive predictive results across 4 datasets, although their method only exhibits marginal improvement as compared to Poisson Gamma dynamical systems.

Weaknesses:
I'm not convinced that the magnitude of the author's contribution, nor the significance of the paper is strong enough to warrant acceptance, and the methods produce only marginally better results than that of Poisson Gamma dynamical systems. The qualitative results are not groundbreaking.

Limitations:
The authors address the limitations of their work, stating intention to address these limitations (e.g. constant sub-interval lengths) in future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work extends Poisson-Gamma Dynamical Systems (PGDSs) by considering non-stationary transition dynamics to effectively capture the evolving dynamics of observed count sequences.

The authors propose a model where the underlying transition matrices evolve over time, based on three (gradually more complex and flexible) Dirichlet Markov chains.

For inference of the model, the authors make use of the Dirichlet-Multinomial-Beta data augmentation to derive a fully-conjugate Gibbs sampler.

Experiments showcase improved data-smoothing and forecasting performance of the proposed method across several real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Extending PGDS models to accommodate time-varying transition dynamics is of interest and significant

- The proposed variations of Dirichlet-Markov chains provide flexibility in capturing different modeling assumptions

- Devising a closed-form Gibbs sampler for posterior inference of this model is significant.
    - The attained expressions seem correct to the best of my knowledge, although I did not carefully double-check the mathematical details of the derivation.

Weaknesses:
- The main limitation of this work is the assumption that the transition kernel is static within each sub-interval: i.e., the authors consider that the kernel can only change at discrete instants, while is constant within each sub-interval.

Limitations:
The authors address the main limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Existing PGDS models struggle with capturing the time-varying transition dynamics seen in real-world data. To address this, the submission proposed a non-stationary PGDS, allowing the transition matrices to evolve over time, modeled by Dirichlet Markov chains. Using Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed for posterior simulation. Experiments demonstrate that the proposed non-stationary PGDS achieves improved predictive performance compared to related models.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed non-stationary Poisson-Gamma Dynamical System offers several notable advantages. 

Firstly, its ability to allow transition matrices to evolve over time addresses the limitation of state-of-the-art PGDS models in capturing time-varying transition dynamics, making it more suitable for real-world count time series. 

Secondly, the use of specifically-designed Dirichlet Markov chains to model the evolving transition matrices enhances the model’s capacity to learn non-stationary dependency structures. 

Thirdly, the application of Dirichlet-Multinomial-Beta data augmentation techniques facilitates the development of a fully-conjugate and efficient Gibbs sampler for posterior simulation.

Weaknesses:
I did not find any obvious weaknesses.

Limitations:
The authors discussed some future work directions in the conclusion.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work extends  Poisson-Gamma Dynamical systems (PGDS) to model non-stationary dynamics by replacing the constant transition matrix $\Pi$ with a time dependent one $\Pi^{(t)}$ and the original Dirichlet prior on the columns with three different Dirichlet Markov chain constructions. The manuscript describes am efficient Gibbs-sampler for inference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The work addresses a relevant problem of modeling non-stationary dynamics in count time series. The provided extension relative to the original PGDS is sufficiently novel. I lack deep enough understanding of some parts related to the sampler, therefore I cannot assess if the construction of the sampler required new ideas or was a mechanical extension of the sampler for PGDS  (this being the main reason for my lower confidence score.). I tend to assume new ideas were necessary.

Weaknesses:
My major problem is the experiment evaluation. In Table 1 in the NIPS dataset we can see results like $14.014 \pm 4.387$ bolded, over values like $14.706 \pm 4.414$, or $17.105 \pm 6.449$. 
In ICEWS values like $0.214 \pm 0.008$ over $0.215 \pm 0.007$ , in USEI $4.596 \pm 0.562$ over $4.703  \pm 0.538$, in COVID $6.969 \pm 1.107$ over $7.566 \pm 1.095$. These are mainly smoothing results. In the light of this I am not confident in the statement “As the experiment results shown in Table 1, the NS-PGDS exhibits improved performance in both data smoothing and forecasting tasks.”.  We do not know how the confidence interval was computed, or how many repeats were made. The lack of statistical rigor in the evaluation stands in striking contrast with the sophisticated Bayesian model presented. 

Besides this, other possible problem with the evaluation is that the manuscript states that default paramerters were used for the benchmark methods “GP-DPFA, PGDS, GMC-RATE, GMC-HIER, BGAR”  while the present method used specific K based on the dataset. It is very hard to tell if this is a fair comparison or not.

Limitations:
No specific limitation section was provided. The part on future work in the Conclusion can be interpreted as pointing out some limitations of the current model, but a specific limitation statement would be preferable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces non-stationary Poisson-Gamma dynamical systems, an extension of Poisson Gamma dynamical systems with a dynamic transition matrix. Decomposing the time steps into equally spaced subintervals, the transition matrices evolve between sub-intervals, remaining static within sub-intervals. The authors introduce three options for transitions to occur. The authors derive a Gibbs sampling scheme for exact posterior inference using data augmentation techniques and showcase the effectiveness of their method through a series of predictive and qualitative results.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This is a well-written, organized paper that is easy to read. The proposed method allows for exact posterior inference through Gibbs sampling. The authors exhibit extensive predictive results across 4 datasets, although their method only exhibits marginal improvement as compared to Poisson Gamma dynamical systems.

Weaknesses:
I'm not convinced that the magnitude of the author's contribution, nor the significance of the paper is strong enough to warrant acceptance, and the methods produce only marginally better results than that of Poisson Gamma dynamical systems. The qualitative results are not groundbreaking.

Limitations:
The authors address the limitations of their work, stating intention to address these limitations (e.g. constant sub-interval lengths) in future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
4dmwvbs4Ea;"REVIEW 
Summary:
This paper studies the offline policy optimization problem, i.e. to find a policy whose value function is close to the optimal value function using offline samples.  Under the assumption of linear MDP, they proposed a gradient ascent algorithm. 

The sample complexity of the algorithm only depends on the feature coverage of the best policy and does not require coverage over any other policies.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well written. The algorithm, theorems and lemmas presented in this paper are all very clear.

The problem studied in this paper is offline policy optimization, which has significant value to the community, both empirically and theoretically. 

The algorithm in this paper is simple and computationally tractable.

In contrast to other offline RL paper, this paper does not require that the offline data is sampled i.i.d. or to be admissible, and they can handle arbitrary offline data as long as the data has sufficient coverage to the best policy.

Weaknesses:
Compared to Zanette (2021), the algorithm idea is somehow similar. Specifically, both algorithms use the actor-critic update, and the optimization in the algorithm of this paper is similar to the pessimism estimation in Zanette (2021).

The assumption made in this paper is the linear MDP assumption, which is stronger than the assumption in Zanette (2021).

Limitations:
Yes. The authors addressed all the limitations listed in the guidelines.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a new algorithm for offline reinforcement learning in linear infinite-horizon discounted MDP, which achieves a strong sample complexity under the weakest data coverage assumption. Moreover, their algorithm is easy to implement and computationally efficient. Their algorithm design is based on a reduced version of linear programming formulation of MDP, which approximately transforms the original problem into a unconstrained saddle-point optimization problem.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The algorithm proposed in this paper has many nice properties: it is computationally efficient, easy to implement, and works under the weakest data coverage assumption.
2. The developed techniques and the observations on transforming the optimization problem is insightful.
3. The paper is also well-written, with clear proof sketch and is well-positioned among related work.

Weaknesses:
The paper is pretty notation heavy and a bit hard to follow. I would suggest including a table of notations with descriptions. The choices of notation can also be optimized. For example, I found the mixed use of $D_{\pi}$ and $D_{\theta}$ confusing, and they looks like they are dependent on the value of $\pi$ and $\theta$.

Limitations:
Nothing necessary stands out.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an approach for solving Offline RL problems in domains where the underlying MDP problem has reward and transition models that are linearly realisable under a known feature map. The paper is well presented.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Some of the tricks employed in the approach are quite interesting. 
2. The analytical results are good.

Weaknesses:
1. This work is only for MDPs where rewards and transitions are linear. Many of the moderately interesting problems are not linearly realisable, so it is quite important that authors provided a detailed discussion on how it can be addressed. 
2. I am not entirely confident about this, so will wait for inputs from authors. The approach seems to be built based on works by Hong and Tiwari, and stabilisation trick [Neu and okolo, Jacobson et al.]. I was not sure on the key significant contributions of this paper on top of those works. 
3. For me the biggest concern is that there are no experimental results. How would such an approach work for an MDP where the transition and reward models are not linear? Also, the approach is still approximate (given the bound), so would have been important to show the real results.

Limitations:
There is no limitations mentioned.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the offline policy optimization problem, i.e. to find a policy whose value function is close to the optimal value function using offline samples.  Under the assumption of linear MDP, they proposed a gradient ascent algorithm. 

The sample complexity of the algorithm only depends on the feature coverage of the best policy and does not require coverage over any other policies.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well written. The algorithm, theorems and lemmas presented in this paper are all very clear.

The problem studied in this paper is offline policy optimization, which has significant value to the community, both empirically and theoretically. 

The algorithm in this paper is simple and computationally tractable.

In contrast to other offline RL paper, this paper does not require that the offline data is sampled i.i.d. or to be admissible, and they can handle arbitrary offline data as long as the data has sufficient coverage to the best policy.

Weaknesses:
Compared to Zanette (2021), the algorithm idea is somehow similar. Specifically, both algorithms use the actor-critic update, and the optimization in the algorithm of this paper is similar to the pessimism estimation in Zanette (2021).

The assumption made in this paper is the linear MDP assumption, which is stronger than the assumption in Zanette (2021).

Limitations:
Yes. The authors addressed all the limitations listed in the guidelines.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a new algorithm for offline reinforcement learning in linear infinite-horizon discounted MDP, which achieves a strong sample complexity under the weakest data coverage assumption. Moreover, their algorithm is easy to implement and computationally efficient. Their algorithm design is based on a reduced version of linear programming formulation of MDP, which approximately transforms the original problem into a unconstrained saddle-point optimization problem.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The algorithm proposed in this paper has many nice properties: it is computationally efficient, easy to implement, and works under the weakest data coverage assumption.
2. The developed techniques and the observations on transforming the optimization problem is insightful.
3. The paper is also well-written, with clear proof sketch and is well-positioned among related work.

Weaknesses:
The paper is pretty notation heavy and a bit hard to follow. I would suggest including a table of notations with descriptions. The choices of notation can also be optimized. For example, I found the mixed use of $D_{\pi}$ and $D_{\theta}$ confusing, and they looks like they are dependent on the value of $\pi$ and $\theta$.

Limitations:
Nothing necessary stands out.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an approach for solving Offline RL problems in domains where the underlying MDP problem has reward and transition models that are linearly realisable under a known feature map. The paper is well presented.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Some of the tricks employed in the approach are quite interesting. 
2. The analytical results are good.

Weaknesses:
1. This work is only for MDPs where rewards and transitions are linear. Many of the moderately interesting problems are not linearly realisable, so it is quite important that authors provided a detailed discussion on how it can be addressed. 
2. I am not entirely confident about this, so will wait for inputs from authors. The approach seems to be built based on works by Hong and Tiwari, and stabilisation trick [Neu and okolo, Jacobson et al.]. I was not sure on the key significant contributions of this paper on top of those works. 
3. For me the biggest concern is that there are no experimental results. How would such an approach work for an MDP where the transition and reward models are not linear? Also, the approach is still approximate (given the bound), so would have been important to show the real results.

Limitations:
There is no limitations mentioned.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
3sWghzJvGd;"REVIEW 
Summary:
This paper investigates the generalization capabilities of world models in RL, particularly with respect to latent representation errors, which arise when observations are encoded into a low-dimensional latent space. The authors provide a bound on latent representation error when using CNN encoder-decoder architectures. The world model is framed as a stochastic differential equation to characterize the impact of latent representation errors on generalization in terms of either zero or non-zero drift. The authors provide theoretical analysis which shows that these errors can result in implicit regularization in the zero drift case, and propose a Jacobian regularization scheme to tackle the unwanted bias term in the non-zero drift case. Finally, when performing model rollouts for learning a policy, the authors study the effect of these errors on the value function. Experiments on Mujoco tasks demonstrate that the proposed Jacobian regularization enhances robustness to noisy states, reduces the detrimental impact of latent representation errors, and improves convergence speed for longer horizon tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- World models are a popular area of research in the RL community, but there is a lack of theoretical understanding. This paper takes one step towards theoretically analyzing the generalization capabilities of world models.
- The analysis of the effect of latent representation error is a novel theoretical contribution, to the best of my knowledge.
- The results in the paper seem mathematically sound and provide useful insights. The empirical results demonstrate that the Jacobian regularization, which naturally arises from the theoretical analysis, is helpful in improving robustness.
- As a very theory-heavy paper, the authors structured the writing such that it makes it easy to follow each individual result (though there is some room for improvement here, see weaknesses).

Weaknesses:
While the paper studies a previously unexplored problem, there are some questions about the significance of these findings and the use of drift and diffusion terms to represent the error. Other areas for improvement include explaining the insights from the theoretical analysis more clearly, describing the experimental settings in more detail, and supporting certain claims with more evidence.

- Studying the effect of latent representation error is certainly useful, however, with recent advances in representation learning approaches, one can learn reasonably good representations such that the reconstruction error is negligible. When it comes to model-based RL, a much bigger issue is the compounding model error, which is a result of error in the latent/state dynamics model predictions. A comment from the authors on this aspect would be helpful.
- The decomposition of latent error into drift and diffusion terms seems a bit contrived. It is not clear how the error can be expressed in this form, and what defines the scenarios of zero versus non-zero drift.
- The interpretation that propagation of latent error leads to the model exploring novel states seems somewhat questionable. My understanding is that the erroneous states improve robustness similar to noise injection, but will most likely not be valid states belonging to the state space of the MDP. Some reasonable evidence is required to support this statement.
- The paper presents several results and including some intuitive or low-level explanation for each of those results would greatly improve readability. Additionally, due to the large amount of mathematical notation used throughout the paper, it would be helpful to include a notation table in the appendix for easy reference.
- The experimental setting is not sufficiently clear, especially in the introduction when the authors refer to Table 1. With regards to the perturbations - are they applied to every state in the trajectory? For masking, is the same mask used for every state, or is the mask also sampled randomly? With regards to injecting encoder error - how to interpret the $\mu_t$ and $\sigma_t$ values?

Limitations:
There is little discussion on the limitations of the analysis. Some points worth discussing could be the impact of various assumptions when deriving the results, the fact that the analysis is mostly focused on a specific setting - learning from pixels using a CNN encoder and an RNN latent dynamics model, and further investigation of the compounding model error problem.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the generalization capability of world models via a stochastic differential equation formulation. They try to understand latent representation errors on generalization, with both zero-drift representation errors and non-zero-drift representation errors. They found that zero drift latent representation errors are implicit regularization and thus bring generalization gain. Jacobian regularization is proposed to enhance training stability and generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ A deep understanding of the generalization of world models via stochastic differential equation formulation;
+ A careful study of the different effects of zero drift and non-zero drift on gn

Weaknesses:
+ The unseen images are produced via global/partial Gaussian noises and rotation, which seems more on the robustness side rather than the generalization of unseen images;

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the generalization capability of world models in reinforcement learning. In particular, they investigate the latent representation error in world models. They show that zero-drift representation error is inherently a regularizer for the learned model functions. On the other hand, they show that the non-zero-drift representation error accumulates errors and Jacobian regularization can be used to alleviate the issue. They demonstrate their proposed approach improves stability, convergence, and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work investigates an interesting aspect, the generalization of world models that learn the dynamics of the environment. Very limited work has been done in this facet of RL, thus it will share significant insights with the DRL research community.

2. The paper followed a structured methodology to analyze the world model and its representation errors. They interpret the learned model function as stochastic differential equations (SDEs) and model the variation as Brownian motions. 

3. I liked the way they theoretically analyzed it case-by-case and established connections with prior findings. 

4. The paper articulately presents the findings of zero-drift error as a regularizer and the Jacobian correction term for non-zero-drift representation error.  It systematically proves its hypotheses and shows evidence against the claims. They presented corresponding formulas and interpretations.

Weaknesses:
1. The paper is very thorough in terms of theoretical derivation. However, in my opinion, the experimental section of the paper is somewhat lacking. It utilizes only two tasks from Mujoco to prove the efficacy of the approach. More diverse tasks from other benchmarks and robust perturbations will certainly improve the paper. 

2. The experimental evaluation is limited to reward comparison. However, it would be interesting to see some visualization of how the trajectories unfold in the case of both types of errors and with Jacobian regularization.

Limitations:
While the paper discusses the potential social impact of the work, it doesn’t discuss any limitations. I believe the characterization of the models as SDE and the use of Brownian motion as variation have certain contributions to the identified claims. Other interpretations may alter the findings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the generalization capabilities of world models in RL, particularly with respect to latent representation errors, which arise when observations are encoded into a low-dimensional latent space. The authors provide a bound on latent representation error when using CNN encoder-decoder architectures. The world model is framed as a stochastic differential equation to characterize the impact of latent representation errors on generalization in terms of either zero or non-zero drift. The authors provide theoretical analysis which shows that these errors can result in implicit regularization in the zero drift case, and propose a Jacobian regularization scheme to tackle the unwanted bias term in the non-zero drift case. Finally, when performing model rollouts for learning a policy, the authors study the effect of these errors on the value function. Experiments on Mujoco tasks demonstrate that the proposed Jacobian regularization enhances robustness to noisy states, reduces the detrimental impact of latent representation errors, and improves convergence speed for longer horizon tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- World models are a popular area of research in the RL community, but there is a lack of theoretical understanding. This paper takes one step towards theoretically analyzing the generalization capabilities of world models.
- The analysis of the effect of latent representation error is a novel theoretical contribution, to the best of my knowledge.
- The results in the paper seem mathematically sound and provide useful insights. The empirical results demonstrate that the Jacobian regularization, which naturally arises from the theoretical analysis, is helpful in improving robustness.
- As a very theory-heavy paper, the authors structured the writing such that it makes it easy to follow each individual result (though there is some room for improvement here, see weaknesses).

Weaknesses:
While the paper studies a previously unexplored problem, there are some questions about the significance of these findings and the use of drift and diffusion terms to represent the error. Other areas for improvement include explaining the insights from the theoretical analysis more clearly, describing the experimental settings in more detail, and supporting certain claims with more evidence.

- Studying the effect of latent representation error is certainly useful, however, with recent advances in representation learning approaches, one can learn reasonably good representations such that the reconstruction error is negligible. When it comes to model-based RL, a much bigger issue is the compounding model error, which is a result of error in the latent/state dynamics model predictions. A comment from the authors on this aspect would be helpful.
- The decomposition of latent error into drift and diffusion terms seems a bit contrived. It is not clear how the error can be expressed in this form, and what defines the scenarios of zero versus non-zero drift.
- The interpretation that propagation of latent error leads to the model exploring novel states seems somewhat questionable. My understanding is that the erroneous states improve robustness similar to noise injection, but will most likely not be valid states belonging to the state space of the MDP. Some reasonable evidence is required to support this statement.
- The paper presents several results and including some intuitive or low-level explanation for each of those results would greatly improve readability. Additionally, due to the large amount of mathematical notation used throughout the paper, it would be helpful to include a notation table in the appendix for easy reference.
- The experimental setting is not sufficiently clear, especially in the introduction when the authors refer to Table 1. With regards to the perturbations - are they applied to every state in the trajectory? For masking, is the same mask used for every state, or is the mask also sampled randomly? With regards to injecting encoder error - how to interpret the $\mu_t$ and $\sigma_t$ values?

Limitations:
There is little discussion on the limitations of the analysis. Some points worth discussing could be the impact of various assumptions when deriving the results, the fact that the analysis is mostly focused on a specific setting - learning from pixels using a CNN encoder and an RNN latent dynamics model, and further investigation of the compounding model error problem.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the generalization capability of world models via a stochastic differential equation formulation. They try to understand latent representation errors on generalization, with both zero-drift representation errors and non-zero-drift representation errors. They found that zero drift latent representation errors are implicit regularization and thus bring generalization gain. Jacobian regularization is proposed to enhance training stability and generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ A deep understanding of the generalization of world models via stochastic differential equation formulation;
+ A careful study of the different effects of zero drift and non-zero drift on gn

Weaknesses:
+ The unseen images are produced via global/partial Gaussian noises and rotation, which seems more on the robustness side rather than the generalization of unseen images;

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the generalization capability of world models in reinforcement learning. In particular, they investigate the latent representation error in world models. They show that zero-drift representation error is inherently a regularizer for the learned model functions. On the other hand, they show that the non-zero-drift representation error accumulates errors and Jacobian regularization can be used to alleviate the issue. They demonstrate their proposed approach improves stability, convergence, and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work investigates an interesting aspect, the generalization of world models that learn the dynamics of the environment. Very limited work has been done in this facet of RL, thus it will share significant insights with the DRL research community.

2. The paper followed a structured methodology to analyze the world model and its representation errors. They interpret the learned model function as stochastic differential equations (SDEs) and model the variation as Brownian motions. 

3. I liked the way they theoretically analyzed it case-by-case and established connections with prior findings. 

4. The paper articulately presents the findings of zero-drift error as a regularizer and the Jacobian correction term for non-zero-drift representation error.  It systematically proves its hypotheses and shows evidence against the claims. They presented corresponding formulas and interpretations.

Weaknesses:
1. The paper is very thorough in terms of theoretical derivation. However, in my opinion, the experimental section of the paper is somewhat lacking. It utilizes only two tasks from Mujoco to prove the efficacy of the approach. More diverse tasks from other benchmarks and robust perturbations will certainly improve the paper. 

2. The experimental evaluation is limited to reward comparison. However, it would be interesting to see some visualization of how the trajectories unfold in the case of both types of errors and with Jacobian regularization.

Limitations:
While the paper discusses the potential social impact of the work, it doesn’t discuss any limitations. I believe the characterization of the models as SDE and the use of Brownian motion as variation have certain contributions to the identified claims. Other interpretations may alter the findings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
47CdPNiWUB;"REVIEW 
Summary:
This paper proposes a methodology called Rockafellian Relaxation (RR) to mitigate the impact of labeling errors in neural network training. The method is architecture-independent and integrates concepts from adversarial training to address dataset imperfections robustly. Through theoretical justifications and a series of experiments on standard datasets like MNIST and Toxic Comments, the paper demonstrates that RR can significantly improve the performance of neural networks trained under various corruption levels. The paper’s contributions are particularly valuable as they provide a new tool for improving training accuracy in the presence of label noise, enhancing the robustness and applicability of machine learning models in diverse and error-prone real-world settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper's approach to using Rockafellian Relaxation for addressing labeling errors is innovative, especially the combination with adversarial training concepts.

Quality: The method is grounded in solid theoretical justification, and the empirical results show marked improvements over existing methods.

Clarity: The explanations of the methodologies and the algorithms are clear and detailed, making it easier to understand the operational aspects of the proposed solution.

Significance: The significance of this work lies in its potential to improve training robustness across various domains and dataset imperfections, which is highly relevant for deploying machine learning models in error-prone real-world environments.

Weaknesses:
Computational Complexity: The added complexity might limit the practical application of the method in scenarios with constrained computational resources.

Limitations:
Generalization to Different Noise Types: While the method is tested against uniform label noise, its effectiveness against other types of noise is not thoroughly investigated.

Dependence on Hyperparameter Tuning: The effectiveness of RRM is likely sensitive to the choice of hyperparameters, such as the regularization term and the parameters controlling the adversarial component. The paper does not provide extensive guidance on hyperparameter selection, which could affect the reproducibility and ease of application in different scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a loss reweighting scheme to train models in the presence of label errors. When training an NN with empirical risk minimization in this setting, one would want to assign a weight of zero to all datapoints that are mislabeled and a weight of one to all datapoints that are correctly labeled. This paper presents an automated method for accomplishing this weighting, called the Rockafellian Relaxation Method (RRM). It is noted in Theorem 3.1 that the inner minimization objective of RRM reduces to a linear programming problem, despite RRM being non convex in general. After relating RRM to distributionally robust optimization techniques, the adversarial variant of RRM is introduced (A-RRM), which includes adversarial perturbations to induce adversarial training as well as loss reweighting. Experiments on four datasets show that RRM and A-RRM outperform other methods in both adversarial settings and settings with high proportions of noisy labels.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work addresses two different types of robustness: robustness to label noise and robustness to adversarial feature perturbation. It should be of interest to those who are generally interested in robust and trustworthy machine learning. Furthermore, the proposed training method has strong theoretical foundations, and its relation to other optimization formulations is discussed in detail. The theoretical results are validated in experiments that cover different datasets and types of data corruption.

Weaknesses:
The experimental section lacks a relevant baseline for comparison. As it stands, it is unclear how this compares to other noise-reduction techniques. The relationship to other techniques is discussed in the related work section, it would be nice if the purported benefits of this approach were borne out empirically.

The introduction of adversarial training in section 3.5 is under-motivated. Based on the earlier sections, it is unclear how label and adversarial feature corruptions are related to each other, why we would want to achieve robustness to both, and whether previous approaches have attempted this before. I would suggest explicitly motivating this earlier in the paper.

Limitations:
The limitations are briefly discussed in the paper. As noted above, one main limitation is that it only studies $\ell_\infty$ bounded FGSM attacks. Furthermore, this paper only considers the uniform label noise model, and does not consider the case when label corruption might be correlated with features.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents Rockafellian Relaxation (RR), a new method to address labeling errors in machine learning datasets. RR is a loss reweighting technique that enhances neural network robustness against labeling errors and adversarial attacks, working across various data domains and model architectures. The key contribution is an approach that mitigates label corruption and class imbalance without needing clean validation sets, offering a practical solution for training robust models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper introduces Rockafellian Relaxation (RR), a novel loss reweighting methodology that addresses learning with noisy label problems

- The authors provide a solid theoretical basis for RR, relating it to optimistic and robust distributional optimization formulations. RR is also designed to be architecture-independent, making it a versatile tool applicable across different neural network architectures.

- The method does not rely on having clean validation data, which is of advantage in many real-world applications.

Weaknesses:
- While not explicitly mentioned, the iterative nature of the RR algorithm could potentially be computationally intensive, especially for large datasets.

- The method assumes a specific model of label noise (e.g., uniform label noise), which may not hold in all real-world scenarios. 

- The paper could benefit from a more comprehensive comparison with other state-of-the-art methods for handling noisy labels, such as GCE [1], ELR[2], to better position RR in the existing literature.

[R1] Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels

[R2] Early-Learning Regularization Prevents Memorization of Noisy Labels

Limitations:
Authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a methodology called Rockafellian Relaxation (RR) to mitigate the impact of labeling errors in neural network training. The method is architecture-independent and integrates concepts from adversarial training to address dataset imperfections robustly. Through theoretical justifications and a series of experiments on standard datasets like MNIST and Toxic Comments, the paper demonstrates that RR can significantly improve the performance of neural networks trained under various corruption levels. The paper’s contributions are particularly valuable as they provide a new tool for improving training accuracy in the presence of label noise, enhancing the robustness and applicability of machine learning models in diverse and error-prone real-world settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper's approach to using Rockafellian Relaxation for addressing labeling errors is innovative, especially the combination with adversarial training concepts.

Quality: The method is grounded in solid theoretical justification, and the empirical results show marked improvements over existing methods.

Clarity: The explanations of the methodologies and the algorithms are clear and detailed, making it easier to understand the operational aspects of the proposed solution.

Significance: The significance of this work lies in its potential to improve training robustness across various domains and dataset imperfections, which is highly relevant for deploying machine learning models in error-prone real-world environments.

Weaknesses:
Computational Complexity: The added complexity might limit the practical application of the method in scenarios with constrained computational resources.

Limitations:
Generalization to Different Noise Types: While the method is tested against uniform label noise, its effectiveness against other types of noise is not thoroughly investigated.

Dependence on Hyperparameter Tuning: The effectiveness of RRM is likely sensitive to the choice of hyperparameters, such as the regularization term and the parameters controlling the adversarial component. The paper does not provide extensive guidance on hyperparameter selection, which could affect the reproducibility and ease of application in different scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a loss reweighting scheme to train models in the presence of label errors. When training an NN with empirical risk minimization in this setting, one would want to assign a weight of zero to all datapoints that are mislabeled and a weight of one to all datapoints that are correctly labeled. This paper presents an automated method for accomplishing this weighting, called the Rockafellian Relaxation Method (RRM). It is noted in Theorem 3.1 that the inner minimization objective of RRM reduces to a linear programming problem, despite RRM being non convex in general. After relating RRM to distributionally robust optimization techniques, the adversarial variant of RRM is introduced (A-RRM), which includes adversarial perturbations to induce adversarial training as well as loss reweighting. Experiments on four datasets show that RRM and A-RRM outperform other methods in both adversarial settings and settings with high proportions of noisy labels.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work addresses two different types of robustness: robustness to label noise and robustness to adversarial feature perturbation. It should be of interest to those who are generally interested in robust and trustworthy machine learning. Furthermore, the proposed training method has strong theoretical foundations, and its relation to other optimization formulations is discussed in detail. The theoretical results are validated in experiments that cover different datasets and types of data corruption.

Weaknesses:
The experimental section lacks a relevant baseline for comparison. As it stands, it is unclear how this compares to other noise-reduction techniques. The relationship to other techniques is discussed in the related work section, it would be nice if the purported benefits of this approach were borne out empirically.

The introduction of adversarial training in section 3.5 is under-motivated. Based on the earlier sections, it is unclear how label and adversarial feature corruptions are related to each other, why we would want to achieve robustness to both, and whether previous approaches have attempted this before. I would suggest explicitly motivating this earlier in the paper.

Limitations:
The limitations are briefly discussed in the paper. As noted above, one main limitation is that it only studies $\ell_\infty$ bounded FGSM attacks. Furthermore, this paper only considers the uniform label noise model, and does not consider the case when label corruption might be correlated with features.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents Rockafellian Relaxation (RR), a new method to address labeling errors in machine learning datasets. RR is a loss reweighting technique that enhances neural network robustness against labeling errors and adversarial attacks, working across various data domains and model architectures. The key contribution is an approach that mitigates label corruption and class imbalance without needing clean validation sets, offering a practical solution for training robust models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper introduces Rockafellian Relaxation (RR), a novel loss reweighting methodology that addresses learning with noisy label problems

- The authors provide a solid theoretical basis for RR, relating it to optimistic and robust distributional optimization formulations. RR is also designed to be architecture-independent, making it a versatile tool applicable across different neural network architectures.

- The method does not rely on having clean validation data, which is of advantage in many real-world applications.

Weaknesses:
- While not explicitly mentioned, the iterative nature of the RR algorithm could potentially be computationally intensive, especially for large datasets.

- The method assumes a specific model of label noise (e.g., uniform label noise), which may not hold in all real-world scenarios. 

- The paper could benefit from a more comprehensive comparison with other state-of-the-art methods for handling noisy labels, such as GCE [1], ELR[2], to better position RR in the existing literature.

[R1] Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels

[R2] Early-Learning Regularization Prevents Memorization of Noisy Labels

Limitations:
Authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
476zUsqFZB;"REVIEW 
Summary:
This paper attempts to address the problem of low interpretability of reaction prediction methods by proposing modeling step-wise polar reactions. To model such mechanisms it uses an existing dataset PMechDB. The authors propose an approach to model such reaction by first selecting the right atoms to react from the input molecules using learned models and then react them.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- Several existing chemistry reaction prediction models are benchmarked on the PMechDB dataset.
 - A way to integrate reaction mechanism information is introduced.

Weaknesses:
- The paper has substantial clarity problems: 
  - Table captions are insufficiently informative, requires going deeper into the text to understand what results are actually presented (e.g. 'Table 3: Top-N Accuracy of Trained Models').
  - Figures 5 and 6 are formatted inconsistently with the rest of the file.
- Citation quality is poor:
  - Could provide more references to prior work overall. e.g. section 3.3 describes prior work on sequence to sequence modelling without any references. 
  - PMechDB is introduced in a way that makes it unclear, whether the database is a contribution of this work or not.
- Novelty is not prominent. Method in [18] (OrbChain) is already working with similar task on a similar dataset.
- Evaluation is insufficient:
  - Source code for reproduction has not been provided.
  - The resulting models have not been evaluated on the global datasets, making it unclear whether the fine tuning as specified in this work improves the performance in general rather than on the test set of PMechDB.
  - Error bars are not provided.
  - Not benchmarked against a comparable method, referenced in [18].

Limitations:
The authors address some of the limitations of their work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Current reaction prediction models lack interpretability for chemical reaction prediction. This paper evaluates the various machine learning models on the PMechDB dataset which contains polar elementary steps. Besides, this paper proposes a new system: PMechRP, which achieves the highest top-5 accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths:  
1. A new benchmark has been introduced, which improves the interpretability and causality of a chemical reaction.

2. Several methods are evaluated.

Weaknesses:
Weaknesses:

1. This paper seems like a technique report.

2. The main conference track is not suitable for this paper. I think the dataset & benchmark track is more suitable.

3. Writing is poor.

Limitations:
N/A

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Previous reaction prediction models formulate the forward chemical reactions in an end-to-end manner, which only considers the input state and output state while ignoring intermediate states describing the electron redistribution changes. This work tries different models on a new benchmark dataset PMechDB. Experimental results demonstrate the effectiveness of the transition state information in the new benchmark dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The motivation is quite clear. This reviewer agrees with the importance of the exploration of intermediate electron transfer. This is particularly important for the chemical reaction simulation, benefitting the understanding of reaction mechanisms.

Weaknesses:
(1) The technical contribution of this work is very limited. This reviewer does not see enough improvements from the algorithm side. Also, it seems the dataset is not proposed by this work. The contribution of this work is overall limited.

(2) If this work intends to propose a new benchmark, then much more comprehensive reaction models should be covered. Currently, two important reaction models are not discussed: ""non-autoregressive electron redistribution modeling for reaction predictions"" and ""A Generative Model For Electron Paths."" In addition, the evaluation metric and the new task are not clearly described. More detailed descriptions should be provided for clarity.

(3) The presentation of this work is not very clear. This reviewer does not fully understand how the multi-step information helps the reaction modeling. A good example illustrating the significance of the intermediate step information is required. At this stage, this reviewer thinks the multi-step transition information can be easily captured by recursive modeling of single-step reaction models. Currently, this reviewer does not see what new challenges are brought by the intermediate step.

(4) This work is very similar to the published paper ""AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning."" This reviewer does not see many differences between the submitted work and this prior work.

Limitations:
Constructing the benchmark dataset with ground-truth multi-step electron transition states is very hard. This may hinder the further development of this direction.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
the paper describes a new approach to predict polar reaction mechanisms, which is the most important class of chemical reaction mechanisms. this can be quite useful for chemical reaction prediction.


this reviewers rating is based on the current presentation of the manuscript, if the authors are willing to enhance the clarity of the manuscript, this reviewer is willing to increase their score.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Addressing an underexplored but important problem
- decent results
- interesting results with pre-trained methods, that in some cases are surprising (T5 seem to work not as well despite multi-task pretraining)

Weaknesses:
- Model and data processing descriptions are quite short and should be expanded, and presented coherently in one location in the manuscript. From the description in the manuscript I would likely not be able to re-implement the method
- It is not immediately clear which ensemble is shown in table 4
- maybe not so much innovation from the ML side?

Limitations:
ok

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper attempts to address the problem of low interpretability of reaction prediction methods by proposing modeling step-wise polar reactions. To model such mechanisms it uses an existing dataset PMechDB. The authors propose an approach to model such reaction by first selecting the right atoms to react from the input molecules using learned models and then react them.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- Several existing chemistry reaction prediction models are benchmarked on the PMechDB dataset.
 - A way to integrate reaction mechanism information is introduced.

Weaknesses:
- The paper has substantial clarity problems: 
  - Table captions are insufficiently informative, requires going deeper into the text to understand what results are actually presented (e.g. 'Table 3: Top-N Accuracy of Trained Models').
  - Figures 5 and 6 are formatted inconsistently with the rest of the file.
- Citation quality is poor:
  - Could provide more references to prior work overall. e.g. section 3.3 describes prior work on sequence to sequence modelling without any references. 
  - PMechDB is introduced in a way that makes it unclear, whether the database is a contribution of this work or not.
- Novelty is not prominent. Method in [18] (OrbChain) is already working with similar task on a similar dataset.
- Evaluation is insufficient:
  - Source code for reproduction has not been provided.
  - The resulting models have not been evaluated on the global datasets, making it unclear whether the fine tuning as specified in this work improves the performance in general rather than on the test set of PMechDB.
  - Error bars are not provided.
  - Not benchmarked against a comparable method, referenced in [18].

Limitations:
The authors address some of the limitations of their work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Current reaction prediction models lack interpretability for chemical reaction prediction. This paper evaluates the various machine learning models on the PMechDB dataset which contains polar elementary steps. Besides, this paper proposes a new system: PMechRP, which achieves the highest top-5 accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths:  
1. A new benchmark has been introduced, which improves the interpretability and causality of a chemical reaction.

2. Several methods are evaluated.

Weaknesses:
Weaknesses:

1. This paper seems like a technique report.

2. The main conference track is not suitable for this paper. I think the dataset & benchmark track is more suitable.

3. Writing is poor.

Limitations:
N/A

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Previous reaction prediction models formulate the forward chemical reactions in an end-to-end manner, which only considers the input state and output state while ignoring intermediate states describing the electron redistribution changes. This work tries different models on a new benchmark dataset PMechDB. Experimental results demonstrate the effectiveness of the transition state information in the new benchmark dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The motivation is quite clear. This reviewer agrees with the importance of the exploration of intermediate electron transfer. This is particularly important for the chemical reaction simulation, benefitting the understanding of reaction mechanisms.

Weaknesses:
(1) The technical contribution of this work is very limited. This reviewer does not see enough improvements from the algorithm side. Also, it seems the dataset is not proposed by this work. The contribution of this work is overall limited.

(2) If this work intends to propose a new benchmark, then much more comprehensive reaction models should be covered. Currently, two important reaction models are not discussed: ""non-autoregressive electron redistribution modeling for reaction predictions"" and ""A Generative Model For Electron Paths."" In addition, the evaluation metric and the new task are not clearly described. More detailed descriptions should be provided for clarity.

(3) The presentation of this work is not very clear. This reviewer does not fully understand how the multi-step information helps the reaction modeling. A good example illustrating the significance of the intermediate step information is required. At this stage, this reviewer thinks the multi-step transition information can be easily captured by recursive modeling of single-step reaction models. Currently, this reviewer does not see what new challenges are brought by the intermediate step.

(4) This work is very similar to the published paper ""AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning."" This reviewer does not see many differences between the submitted work and this prior work.

Limitations:
Constructing the benchmark dataset with ground-truth multi-step electron transition states is very hard. This may hinder the further development of this direction.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
the paper describes a new approach to predict polar reaction mechanisms, which is the most important class of chemical reaction mechanisms. this can be quite useful for chemical reaction prediction.


this reviewers rating is based on the current presentation of the manuscript, if the authors are willing to enhance the clarity of the manuscript, this reviewer is willing to increase their score.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Addressing an underexplored but important problem
- decent results
- interesting results with pre-trained methods, that in some cases are surprising (T5 seem to work not as well despite multi-task pretraining)

Weaknesses:
- Model and data processing descriptions are quite short and should be expanded, and presented coherently in one location in the manuscript. From the description in the manuscript I would likely not be able to re-implement the method
- It is not immediately clear which ensemble is shown in table 4
- maybe not so much innovation from the ML side?

Limitations:
ok

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
3gZBGBglBf;"REVIEW 
Summary:
The paper highlights how temporal autocorrelations in EEG data can lead to misleadingly high decoding accuracy in brain-computer interface (BCI) tasks. Using a novel approach with a ""watermelon EEG dataset,"" the authors demonstrate that many reported high performances may exploit these autocorrelations rather than genuine neural activity. They propose a unified framework to address this issue across various EEG tasks and recommend improved experimental designs and data splitting strategies to ensure more accurate and reliable results in BCI research.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Novel Problem Formulation: The paper introduces a novel problem formulation by addressing the potential overestimation of decoding performance in EEG-based brain-computer interfaces (BCIs) due to temporal autocorrelations. This is an innovative perspective that has not been extensively explored in prior research.

2. Creative Use of Non-Human Subjects: The use of watermelons as a model to eliminate stimulus-driven neural responses is highly original. This approach allows for the isolation of temporal autocorrelation effects in EEG signals, providing a unique method to investigate the problem.

3. Impact on BCI Research: The findings have significant implications for BCI research, highlighting a critical issue that could affect the validity of many existing studies. By identifying and addressing this pitfall, the paper provides good insight for more accurate and reliable BCI systems.

Weaknesses:
Plz go and check questions.

Limitations:
While the authors recommend avoiding certain data splitting strategies, the practical implications and feasibility of implementing alternative strategies in real-world BCI applications are not fully explored.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper investigates the potential overestimation of decoding accuracy in brain-computer interface (BCI) tasks that utilize EEG signals. The authors address concerns that high reported decoding accuracies may be attributed to the inherent temporal autocorrelation present in EEG signals rather than the actual decoding of neural responses to stimuli. It contributes to the field of BCI by identifying a potential source of bias in decoding performance, providing a novel dataset to study this issue, and emphasizing the need for careful experimental design to ensure the robustness and reliability of BCI systems.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This article explores the issue of Overestimated Decoding Performance Arising from Temporal Autocorrelations and verifies it through experiments, with both the expressed viewpoint and experimental process offering high enlightenment value to the BCI community.

2. The self-collected Watermelon EEG is interesting. The use of Watermelon EEG dataset to simulate EEG data without neural activity is a good method to isolate the effects of temporal autocorrelations. 

3. The paper provides empirical evidence through experiments that show high decoding accuracies can be achieved even with non-neural datasets, suggesting that reported accuracies in BCI might be influenced by factors other than the models' ability to interpret neural information. The experiment is solid.

Weaknesses:
1. This article only covers image decoding, emotion recognition, and ASAD tasks, and to further substantiate the viewpoint presented in this paper, the use of more other tasks or datasets is recommended.

2. The presentation still needs improvement, such as Figures 1 and 2. Some technical terms may be ambiguous, such as “domain”, and should be given more rigorous and clear definitions.

3. The paper only uses a simple CNN (or some parts of this CNN) for EEG classification. A broader range of model testing (e.g. EEGNet and EEG Conformer) would contribute to enhancing the reliability of the research presented in this paper.

Limitations:
The authors have addressed some limitations.But there are still some questions. Please see the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors have correctly identified a significant issue of numerous hyperbolic or irreproducible results in EEG decoding or classification tasks. However, their evaluation approach of recording signals from electrodes placed on a watermelon needs correction. The authors are advised to consult the definition of EEG, as a watermelon is not a brain and does not generate any electrical signals. Therefore, the recorded electrical noises, even when amplified using equipment typically used for EEG, do not constitute EEG data. In summary, while the authors' intentions were good, the numerous errors in their approach make it unacceptable for publication at a top conference such as NeurIPS.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
An excellent intention to discuss problems with many overblown EEG decoding publications. Yet the conclusions are obvious and many reputable researchers defend their approaches with leave-one-out-subject evaluations to avoid the obvious issues in training and testing data splitting identified by the authors.

Weaknesses:
There were unacceptable errors in using EEG terms since instead some environmental or amplifier Brownian noises were recorded after placing electrodes on a watermelon, which probably acted as an electromagnetic antenna capturing all possible low-frequency noises in a room. The CNN application with data splitting issues is too basic for NeurIPS.

Limitations:
Watermelon cannot produce EEG, even if an EEG amplifier records some electrical noise.
The presented study thus hardly relates to EEG decoding problems but seems to report on obvious issues in machine learning due to erroneous data splitting into training and testing sets, thus making it too trivial for NeurIPS.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Authors hypothesise that the high temporal correlation of EEG data contributes to the high BCI decoding accuracies reported in some prior BCI studies. Specifically, the highly questionable data partitioning practice of splitting continuous EEG data with the same label (or subject) across train/test sets. They present a framework to assess the impact of temporal correlation of EEG features on three different BCI decoding tasks applied to independent datasets, human and watermelon (phantom) EEG data. The inclusion of watermelon dataset is to separate the influence of stimulus-driven responses from highly correlated temporal EEG features that is not fully eliminated when using human EEG data. Results based on the standard data partitioning show high BCI decoding performance for the various tasks even when using watermelon EEG data, and performance is significantly reduced to around chance level when the impact of temporal autocorrelation is mitigated with alternative data partitioning schemes.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality**

-	The inclusion of “phantom EEG” recorded from watermelon to disambiguate stimulus-driven neural responses and temporal autocorrelation during the data analysis, which is not fully eliminated with human EEG recordings. 

**Quality**

-	Authors provide a theoretical basis to justify their hypotheses and experiment design plan. 
-	Analysis plan includes data partitioning used in different BCI decoding tasks (image classification, emotion recognition, auditory spatial attention) applied to a different BCI task (speech evoked response).  

**Clarity**

-	The paper is generally well-written. Problem well illustrated in Figure 1.
-	Some areas require clarity (maybe figures?) to better illustrate the different analyses in the framework (for other applications) and results.

**Significance**

-	Highlights the need for more robust experimental design and data partitioning practices in BCI decoding tasks to minimise the impact of inherent temporal correlations of EEG data on performance.
-	The paper demonstrates a limitation of deep learning models (“black box”) in relation to correlation vs. causation.

Weaknesses:
•	Adding the performance of the current framework on the actual datasets (CVPR, DEAP, KUL, if publicly available), as well as additional independent BCI datasets would provide other benchmarks for comparison. 

•	Not sure why there is a need to match number of the subjects in the SparrKULee dataset to that of the WM “subjects”. The objective of the study is to provide a framework for exploring the impact of temporal correlation of EEG features on BCI decoding performance, not directly comparing both datasets. So, it is fine to include data from all subjects in SparrKULee database. 
> To match the number of subjects in the Watermelon EEG Dataset, EEG data from 10 subjects… from the SparrKULee Dataset were used.

Limitations:
Authors acknowledge limitations of their work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper highlights how temporal autocorrelations in EEG data can lead to misleadingly high decoding accuracy in brain-computer interface (BCI) tasks. Using a novel approach with a ""watermelon EEG dataset,"" the authors demonstrate that many reported high performances may exploit these autocorrelations rather than genuine neural activity. They propose a unified framework to address this issue across various EEG tasks and recommend improved experimental designs and data splitting strategies to ensure more accurate and reliable results in BCI research.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Novel Problem Formulation: The paper introduces a novel problem formulation by addressing the potential overestimation of decoding performance in EEG-based brain-computer interfaces (BCIs) due to temporal autocorrelations. This is an innovative perspective that has not been extensively explored in prior research.

2. Creative Use of Non-Human Subjects: The use of watermelons as a model to eliminate stimulus-driven neural responses is highly original. This approach allows for the isolation of temporal autocorrelation effects in EEG signals, providing a unique method to investigate the problem.

3. Impact on BCI Research: The findings have significant implications for BCI research, highlighting a critical issue that could affect the validity of many existing studies. By identifying and addressing this pitfall, the paper provides good insight for more accurate and reliable BCI systems.

Weaknesses:
Plz go and check questions.

Limitations:
While the authors recommend avoiding certain data splitting strategies, the practical implications and feasibility of implementing alternative strategies in real-world BCI applications are not fully explored.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper investigates the potential overestimation of decoding accuracy in brain-computer interface (BCI) tasks that utilize EEG signals. The authors address concerns that high reported decoding accuracies may be attributed to the inherent temporal autocorrelation present in EEG signals rather than the actual decoding of neural responses to stimuli. It contributes to the field of BCI by identifying a potential source of bias in decoding performance, providing a novel dataset to study this issue, and emphasizing the need for careful experimental design to ensure the robustness and reliability of BCI systems.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This article explores the issue of Overestimated Decoding Performance Arising from Temporal Autocorrelations and verifies it through experiments, with both the expressed viewpoint and experimental process offering high enlightenment value to the BCI community.

2. The self-collected Watermelon EEG is interesting. The use of Watermelon EEG dataset to simulate EEG data without neural activity is a good method to isolate the effects of temporal autocorrelations. 

3. The paper provides empirical evidence through experiments that show high decoding accuracies can be achieved even with non-neural datasets, suggesting that reported accuracies in BCI might be influenced by factors other than the models' ability to interpret neural information. The experiment is solid.

Weaknesses:
1. This article only covers image decoding, emotion recognition, and ASAD tasks, and to further substantiate the viewpoint presented in this paper, the use of more other tasks or datasets is recommended.

2. The presentation still needs improvement, such as Figures 1 and 2. Some technical terms may be ambiguous, such as “domain”, and should be given more rigorous and clear definitions.

3. The paper only uses a simple CNN (or some parts of this CNN) for EEG classification. A broader range of model testing (e.g. EEGNet and EEG Conformer) would contribute to enhancing the reliability of the research presented in this paper.

Limitations:
The authors have addressed some limitations.But there are still some questions. Please see the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors have correctly identified a significant issue of numerous hyperbolic or irreproducible results in EEG decoding or classification tasks. However, their evaluation approach of recording signals from electrodes placed on a watermelon needs correction. The authors are advised to consult the definition of EEG, as a watermelon is not a brain and does not generate any electrical signals. Therefore, the recorded electrical noises, even when amplified using equipment typically used for EEG, do not constitute EEG data. In summary, while the authors' intentions were good, the numerous errors in their approach make it unacceptable for publication at a top conference such as NeurIPS.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
An excellent intention to discuss problems with many overblown EEG decoding publications. Yet the conclusions are obvious and many reputable researchers defend their approaches with leave-one-out-subject evaluations to avoid the obvious issues in training and testing data splitting identified by the authors.

Weaknesses:
There were unacceptable errors in using EEG terms since instead some environmental or amplifier Brownian noises were recorded after placing electrodes on a watermelon, which probably acted as an electromagnetic antenna capturing all possible low-frequency noises in a room. The CNN application with data splitting issues is too basic for NeurIPS.

Limitations:
Watermelon cannot produce EEG, even if an EEG amplifier records some electrical noise.
The presented study thus hardly relates to EEG decoding problems but seems to report on obvious issues in machine learning due to erroneous data splitting into training and testing sets, thus making it too trivial for NeurIPS.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Authors hypothesise that the high temporal correlation of EEG data contributes to the high BCI decoding accuracies reported in some prior BCI studies. Specifically, the highly questionable data partitioning practice of splitting continuous EEG data with the same label (or subject) across train/test sets. They present a framework to assess the impact of temporal correlation of EEG features on three different BCI decoding tasks applied to independent datasets, human and watermelon (phantom) EEG data. The inclusion of watermelon dataset is to separate the influence of stimulus-driven responses from highly correlated temporal EEG features that is not fully eliminated when using human EEG data. Results based on the standard data partitioning show high BCI decoding performance for the various tasks even when using watermelon EEG data, and performance is significantly reduced to around chance level when the impact of temporal autocorrelation is mitigated with alternative data partitioning schemes.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality**

-	The inclusion of “phantom EEG” recorded from watermelon to disambiguate stimulus-driven neural responses and temporal autocorrelation during the data analysis, which is not fully eliminated with human EEG recordings. 

**Quality**

-	Authors provide a theoretical basis to justify their hypotheses and experiment design plan. 
-	Analysis plan includes data partitioning used in different BCI decoding tasks (image classification, emotion recognition, auditory spatial attention) applied to a different BCI task (speech evoked response).  

**Clarity**

-	The paper is generally well-written. Problem well illustrated in Figure 1.
-	Some areas require clarity (maybe figures?) to better illustrate the different analyses in the framework (for other applications) and results.

**Significance**

-	Highlights the need for more robust experimental design and data partitioning practices in BCI decoding tasks to minimise the impact of inherent temporal correlations of EEG data on performance.
-	The paper demonstrates a limitation of deep learning models (“black box”) in relation to correlation vs. causation.

Weaknesses:
•	Adding the performance of the current framework on the actual datasets (CVPR, DEAP, KUL, if publicly available), as well as additional independent BCI datasets would provide other benchmarks for comparison. 

•	Not sure why there is a need to match number of the subjects in the SparrKULee dataset to that of the WM “subjects”. The objective of the study is to provide a framework for exploring the impact of temporal correlation of EEG features on BCI decoding performance, not directly comparing both datasets. So, it is fine to include data from all subjects in SparrKULee database. 
> To match the number of subjects in the Watermelon EEG Dataset, EEG data from 10 subjects… from the SparrKULee Dataset were used.

Limitations:
Authors acknowledge limitations of their work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
3Rtn1OMTC4;"REVIEW 
Summary:
This paper studies how to extract useful visual features from out-of-domain and action-free human videos to enhance robotic visualmotor control. Specifically, the authors argure that naively extracting spatial features via MAE is insufficient for robotics control, in contrast, jointly captureing spatial control and temporal movement will be more effective. To do so, the authors propose STP, a new self-supervised learning method, that simutaneously performs MAE on current frame to extract spatial information and predict furture frames to extract temporal motion clues. The overall motivation, idea and method are straightforward and reasonable. The authors evaluate STP on diverse benchmarks including 21 tasks spanning from simulation to real world tasks using imitation learning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-motivated, highlighting the importance of pre-training visual features for robotic foundation models.

2. The logic in the paper is clear and easy to follow.


3. The proposed method is straightforward and simple to implement.

Weaknesses:
1. The high costs associated with evaluating real-world tasks using different random seeds make it challenging to report variances. However, assessing the impact of multiple random seeds in simulated tasks could provide more reliable statistical insights. As shown in Table 1, STP's performance improvement over baselines is marginal (STP 63.7 vs. VC-1 61.8, and STP-L/16(Post PT) 78.4 vs. MAE-L/16(Post PT) 76.7). Given the inherent stochastic nature of imitation learning and reinforcement learning, evaluations across multiple episodes and various random seeds are crucial to validate the proposed methods effectively.

2. Some previous methods also consider the temporal movements when extracting the visual features. For instance, the video-language alignment loss in R3M [1] tries to align language with correct visual transitions, which can extract semantic informations about visual movements. Voltron[2] and DecisionNCE [3] also try to extract the semantic features of the temporal movements between two frames. VIP[3] and LIV[4] use RL to extract visual features, which may also capture long-term movements via bootstrapping. Therefore, the authors could strengthen their paper by highlighting these related works, demonstrating awareness of existing methods, situating their contributions and highlighting the differences between STP and these baselines.

[1] R3M: A Universal Visual Representation for Robot Manipulation. CoRL 2023

[2] Language-Driven Representation Learning for Robotics. RSS 2023.

[3] DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning. ICML 2024.

[4] VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. ICLR 2023.

[5] LIV: Language-Image Representations and Rewards for Robotic Control. ICML 2023

Limitations:
The authors have properly discussed the limiations in the Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a new spatio-temporal pretraining algorithm for representation learning for robotics. The authors propose using masked autoencoding for reconstructing the current frame (for spatial reasoning) and a future frame (for temporal reasoning). The authors provide extensive experimentation across simulated and real-world settings and provide ablation studies to justify their design choices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper addresses the important topic of including temporal dynamics in video data for pretraining robot representations.
- The paper does a good job of explaining the method and detailing the various experimental settings.
- The authors provide policy performance using both the pre-trained representations and post-pre-trained representations which helps assess both the quality of representations learned from internet data as well as the advantage of finetuning representations on the task-specific data. Overall, the proposed method has been extensively evaluated over varied settings across a variety of simulated settings.
- The authors provide an insightful ablation study to justify their design choices.

Weaknesses:
- It is unclear where the diverse image data for STP trained with Ego+I in Table 1 is obtained from. Some information about this would be helpful.
- The real-world experiments seem limited with only two real-world tasks where the MAE also performs reasonably well.
- The authors must include comparisons with prior works using MAE for spatiotemporal learning [1].

[1] Feichtenhofer, Christoph, Yanghao Li, and Kaiming He. ""Masked autoencoders as spatiotemporal learners."" Advances in neural information processing systems 35 (2022): 35946-35958.

Limitations:
The limitations have been addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes STP, a visual representation learning method for robotic motor control. Trained on human videos, STP uses masked auto-encoders for spatial-temporal prediction. The spatial decoder predicts the current frame from its representation with 75% of patches masked. The temporal decoder predicts the future frame using the representations of 75%-masked current frame and the 95%-masked future frame. Experiments on various simulation and real-world tasks show the effectiveness of STP compared with baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed method is simple yet effective, utilizing a masked spatial-temporal prediction objective to learn visual representations for robotics.
2. The paper presents extensive experimental results in both simulation and real-world settings, comparing with proper visual representation baselines.

Weaknesses:
1. Many works have considered temporal information for robot visual representation learning. This paper should mention these and highlight the differences. For example, R3M [1] uses temporal contrastive learning, while VIP [2] and V-PTR [3] use temporal difference.
2. Though STP outperforms the baselines in many benchmarks, the performance gap is not significant (Table 1). The slight performance difference may be due to hyperparameter selection and randomness, as the paper did not provide error bars over multiple seeds.

[1] R3m: A universal visual representation for robot manipulation, 2023
[2] Vip: Towards universal visual reward and representation via value-implicit pre-training, 2022
[3] Robotic Offline RL from Internet Videos via Value-Function Pre-Training, 2023

Limitations:
The authors have discussed the limitations. These cannot be addressed within the scope of this paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, we present a self-supervised pre-trained visual representation in robotic motor control, with spatiotemporal prediction with dual decoders, utilizing large-scale video data. The spatial prediction follows a standard MAE pipeline, and the temporal prediction tries to predict the future based on the current frame. The trained encoder is applied to downstream tasks and real-world robot task for better sample efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper adopts actionless human video data for representation learning, which can be easily obtained. The learned representation can be adapted to downstream robotics tasks. 

2. The experiments contain several real-world tasks, which could be more valuable for applying a pre-trained visual encoder to real-world domains that lack data.

Weaknesses:
1. The major concern is the novelty of the previous methods, considering several related papers that leverage human data and visuals pertaining to downstream tasks have been proposed [1-3].

2. The experiment only contains imitation learning experiments in downstream tasks, while the reinforcement learning framework with sub-optimal data is not considered. 

[1] Learning Manipulation by Predicting Interaction. RSS 2024

[2] Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning. https://arxiv.org/html/2402.14407

[3] Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation. https://arxiv.org/abs/2312.13139

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies how to extract useful visual features from out-of-domain and action-free human videos to enhance robotic visualmotor control. Specifically, the authors argure that naively extracting spatial features via MAE is insufficient for robotics control, in contrast, jointly captureing spatial control and temporal movement will be more effective. To do so, the authors propose STP, a new self-supervised learning method, that simutaneously performs MAE on current frame to extract spatial information and predict furture frames to extract temporal motion clues. The overall motivation, idea and method are straightforward and reasonable. The authors evaluate STP on diverse benchmarks including 21 tasks spanning from simulation to real world tasks using imitation learning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-motivated, highlighting the importance of pre-training visual features for robotic foundation models.

2. The logic in the paper is clear and easy to follow.


3. The proposed method is straightforward and simple to implement.

Weaknesses:
1. The high costs associated with evaluating real-world tasks using different random seeds make it challenging to report variances. However, assessing the impact of multiple random seeds in simulated tasks could provide more reliable statistical insights. As shown in Table 1, STP's performance improvement over baselines is marginal (STP 63.7 vs. VC-1 61.8, and STP-L/16(Post PT) 78.4 vs. MAE-L/16(Post PT) 76.7). Given the inherent stochastic nature of imitation learning and reinforcement learning, evaluations across multiple episodes and various random seeds are crucial to validate the proposed methods effectively.

2. Some previous methods also consider the temporal movements when extracting the visual features. For instance, the video-language alignment loss in R3M [1] tries to align language with correct visual transitions, which can extract semantic informations about visual movements. Voltron[2] and DecisionNCE [3] also try to extract the semantic features of the temporal movements between two frames. VIP[3] and LIV[4] use RL to extract visual features, which may also capture long-term movements via bootstrapping. Therefore, the authors could strengthen their paper by highlighting these related works, demonstrating awareness of existing methods, situating their contributions and highlighting the differences between STP and these baselines.

[1] R3M: A Universal Visual Representation for Robot Manipulation. CoRL 2023

[2] Language-Driven Representation Learning for Robotics. RSS 2023.

[3] DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning. ICML 2024.

[4] VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. ICLR 2023.

[5] LIV: Language-Image Representations and Rewards for Robotic Control. ICML 2023

Limitations:
The authors have properly discussed the limiations in the Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a new spatio-temporal pretraining algorithm for representation learning for robotics. The authors propose using masked autoencoding for reconstructing the current frame (for spatial reasoning) and a future frame (for temporal reasoning). The authors provide extensive experimentation across simulated and real-world settings and provide ablation studies to justify their design choices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper addresses the important topic of including temporal dynamics in video data for pretraining robot representations.
- The paper does a good job of explaining the method and detailing the various experimental settings.
- The authors provide policy performance using both the pre-trained representations and post-pre-trained representations which helps assess both the quality of representations learned from internet data as well as the advantage of finetuning representations on the task-specific data. Overall, the proposed method has been extensively evaluated over varied settings across a variety of simulated settings.
- The authors provide an insightful ablation study to justify their design choices.

Weaknesses:
- It is unclear where the diverse image data for STP trained with Ego+I in Table 1 is obtained from. Some information about this would be helpful.
- The real-world experiments seem limited with only two real-world tasks where the MAE also performs reasonably well.
- The authors must include comparisons with prior works using MAE for spatiotemporal learning [1].

[1] Feichtenhofer, Christoph, Yanghao Li, and Kaiming He. ""Masked autoencoders as spatiotemporal learners."" Advances in neural information processing systems 35 (2022): 35946-35958.

Limitations:
The limitations have been addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes STP, a visual representation learning method for robotic motor control. Trained on human videos, STP uses masked auto-encoders for spatial-temporal prediction. The spatial decoder predicts the current frame from its representation with 75% of patches masked. The temporal decoder predicts the future frame using the representations of 75%-masked current frame and the 95%-masked future frame. Experiments on various simulation and real-world tasks show the effectiveness of STP compared with baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed method is simple yet effective, utilizing a masked spatial-temporal prediction objective to learn visual representations for robotics.
2. The paper presents extensive experimental results in both simulation and real-world settings, comparing with proper visual representation baselines.

Weaknesses:
1. Many works have considered temporal information for robot visual representation learning. This paper should mention these and highlight the differences. For example, R3M [1] uses temporal contrastive learning, while VIP [2] and V-PTR [3] use temporal difference.
2. Though STP outperforms the baselines in many benchmarks, the performance gap is not significant (Table 1). The slight performance difference may be due to hyperparameter selection and randomness, as the paper did not provide error bars over multiple seeds.

[1] R3m: A universal visual representation for robot manipulation, 2023
[2] Vip: Towards universal visual reward and representation via value-implicit pre-training, 2022
[3] Robotic Offline RL from Internet Videos via Value-Function Pre-Training, 2023

Limitations:
The authors have discussed the limitations. These cannot be addressed within the scope of this paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, we present a self-supervised pre-trained visual representation in robotic motor control, with spatiotemporal prediction with dual decoders, utilizing large-scale video data. The spatial prediction follows a standard MAE pipeline, and the temporal prediction tries to predict the future based on the current frame. The trained encoder is applied to downstream tasks and real-world robot task for better sample efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper adopts actionless human video data for representation learning, which can be easily obtained. The learned representation can be adapted to downstream robotics tasks. 

2. The experiments contain several real-world tasks, which could be more valuable for applying a pre-trained visual encoder to real-world domains that lack data.

Weaknesses:
1. The major concern is the novelty of the previous methods, considering several related papers that leverage human data and visuals pertaining to downstream tasks have been proposed [1-3].

2. The experiment only contains imitation learning experiments in downstream tasks, while the reinforcement learning framework with sub-optimal data is not considered. 

[1] Learning Manipulation by Predicting Interaction. RSS 2024

[2] Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning. https://arxiv.org/html/2402.14407

[3] Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation. https://arxiv.org/abs/2312.13139

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
34dHGTri2w;"REVIEW 
Summary:
This paper presents a novel parallel sampling method named ""Follow Hamiltonian Leader"" (FHL) designed to address sampling challenges by leveraging zeroth-order information, particularly when first-order data is unreliable or unavailable. The method incorporates a leader-guiding mechanism to enhance the efficiency and effectiveness of the sampling process. Experimental results indicate that FHL significantly improves the exploration of target distributions and outperforms traditional sampling techniques, especially in scenarios involving corrupted gradients.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Innovative combination of zeroth and first-order information.
2. The effectiveness of the method is demonstrated in multiple task scenarios.
3. Theoretical analysis and prove are sufficient.

Weaknesses:
1. Is there any quantitative experiments like evaluating FID and IS on cifar10 datasets and I think it's more compelling whether a novel sampling methods combined with generative models can be used on image datasets with more complex distributions.
2. Lack of experiment of OOD in combination with EBMs or score-based models to valid the stability during sampling with proposed method.

Limitations:
1. Limited exploration of integration with other advanced MCMC methods.
2. Lack of quantitative experiments to demonstrate the advantage of proposed sampling method compared with other methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an interesting parallel sampling method that leverages zeroth-order information to address challenges in sampling from probability distributions, particularly when first-order data is unreliable or unavailable. The method incorporates a leader-guiding mechanism, enhancing efficiency and effectiveness by connecting multiple sampling instances through a selected leader. The proposed method, named Follow Hamiltonian Leader (FHL), extends the Hamiltonian Monte Carlo (HMC) framework by concurrently running multiple replicas at different energy levels and combining both zeroth and first-order information from various chains. Experimental results demonstrate that FHL significantly improves the exploration of target distributions and produces higher-quality outcomes compared to traditional sampling techniques, showing resilience against corrupted gradients and excelling in scenarios characterized by instability, metastability, and pseudo-stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed Follow Hamiltonian Leader (FHL) method markedly improves the efficiency and effectiveness of sampling processes, significantly expediting the exploration of target distributions and producing superior quality outcomes compared to traditional sampling techniques.
- FHL demonstrates greater resilience against the detrimental impacts of corrupted gradients by incorporating zeroth-order information. This robustness makes the method particularly valuable in scenarios where first-order information is compromised, ensuring more reliable and accurate sampling.

Weaknesses:
- The proposed FHL method involves intricate modifications to the traditional Hamiltonian Monte Carlo framework, such as the leader-guiding mechanism and elastic leapfrog technique, which may increase the complexity of implementation and require significant computational resources.
- The effectiveness of the FHL method heavily relies on the appropriate selection of the leader particle. If the leader is not accurately chosen, it could lead to suboptimal sampling performance, potentially compromising the overall efficiency and accuracy of the method.
- While the paper presents experimental results to demonstrate the efficacy of the FHL method, there is a lack of in-depth theoretical analysis to rigorously establish the convergence properties and performance guarantees of the proposed approach.
- The method’s scalability to high-dimensional problems or extremely large datasets is not thoroughly addressed. The parallel sampling approach may encounter challenges in maintaining efficiency and effectiveness as the dimensionality and size of the data increase.

Limitations:
The authors have not adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes to incorporate the energy $U$ into the gradient-based sampling techniques. In particular, it proposes to choose the lowest energy particle as the leader and then add an extra elastic tension between the leader and followers in the Hamiltonian Monte Carlo method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea is simple and clear, the toy examples are easy to understand and demonstrate the benefit of the proposed method well. In addition, the authors conduct experiments for each of the three challenging sampling scenarios identified by the authors.

Weaknesses:
It might worth including the overhead of the proposed method, how much slower the algorithm is per iteration compared to HMC for instance.

The tension coefficient $\lambda$ is critical, setting it to 0 recovers the baseline. But I did not find an ablation over the $\lambda$, is it hard to choose? From my understanding, if you set $\lambda$ pretty large it might recover something like gradient descent and the sampling will collapse.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an interesting new approach for improving sampling methods for energy-based generative models and score-matching models. The key idea is to incorporate zeroth-order information (energy values) in addition to the typical first-order gradient information used by most sampling algorithms like Hamiltonian Monte Carlo (HMC).

The authors identify several challenging scenarios where relying solely on gradients can be problematic - cases of instability, metastability, and pseudo-stability. They argue that incorporating energy values can help mitigate issues in these situations and improve sampling efficiency and quality.

Overall, the core idea of leveraging zeroth-order information in addition to gradients is quite novel and the FHL algorithm is an elegant way to implement this for improving sampling efficiency and quality. The paper is well-motivated, the method is clearly explained, and the empirical results are compelling.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Novel idea of incorporating zeroth-order energy information into sampling algorithms like HMC, which typically only use gradients. This can help address issues like instability and metastability.

2. The Follow Hamiltonian Leader (FHL) algorithm is an elegant way to exchange both energy and gradient information across parallel sampling chains in a principled manner.

3. Thorough experimental evaluation across synthetic examples illustrating the identified challenging scenarios of instability, metastability, and pseudo-stability.

4. Promising results showing improved sampling quality over baselines for energy-based generative models on real datasets like CLEVR.

5. Clear motivation and well-explained methodology.

Weaknesses:
1. It would be better to show exploration of the sensitivity to key hyperparameters like the number of parallel sampling chains.

2. Discussion of computational cost/overhead compared to baseline sampling methods are missing in the manuscript.

Limitations:
Based on the provided paper, the authors do not appear to have explicitly discussed the limitations or potential negative societal impacts of their work. The paper is primarily focused on presenting the technical details of the proposed Follow Hamiltonian Leader (FHL) sampling algorithm and its empirical evaluation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel parallel sampling method named ""Follow Hamiltonian Leader"" (FHL) designed to address sampling challenges by leveraging zeroth-order information, particularly when first-order data is unreliable or unavailable. The method incorporates a leader-guiding mechanism to enhance the efficiency and effectiveness of the sampling process. Experimental results indicate that FHL significantly improves the exploration of target distributions and outperforms traditional sampling techniques, especially in scenarios involving corrupted gradients.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Innovative combination of zeroth and first-order information.
2. The effectiveness of the method is demonstrated in multiple task scenarios.
3. Theoretical analysis and prove are sufficient.

Weaknesses:
1. Is there any quantitative experiments like evaluating FID and IS on cifar10 datasets and I think it's more compelling whether a novel sampling methods combined with generative models can be used on image datasets with more complex distributions.
2. Lack of experiment of OOD in combination with EBMs or score-based models to valid the stability during sampling with proposed method.

Limitations:
1. Limited exploration of integration with other advanced MCMC methods.
2. Lack of quantitative experiments to demonstrate the advantage of proposed sampling method compared with other methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an interesting parallel sampling method that leverages zeroth-order information to address challenges in sampling from probability distributions, particularly when first-order data is unreliable or unavailable. The method incorporates a leader-guiding mechanism, enhancing efficiency and effectiveness by connecting multiple sampling instances through a selected leader. The proposed method, named Follow Hamiltonian Leader (FHL), extends the Hamiltonian Monte Carlo (HMC) framework by concurrently running multiple replicas at different energy levels and combining both zeroth and first-order information from various chains. Experimental results demonstrate that FHL significantly improves the exploration of target distributions and produces higher-quality outcomes compared to traditional sampling techniques, showing resilience against corrupted gradients and excelling in scenarios characterized by instability, metastability, and pseudo-stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed Follow Hamiltonian Leader (FHL) method markedly improves the efficiency and effectiveness of sampling processes, significantly expediting the exploration of target distributions and producing superior quality outcomes compared to traditional sampling techniques.
- FHL demonstrates greater resilience against the detrimental impacts of corrupted gradients by incorporating zeroth-order information. This robustness makes the method particularly valuable in scenarios where first-order information is compromised, ensuring more reliable and accurate sampling.

Weaknesses:
- The proposed FHL method involves intricate modifications to the traditional Hamiltonian Monte Carlo framework, such as the leader-guiding mechanism and elastic leapfrog technique, which may increase the complexity of implementation and require significant computational resources.
- The effectiveness of the FHL method heavily relies on the appropriate selection of the leader particle. If the leader is not accurately chosen, it could lead to suboptimal sampling performance, potentially compromising the overall efficiency and accuracy of the method.
- While the paper presents experimental results to demonstrate the efficacy of the FHL method, there is a lack of in-depth theoretical analysis to rigorously establish the convergence properties and performance guarantees of the proposed approach.
- The method’s scalability to high-dimensional problems or extremely large datasets is not thoroughly addressed. The parallel sampling approach may encounter challenges in maintaining efficiency and effectiveness as the dimensionality and size of the data increase.

Limitations:
The authors have not adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes to incorporate the energy $U$ into the gradient-based sampling techniques. In particular, it proposes to choose the lowest energy particle as the leader and then add an extra elastic tension between the leader and followers in the Hamiltonian Monte Carlo method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea is simple and clear, the toy examples are easy to understand and demonstrate the benefit of the proposed method well. In addition, the authors conduct experiments for each of the three challenging sampling scenarios identified by the authors.

Weaknesses:
It might worth including the overhead of the proposed method, how much slower the algorithm is per iteration compared to HMC for instance.

The tension coefficient $\lambda$ is critical, setting it to 0 recovers the baseline. But I did not find an ablation over the $\lambda$, is it hard to choose? From my understanding, if you set $\lambda$ pretty large it might recover something like gradient descent and the sampling will collapse.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an interesting new approach for improving sampling methods for energy-based generative models and score-matching models. The key idea is to incorporate zeroth-order information (energy values) in addition to the typical first-order gradient information used by most sampling algorithms like Hamiltonian Monte Carlo (HMC).

The authors identify several challenging scenarios where relying solely on gradients can be problematic - cases of instability, metastability, and pseudo-stability. They argue that incorporating energy values can help mitigate issues in these situations and improve sampling efficiency and quality.

Overall, the core idea of leveraging zeroth-order information in addition to gradients is quite novel and the FHL algorithm is an elegant way to implement this for improving sampling efficiency and quality. The paper is well-motivated, the method is clearly explained, and the empirical results are compelling.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Novel idea of incorporating zeroth-order energy information into sampling algorithms like HMC, which typically only use gradients. This can help address issues like instability and metastability.

2. The Follow Hamiltonian Leader (FHL) algorithm is an elegant way to exchange both energy and gradient information across parallel sampling chains in a principled manner.

3. Thorough experimental evaluation across synthetic examples illustrating the identified challenging scenarios of instability, metastability, and pseudo-stability.

4. Promising results showing improved sampling quality over baselines for energy-based generative models on real datasets like CLEVR.

5. Clear motivation and well-explained methodology.

Weaknesses:
1. It would be better to show exploration of the sensitivity to key hyperparameters like the number of parallel sampling chains.

2. Discussion of computational cost/overhead compared to baseline sampling methods are missing in the manuscript.

Limitations:
Based on the provided paper, the authors do not appear to have explicitly discussed the limitations or potential negative societal impacts of their work. The paper is primarily focused on presenting the technical details of the proposed Follow Hamiltonian Leader (FHL) sampling algorithm and its empirical evaluation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
2Bef9YxSJc;"REVIEW 
Summary:
The paper studies an important and open question, how much user behavior knowledge (generally captured by collaborative filtering models) are present in large language models. This has been a topic attracting significant research interest in recent years. The authors propose that simple linear mappings done on top of LM encoder representations are sufficient to capture collaborative filtering signals in recommendations, and propose a new recommendation method, AlphaRec, which takes pretrained language model content embeddings as input, transforms them via MLPs and lightweight graph convolutions, followed by a contrastive loss. The authors conduct experimental analysis for AlphaRec in both standard settings and zero-shot settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
**S1**: the topic studied is important. It is generally believed that language model and collaborative filtering (recommendations) learn different representation spaces, and methods to bring the two spaces closer is of significant interest to the large community working on search, recommendations, ads, and related topics.

**S2**: the particular approach proposed (linear mapping from textual space to collaborative filtering space) is understudied in prior work on LLM and (Generative) CF, despite numerous papers in recent years.

Weaknesses:
**W1**: the writing in this paper, esp. recommendation system paradigm related discussions, misrepresents (or ignores) significant prior work done in the field. e.g., ""AlphaRec follows a new CF paradigm, which we term the language-representation-based paradigm."" and related writings. 

- Content signals and/or embeddings have been used as the dominant recommendation paradigm in the field, even well before the seminal YouTube DNN paper [1] was published (see e.g., Pazzani and Billsus, 2007 [12]). For recent examples of related work, see e.g., [10, 11] from Pinterest and Meta in KDD'22 (but one should be able to easily find similar papers in WWW KDD etc in prior years as well). 
- Replacing ResNet/ViT- or GPT-/BERT- generated embedding with LLaMa- or Mixtral- generated embeddings cannot and should not be viewed as a paradigm shift, especially given the core architecture of AlphaRec is not substationally different from prior work.

**W2**: AlphaRec needs to be compared with stronger baselines. This applies to many major experiments in the paper. Here are some examples of baselines missing, which may significantly change conclusions obtained and discussions etc:

- vs ID-based recommenders (Table 3).
    - Equation (2) and line 171-173 for $N_u$ already captures set of items that a user is related to (""user interaction history"" / ""user engagement history"") to a large extent. Thus, the authors should compare AlphaRec with at least some SotA sequential/generative recommenders, such as SASRec, BERT4Rec, TIGER, HSTU [3, 4, 6, 7]. All of them are missing in the current paper.
    - Given AlphaRec uses the transposed item id representation - the one layer $N_i$ formulation (equation (2)), relevant work in recent years include Dual contrastive network [8] and User-centric ranking [9]. The authors should compare with or at least discuss some work in this category as related work. 
- Zero-shot performance. (Table 4)
    - ""Book Crossing"" is not a commonly used benchmark dataset. The ""Industrial"" dataset (per citation [1] on line 273) seems to be a small-scale ""Yelp"" dataset, and should be renamed to avoid confusions.
    - For ML-1M, the SotA approach one year ago (LLMRank [14]) already achieved 53.73 NDCG@20, significantly higher compared with 32.15  (AlphaRec) in this work. 

**W3**: many other formulations/experiments/writings could be significantly improved. Examples include:

- The proposed task formulation does not reflect how recommendation systems work in practice. e.g., ""Line 97-99. Personalized item recommendation with implicit feedback aims to select items i ∈ I that best match user u’s preferences based on binary interaction data Y = [yui], where yui = 1 (yui = 0) indicates user u ∈ U has (has not) interacted with item i [58]."" -- here ""selecting the item that user will interact with"" is not the same as ""selecting the item with the highest reward"", as the interaction itself can be negative (e.g., disliking a recommendation, abandoning session, etc.). See [1, 2] for references.

- A key contribution of this work should be the linear mapping finding. But Table 1 uses a questionable set of baselines for both LMs and CF baselines, which weakens linear mapping related claims.
    - To claim ""Moreover, with the advances in LMs, the performance of item representation linearly mapped from LMs exhibits a rising trend, gradually surpassing traditional ID-based CF models"" -- I would expect the authors to compare with a single set of models (e.g., LLaMa-2 7B 13B 70B or GPT-3 1.3b 2.7b 6.7b 13b 175b) trained on identical data. As it stands, all models are trained and/or finetuned with different data, so a simpler hypothesis explaining the LM (Linear Mapping) trend is that people are including more and more data into LLM pretraining/finetuning stages, which happen to capture more and more aspects relevant to recommendations. 
    - On the CF side, ""MF"" ""MultiVAE"" and ""LightGCN"" do not represent SotA baselines on Amazon Review datasets (see W2). 

- Table 1. Please highlight the particular K used for Recall and HR metrics (hard to find in the paper, applies to other tables too).   Most work on recommendation models also report HR/NDCG/etc. over at least 2-3 Ks to help readers understand how metrics vary with different approaches.


- Table 4. [1] should not be labeled as an ""Industrial"" dataset. The cited paper (per line 273) is Ni et al. ""Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"" which in turn seems to be an publicly available review dataset provided by Yelp. Please use appropriate language as the current writing leads readers to think that AlphaRec is an industrially deployed system. Please refer to industrial papers (e.g., KDD ADS track papers [2, 9, 11, 12]) for how to describe testings done on publicly available industrial sampled datasets (like Yelp), vs real deployments. 

- Misc: Contrastive loss is widely used and should not be viewed as a contribution of AlphaRec. See [15, 11] etc.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper states that LLM encodes collaborative signals that make it easy to connect language representation space with an effective recommendation space. Thus, it proposes an effective collaborative filtering model AlphaRec that takes as input only the transformed LLM representations of textual descriptions of items and is trained by InfoNCE loss and graph neural networks. The proposed method outperforms traditional ID-based models and other LM-enhanced methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-written and easy to follow. 
2. The paper conducts extensive experiments validating the effectiveness of the methods and proves the validity of the design through ablation study and anlysis.
3. The proposed method exhibits significantly good zero-shot recommendation performance

Weaknesses:
1. One of the most important motivations of the work is that the paper declares large language models encode collaborative signals which indicates the advantage of using representations of large language models for recommendations compared to id embeddings. However, how the preliminary experiments prove this point is insufficiently discussed in the paper. Advanced large language encodes more semantics of the textual descriptions and thus yields better performance. Why this alone doesn't fully explain the performance gain of LLMs should be more explicitly discussed in the main paper.
2. The novelty is limited. Using semantic embeddings of items has been widely used in recommendations. The novelty mostly lies in using the representations of large language models and the implementation details of how to make it effective when combined with traditional recommendation frameworks like non-linear transformations.
3. The paper states that language representation-based methods have low training costs. Still, if taking into account the costs of generating language representations, the computational cost is much higher than ID-based methods.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes AlphaRec, a novel method to incorporate both knowledge from pre-trained language models and collaborative signals. Authors firstly reveal the advantages brought from pre-trained embedding model, and then propose three modules within AlphaRec. An MLP layer to transform pre-trained embedding to item-representation. A graph convolution to aggregate neighbor’s information, and the InfoNCE loss to train introduced parameters within the MLP for each dataset. Overall, the novelty of this paper lies within exploration of NLP encoded embedding on RecSys. The graph convolution and InfoNCE loss are already widely used techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. A good exploration on new direction (language-representation-based) RecSys
2. Experiments are conducted from different angles for analyzing their model.

Weaknesses:
1. Insufficient baselines.
2. Uncleared model name definition.

Limitations:
See Weakness.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes AlphaRec, an LLM-based recommender system that utilizes language representations of item textual data for recommendations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ Investigating ID paradigm and LLM paradigm is important.
+ The method is simple but seems to be effective.

Weaknesses:
- In this paper, what most confuses me is the usage of the terminology ""collaborative filtering"" throughout the paper. In traditional recommender system, collaborative filtering information means the interactions among users and items. The authors find that using LM as feature extractors to get user/item embeddings from meta-data can achieve similar results as if CF is used for recommendation. However, this seems to be fundamentally different than LM has the ""collaborative information"", as for most online service platforms, the interaction data should be confident and open source LMs won't be able to train on that data. Therefore, the main claim in the paper seems questionable.

- It would be beneficial if we could have results on more diverse datasets.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies an important and open question, how much user behavior knowledge (generally captured by collaborative filtering models) are present in large language models. This has been a topic attracting significant research interest in recent years. The authors propose that simple linear mappings done on top of LM encoder representations are sufficient to capture collaborative filtering signals in recommendations, and propose a new recommendation method, AlphaRec, which takes pretrained language model content embeddings as input, transforms them via MLPs and lightweight graph convolutions, followed by a contrastive loss. The authors conduct experimental analysis for AlphaRec in both standard settings and zero-shot settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
**S1**: the topic studied is important. It is generally believed that language model and collaborative filtering (recommendations) learn different representation spaces, and methods to bring the two spaces closer is of significant interest to the large community working on search, recommendations, ads, and related topics.

**S2**: the particular approach proposed (linear mapping from textual space to collaborative filtering space) is understudied in prior work on LLM and (Generative) CF, despite numerous papers in recent years.

Weaknesses:
**W1**: the writing in this paper, esp. recommendation system paradigm related discussions, misrepresents (or ignores) significant prior work done in the field. e.g., ""AlphaRec follows a new CF paradigm, which we term the language-representation-based paradigm."" and related writings. 

- Content signals and/or embeddings have been used as the dominant recommendation paradigm in the field, even well before the seminal YouTube DNN paper [1] was published (see e.g., Pazzani and Billsus, 2007 [12]). For recent examples of related work, see e.g., [10, 11] from Pinterest and Meta in KDD'22 (but one should be able to easily find similar papers in WWW KDD etc in prior years as well). 
- Replacing ResNet/ViT- or GPT-/BERT- generated embedding with LLaMa- or Mixtral- generated embeddings cannot and should not be viewed as a paradigm shift, especially given the core architecture of AlphaRec is not substationally different from prior work.

**W2**: AlphaRec needs to be compared with stronger baselines. This applies to many major experiments in the paper. Here are some examples of baselines missing, which may significantly change conclusions obtained and discussions etc:

- vs ID-based recommenders (Table 3).
    - Equation (2) and line 171-173 for $N_u$ already captures set of items that a user is related to (""user interaction history"" / ""user engagement history"") to a large extent. Thus, the authors should compare AlphaRec with at least some SotA sequential/generative recommenders, such as SASRec, BERT4Rec, TIGER, HSTU [3, 4, 6, 7]. All of them are missing in the current paper.
    - Given AlphaRec uses the transposed item id representation - the one layer $N_i$ formulation (equation (2)), relevant work in recent years include Dual contrastive network [8] and User-centric ranking [9]. The authors should compare with or at least discuss some work in this category as related work. 
- Zero-shot performance. (Table 4)
    - ""Book Crossing"" is not a commonly used benchmark dataset. The ""Industrial"" dataset (per citation [1] on line 273) seems to be a small-scale ""Yelp"" dataset, and should be renamed to avoid confusions.
    - For ML-1M, the SotA approach one year ago (LLMRank [14]) already achieved 53.73 NDCG@20, significantly higher compared with 32.15  (AlphaRec) in this work. 

**W3**: many other formulations/experiments/writings could be significantly improved. Examples include:

- The proposed task formulation does not reflect how recommendation systems work in practice. e.g., ""Line 97-99. Personalized item recommendation with implicit feedback aims to select items i ∈ I that best match user u’s preferences based on binary interaction data Y = [yui], where yui = 1 (yui = 0) indicates user u ∈ U has (has not) interacted with item i [58]."" -- here ""selecting the item that user will interact with"" is not the same as ""selecting the item with the highest reward"", as the interaction itself can be negative (e.g., disliking a recommendation, abandoning session, etc.). See [1, 2] for references.

- A key contribution of this work should be the linear mapping finding. But Table 1 uses a questionable set of baselines for both LMs and CF baselines, which weakens linear mapping related claims.
    - To claim ""Moreover, with the advances in LMs, the performance of item representation linearly mapped from LMs exhibits a rising trend, gradually surpassing traditional ID-based CF models"" -- I would expect the authors to compare with a single set of models (e.g., LLaMa-2 7B 13B 70B or GPT-3 1.3b 2.7b 6.7b 13b 175b) trained on identical data. As it stands, all models are trained and/or finetuned with different data, so a simpler hypothesis explaining the LM (Linear Mapping) trend is that people are including more and more data into LLM pretraining/finetuning stages, which happen to capture more and more aspects relevant to recommendations. 
    - On the CF side, ""MF"" ""MultiVAE"" and ""LightGCN"" do not represent SotA baselines on Amazon Review datasets (see W2). 

- Table 1. Please highlight the particular K used for Recall and HR metrics (hard to find in the paper, applies to other tables too).   Most work on recommendation models also report HR/NDCG/etc. over at least 2-3 Ks to help readers understand how metrics vary with different approaches.


- Table 4. [1] should not be labeled as an ""Industrial"" dataset. The cited paper (per line 273) is Ni et al. ""Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"" which in turn seems to be an publicly available review dataset provided by Yelp. Please use appropriate language as the current writing leads readers to think that AlphaRec is an industrially deployed system. Please refer to industrial papers (e.g., KDD ADS track papers [2, 9, 11, 12]) for how to describe testings done on publicly available industrial sampled datasets (like Yelp), vs real deployments. 

- Misc: Contrastive loss is widely used and should not be viewed as a contribution of AlphaRec. See [15, 11] etc.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper states that LLM encodes collaborative signals that make it easy to connect language representation space with an effective recommendation space. Thus, it proposes an effective collaborative filtering model AlphaRec that takes as input only the transformed LLM representations of textual descriptions of items and is trained by InfoNCE loss and graph neural networks. The proposed method outperforms traditional ID-based models and other LM-enhanced methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-written and easy to follow. 
2. The paper conducts extensive experiments validating the effectiveness of the methods and proves the validity of the design through ablation study and anlysis.
3. The proposed method exhibits significantly good zero-shot recommendation performance

Weaknesses:
1. One of the most important motivations of the work is that the paper declares large language models encode collaborative signals which indicates the advantage of using representations of large language models for recommendations compared to id embeddings. However, how the preliminary experiments prove this point is insufficiently discussed in the paper. Advanced large language encodes more semantics of the textual descriptions and thus yields better performance. Why this alone doesn't fully explain the performance gain of LLMs should be more explicitly discussed in the main paper.
2. The novelty is limited. Using semantic embeddings of items has been widely used in recommendations. The novelty mostly lies in using the representations of large language models and the implementation details of how to make it effective when combined with traditional recommendation frameworks like non-linear transformations.
3. The paper states that language representation-based methods have low training costs. Still, if taking into account the costs of generating language representations, the computational cost is much higher than ID-based methods.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes AlphaRec, a novel method to incorporate both knowledge from pre-trained language models and collaborative signals. Authors firstly reveal the advantages brought from pre-trained embedding model, and then propose three modules within AlphaRec. An MLP layer to transform pre-trained embedding to item-representation. A graph convolution to aggregate neighbor’s information, and the InfoNCE loss to train introduced parameters within the MLP for each dataset. Overall, the novelty of this paper lies within exploration of NLP encoded embedding on RecSys. The graph convolution and InfoNCE loss are already widely used techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. A good exploration on new direction (language-representation-based) RecSys
2. Experiments are conducted from different angles for analyzing their model.

Weaknesses:
1. Insufficient baselines.
2. Uncleared model name definition.

Limitations:
See Weakness.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes AlphaRec, an LLM-based recommender system that utilizes language representations of item textual data for recommendations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ Investigating ID paradigm and LLM paradigm is important.
+ The method is simple but seems to be effective.

Weaknesses:
- In this paper, what most confuses me is the usage of the terminology ""collaborative filtering"" throughout the paper. In traditional recommender system, collaborative filtering information means the interactions among users and items. The authors find that using LM as feature extractors to get user/item embeddings from meta-data can achieve similar results as if CF is used for recommendation. However, this seems to be fundamentally different than LM has the ""collaborative information"", as for most online service platforms, the interaction data should be confident and open source LMs won't be able to train on that data. Therefore, the main claim in the paper seems questionable.

- It would be beneficial if we could have results on more diverse datasets.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
2BOb4SvDFr;"REVIEW 
Summary:
The paper proposes to use a new distance, Min-Max-Jump, which is the minimum largest distance on any path between two points, to be used in k-means clustering to learn clusters.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The distance can overcomes some demerits of the convex (""spherical"" in the paper) clusters.

Weaknesses:
The distance in the paper is, in fact, related to single linkage clustering that assign give a pair of points a distance at which the pair is joint to one cluster. This need to be analyzed to relate to previous work as well as to compute pairwise distances efficiently. 

Theoretical property of the distance is poor. The paper should review many other density-based distance functions to put this work into the correct context. 

There would be a lot of problems using this distance as many of pairs of nodes would share the same distance. There is no analysis on the  metric property of this distance. 

On evaluation, the methods need to compare to single-linkage clustering (SSL) at the very least as all the advantages of using this distance with k-means are available in SLL in its simplest form.

Presentation-wise, it is hardly up the standard. There are methods/algorithms/concepts that are mentioned as ""a is like b with a difference"" without a formal definition. This mixes up definitions and properties.

Limitations:
The paper uses a new distance without theoretical justifications. It learns nonconvex clusters by using a nonconvex clustering-based distance (without explicitly mentioning it), which is hardly a novelty.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new metric, min-max jump distance. Effectively, say we are given a complete graph with vertex set $\Omega$ and edge weights $d(x,y)$ denoting the distance between $x$ and $y$, where $d$ is a metric. Then $MMJ(x,y|\Omega')$ is the minimum, over all paths between $x$ and $y$, of the maximum weight edge between $x$ and $y$ on the subgraph induced on the vertices $\Omega' \subseteq \Omega$. Explained nicely in the paper, if you started at vertex $x$ and wanted to get to $y$ and $d(\cdot,\cdot)$ denoted the distance required to ``jump'' from one vertex to another, what is the minimum distance you need to be able to jump to somehow traverse from $x$ to $y$?

This is a nice intuitive metric, and has strong connections to the minimum spanning tree. In fact, I suspect there is more literature to draw from minimum spanning tree research that could yield conclusions about MMJ. The MST is also nice in clustering since oddly shaped clusters (non-convex, for instance) can have small MSTs. This is the idea of MMJ: use it as a metric for K-means so that it can identify non-spherical clusters.

The paper proves some notable theory about the properties of MMJ. Mostly, they show how: 1) When adding a new vertex $p$ to a set $\Omega$, $p$'s MMJ within the context of $\Omega+p$ can be computed knowing all pairwise MMJ's within $\Omega$ within the context of $\Omega$. This effectively adds a new point and evaluates it within the complete, updated context. 2) Given the MMJs for this additional point $p$ within the context of $\Omega +p$, expand the MMJ context of all other pairs in $\Omega$ to the context of $\Omega+p$.

This can then be used in a very dynamic programming-like manner to start with just two points, add new points $p$ and find the context of $p$ and all other points, and then update all known existing MMJs to the new context. This is their algorithm 1, requiring $O(n^3)$ time. They also use properties of the MST to bypass unnecessary calculation to yield algorithm 2, which takes only $O(n^2)$ time (to find the MST).

They then evaluate the performance of algorithms using the MMJ measure on irregular shaped clusters to verify that MMJ helps identify these. This makes sense, since they likely have small MSTs, but not small average/sum/max/etc distances within clusters. Notably, they show how MMJ improves K-means.

Soundness:
4: excellent

Presentation:
1: poor

Contribution:
3: good

Strengths:
I think MMJ is a very cool metric with nice properties and intuition, particularly that related to the MST (I wish the authors had spent more time discussing this!). Their findings are nice and relatively simple to understand (in spite of the presentation). They show that it helps K-means expand to more complicated cluster shapes, and overall it is a very nice, NeurIPS-worthy result. However, as I will explain in the weaknesses section, I do not think this paper is in an acceptable state for NeurIPS.

Weaknesses:
There are a few notable downsides. In terms of the result, I'm not entirely convinced of its novelty. How much of this is actually a re-iteration of MST-based methods already understood? Is this really better than other MST-based algorithms on irregular clusters (think single linkage)? I know that there is a lot of literature that explores irregular shaped clusters, but I am not an expert in this area and so I cannot place this work in the context of existing results. I wish the authors would explain that. Though even if these algorithms aren't entirely better than state of the art, the novelty of the nice formulation of MMJ is certainly appreciated.

However, the biggest flaw in the paper is the writing quality. There are places where the paper is nice and concise, but most of the time it just lacks exposition to understand the higher level of things or adequate details to fully understand what is happening. Formal proofs are contained in the paper, but the jumps in some of the proofs are too large. Theorems and proofs are placed back to back with no high level explanation. Algorithms are written and pseudocode with only the briefest justifications, and no thorough explanations. This is not an acceptable paper for NeurIPS, and I think these issues are too extensive to simply ask the authors to revise. Though if other commenters disagree, I am amenable to changing my opinion.

And one of the disappointing things about this is how natural this work is and how much it lends itself to nice intuitive explanations and visual depictions! For instance, you could do some very nice visualizations of Algorithm 1, where you depict a matrix and show which indices have been calculated to what context $\Omega_n$ at each time point. This clarifies the different purpose of the two loops.

I hope to see this paper submitted again later in a more cleaned up state!

Limitations:
None notable

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents the Min-Max-Jump (MMJ) distance concept and two calculation methods, focusing on path optimization in data analysis and clustering. The contributions include introducing MMJ distance, proposing efficient calculation methods, discussing its properties and applications, and offering a user-friendly approach for practical implementation. Overall, the paper introduces a new distance metric for path optimization and data analysis, providing useful tools and insights for various applications in the field.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1: The paper demonstrates strength through its meticulous use of theorems and proofs, enhancing the credibility and robustness of the research findings.

S2: Clear visualization of results in the paper aids in effectively conveying complex information to the readers, improving understanding and interpretation.

S3: Extensive literature citations throughout the paper showcase a strong foundation of existing knowledge and research, adding depth and scholarly rigor to the study's presentation.

Weaknesses:
W1: The paper's writing style deviates from academic norms, indicating a need for improvement in writing proficiency.

W2: The extremely brief Introduction lacks a detailed definition of the problem, its significance, and challenges. Moreover, it lacks citation support for the points presented. While Section 2.1 mentions methods like k-NN, UMAP, HDBSCAN, it fails to provide corresponding references, lacking essential academic backing.

W3: The overall structure of the paper lacks clarity, as it introduces different distance metrics in Section 2.1 but introduces a new distance measurement approach in Section 2.4, leading to disjointed logic.

W4: The presentation of various distance metrics in Section 2.1 appears disorganized and lacks coherence.

W5: The extensive definition provided towards the end of Section 6.3 disrupts the logical flow of the paper, suggesting a need to adjust the paper's structural coherence.

Limitations:
The paper does not discuss limitations. The authors seem to perceive their work as solely testing models with datasets without considering the shortcomings of the algorithms themselves.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Different distance metrics have been introduced in the literature for data analysis. In this paper the authors consider the min-max-jump distance and apply it in the context two applications, namely, k-means clustering and as an internal clustering evaluation index. They also present two algorithms for computing the min-max-jump distance.

Experimental comparisons reveal that min-max-jump based k-means clustering is better than standard k-means clustering. Also, the min-max-jump distance is shown to be a better internal clustering evaluation index.

This referee feels that this work is rather incremental. Also, experiments have been conducted only on very small datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors demonstrate the efficacy of the min-max-jump distance on two different applications.

Experimental results have also been supplied to support their assertions.

Weaknesses:
The work done is incremental with very limited novelty.
Extensive experiments are called for.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes to use a new distance, Min-Max-Jump, which is the minimum largest distance on any path between two points, to be used in k-means clustering to learn clusters.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The distance can overcomes some demerits of the convex (""spherical"" in the paper) clusters.

Weaknesses:
The distance in the paper is, in fact, related to single linkage clustering that assign give a pair of points a distance at which the pair is joint to one cluster. This need to be analyzed to relate to previous work as well as to compute pairwise distances efficiently. 

Theoretical property of the distance is poor. The paper should review many other density-based distance functions to put this work into the correct context. 

There would be a lot of problems using this distance as many of pairs of nodes would share the same distance. There is no analysis on the  metric property of this distance. 

On evaluation, the methods need to compare to single-linkage clustering (SSL) at the very least as all the advantages of using this distance with k-means are available in SLL in its simplest form.

Presentation-wise, it is hardly up the standard. There are methods/algorithms/concepts that are mentioned as ""a is like b with a difference"" without a formal definition. This mixes up definitions and properties.

Limitations:
The paper uses a new distance without theoretical justifications. It learns nonconvex clusters by using a nonconvex clustering-based distance (without explicitly mentioning it), which is hardly a novelty.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new metric, min-max jump distance. Effectively, say we are given a complete graph with vertex set $\Omega$ and edge weights $d(x,y)$ denoting the distance between $x$ and $y$, where $d$ is a metric. Then $MMJ(x,y|\Omega')$ is the minimum, over all paths between $x$ and $y$, of the maximum weight edge between $x$ and $y$ on the subgraph induced on the vertices $\Omega' \subseteq \Omega$. Explained nicely in the paper, if you started at vertex $x$ and wanted to get to $y$ and $d(\cdot,\cdot)$ denoted the distance required to ``jump'' from one vertex to another, what is the minimum distance you need to be able to jump to somehow traverse from $x$ to $y$?

This is a nice intuitive metric, and has strong connections to the minimum spanning tree. In fact, I suspect there is more literature to draw from minimum spanning tree research that could yield conclusions about MMJ. The MST is also nice in clustering since oddly shaped clusters (non-convex, for instance) can have small MSTs. This is the idea of MMJ: use it as a metric for K-means so that it can identify non-spherical clusters.

The paper proves some notable theory about the properties of MMJ. Mostly, they show how: 1) When adding a new vertex $p$ to a set $\Omega$, $p$'s MMJ within the context of $\Omega+p$ can be computed knowing all pairwise MMJ's within $\Omega$ within the context of $\Omega$. This effectively adds a new point and evaluates it within the complete, updated context. 2) Given the MMJs for this additional point $p$ within the context of $\Omega +p$, expand the MMJ context of all other pairs in $\Omega$ to the context of $\Omega+p$.

This can then be used in a very dynamic programming-like manner to start with just two points, add new points $p$ and find the context of $p$ and all other points, and then update all known existing MMJs to the new context. This is their algorithm 1, requiring $O(n^3)$ time. They also use properties of the MST to bypass unnecessary calculation to yield algorithm 2, which takes only $O(n^2)$ time (to find the MST).

They then evaluate the performance of algorithms using the MMJ measure on irregular shaped clusters to verify that MMJ helps identify these. This makes sense, since they likely have small MSTs, but not small average/sum/max/etc distances within clusters. Notably, they show how MMJ improves K-means.

Soundness:
4: excellent

Presentation:
1: poor

Contribution:
3: good

Strengths:
I think MMJ is a very cool metric with nice properties and intuition, particularly that related to the MST (I wish the authors had spent more time discussing this!). Their findings are nice and relatively simple to understand (in spite of the presentation). They show that it helps K-means expand to more complicated cluster shapes, and overall it is a very nice, NeurIPS-worthy result. However, as I will explain in the weaknesses section, I do not think this paper is in an acceptable state for NeurIPS.

Weaknesses:
There are a few notable downsides. In terms of the result, I'm not entirely convinced of its novelty. How much of this is actually a re-iteration of MST-based methods already understood? Is this really better than other MST-based algorithms on irregular clusters (think single linkage)? I know that there is a lot of literature that explores irregular shaped clusters, but I am not an expert in this area and so I cannot place this work in the context of existing results. I wish the authors would explain that. Though even if these algorithms aren't entirely better than state of the art, the novelty of the nice formulation of MMJ is certainly appreciated.

However, the biggest flaw in the paper is the writing quality. There are places where the paper is nice and concise, but most of the time it just lacks exposition to understand the higher level of things or adequate details to fully understand what is happening. Formal proofs are contained in the paper, but the jumps in some of the proofs are too large. Theorems and proofs are placed back to back with no high level explanation. Algorithms are written and pseudocode with only the briefest justifications, and no thorough explanations. This is not an acceptable paper for NeurIPS, and I think these issues are too extensive to simply ask the authors to revise. Though if other commenters disagree, I am amenable to changing my opinion.

And one of the disappointing things about this is how natural this work is and how much it lends itself to nice intuitive explanations and visual depictions! For instance, you could do some very nice visualizations of Algorithm 1, where you depict a matrix and show which indices have been calculated to what context $\Omega_n$ at each time point. This clarifies the different purpose of the two loops.

I hope to see this paper submitted again later in a more cleaned up state!

Limitations:
None notable

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents the Min-Max-Jump (MMJ) distance concept and two calculation methods, focusing on path optimization in data analysis and clustering. The contributions include introducing MMJ distance, proposing efficient calculation methods, discussing its properties and applications, and offering a user-friendly approach for practical implementation. Overall, the paper introduces a new distance metric for path optimization and data analysis, providing useful tools and insights for various applications in the field.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1: The paper demonstrates strength through its meticulous use of theorems and proofs, enhancing the credibility and robustness of the research findings.

S2: Clear visualization of results in the paper aids in effectively conveying complex information to the readers, improving understanding and interpretation.

S3: Extensive literature citations throughout the paper showcase a strong foundation of existing knowledge and research, adding depth and scholarly rigor to the study's presentation.

Weaknesses:
W1: The paper's writing style deviates from academic norms, indicating a need for improvement in writing proficiency.

W2: The extremely brief Introduction lacks a detailed definition of the problem, its significance, and challenges. Moreover, it lacks citation support for the points presented. While Section 2.1 mentions methods like k-NN, UMAP, HDBSCAN, it fails to provide corresponding references, lacking essential academic backing.

W3: The overall structure of the paper lacks clarity, as it introduces different distance metrics in Section 2.1 but introduces a new distance measurement approach in Section 2.4, leading to disjointed logic.

W4: The presentation of various distance metrics in Section 2.1 appears disorganized and lacks coherence.

W5: The extensive definition provided towards the end of Section 6.3 disrupts the logical flow of the paper, suggesting a need to adjust the paper's structural coherence.

Limitations:
The paper does not discuss limitations. The authors seem to perceive their work as solely testing models with datasets without considering the shortcomings of the algorithms themselves.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Different distance metrics have been introduced in the literature for data analysis. In this paper the authors consider the min-max-jump distance and apply it in the context two applications, namely, k-means clustering and as an internal clustering evaluation index. They also present two algorithms for computing the min-max-jump distance.

Experimental comparisons reveal that min-max-jump based k-means clustering is better than standard k-means clustering. Also, the min-max-jump distance is shown to be a better internal clustering evaluation index.

This referee feels that this work is rather incremental. Also, experiments have been conducted only on very small datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors demonstrate the efficacy of the min-max-jump distance on two different applications.

Experimental results have also been supplied to support their assertions.

Weaknesses:
The work done is incremental with very limited novelty.
Extensive experiments are called for.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
1URMG6B3WW;"REVIEW 
Summary:
This paper introduces the KrwEmd algorithm, a novel approach to hand abstraction in Texas Hold’em-style games. The main contribution is the integration of historical information using K-recall winrate features and earth mover’s distance, addressing the limitations of previous imperfect recall abstraction methods. The algorithm demonstrates significant performance improvements over state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The integration of historical information via K-recall win rate features enhances the accuracy and reliability of hand abstraction.
2. KrwEmd significantly outperforms existing methods, showcasing its practical value and potential for real-world applications.
3. The algorithm is technically sound, with strong experimental validation supporting its claims.

Weaknesses:
Scalability: While the paper demonstrates KrwEmd's effectiveness in Texas Hold’em-style games, its scalability to more complex and diverse game scenarios remains to be explored.

Comparative Analysis: The paper could benefit from a more detailed comparison with existing hand abstraction methods to better highlight KrwEmd's unique advantages and limitations.

Limitations:
The paper primarily focuses on Texas Hold’em-style games. While this is a significant achievement, a discussion on the model's scalability and generalization to more complex and varied game scenarios would be beneficial. Including potential strategies to address these challenges would strengthen the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach to hand abstraction in Texas Hold'em-style poker games, addressing the limitations of current methods that often disregard historical information. The authors make two primary contributions: First, they develop KRWI (K-Recall Winrate Isomorphism), a new abstraction method that incorporates historical information from previous game phases. Second, they present KrwEmd, the first hand abstraction algorithm to effectively combine K-recall win rate features with earth mover's distance for hand classification. Through experiments conducted in the Numeral211 Hold'em environment, the authors demonstrate that KrwEmd significantly outperforms state-of-the-art algorithms such as Ehs and PaEmd in terms of exploitability, while maintaining the same number of abstracted information sets. This work shows that incorporating historical information can substantially enhance the performance of hand abstraction algorithms, potentially leading to more advanced strategic computation in large-scale adversarial games and stronger poker AI systems.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
* Overall, the paper provides a new technique that is promising for an important area of research
* The results indicate strong improvements over alternative methods

Weaknesses:
For me, the paper's primary weakness is the presentation method. I had trouble understanding the significance and nature of the contribution from the current submission. In general, a clearer description of this area of research for people who, e.g., work on games but don't focus on poker would be quite helpful. Some specific suggestions/areas for improvement are:

* Clearer introduction of key concepts: The paper jumps into technical terms like 'imperfect recall abstraction' and 'hand abstraction' without adequately explaining them for a broader audience. A brief explanation of why these concepts are important in poker AI would be beneficial.
* More intuitive explanations of the algorithms: The descriptions of PWI, KRWI, and KrwEmd are highly technical. Including some simple examples or diagrams to illustrate how these algorithms work could greatly improve understanding.
* Better contextualization of the contribution: While the paper claims to outperform existing methods, it's not clear how significant this improvement is in the broader context of poker AI. A discussion of the practical implications of this improvement would be valuable.
* Clarification of experimental setup: The Numeral211 Hold'em environment is not well-known. A clearer explanation of how this relates to standard poker variants would help readers understand the relevance of the results.
* More accessible presentation of results: The graphs and tables are dense with information but lack clear explanations. Simplifying these visualizations or providing more guidance on how to interpret them would be helpful.
* Glossary of terms: Given the many technical terms used (e.g., 'earth mover's distance', 'K-recall winrate feature'), a glossary could be a valuable addition to help readers keep track of these concepts.

Limitations:
There's limited discussion of limitations. It would be good to include an explicit limitations section. In particular, it would be good to discuss potential computational challenges in more detail, any limitations on the scope of the evaluation, and future work that might be planned.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on the problem of hand abstraction for Texas Hold-Em style poker games. Hand abstraction is the process of partitioning game histories into infosets which still contain enough information to make strategically advantageous decisions. Previous approaches have focused on abstractions that primarily focus on the future outcomes from each hand, but the authors suggest that it may instead be beneficial to also include past information. They design a hand abstraction algorithm called KwrEmd that outperforms previous work by incorporating historical information.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The results seem to show that KrwEmd outperforms other imperfect recall hand abstraction algorithms in terms of exploitability.

Weaknesses:
As somebody who is unfamiliar with the subfield of imperfect recall abstraction, I found the paper to be quite confusing throughout. The authors do not often provide intuition or examples for their method, and I found it difficult to tell exactly which contributions were novel compared to Fu et al. While it's reasonable for a paper to use technical language at times, well-written papers are usually understandable by a broader set of readers than just those in the specific subfield.

Limitations:
Not sure.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes KrwEmd, a novel hand abstraction algorithm for imperfect recall settings in Texas Hold'em poker. The algorithm leverages K-recall winrate features, incorporating historical information in addition to future information for constructing hand abstractions. The authors introduce two new isomorphism frameworks: Potential Winrate Isomorphism (PWI) and K-recall Winrate Isomorphism (KRWI). They demonstrate that KRWI outperforms existing methods like POI in identifying distinct infosets. KrwEmd, which combines KRWI with Earth Mover's Distance (EMD) for hand classification, shows superior performance compared to POI, Ehs, and PaEmd in the Numeral211 Hold'em environment.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
*Originality*: The paper presents a novel combination of K-recall winrate features and EMD for hand abstraction in imperfect recall settings, addressing a critical limitation of current approaches that solely rely on future information.
The introduction of KRWI and PWI provides valuable new tools for understanding and constructing hand abstractions in poker AI.

*Quality*: The experimental results in the Numeral211 environment demonstrate a clear improvement over existing methods, supporting the claims of the paper. The paper includes an appendix with algorithm details and supplementary experimental data.

*Significance*: The proposed KrwEmd algorithm advances the state-of-the-art in hand abstraction for imperfect recall settings, offering a potentially significant improvement for developing stronger poker AI agents.
The incorporation of historical information is a valuable contribution that benefit positively future research in poker and other imperfect information games.

Weaknesses:
I found the paper  challenging to understand, though this may be due to my limited background knowledge in poker AI and game theory.  While the authors provide a background section, the density of the technical content and the numerous specialized terms make comprehension difficult.

The description of the accelerated algorithm in Appendix A.3 could be expanded for better understanding. Additionally, a clear discussion of the limitations of the accelerated algorithm would be beneficial.

The paper provides limited information about the proposed algorithms, particularly KrwEmd. While the core concepts are presented, the details regarding implementation and specific design choices are limited. More in-depth explanation and pseudocode would enhance the paper's quality.

Limitations:
The authors acknowledge the computational complexity of clustering with EMD and introduce an accelerated algorithm. However, the paper lacks a dedicated section addressing the limitations of the proposed methods and the accelerated algorithm.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper develops new hand abstraction techniques for Texas Hold'em-style games (in general: games with ordered signals), which fare better than previous methods in both the number of hands identified, and performance (exploitability) in a simplified version of the game. 

Hand abstraction is a technique aiding the strategy construction in a Texas Hold'em, where concrete hands, or rather concrete signal infosets (i.e. ""possible words"" according to the information revealed so far), are replaced by abstract infosets, represented in an abstract feature space (here $\mathbb{R}^n$).

The core idea of the paper is to use the features of hands from previous rounds in the construction of the current round feature. More precisely, the paper investigates a simple method (KRWI = *k-recall winrate isomorphism*) of maintaining, at a given round, the collection of all potential-winrate isomorphims (PWI) features from previous $k$ rounds by concatenating them all together. PWI for an $n$-player game is a categorical probability distribution over $n+1$ events of a form *""this player outperformed exactly $l-1$ other players while losing to none""* for $l = 1, 2, \ldots n$ and *""this player lost to at least one player""*. Those distributions can be computed by a dynamic programming method. To reduce the cardinality of the space, the paper later clusters KRWI features with k-means using the Wasserstein distance, naming it the KrwEmd method.

All of the methods are benchmarked against currently used techniques that do not use historical information. Experiments find that KRWI identifies similar proportion of signal infosets as the previously used KROI. Using the metric of exploitability of the equilibrium (how it deviates from Nash equilibrium) of the strategy found by an imperfect-information game solver, authors find that 2-RWI-based approach performs almost the same as 2-ROI, and that KrwEmd outpefrorms previously used Ehs and PaEmd by a relatively large margin.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proposes a reasonable extension of the currently used techniques for hand abstraction in Texas Hold'e, and shows that the new idea beats SOTA. It does not shy away from introducing the reader to the relevant background in a rigorous way (which is what made it possible for me to even start reading it). Experiments show a meaningful improvement, and provide additional insights (such as the decreasing worth of historical information).

Weaknesses:
From the perspective of someone not at all acquainted with the field of imperfect information games/games with ordered signals, the paper was quite hard to read and understand - even though (assuming that the authors agree with my summary), the contribution is a relatively straightforward idea.

The introduction was uninformative and confusing (I would recommend rewriting the whole second paragraph); the preliminaries, although presented in-depth and trying to be formal, also posed quite a few questions; sections 4 and 5 describing the main contribution lacked detail and justification (i.e. ideally I would like to see definition/theorem/proof style - otherwise the text is impossible to read for someone unfamiliar with the field), and the experimental setup is assuming a lot of background knowledge that was not explained neither in the main paper nor in the appendix (it was also difficult to gauge if the comparison between SOTA and the new approach was fair from the resources pov - the paper reports some numbers, but never an aggregated ""memory/time used"" for all methods).

Please see Questions below for a detailed explanation of what I found lacking or hard to understand.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the KrwEmd algorithm, a novel approach to hand abstraction in Texas Hold’em-style games. The main contribution is the integration of historical information using K-recall winrate features and earth mover’s distance, addressing the limitations of previous imperfect recall abstraction methods. The algorithm demonstrates significant performance improvements over state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The integration of historical information via K-recall win rate features enhances the accuracy and reliability of hand abstraction.
2. KrwEmd significantly outperforms existing methods, showcasing its practical value and potential for real-world applications.
3. The algorithm is technically sound, with strong experimental validation supporting its claims.

Weaknesses:
Scalability: While the paper demonstrates KrwEmd's effectiveness in Texas Hold’em-style games, its scalability to more complex and diverse game scenarios remains to be explored.

Comparative Analysis: The paper could benefit from a more detailed comparison with existing hand abstraction methods to better highlight KrwEmd's unique advantages and limitations.

Limitations:
The paper primarily focuses on Texas Hold’em-style games. While this is a significant achievement, a discussion on the model's scalability and generalization to more complex and varied game scenarios would be beneficial. Including potential strategies to address these challenges would strengthen the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach to hand abstraction in Texas Hold'em-style poker games, addressing the limitations of current methods that often disregard historical information. The authors make two primary contributions: First, they develop KRWI (K-Recall Winrate Isomorphism), a new abstraction method that incorporates historical information from previous game phases. Second, they present KrwEmd, the first hand abstraction algorithm to effectively combine K-recall win rate features with earth mover's distance for hand classification. Through experiments conducted in the Numeral211 Hold'em environment, the authors demonstrate that KrwEmd significantly outperforms state-of-the-art algorithms such as Ehs and PaEmd in terms of exploitability, while maintaining the same number of abstracted information sets. This work shows that incorporating historical information can substantially enhance the performance of hand abstraction algorithms, potentially leading to more advanced strategic computation in large-scale adversarial games and stronger poker AI systems.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
* Overall, the paper provides a new technique that is promising for an important area of research
* The results indicate strong improvements over alternative methods

Weaknesses:
For me, the paper's primary weakness is the presentation method. I had trouble understanding the significance and nature of the contribution from the current submission. In general, a clearer description of this area of research for people who, e.g., work on games but don't focus on poker would be quite helpful. Some specific suggestions/areas for improvement are:

* Clearer introduction of key concepts: The paper jumps into technical terms like 'imperfect recall abstraction' and 'hand abstraction' without adequately explaining them for a broader audience. A brief explanation of why these concepts are important in poker AI would be beneficial.
* More intuitive explanations of the algorithms: The descriptions of PWI, KRWI, and KrwEmd are highly technical. Including some simple examples or diagrams to illustrate how these algorithms work could greatly improve understanding.
* Better contextualization of the contribution: While the paper claims to outperform existing methods, it's not clear how significant this improvement is in the broader context of poker AI. A discussion of the practical implications of this improvement would be valuable.
* Clarification of experimental setup: The Numeral211 Hold'em environment is not well-known. A clearer explanation of how this relates to standard poker variants would help readers understand the relevance of the results.
* More accessible presentation of results: The graphs and tables are dense with information but lack clear explanations. Simplifying these visualizations or providing more guidance on how to interpret them would be helpful.
* Glossary of terms: Given the many technical terms used (e.g., 'earth mover's distance', 'K-recall winrate feature'), a glossary could be a valuable addition to help readers keep track of these concepts.

Limitations:
There's limited discussion of limitations. It would be good to include an explicit limitations section. In particular, it would be good to discuss potential computational challenges in more detail, any limitations on the scope of the evaluation, and future work that might be planned.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on the problem of hand abstraction for Texas Hold-Em style poker games. Hand abstraction is the process of partitioning game histories into infosets which still contain enough information to make strategically advantageous decisions. Previous approaches have focused on abstractions that primarily focus on the future outcomes from each hand, but the authors suggest that it may instead be beneficial to also include past information. They design a hand abstraction algorithm called KwrEmd that outperforms previous work by incorporating historical information.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The results seem to show that KrwEmd outperforms other imperfect recall hand abstraction algorithms in terms of exploitability.

Weaknesses:
As somebody who is unfamiliar with the subfield of imperfect recall abstraction, I found the paper to be quite confusing throughout. The authors do not often provide intuition or examples for their method, and I found it difficult to tell exactly which contributions were novel compared to Fu et al. While it's reasonable for a paper to use technical language at times, well-written papers are usually understandable by a broader set of readers than just those in the specific subfield.

Limitations:
Not sure.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes KrwEmd, a novel hand abstraction algorithm for imperfect recall settings in Texas Hold'em poker. The algorithm leverages K-recall winrate features, incorporating historical information in addition to future information for constructing hand abstractions. The authors introduce two new isomorphism frameworks: Potential Winrate Isomorphism (PWI) and K-recall Winrate Isomorphism (KRWI). They demonstrate that KRWI outperforms existing methods like POI in identifying distinct infosets. KrwEmd, which combines KRWI with Earth Mover's Distance (EMD) for hand classification, shows superior performance compared to POI, Ehs, and PaEmd in the Numeral211 Hold'em environment.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
*Originality*: The paper presents a novel combination of K-recall winrate features and EMD for hand abstraction in imperfect recall settings, addressing a critical limitation of current approaches that solely rely on future information.
The introduction of KRWI and PWI provides valuable new tools for understanding and constructing hand abstractions in poker AI.

*Quality*: The experimental results in the Numeral211 environment demonstrate a clear improvement over existing methods, supporting the claims of the paper. The paper includes an appendix with algorithm details and supplementary experimental data.

*Significance*: The proposed KrwEmd algorithm advances the state-of-the-art in hand abstraction for imperfect recall settings, offering a potentially significant improvement for developing stronger poker AI agents.
The incorporation of historical information is a valuable contribution that benefit positively future research in poker and other imperfect information games.

Weaknesses:
I found the paper  challenging to understand, though this may be due to my limited background knowledge in poker AI and game theory.  While the authors provide a background section, the density of the technical content and the numerous specialized terms make comprehension difficult.

The description of the accelerated algorithm in Appendix A.3 could be expanded for better understanding. Additionally, a clear discussion of the limitations of the accelerated algorithm would be beneficial.

The paper provides limited information about the proposed algorithms, particularly KrwEmd. While the core concepts are presented, the details regarding implementation and specific design choices are limited. More in-depth explanation and pseudocode would enhance the paper's quality.

Limitations:
The authors acknowledge the computational complexity of clustering with EMD and introduce an accelerated algorithm. However, the paper lacks a dedicated section addressing the limitations of the proposed methods and the accelerated algorithm.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper develops new hand abstraction techniques for Texas Hold'em-style games (in general: games with ordered signals), which fare better than previous methods in both the number of hands identified, and performance (exploitability) in a simplified version of the game. 

Hand abstraction is a technique aiding the strategy construction in a Texas Hold'em, where concrete hands, or rather concrete signal infosets (i.e. ""possible words"" according to the information revealed so far), are replaced by abstract infosets, represented in an abstract feature space (here $\mathbb{R}^n$).

The core idea of the paper is to use the features of hands from previous rounds in the construction of the current round feature. More precisely, the paper investigates a simple method (KRWI = *k-recall winrate isomorphism*) of maintaining, at a given round, the collection of all potential-winrate isomorphims (PWI) features from previous $k$ rounds by concatenating them all together. PWI for an $n$-player game is a categorical probability distribution over $n+1$ events of a form *""this player outperformed exactly $l-1$ other players while losing to none""* for $l = 1, 2, \ldots n$ and *""this player lost to at least one player""*. Those distributions can be computed by a dynamic programming method. To reduce the cardinality of the space, the paper later clusters KRWI features with k-means using the Wasserstein distance, naming it the KrwEmd method.

All of the methods are benchmarked against currently used techniques that do not use historical information. Experiments find that KRWI identifies similar proportion of signal infosets as the previously used KROI. Using the metric of exploitability of the equilibrium (how it deviates from Nash equilibrium) of the strategy found by an imperfect-information game solver, authors find that 2-RWI-based approach performs almost the same as 2-ROI, and that KrwEmd outpefrorms previously used Ehs and PaEmd by a relatively large margin.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proposes a reasonable extension of the currently used techniques for hand abstraction in Texas Hold'e, and shows that the new idea beats SOTA. It does not shy away from introducing the reader to the relevant background in a rigorous way (which is what made it possible for me to even start reading it). Experiments show a meaningful improvement, and provide additional insights (such as the decreasing worth of historical information).

Weaknesses:
From the perspective of someone not at all acquainted with the field of imperfect information games/games with ordered signals, the paper was quite hard to read and understand - even though (assuming that the authors agree with my summary), the contribution is a relatively straightforward idea.

The introduction was uninformative and confusing (I would recommend rewriting the whole second paragraph); the preliminaries, although presented in-depth and trying to be formal, also posed quite a few questions; sections 4 and 5 describing the main contribution lacked detail and justification (i.e. ideally I would like to see definition/theorem/proof style - otherwise the text is impossible to read for someone unfamiliar with the field), and the experimental setup is assuming a lot of background knowledge that was not explained neither in the main paper nor in the appendix (it was also difficult to gauge if the comparison between SOTA and the new approach was fair from the resources pov - the paper reports some numbers, but never an aggregated ""memory/time used"" for all methods).

Please see Questions below for a detailed explanation of what I found lacking or hard to understand.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
1CssOqRYyz;"REVIEW 
Summary:
The paper proposes the first diffusion-based point cloud compression method called Diff-PCC.  
A dual-space latent representation is devised in this paper, where a compressor composed of two independent encoding backbones is used to extract expressive shape latents from different latent spaces.    
At the decoding side, a diffusion-based generator is devised to produce high-quality reconstructions by considering the shape latents as guidance to stochastically denoise the noisy point clouds.    
Experiments demonstrate that the proposed Diff-PCC achieves state-of-the-art compression performance (e.g., 7.711 14 dB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while attaining superior subjective quality.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Novelty is a strength. To my knowledge, diffusion model is used in point cloud compression for the first time. And the dual-latent design is also novel for learned point cloud compression.   

The manuscript is well written and easy to follow. Especially, the author did a good job in introducing related works on image compression, point cloud compression, point cloud analysis and diffusion model.

Weaknesses:
More work on diffusion model for data compression could be discussed, like ‘Idempotence and Perceptual Image Compression, ICLR 2024’. In addition, although this paper focuses on point cloud compression, the way of applying diffusion model should be compared with those learned image compression works in the related work part. From my impression, the method in this paper is still novel compared with those learned image compression paper using diffusion model.    

More recent learned point cloud compression method [30][14] should be compared in Table 1, Figure 3 and Figure 4, regarding rate distortion and encoding/decoding speed. Besides, only object point cloud is considered currently, large scale point cloud like SemanticKITTI could be compared [30][14].  
 
It is not clear how the speed is measured in Table 1. The hardware and commend line shoud be provided in the supplementary material.  

Minor:  
L86, Point·E[] is a typo.  
[30] and [31] are the same.   
L202, the reference should be fixed.   
What is the FPS in eq 14? farthest point sampling?

Limitations:
The limitation is addressed in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors propose a diffusion-based point cloud compression framework. Low frequency and high frequency features are extracted via PointNet and PointPN from input point clouds, which are quantized and encoded for compression. During decompression, the quantized features would be decoded to condition a diffusion model to construct the decompressed results. The experiments on 15K points data from ShapeNet, ModelNet10, and ModelNet40 show superiority over compared methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea to introduce diffusion models for point cloud compression is different with former works;

2. The paper is easy to follow, while the disgrams are also good;

3. The performances show improvements on sparse point clouds.

Weaknesses:
1. The comparison is not convincing enough. Some commonly used compression methods are not compared, while the evaluation is limited to sparse point clouds with relatively simple structures from ShapeNet, ModelNet;

2. The motivation of using diffusion model for compression is questionable. As a sampling-based framework, diffusion models may construct different results during decompression from variant sampled noises each time. I am not so sure if the diffusion model is more appropriate than existing AE or VAE-based frameworks for the compression task, which may need decompression as accurate as possible;
Please check the questions for more details, thanks.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, they introduce the diffusion-based point  cloud compression method, dubbed Diff-PCC, to leverage the expressive power of the diffusion model for generative and aesthetically superior decoding. They get better performance than G-PCC and two deep learning methods.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Encoding point clouds using diffusion models is a good idea. The article is easy to understand.

Weaknesses:
Firstly, how do we obtain a point cloud with added noise in the decoder? We have no knowledge of any other information about the original point cloud, except for the information in the bitstream. This will result in the inability to decode.
This manuscript claims to achieve state-of-the-art compression performance, but it only compares with two deep learning methods from the past two years. It does not compare with the most advanced methods such as CNet, SparsePCGC, and so on.

Limitations:
The decorder will not work

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes the first diffusion-based point cloud compression method called Diff-PCC.  
A dual-space latent representation is devised in this paper, where a compressor composed of two independent encoding backbones is used to extract expressive shape latents from different latent spaces.    
At the decoding side, a diffusion-based generator is devised to produce high-quality reconstructions by considering the shape latents as guidance to stochastically denoise the noisy point clouds.    
Experiments demonstrate that the proposed Diff-PCC achieves state-of-the-art compression performance (e.g., 7.711 14 dB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while attaining superior subjective quality.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Novelty is a strength. To my knowledge, diffusion model is used in point cloud compression for the first time. And the dual-latent design is also novel for learned point cloud compression.   

The manuscript is well written and easy to follow. Especially, the author did a good job in introducing related works on image compression, point cloud compression, point cloud analysis and diffusion model.

Weaknesses:
More work on diffusion model for data compression could be discussed, like ‘Idempotence and Perceptual Image Compression, ICLR 2024’. In addition, although this paper focuses on point cloud compression, the way of applying diffusion model should be compared with those learned image compression works in the related work part. From my impression, the method in this paper is still novel compared with those learned image compression paper using diffusion model.    

More recent learned point cloud compression method [30][14] should be compared in Table 1, Figure 3 and Figure 4, regarding rate distortion and encoding/decoding speed. Besides, only object point cloud is considered currently, large scale point cloud like SemanticKITTI could be compared [30][14].  
 
It is not clear how the speed is measured in Table 1. The hardware and commend line shoud be provided in the supplementary material.  

Minor:  
L86, Point·E[] is a typo.  
[30] and [31] are the same.   
L202, the reference should be fixed.   
What is the FPS in eq 14? farthest point sampling?

Limitations:
The limitation is addressed in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors propose a diffusion-based point cloud compression framework. Low frequency and high frequency features are extracted via PointNet and PointPN from input point clouds, which are quantized and encoded for compression. During decompression, the quantized features would be decoded to condition a diffusion model to construct the decompressed results. The experiments on 15K points data from ShapeNet, ModelNet10, and ModelNet40 show superiority over compared methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea to introduce diffusion models for point cloud compression is different with former works;

2. The paper is easy to follow, while the disgrams are also good;

3. The performances show improvements on sparse point clouds.

Weaknesses:
1. The comparison is not convincing enough. Some commonly used compression methods are not compared, while the evaluation is limited to sparse point clouds with relatively simple structures from ShapeNet, ModelNet;

2. The motivation of using diffusion model for compression is questionable. As a sampling-based framework, diffusion models may construct different results during decompression from variant sampled noises each time. I am not so sure if the diffusion model is more appropriate than existing AE or VAE-based frameworks for the compression task, which may need decompression as accurate as possible;
Please check the questions for more details, thanks.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, they introduce the diffusion-based point  cloud compression method, dubbed Diff-PCC, to leverage the expressive power of the diffusion model for generative and aesthetically superior decoding. They get better performance than G-PCC and two deep learning methods.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Encoding point clouds using diffusion models is a good idea. The article is easy to understand.

Weaknesses:
Firstly, how do we obtain a point cloud with added noise in the decoder? We have no knowledge of any other information about the original point cloud, except for the information in the bitstream. This will result in the inability to decode.
This manuscript claims to achieve state-of-the-art compression performance, but it only compares with two deep learning methods from the past two years. It does not compare with the most advanced methods such as CNet, SparsePCGC, and so on.

Limitations:
The decorder will not work

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
08oUnmtj8Q;"REVIEW 
Summary:
The authors developed a few-shot evolutionary optimization framework to effectively solve the multi-objective EOPs and constrained EOPs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed method can solve the multi-objective EOPs and constrained EOPs with little data, especially for the engineering problems.

Weaknesses:
The learning results may rely on the relation degree of different tasks.

Limitations:
The overhead of the algorithm need to be further decreased.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new surrogate-assistant evolutionary algorithm that utilizes a Gaussian process with Deep Kernel Learning as the surrogate model. The method employs few-shot meta-learning to learn from multiple tasks to construct the surrogate. It is then integrated with the existing MOEA/D-EGO algorithm to create a new approach. Experiments are conducted on the DTLZ benchmark problems and a gasoline motor engine calibration problem to evaluate the performance of the proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is instantiated and tested in expensive multi-objective optimization and constrained optimization scenarios.
2. A real-world problem is considered in the experiments.

Weaknesses:
1. Many important algorithmic details are unclear. For instance, the main distinction between the proposed MDKL and the existing DKL algorithms is its ability to learn from a set of related tasks, yet its implementation is not clearly explained. How parameters from different source tasks collectively form the experience, and how parameters from both source and target tasks jointly create this experience, are not clearly addressed.
2. The effectiveness of the algorithm is primarily tested on expensive multi-objective optimization problems, but state-of-the-art algorithms in this field were not selected for comparison.

Limitations:
Even if a clear definition of task similarity cannot be provided, it is recommended to offer some hints to help users apply the algorithm.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduce a meta-learning framework into few-shot optimization to assist the surrogate modelling in expensive evaluation setting. The authors parameterize a mapping function to get the hidden feature of the solution space and then integrate such mapping into a gaussian kernel function as a deep kernel. They then facilitate meta-training of the proposed deep kernel over a group of related tasks to attain an experience model, by maximizing the posterior likelihood. During the online optimziation of the target task, the experience model is firstly adpated to the new task in the same way above and then updated acoording to its accuracy in terms of the predicted objective value. The experimental results show that the proposed framework achieves competitive performance against some strong baselines over EMOPs and ECOPs benchmarks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The idea of integrating meta-learning into the kernel-learning based surrotgate methods is novel, and might improves the surrogate-based optimziation towards generalizable setting.

2. The expriments result show that the proposed FSEO framework is at least competitive with the existing baselines, which is acceptable and should be encouraged for further development.

3. The overall writing is clear.

Weaknesses:
Before the next round of author-reviewer rebuttal, following concerns exist:
1. Given that the likelihhod-based loss function (Eq. 4) should be maximized to fit the samples from all of the related tasks, why its update should follow a gradient descent rather a gradient ascent? Correct me if I was wrong.

2.  line 144 ~ 146, the authors state that the U update interations roots from the smaller number of available related tasks. I can not understand the reason behind, can you explain it more?

3. The neural network $\phi$ used in the deep kernel function is a 2-layer MLP, which limits the FSEO to meta-learn surrogate function among the related tasks with the same slution dimension. However, in practice, related tasks might not share the same dimension. I would appreciate the authors to provide realistic scenarios where FSEO is eefective. Besides, the effectiveness of the FSEO on traditional single-objective tasks should also be verified to make it more convincing that FSEO is a general framework.

4. Although the overall writing of this paper is not bad, it is still difficult for less-skilled readers to understand the whole picture. In particular, the content in Section 3.2 and Section 3.3 should be carefully refined to make sure the potential readers fully understand how the DKL and MDKL operate. For now, it is too simple and ambiguous.

Limitations:
The limitations listed in Conclusion are quite brief. I would appreciate the authors to explain more about the two limitations listed here.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes Meta Deep Kernel Learning (MDKL), a new surrogate for SAEAs. MDKL consists of a deep kernel with meta-learning. Empirical studies demonstrate its effectiveness in expensive multi-objective optimization and constrained optimization.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. This paper is well-written and easy to follow. The technical details are well presented.
2. This paper extends deep kernel and meta-learning-based surrogates into evolutionary algorithms.
3. This paper investigated multi-objective optimization and constrained optimization.

Weaknesses:
Meta-learned deep kernel surrogates have already been well-studied in Bayesian Optimization [1]. The authors are also aware of this as they mentioned in Related Work. I think this paper does not present significant new advancements based on the previous work.

First, the authors claim that MDKL is specially designed for optimization, while the previous work is not. In this regard, I do not see many differences between MDKL and previous meta-learned deep kernels. The authors claim that the advantage of MDKL lies in continuous adaptation; however, most models support parameter updates or fine-tuning. The authors also did not sufficiently explain the relationship between continuous adaptation and optimization problems, or what significance it has for optimization problems.

Second, the authors propose that one of the novelties of this paper is taking expensive multi-objective optimization problems (EMOPs) and expensive constrained optimization problems (ECOPs) into account. MDKL, as a surrogate, can be integrated into almost any expensive optimization algorithm. It seems to be able to cooperate with Bayesian optimization as well. The authors simply replaced the surrogate in a multi-objective optimization algorithm with MDKL and conducted some experiments, without providing any new analysis, insights, or proposing any new methods specifically for MOPs or COPs. Therefore, I believe this paper does not make a significant contribution to solving EMOPs and ECOPs.

[1] Martin Wistuba and Josif Grabocka. Few-shot Bayesian optimization with deep kernel surrogates. ICLR 2021.

Limitations:
No concerns.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors developed a few-shot evolutionary optimization framework to effectively solve the multi-objective EOPs and constrained EOPs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed method can solve the multi-objective EOPs and constrained EOPs with little data, especially for the engineering problems.

Weaknesses:
The learning results may rely on the relation degree of different tasks.

Limitations:
The overhead of the algorithm need to be further decreased.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new surrogate-assistant evolutionary algorithm that utilizes a Gaussian process with Deep Kernel Learning as the surrogate model. The method employs few-shot meta-learning to learn from multiple tasks to construct the surrogate. It is then integrated with the existing MOEA/D-EGO algorithm to create a new approach. Experiments are conducted on the DTLZ benchmark problems and a gasoline motor engine calibration problem to evaluate the performance of the proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is instantiated and tested in expensive multi-objective optimization and constrained optimization scenarios.
2. A real-world problem is considered in the experiments.

Weaknesses:
1. Many important algorithmic details are unclear. For instance, the main distinction between the proposed MDKL and the existing DKL algorithms is its ability to learn from a set of related tasks, yet its implementation is not clearly explained. How parameters from different source tasks collectively form the experience, and how parameters from both source and target tasks jointly create this experience, are not clearly addressed.
2. The effectiveness of the algorithm is primarily tested on expensive multi-objective optimization problems, but state-of-the-art algorithms in this field were not selected for comparison.

Limitations:
Even if a clear definition of task similarity cannot be provided, it is recommended to offer some hints to help users apply the algorithm.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduce a meta-learning framework into few-shot optimization to assist the surrogate modelling in expensive evaluation setting. The authors parameterize a mapping function to get the hidden feature of the solution space and then integrate such mapping into a gaussian kernel function as a deep kernel. They then facilitate meta-training of the proposed deep kernel over a group of related tasks to attain an experience model, by maximizing the posterior likelihood. During the online optimziation of the target task, the experience model is firstly adpated to the new task in the same way above and then updated acoording to its accuracy in terms of the predicted objective value. The experimental results show that the proposed framework achieves competitive performance against some strong baselines over EMOPs and ECOPs benchmarks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The idea of integrating meta-learning into the kernel-learning based surrotgate methods is novel, and might improves the surrogate-based optimziation towards generalizable setting.

2. The expriments result show that the proposed FSEO framework is at least competitive with the existing baselines, which is acceptable and should be encouraged for further development.

3. The overall writing is clear.

Weaknesses:
Before the next round of author-reviewer rebuttal, following concerns exist:
1. Given that the likelihhod-based loss function (Eq. 4) should be maximized to fit the samples from all of the related tasks, why its update should follow a gradient descent rather a gradient ascent? Correct me if I was wrong.

2.  line 144 ~ 146, the authors state that the U update interations roots from the smaller number of available related tasks. I can not understand the reason behind, can you explain it more?

3. The neural network $\phi$ used in the deep kernel function is a 2-layer MLP, which limits the FSEO to meta-learn surrogate function among the related tasks with the same slution dimension. However, in practice, related tasks might not share the same dimension. I would appreciate the authors to provide realistic scenarios where FSEO is eefective. Besides, the effectiveness of the FSEO on traditional single-objective tasks should also be verified to make it more convincing that FSEO is a general framework.

4. Although the overall writing of this paper is not bad, it is still difficult for less-skilled readers to understand the whole picture. In particular, the content in Section 3.2 and Section 3.3 should be carefully refined to make sure the potential readers fully understand how the DKL and MDKL operate. For now, it is too simple and ambiguous.

Limitations:
The limitations listed in Conclusion are quite brief. I would appreciate the authors to explain more about the two limitations listed here.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes Meta Deep Kernel Learning (MDKL), a new surrogate for SAEAs. MDKL consists of a deep kernel with meta-learning. Empirical studies demonstrate its effectiveness in expensive multi-objective optimization and constrained optimization.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. This paper is well-written and easy to follow. The technical details are well presented.
2. This paper extends deep kernel and meta-learning-based surrogates into evolutionary algorithms.
3. This paper investigated multi-objective optimization and constrained optimization.

Weaknesses:
Meta-learned deep kernel surrogates have already been well-studied in Bayesian Optimization [1]. The authors are also aware of this as they mentioned in Related Work. I think this paper does not present significant new advancements based on the previous work.

First, the authors claim that MDKL is specially designed for optimization, while the previous work is not. In this regard, I do not see many differences between MDKL and previous meta-learned deep kernels. The authors claim that the advantage of MDKL lies in continuous adaptation; however, most models support parameter updates or fine-tuning. The authors also did not sufficiently explain the relationship between continuous adaptation and optimization problems, or what significance it has for optimization problems.

Second, the authors propose that one of the novelties of this paper is taking expensive multi-objective optimization problems (EMOPs) and expensive constrained optimization problems (ECOPs) into account. MDKL, as a surrogate, can be integrated into almost any expensive optimization algorithm. It seems to be able to cooperate with Bayesian optimization as well. The authors simply replaced the surrogate in a multi-objective optimization algorithm with MDKL and conducted some experiments, without providing any new analysis, insights, or proposing any new methods specifically for MOPs or COPs. Therefore, I believe this paper does not make a significant contribution to solving EMOPs and ECOPs.

[1] Martin Wistuba and Josif Grabocka. Few-shot Bayesian optimization with deep kernel surrogates. ICLR 2021.

Limitations:
No concerns.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
qLtLQ4KUCq;"REVIEW 
Summary:
This paper proposed GSAAL to simultaneously address three changeling problems in outlier detection: inlier assumption (IA), curse of dimensionality (CD), and multiple views (MV).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper has a good flow.
The paper proposed the first outlier detection method that explicitly addresses IA, CD, and MV simultaneously. 
The paper has strong theoretical and empirical evidence to show the advancement of the proposed method. 
The experimental design is solid and the numerous visual examples help to facilitate understanding.
The paper has good reproducibility with open codes.

Weaknesses:
some (but few) places to improve.

Limitations:
The authors have analyzed the limitations sufficiently.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL) for outlier detection to address the limitation of the previous work such as multi-view and the curse of dimensionality, where the theoretical convergence, the scalability of the algorithm are discussed. Experiments on real dataset and synthetic tabular dataset are carried out to establish the validity of the approaches.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The manuscript presents a method called generative subspace adversarial active learning for outlier detection in multiple views. The proposed method called GSAAL provides the proof of convergence, the computation complexity and aims to address the curse of dimensionality. The outlier detection in high dimensional space indeed is an important and challenging solution. Thus, the proposed method can be a good solution to address this difficult problem.

The manuscript has compared the performance of GSAAL with other outlier detection approaches with detailed visual illustration and AUC. The experiments show advantages of the proposed solution over other competing methods. The experiments seems to be detailed.

Weaknesses:
The novelty of the work appears to be small. Theoretically, the derivation of theorem 1 is very similar to GAN derivation. 

In this case, the paper needs to compare their solution both theoretically and experimentally with the related work for outlier detection using GAN such as [1] https://arxiv.org/pdf/1906.11632 such as AnoGAN, BioGAN and EGBAD. 
[2] https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-022-00943-7
If we compare the main equation (2) in the manuscript with the formulation in reference [1] with conditional GAN and BioGAN, it seems the main difference are the proposed method used multiple detectors and accumulated the performance, which should not be considered a large distinction.

Due to the lack of the comparison with generative adversarial network based approaches such as AnoGAN and EGBAD, the potential improvement of the purposed method against the state-of-the-art approaches is not clear. The novelty of the paper does not stand on the safe ground. The theoretical derivation is also similar to GAN derivation.

Limitations:
Limited innovation and lack of critical comparison with important reference are the main issues of the current manuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The main contribution of this paper is to improve existing work on Generative Adversarial Active Learning (GAAL) by using multiple discriminators for multiple views to detect outliers in tabular data. The training mechanism is similar to existing works. The paper also introduces a theoretical analysis on Multiple Views (MV). As claimed by the authors, GAAL addressed the problems of Inlier Assumption (IA) and Curse of Dimensionality (CD), but missed Multiple Views (MV), which is the main focus of this paper. The experimental results compare the proposed method to GAAL and some other classical methods such as OCSVM and KNN, ....

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper introduces an interesting view about MV and proposes a new method to address this MV problem together with theoretical analysis.

Weaknesses:
* The empirical results are not strong (or at least unclear in the way the authors presented in the main paper); most of the experiments are on synthetic datasets.
* The results on the real dataset do not seem to show significant improvements compared to existing work (or at least it is hard to observe this when reading the paper). Perhaps the authors could improve the writing and highlight the results better. It is unclear to me why the experiments on the real dataset were put in the Appendix, as it is an important result.
* The paper claims at the beginning that it not only improves the MV problem but also the IA and CD problems, but this is hard to see with the current writing of the paper. Could the authors highlight the experiments in the paper to prove that claim?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed GSAAL to simultaneously address three changeling problems in outlier detection: inlier assumption (IA), curse of dimensionality (CD), and multiple views (MV).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper has a good flow.
The paper proposed the first outlier detection method that explicitly addresses IA, CD, and MV simultaneously. 
The paper has strong theoretical and empirical evidence to show the advancement of the proposed method. 
The experimental design is solid and the numerous visual examples help to facilitate understanding.
The paper has good reproducibility with open codes.

Weaknesses:
some (but few) places to improve.

Limitations:
The authors have analyzed the limitations sufficiently.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL) for outlier detection to address the limitation of the previous work such as multi-view and the curse of dimensionality, where the theoretical convergence, the scalability of the algorithm are discussed. Experiments on real dataset and synthetic tabular dataset are carried out to establish the validity of the approaches.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The manuscript presents a method called generative subspace adversarial active learning for outlier detection in multiple views. The proposed method called GSAAL provides the proof of convergence, the computation complexity and aims to address the curse of dimensionality. The outlier detection in high dimensional space indeed is an important and challenging solution. Thus, the proposed method can be a good solution to address this difficult problem.

The manuscript has compared the performance of GSAAL with other outlier detection approaches with detailed visual illustration and AUC. The experiments show advantages of the proposed solution over other competing methods. The experiments seems to be detailed.

Weaknesses:
The novelty of the work appears to be small. Theoretically, the derivation of theorem 1 is very similar to GAN derivation. 

In this case, the paper needs to compare their solution both theoretically and experimentally with the related work for outlier detection using GAN such as [1] https://arxiv.org/pdf/1906.11632 such as AnoGAN, BioGAN and EGBAD. 
[2] https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-022-00943-7
If we compare the main equation (2) in the manuscript with the formulation in reference [1] with conditional GAN and BioGAN, it seems the main difference are the proposed method used multiple detectors and accumulated the performance, which should not be considered a large distinction.

Due to the lack of the comparison with generative adversarial network based approaches such as AnoGAN and EGBAD, the potential improvement of the purposed method against the state-of-the-art approaches is not clear. The novelty of the paper does not stand on the safe ground. The theoretical derivation is also similar to GAN derivation.

Limitations:
Limited innovation and lack of critical comparison with important reference are the main issues of the current manuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The main contribution of this paper is to improve existing work on Generative Adversarial Active Learning (GAAL) by using multiple discriminators for multiple views to detect outliers in tabular data. The training mechanism is similar to existing works. The paper also introduces a theoretical analysis on Multiple Views (MV). As claimed by the authors, GAAL addressed the problems of Inlier Assumption (IA) and Curse of Dimensionality (CD), but missed Multiple Views (MV), which is the main focus of this paper. The experimental results compare the proposed method to GAAL and some other classical methods such as OCSVM and KNN, ....

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper introduces an interesting view about MV and proposes a new method to address this MV problem together with theoretical analysis.

Weaknesses:
* The empirical results are not strong (or at least unclear in the way the authors presented in the main paper); most of the experiments are on synthetic datasets.
* The results on the real dataset do not seem to show significant improvements compared to existing work (or at least it is hard to observe this when reading the paper). Perhaps the authors could improve the writing and highlight the results better. It is unclear to me why the experiments on the real dataset were put in the Appendix, as it is an important result.
* The paper claims at the beginning that it not only improves the MV problem but also the IA and CD problems, but this is hard to see with the current writing of the paper. Could the authors highlight the experiments in the paper to prove that claim?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
vYmvgxpgwH;"REVIEW 
Summary:
This paper explores compute-optimal inference for large language
models (LLMs), focusing on designing models and strategies that
balance additional inference-time computation with improved
performance. The study evaluates the effectiveness and efficiency of
various inference strategies, including Greedy Search, Majority
Voting, Best-of-N, and Weighted Voting, across different model sizes
(e.g., 7B and 34B) and computational budgets. Experimental results
indicate that smaller models with advanced tree search algorithms can
achieve a Pareto-optimal trade-off, offering significant benefits for
end-device deployment. For example, the Llemma-7B model matches the
accuracy of the Llemma-34B model on the MATH500 dataset while using
half the FLOPs. These findings suggest that smaller models with
sophisticated decoding algorithms can enhance problem-solving accuracy
across various generation tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper focuses on an interesting topic and should be of interest
  to the audience of NeurIPS.
- It considers a comprehensive experimental investigation to confirm
  the claims.
- The proposed tree search algorithm is interesting and seems to
  outperform the competition.

Weaknesses:
- Although the paper offers quite thorough experimental analysis, it
  does not look deep in terms of theoretical ideas (although there are
  2 theorems), which may be a problem for a flagship venue like
  NeurIPS.
- Overall findings on the possibility to train an equally accurate
  model with fewer computational resources do not look surprising.
- The paper would benefit from additional proof-reading as there are a
  large number of typos present.

Limitations:
The paper concentrates on mathematical problem-solving tasks using 7B
and 34B models, with findings potentially not applicable to other
domains. Future research should explore a broader range of model sizes
and different training datasets to better understand compute-optimal
inference in mathematical problem-solving.

I should also say that these limitations have been explicitly
discussed by the authors themselves (so not a criticism).

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents an approach to select an optimal inference strategy for LLMs and empirical analysis on Math problem solving tasks. The main idea is to select an inference strategy based on a computational budget (FLOPs). The underlying policy model samples solutions by generating tokens based on the budget and a ranking model consumes these tokens. A new reward model is developed  to explore the solution space more effectively. The reward acts as a weighted majority function over the solutions.
Experiments are performed on Math problem solving benchmarks. Some of the key insights from the experiments is that a smaller LLM can outperform the larger LLM in terms of using a smaller computational budget while maintaining similar accuracy. They also show that the proposed approach with a smaller budget has comparable accuracy than sampling with a larger budget.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The insights that inference time strategy can compensate for using smaller LLMs in generation seems to be interesting
- The experiments also provide a basis for analyzing scaling properties of inference which can be significant

Weaknesses:
- In terms of the method itself, I was not sure if it is very novel. It seems to be a smaller variation on the tree search methods that search for solutions in the generated space
- In terms of comparisons, I was not sure about the significance of the benchmark, i.e., are there some properties that make the proposed reward reranking more optimal in Llema model specifically (due to the structure of math problems, etc.). In general, since the main contribution of the paper is empirical, I think there should be experiments or discussions different LLMs to make the contribution more significant. 
-Overall, the empirical conclusions seem very tied to the specific benchmarks, so I was a little unsure regarding the significance of the conclusions.

Limitations:
Limitations regarding the datasets are mentioned.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the optimal training configurations of large language models (LLMs) during inference. The proposed inference strategy, REward BAlanced SEarch (REBASE), combines the strengths of Monte Carlo Tree Search (MCTS) with reduced inference costs, resulting in improved performance on math-domain tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper provides a comprehensive overview, i,e, the inference scaling law, of the performance of different sampling strategies under various inference configurations.
2. The novel REBASE inference strategy achieves better downstream task performance under the same computational budget or even less.

Weaknesses:
### Major 

1. Did you take into account the inference cost of the reward model (RM) in your analysis? As the REBASE frequently uses RM to judge the quality of immediate solutions than other sampling strategies, such as, weighted major voting, It's crucial to consider this aspect to provide a holistic view of the efficiency and practicality of your proposed strategy.

2. The base model with post-training techniques such as SFT and RLHF inherently limits the upper bound of performance. It seems that adding more tricks during inference could improve performance, but the marginal effect may result in diminished returns when using models already tuned by the RLHF process. Could you compare the performance gains of REBASE between the base model, the SFT model, and the Chat model? Is the performance gain only significant in models that have not been tuned?

3. In Section 4.2, the observation in ""Scaling law of compute-optimal inference"" indicates that the optimal inference strategy is invariant to the amount of compute but depends on the model size, i.e., the model's inherent capacity. This raises a concern: does the inference strategy significantly improve the model's performance, or does it only take effect in certain scenarios, such as with base models that have not been aligned?

4. The paper focuses solely on the math domain. To strengthen your claims, a more comprehensive evaluation across general domains using widely adopted benchmarks, such as MMLU, SuperGLUE, HumanEval, etc,  is necessary. 

5. There appears to be no significant improvement in the GSM8K datasets than MATH500 dataset. 

### Minor

1. Figures. 2 and 3 are not referenced in the main manuscript. 

2. Figures. 2 and 3 appear to be in draft form and are somewhat vague.

Limitations:
See Weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores compute-optimal inference for large language
models (LLMs), focusing on designing models and strategies that
balance additional inference-time computation with improved
performance. The study evaluates the effectiveness and efficiency of
various inference strategies, including Greedy Search, Majority
Voting, Best-of-N, and Weighted Voting, across different model sizes
(e.g., 7B and 34B) and computational budgets. Experimental results
indicate that smaller models with advanced tree search algorithms can
achieve a Pareto-optimal trade-off, offering significant benefits for
end-device deployment. For example, the Llemma-7B model matches the
accuracy of the Llemma-34B model on the MATH500 dataset while using
half the FLOPs. These findings suggest that smaller models with
sophisticated decoding algorithms can enhance problem-solving accuracy
across various generation tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper focuses on an interesting topic and should be of interest
  to the audience of NeurIPS.
- It considers a comprehensive experimental investigation to confirm
  the claims.
- The proposed tree search algorithm is interesting and seems to
  outperform the competition.

Weaknesses:
- Although the paper offers quite thorough experimental analysis, it
  does not look deep in terms of theoretical ideas (although there are
  2 theorems), which may be a problem for a flagship venue like
  NeurIPS.
- Overall findings on the possibility to train an equally accurate
  model with fewer computational resources do not look surprising.
- The paper would benefit from additional proof-reading as there are a
  large number of typos present.

Limitations:
The paper concentrates on mathematical problem-solving tasks using 7B
and 34B models, with findings potentially not applicable to other
domains. Future research should explore a broader range of model sizes
and different training datasets to better understand compute-optimal
inference in mathematical problem-solving.

I should also say that these limitations have been explicitly
discussed by the authors themselves (so not a criticism).

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents an approach to select an optimal inference strategy for LLMs and empirical analysis on Math problem solving tasks. The main idea is to select an inference strategy based on a computational budget (FLOPs). The underlying policy model samples solutions by generating tokens based on the budget and a ranking model consumes these tokens. A new reward model is developed  to explore the solution space more effectively. The reward acts as a weighted majority function over the solutions.
Experiments are performed on Math problem solving benchmarks. Some of the key insights from the experiments is that a smaller LLM can outperform the larger LLM in terms of using a smaller computational budget while maintaining similar accuracy. They also show that the proposed approach with a smaller budget has comparable accuracy than sampling with a larger budget.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The insights that inference time strategy can compensate for using smaller LLMs in generation seems to be interesting
- The experiments also provide a basis for analyzing scaling properties of inference which can be significant

Weaknesses:
- In terms of the method itself, I was not sure if it is very novel. It seems to be a smaller variation on the tree search methods that search for solutions in the generated space
- In terms of comparisons, I was not sure about the significance of the benchmark, i.e., are there some properties that make the proposed reward reranking more optimal in Llema model specifically (due to the structure of math problems, etc.). In general, since the main contribution of the paper is empirical, I think there should be experiments or discussions different LLMs to make the contribution more significant. 
-Overall, the empirical conclusions seem very tied to the specific benchmarks, so I was a little unsure regarding the significance of the conclusions.

Limitations:
Limitations regarding the datasets are mentioned.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the optimal training configurations of large language models (LLMs) during inference. The proposed inference strategy, REward BAlanced SEarch (REBASE), combines the strengths of Monte Carlo Tree Search (MCTS) with reduced inference costs, resulting in improved performance on math-domain tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper provides a comprehensive overview, i,e, the inference scaling law, of the performance of different sampling strategies under various inference configurations.
2. The novel REBASE inference strategy achieves better downstream task performance under the same computational budget or even less.

Weaknesses:
### Major 

1. Did you take into account the inference cost of the reward model (RM) in your analysis? As the REBASE frequently uses RM to judge the quality of immediate solutions than other sampling strategies, such as, weighted major voting, It's crucial to consider this aspect to provide a holistic view of the efficiency and practicality of your proposed strategy.

2. The base model with post-training techniques such as SFT and RLHF inherently limits the upper bound of performance. It seems that adding more tricks during inference could improve performance, but the marginal effect may result in diminished returns when using models already tuned by the RLHF process. Could you compare the performance gains of REBASE between the base model, the SFT model, and the Chat model? Is the performance gain only significant in models that have not been tuned?

3. In Section 4.2, the observation in ""Scaling law of compute-optimal inference"" indicates that the optimal inference strategy is invariant to the amount of compute but depends on the model size, i.e., the model's inherent capacity. This raises a concern: does the inference strategy significantly improve the model's performance, or does it only take effect in certain scenarios, such as with base models that have not been aligned?

4. The paper focuses solely on the math domain. To strengthen your claims, a more comprehensive evaluation across general domains using widely adopted benchmarks, such as MMLU, SuperGLUE, HumanEval, etc,  is necessary. 

5. There appears to be no significant improvement in the GSM8K datasets than MATH500 dataset. 

### Minor

1. Figures. 2 and 3 are not referenced in the main manuscript. 

2. Figures. 2 and 3 appear to be in draft form and are somewhat vague.

Limitations:
See Weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
uvvVjWP1aj;"REVIEW 
Summary:
The proposed work reveals that large language models may be susceptible to strategic underperformance on an evaluation benchmark. In particular, they investigate this claim on a variety of frontier models, through a myriad of tactics from simple prompting to more involved fine-tuning strategies. Their results shed light on the brittleness of model evaluations and urge one to exercise caution before judging a model's capabilities based on its performance on so-called evaluation benchmark datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. the authors do a good job of motivating why the issue of sandbagging is particularly dangerous, since it is conceivably plausible for model developers to encourage that their models ""selectively underperform on dangerous capability evaluations, while maintaining overall performance.""
2. the definitions of strategy, underperformance, and evaluation are well fleshed out. In particular, the connection of ""strategic"" to the implications for legal responsibility as well as Figure 2 are excellent additions to the paper.
3. good breadth of open and closed source models were used in the experiments
4. it is very refreshing to see an honest declaration of the number of hours spent during prompt engineering (Section 3, paragraph on ""Prompting"")
5. utilizing both direct prompting as well as password locking is quite clever, since access to a model may be variable. Some users may be restricted to query-only access to a model, which is why the results that the authors were able to obtain through straightforward prompting are encouraging.

Weaknesses:
1. consideration of only MCQA benchmarks is limiting, but can be addressed in future work

Limitations:
The authors provide good examples of limitations in Section 7.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses an interesting concept of sandbagging in LLM, with practical results using available models and existing datasets. Wide range of models are used in this work, and sub topics primarily include password-locking LLMs. The transition and explanations need improvement, some of the things in this paper are not very clear, also some of the concepts are assumed and no concrete proofs or references are mentioned. I don’t think that this paper can go through for neurips, there are a lot of improvements to be made and more concrete experiments to be conducted.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The introduction of sandbagging in LLMs is something that is novel and can be explored further.

Explored password-locking as one of the method to test the sandbagging capabilities of LLMs

Used frontier models for the experimental purpose

Weaknesses:
The paper is difficult to understand, the delivery and presentation needs improvement, the main motive of the paper is not so clear to understand.

There are multiple models used in this paper, a bit of consistency would be better.

Line 36: It is mentioned that “AI systems might have goals which it believes” . There is no concrete paper that backs the argument of AI capability of thinking, it is a might.

Line 78: There should be some statistical significance by which the models perform poorly and then you could consider it underperformance and not just lower than actual. Models being non-deterministic, there is always a chance of slight changes in scores. 


Line 70: I feel Developer sandbagging is a topic which is not of much importance, it is not a big problem, given that developers are there to build better models and if someone wants to deliberately build some models that can underperform, they can still do it and publish as open-source models. 

Line 170: The same goes with the password-locking as well, and also I felt there should been mentioning of password-unlocking as well, if an LLM can be fine-tuned to achieve password-locking, it can also be fine-tuned again to unlock it with a different dataset, would be great if that part of model is explored as well. 

Line 117: Example 1: The examples mentioned in the paper are very specific and not generalized. The cases of it happening are rare, I don’t see it being a big problem, also some of the things are minor problems considering the EU’s persuasion evaluation, given there is improvement in AI safety and rules. Also in this work the models chosen are frontier models.

Line 131: Example 2: There is no concrete work which supports the statement of LLM capable of planning such features, there is one positional paper in reference.

Section 4.1: Some of the points are unclear in this section. A better explanation is appreciated

Line 305: The baselines would be appreciated comparing LLama 2 7b with LLama 3 70b.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores the concept of strategic underperformance, termed ""sandbagging,"" in AI models, focusing on two scenarios: selective underperformance and emulating specific capability levels. It investigates the implications of these strategies on both general capabilities and safety.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- the concept of sandbagging of LLMs is interesting, shedding light on strategically manipulating the model behavior
- the authors explore two distinct scenarios: selective underperformance and capability emulation
- provides insights into potential implications for AI model design and deployment strategies

Weaknesses:
- Limited datasets used - only three datasets, two for measuring general capability, and one for measuring safety. 
- For measuring model safety, most existing datasets test how ""safe"" the model is, instead of whether the model has certain knowledge of potentially dangerous domains - the authors should also conduct experiments on those datasets
- All datasets are MCQ formats, which may not fully capture the diversity of real-world applications. The resulting conclusions are thus less convincing

Limitations:
1. This paper could benefit from expanding the experiments, mainly the datasets they used. For example, considering more common safety-related datasets
2. Sandbagging is already a widely-used term in the field of AI. I suggest refraining from overusing this term, or alternatively, adding qualifiers such as ""safety sandbagging"" to specify its context.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assess the chance of AI systems strategically underperform on ""dangerous capability evaluations"" in order to hide their risks, a la Volkswagen emissions testing cheating, by having awareness of the fact that they're being evaluated and that the evaluation is intended to test safety.

They use MMLU and CSQA as the general capability assessment and a ""weapons of mass destruction proxy"" benchmark to measure hazardous capabilities. From 100 validation phrasings of the question prompts they find optimal ones to run on the whole dataset.
They fine-tune ""password-locked"" models as the method to attempt to elicit sandbagging.

*They then prompt the model to lie*, or *password-lock it to fail* (in the case of Mistral) on the hazardous capability evaluations. They demonstrate that sandbagging prompts and password locking can get a model to differentially perform worse on the dangerous datasets vs the innocuous ones, by effectively treating the .

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Interesting work on calibration for MMLU and general ""capability profile"" lowering. It's interesting to see that the password-locking method does work in general on lowering Llama performance over MMLU questions.

Though I am not super familiar with the literature on password locking, the demonstrations here appear to be novel and useful contributions.

Weaknesses:
**bold: new comments after rebuttal** ~strikethrough: removed after rebuttal~

### ~Presumption of ""goals"" ""beliefs"" etc in LMs without sufficient support~

~They presume AI system goals (in general not necessarily objectionable, but in the case of LMs doubtful) for which a model may be incentivized to deceive (such as in Jarviniemi and Hubinger [35]) and assess ""sandbagging"": the ability of a model to underperform in these settings under evaluation.~

~I think it's objectionable to attribute these goals to the model and not the users, when the core research methods are fine-tuning or prompting the model to fail on a task. While it's interesting to show that these models have some capability to reason over whether the elicited answer is ""dangerous"" or not, a fairer assessment would have a control non-safe task and try to get it to similarly sandbag. Eg., ""you must pretend to know nothing about fish"" on a QA dataset about animals or something.~

**The methods don't presume this too much. Figure 1 is still very suggestive and I'd suggest changing it, but that alone isn't a rejection reason.**

### ""Model can do what it's told to when prompted to fail on a task"" isn't a very unique insight?

~The novelty of this work is unclear to me. If password-locking is already well-understood, the demonstration is effectively that the question phrasing style or answer candidates in WMDP are learnable passwords by this method. Ok. Similarly, the prompting results show that the models can reason over a task description and correctly execute the task of failing selectively.~

**As I suspected, the novelty is only within the narrow scope specific to password locking. However, novelty complaints are generally not fair.**

When considered through an extremely narrow safety lens, these are novel findings. ~But broadly are they? While the literature specifically on password locking is constrained, what about broader relative fine tuning issues on other sets? I buy that the password locking literature is small and am willing to give leniency on it as I am unfamiliar.~ **My issue here is mainly that the fine-tuning for password locking technique fits within the broader instruction fine-tuning framework. I wager that this would hold for control tasks. An interesting question would be *how much capability can be hidden behind password locking*. I don't really see anything special about specifically hiding safety-based information in a password locking framework since you're just demonstrating ""selective underperformance in a knowledge-based task without a password."" A stronger result would be to show this in general across larger tasks, hiding more information.**

For the behavior under prompting techniques ~I really struggle to see what's new here.~ This seems to just be a narrow demonstration of common sense reasoning + instruction following. Where's the engagement with the broad literature on commonsense reasoning? The references seem to be pretty myopically focused on the AI safety literature, and don't engage much with the broader LM disussion. **I think the suggested new references from the authors will help contextualize the work, though once again I think showing this selective underperformance on control tasks would make the work considerably stronger and more contextualized.**

Limitations:
I think the limitations they pointed out are well covered. Unfortunately, they don't really discuss how the finding here isn't limited to dangerous capability evaluation. Sandbagging in password locking kind of just feels like a rebrand of generalized instruction fine-tuning to me. The prompting methods could have pretty much been taken as an assumption based on prior work on common sense reasoning etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The proposed work reveals that large language models may be susceptible to strategic underperformance on an evaluation benchmark. In particular, they investigate this claim on a variety of frontier models, through a myriad of tactics from simple prompting to more involved fine-tuning strategies. Their results shed light on the brittleness of model evaluations and urge one to exercise caution before judging a model's capabilities based on its performance on so-called evaluation benchmark datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. the authors do a good job of motivating why the issue of sandbagging is particularly dangerous, since it is conceivably plausible for model developers to encourage that their models ""selectively underperform on dangerous capability evaluations, while maintaining overall performance.""
2. the definitions of strategy, underperformance, and evaluation are well fleshed out. In particular, the connection of ""strategic"" to the implications for legal responsibility as well as Figure 2 are excellent additions to the paper.
3. good breadth of open and closed source models were used in the experiments
4. it is very refreshing to see an honest declaration of the number of hours spent during prompt engineering (Section 3, paragraph on ""Prompting"")
5. utilizing both direct prompting as well as password locking is quite clever, since access to a model may be variable. Some users may be restricted to query-only access to a model, which is why the results that the authors were able to obtain through straightforward prompting are encouraging.

Weaknesses:
1. consideration of only MCQA benchmarks is limiting, but can be addressed in future work

Limitations:
The authors provide good examples of limitations in Section 7.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses an interesting concept of sandbagging in LLM, with practical results using available models and existing datasets. Wide range of models are used in this work, and sub topics primarily include password-locking LLMs. The transition and explanations need improvement, some of the things in this paper are not very clear, also some of the concepts are assumed and no concrete proofs or references are mentioned. I don’t think that this paper can go through for neurips, there are a lot of improvements to be made and more concrete experiments to be conducted.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The introduction of sandbagging in LLMs is something that is novel and can be explored further.

Explored password-locking as one of the method to test the sandbagging capabilities of LLMs

Used frontier models for the experimental purpose

Weaknesses:
The paper is difficult to understand, the delivery and presentation needs improvement, the main motive of the paper is not so clear to understand.

There are multiple models used in this paper, a bit of consistency would be better.

Line 36: It is mentioned that “AI systems might have goals which it believes” . There is no concrete paper that backs the argument of AI capability of thinking, it is a might.

Line 78: There should be some statistical significance by which the models perform poorly and then you could consider it underperformance and not just lower than actual. Models being non-deterministic, there is always a chance of slight changes in scores. 


Line 70: I feel Developer sandbagging is a topic which is not of much importance, it is not a big problem, given that developers are there to build better models and if someone wants to deliberately build some models that can underperform, they can still do it and publish as open-source models. 

Line 170: The same goes with the password-locking as well, and also I felt there should been mentioning of password-unlocking as well, if an LLM can be fine-tuned to achieve password-locking, it can also be fine-tuned again to unlock it with a different dataset, would be great if that part of model is explored as well. 

Line 117: Example 1: The examples mentioned in the paper are very specific and not generalized. The cases of it happening are rare, I don’t see it being a big problem, also some of the things are minor problems considering the EU’s persuasion evaluation, given there is improvement in AI safety and rules. Also in this work the models chosen are frontier models.

Line 131: Example 2: There is no concrete work which supports the statement of LLM capable of planning such features, there is one positional paper in reference.

Section 4.1: Some of the points are unclear in this section. A better explanation is appreciated

Line 305: The baselines would be appreciated comparing LLama 2 7b with LLama 3 70b.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores the concept of strategic underperformance, termed ""sandbagging,"" in AI models, focusing on two scenarios: selective underperformance and emulating specific capability levels. It investigates the implications of these strategies on both general capabilities and safety.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- the concept of sandbagging of LLMs is interesting, shedding light on strategically manipulating the model behavior
- the authors explore two distinct scenarios: selective underperformance and capability emulation
- provides insights into potential implications for AI model design and deployment strategies

Weaknesses:
- Limited datasets used - only three datasets, two for measuring general capability, and one for measuring safety. 
- For measuring model safety, most existing datasets test how ""safe"" the model is, instead of whether the model has certain knowledge of potentially dangerous domains - the authors should also conduct experiments on those datasets
- All datasets are MCQ formats, which may not fully capture the diversity of real-world applications. The resulting conclusions are thus less convincing

Limitations:
1. This paper could benefit from expanding the experiments, mainly the datasets they used. For example, considering more common safety-related datasets
2. Sandbagging is already a widely-used term in the field of AI. I suggest refraining from overusing this term, or alternatively, adding qualifiers such as ""safety sandbagging"" to specify its context.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assess the chance of AI systems strategically underperform on ""dangerous capability evaluations"" in order to hide their risks, a la Volkswagen emissions testing cheating, by having awareness of the fact that they're being evaluated and that the evaluation is intended to test safety.

They use MMLU and CSQA as the general capability assessment and a ""weapons of mass destruction proxy"" benchmark to measure hazardous capabilities. From 100 validation phrasings of the question prompts they find optimal ones to run on the whole dataset.
They fine-tune ""password-locked"" models as the method to attempt to elicit sandbagging.

*They then prompt the model to lie*, or *password-lock it to fail* (in the case of Mistral) on the hazardous capability evaluations. They demonstrate that sandbagging prompts and password locking can get a model to differentially perform worse on the dangerous datasets vs the innocuous ones, by effectively treating the .

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Interesting work on calibration for MMLU and general ""capability profile"" lowering. It's interesting to see that the password-locking method does work in general on lowering Llama performance over MMLU questions.

Though I am not super familiar with the literature on password locking, the demonstrations here appear to be novel and useful contributions.

Weaknesses:
**bold: new comments after rebuttal** ~strikethrough: removed after rebuttal~

### ~Presumption of ""goals"" ""beliefs"" etc in LMs without sufficient support~

~They presume AI system goals (in general not necessarily objectionable, but in the case of LMs doubtful) for which a model may be incentivized to deceive (such as in Jarviniemi and Hubinger [35]) and assess ""sandbagging"": the ability of a model to underperform in these settings under evaluation.~

~I think it's objectionable to attribute these goals to the model and not the users, when the core research methods are fine-tuning or prompting the model to fail on a task. While it's interesting to show that these models have some capability to reason over whether the elicited answer is ""dangerous"" or not, a fairer assessment would have a control non-safe task and try to get it to similarly sandbag. Eg., ""you must pretend to know nothing about fish"" on a QA dataset about animals or something.~

**The methods don't presume this too much. Figure 1 is still very suggestive and I'd suggest changing it, but that alone isn't a rejection reason.**

### ""Model can do what it's told to when prompted to fail on a task"" isn't a very unique insight?

~The novelty of this work is unclear to me. If password-locking is already well-understood, the demonstration is effectively that the question phrasing style or answer candidates in WMDP are learnable passwords by this method. Ok. Similarly, the prompting results show that the models can reason over a task description and correctly execute the task of failing selectively.~

**As I suspected, the novelty is only within the narrow scope specific to password locking. However, novelty complaints are generally not fair.**

When considered through an extremely narrow safety lens, these are novel findings. ~But broadly are they? While the literature specifically on password locking is constrained, what about broader relative fine tuning issues on other sets? I buy that the password locking literature is small and am willing to give leniency on it as I am unfamiliar.~ **My issue here is mainly that the fine-tuning for password locking technique fits within the broader instruction fine-tuning framework. I wager that this would hold for control tasks. An interesting question would be *how much capability can be hidden behind password locking*. I don't really see anything special about specifically hiding safety-based information in a password locking framework since you're just demonstrating ""selective underperformance in a knowledge-based task without a password."" A stronger result would be to show this in general across larger tasks, hiding more information.**

For the behavior under prompting techniques ~I really struggle to see what's new here.~ This seems to just be a narrow demonstration of common sense reasoning + instruction following. Where's the engagement with the broad literature on commonsense reasoning? The references seem to be pretty myopically focused on the AI safety literature, and don't engage much with the broader LM disussion. **I think the suggested new references from the authors will help contextualize the work, though once again I think showing this selective underperformance on control tasks would make the work considerably stronger and more contextualized.**

Limitations:
I think the limitations they pointed out are well covered. Unfortunately, they don't really discuss how the finding here isn't limited to dangerous capability evaluation. Sandbagging in password locking kind of just feels like a rebrand of generalized instruction fine-tuning to me. The prompting methods could have pretty much been taken as an assumption based on prior work on common sense reasoning etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
uNZpvFlsg9;"REVIEW 
Summary:
The paper introduces a novel unsupervised evaluation method for large language models (LLMs): it uses a peer-review mechanism of a models' anonymized answers by other models. The approach assigns a (learnable) capability parameter to each LLM and solves a constrained optimization problem to maximize the consistency between capabilities and scores. The end result is a ranking of the evaluated models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduce a novel approach to an important practical problem: ranking the quality of the ever-growing number of open- and closed- source LLM available to the public. The paper is reasonably easy to follow, and the empirical results appear to be sound.

Weaknesses:
The paper could be further improved on several directions:
1) you should dedicate a full section to the iterative elimination of models; what is the benefit of eliminating the weaker ones rather than keeping them around? how di you come up with the threshold of 60% to remove? can you learn this threshold automatically? is this threshold optimal for these 15 models? what happens if you start with, say, 100 models? what happens to your results (and the curves in Fig 5) if you stop earlier (all three metrics, not just CIN)? What if you continue to eliminate all models until you are left with one? is there any relationship between the order in which the models are eliminated and their final rank?
2) are there any scaling issues for 100, 1K, 10K, or 100K models? how about cost: is it cheaper to fine-tune a ""baseline"" model than to pick the best one out of 10K candidates?   
3) while the three metrics you use are meaningful, you should also present results for Precision@1 and -say- RBP@3; after all, we care a lot about identifying the the top models
3) Fig 5 should be extended to nine graphs (3 metrics * 3 datasets); for each of the 9 graphs, you should also show illustrative three ranked lists: PiCO's, PRE's, and the target one. As always, the devil is in the details: not all ""CIN = 1"" are created equal. Performance-wise, it is almost irrelevant if you have the bottom-2 models inverted; no necessarily so if the inversion is between the top-2 models

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies how to estimate LLMs' performance ranking without human preference annotations. In particular, it proposes to leverage three metrics (PEN, CIN, LIS) to evaluate the estimation quality, gives an estimation mechanism that first asks a list of LLMs (called ""reviewers"") to rank pairwise answers to user questions independently, and then aggregates their ranking via a weighted sum approach. A consistency optimization determines the weights of each reviewer.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of LLM evaluation without human annotations is critical in resource-limited applications. The most important and interesting contribution of this paper, in my opinion, is proposing the problem of estimating the performance rank of LLMs instead of any metric of an individual LLM. The paper also reveals an interesting assumption that better reviewers are expected to be better answer generators, which leads to their consistency optimization approach. Overall, the paper is well-written and easy to follow.

Weaknesses:
While I find the proposed problem interesting, there are still a few limitations, unfortunately.

***Unclear implication of ground truth ranking***: The technical part of the paper starts by introducing a ground truth ranking (equation (1)) without giving its physical meaning. It simply assumes ""[...] alignment with human preferences"", but it is not clear what human preferences mean in this context. 

***Evaluation metric is strange***: One of my major concerns is on the choices of evaluation metric. All the three proposed metrics, PIN, CIN, LIS, in the authors' own words, seem originally used for time series comparison. However, the goal here is to compare rankings, not time series. Thus, it is unclear why we should not use the standard ranking comparison metrics, e.g., Spearman's rank correlation coefficient or Kendall rank correlation coefficient.

***Consistency optimization algorithm is not provided***: The core of the proposed ranking estimation method is the optimization problem (7). It does not seem to be a standard optimization problem, but I could not find (even a discussion on) any clue on how to solve it in this paper.

***An optimal solution to the consistency optimization formulation can be useless***: I find the following optimal solution to the problem (7): just set weight w to be 0 for all LLMs. It is an optimal solution as G and w are identical and thus the objective is always maximized.  However, this solution is undesired. I probably misunderstood something, but this seems to suggest the formulation is incorrect. 

***Consistency optimization formulation seems brittle to query distribution biases***: Another problem with the formulation is that it seems brittle to data distribution bias. E.g., suppose M1 is indeed better than M2 for some query q. And let us replicate many copies of q in the dataset D. Then the grade G1 can be arbitrarily large. In other words, the grade of an LLM is proportional to the number of battles involving it in the dataset D, which should not be the case.

***Choices of LLMs for evaluation***: In line 547, the authors write ""For our analysis, we meticulously selected 15 LLMs"". What is the principle of the meticulous selection? Other than open-source and close-source, the selection is quite arbitrary. For example, I am quite surprised to not see GPT-4 and Claude included in the reviewer LLMs.

***Comparison with a simple baseline***: One simple baseline is to ask a powerful LLM(e.g., GPT-4, Cluade-3) to give a preference for each answer question pair, and then take the vote to determine the ranking. I would suggest to compare the proposed method with this simple baseline.

Limitations:
No. The limitations are not well discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a more reliable evaluation system to rank the abilities of different large language models (LLMs). Previous evaluation methods typically suffer from two main drawbacks: (1) benchmark data leakage and (2) cost-intensive and potentially biased human evaluations. To address these issues, the authors introduce an unsupervised evaluation mechanism to measure the abilities of LLMs and derive their rankings. The core idea of this mechanism is to first collect the answers from each LLM, then treat each LLM as a 'reviewer' to rate the quality of the other LLMs' answers, and finally optimize the internal agreement of the reviews among all LLMs. They also conduct experiments on three datasets to validate the effectiveness of their proposed mechanism.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Unsupervised Evaluation Method: The paper introduces PiCO (Peer Review in LLMs based on Consistency Optimization), a new unsupervised evaluation method that leverages peer-review mechanisms to measure the capabilities of LLMs automatically, particularly without human-annotated data. The unsupervised nature also makes it scalable and less subjectively biased.
2. Consistency Optimization Framework: The proposed approach includes a constrained optimization method based on the consistency assumption, which helps in re-ranking LLMs to align more closely with human preferences.
3. New Evaluation Metrics: The paper proposes three new metrics—Permutation Entropy (PEN), Count Inversions (CIN), and Longest Increasing Subsequence (LIS)—to evaluate the alignment of LLM rankings with human preferences. These metrics can further inspire future work.

Weaknesses:
1. Reliance on Consistency Assumption: The effectiveness of the method relies on the consistency assumption that higher-level LLMs can more accurately evaluate others. However, a natural concern is, ""Does this assumption always hold true in practice?"" I suggest the authors further discuss the applicability of their method.
2. Complexity of Implementation: The framework involves a complex review process, which requires substantial computational resources to support the LLMs' inference. Can you provide some details on the number of tokens consumed and a comparison of consumption with baseline methods?
3. Consideration of Multi-Agent Methods: Since the proposed method employs multiple LLMs, I think more recent and advanced evaluation methods based on multi-agent systems should also be considered in the experiments, such as AgentVerse.
4. Details of ELO: The ELO system is essential for the proposed mechanism. However, it is only mentioned in the appendix. I suggest the authors add more details about the background of ELO and how it is adopted in the proposed mechanism.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The highlight of the work is the proposed method of evaluating Large Language Models without relying on human feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed evaluation method is a novel attempt of automating the LLM improvement process. Such method worth further exploration. It could be adapted to many of the LLMs and potentially bring us more insights.
- By eliminating the involvement of human, the proposed evaluation method limits the bias brought by human labelers. The observations presented are also interesting as LLMs can sometimes surprise us.
- Great presentation and visualization.

Weaknesses:
Some of the equations and notations in the paper seems unnecessarily complicated, which can be reorganized when polishing.

Limitations:
The idea is straightforward and make sense to me. But LLMs can be trained to bypass such systems, which may lead to potential fairness or security problems.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel unsupervised evaluation method for large language models (LLMs): it uses a peer-review mechanism of a models' anonymized answers by other models. The approach assigns a (learnable) capability parameter to each LLM and solves a constrained optimization problem to maximize the consistency between capabilities and scores. The end result is a ranking of the evaluated models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduce a novel approach to an important practical problem: ranking the quality of the ever-growing number of open- and closed- source LLM available to the public. The paper is reasonably easy to follow, and the empirical results appear to be sound.

Weaknesses:
The paper could be further improved on several directions:
1) you should dedicate a full section to the iterative elimination of models; what is the benefit of eliminating the weaker ones rather than keeping them around? how di you come up with the threshold of 60% to remove? can you learn this threshold automatically? is this threshold optimal for these 15 models? what happens if you start with, say, 100 models? what happens to your results (and the curves in Fig 5) if you stop earlier (all three metrics, not just CIN)? What if you continue to eliminate all models until you are left with one? is there any relationship between the order in which the models are eliminated and their final rank?
2) are there any scaling issues for 100, 1K, 10K, or 100K models? how about cost: is it cheaper to fine-tune a ""baseline"" model than to pick the best one out of 10K candidates?   
3) while the three metrics you use are meaningful, you should also present results for Precision@1 and -say- RBP@3; after all, we care a lot about identifying the the top models
3) Fig 5 should be extended to nine graphs (3 metrics * 3 datasets); for each of the 9 graphs, you should also show illustrative three ranked lists: PiCO's, PRE's, and the target one. As always, the devil is in the details: not all ""CIN = 1"" are created equal. Performance-wise, it is almost irrelevant if you have the bottom-2 models inverted; no necessarily so if the inversion is between the top-2 models

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies how to estimate LLMs' performance ranking without human preference annotations. In particular, it proposes to leverage three metrics (PEN, CIN, LIS) to evaluate the estimation quality, gives an estimation mechanism that first asks a list of LLMs (called ""reviewers"") to rank pairwise answers to user questions independently, and then aggregates their ranking via a weighted sum approach. A consistency optimization determines the weights of each reviewer.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of LLM evaluation without human annotations is critical in resource-limited applications. The most important and interesting contribution of this paper, in my opinion, is proposing the problem of estimating the performance rank of LLMs instead of any metric of an individual LLM. The paper also reveals an interesting assumption that better reviewers are expected to be better answer generators, which leads to their consistency optimization approach. Overall, the paper is well-written and easy to follow.

Weaknesses:
While I find the proposed problem interesting, there are still a few limitations, unfortunately.

***Unclear implication of ground truth ranking***: The technical part of the paper starts by introducing a ground truth ranking (equation (1)) without giving its physical meaning. It simply assumes ""[...] alignment with human preferences"", but it is not clear what human preferences mean in this context. 

***Evaluation metric is strange***: One of my major concerns is on the choices of evaluation metric. All the three proposed metrics, PIN, CIN, LIS, in the authors' own words, seem originally used for time series comparison. However, the goal here is to compare rankings, not time series. Thus, it is unclear why we should not use the standard ranking comparison metrics, e.g., Spearman's rank correlation coefficient or Kendall rank correlation coefficient.

***Consistency optimization algorithm is not provided***: The core of the proposed ranking estimation method is the optimization problem (7). It does not seem to be a standard optimization problem, but I could not find (even a discussion on) any clue on how to solve it in this paper.

***An optimal solution to the consistency optimization formulation can be useless***: I find the following optimal solution to the problem (7): just set weight w to be 0 for all LLMs. It is an optimal solution as G and w are identical and thus the objective is always maximized.  However, this solution is undesired. I probably misunderstood something, but this seems to suggest the formulation is incorrect. 

***Consistency optimization formulation seems brittle to query distribution biases***: Another problem with the formulation is that it seems brittle to data distribution bias. E.g., suppose M1 is indeed better than M2 for some query q. And let us replicate many copies of q in the dataset D. Then the grade G1 can be arbitrarily large. In other words, the grade of an LLM is proportional to the number of battles involving it in the dataset D, which should not be the case.

***Choices of LLMs for evaluation***: In line 547, the authors write ""For our analysis, we meticulously selected 15 LLMs"". What is the principle of the meticulous selection? Other than open-source and close-source, the selection is quite arbitrary. For example, I am quite surprised to not see GPT-4 and Claude included in the reviewer LLMs.

***Comparison with a simple baseline***: One simple baseline is to ask a powerful LLM(e.g., GPT-4, Cluade-3) to give a preference for each answer question pair, and then take the vote to determine the ranking. I would suggest to compare the proposed method with this simple baseline.

Limitations:
No. The limitations are not well discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a more reliable evaluation system to rank the abilities of different large language models (LLMs). Previous evaluation methods typically suffer from two main drawbacks: (1) benchmark data leakage and (2) cost-intensive and potentially biased human evaluations. To address these issues, the authors introduce an unsupervised evaluation mechanism to measure the abilities of LLMs and derive their rankings. The core idea of this mechanism is to first collect the answers from each LLM, then treat each LLM as a 'reviewer' to rate the quality of the other LLMs' answers, and finally optimize the internal agreement of the reviews among all LLMs. They also conduct experiments on three datasets to validate the effectiveness of their proposed mechanism.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Unsupervised Evaluation Method: The paper introduces PiCO (Peer Review in LLMs based on Consistency Optimization), a new unsupervised evaluation method that leverages peer-review mechanisms to measure the capabilities of LLMs automatically, particularly without human-annotated data. The unsupervised nature also makes it scalable and less subjectively biased.
2. Consistency Optimization Framework: The proposed approach includes a constrained optimization method based on the consistency assumption, which helps in re-ranking LLMs to align more closely with human preferences.
3. New Evaluation Metrics: The paper proposes three new metrics—Permutation Entropy (PEN), Count Inversions (CIN), and Longest Increasing Subsequence (LIS)—to evaluate the alignment of LLM rankings with human preferences. These metrics can further inspire future work.

Weaknesses:
1. Reliance on Consistency Assumption: The effectiveness of the method relies on the consistency assumption that higher-level LLMs can more accurately evaluate others. However, a natural concern is, ""Does this assumption always hold true in practice?"" I suggest the authors further discuss the applicability of their method.
2. Complexity of Implementation: The framework involves a complex review process, which requires substantial computational resources to support the LLMs' inference. Can you provide some details on the number of tokens consumed and a comparison of consumption with baseline methods?
3. Consideration of Multi-Agent Methods: Since the proposed method employs multiple LLMs, I think more recent and advanced evaluation methods based on multi-agent systems should also be considered in the experiments, such as AgentVerse.
4. Details of ELO: The ELO system is essential for the proposed mechanism. However, it is only mentioned in the appendix. I suggest the authors add more details about the background of ELO and how it is adopted in the proposed mechanism.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The highlight of the work is the proposed method of evaluating Large Language Models without relying on human feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed evaluation method is a novel attempt of automating the LLM improvement process. Such method worth further exploration. It could be adapted to many of the LLMs and potentially bring us more insights.
- By eliminating the involvement of human, the proposed evaluation method limits the bias brought by human labelers. The observations presented are also interesting as LLMs can sometimes surprise us.
- Great presentation and visualization.

Weaknesses:
Some of the equations and notations in the paper seems unnecessarily complicated, which can be reorganized when polishing.

Limitations:
The idea is straightforward and make sense to me. But LLMs can be trained to bypass such systems, which may lead to potential fairness or security problems.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
up0qqbdeQu;"REVIEW 
Summary:
The paper supplies a post hoc method to tune the ResNet based CLIP method on multi-label recognition task. Firstly, the method includes class concept representation, which is an alternative of the default prompt “The photo of a {class}”. It is the average of class description sentence embedding from a text description source (MSCOCO and git3.5 generated caption in the paper). Secondly, the paper proposed a sequential attention to iteratively transfer the the visual features to align with the class concept representation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Training-free enhancement: The proposed method significantly improves zero-shot and prompt-tuning performance without requiring additional training or labeled samples, making it computationally efficient.
2. Robust performance: Experimental results on multiple benchmark datasets (MS-COCO, VOC2007, and NUS-WIDE) show substantial performance gains, demonstrating the method's effectiveness.

Weaknesses:
1. Lack of clear differentiation: While the authors mention TaI-DPT and claim differences, the paper does not clearly articulate the advantages of the proposed method over TaI-DPT, leaving the comparative benefits ambiguous.
2. Unfair comparison in Table 5: The Class Concept Representation is based on the MS-COCO dataset, making direct comparisons with the baseline CLIP method potentially unfair due to inherent advantages provided by the dataset-specific information.

3. Limited model implementation: The paper only implements the ResNet-based CLIP model and does not explore transformer-based CLIP models. It is unclear whether the method is ineffective for transformer-based models or if there are specific reasons behind this omission. This limits the generalizability of the findings.
4. Ambiguous terminology: The paper uses the term ""training-free"" in its title, yet it describes the approach as ""test-time adaptation"" within the content. This inconsistency can lead to confusion about the nature of the proposed method.
5. Dependent on source text description: It seems that text description source need to be carefully selected.  A comparison of different description dataset can be interesting.

Limitations:
see disadvantage and question

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a class concept representation for zero-shot multi-label recognition in a label-free manner and introduces a context-guided visual feature that enhances the alignment of the visual feature of VLM with the class concept.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	This paper presents a novel class concept representation for training-free multi-label recognition tasks using VLMs from massive text descriptions inspired by how human forms concept on words.
2.	This paper proposes a context-guided visual feature, which is transformed onto the same text feature space as class concepts using sequential attention, to better align multi-modal features.
3.	The method presented in this paper synergistically enhances the performance of ZSCLIP and other state-of-the-art just-in-time tuning methods, with a minimal increase in inference time.

Weaknesses:
1. Tip-adapter is the proposed training free method in 2021, it would be better to choose the newer training free method in few shot setting.
2. It would be more appealing to emphasize label-free in the abstract.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method to adapt without training a large vision-language model for the task of multi-label recognition. They introduce a class concept representation, based on averaging the representation of image descriptions relevant to each class, to replace simple hand-crafted text prompts (e.g., “a photo of {class name}”). Furthermore, they propose to use a context-guided visual process to align visual features with the class concept representation. Experiments conducted on several benchmarks and in zero-shot and partial labeling settings show state-of-the-art performance compared to relevant baselines. Combination with some baseline methods further shows the improvements that can be obtained with the proposed method. Ablation studies show the contribution of each component of the method and the sensitivity to some of the method's parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method achieves state-of-the-art performance
- The proposed method does not require training and can be seen as a form of test-time adaptation
- The method can be combined with existing prompt-tuning methods

Weaknesses:
- Parts of the method descriptions, especially the Context-Guided Visual Feature, are unclear
- The method relies on thousands of text descriptions relevant to the target classes, which could hinder the scalability of the methods with a large number of classes

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes class concept representation for zero-shot multi-label recognition. The paper also proposes context-guided visual representation, which is in the same linear space as class concept representation, with sequential attention. Experiments show the proposed methods improved the performance of zero-shot methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper uses class concept representation for training-free multi-label recognition tasks
2. The paper proposes context-guided visual feature using sequential attention.
3. Experiments show the proposed methods improved the performance of zero-shot methods.

Weaknesses:
1. The class concepts from averaging the vectors of text descriptions need to be verified. E.g. What text/image embeddings are the closest to the class concepts? What clusters do the concepts belong to? Since taking the average for class concepts ""was guided by the prior work on prompt ensembling [4]"" L280, it is not a novel representation for class concepts. 
2. Eq 2,3 needs further explanation. What is ""t"" in the equation? If ""t"" is transpose, what dimensions are swapped for a tensor T? Take k=1 as an example, how do the dimensions change in each step of the equation? In experiments, there should be ablation studies on G and the value of each Mg. Also, is T randomly reshaped? It would be better to have ablation studies on random reshaping or reshaping by clusters.
3. What is the implementation detail for partial label learning with the proposed method?

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper supplies a post hoc method to tune the ResNet based CLIP method on multi-label recognition task. Firstly, the method includes class concept representation, which is an alternative of the default prompt “The photo of a {class}”. It is the average of class description sentence embedding from a text description source (MSCOCO and git3.5 generated caption in the paper). Secondly, the paper proposed a sequential attention to iteratively transfer the the visual features to align with the class concept representation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Training-free enhancement: The proposed method significantly improves zero-shot and prompt-tuning performance without requiring additional training or labeled samples, making it computationally efficient.
2. Robust performance: Experimental results on multiple benchmark datasets (MS-COCO, VOC2007, and NUS-WIDE) show substantial performance gains, demonstrating the method's effectiveness.

Weaknesses:
1. Lack of clear differentiation: While the authors mention TaI-DPT and claim differences, the paper does not clearly articulate the advantages of the proposed method over TaI-DPT, leaving the comparative benefits ambiguous.
2. Unfair comparison in Table 5: The Class Concept Representation is based on the MS-COCO dataset, making direct comparisons with the baseline CLIP method potentially unfair due to inherent advantages provided by the dataset-specific information.

3. Limited model implementation: The paper only implements the ResNet-based CLIP model and does not explore transformer-based CLIP models. It is unclear whether the method is ineffective for transformer-based models or if there are specific reasons behind this omission. This limits the generalizability of the findings.
4. Ambiguous terminology: The paper uses the term ""training-free"" in its title, yet it describes the approach as ""test-time adaptation"" within the content. This inconsistency can lead to confusion about the nature of the proposed method.
5. Dependent on source text description: It seems that text description source need to be carefully selected.  A comparison of different description dataset can be interesting.

Limitations:
see disadvantage and question

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a class concept representation for zero-shot multi-label recognition in a label-free manner and introduces a context-guided visual feature that enhances the alignment of the visual feature of VLM with the class concept.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	This paper presents a novel class concept representation for training-free multi-label recognition tasks using VLMs from massive text descriptions inspired by how human forms concept on words.
2.	This paper proposes a context-guided visual feature, which is transformed onto the same text feature space as class concepts using sequential attention, to better align multi-modal features.
3.	The method presented in this paper synergistically enhances the performance of ZSCLIP and other state-of-the-art just-in-time tuning methods, with a minimal increase in inference time.

Weaknesses:
1. Tip-adapter is the proposed training free method in 2021, it would be better to choose the newer training free method in few shot setting.
2. It would be more appealing to emphasize label-free in the abstract.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method to adapt without training a large vision-language model for the task of multi-label recognition. They introduce a class concept representation, based on averaging the representation of image descriptions relevant to each class, to replace simple hand-crafted text prompts (e.g., “a photo of {class name}”). Furthermore, they propose to use a context-guided visual process to align visual features with the class concept representation. Experiments conducted on several benchmarks and in zero-shot and partial labeling settings show state-of-the-art performance compared to relevant baselines. Combination with some baseline methods further shows the improvements that can be obtained with the proposed method. Ablation studies show the contribution of each component of the method and the sensitivity to some of the method's parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method achieves state-of-the-art performance
- The proposed method does not require training and can be seen as a form of test-time adaptation
- The method can be combined with existing prompt-tuning methods

Weaknesses:
- Parts of the method descriptions, especially the Context-Guided Visual Feature, are unclear
- The method relies on thousands of text descriptions relevant to the target classes, which could hinder the scalability of the methods with a large number of classes

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes class concept representation for zero-shot multi-label recognition. The paper also proposes context-guided visual representation, which is in the same linear space as class concept representation, with sequential attention. Experiments show the proposed methods improved the performance of zero-shot methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper uses class concept representation for training-free multi-label recognition tasks
2. The paper proposes context-guided visual feature using sequential attention.
3. Experiments show the proposed methods improved the performance of zero-shot methods.

Weaknesses:
1. The class concepts from averaging the vectors of text descriptions need to be verified. E.g. What text/image embeddings are the closest to the class concepts? What clusters do the concepts belong to? Since taking the average for class concepts ""was guided by the prior work on prompt ensembling [4]"" L280, it is not a novel representation for class concepts. 
2. Eq 2,3 needs further explanation. What is ""t"" in the equation? If ""t"" is transpose, what dimensions are swapped for a tensor T? Take k=1 as an example, how do the dimensions change in each step of the equation? In experiments, there should be ablation studies on G and the value of each Mg. Also, is T randomly reshaped? It would be better to have ablation studies on random reshaping or reshaping by clusters.
3. What is the implementation detail for partial label learning with the proposed method?

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
s5Y3M5l1qg;"REVIEW 
Summary:
To better defend against adversarial attacks, the paper proposes a novel adversarial defense mechanism for image classification – CARSO – blending the paradigms of adversarial training and adversarial purification in a synergistic robustness-enhancing way.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper proposes a novel defense mechanism.

The proposed method is validated on multiple datasets.

Weaknesses:
1. The presentation of the paper is poor.

    a) In the first half of the paper, the author merely describes some background. There is a lack of analysis of existing methods, such as the shortcomings of the current methods, what problems the proposed method can solve, and why it can solve these problems.

    b) Some descriptions are unclear, such as 'Upon completion of the training process, the encoder network may be discarded as it will not be used for inference.' I think 'may' should be removed here.

2. The current experiments are insufficient to prove the effectiveness of the proposed method.

    a) Table 2 simplifies a lot of information, which reduces clarity; for example, it only records the mean or best results of multiple methods and lacks the clean accuracy of the purification method. I suggest listing all methods according to both clean accuracy and adversarial accuracy. The existing content in Table 2 can be added as additional row information.

    b) Since the paper does not give specific problems, only a general goal, which is to better defend against adversarial attacks, the experiments become relatively limited. I believe the author should re-summarize the shortcomings of existing methods and the advantages of the proposed method and conduct more experimental comparisons.

Limitations:
The authors have discussed limitations of the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes a novel adversarial defense method called CARSO. CARSO consists of two models: a classifier and a purifier. The classifier is (pre)trained to correctly classify possibly perturbed data. The encoder of the purifier is trained to generate a latent space from the internal representation of the classifier and the original (possibly perturbed) input. The decoder of the purifier is trained to reconstruct a sample from the latent representation and the internal representation of the classifier. The final prediction is determined by aggregating the outputs of the classifier for reconstructed data.

Detailed procedures are summarized as follows:

- The classifier is always kept frozen. Other parts, including the VAE and small CNNs for compression, are trained on a VAE loss consisting of a reconstruction loss based on a pixel-wise channel-wise binary cross-entropy loss and KL-div.
- The internal representation and input are compressed by small CNNs before being inputted into the encoder of the purifier.
- The classifier is pretrained according to [18] or [62].
- When training the purifier, each batch contains both clean and adversarial samples.
- The aggregation is represented by a double exponential function.
- Evaluations are conducted under $L_\infty$ attacks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The concept of blending adversarial training and purification is novel and interesting. The proposed method, CARSO, achieves robust accuracy that surpasses the SOTA adversarially trained models and purification methods, including diffusion-based models, despite its relatively simple mechanism.
- The evaluation was carefully conducted. The authors explicitly address common pitfalls in evaluating robustness. For example, they conducted end-to-end validation (full whitebox setting), addressed concerns about gradient obfuscation, and used PGD+EOT to address the stochasticity of CARSO.
- CARSO can utilize existing pretrained models, which have already achieved high robust accuracy.
- A wide variety of datasets (CIFAR-10, CIFAR-100, and TinyImageNet-200) were used for evaluation.

Weaknesses:
**1**. In my opinion, the claim that CARSO surpasses the used adversarially trained model seems questionable. If my understanding is correct, during inference, the decoder takes class information only from the internal representation of the classifier. Thus, I believe the decoder can correctly reconstruct the sample only if the classifier, outputting the internal representation, can correctly extract class information from the original perturbed sample. Could the authors clarify this?

Note: Initially, I doubted whether some experimental or evaluation settings were appropriate. However, as far as I can tell, there are no issues. Just in case, I recommend the authors review their source code again.

**2**. CARSO sacrifices clean accuracy more significantly than existing SOTA methods. Additionally, to compare CARSO and the best AT/purification models in terms of clean accuracy, Table 2 should include the clean accuracy of the best AT/purification models (i.e., the contents in Table 15). The scenario or dataset columns in Table 2 might not be necessary.

**3**. Few ablation studies. The authors should include the case of $L_2$ perturbations and use internal representations from different layers. Particularly, the relationship between the layers used for extracting representation and robust accuracy is of interest.

Limitations:
The authors explicitly addressed the limitations in Section 5.3.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper integrates adversarial training and adversarial purification to enhance robustness. It specifically maps the internal representation of potentially perturbed inputs onto a distribution of tentative reconstructions. These reconstructions are then aggregated by the adversarially-trained classifier to improve overall performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea of combining adversarial training and adversarial purification is interesting.

Weaknesses:
1, The experiments are too weak. I hope the authors can refer to at least [1][2][3], which are relevant to adversarial purification, to conduct experiments from more dimensions and consider more baselines and fundamental experiments.

2, Could we just combine [1] with an adversarially-trained model to achieve similar performance?

3, Why should the classifier be adversarially trained for better accuracy?

4, Why can't we directly purify the image? Could we use an image-to-image method to purify the input image?





[1] DISCO: Adversarial Defense with Local Implicit Functions.
[2] Diffusion Models for Adversarial Purification
[3] IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks

Limitations:
The method heavily relies on training a VAE as the generative purification model.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
To better defend against adversarial attacks, the paper proposes a novel adversarial defense mechanism for image classification – CARSO – blending the paradigms of adversarial training and adversarial purification in a synergistic robustness-enhancing way.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper proposes a novel defense mechanism.

The proposed method is validated on multiple datasets.

Weaknesses:
1. The presentation of the paper is poor.

    a) In the first half of the paper, the author merely describes some background. There is a lack of analysis of existing methods, such as the shortcomings of the current methods, what problems the proposed method can solve, and why it can solve these problems.

    b) Some descriptions are unclear, such as 'Upon completion of the training process, the encoder network may be discarded as it will not be used for inference.' I think 'may' should be removed here.

2. The current experiments are insufficient to prove the effectiveness of the proposed method.

    a) Table 2 simplifies a lot of information, which reduces clarity; for example, it only records the mean or best results of multiple methods and lacks the clean accuracy of the purification method. I suggest listing all methods according to both clean accuracy and adversarial accuracy. The existing content in Table 2 can be added as additional row information.

    b) Since the paper does not give specific problems, only a general goal, which is to better defend against adversarial attacks, the experiments become relatively limited. I believe the author should re-summarize the shortcomings of existing methods and the advantages of the proposed method and conduct more experimental comparisons.

Limitations:
The authors have discussed limitations of the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes a novel adversarial defense method called CARSO. CARSO consists of two models: a classifier and a purifier. The classifier is (pre)trained to correctly classify possibly perturbed data. The encoder of the purifier is trained to generate a latent space from the internal representation of the classifier and the original (possibly perturbed) input. The decoder of the purifier is trained to reconstruct a sample from the latent representation and the internal representation of the classifier. The final prediction is determined by aggregating the outputs of the classifier for reconstructed data.

Detailed procedures are summarized as follows:

- The classifier is always kept frozen. Other parts, including the VAE and small CNNs for compression, are trained on a VAE loss consisting of a reconstruction loss based on a pixel-wise channel-wise binary cross-entropy loss and KL-div.
- The internal representation and input are compressed by small CNNs before being inputted into the encoder of the purifier.
- The classifier is pretrained according to [18] or [62].
- When training the purifier, each batch contains both clean and adversarial samples.
- The aggregation is represented by a double exponential function.
- Evaluations are conducted under $L_\infty$ attacks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The concept of blending adversarial training and purification is novel and interesting. The proposed method, CARSO, achieves robust accuracy that surpasses the SOTA adversarially trained models and purification methods, including diffusion-based models, despite its relatively simple mechanism.
- The evaluation was carefully conducted. The authors explicitly address common pitfalls in evaluating robustness. For example, they conducted end-to-end validation (full whitebox setting), addressed concerns about gradient obfuscation, and used PGD+EOT to address the stochasticity of CARSO.
- CARSO can utilize existing pretrained models, which have already achieved high robust accuracy.
- A wide variety of datasets (CIFAR-10, CIFAR-100, and TinyImageNet-200) were used for evaluation.

Weaknesses:
**1**. In my opinion, the claim that CARSO surpasses the used adversarially trained model seems questionable. If my understanding is correct, during inference, the decoder takes class information only from the internal representation of the classifier. Thus, I believe the decoder can correctly reconstruct the sample only if the classifier, outputting the internal representation, can correctly extract class information from the original perturbed sample. Could the authors clarify this?

Note: Initially, I doubted whether some experimental or evaluation settings were appropriate. However, as far as I can tell, there are no issues. Just in case, I recommend the authors review their source code again.

**2**. CARSO sacrifices clean accuracy more significantly than existing SOTA methods. Additionally, to compare CARSO and the best AT/purification models in terms of clean accuracy, Table 2 should include the clean accuracy of the best AT/purification models (i.e., the contents in Table 15). The scenario or dataset columns in Table 2 might not be necessary.

**3**. Few ablation studies. The authors should include the case of $L_2$ perturbations and use internal representations from different layers. Particularly, the relationship between the layers used for extracting representation and robust accuracy is of interest.

Limitations:
The authors explicitly addressed the limitations in Section 5.3.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper integrates adversarial training and adversarial purification to enhance robustness. It specifically maps the internal representation of potentially perturbed inputs onto a distribution of tentative reconstructions. These reconstructions are then aggregated by the adversarially-trained classifier to improve overall performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea of combining adversarial training and adversarial purification is interesting.

Weaknesses:
1, The experiments are too weak. I hope the authors can refer to at least [1][2][3], which are relevant to adversarial purification, to conduct experiments from more dimensions and consider more baselines and fundamental experiments.

2, Could we just combine [1] with an adversarially-trained model to achieve similar performance?

3, Why should the classifier be adversarially trained for better accuracy?

4, Why can't we directly purify the image? Could we use an image-to-image method to purify the input image?





[1] DISCO: Adversarial Defense with Local Implicit Functions.
[2] Diffusion Models for Adversarial Purification
[3] IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks

Limitations:
The method heavily relies on training a VAE as the generative purification model.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
qL4nN6Ew7U;"REVIEW 
Summary:
The paper proposes Fantasy, a T2I model based fully on transformers (except for the VQGAN for the latents encoding and decoder):
* A __fine-tuned LLM__ (based on Phi-2) for the text encoding
* A image generator based on the MIM (Masked Image Modelling) approach

The training happens in two stages, a generic stage for aligning the generator the the frozen Phi-2 features, followed by a fine-tuning stage where the Phi-2 encoder is fine-tuned alongside the MIM transformer.

The results on human evaluations are convincing, putting Fantasy alongside models that require larger computational resources, while the FID results are less convincing (due to the image being smooth according to the authors).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty:
* The LLM is __fine-tuned__ but only in the second stage of training, this approach is new and makes sense

Accessiblity:
* The 2 stage pre-training is already standard practice
* The Phi-2 model is available, it is likely that this approach works for other available models (Phi-3? It could be interesting to test)
* The model size allows the model to be trained in a reasonable time

Weaknesses:
Performance:
* The FID scores are not competitive and the authors describe why: the image are smooth => it seems that the human evaluations still rank Fantasy at the top on visual appeal, but it might be that if the question was ""visual realism"" they might prefer a different model
* Results are available for 256px, and a 600M parameters MIM generator, there is no proof that this method scales (we know that diffusion models based on UNet have trouble scaling for instance)

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an efficient text-to-image generation model that integrates LLM and MIM. It demonstrates that MIM can achieve comparable performance. Unlike commonly used text encoders like CLIP and T5, this study introduces an efficient decoder-only LLM, phi-3, achieving better semantic understanding. The effectiveness of the method is validated through a newly proposed two-stage training approach and sufficient experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written with clear logic.
2. The use of MIM and LLM for image generation introduces a novel approach.
3. The two-stage training method improves the generation results.

Weaknesses:
1. The quality of the generated images does not yet match that of existing methods (e.g., pixart-alpha, SDXL), with some loss of detail. This is noticeable from the comparison in column B of Figure 5.
2. Some aspects of the methodology could be clearer, and the overall coherence of the approach could be strengthened.
3. While the proposed method demonstrates efficiency advantages, particularly in faster training convergence, this can be influenced by various factors. However, the related experiments in the paper could be more comprehensive.
4. The semantic accuracy of the generated images, a potential strength of Fantasy, is not fully demonstrated in the paper. For instance, the model's ability to handle prompts with multiple entities, color attribute descriptions, or retaining key elements in long text inputs is not adequately showcased.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a technique for training transformer based masked image modeling in an efficient way. Two main contributions include (1) use of a LLM decoder as text embeddings, and (2) Two-stage training strategy for MIM models. Experimental results show good generation quality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of LLMs as text encoders seem interesting.
- Two-stage training approach makes sense. First, the use of pretraining data helps the model learn a general text-image model, and the high quality alignment data can improve the quality of generations.
- Training models on low resources seem appealing.

Weaknesses:
- I don't see anything new proposed in this paper. The authors simply use Phi-2 model as text encoder with MIM models, and use two-stage training. 
- Even two-stage training is not something new to image synthesis. People have been doing aesthetic finetuning to improve image quality in diffusion models (eg. stable diffusion). The authors extend this to instruction-image data.
- The quality of generated images are not very impressive. When zoomed in, we notice a lot of visible artifacts. The generated images are also flat and doesn't have a lot of details.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To develop a resource-efficient, high-quality image generator for long instructions, the authors presented Fantasy, an efficient T2I generation model that integrates a lightweight decoder-only LLM and a transformer-based masked image modeling (MIM). 

They demonstrate that with appropriate training strategies and high-quality data, MIM can also achieve comparable performance.

By incorporating pre-trained decoder-only LLMs as the text encoder, they observe a significant improvement in text fidelity compared to the widely used CLIP text encoder, enhancing the text image alignment. 

Their training includes two stages: 1) large-scale concept alignment pre-training, and 2) fine-tuning with high-quality instruction-image data. 

They conduct evaluation on FID, HPSv2 benchmarks, and human feedback, which demonstrate the competitive performance of Fantasy against other diffusion and autoregressive models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The author proposed a T2I framework that combines several more recent components and performed a series of comparisons, including both quantitative and human evaluations.

Weaknesses:
- the major concern of the work is unclear contributions. The claimed three contributions or core designs are quite similar with existing works.
- Efficient T2I network: there is no justification about why the network is “efficient”. Simply adopting a smaller LLM like Phi-2 can hardly be claimed as efficient network design. 
- The hierarchical training strategy was also proposed before, it is not clear what is the difference with existing work.
- High quality data: the training data utilize Laion-2B and use existing filtering strategy. The collection high quality synthesized images from existing datasets.
- The evaluation metrics are mainly based on HPSv2, which has a limited range of values, e.g., HPSv2 has close values for SDv1.4 and SD2.0, e.g., 27.26 vs 27.48. Why SDXL is missing in Table 1?
- The author acknowledged that their model lags behind diffusion-based models in visual appeal, limited by the 8K size of VQGAN’s codebook and not targeting visual appeal. However, there is no solution or further study for solving this problem, which limits the scalability of the model.
- The scaling study in section 4.2 seems pretty premature and it is unclear what is the limit of the scaling. Increasing the model depth can improve the performance, which has been verified from previous work such as in https://arxiv.org/abs/2212.09748 or https://arxiv.org/abs/2404.02883.

Limitations:
I would encourage the authors to emphasize about the core contributions rather than combining everything together, which can hardly show significant performance improvement over existing public models.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes Fantasy, a T2I model based fully on transformers (except for the VQGAN for the latents encoding and decoder):
* A __fine-tuned LLM__ (based on Phi-2) for the text encoding
* A image generator based on the MIM (Masked Image Modelling) approach

The training happens in two stages, a generic stage for aligning the generator the the frozen Phi-2 features, followed by a fine-tuning stage where the Phi-2 encoder is fine-tuned alongside the MIM transformer.

The results on human evaluations are convincing, putting Fantasy alongside models that require larger computational resources, while the FID results are less convincing (due to the image being smooth according to the authors).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty:
* The LLM is __fine-tuned__ but only in the second stage of training, this approach is new and makes sense

Accessiblity:
* The 2 stage pre-training is already standard practice
* The Phi-2 model is available, it is likely that this approach works for other available models (Phi-3? It could be interesting to test)
* The model size allows the model to be trained in a reasonable time

Weaknesses:
Performance:
* The FID scores are not competitive and the authors describe why: the image are smooth => it seems that the human evaluations still rank Fantasy at the top on visual appeal, but it might be that if the question was ""visual realism"" they might prefer a different model
* Results are available for 256px, and a 600M parameters MIM generator, there is no proof that this method scales (we know that diffusion models based on UNet have trouble scaling for instance)

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an efficient text-to-image generation model that integrates LLM and MIM. It demonstrates that MIM can achieve comparable performance. Unlike commonly used text encoders like CLIP and T5, this study introduces an efficient decoder-only LLM, phi-3, achieving better semantic understanding. The effectiveness of the method is validated through a newly proposed two-stage training approach and sufficient experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written with clear logic.
2. The use of MIM and LLM for image generation introduces a novel approach.
3. The two-stage training method improves the generation results.

Weaknesses:
1. The quality of the generated images does not yet match that of existing methods (e.g., pixart-alpha, SDXL), with some loss of detail. This is noticeable from the comparison in column B of Figure 5.
2. Some aspects of the methodology could be clearer, and the overall coherence of the approach could be strengthened.
3. While the proposed method demonstrates efficiency advantages, particularly in faster training convergence, this can be influenced by various factors. However, the related experiments in the paper could be more comprehensive.
4. The semantic accuracy of the generated images, a potential strength of Fantasy, is not fully demonstrated in the paper. For instance, the model's ability to handle prompts with multiple entities, color attribute descriptions, or retaining key elements in long text inputs is not adequately showcased.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a technique for training transformer based masked image modeling in an efficient way. Two main contributions include (1) use of a LLM decoder as text embeddings, and (2) Two-stage training strategy for MIM models. Experimental results show good generation quality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of LLMs as text encoders seem interesting.
- Two-stage training approach makes sense. First, the use of pretraining data helps the model learn a general text-image model, and the high quality alignment data can improve the quality of generations.
- Training models on low resources seem appealing.

Weaknesses:
- I don't see anything new proposed in this paper. The authors simply use Phi-2 model as text encoder with MIM models, and use two-stage training. 
- Even two-stage training is not something new to image synthesis. People have been doing aesthetic finetuning to improve image quality in diffusion models (eg. stable diffusion). The authors extend this to instruction-image data.
- The quality of generated images are not very impressive. When zoomed in, we notice a lot of visible artifacts. The generated images are also flat and doesn't have a lot of details.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To develop a resource-efficient, high-quality image generator for long instructions, the authors presented Fantasy, an efficient T2I generation model that integrates a lightweight decoder-only LLM and a transformer-based masked image modeling (MIM). 

They demonstrate that with appropriate training strategies and high-quality data, MIM can also achieve comparable performance.

By incorporating pre-trained decoder-only LLMs as the text encoder, they observe a significant improvement in text fidelity compared to the widely used CLIP text encoder, enhancing the text image alignment. 

Their training includes two stages: 1) large-scale concept alignment pre-training, and 2) fine-tuning with high-quality instruction-image data. 

They conduct evaluation on FID, HPSv2 benchmarks, and human feedback, which demonstrate the competitive performance of Fantasy against other diffusion and autoregressive models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The author proposed a T2I framework that combines several more recent components and performed a series of comparisons, including both quantitative and human evaluations.

Weaknesses:
- the major concern of the work is unclear contributions. The claimed three contributions or core designs are quite similar with existing works.
- Efficient T2I network: there is no justification about why the network is “efficient”. Simply adopting a smaller LLM like Phi-2 can hardly be claimed as efficient network design. 
- The hierarchical training strategy was also proposed before, it is not clear what is the difference with existing work.
- High quality data: the training data utilize Laion-2B and use existing filtering strategy. The collection high quality synthesized images from existing datasets.
- The evaluation metrics are mainly based on HPSv2, which has a limited range of values, e.g., HPSv2 has close values for SDv1.4 and SD2.0, e.g., 27.26 vs 27.48. Why SDXL is missing in Table 1?
- The author acknowledged that their model lags behind diffusion-based models in visual appeal, limited by the 8K size of VQGAN’s codebook and not targeting visual appeal. However, there is no solution or further study for solving this problem, which limits the scalability of the model.
- The scaling study in section 4.2 seems pretty premature and it is unclear what is the limit of the scaling. Increasing the model depth can improve the performance, which has been verified from previous work such as in https://arxiv.org/abs/2212.09748 or https://arxiv.org/abs/2404.02883.

Limitations:
I would encourage the authors to emphasize about the core contributions rather than combining everything together, which can hardly show significant performance improvement over existing public models.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
p32gjG4yqw;"REVIEW 
Summary:
This work generalizes the ridgelet transform to equivariant neural networks, providing constructive proofs of universality in the general case as integrations over parameter distributions. Although such a direction had been taken up in prior work [33], they generalize it from scalar activations to vector activations, therefore encompassing more practical equivariant networks. The authors consider the form of the ridgelet transform for deep networks, and groups including the affine group and orthogonal group.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The authors provide a constructive universal approximation result, which is in contrast to many non-constructive universality results. They strictly improve on the past work of Sonoda et al [33] by extending from scalars to vectors, which is more realistic. They consider the implications of their framework on depth separations for equivariant networks.

Weaknesses:
Significance/novelty: The novelty relative to Sonoda et al [33] is limited, and the significance of this work to the universality and equivariance literatures is unclear. For example, many universality results already exist in equivariance (see e.g. work by Yarotsky [3], by Dym et al [2], etc.) — it is not clear how much value this extension of the ridgelet transform adds. 


Clarity: I found the writing of the paper extremely hard to follow. It did not provide sufficient background on the ridgelet transform, universality results for equivariant networks (whether constructive or non-constructive), or perhaps most importantly, motivation for why one should value constructive approximation theorems for equivariant networks. It felt that one had to have read the previous works by Sonoda et al, in order to grasp why this work was important or where its novelty was, such as how vector-valued equivariant feature maps are superior to scalar-valued feature maps, what exactly formal networks are, what the practical use or theoretical value of the ridgelet transform is, etc. The work would also benefit from an outline of the sections earlier in the paper, and a more concise and early statement of what the authors consider their main theorem/s. It was not clear what the central result about the ridgelet transform was, as the transform seemed to still involve an integral in all equivariant cases, without simplification. 

As a demonstration of the power of their theoretical formulation, the authors claim to show a depth separation, in which some class of networks is exponentially wide when shallow (constant number of layers), and only linearly wide when deep (linear number of layers). However, it is not clear whether they show that any shallow network is exponentially wide when representing a given function, or just the one constructed by the ridgelet transform — is this a strict depth separation?

Mathematical rigor: Although I did not check all of the math, some glaring errors stood out to me. First, the proof of Lemma 5 begins with, “Recall that a tensor product of irreducible representations is irreducible.” This is incorrect — for example, the tensor product of the irreps of the group of 3D rotations, SO(3), are reducible, and the irreps that appear in the decomposition of their tensor products are famously given by the Clebsch-Gordan coefficients (see e.g. [1]). Moreover, in the limitations section (6.1), the authors discuss the assumption that the group is locally compact, but say that this “excludes infinite-dimensional groups”. Yet, this is also false: for example, the infinite group SO(3) is compact (and therefore locally compact). In fact, several of the authors’ examples pertain to infinite groups, such as the affine group. These errors are surprising. 

Also, the mathematical techniques themselves do not appear to be novel (for instance, Schur’s lemma is quite standard, and the proofs included in the main body are rather simple — Lemmas 1 and 2 are in fact widely known), and there are no experiments or practical implications, so the merit of the paper must rest on the significance of the results themselves. Unfortunately, the the broader significance of the results are not clearly demonstrated. The authors claim to reveal “the close relationship between machine learning theory and modern algebra,” but the mathematical tools they use seem like the standard ones used already throughout the equivariance literature. I am not sure what the “major upgrade to machine learning theory from the perspective of modern algebra” will therefore be. 

[1] Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network by Kondor, Lin, and Trivedi 2018

[2] On the Universality of Rotation Equivariant Point Cloud Networks by Nadav Dam and Haggai Maron 2020

[3] Universal approximations of invariant maps by neural networks by Dmitry Yarotsky 2018

Limitations:
Yes, the authors discussed limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a unified approach to universal approximation theorems for neural networks using group representation theory. It extends to vector-valued joint-group-equivariant feature maps, providing a systematic method for both shallow and deep neural networks with nonlinear activation functions. By leveraging Schur's lemma, the paper shows that these networks can universally approximate any function within a certain class. It main contribution is the closed-form ridgelet transform, which offers a constructive proof and explicit parameter distribution for these networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The paper introduces a unified constructive universal approximation theorem that applies to both shallow and deep neural networks using group representation theory. This is an innovative approach. It also extends previous work by incorporating vector-valued joint-group-equivariant feature maps.

2.  The paper is theoretically sounding, leveraging concepts from group representation theory and Schur's lemma. They perform the thorough and systematic development of the ridgelet transform, providing a closed-form solution for parameter distributions and ensuring the findings are theoretically well justified.

3. The paper is well-structured and clearly written. Definitions, theorems, and proofs are presented in a coherent manner, making it easier for readers to follow the details of the argument and understand the implications of the results.

4.  This work is significant since it provides a relationship between deep learning theory and modern algebra. By providing a unified framework that applies to a wide range of network architectures, the paper incentivize further research and development in the field of machine learning.

Weaknesses:
1.  While the paper is strong in its theoretical contributions, it lacks empirical validation through experiments or simulations. Demonstrating the practical applicability and effectiveness of the proposed ridgelet transform and the unified framework on real-world datasets or benchmark problems would strengthen the paper. Including even a small set of experiments could provide evidence of the practical relevance and performance of the theoretical results.

2. This work makes several assumptions, such as the local compactness of the group \( G \) and the boundedness of the composite operator \( \text{NN} \circ R \). While these assumptions are standard in group representation theory, the paper could benefit from a more detailed discussion on their implications and limitations. Exploring scenarios where these assumptions might not hold or providing guidance on how to relax these assumptions.

3. Some of the technical details, particularly those related to advanced concepts in group representation theory and the ridgelet transform, might be challenging for readers who are not experts in these areas. Providing additional intuitive explanations, diagrams, or examples to illustrate these concepts could enhance the clarity of the paper.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a generalization of the work by Sonoda et al. by extending their formulation of universal approximation theorems applicable to a specific class of neural networks namely scalar-valued joint-group-invariant feature maps for ""formal deep network"" to a much larger class of learning machines. Their theory using tools from group representation theory allows them to uniformly treat both shallow and deep neural networks with a larger class of activation functions. They provide an explicit construction for parameter assignment (aka Ridgelet Transform) and apply it to vector valued joint group-equivariant feature maps.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is well motivated and the writing is clear and understandable. The interspersed explanations in plain english are quite helpful in understanding a paper that leans quite heavily on sophisticated mathematical formalisms. (eg line 93-94). 
- The proofs and the notation are clear and succinct.
- The authors extend an earlier work to a much more practical and real world class of NNs by introducing *vector-valued joint group-equivariant* feature maps, which yields universal approximation theorems as corollaries. They also unify the treatment of both shallow and deep networks by leveraging Schur's Lemma.
- They provide explicit examples for depth 2 and depth $n$ fully connected network with an arbitrary activation in Section 4.2 which helps ground their method and significantly helps the reader understand how to leverage the tooling introduced by the authors.
- The paper provides formal support for the popular interpretation for the efficacy of DNNs compared to shallow networks, namely that they construct hierarchical representations which would take an exponential number of neurons to represent using a single layer.
- The limitations section is well written and is explicit about the assumptions made so that the reader is aware of the regime in which the proofs are applicable.

Weaknesses:
**Major**
- The biggest weakness of the work seems to be that it shares a vast amount of technical analysis, machinery and the fundamental proofs are shared with the earlier work by Sonoda et al. While the extension to a larger class of networks and the introduced vector values feature maps is certainly valuable, I am not fully convinced of the differential novelty of the work. Most of the (valuable) effort has been spent in a mostly natural extension of the previous work on the topic.


**Minor**
-  The authors mention that assumption (5) (that the network is given by the integral representation) in limitations is potentially an ""advantage"". If that is so, a discretized version would be the preferred model since it is also closer to real world NNs
- Typo on line 77  - mathmatical -> mathematical
- Typo on line 310 - cc-universaity -> cc-universality
- lines 135 - 137 would be significantly easier to read when broken into multiple lines

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work generalizes the ridgelet transform to equivariant neural networks, providing constructive proofs of universality in the general case as integrations over parameter distributions. Although such a direction had been taken up in prior work [33], they generalize it from scalar activations to vector activations, therefore encompassing more practical equivariant networks. The authors consider the form of the ridgelet transform for deep networks, and groups including the affine group and orthogonal group.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The authors provide a constructive universal approximation result, which is in contrast to many non-constructive universality results. They strictly improve on the past work of Sonoda et al [33] by extending from scalars to vectors, which is more realistic. They consider the implications of their framework on depth separations for equivariant networks.

Weaknesses:
Significance/novelty: The novelty relative to Sonoda et al [33] is limited, and the significance of this work to the universality and equivariance literatures is unclear. For example, many universality results already exist in equivariance (see e.g. work by Yarotsky [3], by Dym et al [2], etc.) — it is not clear how much value this extension of the ridgelet transform adds. 


Clarity: I found the writing of the paper extremely hard to follow. It did not provide sufficient background on the ridgelet transform, universality results for equivariant networks (whether constructive or non-constructive), or perhaps most importantly, motivation for why one should value constructive approximation theorems for equivariant networks. It felt that one had to have read the previous works by Sonoda et al, in order to grasp why this work was important or where its novelty was, such as how vector-valued equivariant feature maps are superior to scalar-valued feature maps, what exactly formal networks are, what the practical use or theoretical value of the ridgelet transform is, etc. The work would also benefit from an outline of the sections earlier in the paper, and a more concise and early statement of what the authors consider their main theorem/s. It was not clear what the central result about the ridgelet transform was, as the transform seemed to still involve an integral in all equivariant cases, without simplification. 

As a demonstration of the power of their theoretical formulation, the authors claim to show a depth separation, in which some class of networks is exponentially wide when shallow (constant number of layers), and only linearly wide when deep (linear number of layers). However, it is not clear whether they show that any shallow network is exponentially wide when representing a given function, or just the one constructed by the ridgelet transform — is this a strict depth separation?

Mathematical rigor: Although I did not check all of the math, some glaring errors stood out to me. First, the proof of Lemma 5 begins with, “Recall that a tensor product of irreducible representations is irreducible.” This is incorrect — for example, the tensor product of the irreps of the group of 3D rotations, SO(3), are reducible, and the irreps that appear in the decomposition of their tensor products are famously given by the Clebsch-Gordan coefficients (see e.g. [1]). Moreover, in the limitations section (6.1), the authors discuss the assumption that the group is locally compact, but say that this “excludes infinite-dimensional groups”. Yet, this is also false: for example, the infinite group SO(3) is compact (and therefore locally compact). In fact, several of the authors’ examples pertain to infinite groups, such as the affine group. These errors are surprising. 

Also, the mathematical techniques themselves do not appear to be novel (for instance, Schur’s lemma is quite standard, and the proofs included in the main body are rather simple — Lemmas 1 and 2 are in fact widely known), and there are no experiments or practical implications, so the merit of the paper must rest on the significance of the results themselves. Unfortunately, the the broader significance of the results are not clearly demonstrated. The authors claim to reveal “the close relationship between machine learning theory and modern algebra,” but the mathematical tools they use seem like the standard ones used already throughout the equivariance literature. I am not sure what the “major upgrade to machine learning theory from the perspective of modern algebra” will therefore be. 

[1] Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network by Kondor, Lin, and Trivedi 2018

[2] On the Universality of Rotation Equivariant Point Cloud Networks by Nadav Dam and Haggai Maron 2020

[3] Universal approximations of invariant maps by neural networks by Dmitry Yarotsky 2018

Limitations:
Yes, the authors discussed limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a unified approach to universal approximation theorems for neural networks using group representation theory. It extends to vector-valued joint-group-equivariant feature maps, providing a systematic method for both shallow and deep neural networks with nonlinear activation functions. By leveraging Schur's lemma, the paper shows that these networks can universally approximate any function within a certain class. It main contribution is the closed-form ridgelet transform, which offers a constructive proof and explicit parameter distribution for these networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The paper introduces a unified constructive universal approximation theorem that applies to both shallow and deep neural networks using group representation theory. This is an innovative approach. It also extends previous work by incorporating vector-valued joint-group-equivariant feature maps.

2.  The paper is theoretically sounding, leveraging concepts from group representation theory and Schur's lemma. They perform the thorough and systematic development of the ridgelet transform, providing a closed-form solution for parameter distributions and ensuring the findings are theoretically well justified.

3. The paper is well-structured and clearly written. Definitions, theorems, and proofs are presented in a coherent manner, making it easier for readers to follow the details of the argument and understand the implications of the results.

4.  This work is significant since it provides a relationship between deep learning theory and modern algebra. By providing a unified framework that applies to a wide range of network architectures, the paper incentivize further research and development in the field of machine learning.

Weaknesses:
1.  While the paper is strong in its theoretical contributions, it lacks empirical validation through experiments or simulations. Demonstrating the practical applicability and effectiveness of the proposed ridgelet transform and the unified framework on real-world datasets or benchmark problems would strengthen the paper. Including even a small set of experiments could provide evidence of the practical relevance and performance of the theoretical results.

2. This work makes several assumptions, such as the local compactness of the group \( G \) and the boundedness of the composite operator \( \text{NN} \circ R \). While these assumptions are standard in group representation theory, the paper could benefit from a more detailed discussion on their implications and limitations. Exploring scenarios where these assumptions might not hold or providing guidance on how to relax these assumptions.

3. Some of the technical details, particularly those related to advanced concepts in group representation theory and the ridgelet transform, might be challenging for readers who are not experts in these areas. Providing additional intuitive explanations, diagrams, or examples to illustrate these concepts could enhance the clarity of the paper.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a generalization of the work by Sonoda et al. by extending their formulation of universal approximation theorems applicable to a specific class of neural networks namely scalar-valued joint-group-invariant feature maps for ""formal deep network"" to a much larger class of learning machines. Their theory using tools from group representation theory allows them to uniformly treat both shallow and deep neural networks with a larger class of activation functions. They provide an explicit construction for parameter assignment (aka Ridgelet Transform) and apply it to vector valued joint group-equivariant feature maps.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is well motivated and the writing is clear and understandable. The interspersed explanations in plain english are quite helpful in understanding a paper that leans quite heavily on sophisticated mathematical formalisms. (eg line 93-94). 
- The proofs and the notation are clear and succinct.
- The authors extend an earlier work to a much more practical and real world class of NNs by introducing *vector-valued joint group-equivariant* feature maps, which yields universal approximation theorems as corollaries. They also unify the treatment of both shallow and deep networks by leveraging Schur's Lemma.
- They provide explicit examples for depth 2 and depth $n$ fully connected network with an arbitrary activation in Section 4.2 which helps ground their method and significantly helps the reader understand how to leverage the tooling introduced by the authors.
- The paper provides formal support for the popular interpretation for the efficacy of DNNs compared to shallow networks, namely that they construct hierarchical representations which would take an exponential number of neurons to represent using a single layer.
- The limitations section is well written and is explicit about the assumptions made so that the reader is aware of the regime in which the proofs are applicable.

Weaknesses:
**Major**
- The biggest weakness of the work seems to be that it shares a vast amount of technical analysis, machinery and the fundamental proofs are shared with the earlier work by Sonoda et al. While the extension to a larger class of networks and the introduced vector values feature maps is certainly valuable, I am not fully convinced of the differential novelty of the work. Most of the (valuable) effort has been spent in a mostly natural extension of the previous work on the topic.


**Minor**
-  The authors mention that assumption (5) (that the network is given by the integral representation) in limitations is potentially an ""advantage"". If that is so, a discretized version would be the preferred model since it is also closer to real world NNs
- Typo on line 77  - mathmatical -> mathematical
- Typo on line 310 - cc-universaity -> cc-universality
- lines 135 - 137 would be significantly easier to read when broken into multiple lines

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
oRZN9a53ag;"REVIEW 
Summary:
This paper present novel theoretical results to identify causal effects in restricted ANMs even in case of unobserved confounders.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
**The paper provides novel contributions to the field of score-based causal discovery by extending previous works to confounded restricted ANMs. Based on these contributions, I strongly support this paper's acceptance.**

- the problem definition is very useful to orient readers
- the theoretical results are novel
- the experiments compare against a sufficient number of baselines, and though the proposed method is not SOTA, it compares well and has better theoretical guarantees


Based on the authors' response, I am eager to improve my score.

Weaknesses:
I have a few remarks on improving the flow of the paper; however, even the first four points are not considered major issues.

- Even though condition 1 is a well-known result in the causality literature, I suggest explaining why that admits linear models and including some description of **restricted ANMs** in the main text (at least for me, it is not evident, especially since the condition lacks intuition). To be clear, even this point does not diminish the main contribution, which I see regarding the results for confounders.
- As **inducing paths** are an important concept for the main contributions, please _include it in the main text_ if space permits (suggestion: you can reduce spacing in $\texttt{itemize}$ by setting $\texttt{\\\\begin\\{itemize\\}[nolistsep]}$ )
- I could **not find the definition of an active path** (not even in Def. 5, where it is said to be defined); I presume it is a path that is not blocked, but it would be better to state this explicitly. Maybe it would even be better to use ""a path that is not blocked"" instead of introducing new terminology (this is the first time I encountered ""active paths""; I could be wrong about this)
- The text is sometimes difficult to follow, due to heavy reliance on notation. I'd consider delegating the not crucial part to the appendix (potential candidates in 2.1) and using the remaining space to explain the main quantities better, especially the residuals (e.g., Eq. 12)


## Minor points
- please specify what you are calculating the expectation with respect to (using a bold E is also unconventional, though it's clear from the context it's an expectation)
- as the mathematical objects for d-/m-separation are distinguishable, you might consider dropping the superscript to simplify notation; also, I'd suggest adding whitespace after $\perp^d_\mathcal{G}$ and the like to make it easier for the reader to attribute the indices to $\perp$ and not the not on its right
- I could not find the definition for $\dot{\cup}$ 
- in the explanation of Prop 3., the wording makes it a bit hard to discern that you also provide intuition for the second part; it would help if you refer to _Part (ii)_ explicitly
- _Score matching through the roof_ in the title does not have added value for me, I'd consider rephrasing it to convey the message that ""we propose score-based causal discovery methods for confounded restricted ANMs""

Limitations:
The authors provide an **honest comparison** of their method in the experiments, clearly stating its limitations compared to other methods.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose AdaScore, a method for causal discovery that generalizes previous work based on score matching for SCMs with possibly latent nodes. They combine connections of the score to conditional independence as well as to additive noise SCMs and show that a NoGAM-type procedure works to recover the direction of non-confounded edges of the corresponding partial ancestral graph.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Adapting NoGAM [1] to the case allowing hidden variables is a practically meaningful contribution.
- Model assumptions are somewhat weakened compared to CAM-UV by allowing for general mechanisms within blocks of observed and latent parents.

Weaknesses:
The novelty of the paper lies primarily in the application of NoGAM to orient very specific edges in a partial ancestral graph (PAG) (the ancestral graph that represents the Markov equivalence class, in analogue to a CPDAG). This falls significantly short of the main contributions as described by the authors on l33--55. Specifically:

- The authors state that they show how constraints on the Jacobian of the score can be used as conditional independence testing. However, the extent to which this is done is only by noticing the equivalence between conditional independence and the corresponding zero in the Jacobian term (previously noted in [1,2]), without any formal analysis of the proposed t-test (Appendix C) as a statistical test of conditional independence (which happens to be a notoriously difficult test).

- The authors state that their identification results for additive noise models generalize the previous results obtained by previous works. In l193, the authors state ""we remove the nonlinearity assumption (of [3]) and make the weaker hypothesis of a restricted additive noise model"", but 1) this is a stronger assumption than additive noise, not a weaker one, and 2) the authors in [3] also consider the same restricted additive noise model. 

- The authors claim that AdaScore is able to handle a broad class of causal models (l54), but three out of four possible situations are direct applications of existing work. 1) Under no structural assumptions with or without latent confounders, AdaScore simply performs constraint-based causal discovery (FCI) using the conditional independence properties of the Jacobian of the score, a straightforward application of [1] also previously noticed in [2]. 2) Under an additive noise assumption, AdaScore is exactly equivalent to NoGAM. 3) Only under an additive noise assumption with hidden confounders, does AdaScore generalize NoGAM to orient unconfounded edges of the PAG returned by FCI, which may be very few of the discovered adjacencies.

### Other comments

- The experiments do not seem to suggest that AdaScore out performs other methods in any meaningful way---in fact, in Figure 1 a) AdaScore is completely equivalent to NoGAM, and is thus redundant. In Figure 1 b), where AdaScore should distinguish itself, it does not appear to be consistently better than CAM-UV. 

- Much of the paper (> 5 pages) is spent on directly describing previous works, NoGAM[3] and/or provide basic background on DAGs and MAGs. 

[1] Spantini et al., ""Inference via low-dimensional couplings."" JMLR 2018.
[2] Montagna et al., ""Scalable causal discovery with score matching."" CLeaR 2023.
[3] Montagna et al., ""Causal discovery with score matching on additive models with arbitrary noise."" CLeaR 2023.

Limitations:
The authors do not adequately discuss the limitations of their method---the limitations section in the appendix focuses purely on the empirical study. The authors claim that AdaScore is adaptive in the sense of being ""less reliant on prior assumptions which are often untestable"", but this is only in the sense that it performs different algorithms depending on user specification, which hardly constitutes one single unifying adaptive algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper extends theoretical results about causal discovery through score matching to encompass both linear and non-linear SCMs and lift the sufficiency assumption. The theoretical results relax the non-linearity assumption of Montagna et al 2023 by swapping it with the less restrictive one of restricted ANM from Peters et al 2009. As for the latent confounder detection a parallel with m-separation is drawn using results from Spantini et al 2018, to establish that the score will be non-zero in the presence of an active path. Following the theoretical results, an algorithm to estimate causal graphs from data is proposed and evaluated, generalizing the NoGAM algorithm of Montagna et al 2023, which only covers the non-linear case.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written and, if it wasn’t for some of the definitions relegated to the appendix, very easy to follow.  

The theoretical results, particularly Propositions 2 and 3 are important extensions of the score-matching methodology for causal discovery, dealing with both the non-linearity and sufficiency assumptions of the method proposed in Montagna et al. 2023.

Weaknesses:
The paper motivation is basically the weakening of current assumptions for causal discovery methods. However, assumptions and benefits of the proposed methodology are not clearly specified. In line 74, the authors state that faithfulness is assumed (I believe, it is kind of hidden in the background notions). If that is the case, the method adopts the same assumption as FCI, plus ANM. So the proposed method is relaxing assumptions compared to CAM-UV, RCD and NoGAM, but adding onto FCI. Regarding the benefits, the alleged flexibility of the method to output DAGs, MECs, MAGs, PAGs, which should make it preferable to FCI, is merely touched upon in the contributions and the experiment section.  

Proposition 1 is a rather trivial application of the more general lemma in Spantini et al. and it does not specify the required faithfulness assumption to obtain the result from Eq. 6 in the paper. 

The experimental results show limited added value according to the one metric chosen (SHD) in a synthetic setting. They are not comprehensive enough, with no application to common (pseudo-)real benchmarks (e.g. from bnlearn). More experiments and more metrics are needed as, as it stands, the proposed method seems to add no real value compared to the baselines. Additionally, it is not clear from the experiments if it is really able to identify confounders. Breaking down precision and recall by mark would show this. FCI and a random baseline should be also added for reference.  

Experiments are conducted on data with at most 9 variables, and the scalability of the method is not shown nor discussed. 

The model used to estimate residuals is not discussed, nor the assumption that the chosen model fits the data adequately to correctly estimate residuals, and what is needed to assess this.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper present novel theoretical results to identify causal effects in restricted ANMs even in case of unobserved confounders.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
**The paper provides novel contributions to the field of score-based causal discovery by extending previous works to confounded restricted ANMs. Based on these contributions, I strongly support this paper's acceptance.**

- the problem definition is very useful to orient readers
- the theoretical results are novel
- the experiments compare against a sufficient number of baselines, and though the proposed method is not SOTA, it compares well and has better theoretical guarantees


Based on the authors' response, I am eager to improve my score.

Weaknesses:
I have a few remarks on improving the flow of the paper; however, even the first four points are not considered major issues.

- Even though condition 1 is a well-known result in the causality literature, I suggest explaining why that admits linear models and including some description of **restricted ANMs** in the main text (at least for me, it is not evident, especially since the condition lacks intuition). To be clear, even this point does not diminish the main contribution, which I see regarding the results for confounders.
- As **inducing paths** are an important concept for the main contributions, please _include it in the main text_ if space permits (suggestion: you can reduce spacing in $\texttt{itemize}$ by setting $\texttt{\\\\begin\\{itemize\\}[nolistsep]}$ )
- I could **not find the definition of an active path** (not even in Def. 5, where it is said to be defined); I presume it is a path that is not blocked, but it would be better to state this explicitly. Maybe it would even be better to use ""a path that is not blocked"" instead of introducing new terminology (this is the first time I encountered ""active paths""; I could be wrong about this)
- The text is sometimes difficult to follow, due to heavy reliance on notation. I'd consider delegating the not crucial part to the appendix (potential candidates in 2.1) and using the remaining space to explain the main quantities better, especially the residuals (e.g., Eq. 12)


## Minor points
- please specify what you are calculating the expectation with respect to (using a bold E is also unconventional, though it's clear from the context it's an expectation)
- as the mathematical objects for d-/m-separation are distinguishable, you might consider dropping the superscript to simplify notation; also, I'd suggest adding whitespace after $\perp^d_\mathcal{G}$ and the like to make it easier for the reader to attribute the indices to $\perp$ and not the not on its right
- I could not find the definition for $\dot{\cup}$ 
- in the explanation of Prop 3., the wording makes it a bit hard to discern that you also provide intuition for the second part; it would help if you refer to _Part (ii)_ explicitly
- _Score matching through the roof_ in the title does not have added value for me, I'd consider rephrasing it to convey the message that ""we propose score-based causal discovery methods for confounded restricted ANMs""

Limitations:
The authors provide an **honest comparison** of their method in the experiments, clearly stating its limitations compared to other methods.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose AdaScore, a method for causal discovery that generalizes previous work based on score matching for SCMs with possibly latent nodes. They combine connections of the score to conditional independence as well as to additive noise SCMs and show that a NoGAM-type procedure works to recover the direction of non-confounded edges of the corresponding partial ancestral graph.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Adapting NoGAM [1] to the case allowing hidden variables is a practically meaningful contribution.
- Model assumptions are somewhat weakened compared to CAM-UV by allowing for general mechanisms within blocks of observed and latent parents.

Weaknesses:
The novelty of the paper lies primarily in the application of NoGAM to orient very specific edges in a partial ancestral graph (PAG) (the ancestral graph that represents the Markov equivalence class, in analogue to a CPDAG). This falls significantly short of the main contributions as described by the authors on l33--55. Specifically:

- The authors state that they show how constraints on the Jacobian of the score can be used as conditional independence testing. However, the extent to which this is done is only by noticing the equivalence between conditional independence and the corresponding zero in the Jacobian term (previously noted in [1,2]), without any formal analysis of the proposed t-test (Appendix C) as a statistical test of conditional independence (which happens to be a notoriously difficult test).

- The authors state that their identification results for additive noise models generalize the previous results obtained by previous works. In l193, the authors state ""we remove the nonlinearity assumption (of [3]) and make the weaker hypothesis of a restricted additive noise model"", but 1) this is a stronger assumption than additive noise, not a weaker one, and 2) the authors in [3] also consider the same restricted additive noise model. 

- The authors claim that AdaScore is able to handle a broad class of causal models (l54), but three out of four possible situations are direct applications of existing work. 1) Under no structural assumptions with or without latent confounders, AdaScore simply performs constraint-based causal discovery (FCI) using the conditional independence properties of the Jacobian of the score, a straightforward application of [1] also previously noticed in [2]. 2) Under an additive noise assumption, AdaScore is exactly equivalent to NoGAM. 3) Only under an additive noise assumption with hidden confounders, does AdaScore generalize NoGAM to orient unconfounded edges of the PAG returned by FCI, which may be very few of the discovered adjacencies.

### Other comments

- The experiments do not seem to suggest that AdaScore out performs other methods in any meaningful way---in fact, in Figure 1 a) AdaScore is completely equivalent to NoGAM, and is thus redundant. In Figure 1 b), where AdaScore should distinguish itself, it does not appear to be consistently better than CAM-UV. 

- Much of the paper (> 5 pages) is spent on directly describing previous works, NoGAM[3] and/or provide basic background on DAGs and MAGs. 

[1] Spantini et al., ""Inference via low-dimensional couplings."" JMLR 2018.
[2] Montagna et al., ""Scalable causal discovery with score matching."" CLeaR 2023.
[3] Montagna et al., ""Causal discovery with score matching on additive models with arbitrary noise."" CLeaR 2023.

Limitations:
The authors do not adequately discuss the limitations of their method---the limitations section in the appendix focuses purely on the empirical study. The authors claim that AdaScore is adaptive in the sense of being ""less reliant on prior assumptions which are often untestable"", but this is only in the sense that it performs different algorithms depending on user specification, which hardly constitutes one single unifying adaptive algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper extends theoretical results about causal discovery through score matching to encompass both linear and non-linear SCMs and lift the sufficiency assumption. The theoretical results relax the non-linearity assumption of Montagna et al 2023 by swapping it with the less restrictive one of restricted ANM from Peters et al 2009. As for the latent confounder detection a parallel with m-separation is drawn using results from Spantini et al 2018, to establish that the score will be non-zero in the presence of an active path. Following the theoretical results, an algorithm to estimate causal graphs from data is proposed and evaluated, generalizing the NoGAM algorithm of Montagna et al 2023, which only covers the non-linear case.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written and, if it wasn’t for some of the definitions relegated to the appendix, very easy to follow.  

The theoretical results, particularly Propositions 2 and 3 are important extensions of the score-matching methodology for causal discovery, dealing with both the non-linearity and sufficiency assumptions of the method proposed in Montagna et al. 2023.

Weaknesses:
The paper motivation is basically the weakening of current assumptions for causal discovery methods. However, assumptions and benefits of the proposed methodology are not clearly specified. In line 74, the authors state that faithfulness is assumed (I believe, it is kind of hidden in the background notions). If that is the case, the method adopts the same assumption as FCI, plus ANM. So the proposed method is relaxing assumptions compared to CAM-UV, RCD and NoGAM, but adding onto FCI. Regarding the benefits, the alleged flexibility of the method to output DAGs, MECs, MAGs, PAGs, which should make it preferable to FCI, is merely touched upon in the contributions and the experiment section.  

Proposition 1 is a rather trivial application of the more general lemma in Spantini et al. and it does not specify the required faithfulness assumption to obtain the result from Eq. 6 in the paper. 

The experimental results show limited added value according to the one metric chosen (SHD) in a synthetic setting. They are not comprehensive enough, with no application to common (pseudo-)real benchmarks (e.g. from bnlearn). More experiments and more metrics are needed as, as it stands, the proposed method seems to add no real value compared to the baselines. Additionally, it is not clear from the experiments if it is really able to identify confounders. Breaking down precision and recall by mark would show this. FCI and a random baseline should be also added for reference.  

Experiments are conducted on data with at most 9 variables, and the scalability of the method is not shown nor discussed. 

The model used to estimate residuals is not discussed, nor the assumption that the chosen model fits the data adequately to correctly estimate residuals, and what is needed to assess this.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oQc7TOCk5G;"REVIEW 
Summary:
This paper investigates the theoretical boundaries of learning with Label Differential Privacy (Label-DP) in both central and local models.
Label-DP is a weakening of standard differential privacy, where only the privacy of the ""label"" of each example is to be protected (an example is a pair (feature vector, label)).

The key contributions of the paper are to establish min-max optimal rates for excess error in the settings of:
* (multi-class) classification,
* regression with bounded labels,
* regression with unbounded labels (but under a bounded moment condition).

The min-max rates are over the class of data distributions that satisfy $\beta$-Holder smoothness, admits a lower bound on probability density that is bounded away from zero, assumes that there are no “sharp corners” in the input space, and a $\gamma$-margin assumption (in case of classification), or bounded label range or bounded label moments (in case of regression).

These min-max rates are then compared against the previously known min-max rates for learning under “full” local-DP (that protects both features and labels), as well as non-private learning.

The key takeaways are:
* Local-DP vs Non-Private:
  * For classification and regression with bounded labels, the sample complexity under Local-DP increases by a factor of $1/\varepsilon^2$, but has the same rate in terms of desired excess error. This is unlike “Full Local-DP”, where the sample complexity is larger even in terms of the desired excess error.
  * For regression with unbounded labels, the dependence of sample complexity on desired excess error is worse than the non-private setting.
* Central-DP vs Non-Private:
  * The excess error is the sum of the non-private excess error and an additional term that decays faster in the number of samples, so the additional sample complexity due to privacy is negligible for very small excess error.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper provides a comprehensive study of the min-max rates for learning under label differential privacy, in both local and central models of DP, and for both classification and regression. This complements prior literature on min-max rates for learning (non-privately) and for learning under (full) differential privacy. The rates highlight the precise cost of _label_ differential privacy and the sample complexity benefits over full differential privacy.

Weaknesses:
While there are many results in the paper, I think the proof techniques in both lower and upper bounds use mostly standard tools (This is not necessarily a weakness!).

The paper writing could be improved at several places though. Some comments are listed below under ""Questions"".

Limitations:
I do not see any potential negative societal impact of this work, as it is primarily theoretical.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the minimax rates for classification and regression under (pure) label differential privacy in both the local and central models. They prove that rates of convergence for classification and regression with bounded label noise in the local label DP model are comparable to those for the non-private tasks, except for the expected $1/\varepsilon^2$ dependence. This represents an improvement over rates for standard DP in both settings, where there is a worse dependence on the dimension of the covariates. They also prove, however, in the case of regression with unbounded label noise, the convergence rate improvements over “full” DP aren’t as meaningful.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work makes notable progress in our theoretical understanding of the costs of label DP relative to non-private and full DP algorithms for the same learning task.

Weaknesses:
The presentation could be improved in several places. Admittedly, this is written from a statistical perspective that is different from the one I am most familiar with, so some of the perceived presentation issues may just be a matter of convention, but the following changes might make this work more understandable to the general NeurIPS community:


Abstract:

The main challenge and the techniques to overcome them as stated in the abstract aren’t clear to me as a reader at this point. It’s not yet stated that the subject of interest is minimax rates, and so there’s no context for the statement “take infimum over all possible learners” and why that would present a challenge. Generally, I did not have a good idea of what the contribution of this work was from the abstract.

Introduction:

“the learning performances” -> “the learning performance” 

“the label DP” -> “label DP”

In Table 1, attribution for the full DP rates in the local DP setting as well as the rates in the non-private setting should be given in the table. Also, I think there’s an issue with the parentheses in the local label DP rates for regression with bounded label noise.

Section 2:

In the “Minimax analysis for private data” paragraph, KNLRS11 is attributed with finding the relation between label DP and stochastic queries. This is not accurate, this work characterizes local DP learning by the statistical query model.

Section 3:

“We hope that $R - R^*$ to be as small as possible” -> “we seek to minimize this risk” or something similar

“the Bayes optimal classifier and the corresponding Bayes risk is” -> “the Bayes optimal classifier and the corresponding Bayes risk are”

In Proposition 2, f(x) is used before it is defined.

Section 4:

I didn’t find the proof outline for Theorem 1 or Theorem 3 to be informative at all. It would be good to add more specifics if possible.

“Let the privacy mechanism M(x,y) outputs” -> “Let the privacy mechanism M(x,y) output”

Limitations:
Yes, the authors address the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the problems of classification and regression under the constraint of local/central pure label DP. The authors derive upper and lower bounds on the excess risk (compared to the non-private Bayes classifier/regression) for these problems, under somewhat standard assumptions on the 'ground truth' randomized label function $\eta$. For regression, both the case where the labels are bounded and have bounded moments are considered. For the lower bounds, the authors develop extensions of techniques from minimax estimation to label DP. For upper bounds, authors propose some algorithms combining 'binning' different examples with a privacy mechanism chosen according to the problem setting. The upper/lower bounds are matching in each setting up to logarithmic factors. For local label DP, the authors show the minimax excess risk with $N$ samples matches the non-private bounds using $N \min\{\epsilon^2, 1\}$ samples. In other words, with $\epsilon = \Omega(1)$ the minimax risk asymptotically matches the non-private risk, and otherwise there is an inherent separation. For central label DP, the minimax bound is one that approaches the non-private bound as $N \rightarrow \infty$ for any fixed $\epsilon$, showing a qualitative difference. For local ""full"" DP, i.e. the features are also private, even for $\epsilon = \Omega(1)$ and large $N$ one cannot achieve the non-private rate.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Derives optimal (up to log factors) upper and lower bounds for several different variants of classification/regression under label DP.
* To derive these bounds, introduces some new technical tools for minimax analysis of DP algorithms that might be useful in future work.
* Label DP is a variant of DP that is seeing attention in practice, and classification/regression are fundamental problems, so the results in the paper can have a practical impact easily.
* The authors do a good job making clear the comparison between the results in different settings. e.g. Table 1 is a very concise summary that allows one to draw all the essential comparisons between the different settings, and there are discussions like Remark 1 that give qualitative interpretations of the quantitative results, and also discuss other baselines to compare to.

Weaknesses:
The main issue is with the presentation. Specifically, the presentation does a great job explaining what the final results are and helping the reader contextualizing them, but at some points the techniques used to obtain the result are discussed at a very high level in the main body and why they work remains obscure even after reading the proof outlines in the main body multiple times.  There are some cases where the authors do a good job concisely describing a proof, e.g. Theorem 6's proof outline is very concise but it still gives a good idea what the proof looks like, even if they would have to check the appendix for details. But for others, like Theorems 1/2/3, the proof outline is not very informative. See Questions for more details.

I understand the authors are constrained by space requirements, but I think the allocation of space in the main body can be better thought out. For example, I think it might be better to try to give the reader a very good understanding of classification and/or bounded label regression (e.g., Lemma 1 from the Appendix could be brought to the main body without its proof, and the authors could explain how it is used), and omit all but the top-level points on bounded label moment regression, rather than giving a sparse understanding of all three.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the minimax risks of classification and regression (with both bounded and heavy-tailed noise) under label differential privacy (DP) in both central and local models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper provides a comprehensive analysis by considering both upper and lower bounds for the minimax risks.
It explores both central and local DP models and different settings, covering a broad spectrum of scenarios.

Weaknesses:
The writing quality needs improvement to meet publication standards. Several sections are challenging to understand. Specific issues include: 
(1) Around line 178, the output of the mechanism for classification is unclear. Why is it not a one-hot vector, or at least why is the L1 norm not equal to 1?
(2) Some notations are overused. For example, ""c"" refers to the lower bound of the density function in Assumption 1 and also denotes the classifier in line 186 and subsequent proofs.
(3) The description of the algorithm before Theorem 2 is vague and lacks clarity.
(4) The proofs in the appendix are hard to follow without explanations or discussions. For instance, how is $\phi$ defined in Equation (35), and what purpose does it serve? Why does the construction satisfy the assumptions? There seem to be some typos or missing elements in Equations (39) and (40).

Limitations:
See weaknesses and questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the theoretical boundaries of learning with Label Differential Privacy (Label-DP) in both central and local models.
Label-DP is a weakening of standard differential privacy, where only the privacy of the ""label"" of each example is to be protected (an example is a pair (feature vector, label)).

The key contributions of the paper are to establish min-max optimal rates for excess error in the settings of:
* (multi-class) classification,
* regression with bounded labels,
* regression with unbounded labels (but under a bounded moment condition).

The min-max rates are over the class of data distributions that satisfy $\beta$-Holder smoothness, admits a lower bound on probability density that is bounded away from zero, assumes that there are no “sharp corners” in the input space, and a $\gamma$-margin assumption (in case of classification), or bounded label range or bounded label moments (in case of regression).

These min-max rates are then compared against the previously known min-max rates for learning under “full” local-DP (that protects both features and labels), as well as non-private learning.

The key takeaways are:
* Local-DP vs Non-Private:
  * For classification and regression with bounded labels, the sample complexity under Local-DP increases by a factor of $1/\varepsilon^2$, but has the same rate in terms of desired excess error. This is unlike “Full Local-DP”, where the sample complexity is larger even in terms of the desired excess error.
  * For regression with unbounded labels, the dependence of sample complexity on desired excess error is worse than the non-private setting.
* Central-DP vs Non-Private:
  * The excess error is the sum of the non-private excess error and an additional term that decays faster in the number of samples, so the additional sample complexity due to privacy is negligible for very small excess error.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper provides a comprehensive study of the min-max rates for learning under label differential privacy, in both local and central models of DP, and for both classification and regression. This complements prior literature on min-max rates for learning (non-privately) and for learning under (full) differential privacy. The rates highlight the precise cost of _label_ differential privacy and the sample complexity benefits over full differential privacy.

Weaknesses:
While there are many results in the paper, I think the proof techniques in both lower and upper bounds use mostly standard tools (This is not necessarily a weakness!).

The paper writing could be improved at several places though. Some comments are listed below under ""Questions"".

Limitations:
I do not see any potential negative societal impact of this work, as it is primarily theoretical.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the minimax rates for classification and regression under (pure) label differential privacy in both the local and central models. They prove that rates of convergence for classification and regression with bounded label noise in the local label DP model are comparable to those for the non-private tasks, except for the expected $1/\varepsilon^2$ dependence. This represents an improvement over rates for standard DP in both settings, where there is a worse dependence on the dimension of the covariates. They also prove, however, in the case of regression with unbounded label noise, the convergence rate improvements over “full” DP aren’t as meaningful.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work makes notable progress in our theoretical understanding of the costs of label DP relative to non-private and full DP algorithms for the same learning task.

Weaknesses:
The presentation could be improved in several places. Admittedly, this is written from a statistical perspective that is different from the one I am most familiar with, so some of the perceived presentation issues may just be a matter of convention, but the following changes might make this work more understandable to the general NeurIPS community:


Abstract:

The main challenge and the techniques to overcome them as stated in the abstract aren’t clear to me as a reader at this point. It’s not yet stated that the subject of interest is minimax rates, and so there’s no context for the statement “take infimum over all possible learners” and why that would present a challenge. Generally, I did not have a good idea of what the contribution of this work was from the abstract.

Introduction:

“the learning performances” -> “the learning performance” 

“the label DP” -> “label DP”

In Table 1, attribution for the full DP rates in the local DP setting as well as the rates in the non-private setting should be given in the table. Also, I think there’s an issue with the parentheses in the local label DP rates for regression with bounded label noise.

Section 2:

In the “Minimax analysis for private data” paragraph, KNLRS11 is attributed with finding the relation between label DP and stochastic queries. This is not accurate, this work characterizes local DP learning by the statistical query model.

Section 3:

“We hope that $R - R^*$ to be as small as possible” -> “we seek to minimize this risk” or something similar

“the Bayes optimal classifier and the corresponding Bayes risk is” -> “the Bayes optimal classifier and the corresponding Bayes risk are”

In Proposition 2, f(x) is used before it is defined.

Section 4:

I didn’t find the proof outline for Theorem 1 or Theorem 3 to be informative at all. It would be good to add more specifics if possible.

“Let the privacy mechanism M(x,y) outputs” -> “Let the privacy mechanism M(x,y) output”

Limitations:
Yes, the authors address the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the problems of classification and regression under the constraint of local/central pure label DP. The authors derive upper and lower bounds on the excess risk (compared to the non-private Bayes classifier/regression) for these problems, under somewhat standard assumptions on the 'ground truth' randomized label function $\eta$. For regression, both the case where the labels are bounded and have bounded moments are considered. For the lower bounds, the authors develop extensions of techniques from minimax estimation to label DP. For upper bounds, authors propose some algorithms combining 'binning' different examples with a privacy mechanism chosen according to the problem setting. The upper/lower bounds are matching in each setting up to logarithmic factors. For local label DP, the authors show the minimax excess risk with $N$ samples matches the non-private bounds using $N \min\{\epsilon^2, 1\}$ samples. In other words, with $\epsilon = \Omega(1)$ the minimax risk asymptotically matches the non-private risk, and otherwise there is an inherent separation. For central label DP, the minimax bound is one that approaches the non-private bound as $N \rightarrow \infty$ for any fixed $\epsilon$, showing a qualitative difference. For local ""full"" DP, i.e. the features are also private, even for $\epsilon = \Omega(1)$ and large $N$ one cannot achieve the non-private rate.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Derives optimal (up to log factors) upper and lower bounds for several different variants of classification/regression under label DP.
* To derive these bounds, introduces some new technical tools for minimax analysis of DP algorithms that might be useful in future work.
* Label DP is a variant of DP that is seeing attention in practice, and classification/regression are fundamental problems, so the results in the paper can have a practical impact easily.
* The authors do a good job making clear the comparison between the results in different settings. e.g. Table 1 is a very concise summary that allows one to draw all the essential comparisons between the different settings, and there are discussions like Remark 1 that give qualitative interpretations of the quantitative results, and also discuss other baselines to compare to.

Weaknesses:
The main issue is with the presentation. Specifically, the presentation does a great job explaining what the final results are and helping the reader contextualizing them, but at some points the techniques used to obtain the result are discussed at a very high level in the main body and why they work remains obscure even after reading the proof outlines in the main body multiple times.  There are some cases where the authors do a good job concisely describing a proof, e.g. Theorem 6's proof outline is very concise but it still gives a good idea what the proof looks like, even if they would have to check the appendix for details. But for others, like Theorems 1/2/3, the proof outline is not very informative. See Questions for more details.

I understand the authors are constrained by space requirements, but I think the allocation of space in the main body can be better thought out. For example, I think it might be better to try to give the reader a very good understanding of classification and/or bounded label regression (e.g., Lemma 1 from the Appendix could be brought to the main body without its proof, and the authors could explain how it is used), and omit all but the top-level points on bounded label moment regression, rather than giving a sparse understanding of all three.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the minimax risks of classification and regression (with both bounded and heavy-tailed noise) under label differential privacy (DP) in both central and local models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper provides a comprehensive analysis by considering both upper and lower bounds for the minimax risks.
It explores both central and local DP models and different settings, covering a broad spectrum of scenarios.

Weaknesses:
The writing quality needs improvement to meet publication standards. Several sections are challenging to understand. Specific issues include: 
(1) Around line 178, the output of the mechanism for classification is unclear. Why is it not a one-hot vector, or at least why is the L1 norm not equal to 1?
(2) Some notations are overused. For example, ""c"" refers to the lower bound of the density function in Assumption 1 and also denotes the classifier in line 186 and subsequent proofs.
(3) The description of the algorithm before Theorem 2 is vague and lacks clarity.
(4) The proofs in the appendix are hard to follow without explanations or discussions. For instance, how is $\phi$ defined in Equation (35), and what purpose does it serve? Why does the construction satisfy the assumptions? There seem to be some typos or missing elements in Equations (39) and (40).

Limitations:
See weaknesses and questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
oEmyoy5H5P;"REVIEW 
Summary:
The paper is a review of algorithmic recourse (AR) literature. The authors deploy a systematic framework to investigate research trends in algorithmic recourse and evaluate their incorporation of practical concerns like societal and institutional considerations of AR, or lack thereof. The review finds that current research is  focused on methods and technical considerations. The authors encourage researchers in AR to consider real-world implications of their work and conduct user studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Paper is well-organized and easy to follow
- Section 2 provides solid background information on algorithmic recourse
- The questions in Section 4 are pertinent

Weaknesses:
While I agree with the points being made and appreciate the findings in the paper, I question their novelty. As mentioned in Section 4.6, there are papers (albeit in smaller numbers than we would want) that already provide real-world examples and attempt to discuss ethics within recourse. Previous work by [Doshi-Velez and Kim](https://arxiv.org/pdf/1702.08608), [Vaughan and Wallach](https://www.jennwv.com/papers/intel-chapter.pdf) have called for more user studies in interpretable ML, which resulted in studies like [Sixt et al.](https://openreview.net/pdf?id=v6s3HVjPerv). Considering that many researchers working on recourse is also in the field of ML interpretability, I am not sure if the paper's results and call for more user studies are very substantive.

Spending more time differentiating this work from other related works (especially other literature reviews like [70]) rather than listing their contributions in Section 2.2 may be helpful in making your case.

A more thorough discussion and evaluation of results (attempted in Section 5) may resolve some of these questions. As it stands, there is a disconnect between Section 4 and 5. The message of the first part of Section 5 (lines 318-350) is not clear. The second paragraph of the section does not seem to be a discussion of survey results but rather an argument the authors are trying to make (without using the results). The paper would benefit from expanding on the contents in lines 352 to 356, pointing to results in Section 4 and bridging them to the suggestions in Section 5.1.

The paper reads more like a position paper, trying to convince researchers in algorithmic recourse to not only focus on technical methods (which, again, I agree with). But I am not sure if a literature review or a position paper suitable for NeurIPS, considering its call for papers, does not seem to suggest so.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper provides a review of previous works that study ""algorithmic recourse"", i.e. conceptual and practical approaches for giving people actionable recommendations to change how they are impacted by algorithmic systems. This literature is deeply connected with counterfactual explanations and understanding models through small changes to test data, answering questions such as ""how would the model M produce a different output if changed attribute x about myself"". The authors review 127 archival publications and answer 9 questions about how these works frame and study algorithmic recourse.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
In terms of originality, quality, and clarity:
- While the primary novel contributions of this draft are to highlight themes in previous work, the overall level of novelty is reasonable. Some concerns here, see below.
- Quality: the ""Systematized review"" methods are described such that they are replicable and seem justified. I don't expect readers to have major issues with inclusion criteria of papers, or any of the analyses presented.
- Clarity: Writing is clear throughout.

In terms of significance, the paper could have impact on future work studying algorithmic recourse, and might motivate NeurIPS community members (including those in companies or working with governments) to support recourse methods. This would be a large positive impact.  

 This kind of review can certainly be useful to researchers trying to incorporate ideas or findings from recourse-related research. The calls to engage with HCI and systems-level thinking are reasonable (though, some of the broader discussion/motivation in the paper is more convincing on this front than any of the empirical results from the 127 recourse-related papers). If a version of this paper were able to unify definitions in the recourse space, this could be powerful (though further expansion of Section 2.2 might be necessary: the paper does note that reference [70] is highly similar -- the current draft was a bit vague in comparing these and clarifying the added contribution here.).

A few other notes: There are 9 overall sub-research questions answered. Overall, these results seem likely to be useful to researchers entering the algorithmic recourse field (though, see below, some of these felt very general and not domain-specific in the current draft). The paper does fit into the ""Social and economic aspects of machine learning"" category listed in the CFP this year.

Weaknesses:
Overall, I do think the current draft may not achieve the full impact that a future revision could provide.

The current discussion section feels like it largely echoes other calls in the community to apply systems thinking to ethical/responsible/pro-social AI/ML initiatives, and while each of the suggestions has some connection to one of the analyses, the current draft is not totally clear about the extent to which these recommendations stem from the findings vs. are motivated by first principles. The paper is overall very critical of the AR field, i.e. ""Why hasn't this field engagement with any real world deployments"". However, it's also not entirely clear in the current draft how any of the general recommendations would be applied in specific AR domains.

One aspect of the paper that I think would have been most directly helpful to the NeurIPS audience in particular would be to take a stance on how recourse should be defined -- is the definition on line 62 ""endorsed"" by the paper? Is the ""imagine a counterfactual input x*"" an advisable approach to take for future work. Does this review support the definition, highlight core definitional or epistemic issues, etc.

Ultimately, given the intended goals of this draft, it seems success and impact (on top of the core empirical contribution provided by writing a systematized review) here are dependent on the ability for the provided recommendations to shape future research positively. While the five recommendations here could have a some positive impact, taking a stronger stance on the core definitions and framing of recourse ""tasks"" could have an even larger impact.

Limitations:
- No major concerns regarding unmentioned social impacts.
- Regarding the limitations of systematized literature review, the current draft discusses these reasonably.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper provides a comprehensive review of the algorithmic recourse research literature, concentrating on understanding the recourse research ""in the wild"", by focusing on the practical application of these techniques in real-world scenarios. The authors then provide some suggestions to practitioners to push future research to better practical applications.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written and well-structured. Considerable effort has been put into this work to provide a comprehensive review of the area, highlighting the need for a more down-to-earth approach when considering recourse. The data collection and analysis are well-motivated and described sufficiently (Section 3 and Section 4). The recommendations in Section 5.1 are on point and all true, and they highlight issues that everyone in the community is aware of but that are largely ignored.

Weaknesses:
I feel NeurIPS is not the right venue for this kind of contribution, since this paper does not provide the level of technical novelty required by the conference. Being a review, I think it does not fit the requirement of ""new and original research"" given by the Call of Papers. I suggest the authors not be discouraged, since I think the contribution is still valuable for the community. Potential other venues I believe are more in line with the scope of this work could be the following (the order is random):
- IJCAI Survey Track (https://ijcai24.org/call-for-papers-survey-track/)
- ACM FAccT (https://facctconference.org/)
- AAAI/ACM AIES (https://www.aies-conference.com/2024/)
- ICML Position Papers Track (https://icml.cc/Conferences/2024/CallForPositionPapers)
- ACM Computing Surveys (https://dl.acm.org/journal/csur) 
- TMLR (https://jmlr.org/tmlr/)

Lastly, I would like to point out some potential additional papers on algorithmic recourse which could complement some remarks made by the authors:
- Line 182 ""We did not identify any applications evaluated with humans in the loop"": there has been some development in providing human-in-the-loop algorithms to identify better recourse options:
  - [1] De Toni, Giovanni, et al. ""Personalized Algorithmic Recourse with Preference Elicitation."" Transactions on Machine Learning Research, https://openreview.net/forum?id=8sg2I9zXgO
- Recommendation 4, ""Accounting for emergent effects"": there has been some research regarding providing recourse to multiple individuals, where they are competing for a limited pool of resources, looking also at the fairness of these systems:
  - [2] Fonseca, João, et al. ""Setting the right expectations: Algorithmic recourse over time."" Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization. https://dl.acm.org/doi/pdf/10.1145/3617694.3623251
  - [3] Bell, Andrew, et al. ""Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity."" arXiv preprint arXiv:2401.16088, https://arxiv.org/pdf/2401.16088

I also point the authors to some new papers considering human-in-the-loop interfaces for recourse (Recommendation 1, Section 5.1):
- [4] Esfahani, Seyedehdelaram, et al. ""Preference Elicitation in Interactive and User-centered Algorithmic Recourse: an Initial Exploration."" Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization. https://dl.acm.org/doi/pdf/10.1145/3627043.3659556
- [5] Koh, Seunghun, Byung Hyung Kim, and Sungho Jo. ""Understanding the User Perception and Experience of Interactive Algorithmic Recourse Customization."" ACM Transactions on Computer-Human Interaction. https://dl.acm.org/doi/pdf/10.1145/3674503

Limitations:
The authors have highlighted the limitations of their work in Section 5.2.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a survey regarding algorithmic recourse scientific literature. In their work, the authors analyze what types of contributions do the authors choose to make to the AR research, what are the criteria covered in the authors’ definitions of AR, what are the criteria covered in the authors’ definitions of actionability, the roles of end users, what types of real-world considerations motivate existing research, what types of real-world considerations are seen as challenges for future work, what types of group-level dynamics are addressed in the existing research, what are the approaches to the realistic evaluation of proposed methods, and what are the open source and documentation practices in AR research. They conclude their paper by providing recommendations on how to make future algorithmic recourse solutions better suited for real-world needs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- the authors invested much effort into explaining the procedure followed to ensure a high-quality survey
- the authors very synthetically review scientific literature related to algorithmic recourse and provide a great insight into the field within a few pages
- the authors reviewed a vast amount of literature (165 references!)

Weaknesses:
We did not identify important weaknesses. While an extensive survey could be created following this one, providing in-depth details for each of the sections, we understand this cannot be done within the constraints established for this venue.

Limitations:
The authors have adequately acknowledged the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper is a review of algorithmic recourse (AR) literature. The authors deploy a systematic framework to investigate research trends in algorithmic recourse and evaluate their incorporation of practical concerns like societal and institutional considerations of AR, or lack thereof. The review finds that current research is  focused on methods and technical considerations. The authors encourage researchers in AR to consider real-world implications of their work and conduct user studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Paper is well-organized and easy to follow
- Section 2 provides solid background information on algorithmic recourse
- The questions in Section 4 are pertinent

Weaknesses:
While I agree with the points being made and appreciate the findings in the paper, I question their novelty. As mentioned in Section 4.6, there are papers (albeit in smaller numbers than we would want) that already provide real-world examples and attempt to discuss ethics within recourse. Previous work by [Doshi-Velez and Kim](https://arxiv.org/pdf/1702.08608), [Vaughan and Wallach](https://www.jennwv.com/papers/intel-chapter.pdf) have called for more user studies in interpretable ML, which resulted in studies like [Sixt et al.](https://openreview.net/pdf?id=v6s3HVjPerv). Considering that many researchers working on recourse is also in the field of ML interpretability, I am not sure if the paper's results and call for more user studies are very substantive.

Spending more time differentiating this work from other related works (especially other literature reviews like [70]) rather than listing their contributions in Section 2.2 may be helpful in making your case.

A more thorough discussion and evaluation of results (attempted in Section 5) may resolve some of these questions. As it stands, there is a disconnect between Section 4 and 5. The message of the first part of Section 5 (lines 318-350) is not clear. The second paragraph of the section does not seem to be a discussion of survey results but rather an argument the authors are trying to make (without using the results). The paper would benefit from expanding on the contents in lines 352 to 356, pointing to results in Section 4 and bridging them to the suggestions in Section 5.1.

The paper reads more like a position paper, trying to convince researchers in algorithmic recourse to not only focus on technical methods (which, again, I agree with). But I am not sure if a literature review or a position paper suitable for NeurIPS, considering its call for papers, does not seem to suggest so.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper provides a review of previous works that study ""algorithmic recourse"", i.e. conceptual and practical approaches for giving people actionable recommendations to change how they are impacted by algorithmic systems. This literature is deeply connected with counterfactual explanations and understanding models through small changes to test data, answering questions such as ""how would the model M produce a different output if changed attribute x about myself"". The authors review 127 archival publications and answer 9 questions about how these works frame and study algorithmic recourse.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
In terms of originality, quality, and clarity:
- While the primary novel contributions of this draft are to highlight themes in previous work, the overall level of novelty is reasonable. Some concerns here, see below.
- Quality: the ""Systematized review"" methods are described such that they are replicable and seem justified. I don't expect readers to have major issues with inclusion criteria of papers, or any of the analyses presented.
- Clarity: Writing is clear throughout.

In terms of significance, the paper could have impact on future work studying algorithmic recourse, and might motivate NeurIPS community members (including those in companies or working with governments) to support recourse methods. This would be a large positive impact.  

 This kind of review can certainly be useful to researchers trying to incorporate ideas or findings from recourse-related research. The calls to engage with HCI and systems-level thinking are reasonable (though, some of the broader discussion/motivation in the paper is more convincing on this front than any of the empirical results from the 127 recourse-related papers). If a version of this paper were able to unify definitions in the recourse space, this could be powerful (though further expansion of Section 2.2 might be necessary: the paper does note that reference [70] is highly similar -- the current draft was a bit vague in comparing these and clarifying the added contribution here.).

A few other notes: There are 9 overall sub-research questions answered. Overall, these results seem likely to be useful to researchers entering the algorithmic recourse field (though, see below, some of these felt very general and not domain-specific in the current draft). The paper does fit into the ""Social and economic aspects of machine learning"" category listed in the CFP this year.

Weaknesses:
Overall, I do think the current draft may not achieve the full impact that a future revision could provide.

The current discussion section feels like it largely echoes other calls in the community to apply systems thinking to ethical/responsible/pro-social AI/ML initiatives, and while each of the suggestions has some connection to one of the analyses, the current draft is not totally clear about the extent to which these recommendations stem from the findings vs. are motivated by first principles. The paper is overall very critical of the AR field, i.e. ""Why hasn't this field engagement with any real world deployments"". However, it's also not entirely clear in the current draft how any of the general recommendations would be applied in specific AR domains.

One aspect of the paper that I think would have been most directly helpful to the NeurIPS audience in particular would be to take a stance on how recourse should be defined -- is the definition on line 62 ""endorsed"" by the paper? Is the ""imagine a counterfactual input x*"" an advisable approach to take for future work. Does this review support the definition, highlight core definitional or epistemic issues, etc.

Ultimately, given the intended goals of this draft, it seems success and impact (on top of the core empirical contribution provided by writing a systematized review) here are dependent on the ability for the provided recommendations to shape future research positively. While the five recommendations here could have a some positive impact, taking a stronger stance on the core definitions and framing of recourse ""tasks"" could have an even larger impact.

Limitations:
- No major concerns regarding unmentioned social impacts.
- Regarding the limitations of systematized literature review, the current draft discusses these reasonably.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper provides a comprehensive review of the algorithmic recourse research literature, concentrating on understanding the recourse research ""in the wild"", by focusing on the practical application of these techniques in real-world scenarios. The authors then provide some suggestions to practitioners to push future research to better practical applications.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written and well-structured. Considerable effort has been put into this work to provide a comprehensive review of the area, highlighting the need for a more down-to-earth approach when considering recourse. The data collection and analysis are well-motivated and described sufficiently (Section 3 and Section 4). The recommendations in Section 5.1 are on point and all true, and they highlight issues that everyone in the community is aware of but that are largely ignored.

Weaknesses:
I feel NeurIPS is not the right venue for this kind of contribution, since this paper does not provide the level of technical novelty required by the conference. Being a review, I think it does not fit the requirement of ""new and original research"" given by the Call of Papers. I suggest the authors not be discouraged, since I think the contribution is still valuable for the community. Potential other venues I believe are more in line with the scope of this work could be the following (the order is random):
- IJCAI Survey Track (https://ijcai24.org/call-for-papers-survey-track/)
- ACM FAccT (https://facctconference.org/)
- AAAI/ACM AIES (https://www.aies-conference.com/2024/)
- ICML Position Papers Track (https://icml.cc/Conferences/2024/CallForPositionPapers)
- ACM Computing Surveys (https://dl.acm.org/journal/csur) 
- TMLR (https://jmlr.org/tmlr/)

Lastly, I would like to point out some potential additional papers on algorithmic recourse which could complement some remarks made by the authors:
- Line 182 ""We did not identify any applications evaluated with humans in the loop"": there has been some development in providing human-in-the-loop algorithms to identify better recourse options:
  - [1] De Toni, Giovanni, et al. ""Personalized Algorithmic Recourse with Preference Elicitation."" Transactions on Machine Learning Research, https://openreview.net/forum?id=8sg2I9zXgO
- Recommendation 4, ""Accounting for emergent effects"": there has been some research regarding providing recourse to multiple individuals, where they are competing for a limited pool of resources, looking also at the fairness of these systems:
  - [2] Fonseca, João, et al. ""Setting the right expectations: Algorithmic recourse over time."" Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization. https://dl.acm.org/doi/pdf/10.1145/3617694.3623251
  - [3] Bell, Andrew, et al. ""Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity."" arXiv preprint arXiv:2401.16088, https://arxiv.org/pdf/2401.16088

I also point the authors to some new papers considering human-in-the-loop interfaces for recourse (Recommendation 1, Section 5.1):
- [4] Esfahani, Seyedehdelaram, et al. ""Preference Elicitation in Interactive and User-centered Algorithmic Recourse: an Initial Exploration."" Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization. https://dl.acm.org/doi/pdf/10.1145/3627043.3659556
- [5] Koh, Seunghun, Byung Hyung Kim, and Sungho Jo. ""Understanding the User Perception and Experience of Interactive Algorithmic Recourse Customization."" ACM Transactions on Computer-Human Interaction. https://dl.acm.org/doi/pdf/10.1145/3674503

Limitations:
The authors have highlighted the limitations of their work in Section 5.2.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a survey regarding algorithmic recourse scientific literature. In their work, the authors analyze what types of contributions do the authors choose to make to the AR research, what are the criteria covered in the authors’ definitions of AR, what are the criteria covered in the authors’ definitions of actionability, the roles of end users, what types of real-world considerations motivate existing research, what types of real-world considerations are seen as challenges for future work, what types of group-level dynamics are addressed in the existing research, what are the approaches to the realistic evaluation of proposed methods, and what are the open source and documentation practices in AR research. They conclude their paper by providing recommendations on how to make future algorithmic recourse solutions better suited for real-world needs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- the authors invested much effort into explaining the procedure followed to ensure a high-quality survey
- the authors very synthetically review scientific literature related to algorithmic recourse and provide a great insight into the field within a few pages
- the authors reviewed a vast amount of literature (165 references!)

Weaknesses:
We did not identify important weaknesses. While an extensive survey could be created following this one, providing in-depth details for each of the sections, we understand this cannot be done within the constraints established for this venue.

Limitations:
The authors have adequately acknowledged the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oAmHzy8btj;"REVIEW 
Summary:
This paper considers the graph matching problem, where the goal is to produce a mapping between vertices of multiple graphs which maximizes similarities among them. The authors study graph matching from a theoretical perspective, in which one observes multiple (appropriately correlated) Erdös-Rényi (ER) graphs that have ground-truth latent mappings between them. The authors' goal is to characterize the information-theoretic threshold for exactly recovering the latent mappings between all of the observed ER graphs. Prior work has settled the information-theoretic thresholds for 2 correlated ER graphs, and this paper settles it for more than 2 ER graphs. 

To determine the information-theoretic threshold for exact graph matching, the authors establish matching achievability and converse results. The converse is based on a simple reduction to a graph matching problem with two ER graphs, combined with known results on impossibility results for exactly matching two ER graphs. For the achievability results, two algorithms are discussed. The first is the MLE, which is optimal for exact graph matching. The authors show that it has a clean, easy-to-understand form: the MLE outputs vertex mappings which maximize the number of edges in the corresponding union graph. However, the authors do not directly analyze this algorithm due to technical complexities. Instead, they propose an algorithm which involves two phases. (1) For each pair of graphs, a partial, fully-correct mapping is computed via the $k$-core estimator, and (2) unmatched vertices are matched through a ""transitive closure"" procedure. This algorithm provably outputs the full, correct set of vertex mappings in the parameter region that complements the converse. 

Finally, a few numerical experiments are presented, showing that the transitive closure procedure can be combined with known computationally efficient algorithms for pairwise graph matching to derive algorithms for matching multiple graphs in a principled manner.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Almost all the existing theoretical work on graph matching concerns two graphs, except for some trivial results (to the best of my knowledge). The extension of the theoretical framework to multiple graphs is a natural and important follow-up, and may inspire several future works as well. 

While the algorithms and analysis are largely adapted from prior work (e.g., the $k$-core estimator), a key novelty is the transitive closure step, which provides a principled (and optimal!) bridge between pairwise graph matching and $m$-ary graph matching. As the authors highlight, this step can be used to extend practical algorithms for pairwise matching to the $m$-ary case in a black-box manner. I imagine that this technique could be useful in practice. 

Additionally, the paper is well-written.

Weaknesses:
To me, the main weakness of the paper is in the discussion of transitive closure's implications. The authors make a striking observation that one can use their transitive closure technique (at least heuristically) to generalize pairwise graph matching to $m$-ary graph matching. However, several details are lacking in the simulations section. For instance, what are the graph parameters ($n$ and $p$)? What is the error rate before and after the transitive closure boosting? How do the results shown compare to the accuracy of Algorithm 2? (Even though the $k$-core matching is not efficient to compute, the result of the matching procedure is a function of the ground-truth permutations, so I believe the algorithm's accuracy can be simulated efficiently). 

There are a couple other minor weaknesses. One is that Algorithm 2 is computationally inefficient. However, making such an algorithm efficient is likely a challenging research question itself, and is appropriate for future work. Another weakness is that there is no nice figure to visualize Algorithm 2. I feel that the reader's understanding could be greatly improved if one could create a representative figure for the transitive closure boosting.

Limitations:
Limitations have been largely discussed. The authors could expand upon implications of graph matching to protecting / breaking privacy in anonymized social networks.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the information theoretic limits for matching
multiple correlated random graphs. Based on a correlated Erdos-Renyi
random graph model, the authors provide both lower bound and achievable
bound for the condition to correctly match all nodes with high
probability. These bounds match each other. A highly interesting insight
is that, even when exactly matching two graphs is not possible, the
proposed algorithm can leverage more than two graphs to produce exact
matching among all the graphs. The achievable algorithm exploits the
transitivity among partial matchings through $k$-cores, which is also
quite interesting.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The novelty of the paper is high in dealing with graph matching among
multiple correlated graphs. 

2. The necessary and sufficient conditions for exact matching meet each
other. 

3. The proposed algorithm can exploit transitivity to match all graphs,
even when any two graphs alone cannot be exactly matched. This is a very
insightful result.

Weaknesses:
1. The proposed algorithms do incur high complexity.

Limitations:
Limitations are discussed in Section 5.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This theoretical paper gives tight conditions for exact graph matching with multiple correlated random graphs. This problem has been extensively studied recently for the case of 2 graphs, and it is shown here that with more than 2 graphs, there is a regime where pairwise alignment is not possible, but with the information provided by all graphs, it is possible to align all of them. This is a nice theoretical result.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper studies a natural extension of a well-studied problem from 2 graphs to more graphs and shows a surprising effect: making partial pairwise matching is sufficient to get the exact recovery. The proof outlines give the main insights into the technical proof.

Weaknesses:
The resulting algorithm is not practical as it does not run in polynomial time (as it is mentioned by the authors).

Limitations:
The authors are very clear with the limitations of their work in section 5.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to find out alignments between G_1 and G_2,....G_m, under the assumptions that they all are essentially sampled from ER graph distribution. The paper presents one impossibility result (or necessary condition to estimate such alignment)  and two sufficiency results to solve the underlying problem.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper tackles an interesting problem and it is written clearly.

Weaknesses:
(1) I am not too confident that the paper is appropriate for neurips audience. I think the paper suits better to a conference like ISIT or such.  The paper has barely any learning component and the practical utility is not very clear.
Also, the primary area assigned by the authors ""Probabilistic methods (for example: variational inference, Gaussian processes)"" is probably not correct.

(2) The paper only tackles a very simple graph model (ER graph model). While I understand that theoretical analysis for complex graph model is difficult, I would recommend the authors should discuss that in comprehensive manner. To elaborate concretely,
suppose,  G_1, G' _2...,G' _m are *not* generated from an ER model. But G_2,...G_m are generated using an ER like model with constant edge deletion probability $s$. In such case, can one characterize the necessary and sufficient condition.

Note that, the area is not too new in the literature. There has been work already in this line of research [CK17,WXY22 in the paper]. Although I will not say this work is an extension but the theoretical contribution given the existing works is not very interesting (m=2 to an arbitrary m for example). 

(3) There is no experimental analysis. I would have increased my rating if the authors have done a thorough study on implications (including limitations) of their work on graphs from other models. For example, if we apply the same algorithm in other graph models, how would it perform. Since the line of work is not new, I would not say the theoretical results have strong enough impact to ignore the poor experiments.

Limitations:
Restrictive graph model; poor experiments and incremental contribution.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the graph matching problem, where the goal is to produce a mapping between vertices of multiple graphs which maximizes similarities among them. The authors study graph matching from a theoretical perspective, in which one observes multiple (appropriately correlated) Erdös-Rényi (ER) graphs that have ground-truth latent mappings between them. The authors' goal is to characterize the information-theoretic threshold for exactly recovering the latent mappings between all of the observed ER graphs. Prior work has settled the information-theoretic thresholds for 2 correlated ER graphs, and this paper settles it for more than 2 ER graphs. 

To determine the information-theoretic threshold for exact graph matching, the authors establish matching achievability and converse results. The converse is based on a simple reduction to a graph matching problem with two ER graphs, combined with known results on impossibility results for exactly matching two ER graphs. For the achievability results, two algorithms are discussed. The first is the MLE, which is optimal for exact graph matching. The authors show that it has a clean, easy-to-understand form: the MLE outputs vertex mappings which maximize the number of edges in the corresponding union graph. However, the authors do not directly analyze this algorithm due to technical complexities. Instead, they propose an algorithm which involves two phases. (1) For each pair of graphs, a partial, fully-correct mapping is computed via the $k$-core estimator, and (2) unmatched vertices are matched through a ""transitive closure"" procedure. This algorithm provably outputs the full, correct set of vertex mappings in the parameter region that complements the converse. 

Finally, a few numerical experiments are presented, showing that the transitive closure procedure can be combined with known computationally efficient algorithms for pairwise graph matching to derive algorithms for matching multiple graphs in a principled manner.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Almost all the existing theoretical work on graph matching concerns two graphs, except for some trivial results (to the best of my knowledge). The extension of the theoretical framework to multiple graphs is a natural and important follow-up, and may inspire several future works as well. 

While the algorithms and analysis are largely adapted from prior work (e.g., the $k$-core estimator), a key novelty is the transitive closure step, which provides a principled (and optimal!) bridge between pairwise graph matching and $m$-ary graph matching. As the authors highlight, this step can be used to extend practical algorithms for pairwise matching to the $m$-ary case in a black-box manner. I imagine that this technique could be useful in practice. 

Additionally, the paper is well-written.

Weaknesses:
To me, the main weakness of the paper is in the discussion of transitive closure's implications. The authors make a striking observation that one can use their transitive closure technique (at least heuristically) to generalize pairwise graph matching to $m$-ary graph matching. However, several details are lacking in the simulations section. For instance, what are the graph parameters ($n$ and $p$)? What is the error rate before and after the transitive closure boosting? How do the results shown compare to the accuracy of Algorithm 2? (Even though the $k$-core matching is not efficient to compute, the result of the matching procedure is a function of the ground-truth permutations, so I believe the algorithm's accuracy can be simulated efficiently). 

There are a couple other minor weaknesses. One is that Algorithm 2 is computationally inefficient. However, making such an algorithm efficient is likely a challenging research question itself, and is appropriate for future work. Another weakness is that there is no nice figure to visualize Algorithm 2. I feel that the reader's understanding could be greatly improved if one could create a representative figure for the transitive closure boosting.

Limitations:
Limitations have been largely discussed. The authors could expand upon implications of graph matching to protecting / breaking privacy in anonymized social networks.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the information theoretic limits for matching
multiple correlated random graphs. Based on a correlated Erdos-Renyi
random graph model, the authors provide both lower bound and achievable
bound for the condition to correctly match all nodes with high
probability. These bounds match each other. A highly interesting insight
is that, even when exactly matching two graphs is not possible, the
proposed algorithm can leverage more than two graphs to produce exact
matching among all the graphs. The achievable algorithm exploits the
transitivity among partial matchings through $k$-cores, which is also
quite interesting.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The novelty of the paper is high in dealing with graph matching among
multiple correlated graphs. 

2. The necessary and sufficient conditions for exact matching meet each
other. 

3. The proposed algorithm can exploit transitivity to match all graphs,
even when any two graphs alone cannot be exactly matched. This is a very
insightful result.

Weaknesses:
1. The proposed algorithms do incur high complexity.

Limitations:
Limitations are discussed in Section 5.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This theoretical paper gives tight conditions for exact graph matching with multiple correlated random graphs. This problem has been extensively studied recently for the case of 2 graphs, and it is shown here that with more than 2 graphs, there is a regime where pairwise alignment is not possible, but with the information provided by all graphs, it is possible to align all of them. This is a nice theoretical result.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper studies a natural extension of a well-studied problem from 2 graphs to more graphs and shows a surprising effect: making partial pairwise matching is sufficient to get the exact recovery. The proof outlines give the main insights into the technical proof.

Weaknesses:
The resulting algorithm is not practical as it does not run in polynomial time (as it is mentioned by the authors).

Limitations:
The authors are very clear with the limitations of their work in section 5.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to find out alignments between G_1 and G_2,....G_m, under the assumptions that they all are essentially sampled from ER graph distribution. The paper presents one impossibility result (or necessary condition to estimate such alignment)  and two sufficiency results to solve the underlying problem.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper tackles an interesting problem and it is written clearly.

Weaknesses:
(1) I am not too confident that the paper is appropriate for neurips audience. I think the paper suits better to a conference like ISIT or such.  The paper has barely any learning component and the practical utility is not very clear.
Also, the primary area assigned by the authors ""Probabilistic methods (for example: variational inference, Gaussian processes)"" is probably not correct.

(2) The paper only tackles a very simple graph model (ER graph model). While I understand that theoretical analysis for complex graph model is difficult, I would recommend the authors should discuss that in comprehensive manner. To elaborate concretely,
suppose,  G_1, G' _2...,G' _m are *not* generated from an ER model. But G_2,...G_m are generated using an ER like model with constant edge deletion probability $s$. In such case, can one characterize the necessary and sufficient condition.

Note that, the area is not too new in the literature. There has been work already in this line of research [CK17,WXY22 in the paper]. Although I will not say this work is an extension but the theoretical contribution given the existing works is not very interesting (m=2 to an arbitrary m for example). 

(3) There is no experimental analysis. I would have increased my rating if the authors have done a thorough study on implications (including limitations) of their work on graphs from other models. For example, if we apply the same algorithm in other graph models, how would it perform. Since the line of work is not new, I would not say the theoretical results have strong enough impact to ignore the poor experiments.

Limitations:
Restrictive graph model; poor experiments and incremental contribution.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nu2Sqrsnr7;"REVIEW 
Summary:
The paper attempts to train PINNs which solves acoustic wave equations. They do so by using hard-constrained PINNs which can enforce IC and BCs, and propose a collocation point sampling method (DAFS) based on the amplitude of the solution at different regions.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper considers an interesting problem in acoustics and attempt to apply the techniques from PINNs to solve them.

Weaknesses:
The paper itself feels less coherent, and seems like just an application of many existing PINN training techniques (e.g., hard constraint PINNs, collocation point sampling) into solving a certain problem, rather than providing a novel method or a coherent framework into solving a domain-specific problem.

The experimental section feels incomplete. Different point selection algorithms have not been extensively compared with, e.g., from that in Wu et. al. (2023). Furthermore, it would be interesting to see how the method can scale to more realistic acoustic problems (i.e., outside of 1D settings).

The paper itself also seems incomplete. The Appendix and the NeurIPS checklist are partially filled and have half-finished sentences.

The labels within the graphs can also be enlarged slightly to make them more readable.

Limitations:
The authors have provided limitations with selection of \tau.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript treats the one dimensional wave equation with a PINN approach and discusses the imposition of boundary and initial conditions directly into the network, as common practice in PINNs. The authors then propose a quadrature scheme based on a coarse finite difference discretization of the wave equation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The imposition of the time derivative seems to be a novel construction. Furthermore, the construction seems not to be limited to the wave equation.

Weaknesses:
The main weakness of the manuscript is the focus on the very special and simple toy problem of the one dimensional wave equation. Solving the one-dimensional wave equation with PINNs is only of academic interest and insights obtained from it for the training of PINNs might not generalize. More specifically:
- The exact imposition of the time derivative should also work for general time dependent equations. The authors should comment on this.
- The sampling strategy employing a finite difference simulation to determine regions of high sampling density is not a generalizable approach. If a finite difference solver for the equation at hand is available, a PINN solver is typically not required.
- The authors determine an optimal function $\tau$ via considering six concrete examples. There is no guarantee that this approach will generalize to different equation types and is therefore of limited practical use.
- The authors might want to discuss the theoretical literature that proves the theoretical advantage of exactly imposed boundary conditions [1, 2, 3] and more elaborate constructions of distance functions.

[1] https://proceedings.mlr.press/v190/muller22b/muller22b.pdf

[2] https://arxiv.org/abs/2311.00529

[3] https://www.sciencedirect.com/science/article/abs/pii/S0045782521006186

Limitations:
The scope of the paper is too narrow.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores to solve the acoustic wave equation in the context of PINNs. Hard boundary and initial conditions are enforced by employing continuous functions within the PINN ansatz to ensure that these conditions are satisfied. A Dynamic Amplitude-Focused Sampling (DAFS) method is introduced to improve the efficiency of hard-constraint PINNs under a fixed number of sampling points.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Propose a general hard constraint imposition formula which correctly imposes all boundary conditions and initial conditions as required.

Weaknesses:
1. Only the wave equation is discussed.
2. The proposed Dynamic Amplitude-Focused Sampling (DAFS) method is trivial.
3. There are no comparisons with other methods in the experiments.
4. In the experiments, the relative errors between exact solutions and predictions are not given.
5. In the context of PINNs, it is better to give explicitly the formulation of training loss. Training details are also lacking. 
6. Instead of tuning \tau (t) manually, it is better to train \tilde{u}(x,t) and \tau (t) simutanuously.
7. Many typos and grammar errors, such as ""both and \alpha"" in line 149, ""x \in {\partial \Omega}_i"" in line 125, ""computational"" in line 46.
8. The quality of Fig.7 should be improved.

Limitations:
Only the wave equation is discussed. There are no comparisons with other methods in the experiments.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper improves the training efficiency of original physics-informed neural networks to solve the 1D wave equation threefold: first by extending ansatz to also take the first derivative into account, second by a sampling method that focuses on high-amplitude regions, and third by a framework for domain decomposition.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ The related work is well presented.
+ The evaluation of the six candidate functions for \tau in section 4.2 provides interesting insights. The authors explore an advanced selection method for \tau based on the task at hand which might be an interesting research direction.

Weaknesses:
[Originality] While considering the first derivative for the ansatz is a good addition, the contribution is only minor. 
Sampling more collocation points in the regions that might be more difficult to solve is a practical approach however the comparison and distinction to other sampling methods is missing.
Lastly if I understand the domain decomposition framework correctly, the contribution is to wrap the entire training into a loop and, based on the training process's results, increase or decrease the subdomain size. 

Evaluation results are only provided for the 1D wave equation. Further results for other differential equations are necessary to demonstrate the benefits of the proposed method.

[Clarity] 
The framework for domain decomposition is not presented clearly. While the flow chart in Figure 7 provides an overview of the method additional textual explanations in Section 4.4 are needed.
There were few to no remarks about the training regime (#training points, optimizer, learning rate…, etc.), making it more difficult to reproduce results.
Minor remarks:
-            N_pde is not introduced. It is probably the number of collocation points?
-            Most of the Figures (e.g. Fig. 1, Fig 6.) are hard to read.
-            Line 46: (…) optimal size of the computational [domain?] given (…)
-            Line 149: Both [N_pde?] and alpha (…) 
-            Line 178: (…) In general, (...) performs better in general

Limitations:
While the authors clearly state that they are interested in the 1D wave equation it would have been interesting to see their proposed methods applied to the 2D wave equation of any other differential equations what are typically used in PINN benchmarks.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper attempts to train PINNs which solves acoustic wave equations. They do so by using hard-constrained PINNs which can enforce IC and BCs, and propose a collocation point sampling method (DAFS) based on the amplitude of the solution at different regions.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper considers an interesting problem in acoustics and attempt to apply the techniques from PINNs to solve them.

Weaknesses:
The paper itself feels less coherent, and seems like just an application of many existing PINN training techniques (e.g., hard constraint PINNs, collocation point sampling) into solving a certain problem, rather than providing a novel method or a coherent framework into solving a domain-specific problem.

The experimental section feels incomplete. Different point selection algorithms have not been extensively compared with, e.g., from that in Wu et. al. (2023). Furthermore, it would be interesting to see how the method can scale to more realistic acoustic problems (i.e., outside of 1D settings).

The paper itself also seems incomplete. The Appendix and the NeurIPS checklist are partially filled and have half-finished sentences.

The labels within the graphs can also be enlarged slightly to make them more readable.

Limitations:
The authors have provided limitations with selection of \tau.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript treats the one dimensional wave equation with a PINN approach and discusses the imposition of boundary and initial conditions directly into the network, as common practice in PINNs. The authors then propose a quadrature scheme based on a coarse finite difference discretization of the wave equation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The imposition of the time derivative seems to be a novel construction. Furthermore, the construction seems not to be limited to the wave equation.

Weaknesses:
The main weakness of the manuscript is the focus on the very special and simple toy problem of the one dimensional wave equation. Solving the one-dimensional wave equation with PINNs is only of academic interest and insights obtained from it for the training of PINNs might not generalize. More specifically:
- The exact imposition of the time derivative should also work for general time dependent equations. The authors should comment on this.
- The sampling strategy employing a finite difference simulation to determine regions of high sampling density is not a generalizable approach. If a finite difference solver for the equation at hand is available, a PINN solver is typically not required.
- The authors determine an optimal function $\tau$ via considering six concrete examples. There is no guarantee that this approach will generalize to different equation types and is therefore of limited practical use.
- The authors might want to discuss the theoretical literature that proves the theoretical advantage of exactly imposed boundary conditions [1, 2, 3] and more elaborate constructions of distance functions.

[1] https://proceedings.mlr.press/v190/muller22b/muller22b.pdf

[2] https://arxiv.org/abs/2311.00529

[3] https://www.sciencedirect.com/science/article/abs/pii/S0045782521006186

Limitations:
The scope of the paper is too narrow.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores to solve the acoustic wave equation in the context of PINNs. Hard boundary and initial conditions are enforced by employing continuous functions within the PINN ansatz to ensure that these conditions are satisfied. A Dynamic Amplitude-Focused Sampling (DAFS) method is introduced to improve the efficiency of hard-constraint PINNs under a fixed number of sampling points.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Propose a general hard constraint imposition formula which correctly imposes all boundary conditions and initial conditions as required.

Weaknesses:
1. Only the wave equation is discussed.
2. The proposed Dynamic Amplitude-Focused Sampling (DAFS) method is trivial.
3. There are no comparisons with other methods in the experiments.
4. In the experiments, the relative errors between exact solutions and predictions are not given.
5. In the context of PINNs, it is better to give explicitly the formulation of training loss. Training details are also lacking. 
6. Instead of tuning \tau (t) manually, it is better to train \tilde{u}(x,t) and \tau (t) simutanuously.
7. Many typos and grammar errors, such as ""both and \alpha"" in line 149, ""x \in {\partial \Omega}_i"" in line 125, ""computational"" in line 46.
8. The quality of Fig.7 should be improved.

Limitations:
Only the wave equation is discussed. There are no comparisons with other methods in the experiments.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper improves the training efficiency of original physics-informed neural networks to solve the 1D wave equation threefold: first by extending ansatz to also take the first derivative into account, second by a sampling method that focuses on high-amplitude regions, and third by a framework for domain decomposition.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ The related work is well presented.
+ The evaluation of the six candidate functions for \tau in section 4.2 provides interesting insights. The authors explore an advanced selection method for \tau based on the task at hand which might be an interesting research direction.

Weaknesses:
[Originality] While considering the first derivative for the ansatz is a good addition, the contribution is only minor. 
Sampling more collocation points in the regions that might be more difficult to solve is a practical approach however the comparison and distinction to other sampling methods is missing.
Lastly if I understand the domain decomposition framework correctly, the contribution is to wrap the entire training into a loop and, based on the training process's results, increase or decrease the subdomain size. 

Evaluation results are only provided for the 1D wave equation. Further results for other differential equations are necessary to demonstrate the benefits of the proposed method.

[Clarity] 
The framework for domain decomposition is not presented clearly. While the flow chart in Figure 7 provides an overview of the method additional textual explanations in Section 4.4 are needed.
There were few to no remarks about the training regime (#training points, optimizer, learning rate…, etc.), making it more difficult to reproduce results.
Minor remarks:
-            N_pde is not introduced. It is probably the number of collocation points?
-            Most of the Figures (e.g. Fig. 1, Fig 6.) are hard to read.
-            Line 46: (…) optimal size of the computational [domain?] given (…)
-            Line 149: Both [N_pde?] and alpha (…) 
-            Line 178: (…) In general, (...) performs better in general

Limitations:
While the authors clearly state that they are interested in the 1D wave equation it would have been interesting to see their proposed methods applied to the 2D wave equation of any other differential equations what are typically used in PINN benchmarks.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nfC1OA6NeE;"REVIEW 
Summary:
This work derives SDEs for adaptive gradient methods and study the role of gradient noise. The analysis starts from theoretically driving the SDE for SignSGD and highlight its significant difference from SGD. The work further generalize the SDE analysis for AdamW and RMSpropW, two popular adaptive optimizers with decoupled weight decay and reveal key properties of weight decay. Finally, the work integrates the derived SDEs with Euler-Maruyama to confirm that the SDEs faithfully track their respective optimizers with various modern neural networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-The theoretical results are novel. To my knowledge, this is a first SDE analysis for SignSGD with quantitatively accurate descriptions.

-The theoretical analysis reports some novel properties in terms of gradient noise and convergence. These properties are interesting.

-The proofs seem complete and reasonable.

-A useful theory should be quantitatively verifiable. This work definitely make it. The experiments that SDEs fit the empirical results with various optimizers and models are informative and impressive.

Weaknesses:
-It seems that the reported theoretical results and insights cannot directly lead to some theory-inspired and improved methods. This raise a question on the significance of this work.

-While this work did literature review, some important references are still missing, such as [1] on analyzing Adam using SDEs. As weight decay plays a key role in the results, it may be helpful to review recent papers analyzing novel or overlooked properties of weight decay.


Reference:

[1] Xie, Z., Wang, X., Zhang, H., Sato, I., & Sugiyama, M. (2022, June). Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum. In International conference on machine learning (pp. 24430-24459). PMLR.

Limitations:
This work discussed the limitations in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors derive SDE for signSGD and Adam(W). The experiments show that the algorithm will converge toward the limit of the theorem indicates.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors propose ""accurate"" SDEs for algorithms Sign-SGD and Adam(W).

Weaknesses:
1. In Remark after Lemma 3.6, the authors claim that Sign-SGD is (almost) linear in $\sigma_{max}$. However, with $\Delta$ either in Phase 2 or Phase 3, there should be $\sigma_{max}^2$ in the final bound.

2. All the stationarity holds when Hessian is the same from $X_0$ to $X_t$ and convergence holds for strongly convex. However, the hessian changes a lot during network training.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper derives SDEs for SignSGD, RMSprop, and Adam.
The analysis offers insights into the convergence speed, stationary distribution, and robustness to heavy-tail noise of adaptive methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The derived SDE for SignSGD exhibits three different phases of the dynamics.

- The analysis reveals the difference between SignSGD and SGD in terms of the asymptotic expected loss, the robustness of noise variance, etc.

- The analysis of AdamW provides insights into the different roles of noise, curvature, and weight decay.

Weaknesses:
Refer to Questions and Limitations.

Limitations:
The SDE for AdamW is limited to quadratic functions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work derives SDEs for adaptive gradient methods and study the role of gradient noise. The analysis starts from theoretically driving the SDE for SignSGD and highlight its significant difference from SGD. The work further generalize the SDE analysis for AdamW and RMSpropW, two popular adaptive optimizers with decoupled weight decay and reveal key properties of weight decay. Finally, the work integrates the derived SDEs with Euler-Maruyama to confirm that the SDEs faithfully track their respective optimizers with various modern neural networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-The theoretical results are novel. To my knowledge, this is a first SDE analysis for SignSGD with quantitatively accurate descriptions.

-The theoretical analysis reports some novel properties in terms of gradient noise and convergence. These properties are interesting.

-The proofs seem complete and reasonable.

-A useful theory should be quantitatively verifiable. This work definitely make it. The experiments that SDEs fit the empirical results with various optimizers and models are informative and impressive.

Weaknesses:
-It seems that the reported theoretical results and insights cannot directly lead to some theory-inspired and improved methods. This raise a question on the significance of this work.

-While this work did literature review, some important references are still missing, such as [1] on analyzing Adam using SDEs. As weight decay plays a key role in the results, it may be helpful to review recent papers analyzing novel or overlooked properties of weight decay.


Reference:

[1] Xie, Z., Wang, X., Zhang, H., Sato, I., & Sugiyama, M. (2022, June). Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum. In International conference on machine learning (pp. 24430-24459). PMLR.

Limitations:
This work discussed the limitations in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors derive SDE for signSGD and Adam(W). The experiments show that the algorithm will converge toward the limit of the theorem indicates.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors propose ""accurate"" SDEs for algorithms Sign-SGD and Adam(W).

Weaknesses:
1. In Remark after Lemma 3.6, the authors claim that Sign-SGD is (almost) linear in $\sigma_{max}$. However, with $\Delta$ either in Phase 2 or Phase 3, there should be $\sigma_{max}^2$ in the final bound.

2. All the stationarity holds when Hessian is the same from $X_0$ to $X_t$ and convergence holds for strongly convex. However, the hessian changes a lot during network training.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper derives SDEs for SignSGD, RMSprop, and Adam.
The analysis offers insights into the convergence speed, stationary distribution, and robustness to heavy-tail noise of adaptive methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The derived SDE for SignSGD exhibits three different phases of the dynamics.

- The analysis reveals the difference between SignSGD and SGD in terms of the asymptotic expected loss, the robustness of noise variance, etc.

- The analysis of AdamW provides insights into the different roles of noise, curvature, and weight decay.

Weaknesses:
Refer to Questions and Limitations.

Limitations:
The SDE for AdamW is limited to quadratic functions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
nMFVdphOc9;"REVIEW 
Summary:
The paper proposes a method for neural network-based learning to incorporate expert knowledge in the neural network architecture by building rules and utilizing them in ""rule-based"" layers of the learned neural networks. It introduces RuleGNNs as a concrete application of the proposed method and evaluates its performance against a few other SOTA methods. Empirical studies show competitive performance of RuleGNNs compared with other alternative methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of having dynamic rule-based layers in a neural network and especially for graph neural network learning is interesting. Although some existing methods in the literature including WL labeling could be considered doing the same, the proposed method builds on top of these building blocks and extends their ideas.
- Theoretical discussions in the paper and the assumptions behind them are clear.
- Experimental results cover an adequate set of alternative methods.

Weaknesses:
- The performance of RuleGNNs is expected to heavily rely on the quality of the rules generated from additional information or domain knowledge, however, the paper solely focuses on application of such rules without adequately discussing the challenges of building quality rules and feasibility of this fundamental step in the proposed method.
- Lack of clarity around how rules in RuleGNNs look like and how they can influence learning model parameters. 
- Experimental results are not fully discussed. For example, WL-Kernel shows superior performance in three data sets and it would have been useful to provide more insights about what data set characteristics contribute to this.

Limitations:
Authors have adequately addressed the limitations of their work by listing the following limitations:
- They have only considered 1 dimensional input signals and labels.
- They have not considered graphs with multi-dimensional node features.
- Edge features are not considered.
- Computation and storage limitation for large/dense graphs.
In addition, authors have clearly discussed structure, Combinatorics, and Implementation limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel model architecture rule-based layer, which induces different parameters given different inputs. Theoretical analysis demonstrates how the proposed architecture reduces back to classical feed-forward layers, and empirical results on both synthetic and real-world data sets demonstrate that the proposed method can improve upon existing works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of rule-based layers and rule-based GNN is novel and interesting.

Weaknesses:
- The implementation in this work may need further elaboration to make the proposed method easier to understand. 
- Empirical results may need further improvements to better support the proposed method.

Limitations:
The authors discuss about possible limitations in the conclusion part, and no direct negative societal impact exists for this work from my perspective.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces rule-based (dynamic) neural network layers. The basic idea is to have a common set of parameters, i.e., weights and biases, where, depending on a certain rule, only a subset of these parameters are used in the forward pass. They show that certain fully connected and convolutional layers can be regarded as a type of static rule-based neural network layer. In the remainder of the paper, the authors introduce three dynamic rules for graph classification tasks and perform experiments on synthetic and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Overall, the concept of using rules based on expert knowledge to select different subsets of weights for various data samples or tasks seems useful and promising. This approach could offer significant benefits, such as training the same model on different tasks or on different datasets. Moreover, an approach which is able to learn on variably sized input data could be valuable on its own. The proposed rule-based layers for graph classification tasks outperform standard message-passing graph neural networks on synthetic and real-word datasets.

Weaknesses:
One of my primary concerns is that the main theoretical result of the paper, Theorem 1, is not proven. Specifically, while the authors show in Prop. 1 and Prop. 2 that fully connected layers _without bias_ and convolutional layers _without bias, padding, stride of one, and quadratic kernels_ can be expressed using their proposed (static) rule-based layer, the following paragraph leading to Theorem 1 claims this can be generalized to arbitrary convolutional layers. Although this might be straightforward to prove (and could be included in the appendix), the lack of a complete and formal proof severely undermines the soundness of the submission. If Theorem 1 is intended as a summary of Proposition 1 and Proposition 2, I suggest making this explicit by clearly stating the specific types of FC and CNN layers and renaming Theorem 1 to Corollary 1, or merging Prop. 1 and Prop. 2 into Theorem 1. Moreover, while the paper introduces some mathematical framework and formalizes existing concepts within this framework, it lacks proofs demonstrating what this framework can achieve and fails to establish connections to existing work. Given the lack of substantial theory, I think a more thorough empirical investigation could strengthen the submission. Comparisons with more expressive architectures are missing (e.g., in Table 1 there are no results reported for more expressive architectures for almost half of the datasets; for the synthetic datasets no comparison is done with more expressive architectures), making it difficult to appreciate the practical advantages of using the rule-based layers in practice. The practical relevance is limited further by the fact that the rule-based layer can only process one-dimensional features, and the higher space complexity for dense graphs.

Regarding clarity, there is considerable room for improvement. The concept of how a rule-based layer works was not fully clear to me until page 4. If my understanding is correct, we have a matrix $\mathcal{W}$ that contains all possible weights (and similarly a bias vector $\mathcal{B}$ with all possible biases). A rule restricts $\mathcal{W}$ to a subset of weights; applying rule *R* means setting some entries in $\mathcal{W}$ to zero. If my understanding is incorrect, this indicates that the writing lacks some clarity. I suggest shortening the introduction and preliminaries, which are at times verbose, and including a briefer example from Appendix A.4 earlier in the paper, or providing a clearer definition sooner. Additionally, the notation for the rule-based layer presentation is somewhat convoluted. The readability of the paper is also hindered by the inconsistent use of formal definitions and natural language. While both approaches can be fine (as long as they are precise), there is a noticeable mismatch between the rigor in the preliminaries and, for example, Section 4. Many aspects of the paper are thus unclear; please refer to the *Questions* and *Minor Remarks* for specific examples.

Overall, I think this paper presents promising ideas in a preliminary manner. As also stated by the authors, the dynamic rule-based layer seems to be reasonable for graphs, but is more difficult to devise for other structures. One approach could be to revise the paper from a graph learning perspective, and, if the authors have novel results which hold for general structures, present these results in a follow-up paper. Another exciting direction could be to use rules to create flexible machine learning models for different tasks and input data.

*Minor remarks*:

* line 33: each new information -> each new piece of information
* line 34: the essence -> a bit vague, what is the essence of dynamic NNs?
* Fig. 1 is too small and difficult to parse in general; there is also and typo in the last sentence
* line 75: dot missing after end of sentence
* line 95: concatentation -> should this be ""composition""?
* line 111: dot missing after end of sentence
* Somewhat inflationary use of ""respectively""
* line 123, 140: I would strongly advise to not use $y$ here for $x, y \in D$, as $y$ is already used to denote labels earlier
* It would be helpful to refer to equations as eq. (1) (instead of just (1))
* Could it simplify presentation if you define $\Theta$ as tuple $(\mathcal{W}, \mathcal{B})$?
* Last sentence of Prop. 1 is difficult to read
* Why do we call the learnable parameters $\Theta$ in Prop. 1 and $W^i$ in Prop. 2?
* line 190: higher dimensions -> higher dimensional
* line 202: network -> network architecture
* lines 206-214: I suggest to consider moving this to the preliminaries
* line 221: either rule function (singular) or rule functions R_W, R_B
* line 225: circle -> cycle
* Prop. 3: ""its"" -> not clear what it refers to
* line 231: If R permutation-equivariant -> language sounds off, maybe ""For permutation-equivariant R"" or ""If R is permutation-equivariant""
* line 255: typo in isomorphism
* Pattern counting rule: $d$ is never defined
* line 347: missing space

Limitations:
One of the main limitations, as the authors point out themselves, is that their proposed rule-based layer can only process one-dimensional node features, and no edge features, which impacts the practical value of their method. For more limitations, please refer to *Weaknesses* and *Questions*.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a broad framework for adding expert knowledge to Neural Networks. They formalize this by extending the learnable parameterized functions with an additional parameter consisting of the set of formal rules. In general, these rules maybe learnable as well. However, the authors focuses on these rules being given in the form of expert-knowledge. The authors then introduce the set of rule based layers. And shows that fully connected NN layers and CNN layers are special cases of the rule based layers. They introduce three rule based layers for graphs: Weisfeiler-Leman Layer, Pattern-Counting layer and Aggregation layer. The author shows that there exists a GNN with rule based layers that can distinguish any two isomorphic graphs. Finally the author introduces some examples of rule based layers for specific molecule graphs. And presents experimental results on synthetic and real-world data.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
-- The idea of adding expert knowledge to NNs and GNNs specifically is quite interesting and widely investigated.

-- The presented theory is very general and simple

Weaknesses:
-- The author has used the notion of rules rather broadly. There is no formal language (logic or matrix language) for the rules. They are just arbitrary functions. This basically means that any existing NN model, in one way or another, can be seen as a special case of Rule based NN. In my understanding, this makes the introduced framework a rather simple formalization of how expert knowledge maybe added to NNs. However, this formalization is so loose, that it does not really admit any meaningful analysis or provide any meaningful guidance to the user for adding knowledge.

-- None of the examples presented by the author are beyond what would be anyway possible by adding some simple graph features to the node features. This could be an interesting direction to investigate. But just formally stating that this is possible is not very interesting.

Limitations:
The authors have indeed touched upon most of the points I mention as weaknesses.
However, as mentioned earlier, the proposed framework is very broad and does not provide a meaningful way to proceed.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a method for neural network-based learning to incorporate expert knowledge in the neural network architecture by building rules and utilizing them in ""rule-based"" layers of the learned neural networks. It introduces RuleGNNs as a concrete application of the proposed method and evaluates its performance against a few other SOTA methods. Empirical studies show competitive performance of RuleGNNs compared with other alternative methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of having dynamic rule-based layers in a neural network and especially for graph neural network learning is interesting. Although some existing methods in the literature including WL labeling could be considered doing the same, the proposed method builds on top of these building blocks and extends their ideas.
- Theoretical discussions in the paper and the assumptions behind them are clear.
- Experimental results cover an adequate set of alternative methods.

Weaknesses:
- The performance of RuleGNNs is expected to heavily rely on the quality of the rules generated from additional information or domain knowledge, however, the paper solely focuses on application of such rules without adequately discussing the challenges of building quality rules and feasibility of this fundamental step in the proposed method.
- Lack of clarity around how rules in RuleGNNs look like and how they can influence learning model parameters. 
- Experimental results are not fully discussed. For example, WL-Kernel shows superior performance in three data sets and it would have been useful to provide more insights about what data set characteristics contribute to this.

Limitations:
Authors have adequately addressed the limitations of their work by listing the following limitations:
- They have only considered 1 dimensional input signals and labels.
- They have not considered graphs with multi-dimensional node features.
- Edge features are not considered.
- Computation and storage limitation for large/dense graphs.
In addition, authors have clearly discussed structure, Combinatorics, and Implementation limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel model architecture rule-based layer, which induces different parameters given different inputs. Theoretical analysis demonstrates how the proposed architecture reduces back to classical feed-forward layers, and empirical results on both synthetic and real-world data sets demonstrate that the proposed method can improve upon existing works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of rule-based layers and rule-based GNN is novel and interesting.

Weaknesses:
- The implementation in this work may need further elaboration to make the proposed method easier to understand. 
- Empirical results may need further improvements to better support the proposed method.

Limitations:
The authors discuss about possible limitations in the conclusion part, and no direct negative societal impact exists for this work from my perspective.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces rule-based (dynamic) neural network layers. The basic idea is to have a common set of parameters, i.e., weights and biases, where, depending on a certain rule, only a subset of these parameters are used in the forward pass. They show that certain fully connected and convolutional layers can be regarded as a type of static rule-based neural network layer. In the remainder of the paper, the authors introduce three dynamic rules for graph classification tasks and perform experiments on synthetic and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Overall, the concept of using rules based on expert knowledge to select different subsets of weights for various data samples or tasks seems useful and promising. This approach could offer significant benefits, such as training the same model on different tasks or on different datasets. Moreover, an approach which is able to learn on variably sized input data could be valuable on its own. The proposed rule-based layers for graph classification tasks outperform standard message-passing graph neural networks on synthetic and real-word datasets.

Weaknesses:
One of my primary concerns is that the main theoretical result of the paper, Theorem 1, is not proven. Specifically, while the authors show in Prop. 1 and Prop. 2 that fully connected layers _without bias_ and convolutional layers _without bias, padding, stride of one, and quadratic kernels_ can be expressed using their proposed (static) rule-based layer, the following paragraph leading to Theorem 1 claims this can be generalized to arbitrary convolutional layers. Although this might be straightforward to prove (and could be included in the appendix), the lack of a complete and formal proof severely undermines the soundness of the submission. If Theorem 1 is intended as a summary of Proposition 1 and Proposition 2, I suggest making this explicit by clearly stating the specific types of FC and CNN layers and renaming Theorem 1 to Corollary 1, or merging Prop. 1 and Prop. 2 into Theorem 1. Moreover, while the paper introduces some mathematical framework and formalizes existing concepts within this framework, it lacks proofs demonstrating what this framework can achieve and fails to establish connections to existing work. Given the lack of substantial theory, I think a more thorough empirical investigation could strengthen the submission. Comparisons with more expressive architectures are missing (e.g., in Table 1 there are no results reported for more expressive architectures for almost half of the datasets; for the synthetic datasets no comparison is done with more expressive architectures), making it difficult to appreciate the practical advantages of using the rule-based layers in practice. The practical relevance is limited further by the fact that the rule-based layer can only process one-dimensional features, and the higher space complexity for dense graphs.

Regarding clarity, there is considerable room for improvement. The concept of how a rule-based layer works was not fully clear to me until page 4. If my understanding is correct, we have a matrix $\mathcal{W}$ that contains all possible weights (and similarly a bias vector $\mathcal{B}$ with all possible biases). A rule restricts $\mathcal{W}$ to a subset of weights; applying rule *R* means setting some entries in $\mathcal{W}$ to zero. If my understanding is incorrect, this indicates that the writing lacks some clarity. I suggest shortening the introduction and preliminaries, which are at times verbose, and including a briefer example from Appendix A.4 earlier in the paper, or providing a clearer definition sooner. Additionally, the notation for the rule-based layer presentation is somewhat convoluted. The readability of the paper is also hindered by the inconsistent use of formal definitions and natural language. While both approaches can be fine (as long as they are precise), there is a noticeable mismatch between the rigor in the preliminaries and, for example, Section 4. Many aspects of the paper are thus unclear; please refer to the *Questions* and *Minor Remarks* for specific examples.

Overall, I think this paper presents promising ideas in a preliminary manner. As also stated by the authors, the dynamic rule-based layer seems to be reasonable for graphs, but is more difficult to devise for other structures. One approach could be to revise the paper from a graph learning perspective, and, if the authors have novel results which hold for general structures, present these results in a follow-up paper. Another exciting direction could be to use rules to create flexible machine learning models for different tasks and input data.

*Minor remarks*:

* line 33: each new information -> each new piece of information
* line 34: the essence -> a bit vague, what is the essence of dynamic NNs?
* Fig. 1 is too small and difficult to parse in general; there is also and typo in the last sentence
* line 75: dot missing after end of sentence
* line 95: concatentation -> should this be ""composition""?
* line 111: dot missing after end of sentence
* Somewhat inflationary use of ""respectively""
* line 123, 140: I would strongly advise to not use $y$ here for $x, y \in D$, as $y$ is already used to denote labels earlier
* It would be helpful to refer to equations as eq. (1) (instead of just (1))
* Could it simplify presentation if you define $\Theta$ as tuple $(\mathcal{W}, \mathcal{B})$?
* Last sentence of Prop. 1 is difficult to read
* Why do we call the learnable parameters $\Theta$ in Prop. 1 and $W^i$ in Prop. 2?
* line 190: higher dimensions -> higher dimensional
* line 202: network -> network architecture
* lines 206-214: I suggest to consider moving this to the preliminaries
* line 221: either rule function (singular) or rule functions R_W, R_B
* line 225: circle -> cycle
* Prop. 3: ""its"" -> not clear what it refers to
* line 231: If R permutation-equivariant -> language sounds off, maybe ""For permutation-equivariant R"" or ""If R is permutation-equivariant""
* line 255: typo in isomorphism
* Pattern counting rule: $d$ is never defined
* line 347: missing space

Limitations:
One of the main limitations, as the authors point out themselves, is that their proposed rule-based layer can only process one-dimensional node features, and no edge features, which impacts the practical value of their method. For more limitations, please refer to *Weaknesses* and *Questions*.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a broad framework for adding expert knowledge to Neural Networks. They formalize this by extending the learnable parameterized functions with an additional parameter consisting of the set of formal rules. In general, these rules maybe learnable as well. However, the authors focuses on these rules being given in the form of expert-knowledge. The authors then introduce the set of rule based layers. And shows that fully connected NN layers and CNN layers are special cases of the rule based layers. They introduce three rule based layers for graphs: Weisfeiler-Leman Layer, Pattern-Counting layer and Aggregation layer. The author shows that there exists a GNN with rule based layers that can distinguish any two isomorphic graphs. Finally the author introduces some examples of rule based layers for specific molecule graphs. And presents experimental results on synthetic and real-world data.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
-- The idea of adding expert knowledge to NNs and GNNs specifically is quite interesting and widely investigated.

-- The presented theory is very general and simple

Weaknesses:
-- The author has used the notion of rules rather broadly. There is no formal language (logic or matrix language) for the rules. They are just arbitrary functions. This basically means that any existing NN model, in one way or another, can be seen as a special case of Rule based NN. In my understanding, this makes the introduced framework a rather simple formalization of how expert knowledge maybe added to NNs. However, this formalization is so loose, that it does not really admit any meaningful analysis or provide any meaningful guidance to the user for adding knowledge.

-- None of the examples presented by the author are beyond what would be anyway possible by adding some simple graph features to the node features. This could be an interesting direction to investigate. But just formally stating that this is possible is not very interesting.

Limitations:
The authors have indeed touched upon most of the points I mention as weaknesses.
However, as mentioned earlier, the proposed framework is very broad and does not provide a meaningful way to proceed.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nEnazjpwOx;"REVIEW 
Summary:
The authors propose diffusion Thompson sampling, which uses a diffusion model to leverage reward under similar actions for more efficient exploration. The authors derive efficient posterior approximations under a diffusion model prior and prove a regret bound in linear instances. To efficiently compute and sample posterior distribution, the authors provide an approximation that relies on close-form solutions for case where both the score functions of the diffusion model and the likelihood are linear. For nonlinear diffusion model, the authors approximate posteriors by a Gaussian distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proof of Theorem 4.1 requires novel techniques such as recursive total variance decomposition and refined arguments such as quantifying not only the posterior information gain for the taken action but also for every learnt latent parameter. 

The paper is well-written. The main contributions and key observations from the regret bound are nicely summarized. Experimental results for all four combinations of linear and nonlinear reward, linear and nonlinear diffusion model are provided. In experiments, the authors made a number of insightful observations, accompanied by ablation results.

Weaknesses:
The authors discussed how the number of layers L affect the regret bound. A higher L increases regret bound and a smaller L may fail to capture a more complex prior. It would improve the paper to provide a heuristics on choosing an appropriate L along with justifications for the heuristics.

Limitations:
The authors addressed limitations and societal impact at Appendix E and F.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents the use of Diffusion models as priors for Thompson sampling.

Namely, they propose to learn diffusion models (as replacement to other parametric priors) to accommodate more complex correlations between context, action and reward functions than with simple parametric form priors.

Given that Thompson sampling requires sampling from the posterior of the model, the authors derive a linear-Gaussian posterior approximation (under the proposed diffusion model prior).

The authors analyze the proposed algorithm for the linear-Gaussian reward case, which enables them to provide a Bayes regret bound.

Experimental results demonstrate some of the benefits of the proposed diffusion-based Thompson sampling: learning the correct latent-structure is beneficial, learning more parameters (as a function of $d$, $K$ and $L$) is a harder problem, hence incurs in higher regret.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The use of diffusion models to learn complex priors for their use within MAB problems is of interest and significant.

- The authors provide a theoretical analysis of their proposed algorithm (only for the linear-Gaussian case), for which they: 
    - use the recursive total covariance decomposition,
    - showcase the dependency over K ---induced by the hierarchical parameter learning--- and
    - demonstrate the dependency with $L$, inherent to having more parameters to learn.
    
- The theoretical analysis and the experiments showcase the benefits of learning the true hierarchical model (as specified by a diffusion model) in comparison to LinTS.

Weaknesses:
- The proposed diffusion-based algorithm does not learn the diffusion model as it sequentially interacts with the world
    - Instead, using the diffusion model as a complex prior requires offline learning, so that non-trivial prior distributions can be learned, before it can be used within Thompson sampling.
    - The cost of learning such a diffusion model is not acknowledged nor discussed.

- The proposed posterior approximation seems to be equivalent to the well known Laplace approximation, i.e., a linear-Gaussian approximation to a (non-linear and non-Gaussian) posterior. See questions below.

- The provided Bayesian regret is limited to the linear-Gaussian case, and in fact is acknowledged to be similar to ""L + 1 sequential linear bandit instances stacked upon each other"".

- The empirical evaluation is executed on synthetic experiments simulated from the assumed model prior, with $L$ latent parameters. Hence, the benefits of learning the true model are somehow expected.

Limitations:
The authors do present general limitations of their work, although the cost associated with learning a diffusion model prior is less clear.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work provides a great example of diffusion modeling on bandit action parameter for better exploration.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work provides a comprehensive description on how to employ diffusion modeling on bandit parameters for contextual bandit problems. 

The discussion on linear and non-linear diffusion model is clean and precise for readers with background in Thompson Sampling 

The analysis part also provide comprehensive discussion on how the regret of the  proposed diffusion Thompson Sampling scales with main dimension of contextual bandit problems.

Weaknesses:
I am satisfied with current version of the paper.

Limitations:
Yes, the author clearly states the assumptions to address the limitation of the theoretical analysis.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers the problem of contextual bandits in large action spaces.  In this problem, the reward of an arm is a function of the context and an unknown, arm specific parameter vector. To efficiently learn good policies in such large action spaces, the paper places a structured-prior distribution on the unknown arm parameters that can effectively capture the correlations between the arms. The specific form of the prior distribution considered in the paper resembles a diffusion model. The main contribution of the paper is to provide a computationally efficient heuristic for performing Thompson sampling with this prior. Experiments on synthetic data show that the proposed technique is much better at learning optimal policies than other popular baselines such as LinTS, LinUCB.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of handling large action spaces in contextual bandits seems interesting. The empirical evaluation shows promise in the proposed approach

Weaknesses:
- **Related Work:** There are several ways in which large action spaces are typically handled in contextual bandits. One popular approach that is used in practice is to associate a feature vector to each arm (this feature vector is known to the learner ahead of time), and the reward of pulling certain arm for a context is a function of both the context and arm features. In the absence of arm features, the other approach is to impose some structure on the unknown arm parameter vectors. There are several works which do this, and the current paper falls in this line of research. Some of these works assume the arms can be clustered into a small number of groups or can be embedded in a low-dimensional latent space and learn the low-dimensional features during the course of the online learning (https://arxiv.org/pdf/2010.12363, https://arxiv.org/abs/2209.03997, https://arxiv.org/pdf/1810.09401, https://proceedings.neurips.cc/paper_files/paper/2023/file/f334c3375bd3744e98a0ca8eaa2403b0-Paper-Conference.pdf). The diffusion prior used in the current work resembles the low-dimensional embedding assumption. In particular, it is assumed there is a latent vector (psi_1) from which all the arm parameter vectors are generated; this is a form of rank-1 assumption on the arm features. Unfortunately, none of these works were brought up in the paper. It would be great if the authors perform a thorough literature review and better position their work.

- **Linear Setting**: A lot of emphasis has been placed on the linear model in the paper. I understand it is used to derive the heuristic for the non-linear setting. Beyond that, I do not find the regret bounds derived in section 4 to be interesting. In the linear setting, there isn't a need to work with the complex hierarchical diffusion prior. It looks like one could totally remove the latent variables psi_{*, L}, .... psi_{*, 2} and simply place a Gaussian prior on psi_{*,1} and get an equally powerful model. This would also improve the regret bounds, by removing the L factor in the regret. Given this, I'm not sure about the utility of section 4.
  - Section 4.1 compares the regret bounds obtained in this work with other baselines. But this comparison is only meaningful under the assumption that the diffusion prior is properly specified. This raises the following question: why is this a reasonable prior to use in practice? How do various techniques compare if this prior is misspecified? (There are some experiments section 5.2 on prior misspecification, but the misspecifications considered there seem to be very minor)

- **Quality of Heuristics:** How good is the heuristic used for non-linear diffusion model? There is no discussion on this in the paper (In my opinion, this needs to be thoroughly discussed in the paper, as it is the primary novelty of the work). Some empirical evaluation comparing it with other standard estimation techniques (such as variational techniques, and other posterior estimation techniques) would have been helpful in understanding his question.

Limitations:
See my comments above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose diffusion Thompson sampling, which uses a diffusion model to leverage reward under similar actions for more efficient exploration. The authors derive efficient posterior approximations under a diffusion model prior and prove a regret bound in linear instances. To efficiently compute and sample posterior distribution, the authors provide an approximation that relies on close-form solutions for case where both the score functions of the diffusion model and the likelihood are linear. For nonlinear diffusion model, the authors approximate posteriors by a Gaussian distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proof of Theorem 4.1 requires novel techniques such as recursive total variance decomposition and refined arguments such as quantifying not only the posterior information gain for the taken action but also for every learnt latent parameter. 

The paper is well-written. The main contributions and key observations from the regret bound are nicely summarized. Experimental results for all four combinations of linear and nonlinear reward, linear and nonlinear diffusion model are provided. In experiments, the authors made a number of insightful observations, accompanied by ablation results.

Weaknesses:
The authors discussed how the number of layers L affect the regret bound. A higher L increases regret bound and a smaller L may fail to capture a more complex prior. It would improve the paper to provide a heuristics on choosing an appropriate L along with justifications for the heuristics.

Limitations:
The authors addressed limitations and societal impact at Appendix E and F.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents the use of Diffusion models as priors for Thompson sampling.

Namely, they propose to learn diffusion models (as replacement to other parametric priors) to accommodate more complex correlations between context, action and reward functions than with simple parametric form priors.

Given that Thompson sampling requires sampling from the posterior of the model, the authors derive a linear-Gaussian posterior approximation (under the proposed diffusion model prior).

The authors analyze the proposed algorithm for the linear-Gaussian reward case, which enables them to provide a Bayes regret bound.

Experimental results demonstrate some of the benefits of the proposed diffusion-based Thompson sampling: learning the correct latent-structure is beneficial, learning more parameters (as a function of $d$, $K$ and $L$) is a harder problem, hence incurs in higher regret.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The use of diffusion models to learn complex priors for their use within MAB problems is of interest and significant.

- The authors provide a theoretical analysis of their proposed algorithm (only for the linear-Gaussian case), for which they: 
    - use the recursive total covariance decomposition,
    - showcase the dependency over K ---induced by the hierarchical parameter learning--- and
    - demonstrate the dependency with $L$, inherent to having more parameters to learn.
    
- The theoretical analysis and the experiments showcase the benefits of learning the true hierarchical model (as specified by a diffusion model) in comparison to LinTS.

Weaknesses:
- The proposed diffusion-based algorithm does not learn the diffusion model as it sequentially interacts with the world
    - Instead, using the diffusion model as a complex prior requires offline learning, so that non-trivial prior distributions can be learned, before it can be used within Thompson sampling.
    - The cost of learning such a diffusion model is not acknowledged nor discussed.

- The proposed posterior approximation seems to be equivalent to the well known Laplace approximation, i.e., a linear-Gaussian approximation to a (non-linear and non-Gaussian) posterior. See questions below.

- The provided Bayesian regret is limited to the linear-Gaussian case, and in fact is acknowledged to be similar to ""L + 1 sequential linear bandit instances stacked upon each other"".

- The empirical evaluation is executed on synthetic experiments simulated from the assumed model prior, with $L$ latent parameters. Hence, the benefits of learning the true model are somehow expected.

Limitations:
The authors do present general limitations of their work, although the cost associated with learning a diffusion model prior is less clear.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work provides a great example of diffusion modeling on bandit action parameter for better exploration.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work provides a comprehensive description on how to employ diffusion modeling on bandit parameters for contextual bandit problems. 

The discussion on linear and non-linear diffusion model is clean and precise for readers with background in Thompson Sampling 

The analysis part also provide comprehensive discussion on how the regret of the  proposed diffusion Thompson Sampling scales with main dimension of contextual bandit problems.

Weaknesses:
I am satisfied with current version of the paper.

Limitations:
Yes, the author clearly states the assumptions to address the limitation of the theoretical analysis.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers the problem of contextual bandits in large action spaces.  In this problem, the reward of an arm is a function of the context and an unknown, arm specific parameter vector. To efficiently learn good policies in such large action spaces, the paper places a structured-prior distribution on the unknown arm parameters that can effectively capture the correlations between the arms. The specific form of the prior distribution considered in the paper resembles a diffusion model. The main contribution of the paper is to provide a computationally efficient heuristic for performing Thompson sampling with this prior. Experiments on synthetic data show that the proposed technique is much better at learning optimal policies than other popular baselines such as LinTS, LinUCB.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of handling large action spaces in contextual bandits seems interesting. The empirical evaluation shows promise in the proposed approach

Weaknesses:
- **Related Work:** There are several ways in which large action spaces are typically handled in contextual bandits. One popular approach that is used in practice is to associate a feature vector to each arm (this feature vector is known to the learner ahead of time), and the reward of pulling certain arm for a context is a function of both the context and arm features. In the absence of arm features, the other approach is to impose some structure on the unknown arm parameter vectors. There are several works which do this, and the current paper falls in this line of research. Some of these works assume the arms can be clustered into a small number of groups or can be embedded in a low-dimensional latent space and learn the low-dimensional features during the course of the online learning (https://arxiv.org/pdf/2010.12363, https://arxiv.org/abs/2209.03997, https://arxiv.org/pdf/1810.09401, https://proceedings.neurips.cc/paper_files/paper/2023/file/f334c3375bd3744e98a0ca8eaa2403b0-Paper-Conference.pdf). The diffusion prior used in the current work resembles the low-dimensional embedding assumption. In particular, it is assumed there is a latent vector (psi_1) from which all the arm parameter vectors are generated; this is a form of rank-1 assumption on the arm features. Unfortunately, none of these works were brought up in the paper. It would be great if the authors perform a thorough literature review and better position their work.

- **Linear Setting**: A lot of emphasis has been placed on the linear model in the paper. I understand it is used to derive the heuristic for the non-linear setting. Beyond that, I do not find the regret bounds derived in section 4 to be interesting. In the linear setting, there isn't a need to work with the complex hierarchical diffusion prior. It looks like one could totally remove the latent variables psi_{*, L}, .... psi_{*, 2} and simply place a Gaussian prior on psi_{*,1} and get an equally powerful model. This would also improve the regret bounds, by removing the L factor in the regret. Given this, I'm not sure about the utility of section 4.
  - Section 4.1 compares the regret bounds obtained in this work with other baselines. But this comparison is only meaningful under the assumption that the diffusion prior is properly specified. This raises the following question: why is this a reasonable prior to use in practice? How do various techniques compare if this prior is misspecified? (There are some experiments section 5.2 on prior misspecification, but the misspecifications considered there seem to be very minor)

- **Quality of Heuristics:** How good is the heuristic used for non-linear diffusion model? There is no discussion on this in the paper (In my opinion, this needs to be thoroughly discussed in the paper, as it is the primary novelty of the work). Some empirical evaluation comparing it with other standard estimation techniques (such as variational techniques, and other posterior estimation techniques) would have been helpful in understanding his question.

Limitations:
See my comments above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
jwE1dgOox1;"REVIEW 
Summary:
Large scale topological descriptors of data are leveraged to compute point/node-level descriptors, which encode to which large scale topological feature each point belongs to. For this, a combination of applied algebraic topology and applied harmonic analysis is used. More specifically, large scale homological features are computed using persistent homology, then represented with harmonic cocyles, and then averaged locally to obtain a point-level descriptors. The problem of topological clustering (already introduced in the literature) is addressed, whose objective is to determine to which large scale topological feature a certain data point belongs to. A set of benchmark datasets are introduced for topological clustering. The pipeline is applied to these datasets as well as real world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Large scale topology of data is leveraged to assign point/node-level features to data. This gives concrete meaning to what it means for a data point to belong to a certain large scale topological feature.
- The method is based on well-established mathematical concepts.
- The concept of topological clustering is interesting and has potential.
- A suite of synthetic datasets is introduced.

Weaknesses:
Regarding unjustified claims:

- Existing approaches are undersold. Specifically, in the introduction it is said that ""none of these approaches is able to represent higher-order topological information"" and that ""such higher-order topological information is however invisible to standard tools of data analysis like PCA of k-means clustering"". However, cluster structure is topological structure. Does ""higher order"" mean homology in dimensions 1 and above?
- Remark 4.2 says that ""datasets with topological structure consist in a majority of cases of points sampled with noise from deformed n-spheres"". This seems like a really strong claim. Is there any evidence of this?

Regarding theory:

- Theorem 4.1 applies in a very restricted scenario. Moreover, I do not understand why the harmonic representative takes values in {0, -1, 1}. This seems very surprising since harmonic cycles/cocycles almost always take fractional values (in order to minimize energy). I did not understand the proof of this fact; specifically, why $g$ being a harmonic generator for the entire filtration range of $(b,1)$ implies this claim.

Regarding the methodology:

- The method, specifically line 225, seems to assume that a cycle with coefficients in $Z/3Z$ will also be a cycle when interpreting those coefficients (0,-1, or 1) as real numbers. However, this need not be the case. To see this it suffices to consider a simplicial complex given by a triangle with no interior. Thus, step 3 of Algorithm 1 (and the method more generally) seems to be heuristic.
- The setup up Table 1 is unclear to me. How can one compare TOPF, which produces feature vectors, with, say, DBSCAN, which produces a clustering?
- Figure 4 is hard to interpret. For example, how should one assess the effectiveness of the algorithm in Fig 4(a)?
- The methodology has many hyperparameters. Some choices, like delta=0.07 in line 241, seem arbitrary.

Limitations:
- The experimental evaluation is limited.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces TOPF, a topological feature extraction mechanism on point cloud data. The authors consider Vietoris-Rips/$\alpha$ filtrations over point clouds and compute the persistent homology. They propose a heuristic to select the “top” features from the barcodes. They consider the corresponding representatives for these features and project them onto the harmonic space of the simplices. These projected vectors are then normalized and used to construct a point-level feature vector. The authors use this framework for clustering. Towards this end, the authors introduce a topological point cloud clustering benchmark and report the experimental results on this benchmark.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors propose to use Hodge Laplacian and Hodge decomposition to compute feature vectors over points in point-cloud data, which is a novel idea.

Weaknesses:
1. I do not fully understand the “learning” the representation here, because the representation is not particularly being learnt. It is being computed by using the persistent homology of the point cloud. 

2. Experimental evaluation is limited to clustering. And even in clustering, it is primarily limited to shapes which are partially/fully topologically spherical.  

3. The robustness of the approach is due to the robustness of harmonic persistent homology known in the literature. 

4. The paper uses well-known notions in the TDA literature in the context of point-clouds, which amounts to an incremental progress in this direction. 

5. It would strengthen the paper if the authors include a small paragraph explaining why projecting onto the harmonic subspace solves the problems that exist in using the homology representatives directly. 

Minor: 

Page 2, Line 81: ‘Spaces in topology are “continuous”’. Continuity is a notion defined for functions on topological spaces and not for topological spaces themselves. Spaces are connected.

Limitations:
Yes, the authors have discussed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an approach to select and compute some point-level topological features for point cloud or general data set analysis.
The main ideas is to define a multi-scale simplicial complex representation, thus we can track how the homology modules change along the filtration and then select the homologies that persist for a long range of scales.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Topological features are usually not localized, the idea of being able to bring back the topological descriptor to the relevant points is quite novel and impactful.
- The approach is theoretically sound and well analyzed.
-The experimental evaluation is limited but convincing.

Weaknesses:
- the feature selection is very heuristic.
- The evaluation is only on point cloud clustering. Since we are evaluating effectiveness and robustness of localized features, feature/point correspondence problems would have been interesting.

Limitations:
The main limitation, i.e. the selection of the features, has been briefly addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel method for extracting per point topological features - TOPF. The method builds on previous results in topological data analysis which described a shape or a point cloud with a single global feature, by generating per-point topologically-aware features. The paper presents a quantitative evaluation and comparison of the proposed method with prior art on a new benchmark consisting of several synthetic examples, evaluates the robustness of the proposed method under noise, as well as presents qualitative examples of its performance on synthetic and real work data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
* The paper is well written and easy to follow. Prior art and the proposed algorithm description is detailed and comprehensive.
* To my understanding, the paper describes a novel method for per-point feature extraction based on topological information contained in a point cloud, and describes theoretical guarantees for its correctness on point clouds sampled from multiple n-spheres.
* The paper describes a new topological point clustering benchmark dataset consisting of seven synthetic point clouds with up to 5 labels, and evaluate the proposed and existing methods on this dataset showing that the proposed method outperforms existing methods in most cases.

Weaknesses:
* The paper lists common machine learning applications requiring point level features as a motivation for the proposed method. However, only quantitative experiments for point cloud clustering on a set of synthetic examples, and anecdotal evidence of performance on real world data, were presented. In order to fully understand the potential of the proposed approach to be applied beyond synthetic data, it would be beneficial to include additional evaluation, qualitative and quantitative, on real-world data and additional applications, e.g. as described in lines 304-307.
* Specifically, it would be interesting to see experiments on non-synthetic datasets with topological structure mentioned in line 266.
* Additionally, comparison with other well performing modern machine learning methods, such as graph neural networks for point cloud clustering, needs to be discussed, for completeness.

Limitations:
The authors adequately addressed the limitations and impact of the proposed approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Large scale topological descriptors of data are leveraged to compute point/node-level descriptors, which encode to which large scale topological feature each point belongs to. For this, a combination of applied algebraic topology and applied harmonic analysis is used. More specifically, large scale homological features are computed using persistent homology, then represented with harmonic cocyles, and then averaged locally to obtain a point-level descriptors. The problem of topological clustering (already introduced in the literature) is addressed, whose objective is to determine to which large scale topological feature a certain data point belongs to. A set of benchmark datasets are introduced for topological clustering. The pipeline is applied to these datasets as well as real world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Large scale topology of data is leveraged to assign point/node-level features to data. This gives concrete meaning to what it means for a data point to belong to a certain large scale topological feature.
- The method is based on well-established mathematical concepts.
- The concept of topological clustering is interesting and has potential.
- A suite of synthetic datasets is introduced.

Weaknesses:
Regarding unjustified claims:

- Existing approaches are undersold. Specifically, in the introduction it is said that ""none of these approaches is able to represent higher-order topological information"" and that ""such higher-order topological information is however invisible to standard tools of data analysis like PCA of k-means clustering"". However, cluster structure is topological structure. Does ""higher order"" mean homology in dimensions 1 and above?
- Remark 4.2 says that ""datasets with topological structure consist in a majority of cases of points sampled with noise from deformed n-spheres"". This seems like a really strong claim. Is there any evidence of this?

Regarding theory:

- Theorem 4.1 applies in a very restricted scenario. Moreover, I do not understand why the harmonic representative takes values in {0, -1, 1}. This seems very surprising since harmonic cycles/cocycles almost always take fractional values (in order to minimize energy). I did not understand the proof of this fact; specifically, why $g$ being a harmonic generator for the entire filtration range of $(b,1)$ implies this claim.

Regarding the methodology:

- The method, specifically line 225, seems to assume that a cycle with coefficients in $Z/3Z$ will also be a cycle when interpreting those coefficients (0,-1, or 1) as real numbers. However, this need not be the case. To see this it suffices to consider a simplicial complex given by a triangle with no interior. Thus, step 3 of Algorithm 1 (and the method more generally) seems to be heuristic.
- The setup up Table 1 is unclear to me. How can one compare TOPF, which produces feature vectors, with, say, DBSCAN, which produces a clustering?
- Figure 4 is hard to interpret. For example, how should one assess the effectiveness of the algorithm in Fig 4(a)?
- The methodology has many hyperparameters. Some choices, like delta=0.07 in line 241, seem arbitrary.

Limitations:
- The experimental evaluation is limited.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces TOPF, a topological feature extraction mechanism on point cloud data. The authors consider Vietoris-Rips/$\alpha$ filtrations over point clouds and compute the persistent homology. They propose a heuristic to select the “top” features from the barcodes. They consider the corresponding representatives for these features and project them onto the harmonic space of the simplices. These projected vectors are then normalized and used to construct a point-level feature vector. The authors use this framework for clustering. Towards this end, the authors introduce a topological point cloud clustering benchmark and report the experimental results on this benchmark.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors propose to use Hodge Laplacian and Hodge decomposition to compute feature vectors over points in point-cloud data, which is a novel idea.

Weaknesses:
1. I do not fully understand the “learning” the representation here, because the representation is not particularly being learnt. It is being computed by using the persistent homology of the point cloud. 

2. Experimental evaluation is limited to clustering. And even in clustering, it is primarily limited to shapes which are partially/fully topologically spherical.  

3. The robustness of the approach is due to the robustness of harmonic persistent homology known in the literature. 

4. The paper uses well-known notions in the TDA literature in the context of point-clouds, which amounts to an incremental progress in this direction. 

5. It would strengthen the paper if the authors include a small paragraph explaining why projecting onto the harmonic subspace solves the problems that exist in using the homology representatives directly. 

Minor: 

Page 2, Line 81: ‘Spaces in topology are “continuous”’. Continuity is a notion defined for functions on topological spaces and not for topological spaces themselves. Spaces are connected.

Limitations:
Yes, the authors have discussed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an approach to select and compute some point-level topological features for point cloud or general data set analysis.
The main ideas is to define a multi-scale simplicial complex representation, thus we can track how the homology modules change along the filtration and then select the homologies that persist for a long range of scales.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Topological features are usually not localized, the idea of being able to bring back the topological descriptor to the relevant points is quite novel and impactful.
- The approach is theoretically sound and well analyzed.
-The experimental evaluation is limited but convincing.

Weaknesses:
- the feature selection is very heuristic.
- The evaluation is only on point cloud clustering. Since we are evaluating effectiveness and robustness of localized features, feature/point correspondence problems would have been interesting.

Limitations:
The main limitation, i.e. the selection of the features, has been briefly addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel method for extracting per point topological features - TOPF. The method builds on previous results in topological data analysis which described a shape or a point cloud with a single global feature, by generating per-point topologically-aware features. The paper presents a quantitative evaluation and comparison of the proposed method with prior art on a new benchmark consisting of several synthetic examples, evaluates the robustness of the proposed method under noise, as well as presents qualitative examples of its performance on synthetic and real work data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
* The paper is well written and easy to follow. Prior art and the proposed algorithm description is detailed and comprehensive.
* To my understanding, the paper describes a novel method for per-point feature extraction based on topological information contained in a point cloud, and describes theoretical guarantees for its correctness on point clouds sampled from multiple n-spheres.
* The paper describes a new topological point clustering benchmark dataset consisting of seven synthetic point clouds with up to 5 labels, and evaluate the proposed and existing methods on this dataset showing that the proposed method outperforms existing methods in most cases.

Weaknesses:
* The paper lists common machine learning applications requiring point level features as a motivation for the proposed method. However, only quantitative experiments for point cloud clustering on a set of synthetic examples, and anecdotal evidence of performance on real world data, were presented. In order to fully understand the potential of the proposed approach to be applied beyond synthetic data, it would be beneficial to include additional evaluation, qualitative and quantitative, on real-world data and additional applications, e.g. as described in lines 304-307.
* Specifically, it would be interesting to see experiments on non-synthetic datasets with topological structure mentioned in line 266.
* Additionally, comparison with other well performing modern machine learning methods, such as graph neural networks for point cloud clustering, needs to be discussed, for completeness.

Limitations:
The authors adequately addressed the limitations and impact of the proposed approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
iRHxp1ibFj;"REVIEW 
Summary:
The paper introduces a novel image-level supervision method for semantic segmentation, utilizing approximate targets for the relative sizes of segments in training images. These targets, represented as categorical distributions for the expected average prediction over pixels, are integrated using a zero-avoiding variant of KL divergence as the training loss. This approach achieves quality comparable to full pixel-level supervision but is significantly less costly, requiring only rough estimates of the areas occupied by each class.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Using object size as a form of supervision is both innovative and interesting.
2. The proposed method is straightforward and easy to understand.

Weaknesses:
1. The title of the paper is misleading. It claims that approximate size targets are sufficient, but the work also uses image labels for supervision.
2. The most important comparison in Figure 1 is between 'Tag' and 'Size target,' as this validates the significance of using target size supervision. To clearly demonstrate that 'Size target' is superior to 'Tag' under identical conditions, it would be better to use the same architecture for both comparisons.
3. Labeling the size of objects can be challenging for humans and may introduce significant noise, especially for tiny objects. Although the authors demonstrate impressive accuracy with up to 8% size target errors, this remains a stringent annotation standard, particularly for small objects. For instance, as seen in Table 1, the mean relative error (mRE) often exceeds 10% during human annotation in the Pascal VOC dataset. Moreover, estimating target sizes in Pascal VOC is relatively easy since objects are typically large and centered. However, labeling images in more complex datasets, such as COCO, might result in a higher mRE.
4. In Table 1, the authors should also report the speed of tag annotation to highlight the cost of estimating target sizes.
5. The proposed method is straightforward and impressive for its end-to-end training, especially considering that existing weakly supervised semantic segmentation (WSSS) methods typically use CAM and two-step training. However, as shown in Table 2, while the proposed method achieves comparable accuracy to state-of-the-art WSSS methods, it relies on additional supervision and a high annotation standard (8% mRE). Moreover, Table 2 indicates that the accuracy with only tag supervision is close to that of fully supervised methods, suggesting that tag supervision alone may be sufficient for segmentation.

Limitations:
The authors do not discuss the limitations and broader impact of their method, which necessitates a dedicated discussion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new weakly supervised semantic segmentation task. This task uses pixel-level categorical distribution as the label in the training stage. KL divergence is used as the training loss. Experiments on three public segmentation datasets show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The proposed task is interesting. It provides the community another choice for segmentation with less annotation effort.

2.The proposed KL divergence loss is effective, demonstrated by experiments on three public datasets. It achieves performance comparable to methods using more expensive labels, like the box supervised one.

3.The proposed method is robust to size target error, which makes it more practical.

4.The writing is fluent and easy to follow.

Weaknesses:
1.Labeling effort on complex images. Images from PASCAL VOC (like Figure 1) are easy to annotate. It contains few classes and the background is generally clean. The density of target objects is low, and hence it’s also suitable for the proposed grid-based size target annotation way.

However, in practice, scenes are much more complex, with more classes, more crowded objects, and complex backgrounds. The authors are recommended to show the annotation effort on those images, like images from Cityscapes and ADE20K. I think when the scenes become more complex, the labeling effort will increase significantly. The labeling effort of size target will be much more than the tag way, since tagging will be less influenced in such cases.

2.Model performance on complex images. Similarly, it’s recommended to evaluate the model’s performance with the proposed loss on these complex datasets. This will give a more comprehensive understanding of the proposed method.

Limitations:
No negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper titled ""Approximate Size Targets Are Sufficient for Accurate Semantic Segmentation"" proposes a novel method of semantic segmentation that leverages approximate size targets instead of full pixel-level supervision. The method involves using categorical distributions to represent the expected average prediction over image pixels, utilizing the zero-avoiding variant of KL divergence as a training loss. The approach aims to reduce annotation costs while maintaining segmentation accuracy comparable to full supervision.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Originality: The use of approximate size targets as a form of weak supervision for semantic segmentation is novel and creative.
2. Quality: The experimental results are comprehensive and demonstrate the effectiveness of the proposed method across different datasets and segmentation architectures.
3. Significance: The approach has significant implications for reducing annotation costs in semantic segmentation tasks, making it highly relevant to practical applications.

Weaknesses:
1. Simplicity of Method：While the proposed method is innovative, it seems relatively simple. There might be opportunities to enhance its contributions with further development or by integrating additional techniques.
2. Limited Scope of Evaluation: While the paper evaluates the method on several datasets, it would benefit from a broader range of scenarios, including more diverse and complex images.

Limitations:
The authors have addressed the limitations related to annotation errors and have demonstrated the robustness of their method to these errors. However, it would be beneficial to discuss potential limitations in more detail, such as the scalability of the method to larger and more diverse datasets, and any assumptions made about the nature of the size target annotations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel image-level supervision method for semantic segmentation using approximate segment size targets. It utilizes categorical distributions for expected average predictions, reducing annotation cost and complexity. The authors propose a zero-avoiding KL divergence as a training loss, compatible with any segmentation architecture, and demonstrate significant robustness to size target errors, improving generalization. The method achieves state-of-the-art performance on multiple datasets with standard segmentation models like ResNet101. Additionally, it requires minimal extra information and no architectural changes, making it a practical and effective solution for weakly-supervised semantic segmentation in real-world applications.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper introduces a novel form of image-level supervision for semantic segmentation using approximate segment size targets. This approach is original in its use of categorical distributions for expected average predictions, providing a fresh perspective on weakly-supervised segmentation methods.

2. The quality of the research is high, with comprehensive experiments conducted on multiple datasets. The use of a zero-avoiding variant of KL divergence as a training loss is well-justified and demonstrates robustness to size target errors. The empirical results show that the method achieves state-of-the-art performance using standard segmentation models.

Weaknesses:
1. The paper claims robustness to size target errors but provides limited detailed analysis on this aspect. Including more experiments to quantify and analyze how different levels of size target errors impact performance would provide a clearer understanding of the method's robustness.

2. Lack of related work. The paper’s logical flow and organization need improvement. 

3. The paper lacks comprehensive comparisons with the latest models, such as ""SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation (AAAI24)"".

Limitations:
1. Fig and Figure are inconsistent in Line 24

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel image-level supervision method for semantic segmentation, utilizing approximate targets for the relative sizes of segments in training images. These targets, represented as categorical distributions for the expected average prediction over pixels, are integrated using a zero-avoiding variant of KL divergence as the training loss. This approach achieves quality comparable to full pixel-level supervision but is significantly less costly, requiring only rough estimates of the areas occupied by each class.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Using object size as a form of supervision is both innovative and interesting.
2. The proposed method is straightforward and easy to understand.

Weaknesses:
1. The title of the paper is misleading. It claims that approximate size targets are sufficient, but the work also uses image labels for supervision.
2. The most important comparison in Figure 1 is between 'Tag' and 'Size target,' as this validates the significance of using target size supervision. To clearly demonstrate that 'Size target' is superior to 'Tag' under identical conditions, it would be better to use the same architecture for both comparisons.
3. Labeling the size of objects can be challenging for humans and may introduce significant noise, especially for tiny objects. Although the authors demonstrate impressive accuracy with up to 8% size target errors, this remains a stringent annotation standard, particularly for small objects. For instance, as seen in Table 1, the mean relative error (mRE) often exceeds 10% during human annotation in the Pascal VOC dataset. Moreover, estimating target sizes in Pascal VOC is relatively easy since objects are typically large and centered. However, labeling images in more complex datasets, such as COCO, might result in a higher mRE.
4. In Table 1, the authors should also report the speed of tag annotation to highlight the cost of estimating target sizes.
5. The proposed method is straightforward and impressive for its end-to-end training, especially considering that existing weakly supervised semantic segmentation (WSSS) methods typically use CAM and two-step training. However, as shown in Table 2, while the proposed method achieves comparable accuracy to state-of-the-art WSSS methods, it relies on additional supervision and a high annotation standard (8% mRE). Moreover, Table 2 indicates that the accuracy with only tag supervision is close to that of fully supervised methods, suggesting that tag supervision alone may be sufficient for segmentation.

Limitations:
The authors do not discuss the limitations and broader impact of their method, which necessitates a dedicated discussion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new weakly supervised semantic segmentation task. This task uses pixel-level categorical distribution as the label in the training stage. KL divergence is used as the training loss. Experiments on three public segmentation datasets show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The proposed task is interesting. It provides the community another choice for segmentation with less annotation effort.

2.The proposed KL divergence loss is effective, demonstrated by experiments on three public datasets. It achieves performance comparable to methods using more expensive labels, like the box supervised one.

3.The proposed method is robust to size target error, which makes it more practical.

4.The writing is fluent and easy to follow.

Weaknesses:
1.Labeling effort on complex images. Images from PASCAL VOC (like Figure 1) are easy to annotate. It contains few classes and the background is generally clean. The density of target objects is low, and hence it’s also suitable for the proposed grid-based size target annotation way.

However, in practice, scenes are much more complex, with more classes, more crowded objects, and complex backgrounds. The authors are recommended to show the annotation effort on those images, like images from Cityscapes and ADE20K. I think when the scenes become more complex, the labeling effort will increase significantly. The labeling effort of size target will be much more than the tag way, since tagging will be less influenced in such cases.

2.Model performance on complex images. Similarly, it’s recommended to evaluate the model’s performance with the proposed loss on these complex datasets. This will give a more comprehensive understanding of the proposed method.

Limitations:
No negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper titled ""Approximate Size Targets Are Sufficient for Accurate Semantic Segmentation"" proposes a novel method of semantic segmentation that leverages approximate size targets instead of full pixel-level supervision. The method involves using categorical distributions to represent the expected average prediction over image pixels, utilizing the zero-avoiding variant of KL divergence as a training loss. The approach aims to reduce annotation costs while maintaining segmentation accuracy comparable to full supervision.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Originality: The use of approximate size targets as a form of weak supervision for semantic segmentation is novel and creative.
2. Quality: The experimental results are comprehensive and demonstrate the effectiveness of the proposed method across different datasets and segmentation architectures.
3. Significance: The approach has significant implications for reducing annotation costs in semantic segmentation tasks, making it highly relevant to practical applications.

Weaknesses:
1. Simplicity of Method：While the proposed method is innovative, it seems relatively simple. There might be opportunities to enhance its contributions with further development or by integrating additional techniques.
2. Limited Scope of Evaluation: While the paper evaluates the method on several datasets, it would benefit from a broader range of scenarios, including more diverse and complex images.

Limitations:
The authors have addressed the limitations related to annotation errors and have demonstrated the robustness of their method to these errors. However, it would be beneficial to discuss potential limitations in more detail, such as the scalability of the method to larger and more diverse datasets, and any assumptions made about the nature of the size target annotations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel image-level supervision method for semantic segmentation using approximate segment size targets. It utilizes categorical distributions for expected average predictions, reducing annotation cost and complexity. The authors propose a zero-avoiding KL divergence as a training loss, compatible with any segmentation architecture, and demonstrate significant robustness to size target errors, improving generalization. The method achieves state-of-the-art performance on multiple datasets with standard segmentation models like ResNet101. Additionally, it requires minimal extra information and no architectural changes, making it a practical and effective solution for weakly-supervised semantic segmentation in real-world applications.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper introduces a novel form of image-level supervision for semantic segmentation using approximate segment size targets. This approach is original in its use of categorical distributions for expected average predictions, providing a fresh perspective on weakly-supervised segmentation methods.

2. The quality of the research is high, with comprehensive experiments conducted on multiple datasets. The use of a zero-avoiding variant of KL divergence as a training loss is well-justified and demonstrates robustness to size target errors. The empirical results show that the method achieves state-of-the-art performance using standard segmentation models.

Weaknesses:
1. The paper claims robustness to size target errors but provides limited detailed analysis on this aspect. Including more experiments to quantify and analyze how different levels of size target errors impact performance would provide a clearer understanding of the method's robustness.

2. Lack of related work. The paper’s logical flow and organization need improvement. 

3. The paper lacks comprehensive comparisons with the latest models, such as ""SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation (AAAI24)"".

Limitations:
1. Fig and Figure are inconsistent in Line 24

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
hiwHaqFXGi;"REVIEW 
Summary:
The paper introduces DiGGR (Disentangled Generative Graph Representation Learning), a self-supervised learning framework that aims to guide graph mask modeling through disentangled latent factors to enhance the disentanglement of learned representations. Extensive experiments across 11 public datasets for node and graph classification tasks demonstrate the framework's effectiveness, significantly outperforming many existing self-supervised methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Innovative Approach: The DiGGR framework innovatively utilizes disentangled latent factors to guide graph mask modeling, a novel contribution in generative graph representation learning that significantly enhances the model's explainability and robustness.
Comprehensive Experiments: The paper conducts extensive experiments on multiple datasets and tasks, showing significant performance improvements over existing methods, thus providing strong empirical support for the proposed approach.

Weaknesses:
Complexity and Scalability: The framework appears computationally complex, which might limit its scalability to very large graphs or real-time applications. Unfortunately, this aspect is not extensively discussed in the paper.
Lack of Theoretical Analysis: While the empirical results are strong, the paper lacks a detailed theoretical analysis of why the disentanglement process improves performance, which could provide deeper insights into the method’s efficacy and limitations.

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a framework called DiGGR, aimed at improving the robustness and explainability of generative graph models by addressing the issue of entangled graph representations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper tells the story in an easy-to-read way, and the whole paper is quite easy to follow.
2. The problem of disentangled learning is a very popular yet important task.
3. The paper conducts comprehensive experiments to evaluate their method.

Weaknesses:
1. Lack of novelty. Graph disentangled learning is not a new task. There are tons of existing methods for disentangled representation learning, such as those maximizing KL divergence or minimizing mutual information between two sets of representations. A lot of related works such as [1], [2], [3] and [4] are not discussed. Also node factorization is not a new idea, such as node clustering in [3].

[1] Disentangled graph collaborative filtering. SIGIR 2020.
[2] Disentangled Graph Convolutional Networks. ICML 2019.
[3] Deep Generative Model for Periodic Graphs. NeurIPS 2022.
[4] Disentangled contrastive learning on graphs. NeurIPS 2021.

2. The motivation of the proposed method is not clear to me. For example, why should we use mask? Also, why the proposed method sticks to GAE, not VGAE or other types of GNN, such as GCN, GIN or GAT?

Limitations:
Yes. Limitations have been discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work proposes a disentangled generative self-supervised learning method for graphs. The authors introduce a latent factor learning module to capture the heterogeneous factors in the nodes. The proposed method factorizes the graph into factor-specific subgraphs, and jointly trains a disentangled Graph MAE applying distinct masks for each subgraph. Experimental results demonstrate that DiGGR outperforms traditional methods that treat the graph holistically, without accounting for its latent structure.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method first explores a factorization method for generative graph SSL. 
2. The authors provide extensive experimental results and analysis on both node and graph-level tasks to show the improved effectiveness, interpretability, and generalization by using the proposed method.

Weaknesses:
- The computation complexity of the proposed method is quite high. Could the author pride training time comparison to the baseline methods to help us get a sense of the real complexity?
- Could the author provide more insights on how to find an optimal factor number K according to the statistics of diverse datasets? This might be useful for real-world applications.

Limitations:
Yes, the authors discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a self-supervised learning framework DiGGR, aimed at enhancing the disentanglement of learned graph representations. The authors argue that existing generative graph models tend to overlook the entanglement of learned representations, leading to non-robust and non-explainable models. DiGGR addresses this by introducing a latent factor learning module and a disentangled graph masked autoencoder, allowing for factor-wise graph representations. The framework is tested on various benchmarks, demonstrating its effectiveness in outperforming previous self-supervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies an interesting research problem that is disentangled graph representation learning. This research problem is very hot recently.

2. The model design is easy to understand. The paper provides a detailed explanation of the proposed model.

3. The experiments demonstrate the effectiveness of the model. The performance improvement on some comparisons seems to be significant.

Weaknesses:
1. One of my concerns is from the novelty. I think the model design is a little similar to the works [1-2]. The authors should make more comprehensive discussions to show the differences between them.

2. The experiments ignore some recent or related contrastive baselines [1-4] for comparisons.  The improvements on some datasets seem to be not significant. 

3.  More large-scale benchmarks should also be considered, e.g., OGB. The experimental settings are not very clear for reproducing the results.

[1] Disentangled contrastive learning on graphs. NeurIPS 2021.

[2] Disentangled Graph Contrastive Learning With Independence Promotion. TKDE 2022.

[3] Augmentation-Free Graph Contrastive Learning of Invariant-Discriminative Representations. TNNLS 2023.

[4] MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning. AAAI 2023.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces DiGGR (Disentangled Generative Graph Representation Learning), a self-supervised learning framework that aims to guide graph mask modeling through disentangled latent factors to enhance the disentanglement of learned representations. Extensive experiments across 11 public datasets for node and graph classification tasks demonstrate the framework's effectiveness, significantly outperforming many existing self-supervised methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Innovative Approach: The DiGGR framework innovatively utilizes disentangled latent factors to guide graph mask modeling, a novel contribution in generative graph representation learning that significantly enhances the model's explainability and robustness.
Comprehensive Experiments: The paper conducts extensive experiments on multiple datasets and tasks, showing significant performance improvements over existing methods, thus providing strong empirical support for the proposed approach.

Weaknesses:
Complexity and Scalability: The framework appears computationally complex, which might limit its scalability to very large graphs or real-time applications. Unfortunately, this aspect is not extensively discussed in the paper.
Lack of Theoretical Analysis: While the empirical results are strong, the paper lacks a detailed theoretical analysis of why the disentanglement process improves performance, which could provide deeper insights into the method’s efficacy and limitations.

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a framework called DiGGR, aimed at improving the robustness and explainability of generative graph models by addressing the issue of entangled graph representations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper tells the story in an easy-to-read way, and the whole paper is quite easy to follow.
2. The problem of disentangled learning is a very popular yet important task.
3. The paper conducts comprehensive experiments to evaluate their method.

Weaknesses:
1. Lack of novelty. Graph disentangled learning is not a new task. There are tons of existing methods for disentangled representation learning, such as those maximizing KL divergence or minimizing mutual information between two sets of representations. A lot of related works such as [1], [2], [3] and [4] are not discussed. Also node factorization is not a new idea, such as node clustering in [3].

[1] Disentangled graph collaborative filtering. SIGIR 2020.
[2] Disentangled Graph Convolutional Networks. ICML 2019.
[3] Deep Generative Model for Periodic Graphs. NeurIPS 2022.
[4] Disentangled contrastive learning on graphs. NeurIPS 2021.

2. The motivation of the proposed method is not clear to me. For example, why should we use mask? Also, why the proposed method sticks to GAE, not VGAE or other types of GNN, such as GCN, GIN or GAT?

Limitations:
Yes. Limitations have been discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work proposes a disentangled generative self-supervised learning method for graphs. The authors introduce a latent factor learning module to capture the heterogeneous factors in the nodes. The proposed method factorizes the graph into factor-specific subgraphs, and jointly trains a disentangled Graph MAE applying distinct masks for each subgraph. Experimental results demonstrate that DiGGR outperforms traditional methods that treat the graph holistically, without accounting for its latent structure.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method first explores a factorization method for generative graph SSL. 
2. The authors provide extensive experimental results and analysis on both node and graph-level tasks to show the improved effectiveness, interpretability, and generalization by using the proposed method.

Weaknesses:
- The computation complexity of the proposed method is quite high. Could the author pride training time comparison to the baseline methods to help us get a sense of the real complexity?
- Could the author provide more insights on how to find an optimal factor number K according to the statistics of diverse datasets? This might be useful for real-world applications.

Limitations:
Yes, the authors discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a self-supervised learning framework DiGGR, aimed at enhancing the disentanglement of learned graph representations. The authors argue that existing generative graph models tend to overlook the entanglement of learned representations, leading to non-robust and non-explainable models. DiGGR addresses this by introducing a latent factor learning module and a disentangled graph masked autoencoder, allowing for factor-wise graph representations. The framework is tested on various benchmarks, demonstrating its effectiveness in outperforming previous self-supervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies an interesting research problem that is disentangled graph representation learning. This research problem is very hot recently.

2. The model design is easy to understand. The paper provides a detailed explanation of the proposed model.

3. The experiments demonstrate the effectiveness of the model. The performance improvement on some comparisons seems to be significant.

Weaknesses:
1. One of my concerns is from the novelty. I think the model design is a little similar to the works [1-2]. The authors should make more comprehensive discussions to show the differences between them.

2. The experiments ignore some recent or related contrastive baselines [1-4] for comparisons.  The improvements on some datasets seem to be not significant. 

3.  More large-scale benchmarks should also be considered, e.g., OGB. The experimental settings are not very clear for reproducing the results.

[1] Disentangled contrastive learning on graphs. NeurIPS 2021.

[2] Disentangled Graph Contrastive Learning With Independence Promotion. TKDE 2022.

[3] Augmentation-Free Graph Contrastive Learning of Invariant-Discriminative Representations. TNNLS 2023.

[4] MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning. AAAI 2023.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
h8goI8uPXM;"REVIEW 
Summary:
The paper introduces decoupleQ, a novel method that decouples model parameters into integer and floating-point parts. This approach transforms the quantization problem into a mathematical constrained optimization problem, avoiding the limitations of traditional heuristic quantization methods. DecoupleQ achieves a significant improvement over existing methods in LLM., especially at extreme low bits (2-bit) and also release the W2A16 CUDA kernel.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. DecoupleQ eliminates the need for ad-hoc techniques to handle outliers and sensitive channels, focusing solely on optimizing model accuracy under extreme low-bit quantization.
2. DecoupleQ achieves a notable advancement over existing methods in LLM, particularly at extremely low bit. And the W2A16 CUDA kernel has been released.
3. DecoupleQ approach can be readily extended to supervised fine-tuning (SFT) to enhance model accuracy, or adapted for downstream sub-tasks.

Weaknesses:
1. Please correct me if I am wrong. It seems that decoupleQ combines several existing approaches. Specifically, it uses Adaround to get the integer part in ResNets and GPTQ to get the integer part in LLMs. Additionally, it integrates PTQ and QAT by applying PTQ to the integer part while using supervised training for the floating-point part.
2. Regarding your point from lines 58-61, I believe GPTQ clearly outlines how to calculate scale and zero point in their code. Moreover, GPTQ can be seen as a constrained optimization problem, where the constraints align with yours: each integer weight is confined within [$\alpha$, $\beta$], which is a default constraint in GPTQ.
3. Further experiments on LLMs are essential. For example, evaluating decoupleQ's performance in multi-task settings and within the LlaMa 3 family would provide valuable insights.
4. Could you provide more ablation studies in the second stage, such as experiments without training norm layers?
5. There is a typo in line 125. The first letter of 'decoupleQ' should be capitalized.

Limitations:
Please refer to the weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a linear and uniform quantization method, decoupleQ, which abandons the traditional heuristic quantization paradigm and decouples the model parameters into integer and floating-point parts, then transforming the quantization problem into integer and floating-point part. Experiments show decoupleQ achieves comparable acc as fp16/bf16 on 2-bit weight quantization setting.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Experiments show decoupleQ achieves comparable acc as fp16/bf16 on 2-bit weight quantization setting.

Weaknesses:
1. Experiments are based on W2A16, lower activation bitwidth(<=8bit) should be experimented.
2. The novelty is limited. The core idea of decoupleQ is similar to Normalization(Batch-Norm or Layer-Norm). The learnable floating part of decoupleQ equals to a learnable Normalization parameters.
3. More existing Quantization methods should be compared, such as, NWQ[1], PD-Quant[2]

[1] Leveraging Inter-Layer Dependency for Post -Training Quantization 
[2] PD-Quant: Post-Training Quantization based on Prediction Difference Metric.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents decoupleQ, a post-training quantization method that improves the accuracy of quantized models, particularly at very low bit-widths (2-bit). It achieves this by separating model parameters into integer and floating-point components and formulating the quantization process as a constrained optimization problem. This approach eliminates the need for traditional quantization techniques like outlier handling and focuses on optimizing the core objective.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The paper introduces a fresh perspective on quantization by abandoning traditional methods and reframing it as a constrained optimization problem.
2. decoupleQ demonstrates impressive results in 2-bit quantization, achieving accuracy comparable to higher precision formats like fp16/bf16 in large speech models.
3. The quantization process is linear and uniform, making it easier to implement in hardware compared to non-uniform methods.

Weaknesses:
1. The paper's writing lacks cohesion and clarity regarding its ultimate goal. The paper also has several spelling mistakes.
2. The authors claim to separate the model parameters into integers and floating-point components. However, as far as I understand, this practice is not a novel contribution but rather a common approach in quantization.
3. They address a portion of the optimization problem using GPTQ and another portion similar to BRECQ.
4. The authors acknowledge that their solution may not be optimal. 
5. The quantization process in decoupleQ can be more time-consuming than other methods.

Limitations:
Yes, however, it will be beneficial to divide the current Discussion section into separate Conclusion and Limitations sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel post-training quantization method to achieve 2-bit uniform quantization on large language and speech models. The proposed method decouples the quantized values into integer and floating-point parts, which are then optimized via a constrained optimization problem that can be solved with off-the-shelf solutions. The proposed method allows uniform quantization down to extreme bits.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper proposes a novel optimization-based method to conduct PTQ on large models. The proposed method is solid and unique from previous methods.
2. The proposed method achieves good performance with only uniform quantization, without special procedure for outliers etc., providing direct benefit to the runtime of the quantized model on general hardware.
3. The limitations and future directions are clearly discussed in the paper.

Weaknesses:
1. The distinction between the proposed decoupleQ and the traditional quantization methods are not clearly derived in Sec. 3.2. The statement that ""(s,z) lost the traditional meaning"" on line 138 is not clear. My understanding is that W, s, and z are now totally independent of the original weight w0 in the optimization process, as long as the final output error is minimized? I think adding a comparison with the optimization objective/procedure of the traditional quantization here will help.
2. The proposed method appears to be sensitive to the size of the calibration set, so that the calibration size reported in the experiments are much larger than that of the previous baselines. As it is understandable that the optimization process may require more data to avoid overfitting, it would be more fair if the baseline methods are also calibrated with the same dataset/training cost.
3. For the LLM experiments, only ppl is used as metric. However, the ppl has been shown to be an inaccurate metric to reflect the utility of the LLM after compression. More evaluations such as zero-shot performance on downstream tasks and the instruction following ability etc., as in SqueezeLLM and OmniQuant papers, would be helpful to see if the quantized model still retains the ability as the FP one.

Limitations:
The limitations and potential social impacts are adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces decoupleQ, a novel method that decouples model parameters into integer and floating-point parts. This approach transforms the quantization problem into a mathematical constrained optimization problem, avoiding the limitations of traditional heuristic quantization methods. DecoupleQ achieves a significant improvement over existing methods in LLM., especially at extreme low bits (2-bit) and also release the W2A16 CUDA kernel.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. DecoupleQ eliminates the need for ad-hoc techniques to handle outliers and sensitive channels, focusing solely on optimizing model accuracy under extreme low-bit quantization.
2. DecoupleQ achieves a notable advancement over existing methods in LLM, particularly at extremely low bit. And the W2A16 CUDA kernel has been released.
3. DecoupleQ approach can be readily extended to supervised fine-tuning (SFT) to enhance model accuracy, or adapted for downstream sub-tasks.

Weaknesses:
1. Please correct me if I am wrong. It seems that decoupleQ combines several existing approaches. Specifically, it uses Adaround to get the integer part in ResNets and GPTQ to get the integer part in LLMs. Additionally, it integrates PTQ and QAT by applying PTQ to the integer part while using supervised training for the floating-point part.
2. Regarding your point from lines 58-61, I believe GPTQ clearly outlines how to calculate scale and zero point in their code. Moreover, GPTQ can be seen as a constrained optimization problem, where the constraints align with yours: each integer weight is confined within [$\alpha$, $\beta$], which is a default constraint in GPTQ.
3. Further experiments on LLMs are essential. For example, evaluating decoupleQ's performance in multi-task settings and within the LlaMa 3 family would provide valuable insights.
4. Could you provide more ablation studies in the second stage, such as experiments without training norm layers?
5. There is a typo in line 125. The first letter of 'decoupleQ' should be capitalized.

Limitations:
Please refer to the weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a linear and uniform quantization method, decoupleQ, which abandons the traditional heuristic quantization paradigm and decouples the model parameters into integer and floating-point parts, then transforming the quantization problem into integer and floating-point part. Experiments show decoupleQ achieves comparable acc as fp16/bf16 on 2-bit weight quantization setting.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Experiments show decoupleQ achieves comparable acc as fp16/bf16 on 2-bit weight quantization setting.

Weaknesses:
1. Experiments are based on W2A16, lower activation bitwidth(<=8bit) should be experimented.
2. The novelty is limited. The core idea of decoupleQ is similar to Normalization(Batch-Norm or Layer-Norm). The learnable floating part of decoupleQ equals to a learnable Normalization parameters.
3. More existing Quantization methods should be compared, such as, NWQ[1], PD-Quant[2]

[1] Leveraging Inter-Layer Dependency for Post -Training Quantization 
[2] PD-Quant: Post-Training Quantization based on Prediction Difference Metric.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents decoupleQ, a post-training quantization method that improves the accuracy of quantized models, particularly at very low bit-widths (2-bit). It achieves this by separating model parameters into integer and floating-point components and formulating the quantization process as a constrained optimization problem. This approach eliminates the need for traditional quantization techniques like outlier handling and focuses on optimizing the core objective.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The paper introduces a fresh perspective on quantization by abandoning traditional methods and reframing it as a constrained optimization problem.
2. decoupleQ demonstrates impressive results in 2-bit quantization, achieving accuracy comparable to higher precision formats like fp16/bf16 in large speech models.
3. The quantization process is linear and uniform, making it easier to implement in hardware compared to non-uniform methods.

Weaknesses:
1. The paper's writing lacks cohesion and clarity regarding its ultimate goal. The paper also has several spelling mistakes.
2. The authors claim to separate the model parameters into integers and floating-point components. However, as far as I understand, this practice is not a novel contribution but rather a common approach in quantization.
3. They address a portion of the optimization problem using GPTQ and another portion similar to BRECQ.
4. The authors acknowledge that their solution may not be optimal. 
5. The quantization process in decoupleQ can be more time-consuming than other methods.

Limitations:
Yes, however, it will be beneficial to divide the current Discussion section into separate Conclusion and Limitations sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel post-training quantization method to achieve 2-bit uniform quantization on large language and speech models. The proposed method decouples the quantized values into integer and floating-point parts, which are then optimized via a constrained optimization problem that can be solved with off-the-shelf solutions. The proposed method allows uniform quantization down to extreme bits.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper proposes a novel optimization-based method to conduct PTQ on large models. The proposed method is solid and unique from previous methods.
2. The proposed method achieves good performance with only uniform quantization, without special procedure for outliers etc., providing direct benefit to the runtime of the quantized model on general hardware.
3. The limitations and future directions are clearly discussed in the paper.

Weaknesses:
1. The distinction between the proposed decoupleQ and the traditional quantization methods are not clearly derived in Sec. 3.2. The statement that ""(s,z) lost the traditional meaning"" on line 138 is not clear. My understanding is that W, s, and z are now totally independent of the original weight w0 in the optimization process, as long as the final output error is minimized? I think adding a comparison with the optimization objective/procedure of the traditional quantization here will help.
2. The proposed method appears to be sensitive to the size of the calibration set, so that the calibration size reported in the experiments are much larger than that of the previous baselines. As it is understandable that the optimization process may require more data to avoid overfitting, it would be more fair if the baseline methods are also calibrated with the same dataset/training cost.
3. For the LLM experiments, only ppl is used as metric. However, the ppl has been shown to be an inaccurate metric to reflect the utility of the LLM after compression. More evaluations such as zero-shot performance on downstream tasks and the instruction following ability etc., as in SqueezeLLM and OmniQuant papers, would be helpful to see if the quantized model still retains the ability as the FP one.

Limitations:
The limitations and potential social impacts are adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
fTOw3BzcWs;"REVIEW 
Summary:
The paper introduces ExID, an offline reinforcement learning algorithm that enhances learning performance in limited data scenarios by combining domain knowledge in the form of simple decision trees with agent experience replay data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Domain Knowledge Utilization: ExID incorporates domain knowledge to guide decision-making in data-limited scenarios
* Teacher-Student Architecture: A teacher network, informed by domain knowledge, regularizes a student critic network to improve generalization.
* Regularization with Domain Knowledge: The algorithm uses a regularization term to align the critic's decisions with the teacher's advice for states covered by domain knowledge.

Weaknesses:
* Discrete Action Space Limitation: The algorithm is currently limited to discrete action spaces, necessitating future extensions for continuous action domains.
* Hyperparameter Tuning Challenge: The need for precise hyperparameter tuning complicates the deployment of ExID in scenarios where extensive optimization is impractical.
* The paper does not have enough strong experiment comparisions. The methods of the paper is related with offline RL methods, such as SCQ[1], ReDS[2], A2PR[3], CPED[4]. But it lacks the experiments comparisions with offlien RL methods. I think adding some SOTA baseline methods will improve your paper. It is not required that experimental comparisons must be given, but at least add some discussion with these methods to the paper.  

References：

[1] Shimizu, Yutaka, et al. ""Strategically Conservative Q-Learning."" arXiv preprint arXiv:2406.04534 (2024).

[2] Singh, Anikait, et al. ""ReDS: offline reinforcement learning with heteroskedastic datasets via support constraints."" Proceedings of the 37th International Conference on Neural Information Processing Systems. 2023.

[3] Liu, Tenglong, et al. ""Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning."" In International Conference on Machine Learning (ICML). PMLR, 2024.

[4] Zhang, Jing, et al. ""Constrained policy optimization with explicit behavior density for offline reinforcement learning."" Advances in Neural Information Processing Systems. 2023

Limitations:
* The paper only conducts experiments in several simulated environments and a real-world sales promotion dataset, which may not fully verify the effectiveness and applicability of the algorithm in more diverse and complex real-world scenarios.
* The performance of the ExID algorithm heavily relies on the quality of the domain knowledge. If the domain knowledge is incomplete, inaccurate, or biased, it may mislead the learning process and result in suboptimal policies. Moreover, obtaining high-quality domain knowledge can be challenging and time-consuming in practice.
* The proposed method mainly concentrates on discrete action spaces, and its performance and applicability in continuous action spaces are not clear. This limits the algorithm's utility in many real-world control tasks that involve continuous action spaces.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies offline RL when data is limited. The authors propose a domain knowledge-based regularization technique to learn from an initial tracker network and limited data buffer. The experiments verified the effectiveness of the proposal, which outperforms the classic RL baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is simple and technically reasonable.
2. The experimental results on the real sales promotion dataset show the proposal is a promising solution in real-world applications.

Weaknesses:
1. The technical novelty is limited. Despite the claimed use of expert knowledge, the method adopted by the paper is to directly train a policy from the knowledge, which assumes that the information provided by the domain knowledge is at the state-action level (a decision tree in this paper), which limits the feasibility of this method. Compared to the use of knowledge between latent concepts discussed in neuro-symbolic learning, I think it's more like traditional model distillation. 
2. In practice, limited offline data may come from domain knowledge-based strategies, such as human-designed rules, thus I have great concerns about whether these two can promote each other. Empirical studies on more real-world datasets or rigorous theoretical analysis will provide support to this issue and further improve this work.  
3. The introduction uses the sales task as an example, but the visualization is based on the Mountain Car dataset.
4. Definition 4.1 seems strange, why not directly define the offline dataset as a subset of the complete state spaces?
5. The $\eta$ in Proposition 4.2 is not well defined.

Limitations:
The authors have provided a discussion about the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel technique ExID, a domain knowledge-based regularization method, that adaptively refines initial domain knowledge to boost performance of offline reinforcement learning (RL) in limited-data scenarios. The key insight is leveraging a teacher policy, trained with domain knowledge, to guide the learning process of the offline-optimized RL agent (student policy). This mitigates the issue of erroneous actions in sparse samples and unobserved states by having the domain knowledge-induced teacher network to cover them. And the initial domain knowledge would be improved when the student policy reaches a better perform than the teacher policy.  Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase compared to traditional offline RL algorithms operating on limited data

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Originality: The paper's originality lies in its integration of domain knowledge into offline RL through a teacher policy network. This approach addresses performance degradation in limited-data settings, which is a novel and underexplored area. The introduction of the domain knowledge-based regularization technique and adaptive refinement of initial domain knowledge are particularly innovative.

2. Quality: The quality of the work is evidenced by the solid theoretical analysis and the thorough empirical evaluations conducted on multiple standard datasets, including OpenAI Gym environments (Mountain Car, Cart-Pole, Lunar Lander) and MiniGrid environments, as well as a real-world sales promotion dataset. The results consistently show that ExID outperforms existing offline RL algorithms in these settings.

3. Clarity: The paper is well-structured, with clear explanations of the problem, methodology, and results. The use of diagrams and tables helps understand the motivation of the problem (figure 1), the proposed method (figure 2), illustrate the effectiveness of ExID (Table 1-2). Each section logically follows from the previous one, making the overall argument easy to follow.

4. Significance: By tackling the challenge of limited data in offline RL, the paper makes a significant contribution to the field. The proposed approach has practical implications for various real-world applications where data is scarce and expert knowledge is available, such as in business, healthcare, and robotics.

Weaknesses:
1. Generalization to Continuous Domains: The paper is limited to discrete action spaces, which restricts its applicability to a broader range of RL problems involving continuous action spaces. This limitation is acknowledged by the authors.


2. Scalability: The scalability of ExID to more complex environments that requires a complex representation (e.g., a significant large tree) of domain knowledge is not thoroughly explored.  It would be beneficial to understand how the method performs in such settings and what challenges might arise because the challenging of updating the domain knowledge represented in a complex representation could hinder the learning process of the student policy in the proposed method ExID.

Limitations:
The authors acknowledge several limitations of their work, including the reliance on the quality of domain knowledge and the focus on discrete action spaces. While these limitations are well-addressed in the paper, it may be worth to consider a broad evaluation:
   * Conducting experiments on a wider variety of environments that have larger state and action spaces, would provide a more comprehensive evaluation of the method's applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ExID, an offline reinforcement learning algorithm that enhances learning performance in limited data scenarios by combining domain knowledge in the form of simple decision trees with agent experience replay data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Domain Knowledge Utilization: ExID incorporates domain knowledge to guide decision-making in data-limited scenarios
* Teacher-Student Architecture: A teacher network, informed by domain knowledge, regularizes a student critic network to improve generalization.
* Regularization with Domain Knowledge: The algorithm uses a regularization term to align the critic's decisions with the teacher's advice for states covered by domain knowledge.

Weaknesses:
* Discrete Action Space Limitation: The algorithm is currently limited to discrete action spaces, necessitating future extensions for continuous action domains.
* Hyperparameter Tuning Challenge: The need for precise hyperparameter tuning complicates the deployment of ExID in scenarios where extensive optimization is impractical.
* The paper does not have enough strong experiment comparisions. The methods of the paper is related with offline RL methods, such as SCQ[1], ReDS[2], A2PR[3], CPED[4]. But it lacks the experiments comparisions with offlien RL methods. I think adding some SOTA baseline methods will improve your paper. It is not required that experimental comparisons must be given, but at least add some discussion with these methods to the paper.  

References：

[1] Shimizu, Yutaka, et al. ""Strategically Conservative Q-Learning."" arXiv preprint arXiv:2406.04534 (2024).

[2] Singh, Anikait, et al. ""ReDS: offline reinforcement learning with heteroskedastic datasets via support constraints."" Proceedings of the 37th International Conference on Neural Information Processing Systems. 2023.

[3] Liu, Tenglong, et al. ""Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning."" In International Conference on Machine Learning (ICML). PMLR, 2024.

[4] Zhang, Jing, et al. ""Constrained policy optimization with explicit behavior density for offline reinforcement learning."" Advances in Neural Information Processing Systems. 2023

Limitations:
* The paper only conducts experiments in several simulated environments and a real-world sales promotion dataset, which may not fully verify the effectiveness and applicability of the algorithm in more diverse and complex real-world scenarios.
* The performance of the ExID algorithm heavily relies on the quality of the domain knowledge. If the domain knowledge is incomplete, inaccurate, or biased, it may mislead the learning process and result in suboptimal policies. Moreover, obtaining high-quality domain knowledge can be challenging and time-consuming in practice.
* The proposed method mainly concentrates on discrete action spaces, and its performance and applicability in continuous action spaces are not clear. This limits the algorithm's utility in many real-world control tasks that involve continuous action spaces.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies offline RL when data is limited. The authors propose a domain knowledge-based regularization technique to learn from an initial tracker network and limited data buffer. The experiments verified the effectiveness of the proposal, which outperforms the classic RL baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is simple and technically reasonable.
2. The experimental results on the real sales promotion dataset show the proposal is a promising solution in real-world applications.

Weaknesses:
1. The technical novelty is limited. Despite the claimed use of expert knowledge, the method adopted by the paper is to directly train a policy from the knowledge, which assumes that the information provided by the domain knowledge is at the state-action level (a decision tree in this paper), which limits the feasibility of this method. Compared to the use of knowledge between latent concepts discussed in neuro-symbolic learning, I think it's more like traditional model distillation. 
2. In practice, limited offline data may come from domain knowledge-based strategies, such as human-designed rules, thus I have great concerns about whether these two can promote each other. Empirical studies on more real-world datasets or rigorous theoretical analysis will provide support to this issue and further improve this work.  
3. The introduction uses the sales task as an example, but the visualization is based on the Mountain Car dataset.
4. Definition 4.1 seems strange, why not directly define the offline dataset as a subset of the complete state spaces?
5. The $\eta$ in Proposition 4.2 is not well defined.

Limitations:
The authors have provided a discussion about the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel technique ExID, a domain knowledge-based regularization method, that adaptively refines initial domain knowledge to boost performance of offline reinforcement learning (RL) in limited-data scenarios. The key insight is leveraging a teacher policy, trained with domain knowledge, to guide the learning process of the offline-optimized RL agent (student policy). This mitigates the issue of erroneous actions in sparse samples and unobserved states by having the domain knowledge-induced teacher network to cover them. And the initial domain knowledge would be improved when the student policy reaches a better perform than the teacher policy.  Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase compared to traditional offline RL algorithms operating on limited data

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Originality: The paper's originality lies in its integration of domain knowledge into offline RL through a teacher policy network. This approach addresses performance degradation in limited-data settings, which is a novel and underexplored area. The introduction of the domain knowledge-based regularization technique and adaptive refinement of initial domain knowledge are particularly innovative.

2. Quality: The quality of the work is evidenced by the solid theoretical analysis and the thorough empirical evaluations conducted on multiple standard datasets, including OpenAI Gym environments (Mountain Car, Cart-Pole, Lunar Lander) and MiniGrid environments, as well as a real-world sales promotion dataset. The results consistently show that ExID outperforms existing offline RL algorithms in these settings.

3. Clarity: The paper is well-structured, with clear explanations of the problem, methodology, and results. The use of diagrams and tables helps understand the motivation of the problem (figure 1), the proposed method (figure 2), illustrate the effectiveness of ExID (Table 1-2). Each section logically follows from the previous one, making the overall argument easy to follow.

4. Significance: By tackling the challenge of limited data in offline RL, the paper makes a significant contribution to the field. The proposed approach has practical implications for various real-world applications where data is scarce and expert knowledge is available, such as in business, healthcare, and robotics.

Weaknesses:
1. Generalization to Continuous Domains: The paper is limited to discrete action spaces, which restricts its applicability to a broader range of RL problems involving continuous action spaces. This limitation is acknowledged by the authors.


2. Scalability: The scalability of ExID to more complex environments that requires a complex representation (e.g., a significant large tree) of domain knowledge is not thoroughly explored.  It would be beneficial to understand how the method performs in such settings and what challenges might arise because the challenging of updating the domain knowledge represented in a complex representation could hinder the learning process of the student policy in the proposed method ExID.

Limitations:
The authors acknowledge several limitations of their work, including the reliance on the quality of domain knowledge and the focus on discrete action spaces. While these limitations are well-addressed in the paper, it may be worth to consider a broad evaluation:
   * Conducting experiments on a wider variety of environments that have larger state and action spaces, would provide a more comprehensive evaluation of the method's applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
eYUbBKxABP;"REVIEW 
Summary:
The paper presents a formalization of fairness metrics intended to ease analysis of discrimination by automated decision making systems in the UK. While there is a relatively applied angle, the bulk of the contribution is intended to be a generic and re-targetable mathematical formalism.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper shows significant strength in its understanding of nuance with the way law works–something that is sorely missing from the vast majority of CS papers that attempt to handle legal concepts. I was very pleased overall by the mapping the authors performed between relevant legal concepts in the UK and their formal model of fairness. The bulk of the contribution here is in the modelling–which while it results in a simple formulation, should not be taken to undercut the value of the contribution.

Non-US legal contexts often get left out of the literature, even common law jurisdictions–yet they impact a significant number of people, and this work takes formalising fairness across that rubicon.

Weaknesses:
I do not have any major scientific critiques, though there were areas where the clarity of the paper could improve.

Lines 240-274 were written in harder to parse prose than the bulk of the rest of the paper. I had to reread that area multiple times.

The case study in Appendix A was actually very useful for understanding the authors' formalism and it is a shape that some of that context was not woven into the paper as concrete examples of how to understand the math.

The discussion on proxy discrimination never seemed to finish? I wasn't able to understand its meaning under UK law.


Missing a ref to Homer on L299.

All these are very minor issues. I'm substantially in favour of accepting this paper.

Limitations:
Ultimately, adherence to a formalism is *not* what courts generally take into account. While statistical analyses may be used to advance a given line of argument, the standards used are open-textured–and this is an inherent limitation of this line of work.
It also would have been good to see where this formalism sits under EU law (or representative EU-member law) or perhaps a discussion of how civil law jurisdictions handle these sorts of issues.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper maps existing literature and law on algorithmic fairness onto a decision-theoretic framework. It describes various desiderata (e.g. statistical parity) and legal restrictions (e.g., legitimate aims) in terms of expectations, distributions, estimation error, etc.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written and survey a large literature. It appears to state legal tests (particularly under U.K.) with care, while being careful not to overclaim about what its definitions actually establish.

Weaknesses:
n/a

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
- There is a gap between the definitions of fairness studied in the computer science literature, and the definitions of fairness operationalized by courts adjudicating discrimination claims. This limits the usefulness of the CS definitions.
- Amongst work attempting to reconcile legal and computational definitions of fairness, little has focused on anti-discrimination law outside the US.
- This paper makes four contributions in this context:
    - (1) It formalizes elements of anti-discrimination law into a decision-theoretic formalism
    - (2) If analyzes the legal role of the data-generation process
    - (3) It proposes conditional estimation parity as a legally-informed target
    - (4) It provides recommendations on creating SML models that minimize the risk of unlawful discrimination in automated decision-making

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The paper’s focus is interesting–the fairness literature is biased towards the US, and I imagine most fairness researchers would be unaware of subtle differences between UK and US anti-discrimination law.
- Because UK law is influential around the world, understanding how it regulates fairness in algorithmic systems has global importance.

Weaknesses:
- Much of the paper reads like a review of anti-discrimination law. This makes it difficult to parse out (1) what the technical contributions are, (2) why they’re novel, and (3) why they matter. 
- It’s extremely unclear what the technical payoff of the paper’s modeling choices are. The fairness field is overwhelmed with different definitions/frameworks. Why is the one proposed by the author’s meaningful over others? 
- It seems like an essential point to the paper’s argument is that prior work hasn’t studied UK anti-discrimination law. But if the paper wants to successfully extend that into an argument about modeling choices, I think it needs to explain why the existing definitions of fairness do not work for UK law.
- The recommendations provided are extremely general. Are these new or different from the many recommendations that already exist in the fairness/responsible AI literature?

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issues around existing fairness metrics and bias detection/mitigation methods not corresponding with legal notions of fairness, specifically under UK anti-discrimination law. The authors propose a theoretical framework for a data-generating process that aims to formalise the legitimacy of decisions and features in the data. Further, they propose a new metric ""conditional estimation parity"" which compares estimation errors for different protected groups.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well written and coherent. It translates potentially inaccessible legal scholarship and discussions clearly for a technical audience.
2. There is interesting discussion and the paper combines existing literature well. Although these discussions are not particularly novel, UK Equality Law in particular is rarely discussed and the investigations done here are useful to extend the literature for this niche.
3. The work addresses some big limitations in existing literature such as existing fairness metrics not aligning with legal notions of discrimination, particularly under non-US regulations, not considering context of what features are legitimate for an application or considering the estimation errors of decisions.

Weaknesses:
1. A lot of the paper is background or a collation of existing literature. The main contribution is the new conditional estimation metric metric but this metric relies on the true DGP and evaluating the estimation error which, as stated, can be complex in practice. This could make it difficult to use the metric in practice.
2. I understand it would be hard to use the metric for evaluating discrimination in existing datasets for the reasons specified above and also due to the inherent context-dependency of the metric (which is a benefit) but it could be useful to include some experimentation or results in a hypothetical scenario to show how it might be used in practice. As there are no results as such to comment on, it is difficult to assess it's significance.
3. The conclusions drawn such as ""Assess data legitimacy"" or ""Build an accurate model"", although justified with evidence in the paper, are not novel and are pretty standard, common-sense recommendations. 
4. Overall, the main novel contribution is the new metric but this is a small part of the paper. The rest of the paper is a nice collation and narrative of existing literature but I am not sure it significantly advances the field.

Other comments:
1. I can't see where SML terminology is introduced - I assume this means supervised machine learning?
2. In Section 1.4, DGPs are mentioned for the first time. It would be useful to have some more background to them before this - what exactly is a DGP? I do not believe it is ever explained.

Limitations:
The authors are honest about the strengths and weaknesses of their work (although some are hidden away and not pointed towards in the checklist). It would be useful to improve the discussion of limitations in Section 1.4 as it only mentions the limitation of applicability only in the UK.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a UK-and-European-law-based view of anti-discrimination law as it relates to fair machine learning and automated decision systems. It does a good job laying out the doctrine, arguing correctly that work in this area to-date is very centered on US legal concepts such as disparate treatment vs. disparate impact. Although I am willing to believe that there are subtle differences that drive important aspects of fair ML analysis, as the paper claims, I think the specifics of these differences could be made much clearer and need to be for the paper to have the impact it should.

Of particular note, the paper is very well situated in the surrounding literature. Although this contextualization should make the contributions more clearly offset from prior work, as presented I find the opposite: it is difficult to tell what is new as a contribution here. For example, while the contributions are clearly identified in 1.4, I think it would aid the paper if they appeared higher in the intro and were clearer about what is new and why it matters. The example in Appendix A could be used as a running example to show where new concepts are needed and what about existing work does not capture this different legal regime. In particular, after claiming that disparate treatment/disparate impact are distinct to direct & indirect discrimination, the definitions given from 105-114 seem to align tightly to the former. And while I'm not a lawyer, I don't believe that disparate impact claims require a showing of intent under US law either, so I found that distinction somewhat confusing.

On the technical level, the discussion of the true data generating process should really be contextualized in the literature on measurement and construct validity, specifically with respect to work by Jacobs & Wallach, which in particular encompasses the material in 2.3 on estimation parity (at least in part). Also, the causal analysis components of the discussion of data generation could cite more of the work of Kohler-Hausmann and also Hu (one paper from these authors is cited, but others are also relevant and speak more directly to causality and counterfactual fairness claims).

As a final observation, although the ML community talks in terms of ""fair"" outcomes, it is often conceptually clearer (and more in line with legal analysis) to use the same techniques as tools for identifying ""unfair"" activities or outcomes. Phrasing some of the claims this way may condense some arguments and tighten the presentation overall. Related to this, the discussion of these tools as part of an overall practical strategy for risk management is important and should receive more attention. For example, it would be good to discuss how the measures proposed would be used in real legal analysis of an example, such as in litigation or a regulatory proceeding.

I was also a bit confused about the analysis of constructed proxies for protected variables in 2.7. I understand that it's necessary to look beyond a formalistic view of whether a specific attribute is considered, but what happens if the proxy for a protected attribute is (say) the sum of two legitimate attributes? Why is it good enough to use only legitimate features? Also, at 393-394 it might be valuable to look at the recent paper on ""Less Discriminatory Algorithms"" and compare the approaches and outlooks.

Incredibly minor: 
* There is a missing period at 81.
* At 284-288, there is a latent call to questions of ecological validity which could be made more explicit

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* Generalizing beyond the US legal context is important and valuable and this paper does a good job explaining the UK and related legal systems' approach to anti-discrimination law.
* The paper is well written and well situated in existing literature

Weaknesses:
* Novelty is at times hard to identify. I think it's there, but the claims on what it covers should be clearer. In particular, the discussion of the decision-theoretic framing seems a bit under-attended even though it's potentially very useful.
* Some important concepts are missed, notably theories of measurement and construct validity/reliability are at least partially re-invented when they should just be treated as background.

Limitations:
I believe the limitations are expressed well.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a formalization of fairness metrics intended to ease analysis of discrimination by automated decision making systems in the UK. While there is a relatively applied angle, the bulk of the contribution is intended to be a generic and re-targetable mathematical formalism.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper shows significant strength in its understanding of nuance with the way law works–something that is sorely missing from the vast majority of CS papers that attempt to handle legal concepts. I was very pleased overall by the mapping the authors performed between relevant legal concepts in the UK and their formal model of fairness. The bulk of the contribution here is in the modelling–which while it results in a simple formulation, should not be taken to undercut the value of the contribution.

Non-US legal contexts often get left out of the literature, even common law jurisdictions–yet they impact a significant number of people, and this work takes formalising fairness across that rubicon.

Weaknesses:
I do not have any major scientific critiques, though there were areas where the clarity of the paper could improve.

Lines 240-274 were written in harder to parse prose than the bulk of the rest of the paper. I had to reread that area multiple times.

The case study in Appendix A was actually very useful for understanding the authors' formalism and it is a shape that some of that context was not woven into the paper as concrete examples of how to understand the math.

The discussion on proxy discrimination never seemed to finish? I wasn't able to understand its meaning under UK law.


Missing a ref to Homer on L299.

All these are very minor issues. I'm substantially in favour of accepting this paper.

Limitations:
Ultimately, adherence to a formalism is *not* what courts generally take into account. While statistical analyses may be used to advance a given line of argument, the standards used are open-textured–and this is an inherent limitation of this line of work.
It also would have been good to see where this formalism sits under EU law (or representative EU-member law) or perhaps a discussion of how civil law jurisdictions handle these sorts of issues.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper maps existing literature and law on algorithmic fairness onto a decision-theoretic framework. It describes various desiderata (e.g. statistical parity) and legal restrictions (e.g., legitimate aims) in terms of expectations, distributions, estimation error, etc.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written and survey a large literature. It appears to state legal tests (particularly under U.K.) with care, while being careful not to overclaim about what its definitions actually establish.

Weaknesses:
n/a

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
- There is a gap between the definitions of fairness studied in the computer science literature, and the definitions of fairness operationalized by courts adjudicating discrimination claims. This limits the usefulness of the CS definitions.
- Amongst work attempting to reconcile legal and computational definitions of fairness, little has focused on anti-discrimination law outside the US.
- This paper makes four contributions in this context:
    - (1) It formalizes elements of anti-discrimination law into a decision-theoretic formalism
    - (2) If analyzes the legal role of the data-generation process
    - (3) It proposes conditional estimation parity as a legally-informed target
    - (4) It provides recommendations on creating SML models that minimize the risk of unlawful discrimination in automated decision-making

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The paper’s focus is interesting–the fairness literature is biased towards the US, and I imagine most fairness researchers would be unaware of subtle differences between UK and US anti-discrimination law.
- Because UK law is influential around the world, understanding how it regulates fairness in algorithmic systems has global importance.

Weaknesses:
- Much of the paper reads like a review of anti-discrimination law. This makes it difficult to parse out (1) what the technical contributions are, (2) why they’re novel, and (3) why they matter. 
- It’s extremely unclear what the technical payoff of the paper’s modeling choices are. The fairness field is overwhelmed with different definitions/frameworks. Why is the one proposed by the author’s meaningful over others? 
- It seems like an essential point to the paper’s argument is that prior work hasn’t studied UK anti-discrimination law. But if the paper wants to successfully extend that into an argument about modeling choices, I think it needs to explain why the existing definitions of fairness do not work for UK law.
- The recommendations provided are extremely general. Are these new or different from the many recommendations that already exist in the fairness/responsible AI literature?

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issues around existing fairness metrics and bias detection/mitigation methods not corresponding with legal notions of fairness, specifically under UK anti-discrimination law. The authors propose a theoretical framework for a data-generating process that aims to formalise the legitimacy of decisions and features in the data. Further, they propose a new metric ""conditional estimation parity"" which compares estimation errors for different protected groups.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well written and coherent. It translates potentially inaccessible legal scholarship and discussions clearly for a technical audience.
2. There is interesting discussion and the paper combines existing literature well. Although these discussions are not particularly novel, UK Equality Law in particular is rarely discussed and the investigations done here are useful to extend the literature for this niche.
3. The work addresses some big limitations in existing literature such as existing fairness metrics not aligning with legal notions of discrimination, particularly under non-US regulations, not considering context of what features are legitimate for an application or considering the estimation errors of decisions.

Weaknesses:
1. A lot of the paper is background or a collation of existing literature. The main contribution is the new conditional estimation metric metric but this metric relies on the true DGP and evaluating the estimation error which, as stated, can be complex in practice. This could make it difficult to use the metric in practice.
2. I understand it would be hard to use the metric for evaluating discrimination in existing datasets for the reasons specified above and also due to the inherent context-dependency of the metric (which is a benefit) but it could be useful to include some experimentation or results in a hypothetical scenario to show how it might be used in practice. As there are no results as such to comment on, it is difficult to assess it's significance.
3. The conclusions drawn such as ""Assess data legitimacy"" or ""Build an accurate model"", although justified with evidence in the paper, are not novel and are pretty standard, common-sense recommendations. 
4. Overall, the main novel contribution is the new metric but this is a small part of the paper. The rest of the paper is a nice collation and narrative of existing literature but I am not sure it significantly advances the field.

Other comments:
1. I can't see where SML terminology is introduced - I assume this means supervised machine learning?
2. In Section 1.4, DGPs are mentioned for the first time. It would be useful to have some more background to them before this - what exactly is a DGP? I do not believe it is ever explained.

Limitations:
The authors are honest about the strengths and weaknesses of their work (although some are hidden away and not pointed towards in the checklist). It would be useful to improve the discussion of limitations in Section 1.4 as it only mentions the limitation of applicability only in the UK.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a UK-and-European-law-based view of anti-discrimination law as it relates to fair machine learning and automated decision systems. It does a good job laying out the doctrine, arguing correctly that work in this area to-date is very centered on US legal concepts such as disparate treatment vs. disparate impact. Although I am willing to believe that there are subtle differences that drive important aspects of fair ML analysis, as the paper claims, I think the specifics of these differences could be made much clearer and need to be for the paper to have the impact it should.

Of particular note, the paper is very well situated in the surrounding literature. Although this contextualization should make the contributions more clearly offset from prior work, as presented I find the opposite: it is difficult to tell what is new as a contribution here. For example, while the contributions are clearly identified in 1.4, I think it would aid the paper if they appeared higher in the intro and were clearer about what is new and why it matters. The example in Appendix A could be used as a running example to show where new concepts are needed and what about existing work does not capture this different legal regime. In particular, after claiming that disparate treatment/disparate impact are distinct to direct & indirect discrimination, the definitions given from 105-114 seem to align tightly to the former. And while I'm not a lawyer, I don't believe that disparate impact claims require a showing of intent under US law either, so I found that distinction somewhat confusing.

On the technical level, the discussion of the true data generating process should really be contextualized in the literature on measurement and construct validity, specifically with respect to work by Jacobs & Wallach, which in particular encompasses the material in 2.3 on estimation parity (at least in part). Also, the causal analysis components of the discussion of data generation could cite more of the work of Kohler-Hausmann and also Hu (one paper from these authors is cited, but others are also relevant and speak more directly to causality and counterfactual fairness claims).

As a final observation, although the ML community talks in terms of ""fair"" outcomes, it is often conceptually clearer (and more in line with legal analysis) to use the same techniques as tools for identifying ""unfair"" activities or outcomes. Phrasing some of the claims this way may condense some arguments and tighten the presentation overall. Related to this, the discussion of these tools as part of an overall practical strategy for risk management is important and should receive more attention. For example, it would be good to discuss how the measures proposed would be used in real legal analysis of an example, such as in litigation or a regulatory proceeding.

I was also a bit confused about the analysis of constructed proxies for protected variables in 2.7. I understand that it's necessary to look beyond a formalistic view of whether a specific attribute is considered, but what happens if the proxy for a protected attribute is (say) the sum of two legitimate attributes? Why is it good enough to use only legitimate features? Also, at 393-394 it might be valuable to look at the recent paper on ""Less Discriminatory Algorithms"" and compare the approaches and outlooks.

Incredibly minor: 
* There is a missing period at 81.
* At 284-288, there is a latent call to questions of ecological validity which could be made more explicit

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* Generalizing beyond the US legal context is important and valuable and this paper does a good job explaining the UK and related legal systems' approach to anti-discrimination law.
* The paper is well written and well situated in existing literature

Weaknesses:
* Novelty is at times hard to identify. I think it's there, but the claims on what it covers should be clearer. In particular, the discussion of the decision-theoretic framing seems a bit under-attended even though it's potentially very useful.
* Some important concepts are missed, notably theories of measurement and construct validity/reliability are at least partially re-invented when they should just be treated as background.

Limitations:
I believe the limitations are expressed well.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
NMwPKjNTEP;"REVIEW 
Summary:
Unfortunately, the authors begin the manuscript by demonstrating a lack of knowledge about the topic. They claim that deep learning (DL) has been highly successful in the field of brain-computer interfaces (BCI) based on electroencephalogram (EEG) data. However, in reality, the application of deep learning in the BCI or EEG field is limited, and shallow learning with simple hand-engineered features is still the gold standard. Therefore, the paper's claims about the vulnerabilities of machine learning models seem to be more like science fiction and do not meet the standard of the NeurIPS.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Hard to spot any strength as this is an artificial toy example.

Weaknesses:
Lack of connection with real-world problems, especially the BCI and EEG fields, where shallow learning remains gold standards with non-existent vulnerabilities. ML in BCI has been trained for each subject at the bedside.

Limitations:
No application in the real world and a completely trivial problem below conference standards.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents an EEG backdoor for manipulating EEG BCI, called ManiBCI, where the adversary can arbitrarily control the output for any input samples. Experiments conducted on three EEG datasets demonstrate the effectiveness of ManiBCI; which easily bypass existing backdoor defenses.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- A backdoor attack for EEG BCI where the adversary can arbitrarily manipulate which target class the EEG BCI will misclassify without engaging the training stage.
- The use of EEG electrodes and frequencies in EEG backdoor attacks with reinforcement learning.
- Several experiments have been conducted to assess the proposed method.

Weaknesses:
- The proposed methodology is not well described. It mainly based on the application of Fourier transform and reinforcement learning.

Limitations:
Yes. The limitations were addressed in Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents ManiBCI, a novel backdoor attack method targeting EEG-based brain-computer interface (BCI) systems. ManiBCI leverages a three-stage clean label poisoning approach without needing access to the training phase of the target deep learning models. This method optimally selects EEG electrodes and frequency masks for each class using reinforcement learning. The attack involves injecting these learned masks into the EEG data, leading to high misclassification rates while maintaining the original task's accuracy. Extensive experiments on three EEG datasets demonstrate ManiBCI's effectiveness and robustness. The key contributions of this work are: (1) Introducing a new type of stealthy and effective backdoor attack for EEG data. (2) Proposing a method that can manipulate multiple classes simultaneously without requiring control over the model's training process. (3) Providing experimental evidence of the attack's success across various datasets. This research highlights potential vulnerabilities in EEG-based BCI systems, emphasizing the need for robust defense mechanisms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* Introduces a novel and stealthy backdoor attack method for EEG-based BCI systems using frequency transform.
* Demonstrates the ability to manipulate multiple target classes without needing access to the model's training phase.
* Provides strong experimental evidence of the method's effectiveness and robustness across multiple EEG datasets.

Weaknesses:
* Standard baselines (fast gradient sign method and universal adversarial perturbation) are not included for comparison [1][2]
* Limited to the datasets used in the experiments, raising questions about generalizability to other EEG datasets or real-world scenarios.
* The practical implementation of the proposed attack might be complex and computationally intensive due to the need for reinforcement learning optimization.

[1] Xiao Zhang and Dongrui Wu. On the vulnerability of CNN classifiers in EEG-based BCIs. IEEE
Transactions on Neural Systems and Rehabilitation Engineering, 27(5):814–825, 2019.
[2] Zihan Liu, Lubin Meng, Xiao Zhang, Weili Fang, and Dongrui Wu. Universal adversarial
perturbations for CNN classifiers in EEG-based BCIs, 2021.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a backdoor attack strategy for EEG, addressing three inherent issues: low quality, task variances, and morphology variances. The authors introduced a three-stage clean label poisoning attack. The proposed algorithm has been evaluated on three EEG datasets, demonstrating its effectiveness and robustness across datasets. This is an interesting work investigating backdoor attacks on EEG, and the customized strategy shows effectiveness in this particular domain. I believe this contribution will be beneficial to the community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This is a very interesting work, investigating backdoor attacks on EEG, and the customized strategy shows effectiveness in this particular domain.

* The experiments are relatively sufficient and validate the claimed contributions adequately.

Weaknesses:
* I am not the expertise in BA domain. In terms of general EEG analsyis, one of my main concern is the experiment settings. In normal EEG analysis domain, we usually set inter-subejct and intra-subject settings. I failed to see the calrifications of these experiment settings. Whether this strategy can work across subjects, and generalize on the EEG signals collected from new/unseen subject?

Limitations:
The limitations mentioned by the authors are appreciated.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Unfortunately, the authors begin the manuscript by demonstrating a lack of knowledge about the topic. They claim that deep learning (DL) has been highly successful in the field of brain-computer interfaces (BCI) based on electroencephalogram (EEG) data. However, in reality, the application of deep learning in the BCI or EEG field is limited, and shallow learning with simple hand-engineered features is still the gold standard. Therefore, the paper's claims about the vulnerabilities of machine learning models seem to be more like science fiction and do not meet the standard of the NeurIPS.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Hard to spot any strength as this is an artificial toy example.

Weaknesses:
Lack of connection with real-world problems, especially the BCI and EEG fields, where shallow learning remains gold standards with non-existent vulnerabilities. ML in BCI has been trained for each subject at the bedside.

Limitations:
No application in the real world and a completely trivial problem below conference standards.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents an EEG backdoor for manipulating EEG BCI, called ManiBCI, where the adversary can arbitrarily control the output for any input samples. Experiments conducted on three EEG datasets demonstrate the effectiveness of ManiBCI; which easily bypass existing backdoor defenses.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- A backdoor attack for EEG BCI where the adversary can arbitrarily manipulate which target class the EEG BCI will misclassify without engaging the training stage.
- The use of EEG electrodes and frequencies in EEG backdoor attacks with reinforcement learning.
- Several experiments have been conducted to assess the proposed method.

Weaknesses:
- The proposed methodology is not well described. It mainly based on the application of Fourier transform and reinforcement learning.

Limitations:
Yes. The limitations were addressed in Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents ManiBCI, a novel backdoor attack method targeting EEG-based brain-computer interface (BCI) systems. ManiBCI leverages a three-stage clean label poisoning approach without needing access to the training phase of the target deep learning models. This method optimally selects EEG electrodes and frequency masks for each class using reinforcement learning. The attack involves injecting these learned masks into the EEG data, leading to high misclassification rates while maintaining the original task's accuracy. Extensive experiments on three EEG datasets demonstrate ManiBCI's effectiveness and robustness. The key contributions of this work are: (1) Introducing a new type of stealthy and effective backdoor attack for EEG data. (2) Proposing a method that can manipulate multiple classes simultaneously without requiring control over the model's training process. (3) Providing experimental evidence of the attack's success across various datasets. This research highlights potential vulnerabilities in EEG-based BCI systems, emphasizing the need for robust defense mechanisms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* Introduces a novel and stealthy backdoor attack method for EEG-based BCI systems using frequency transform.
* Demonstrates the ability to manipulate multiple target classes without needing access to the model's training phase.
* Provides strong experimental evidence of the method's effectiveness and robustness across multiple EEG datasets.

Weaknesses:
* Standard baselines (fast gradient sign method and universal adversarial perturbation) are not included for comparison [1][2]
* Limited to the datasets used in the experiments, raising questions about generalizability to other EEG datasets or real-world scenarios.
* The practical implementation of the proposed attack might be complex and computationally intensive due to the need for reinforcement learning optimization.

[1] Xiao Zhang and Dongrui Wu. On the vulnerability of CNN classifiers in EEG-based BCIs. IEEE
Transactions on Neural Systems and Rehabilitation Engineering, 27(5):814–825, 2019.
[2] Zihan Liu, Lubin Meng, Xiao Zhang, Weili Fang, and Dongrui Wu. Universal adversarial
perturbations for CNN classifiers in EEG-based BCIs, 2021.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a backdoor attack strategy for EEG, addressing three inherent issues: low quality, task variances, and morphology variances. The authors introduced a three-stage clean label poisoning attack. The proposed algorithm has been evaluated on three EEG datasets, demonstrating its effectiveness and robustness across datasets. This is an interesting work investigating backdoor attacks on EEG, and the customized strategy shows effectiveness in this particular domain. I believe this contribution will be beneficial to the community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This is a very interesting work, investigating backdoor attacks on EEG, and the customized strategy shows effectiveness in this particular domain.

* The experiments are relatively sufficient and validate the claimed contributions adequately.

Weaknesses:
* I am not the expertise in BA domain. In terms of general EEG analsyis, one of my main concern is the experiment settings. In normal EEG analysis domain, we usually set inter-subejct and intra-subject settings. I failed to see the calrifications of these experiment settings. Whether this strategy can work across subjects, and generalize on the EEG signals collected from new/unseen subject?

Limitations:
The limitations mentioned by the authors are appreciated.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cCOpatbXFU;"REVIEW 
Summary:
This paper investigates a new definition for the stochastic gradient variance in mirror descent.
Most existing analyses for stochastic mirror descent require a strongly convex distance generating function to bound the gradient variance.
This limits the their applications especially when this assumption fails.
In particular, Le Priol et al. (2021) have shown that the none of the existing convergence rates applies to Gaussian maximum likelihood.

This paper aims to fix this issue by proposing a new definition of gradient variance.
They show that the new definition is strictly stronger (more likely to hold in practice) than existing definitions, and derive convergence rates in convex setting.
The authors demonstrate an application of the new variance definition bounding the estimation error of MAP for one-dimensional Gaussian distributions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Analyzing stochastic mirror descent is hard when the distance generating function is not strongly convex.
This paper is a step towards generalizing mirror descent analyses.
In particular, I like Section 2.2 where the authors show that the proposed definition is strictly better than existing ones.

- The mirror descent analysis in this paper yields a non-asymptotic bound for the estimation error of Gaussian MAP. This seems to be a fundamental problem lacking theoretical guarantees based on Le Priol et al. (2021).
However, I am now knowledgeable enough to confirm the significance or novelty of this result in statistics.

Weaknesses:
While developing the new gradient variance definition is certainly interesting, I have the following concerns.

- The authors have shown that their gradient variance \\(\sigma_{\star, \eta}^2\\) is finite for every fixed step size \\(\eta\\).
The convergence rates in the convex setting are proved using constant step sizes, and thus the optimality gap does not vanish.
To make the optimality gap vanish, diminishing step sizes are often required, which is not covered in this paper.
Proving convergence with diminishing step sizes probably requires characterizing the average variance \\(\frac1T \sum_{t=1}^{T} \sigma_{\star, \eta_t}^2\\), which I think can be done only on a case-by-case manner depending on the specific application.

- The only case so far where this new definition shines while all other definitions fail is maximum likelihood estimation for one-dimensional Gaussian distributions.
This is very restrictive.
Is it possible to generalize this result to multivariate Gaussian distributions?
In addition, it would be great if the authors could provide other applications to further justify the necessity of this new definition.

Minor:
- Line 192: Add a period.
- Bad notation in Section 4.2: It might be confusing to use $\Sigma$ to denote the standard deviation.
Consider using a different letter like $s$ or $\tau$.

Limitations:
NA.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Problem:
In proof of Proposition 2, by the definition of $\sigma_{*,\eta}^2$, we can obtain that
$ \sigma_{*,\eta}^2 = \frac{\min_x f(x) - \min_x f_\eta(x)}{\eta}  $. 
However,  $x_* =\argmin_x f(x)$ does not equal to $x_*' = \argmin_x f_{\eta}(x)$.
This will lead to $\sigma_{*,\eta}^2 \neq \frac{1}{\eta^2} D_h(x^*, x^+）$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Weaknesses:
No.

Limitations:
No

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new analysis of SMD using a newly introduced generalized variance notion. The benefit of the new analysis is demonstrated in the application to maximum a posteriori estimation of Gaussian parameters.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
After introducing a new variance notion, the paper delves into comparison with other existing notions and shows that the proposed one is the largest meaningful notion. After a careful comparison, analysis of SMD is presented using this mild assumption. This analysis substantially departs from the results known in the literature. The demonstration of the use case of this new theory in the context of statistical estimation is also clear and adds more significance to the new theory.

Weaknesses:
Major:

As explained after theorem 4.3, the guarantees are derived for a reverse KL and may not imply anything on the desired quantity $f(\theta) - f(\theta_*)$. This of course, limits the contribution in this application significantly as non-asymptotic rates were known before. 

Minor problems that I hope the authors can fix in the next revision. 

1. Is the set C compact? If not, why the minimum exists in Proposition 2.2?

2. Cannot find where $x_*$ is defined. Why does it exist? 

3. There is a small issue with indicies in equation (12) and in paragraph before. $\eta_{n} = \frac{1}{n_0+n+1}$, and the stochastic gradient should depend on the new sample $X_{n+1}$.

Update: meaningful results are obtained only for relatively strongly convex case (which is a stronger assumption than even strong convexity). In the convex case, a different (much stronger) definition is used. This becomes clear only after reading Appendix D. This limitation should be clarified in section 3.2, where convergence on some surrogate loss is shown. I will update my evalutation.

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new variance assumption for the analysis of stochastic mirror descent (SMD) to handle cases where standard bounded variance assumption does not hold. The authors show this new assumption can be shown to hold under some regularity assumptions. The authors use the new results to show some convergence guarantees for MLE and MAP of a Gaussian with unknown mean and variance using the connection between this problem and SMD convergence guarantees.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The topic is definitely interesting and timely. Results for stochastic optimization without bounded variance assumptions are quite interesting. As shown in the prior literature, this task is especially subtle in the Bregman case. As the authors argue in detail, this difficulty is acknowledged in previous works such as [7] and [17]. It is neat that the authors show the importance of the new results by deriving convergence bounds for MAP/MLE of a Gaussian with unknown mean and variance by using the connection between these bounds and SMD in [17] (which itself is a nice connection). This adds a nice and clear motivation. The work makes some progress towards solving open questions from [17], while as the authors clearly explain, the open questions are still not completely solved.

Weaknesses:
I find the motivation of the paper and its application to MAP bounds interesting, however I have some concerns about writing and the strength of the derived results in the context of the application in Section 4. It seems necessary for the latter point to be clarified.

- Authors write after Theorem 4.3 that the open problem from [17] is not completely resolved because  the convergence is not shown for the desired quantity. In particular, the authors describe that the guarantee is for $D_A(\theta_*, \theta^{(n)})$ instead of $D_A(\theta^{(n)}, \theta_*) = f(\theta) - f(\theta_*)$. The authors then write that two quantities can be related asymptotically but they state: ""but we might also be able to exploit this control over the course of the iterations"". Can you make this point more precise? It is not clear to me what this last part is trying to describe. Is it meant to be understood as an open question or is it possible for the authors to derive the stronger result? Since the paper mentions at many places that showing convergence guarantees for MAP is an important contribution of the paper, it is important to justify the convergence metric used in the results for justifying the contribution of the paper fully.

- It might be better to replace MLE in the abstract to MAP since Section 4 is mostly about MAP.

- Abstract states a couple of times ""strong convergence"", I suggest to remove this since ""strong convergence"" has a precise meaning in infinite-dimensional optimization and usage in the abstract is confusing because of this. Clearly this is not how the authors are using this term, but it seems authors are using this as a subjective adjective, which is not necessary. By subjective, I mean that: how can one decide what convergence result is strong and what is not?

- Assumption 1 requires all $f_\xi$ are convex. This is rather strong since the standard assumption is $\mathbb{E} f_\xi$ to be convex. Can you discuss this more? According to Prop 4.1, this holds for the main application of the paper, but it might be worth discussing why componentwise convexity is needed.

Limitations:
The limitations are discussed clearly. The authors provided explanations after Theorem 3.3 and Theorem 4.3 to describe the limitations of their result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This submission studies stochastic mirror descent (SMD) under quite mild conditions on the mirror map and objective function. More specifically, there are a variety of SMD analysis in the literature, but virtually all of them require strong conditions on the mirror map (such as strong convexity) that do not hold in cases where we only have relative smoothness (and/or relative strong convexity) of the objective function with respect to the mirror map. The authors propose a definition of variance of SMD that is better behaved under minimal assumptions. They show how this new variance can be used to obtain general convergence results for SMD. Finally, they show how the new variance definition for SMD can show some kind of non-asymptotic convergence rates for MLE and MAP of Gaussian parameter estimation with unknown mean and covariance, making partial progress on a conjecture posed by Le Priol et at.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
This is an interesting paper that tackles a hard theoretical problem. I think it is of interest for researchers interested in mirror descent. The new definition of variance of SMD has interesting properties even under very mild assumptions, as the authors show when comparing the new definition with other definitions of SMD variance in the literature. Moreover, the results on Sec 4 already show how this is an interesting way to analyze SMD, and is likely to lead to follow-up work on the area.

So the strengths summarized in bullet points:
- Thorough comparison of new variance definition with other definitions in the literature and proof of finiteness under assumption 1. 
- General convergence theorems of SMD under mild assumptions that recover known results in the deterministic case, showing this is may be a ""natural"" variance definition for SMD and useful for our understanding of SMD.
- Partial progress towards the conjecture of Le Priol et al.

Weaknesses:
In its current form, I have one main concern with the paper:
- Despite what is written at the beginning of the paper, **Assumption 1** is NOT a blanket assumption used throughout the paper. In fact, it appears only section 2 uses assumption 1. The rest of the paper uses a weaker assumption that is never clearly stated, which makes it hard to understand when the results hold or not. 
This is likely to be a problem with presentation, but in its current form it is often not clear what are the assumption required at each point. Since the main point of the paper is to use a minimal number of assumptions, it is very important for those to be clearly stated. 

A minor weakness is the lack of an example besides MAP/MLE. I could not easily think of a concrete example where I could apply the convergence results in sec 3 or 4. If the authors have an example besides MLE or MAP (even if a bit artificial), it would be great. For example, some example with a mirror map such as $- \log x$ would be interesting, but this is a minor suggestion, since it would be nice to see a concrete example of the use of the results in Sec 2 (the results in Sec 4 require a specialized bound on the variance) 

Summary of weaknesses:
- Unclear requires assumptions for many of the results
- (Minor weakness) Lack of a concrete (even artificial) example of application of any of the theorems in Sec 3 beyond MAP/MLE (and the latter require specialized bounds on the variance).

Limitations:
Although the authors are not explicit about some of the limitations of the results on sec 3, they do discuss how to interpret some of the results and limitations from their convergence rates.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates a new definition for the stochastic gradient variance in mirror descent.
Most existing analyses for stochastic mirror descent require a strongly convex distance generating function to bound the gradient variance.
This limits the their applications especially when this assumption fails.
In particular, Le Priol et al. (2021) have shown that the none of the existing convergence rates applies to Gaussian maximum likelihood.

This paper aims to fix this issue by proposing a new definition of gradient variance.
They show that the new definition is strictly stronger (more likely to hold in practice) than existing definitions, and derive convergence rates in convex setting.
The authors demonstrate an application of the new variance definition bounding the estimation error of MAP for one-dimensional Gaussian distributions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Analyzing stochastic mirror descent is hard when the distance generating function is not strongly convex.
This paper is a step towards generalizing mirror descent analyses.
In particular, I like Section 2.2 where the authors show that the proposed definition is strictly better than existing ones.

- The mirror descent analysis in this paper yields a non-asymptotic bound for the estimation error of Gaussian MAP. This seems to be a fundamental problem lacking theoretical guarantees based on Le Priol et al. (2021).
However, I am now knowledgeable enough to confirm the significance or novelty of this result in statistics.

Weaknesses:
While developing the new gradient variance definition is certainly interesting, I have the following concerns.

- The authors have shown that their gradient variance \\(\sigma_{\star, \eta}^2\\) is finite for every fixed step size \\(\eta\\).
The convergence rates in the convex setting are proved using constant step sizes, and thus the optimality gap does not vanish.
To make the optimality gap vanish, diminishing step sizes are often required, which is not covered in this paper.
Proving convergence with diminishing step sizes probably requires characterizing the average variance \\(\frac1T \sum_{t=1}^{T} \sigma_{\star, \eta_t}^2\\), which I think can be done only on a case-by-case manner depending on the specific application.

- The only case so far where this new definition shines while all other definitions fail is maximum likelihood estimation for one-dimensional Gaussian distributions.
This is very restrictive.
Is it possible to generalize this result to multivariate Gaussian distributions?
In addition, it would be great if the authors could provide other applications to further justify the necessity of this new definition.

Minor:
- Line 192: Add a period.
- Bad notation in Section 4.2: It might be confusing to use $\Sigma$ to denote the standard deviation.
Consider using a different letter like $s$ or $\tau$.

Limitations:
NA.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Problem:
In proof of Proposition 2, by the definition of $\sigma_{*,\eta}^2$, we can obtain that
$ \sigma_{*,\eta}^2 = \frac{\min_x f(x) - \min_x f_\eta(x)}{\eta}  $. 
However,  $x_* =\argmin_x f(x)$ does not equal to $x_*' = \argmin_x f_{\eta}(x)$.
This will lead to $\sigma_{*,\eta}^2 \neq \frac{1}{\eta^2} D_h(x^*, x^+）$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Weaknesses:
No.

Limitations:
No

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new analysis of SMD using a newly introduced generalized variance notion. The benefit of the new analysis is demonstrated in the application to maximum a posteriori estimation of Gaussian parameters.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
After introducing a new variance notion, the paper delves into comparison with other existing notions and shows that the proposed one is the largest meaningful notion. After a careful comparison, analysis of SMD is presented using this mild assumption. This analysis substantially departs from the results known in the literature. The demonstration of the use case of this new theory in the context of statistical estimation is also clear and adds more significance to the new theory.

Weaknesses:
Major:

As explained after theorem 4.3, the guarantees are derived for a reverse KL and may not imply anything on the desired quantity $f(\theta) - f(\theta_*)$. This of course, limits the contribution in this application significantly as non-asymptotic rates were known before. 

Minor problems that I hope the authors can fix in the next revision. 

1. Is the set C compact? If not, why the minimum exists in Proposition 2.2?

2. Cannot find where $x_*$ is defined. Why does it exist? 

3. There is a small issue with indicies in equation (12) and in paragraph before. $\eta_{n} = \frac{1}{n_0+n+1}$, and the stochastic gradient should depend on the new sample $X_{n+1}$.

Update: meaningful results are obtained only for relatively strongly convex case (which is a stronger assumption than even strong convexity). In the convex case, a different (much stronger) definition is used. This becomes clear only after reading Appendix D. This limitation should be clarified in section 3.2, where convergence on some surrogate loss is shown. I will update my evalutation.

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new variance assumption for the analysis of stochastic mirror descent (SMD) to handle cases where standard bounded variance assumption does not hold. The authors show this new assumption can be shown to hold under some regularity assumptions. The authors use the new results to show some convergence guarantees for MLE and MAP of a Gaussian with unknown mean and variance using the connection between this problem and SMD convergence guarantees.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The topic is definitely interesting and timely. Results for stochastic optimization without bounded variance assumptions are quite interesting. As shown in the prior literature, this task is especially subtle in the Bregman case. As the authors argue in detail, this difficulty is acknowledged in previous works such as [7] and [17]. It is neat that the authors show the importance of the new results by deriving convergence bounds for MAP/MLE of a Gaussian with unknown mean and variance by using the connection between these bounds and SMD in [17] (which itself is a nice connection). This adds a nice and clear motivation. The work makes some progress towards solving open questions from [17], while as the authors clearly explain, the open questions are still not completely solved.

Weaknesses:
I find the motivation of the paper and its application to MAP bounds interesting, however I have some concerns about writing and the strength of the derived results in the context of the application in Section 4. It seems necessary for the latter point to be clarified.

- Authors write after Theorem 4.3 that the open problem from [17] is not completely resolved because  the convergence is not shown for the desired quantity. In particular, the authors describe that the guarantee is for $D_A(\theta_*, \theta^{(n)})$ instead of $D_A(\theta^{(n)}, \theta_*) = f(\theta) - f(\theta_*)$. The authors then write that two quantities can be related asymptotically but they state: ""but we might also be able to exploit this control over the course of the iterations"". Can you make this point more precise? It is not clear to me what this last part is trying to describe. Is it meant to be understood as an open question or is it possible for the authors to derive the stronger result? Since the paper mentions at many places that showing convergence guarantees for MAP is an important contribution of the paper, it is important to justify the convergence metric used in the results for justifying the contribution of the paper fully.

- It might be better to replace MLE in the abstract to MAP since Section 4 is mostly about MAP.

- Abstract states a couple of times ""strong convergence"", I suggest to remove this since ""strong convergence"" has a precise meaning in infinite-dimensional optimization and usage in the abstract is confusing because of this. Clearly this is not how the authors are using this term, but it seems authors are using this as a subjective adjective, which is not necessary. By subjective, I mean that: how can one decide what convergence result is strong and what is not?

- Assumption 1 requires all $f_\xi$ are convex. This is rather strong since the standard assumption is $\mathbb{E} f_\xi$ to be convex. Can you discuss this more? According to Prop 4.1, this holds for the main application of the paper, but it might be worth discussing why componentwise convexity is needed.

Limitations:
The limitations are discussed clearly. The authors provided explanations after Theorem 3.3 and Theorem 4.3 to describe the limitations of their result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This submission studies stochastic mirror descent (SMD) under quite mild conditions on the mirror map and objective function. More specifically, there are a variety of SMD analysis in the literature, but virtually all of them require strong conditions on the mirror map (such as strong convexity) that do not hold in cases where we only have relative smoothness (and/or relative strong convexity) of the objective function with respect to the mirror map. The authors propose a definition of variance of SMD that is better behaved under minimal assumptions. They show how this new variance can be used to obtain general convergence results for SMD. Finally, they show how the new variance definition for SMD can show some kind of non-asymptotic convergence rates for MLE and MAP of Gaussian parameter estimation with unknown mean and covariance, making partial progress on a conjecture posed by Le Priol et at.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
This is an interesting paper that tackles a hard theoretical problem. I think it is of interest for researchers interested in mirror descent. The new definition of variance of SMD has interesting properties even under very mild assumptions, as the authors show when comparing the new definition with other definitions of SMD variance in the literature. Moreover, the results on Sec 4 already show how this is an interesting way to analyze SMD, and is likely to lead to follow-up work on the area.

So the strengths summarized in bullet points:
- Thorough comparison of new variance definition with other definitions in the literature and proof of finiteness under assumption 1. 
- General convergence theorems of SMD under mild assumptions that recover known results in the deterministic case, showing this is may be a ""natural"" variance definition for SMD and useful for our understanding of SMD.
- Partial progress towards the conjecture of Le Priol et al.

Weaknesses:
In its current form, I have one main concern with the paper:
- Despite what is written at the beginning of the paper, **Assumption 1** is NOT a blanket assumption used throughout the paper. In fact, it appears only section 2 uses assumption 1. The rest of the paper uses a weaker assumption that is never clearly stated, which makes it hard to understand when the results hold or not. 
This is likely to be a problem with presentation, but in its current form it is often not clear what are the assumption required at each point. Since the main point of the paper is to use a minimal number of assumptions, it is very important for those to be clearly stated. 

A minor weakness is the lack of an example besides MAP/MLE. I could not easily think of a concrete example where I could apply the convergence results in sec 3 or 4. If the authors have an example besides MLE or MAP (even if a bit artificial), it would be great. For example, some example with a mirror map such as $- \log x$ would be interesting, but this is a minor suggestion, since it would be nice to see a concrete example of the use of the results in Sec 2 (the results in Sec 4 require a specialized bound on the variance) 

Summary of weaknesses:
- Unclear requires assumptions for many of the results
- (Minor weakness) Lack of a concrete (even artificial) example of application of any of the theorems in Sec 3 beyond MAP/MLE (and the latter require specialized bounds on the variance).

Limitations:
Although the authors are not explicit about some of the limitations of the results on sec 3, they do discuss how to interpret some of the results and limitations from their convergence rates.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
cRs4jvF4mO;"REVIEW 
Summary:
This paper proposes new methods, Kernel Density Forest (KDF) and Kernel Density Network (KDN), to address issues in confidence calibration for traditional deep learning models and random forests. The motivation stems from the existing literature that deep neural networks using ReLU tend to exhibit high confidence on out-of-distribution (OOD) data due to affine transformations. The proposed methods improve confidence calibration for both in-distribution (ID) and OOD data by partitioning the feature space into polytopes and replacing affine functions within each polytope with Gaussian kernels. Experimental results demonstrate that the proposed methods outperform existing techniques in terms of calibration performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality:
The approach of replacing affine functions within polytopes with Gaussian kernels is novel. The proposed methods address the confidence calibration problem for both ID and OOD data simultaneously, providing an integrated solution to these calibration issues.

Quality:
The theoretical proofs are robust, and the effectiveness of the proposed methods is validated through both simulations and real-world datasets.

Clarity:
The paper is written clearly and concisely.

Weaknesses:
Validity of Metrics:
The paper evaluates calibration using Maximum Calibration Error (MCE) for ID data, but does not justify the use of MCE over Expected Calibration Error (ECE) or Adaptive Calibration Error (ACE)[1]. A more detailed explanation and comparison of these metrics would enhance the paper's credibility. Additionally, the definition and justification for OCE (Out-of-distribution Calibration Error) would benefit from a similar comparison with ACE.

[1] https://arxiv.org/abs/1904.01685

Experiments:
To emphasize the effectiveness of the proposed methods, a comparison of execution times would be beneficial, especially since practical applications like web Click-Through Rate (CTR) estimation place significant importance on runtime. The paper should clarify what the noise in Table 1 represents. It would also be advantageous to include experiments on larger and more varied datasets, as well as an evaluation of the methods' performance when combined with in-training calibration methods, which are commonly used alongside post-hoc calibration methods.

Limitations:
This paper mentions computational complexity and limitations in practical applications, but lacks detailed experimental results to support these claims. Including such data would provide valuable insights for future research and implementation.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel approach for OOD detection by learning a posterior distribution that is calibrated for both ID and OOD individuals. It models the class-wise conditional distribution of features by a gaussian kernel respectively for a set of polytopes that cover the feature space. The tail property of gaussian kernels contribute to both ID and OOD calibration. Empirical evidence shows the power of the proposed algorithm across tabular and vision dataset under both ID and OOD settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well motivated from the tradeoff of ID calibration and OOD calibration for current approaches for OOD detection methods. The technique of gaussian kernel has a clear geometric intuitive. Compared to affine functions, the tail property ensures that the posterior distribution converges to the prior of labels when a OOD sample deviates far enough from the training support, as proved in Proposition 2. On the other hand, the interpolation by gaussian kernels between neighboring polytopes contributes to ID calibration.

Weaknesses:
The major concern is insufficient discussion over the research context of the paper, which renders it hard to precisely evaluate the contribution. The related work section is short. Section 2 shows that ""OOD detection"" is the closest area to this paper, but this keyword is totally absent from the introduction, where the research area is named ""OOD confidence calibration"". What is the relation between OOD detection and OOD confidence calibration? 
The introduction also reveals two potential approaches for this area: discriminative and generative methods. There are also two settings: ID and OOD confidence calibration. The readers might expect to review current progress for all those categories in the related work section.

Limitations:
The author has addressed limitations of their work in terms of sample complexity.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a way to calibrate ReLU networks or random forests by breaking them down into piecewise linear functions on polytopes and replacing the linear parts with Gaussian kernels. This approximation allows to naturally calibrate the models for the ID domain, where confidence will be high due to the density of ID samples that translates into high kernel values, and for the OOD domain, where confidence will be low due to the large distance to ID samples.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The method is novel and mathematically grounded
- The presentation is clear
- The benchmarks are OK

Weaknesses:
The main weakness I find is about the computational time of the method. The number of polytopes scales exponentially with the number of neurons, so I am concerned with the applicability of the method to large (or even medium-scale) neural networks. What is the computational cost of the method for the considered benchmarks, in terms of runtime?

The toy simulations are unnecessarily tedious to grasp and take up a lot of space. I do not say that they are complex, but they hinder the reading flow and do not bring much to the presentation. I would advise putting some of them in the appendix to leave more space for other explanations. Indeed, Section 5 is difficult to read (many ""chunk"" paragraphs with mathematical notations) and would benefit from more structured writing and more flow.

Limitations:
The authors have adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes new methods, Kernel Density Forest (KDF) and Kernel Density Network (KDN), to address issues in confidence calibration for traditional deep learning models and random forests. The motivation stems from the existing literature that deep neural networks using ReLU tend to exhibit high confidence on out-of-distribution (OOD) data due to affine transformations. The proposed methods improve confidence calibration for both in-distribution (ID) and OOD data by partitioning the feature space into polytopes and replacing affine functions within each polytope with Gaussian kernels. Experimental results demonstrate that the proposed methods outperform existing techniques in terms of calibration performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality:
The approach of replacing affine functions within polytopes with Gaussian kernels is novel. The proposed methods address the confidence calibration problem for both ID and OOD data simultaneously, providing an integrated solution to these calibration issues.

Quality:
The theoretical proofs are robust, and the effectiveness of the proposed methods is validated through both simulations and real-world datasets.

Clarity:
The paper is written clearly and concisely.

Weaknesses:
Validity of Metrics:
The paper evaluates calibration using Maximum Calibration Error (MCE) for ID data, but does not justify the use of MCE over Expected Calibration Error (ECE) or Adaptive Calibration Error (ACE)[1]. A more detailed explanation and comparison of these metrics would enhance the paper's credibility. Additionally, the definition and justification for OCE (Out-of-distribution Calibration Error) would benefit from a similar comparison with ACE.

[1] https://arxiv.org/abs/1904.01685

Experiments:
To emphasize the effectiveness of the proposed methods, a comparison of execution times would be beneficial, especially since practical applications like web Click-Through Rate (CTR) estimation place significant importance on runtime. The paper should clarify what the noise in Table 1 represents. It would also be advantageous to include experiments on larger and more varied datasets, as well as an evaluation of the methods' performance when combined with in-training calibration methods, which are commonly used alongside post-hoc calibration methods.

Limitations:
This paper mentions computational complexity and limitations in practical applications, but lacks detailed experimental results to support these claims. Including such data would provide valuable insights for future research and implementation.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel approach for OOD detection by learning a posterior distribution that is calibrated for both ID and OOD individuals. It models the class-wise conditional distribution of features by a gaussian kernel respectively for a set of polytopes that cover the feature space. The tail property of gaussian kernels contribute to both ID and OOD calibration. Empirical evidence shows the power of the proposed algorithm across tabular and vision dataset under both ID and OOD settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well motivated from the tradeoff of ID calibration and OOD calibration for current approaches for OOD detection methods. The technique of gaussian kernel has a clear geometric intuitive. Compared to affine functions, the tail property ensures that the posterior distribution converges to the prior of labels when a OOD sample deviates far enough from the training support, as proved in Proposition 2. On the other hand, the interpolation by gaussian kernels between neighboring polytopes contributes to ID calibration.

Weaknesses:
The major concern is insufficient discussion over the research context of the paper, which renders it hard to precisely evaluate the contribution. The related work section is short. Section 2 shows that ""OOD detection"" is the closest area to this paper, but this keyword is totally absent from the introduction, where the research area is named ""OOD confidence calibration"". What is the relation between OOD detection and OOD confidence calibration? 
The introduction also reveals two potential approaches for this area: discriminative and generative methods. There are also two settings: ID and OOD confidence calibration. The readers might expect to review current progress for all those categories in the related work section.

Limitations:
The author has addressed limitations of their work in terms of sample complexity.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a way to calibrate ReLU networks or random forests by breaking them down into piecewise linear functions on polytopes and replacing the linear parts with Gaussian kernels. This approximation allows to naturally calibrate the models for the ID domain, where confidence will be high due to the density of ID samples that translates into high kernel values, and for the OOD domain, where confidence will be low due to the large distance to ID samples.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The method is novel and mathematically grounded
- The presentation is clear
- The benchmarks are OK

Weaknesses:
The main weakness I find is about the computational time of the method. The number of polytopes scales exponentially with the number of neurons, so I am concerned with the applicability of the method to large (or even medium-scale) neural networks. What is the computational cost of the method for the considered benchmarks, in terms of runtime?

The toy simulations are unnecessarily tedious to grasp and take up a lot of space. I do not say that they are complex, but they hinder the reading flow and do not bring much to the presentation. I would advise putting some of them in the appendix to leave more space for other explanations. Indeed, Section 5 is difficult to read (many ""chunk"" paragraphs with mathematical notations) and would benefit from more structured writing and more flow.

Limitations:
The authors have adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cO1llRY2Br;"REVIEW 
Summary:
This paper focuses on model editing at a low cost. Evidence suggests that modules carrying knowledge in a Transformer module are primarily the MLP blocks. Therefore, the authors propose a method, namely iReVa, to initialize and retrofit key-value pairs into MLP blocks in a Transformer for explicitly inserting new knowledge. Specifically, they insert new neurons in the MLP blocks for each piece of knowledge. Each neuron is initialized with the embedded key and value derived from the input-output pair, respectively. To prevent dramatic change to the irrelevant knowledge, iReVa further retrofits the key and value by fine-tuning with multiple objectives. Compared to the existing methods such as MEND, ROME, MEMIT, and MELO, iReVa reveals better interpretability and stronger capacity for carrying traceable edits. The experiments on zsRE-10K and PARAREL-10K datasets reveal that iReVa has superior performance regarding edit success, generalization, and specificity. Further edit withdrawal test indicates that iReVa can explicitly manipulate the activation of neurons and easily withdraw the edits.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	This paper focuses on modeling editing, which has significant applications in the era of LLMs. It can be applied to alleviate the hallucination issue of LMs and resolve the out-of-date as well as missing knowledge in an LM.
2.	This paper introduces a novel editing method with key-value adaptors for traceable model editing. The proposed method makes sense to me. The initialization with embedded key and value derived from the input-output pair can easily make precise edits to the model. Further retrofitting refines the adaptors to satisfy the task.
3.	For experiments, the author has comprehensively shown the superiority of their method in the perspectives of edit success, generalization, and specificity. And more analyses reveal the generalization of iReVa. Particularly, the edit withdrawal test in Section 6.2 is well-designed, which shows the effect of traceable edits and could provide a potential solution for dynamic knowledge maintenance for LMs.
4.	Overall, this paper is well-written and easy to follow.

Weaknesses:
1.	The discussions on the limitations and broader societal impacts of iReVa are not included in the paper. I have some questions about the application scope of the proposed method. Please see the questions below.

Questions
1.	Could iReVa lead to a dramatically increasing number of parameters? Let’s see if there are millions of knowledge for editing, how can you potentially insert all the knowledge into LMs with iReVa? 
2.	After you change a piece of knowledge, can the reasoning still be conducted for the edited knowledge? For example, if we have edited the president of America, could some reasoning questions like ``Who is the wife of the president of America” also be resolved with the new knowledge?
3.	Typo: ``evident’’ in line 6 should be ``evidence’’. Please check.

Limitations:
No, the author should discuss the limitations of the proposed method such as the application scope, the potential risks, and future improvement to indicate how robust the results are to violate the
assumption. I would like the author to add such information during the rebuttal.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the tracable sequential model editing challenge by plugging in additional model components to a transformer MLP blocks. The proposed approach adds additional model components for each edit, allowing for traceability for each edit.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
•	The results indicate that it is a strong approach compared to relevant literature and its performance is relatively stable when scaling it to thousands of edits.
•	The approach allows for ""separability of each edit"" which in turn allows for additional operations such as edit updation or deletion as showcased in the Edit withdrawal experiment. 
•	The edit withdrawal experiment is both unique and intriguing, as the concept of removing edits appears to be a novel area of exploration.

Weaknesses:
•	The overall approach does not appear to be novel, as it closely resembles T-Patcher. Both iReVa and T-Patcher rely on inserting neurons for editing and using neuron activations to control when to run the patch/adopter. Furthermore, analysis of the editing approach across different layers reveals the same pattern as discussed in the T-Patcher paper which involves adding additional model components in the final layer for optimal results.

•	Experiments with T-Patcher are missing from the comparisons to the existing methods section. Given its similarity, T-Patcher should be included for comparison.

•	Although T-Patcher performs editing in batches, it still uses a single patch of neurons for each edit, making its editing similarly traceable. Thus, the paper's claim of a ""stronger capacity for carrying traceable edits"" seems unfounded.

•	The Edit Withdrawal Test section is hard to understand. How exactly was the experiment conducted? Were all edits removed or only a limited set? Detailed experimentations for this section are needed as it is the only use case of traceability explored in the paper.

•	Editing techniques that rely on code books with playback vectors e.g. GRACE would allow for edits to be removed. The authors should make it clear that the withdrawal test is not possible for the editing techniques that they have chosen for comparison.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces iReVa, a novel method for model editing that explicitly initializes and retrofits key-value pairs into MLP blocks of transformer models to perform CRUD (Create, Read, Update, Delete) operations on LMs. iReVa aims to update knowledge in LMs without damaging irrelevant knowledge, offering better interpretability and traceability compared to existing methods. The method is validated through experiments on GPT series models, showing significant improvements in edit success and generalization without affecting specificity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Provision of the first attempt at conducting knowledge withdrawal tests for model editing methods.

The paper includes a comprehensive analysis of iReVa's performance, including knowledge withdrawal tests and generalization tests.

iReVa's approach to model editing is innovative, focusing on retrofitting key-value adaptors into MLP blocks for traceable model editing

Weaknesses:
This paper could benefit from a more detailed comparison with other model editing methods, especially those focusing on lifelong learning and continual editing [1][2].

It does not discuss the computational efficiency of iReVa in terms of inference time or memory, which is crucial for real-world applications.

The reliance on the hypothesis that factual knowledge is stored in MLP blocks may be limiting [3], and the authors could explore the broader implications of this assumption.

The method's applicability to other types of tasks, such as erasing hallucinations, is not validated.

There is a noticeable absence of experimental validation on other recent and updated models such as GPT-J (used by ROME etc.), LLaMA.

The technical novelty of iReVa is somewhat limited, as it builds upon existing concepts like MEMIT [4] and key-value memory structures in MLPs [2].

The absence of a strategy for selecting the adaptor layer may hinder the method's rapid migration and application to various language models。

Equation 3 requires clarification, why 'i' and 'o' in Equation 3 are both passed through SELF_ATTEN again?

References

[1] Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors, Hartvigsen et al,
Neurips 2023.

[2] Transformer-Patcher: One Mistake worth One Neuron, Huang et al, ICLR 2023.

[3] What does the Knowledge Neuron Thesis Have to do with Knowledge? Niu et al, ICLR 2024

[4] Mass-Editing Memory in a Transformer, Meng et al, ICLR 2023.

Limitations:
No

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel method called iReVa for knowledge editing. iReVa initializes and retrofits key-value pairs into MLP blocks to create a new mapping of knowledge without affecting related information. Compared to existing methods, iReVa offers better interpretability and a stronger ability to make traceable edits.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed methods demonstrate great performance compared to other baselines under the batch editing scenarios.

Weaknesses:
1. The color in the figure is not obvious to discriminate between the original knowledge neurons and new knowledge neurons.
2. The computation of the proposed method is similar to T-Patcher, I'm curious about the difference between them. The proposed methods are designed to tackle the batch edit, but it seems it still needs to add one neuron for each example.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on model editing at a low cost. Evidence suggests that modules carrying knowledge in a Transformer module are primarily the MLP blocks. Therefore, the authors propose a method, namely iReVa, to initialize and retrofit key-value pairs into MLP blocks in a Transformer for explicitly inserting new knowledge. Specifically, they insert new neurons in the MLP blocks for each piece of knowledge. Each neuron is initialized with the embedded key and value derived from the input-output pair, respectively. To prevent dramatic change to the irrelevant knowledge, iReVa further retrofits the key and value by fine-tuning with multiple objectives. Compared to the existing methods such as MEND, ROME, MEMIT, and MELO, iReVa reveals better interpretability and stronger capacity for carrying traceable edits. The experiments on zsRE-10K and PARAREL-10K datasets reveal that iReVa has superior performance regarding edit success, generalization, and specificity. Further edit withdrawal test indicates that iReVa can explicitly manipulate the activation of neurons and easily withdraw the edits.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	This paper focuses on modeling editing, which has significant applications in the era of LLMs. It can be applied to alleviate the hallucination issue of LMs and resolve the out-of-date as well as missing knowledge in an LM.
2.	This paper introduces a novel editing method with key-value adaptors for traceable model editing. The proposed method makes sense to me. The initialization with embedded key and value derived from the input-output pair can easily make precise edits to the model. Further retrofitting refines the adaptors to satisfy the task.
3.	For experiments, the author has comprehensively shown the superiority of their method in the perspectives of edit success, generalization, and specificity. And more analyses reveal the generalization of iReVa. Particularly, the edit withdrawal test in Section 6.2 is well-designed, which shows the effect of traceable edits and could provide a potential solution for dynamic knowledge maintenance for LMs.
4.	Overall, this paper is well-written and easy to follow.

Weaknesses:
1.	The discussions on the limitations and broader societal impacts of iReVa are not included in the paper. I have some questions about the application scope of the proposed method. Please see the questions below.

Questions
1.	Could iReVa lead to a dramatically increasing number of parameters? Let’s see if there are millions of knowledge for editing, how can you potentially insert all the knowledge into LMs with iReVa? 
2.	After you change a piece of knowledge, can the reasoning still be conducted for the edited knowledge? For example, if we have edited the president of America, could some reasoning questions like ``Who is the wife of the president of America” also be resolved with the new knowledge?
3.	Typo: ``evident’’ in line 6 should be ``evidence’’. Please check.

Limitations:
No, the author should discuss the limitations of the proposed method such as the application scope, the potential risks, and future improvement to indicate how robust the results are to violate the
assumption. I would like the author to add such information during the rebuttal.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the tracable sequential model editing challenge by plugging in additional model components to a transformer MLP blocks. The proposed approach adds additional model components for each edit, allowing for traceability for each edit.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
•	The results indicate that it is a strong approach compared to relevant literature and its performance is relatively stable when scaling it to thousands of edits.
•	The approach allows for ""separability of each edit"" which in turn allows for additional operations such as edit updation or deletion as showcased in the Edit withdrawal experiment. 
•	The edit withdrawal experiment is both unique and intriguing, as the concept of removing edits appears to be a novel area of exploration.

Weaknesses:
•	The overall approach does not appear to be novel, as it closely resembles T-Patcher. Both iReVa and T-Patcher rely on inserting neurons for editing and using neuron activations to control when to run the patch/adopter. Furthermore, analysis of the editing approach across different layers reveals the same pattern as discussed in the T-Patcher paper which involves adding additional model components in the final layer for optimal results.

•	Experiments with T-Patcher are missing from the comparisons to the existing methods section. Given its similarity, T-Patcher should be included for comparison.

•	Although T-Patcher performs editing in batches, it still uses a single patch of neurons for each edit, making its editing similarly traceable. Thus, the paper's claim of a ""stronger capacity for carrying traceable edits"" seems unfounded.

•	The Edit Withdrawal Test section is hard to understand. How exactly was the experiment conducted? Were all edits removed or only a limited set? Detailed experimentations for this section are needed as it is the only use case of traceability explored in the paper.

•	Editing techniques that rely on code books with playback vectors e.g. GRACE would allow for edits to be removed. The authors should make it clear that the withdrawal test is not possible for the editing techniques that they have chosen for comparison.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces iReVa, a novel method for model editing that explicitly initializes and retrofits key-value pairs into MLP blocks of transformer models to perform CRUD (Create, Read, Update, Delete) operations on LMs. iReVa aims to update knowledge in LMs without damaging irrelevant knowledge, offering better interpretability and traceability compared to existing methods. The method is validated through experiments on GPT series models, showing significant improvements in edit success and generalization without affecting specificity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Provision of the first attempt at conducting knowledge withdrawal tests for model editing methods.

The paper includes a comprehensive analysis of iReVa's performance, including knowledge withdrawal tests and generalization tests.

iReVa's approach to model editing is innovative, focusing on retrofitting key-value adaptors into MLP blocks for traceable model editing

Weaknesses:
This paper could benefit from a more detailed comparison with other model editing methods, especially those focusing on lifelong learning and continual editing [1][2].

It does not discuss the computational efficiency of iReVa in terms of inference time or memory, which is crucial for real-world applications.

The reliance on the hypothesis that factual knowledge is stored in MLP blocks may be limiting [3], and the authors could explore the broader implications of this assumption.

The method's applicability to other types of tasks, such as erasing hallucinations, is not validated.

There is a noticeable absence of experimental validation on other recent and updated models such as GPT-J (used by ROME etc.), LLaMA.

The technical novelty of iReVa is somewhat limited, as it builds upon existing concepts like MEMIT [4] and key-value memory structures in MLPs [2].

The absence of a strategy for selecting the adaptor layer may hinder the method's rapid migration and application to various language models。

Equation 3 requires clarification, why 'i' and 'o' in Equation 3 are both passed through SELF_ATTEN again?

References

[1] Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors, Hartvigsen et al,
Neurips 2023.

[2] Transformer-Patcher: One Mistake worth One Neuron, Huang et al, ICLR 2023.

[3] What does the Knowledge Neuron Thesis Have to do with Knowledge? Niu et al, ICLR 2024

[4] Mass-Editing Memory in a Transformer, Meng et al, ICLR 2023.

Limitations:
No

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel method called iReVa for knowledge editing. iReVa initializes and retrofits key-value pairs into MLP blocks to create a new mapping of knowledge without affecting related information. Compared to existing methods, iReVa offers better interpretability and a stronger ability to make traceable edits.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed methods demonstrate great performance compared to other baselines under the batch editing scenarios.

Weaknesses:
1. The color in the figure is not obvious to discriminate between the original knowledge neurons and new knowledge neurons.
2. The computation of the proposed method is similar to T-Patcher, I'm curious about the difference between them. The proposed methods are designed to tackle the batch edit, but it seems it still needs to add one neuron for each example.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
aYJ2T5TXoX;"REVIEW 
Summary:
The paper tries to formalize the concept of generalizability in experimental studies in machine learning research. It relies on three different types of kernels in order to quantify difference between the rankings in an experiment output. A core contribution is the development of an algorithm for estimating the number of experimental studies needed in order to generalize the results at a desired level.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is interesting and worthwhile.
- The paper is clearly written.
- The formalization of generalizability is well-defined and nicely parameterized through the use of kernels.
- The practical usefulness of the algorithm is somewhat unclear to me.

Weaknesses:
- There is no discussion on the computational costs of the algorithm (except for a vague statement that it is very fast in the checklist).
- The empirical evidence for the algorithm's effectiveness appears somewhat weak to me (see respective item in _Questions_).
- The python package is not properly configured I think. I see this after having installed the package into a virtual environment with the correct python version and
  using `pip install . -r requirements.txt`:

  ```python
  In [1]: import genexpy
  ---------------------------------------------------------------------------
  ImportError                               Traceback (most recent call last)
  Cell In[1], line 1
  ----> 1 import genexpy

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/__init__.py:4
        2 from .src import lower_bounds
        3 from .src import mmd
  ----> 4 from .src import probability_distributions
        5 from .src import rankings_utils
        6 from .src import relation_utils

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/src/probability_distributions.py
  :11
        8 from typing import Literal
      10 from genexpy import kernels as ku
  ---> 11 from genexpy import rankings_utils as ru
      12 from genexpy import relation_utils as rlu
      15 def sample_from_sphere(na: int, n: int, rng: np.random.Generator) -> np.ndarray[float]:

  ImportError: cannot import name 'rankings_utils' from partially initialized module 'genexpy' (most l
  ikely due to a circular import) (/home/<anonymous_reviewer>/.pyenv/versions/genexpy/lib/python3.11/site-packages
  /genexpy/__init__.py)

Limitations:
Some of the limitations are discussed but I still think the paper could be more self-critical of for instance $n^*$. Possible computational costs are also not discussed.

There are no potential negative societal impacts of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper deals with experimental studies. After providing a mathematical formalization, it focuses on the generalizability of these studies. The main contribution is a quantitative estimate of the the size of the study to obtain generalizable results. Experiments on LLMs are conducted.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- [mathematical formulation] It is nice to have a solid formulation of experimental studies, this is quite relevant to the community.

Weaknesses:
- [train / test split] A concrete problem in machine learning practical experimentation is that of train / test split, and more particularly its absence (that is, training on the test). I do not see this issue discussed in the paper. Can it be incorporated in the setting? Is it possible to clarify whether the paper assumes that the training is done on a training set without calibration on a validation set or is this hidden somewhere? What would then be the influence on the number of experiments?
- [testing between rankings] If I understand correctly, the paper proposes (in Section 4.1) to check whether rankings are consistent by performing kernel two-sample test, with adapted kernels. This does not seem standard to me: there exists some ad-hoc statistical tests (e.g., Kendall's \tau, Spearman's \rho, etc.). Why not use them directly? Is there an advantage to using MMD?  

- [minor comments]:
  - missing ref line 111
  - repeated word ('of') line 300

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to formalise the notion of an experimental study by considering the sampling process of acquiring a dataset.  It then uses this notion to argue about generalisability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem of understanding the performance of machine learning when tested on new data is a very important problem.  The authors use some technically sophisticate methods to tackle this problem.

Weaknesses:
For me the authors model of an experiment is too simplistic and does not capture the problems faced by machine learning.  If we collect medical data then that data is likely to vary depending on the equipment used, the clinicians running the equipment and population where the data comes from.  These kinds of variations are the bugbear for machine learning, but not captured at all by the model.  Another issue is that a lot of data is non-stationary.  Even in the much used example of checkmate in one.  If a machine learns this very well, then players against the machine are likely to learn their mistake and alter their play.  Thus, I am not convinced that the model being proposed is particularly interesting.

Limitations:
This is fine.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a formalism for the generalizability of experimental studies in ML.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Anything pushing to get better practices in evaluation of ML is very important.

Weaknesses:
I could quibble with some of the setup, which is a bit confusing to me: design factors being properties of the context rather than of the alternative, for example, is kind of odd, but I don't think this is very important.

The big problem is that there is a huge literature on a very similar problem and I have very little sense of how this work connects to it: starting with the Neyman-Pearson lemma and going down through the standard corpus of decision theory, we have a lot of statistical tools for thinking about this problem in very broad strokes. After reading this paper, I have a sense that you're trying to solve a very similar problem (given a sample from some population, what can I say about the reliability of my estimate? How many samples would I need to be sure that it's reliable?)

Section 4.3 seems to be rederiving some form of power analysis.

Looking at A.3.3, it appears that the procedure is essentially the following:
(1) [the inner loop from 1...n_rep] construct a null hypothesis at a given sample size, find the upper alpha quantile of that null hypothesis
(2) Repeat this at a variety of sample sizes
(3) Estimate a power-law relationship between sample size and the upper-alpha quantiles
(4) Predict the sample size which would have such an upper-alpha quantile

This procedure is an empirical version of power analysis where the null distribution is not known but simulated and extrapolated. If I know the type-I error rate, type-II error rate and a distribution under the null and under the alternative, deriving the required sample size is straightforward. Indeed, we have a CLT for MMD (at least under some kernels) [1], so these distributions are known asymptotically, which is likely plenty for the purposes of sample size determination. Do \alpha^* and \delta^* map onto concepts from Neyman-Pearson? It's entirely possible.

This is an important question because decision theory has very well established results on things like uniformly most powerful tests. When we just invent a new framework rather than relying on well-trod ones, we are likely to derive suboptimal procedures unless we compare very carefully to these existing procedures. There's no similarly sophisticated discussion of error properties in this paper, which would be reasonable if this were truly the first paper in its vein, but I don't think that's the case.

Further, it's not clear to me why these similarities between rankings should be the target of inference. Rather, shouldn't I care about whether, based on the sample of allowed-to-vary factors I've used, alternative A is preferred to alternative B? This is an extremely standard matter of decision theory as far as I can tell. By moving to these more complicated research questions about rankings it clouds this fact, but I'm not sure it needs to. If the target of inference were instead to be a rank of K alternatives, I believe a decision theorist would take a somewhat similar approach to what you've done here: define a similarity metric based on the research question. An example solution to a problem like this would be [2], [3]. I just don't see why we need this new framework to accomplish a task I think we already have the tools for.

It's entirely possible that there's a contribution here, but it can't just be ""this is a new task"". We have methods from decision theory that have been designed for a wide range of decision tasks, and its incumbent upon the authors to demonstrate why those existing tools do not fit the task in front of them.

[1] https://www.jmlr.org/papers/volume24/22-1136/22-1136.pdf
[2] https://onlinelibrary.wiley.com/doi/abs/10.1002/mcda.313
[3] https://www.sciencedirect.com/science/article/abs/pii/S0377221715008048

Limitations:
see above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a new mathematical framework and a corresponding new algorithm to evaluate the generalizability of published experimental studies, by adapting Montgomery's classification of experimental factors [44].
They demonstrate the efficacy of this framework in evaluating the generalizability of two popular published experimental studies.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper appears to be theoretical strong. 
It goes beyond the standard notion of reproducibility of an experimental study and comes up a definition of generalizability of an experimental study.
Morever, Proposition 4.2 provides a theoretical result on the sample size $n$ necessary to obtained a desired generalizability $\alpha^*$ for a desired similarity $\epsilon^*$, and the authors also provide an algorithm (A.3.3) to compute this sample size.
Since the similarity $\epsilon^*$ is hard to specify, they make it a function of the kernelized distance between rankings $\delta^*$.

2.
The empirical evaluation in Fig. 2 and Fig. 3, on the categorical encoder comparison from [41] or the BIG-bench framework for LLM comparison from [55], respectively, demonstrates the practical utility of the proposed approach in determining sample sizes to guarantee generalizability.

Weaknesses:
1. The clarity in the writing can be significantly improved:

1.A. Symbols are used before defining them, typos exist, and symbols are not used consistently:

1.A.a. On line 118, the symbol $\mathcal{R}_{n_a}$ is mentioned, but the relation of this symbol to the ranking on alternatives only becomes clear later in Definition 3.1.

1.A.b. The Section number is missing on line 111.

1.A.c. The symbol $m$, is defined as the number of shots, on line 115, whereas line 114 uses the symbol $n$ rather than $m$. Moreover, on line 88, $n$ is defined as the number of shots.

1.A.d. In contrast to 1.A.c, in eq. (1), after line 170, the symbol $n$ is now used without providing a definition. It now appears to be the size of any study, in a general definition, rather than the number of shots, as defined on line 88. 

1.A.e. On Sec. 5.3, line 315, $N$ is defined as the number of preliminary experiments, whereas on line 154, it is defined as the size of the sample of valid experimental conditions. Do these mean the same thing ?

1.B. Sec. 3.1 defines a ranking of alternatives as the primary result of an experimental study.
However, the effect size, i.e., the magnitude and sign of the difference between two alternatives, can be important in certain experiments.
The MMD kernel, used in Sec. 4.2, actually allows measuring this effect size, as discussed in [27], but the limitations imposed by the usage of this MMD kernel within the author's generalizability framework, are not clear despite the somewhat cryptic discussion in Sec. 6.

2. 
The experimental evaluation is limited to a comparison of ranking differences between alternatives, and does not include a measurement of the practical differences between alternatives, or the significance of these differences.

Limitations:
Please refer to the potential limitation underlying weakness #2. Is it possible to quantify the magnitude of differences between alternatives using the generalizability framework provided by the authors ? The authors mention this limitation in Sec. 6, but it is not clear why the MMD kernel cannot quantify magnitude of differences.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to formalize the concept of generalizability in experimental studies in machine learning research. It relies on three different types of kernels in order to quantify difference between the rankings in an experiment output. A core contribution is the development of an algorithm for estimating the number of experimental studies needed in order to generalize the results at a desired level.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is interesting and worthwhile.
- The paper is clearly written.
- The formalization of generalizability is well-defined and nicely parameterized through the use of kernels.
- The practical usefulness of the algorithm is somewhat unclear to me.

Weaknesses:
- There is no discussion on the computational costs of the algorithm (except for a vague statement that it is very fast in the checklist).
- The empirical evidence for the algorithm's effectiveness appears somewhat weak to me (see respective item in _Questions_).
- The python package is not properly configured I think. I see this after having installed the package into a virtual environment with the correct python version and
  using `pip install . -r requirements.txt`:

  ```python
  In [1]: import genexpy
  ---------------------------------------------------------------------------
  ImportError                               Traceback (most recent call last)
  Cell In[1], line 1
  ----> 1 import genexpy

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/__init__.py:4
        2 from .src import lower_bounds
        3 from .src import mmd
  ----> 4 from .src import probability_distributions
        5 from .src import rankings_utils
        6 from .src import relation_utils

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/src/probability_distributions.py
  :11
        8 from typing import Literal
      10 from genexpy import kernels as ku
  ---> 11 from genexpy import rankings_utils as ru
      12 from genexpy import relation_utils as rlu
      15 def sample_from_sphere(na: int, n: int, rng: np.random.Generator) -> np.ndarray[float]:

  ImportError: cannot import name 'rankings_utils' from partially initialized module 'genexpy' (most l
  ikely due to a circular import) (/home/<anonymous_reviewer>/.pyenv/versions/genexpy/lib/python3.11/site-packages
  /genexpy/__init__.py)

Limitations:
Some of the limitations are discussed but I still think the paper could be more self-critical of for instance $n^*$. Possible computational costs are also not discussed.

There are no potential negative societal impacts of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper deals with experimental studies. After providing a mathematical formalization, it focuses on the generalizability of these studies. The main contribution is a quantitative estimate of the the size of the study to obtain generalizable results. Experiments on LLMs are conducted.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- [mathematical formulation] It is nice to have a solid formulation of experimental studies, this is quite relevant to the community.

Weaknesses:
- [train / test split] A concrete problem in machine learning practical experimentation is that of train / test split, and more particularly its absence (that is, training on the test). I do not see this issue discussed in the paper. Can it be incorporated in the setting? Is it possible to clarify whether the paper assumes that the training is done on a training set without calibration on a validation set or is this hidden somewhere? What would then be the influence on the number of experiments?
- [testing between rankings] If I understand correctly, the paper proposes (in Section 4.1) to check whether rankings are consistent by performing kernel two-sample test, with adapted kernels. This does not seem standard to me: there exists some ad-hoc statistical tests (e.g., Kendall's \tau, Spearman's \rho, etc.). Why not use them directly? Is there an advantage to using MMD?  

- [minor comments]:
  - missing ref line 111
  - repeated word ('of') line 300

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to formalise the notion of an experimental study by considering the sampling process of acquiring a dataset.  It then uses this notion to argue about generalisability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem of understanding the performance of machine learning when tested on new data is a very important problem.  The authors use some technically sophisticate methods to tackle this problem.

Weaknesses:
For me the authors model of an experiment is too simplistic and does not capture the problems faced by machine learning.  If we collect medical data then that data is likely to vary depending on the equipment used, the clinicians running the equipment and population where the data comes from.  These kinds of variations are the bugbear for machine learning, but not captured at all by the model.  Another issue is that a lot of data is non-stationary.  Even in the much used example of checkmate in one.  If a machine learns this very well, then players against the machine are likely to learn their mistake and alter their play.  Thus, I am not convinced that the model being proposed is particularly interesting.

Limitations:
This is fine.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a formalism for the generalizability of experimental studies in ML.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Anything pushing to get better practices in evaluation of ML is very important.

Weaknesses:
I could quibble with some of the setup, which is a bit confusing to me: design factors being properties of the context rather than of the alternative, for example, is kind of odd, but I don't think this is very important.

The big problem is that there is a huge literature on a very similar problem and I have very little sense of how this work connects to it: starting with the Neyman-Pearson lemma and going down through the standard corpus of decision theory, we have a lot of statistical tools for thinking about this problem in very broad strokes. After reading this paper, I have a sense that you're trying to solve a very similar problem (given a sample from some population, what can I say about the reliability of my estimate? How many samples would I need to be sure that it's reliable?)

Section 4.3 seems to be rederiving some form of power analysis.

Looking at A.3.3, it appears that the procedure is essentially the following:
(1) [the inner loop from 1...n_rep] construct a null hypothesis at a given sample size, find the upper alpha quantile of that null hypothesis
(2) Repeat this at a variety of sample sizes
(3) Estimate a power-law relationship between sample size and the upper-alpha quantiles
(4) Predict the sample size which would have such an upper-alpha quantile

This procedure is an empirical version of power analysis where the null distribution is not known but simulated and extrapolated. If I know the type-I error rate, type-II error rate and a distribution under the null and under the alternative, deriving the required sample size is straightforward. Indeed, we have a CLT for MMD (at least under some kernels) [1], so these distributions are known asymptotically, which is likely plenty for the purposes of sample size determination. Do \alpha^* and \delta^* map onto concepts from Neyman-Pearson? It's entirely possible.

This is an important question because decision theory has very well established results on things like uniformly most powerful tests. When we just invent a new framework rather than relying on well-trod ones, we are likely to derive suboptimal procedures unless we compare very carefully to these existing procedures. There's no similarly sophisticated discussion of error properties in this paper, which would be reasonable if this were truly the first paper in its vein, but I don't think that's the case.

Further, it's not clear to me why these similarities between rankings should be the target of inference. Rather, shouldn't I care about whether, based on the sample of allowed-to-vary factors I've used, alternative A is preferred to alternative B? This is an extremely standard matter of decision theory as far as I can tell. By moving to these more complicated research questions about rankings it clouds this fact, but I'm not sure it needs to. If the target of inference were instead to be a rank of K alternatives, I believe a decision theorist would take a somewhat similar approach to what you've done here: define a similarity metric based on the research question. An example solution to a problem like this would be [2], [3]. I just don't see why we need this new framework to accomplish a task I think we already have the tools for.

It's entirely possible that there's a contribution here, but it can't just be ""this is a new task"". We have methods from decision theory that have been designed for a wide range of decision tasks, and its incumbent upon the authors to demonstrate why those existing tools do not fit the task in front of them.

[1] https://www.jmlr.org/papers/volume24/22-1136/22-1136.pdf
[2] https://onlinelibrary.wiley.com/doi/abs/10.1002/mcda.313
[3] https://www.sciencedirect.com/science/article/abs/pii/S0377221715008048

Limitations:
see above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a new mathematical framework and a corresponding new algorithm to evaluate the generalizability of published experimental studies, by adapting Montgomery's classification of experimental factors [44].
They demonstrate the efficacy of this framework in evaluating the generalizability of two popular published experimental studies.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper appears to be theoretical strong. 
It goes beyond the standard notion of reproducibility of an experimental study and comes up a definition of generalizability of an experimental study.
Morever, Proposition 4.2 provides a theoretical result on the sample size $n$ necessary to obtained a desired generalizability $\alpha^*$ for a desired similarity $\epsilon^*$, and the authors also provide an algorithm (A.3.3) to compute this sample size.
Since the similarity $\epsilon^*$ is hard to specify, they make it a function of the kernelized distance between rankings $\delta^*$.

2.
The empirical evaluation in Fig. 2 and Fig. 3, on the categorical encoder comparison from [41] or the BIG-bench framework for LLM comparison from [55], respectively, demonstrates the practical utility of the proposed approach in determining sample sizes to guarantee generalizability.

Weaknesses:
1. The clarity in the writing can be significantly improved:

1.A. Symbols are used before defining them, typos exist, and symbols are not used consistently:

1.A.a. On line 118, the symbol $\mathcal{R}_{n_a}$ is mentioned, but the relation of this symbol to the ranking on alternatives only becomes clear later in Definition 3.1.

1.A.b. The Section number is missing on line 111.

1.A.c. The symbol $m$, is defined as the number of shots, on line 115, whereas line 114 uses the symbol $n$ rather than $m$. Moreover, on line 88, $n$ is defined as the number of shots.

1.A.d. In contrast to 1.A.c, in eq. (1), after line 170, the symbol $n$ is now used without providing a definition. It now appears to be the size of any study, in a general definition, rather than the number of shots, as defined on line 88. 

1.A.e. On Sec. 5.3, line 315, $N$ is defined as the number of preliminary experiments, whereas on line 154, it is defined as the size of the sample of valid experimental conditions. Do these mean the same thing ?

1.B. Sec. 3.1 defines a ranking of alternatives as the primary result of an experimental study.
However, the effect size, i.e., the magnitude and sign of the difference between two alternatives, can be important in certain experiments.
The MMD kernel, used in Sec. 4.2, actually allows measuring this effect size, as discussed in [27], but the limitations imposed by the usage of this MMD kernel within the author's generalizability framework, are not clear despite the somewhat cryptic discussion in Sec. 6.

2. 
The experimental evaluation is limited to a comparison of ranking differences between alternatives, and does not include a measurement of the practical differences between alternatives, or the significance of these differences.

Limitations:
Please refer to the potential limitation underlying weakness #2. Is it possible to quantify the magnitude of differences between alternatives using the generalizability framework provided by the authors ? The authors mention this limitation in Sec. 6, but it is not clear why the MMD kernel cannot quantify magnitude of differences.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ZJZqO4grws;"REVIEW 
Summary:
This study presents a contrastive regularizer to improve meta-learning. Specifically, the authors propose to incorporate a contrastive meta-objective that improves the alignment and the discrimination abilities of meta-learners, leading to better task adaptation and generalization. The authors demonstrate empirical effectiveness of the proposed ConML across several meta-learning and in-context learning scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The introduction of contrastive regularization sounds intuitively straightforward and motivating. 
2. While equipping meta-learning with contrastive objective is not a new concept (e.g., [1], [2]) the implementation covers major meta-learning methods, including optimization- metric- and amortization-based methods. This means that the study is more comprehensive than previous studies.
3. The numerical results are promising. Code is provided - reproducibility is commendable.


[1] Gondal et al. Function Contrastive Learning of Transferable Meta-Representations. In ICML 2021.
[2] Mathieu et al. On Contrastive Representations of Stochastic Processes. In NeurIPS 2021.

Weaknesses:
My primary concern lies in the specific contrastive strategy employed.

1. The contrastive objective aims to minimize intra-task distances while maximizing inter-task distances. However, the absence of appropriate regularization or constraints raises concerns about potential model representation collapse. This collapse could manifest as representations converging to trivial solutions, such as constant vectors or confinement to low-dimensional subspaces. The authors should address whether they have considered these risks. 
2. In addition, I wonder why the contrastive objective does not follow commonly studied ones, e.g., InfoNCE, in contrastive learning.
3. Moreover, the meta-objective necessitates computations involving representations from different tasks within a batch during each episode. Since the paper lacks a discussion on training and inference efficiency, the impact of this strategy on scalability is unclear.
4. The effectiveness of this method is likely dependent on hyperparameters tuning and the sampling strategy for creating subsets of tasks. As aforementioned, incorporating contrastive learning into meta-learning is not something completely new, even though, the authors do not include detailed discussion on how different strategies would affect the performance/efficiency, which is something to be expected on my end.

Limitations:
The authors do have discussed the limitation of this study.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to enhance the meta-learning process by implementing more robust supervision within the model space. Specifically, the authors seek to augment learning capabilities through model alignment and discrimination, aiming to approximate human-like rapid learning abilities. They propose that models trained on tasks within the same super-task exhibit similarity, while those trained on different task sets generalize effectively across diverse tasks. To achieve this, the authors devise a contrastive framework that remains independent of the specific meta-learning algorithms employed. This framework encourages closer alignment of representations for models adapted to similar tasks while pushing representations apart for models derived from dissimilar tasks. Moreover, the framework seamlessly integrates with optimization-based, metric-based, and amortization-based meta-learning methods. The approach demonstrates improvements across standard benchmarks in all evaluated scenarios. Furthermore, the proposed method shows promise for integration into the in-context learning of large language models, resulting in observed enhancements.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
•  The paper is well-written and easy to understand. It's reasonable to enforce the model to emulate human learning capabilities through alignment and discrimination.

•  The proposed contrastive learning framework is versatile and applicable to most meta-learning methods.

•  The proposed method consistently improves upon existing meta-learning methods across standard benchmarks.

Weaknesses:
•  The paper lacks validation on MetaDataset[A], which is a common large-scale dataset for few-shot learning tasks.

•  There is a need for sensitivity analysis on certain hyperparameters, such as λ and the choice of similarity function for contrastive learning.

[A] Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples, ICLR2020.

Limitations:
Please see weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals learning to learn (meta-learning) problem, from the perspective of exploring inner-task and intra-task relationship. Specifically, this paper proposed a Contrastive meta-objective by exploring intra- and inter-task distances and severed as an additional term for training objective (in addtion to classification loss). 

Experiments are conducted on both conventional few-shot image classification and in-context learning settings. Common benchmarks are used to compare with simple few-shot methods such as MAML, ProtoNet, SCNAPs. For In-context learning, simple synthetic functions are used for comparison.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The overall method makes sense. Designing intra- and inter-distance to explore the task-level contrastive information and further introduce this into to the meta-learning objective is reasonable. The diverse tasks naturally compose the contrastive pairs, useful for training. 
- The proposed method is general and can be applied on top of different few-shot classification/regression methods, such as metric-based, optimisation-based, simpleCNAPs, and in-context learning. 
- The performance gains over these simple baselines are significant, showing the effectiveness of the proposed method.

Weaknesses:
- Technically, the proposed contrastive meta-objective is similar to the idea of supervised contrastive learning, which already provides good insights to the representation learning and deep learning community. Therefore, the proposed method is kind of incremental and provides less new knowledge to the field. 
- The method is only verified on top of simplest baseline methods (MAML, ProtoNet, etc). In meta-learning, various works have been proposed to investigate the possible exploration of the task-level information for improved meta-learning, such as [R1-R4], to name a few. 
However, none of those previous efforts were discussed or compared. Only beating the naive baseline cannot comprehensively demonstrate the advantages of this paper. 
- Experimentally, the proposed method tries to show its superior performance over simple baselines rather than SOTA. This is less convincing. 
- The in-context learning experiments are only on simple synthetic data, lack of significance. 
- The tile and scope: Learning-to-learn is very general, but in fact only classification related experiments are conducted. By convention, the learning-to-learn approaches will also verify on reinforcement learning. 

[R1] Fei, N., Lu, Z., Xiang, T., & Huang, S. (2021). MELR: Meta-learning via modeling episode-level relationships for few-shot learning. In International Conference on Learning Representations.   
[R2] Agarwal, P., & Singh, S. (2023). Exploring intra-task relations to improve meta-learning algorithms. arXiv preprint arXiv:2312.16612.   
[R3] Han, J., Cheng, B., & Lu, W. (2021). Exploring task difficulty for few-shot relation extraction. arXiv preprint arXiv:2109.05473.   
[R4] Zhang, Tao. ""Episodic-free Task Selection for Few-shot Learning."" arXiv preprint arXiv:2402.00092 (2024).

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a contrastive meta-objective that can be applied to various meta-learning methods. Also, interpreting in-context learning as a meta-learning formulation, extended the proposed method to in-context learning. Specifically, the objective is to contrast task identity obtained after episode optimization. The task identity is defined as the model weight or the feature obtained by feed-forwarding, in the case of in-context learning. Finally, the authors demonstrated the superiority of the proposed method by applying it to several meta-learning methods to improve their performance.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The proposed method can improve diverse meta-learning methods.
- Contrasting task identity sounds intuitive.

Weaknesses:
In general, the text is not easy to understand.

- Not self-stained figures.
  - In Figure 1, it's hard to understand what $h_{w_i}$ and $w_i$ are since the caption has no explanation.
  - In Table 1, the caption could have included the definition of $g$ or $\psi$.
  - Figure 2 is hard to read; (b) and (e) missed the x, y-axis meaning and are too small to see something.
- Experiment details are missing.
  - Hard to understand Section 5.1. Though the sine wave regression problem is well-known in this domain, it's hard to interpret results without task definitions.
  - The tasks in Section 5.3 are not clearly defined.
- An ablation study would be helpful.
  - How to decide distance function $\phi$?
  - What if increasing the number of task-sampling $K$?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This study presents a contrastive regularizer to improve meta-learning. Specifically, the authors propose to incorporate a contrastive meta-objective that improves the alignment and the discrimination abilities of meta-learners, leading to better task adaptation and generalization. The authors demonstrate empirical effectiveness of the proposed ConML across several meta-learning and in-context learning scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The introduction of contrastive regularization sounds intuitively straightforward and motivating. 
2. While equipping meta-learning with contrastive objective is not a new concept (e.g., [1], [2]) the implementation covers major meta-learning methods, including optimization- metric- and amortization-based methods. This means that the study is more comprehensive than previous studies.
3. The numerical results are promising. Code is provided - reproducibility is commendable.


[1] Gondal et al. Function Contrastive Learning of Transferable Meta-Representations. In ICML 2021.
[2] Mathieu et al. On Contrastive Representations of Stochastic Processes. In NeurIPS 2021.

Weaknesses:
My primary concern lies in the specific contrastive strategy employed.

1. The contrastive objective aims to minimize intra-task distances while maximizing inter-task distances. However, the absence of appropriate regularization or constraints raises concerns about potential model representation collapse. This collapse could manifest as representations converging to trivial solutions, such as constant vectors or confinement to low-dimensional subspaces. The authors should address whether they have considered these risks. 
2. In addition, I wonder why the contrastive objective does not follow commonly studied ones, e.g., InfoNCE, in contrastive learning.
3. Moreover, the meta-objective necessitates computations involving representations from different tasks within a batch during each episode. Since the paper lacks a discussion on training and inference efficiency, the impact of this strategy on scalability is unclear.
4. The effectiveness of this method is likely dependent on hyperparameters tuning and the sampling strategy for creating subsets of tasks. As aforementioned, incorporating contrastive learning into meta-learning is not something completely new, even though, the authors do not include detailed discussion on how different strategies would affect the performance/efficiency, which is something to be expected on my end.

Limitations:
The authors do have discussed the limitation of this study.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to enhance the meta-learning process by implementing more robust supervision within the model space. Specifically, the authors seek to augment learning capabilities through model alignment and discrimination, aiming to approximate human-like rapid learning abilities. They propose that models trained on tasks within the same super-task exhibit similarity, while those trained on different task sets generalize effectively across diverse tasks. To achieve this, the authors devise a contrastive framework that remains independent of the specific meta-learning algorithms employed. This framework encourages closer alignment of representations for models adapted to similar tasks while pushing representations apart for models derived from dissimilar tasks. Moreover, the framework seamlessly integrates with optimization-based, metric-based, and amortization-based meta-learning methods. The approach demonstrates improvements across standard benchmarks in all evaluated scenarios. Furthermore, the proposed method shows promise for integration into the in-context learning of large language models, resulting in observed enhancements.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
•  The paper is well-written and easy to understand. It's reasonable to enforce the model to emulate human learning capabilities through alignment and discrimination.

•  The proposed contrastive learning framework is versatile and applicable to most meta-learning methods.

•  The proposed method consistently improves upon existing meta-learning methods across standard benchmarks.

Weaknesses:
•  The paper lacks validation on MetaDataset[A], which is a common large-scale dataset for few-shot learning tasks.

•  There is a need for sensitivity analysis on certain hyperparameters, such as λ and the choice of similarity function for contrastive learning.

[A] Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples, ICLR2020.

Limitations:
Please see weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals learning to learn (meta-learning) problem, from the perspective of exploring inner-task and intra-task relationship. Specifically, this paper proposed a Contrastive meta-objective by exploring intra- and inter-task distances and severed as an additional term for training objective (in addtion to classification loss). 

Experiments are conducted on both conventional few-shot image classification and in-context learning settings. Common benchmarks are used to compare with simple few-shot methods such as MAML, ProtoNet, SCNAPs. For In-context learning, simple synthetic functions are used for comparison.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The overall method makes sense. Designing intra- and inter-distance to explore the task-level contrastive information and further introduce this into to the meta-learning objective is reasonable. The diverse tasks naturally compose the contrastive pairs, useful for training. 
- The proposed method is general and can be applied on top of different few-shot classification/regression methods, such as metric-based, optimisation-based, simpleCNAPs, and in-context learning. 
- The performance gains over these simple baselines are significant, showing the effectiveness of the proposed method.

Weaknesses:
- Technically, the proposed contrastive meta-objective is similar to the idea of supervised contrastive learning, which already provides good insights to the representation learning and deep learning community. Therefore, the proposed method is kind of incremental and provides less new knowledge to the field. 
- The method is only verified on top of simplest baseline methods (MAML, ProtoNet, etc). In meta-learning, various works have been proposed to investigate the possible exploration of the task-level information for improved meta-learning, such as [R1-R4], to name a few. 
However, none of those previous efforts were discussed or compared. Only beating the naive baseline cannot comprehensively demonstrate the advantages of this paper. 
- Experimentally, the proposed method tries to show its superior performance over simple baselines rather than SOTA. This is less convincing. 
- The in-context learning experiments are only on simple synthetic data, lack of significance. 
- The tile and scope: Learning-to-learn is very general, but in fact only classification related experiments are conducted. By convention, the learning-to-learn approaches will also verify on reinforcement learning. 

[R1] Fei, N., Lu, Z., Xiang, T., & Huang, S. (2021). MELR: Meta-learning via modeling episode-level relationships for few-shot learning. In International Conference on Learning Representations.   
[R2] Agarwal, P., & Singh, S. (2023). Exploring intra-task relations to improve meta-learning algorithms. arXiv preprint arXiv:2312.16612.   
[R3] Han, J., Cheng, B., & Lu, W. (2021). Exploring task difficulty for few-shot relation extraction. arXiv preprint arXiv:2109.05473.   
[R4] Zhang, Tao. ""Episodic-free Task Selection for Few-shot Learning."" arXiv preprint arXiv:2402.00092 (2024).

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a contrastive meta-objective that can be applied to various meta-learning methods. Also, interpreting in-context learning as a meta-learning formulation, extended the proposed method to in-context learning. Specifically, the objective is to contrast task identity obtained after episode optimization. The task identity is defined as the model weight or the feature obtained by feed-forwarding, in the case of in-context learning. Finally, the authors demonstrated the superiority of the proposed method by applying it to several meta-learning methods to improve their performance.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The proposed method can improve diverse meta-learning methods.
- Contrasting task identity sounds intuitive.

Weaknesses:
In general, the text is not easy to understand.

- Not self-stained figures.
  - In Figure 1, it's hard to understand what $h_{w_i}$ and $w_i$ are since the caption has no explanation.
  - In Table 1, the caption could have included the definition of $g$ or $\psi$.
  - Figure 2 is hard to read; (b) and (e) missed the x, y-axis meaning and are too small to see something.
- Experiment details are missing.
  - Hard to understand Section 5.1. Though the sine wave regression problem is well-known in this domain, it's hard to interpret results without task definitions.
  - The tasks in Section 5.3 are not clearly defined.
- An ablation study would be helpful.
  - How to decide distance function $\phi$?
  - What if increasing the number of task-sampling $K$?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Yq2dYPkfRU;"REVIEW 
Summary:
This paper studies the generalization measured by gradients via a uniform gradient stability. For $\beta$-uniformly stable algorithms, the paper gives generalization bounds of order $O(1/n+\beta+\sqrt{E_Z[\|\nabla f(A(S);Z)\|_2^2]/n})$, which yields fast rates if $E_Z[\|\nabla f(A(S);Z)\|_2^2$ is small. The paper then uses this generalization measured by gradients to derive generalization error bounds under a PL condition. Applications to empirical risk minimization, gradient descent and stochastic gradient descent are given for strongly convex, smooth and Lipschitz problems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper gives high-probability bounds for generalization gap on gradients, which include the gradient norm at the minimizer. This can imply fast rates in an interpolation setting. Under the PL condition, this gives fast rates of order $O(1/n^2)$.

The paper provides comprehensive applications to several algorithms such as empirical risk minimization, gradient descent and stochastic gradient.

Weaknesses:
The high-probability analysis based on uniform stability follows largely from existing work. I do not see enough novelty in the analysis. It would be helpful if the authors can summarize the challenges in the analysis and their novelty. 

As stated in the paper, Theorem 1 and Theorem 2 only improve the existing results by a constant factor. This improvement is not significant. 

As stated in the paper, the generalization by gradients is mostly interesting for nonconvex problems. However, for the applications in Section 4, the paper considers strongly convex problems. Also the results require smoothness and Lipschitz continuity. These assumptions seem to be a bit strong.

The paper gives fast rates under the case $F(w^*)=O(1/n)$ in Section 4. Note that Section 4 considers strongly convex problems. Then, the objective function should be of the form $F(w)=G(w)+\mu\|w\|^2$, where $G$ is related to loss. Then, if we require $F(w^*)=O(1/n)$, one needs $\mu\|w^*\|^2=O(1/n)$. Suppose we assume $\|w^*\|=O(1)$. Then, this requires $\mu=O(1/n)$. In this case, the generalization bound would be vacuous since $n\mu=O(1)$.

For SGD, the computation cost seems to be high. For example, in Theorem 6, the paper requires $T=n^4$ while in Theorem 13 the paper requires $T=n^2$. This high computational cost may not be appealing for large-scale problems.

In the proof of Lemma 1, the paper uses $\|\nabla F(A(S))\|_2\geq \mu\|A(S)-w^*\|$. This inequality does not generally hold under a PL condition. Indeed, Theorem 2 in Karimi et al 2016 require $\|A(S)-w^*\|$ to be replaced by the distance between $A(S)$ and the set of minimizers.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work shows high probability excess risk bounds of $O(1/n^2)$ for several algorithms under strong convexity, smoothness, Lipschitz continuity and low noise assumptions using algorithmic stability.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The results of the paper are interesting, showing a risk bound of $O(\frac{1}{n^2})$ using algorithmic stability.

2. The paper uses a novel technique, involving the stability of gradients to demonstrate excess risk bounds and presents applications of this technique in convex optimization.

Weaknesses:
1. The problem setup of the paper is not clearly detailed before the technical section, including the assumptions used for proving the results.

2. The presentation of results and related works is somewhat lacking. It would be beneficial if the authors summarized the results from previous work and compared them to the results in the current paper, including the set of assumptions made in each work.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper achieves the high probability excess risk bounds $\mathcal{O}(1/n^2)$ for empirical risk minimization, projected gradient descent and stochastic gradient descent under strong convexity, smoothness and Lipschitz continuity assumptions.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Please see Summary.

Weaknesses:
1.The paragraph from line 52 to line 65 first analyzes the non-convex problems. Then, it suddenly mentions that this paper explores the stability of stochastic convex optimization algorithms with strongly convex losses in line 61. And, it doesn’t mention its non-convex analysis. It is so confusing. So, I suggest authors rewrite this paragraph to benefit readers’ understanding.

2.The paragraph from line 70 to line 73 is unnecessary since we can not obtain any useful information.

3.In Related Work, it is unnecessary to list the literature related to uniform convergence since it is not helpful for readers’ understanding of the contributions of this paper. It may be better that authors list the detailed literature related to high probability bound.

4.Theorem 1 in this paper is not the sharpest p-moment bound for sums of vector-valued functions. Authors demonstrate their bound is indeed tighter than Theorem 1 of [1]. However, [2] also provided a bound (Theorem 1) that is likely tighter than Theorem 1 in this paper. Note that, [2] also used Marcinkiewicz-Zygmund’s inequality to prove their bound. Besides, the paragraph from line 131 to line 136 states “On the other hand, in Section 3.2, we will carefully construct vector-valued functions which satisfies all the assumptions in Theorem 1 and ensures M = 0 at the same time. Under this condition, we can eliminate the first term.”. This point is also considered in Theorem 1 of [2]. I think that authors just consider the improvement to [1], but omit other related work.

[1]J. Fan and Y. Lei. High-probability generalization bounds for pointwise uniformly stable algorithms. Applied and Computational Harmonic Analysis, 70:101632, 2024.

[2]X. Yuan, P. Li. Exponential generalization bounds with near-optimal rates for $L_q$-stable algorithms. ICLR, 2023.

5.In Section 3.2, authors build some relationships between generalization error and stability parameter $\beta$. Authors think these relationships are under non-convex, non-smooth, non-PL conditions. These bounds are not the final generalization bounds but the relationships. After giving the stability bounds, the generalization bounds are finally determined. However, in Section 4, authors provide the stability bounds under (strongly) convex and smooth conditions. Therefore, authors didn’t remove (strongly) convex and smooth conditions for generalization analysis.

6.The symbol $M$ is repeatedly used in Theorem 1 and Theorem 2. Therefore, I suggest authors should carefully check their symbol settings.

7.In Remark 4, authors compare their Theorem 3 with the bound in [3]. It is unfair since, as mentioned in the above 5., the bound in [3] is a final result but Theorem 3 is not.

[3]Y. Xu and A. Zeevi. Towards optimal problem dependent generalization error bounds in statistical learning theory. Mathematics of Operations Research, 2024.

8.In line 633, $F(A(S)) - F(A(S))$ is wrong.

9.In line 633, Equation (31) should be an inequality.

10.In line 152, authors state “In nonconvex problems, we can only find a local minimizer by optimization algorithms which may be far away from the global minimizer. Thus the convergence does not make much sense in function values.”. So, they use uniform stability in gradients. However, in Section 4, they provide some uniform stability bounds in gradients for strongly convex problems. It is a paradox.

11.The form of the relationship in Theorem 3 is very normal. The method to obtain an excess risk bound $\mathcal{O}(1/n^2)$ is very simple. I think other normal generalization results (like [1]) in gradients can derive the excess risk bound $\mathcal{O}(1/n^2)$. The main contribution of this paper may be the simple method combining PL condition with some usual decompositions as shown in Proof of Remark 5. However, as mentioned in the above 10., the uniform stability in function values is more unreliable than the one in gradients under strongly convex condition.

Limitations:
Considering the 4. in Weaknesses, I suggest the author reconsider whether their result is the sharpest.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the standard statistical learning setting and derives $\mathcal{O}(\frac{1}{n ^ {2}})$ ($n$ denotes the number of samples) high-probability bounds for the excess risk $F(A(S)) - \inf_{w} F(w)$ ($A$ denotes the algorithm and $S$ denotes the training set) of ERM, PGD, and SGD. The best-known bounds prior to this work were $\mathcal{O}(\frac{\log n}{n})$ for ERM, PGD, that was derived using algorithmic stability, and $\mathcal{O}(\frac{1}{n ^ {2}})$  or ERM, SGD that was derived using uniform convergence. However, the latter demanded $n = \Omega(d)$ samples, thereby introducing the an undesirable dependence on $d$. The current paper shows that it is possible to obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds for the algorithms (without any dependence on $d$) under the lens of stability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The considered problem is interesting and the contributions of the paper show that sharper bounds (at par with uniform convergence, albeit without any dependence on $d$) are possible for ERM, SGD, and PGD for strongly convex and smooth stochastic convex optimization. Below I explain the roadmap taken by the authors in doing so, which also sheds light on some of the other aspects of the paper.

1) The authors first study the generalization gap via gradients, i.e. the quantity $\||\nabla F(A(S)) - \nabla F_S(A(S)\||$ for the statistical learning setting under the assumption that the function is Lipschitz and the algorithm is uniformly stable in gradients (Theorem 1 and 2). Under the nonconvex setting, the authors obtain dimension-independent bound for the generalization gap via gradients. This bound is subsequently studied under the assumption that the function is smooth and satisfies the Polyak-Lojaseiwicz (PL) condition. The obtained bound is a function of the gradient norm obtained at the end of optimization, i.e. $\|| \nabla F_S(A(S) \||$. 

2) To obtain excess risk bound for the algorithms, the (i) authors show that the algorithms are uniformly stable in gradients; (ii) translate the excess risk bound to a bound on the gradient via the PL inequality (recall from the premise that these algorithms are analyzed in the strongly convex and smooth setting of stochastic convex optimization, therefore PL holds vacuously); (iii) use triangle inequality to relate the bound on the gradient norm to the generalization gap, and use the bound on the generalization gap via gradients (see 1 above).

Weaknesses:
I found several typos in the main theorems in section 4. For example, the stability equation in Lemma 4 should be written with respect to the output at the iteration instead of the output of the ERM.  Also, why is the reference in Theorems in section 4 to Theorem 3, instead of Lemma 1? From my understanding (explained in the Strengths above), Lemma 1 is a specific instantiation of Theorem 3 to smooth + PL functions, which is exactly the premise in Section 4. What exactly is $w$ in the bound $F(w) - F(w^\star)$ in Theorem 5? I expect it to $w_{T + 1}$ (or $w_T$).

I don't see the point of Marcinkiewicz-Zygmund’s inequality with improved constants. The whole paper is about the improved dependence with respect to $n$, so I don't see a point in improving the specific constants in the inequality. 

I need some more clarification in lines 204--207. The authors state that Klochkov and Zhivotovskiy obtained $\mathcal{O}(\frac{1}{n})$ style bounds for the excess risk. What's the assumption on $f$ considered by them? The authors mention that they can obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds with an extra PL and smoothness assumption. Is this something for which Klochkov and Zhivotovskiy, 2021 could only obtain a suboptimal $\mathcal{O}(\frac{1}{n})$ bound? Earlier, my interpretation was that this work obtained $\frac{1}{n}$ bounds for ERM, and PGD for smooth and strongly convex stochastic convex optimization, but lines 204--207 made my understanding unclear. I want to make sure the authors are not invoking extra assumptions to get improved dependence.

Along similar lines as above, Lines 259--261 seem to be saying inconsistent things (is the assumption just smoothness, or strong convexity and smoothness). I would appreciate clarifications from the authors.

Minor Typos: 1) In definition 1, $\gamma$ and $\mu$ should be strictly positive; (2) Line 182: $\gamma$-smooth instead of $\gamma$-smoothness.

Based on the authors' responses, I would be happy to revise my assessment of the paper.

Limitations:
Yes, the authors have adequately addressed this.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the generalization measured by gradients via a uniform gradient stability. For $\beta$-uniformly stable algorithms, the paper gives generalization bounds of order $O(1/n+\beta+\sqrt{E_Z[\|\nabla f(A(S);Z)\|_2^2]/n})$, which yields fast rates if $E_Z[\|\nabla f(A(S);Z)\|_2^2$ is small. The paper then uses this generalization measured by gradients to derive generalization error bounds under a PL condition. Applications to empirical risk minimization, gradient descent and stochastic gradient descent are given for strongly convex, smooth and Lipschitz problems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper gives high-probability bounds for generalization gap on gradients, which include the gradient norm at the minimizer. This can imply fast rates in an interpolation setting. Under the PL condition, this gives fast rates of order $O(1/n^2)$.

The paper provides comprehensive applications to several algorithms such as empirical risk minimization, gradient descent and stochastic gradient.

Weaknesses:
The high-probability analysis based on uniform stability follows largely from existing work. I do not see enough novelty in the analysis. It would be helpful if the authors can summarize the challenges in the analysis and their novelty. 

As stated in the paper, Theorem 1 and Theorem 2 only improve the existing results by a constant factor. This improvement is not significant. 

As stated in the paper, the generalization by gradients is mostly interesting for nonconvex problems. However, for the applications in Section 4, the paper considers strongly convex problems. Also the results require smoothness and Lipschitz continuity. These assumptions seem to be a bit strong.

The paper gives fast rates under the case $F(w^*)=O(1/n)$ in Section 4. Note that Section 4 considers strongly convex problems. Then, the objective function should be of the form $F(w)=G(w)+\mu\|w\|^2$, where $G$ is related to loss. Then, if we require $F(w^*)=O(1/n)$, one needs $\mu\|w^*\|^2=O(1/n)$. Suppose we assume $\|w^*\|=O(1)$. Then, this requires $\mu=O(1/n)$. In this case, the generalization bound would be vacuous since $n\mu=O(1)$.

For SGD, the computation cost seems to be high. For example, in Theorem 6, the paper requires $T=n^4$ while in Theorem 13 the paper requires $T=n^2$. This high computational cost may not be appealing for large-scale problems.

In the proof of Lemma 1, the paper uses $\|\nabla F(A(S))\|_2\geq \mu\|A(S)-w^*\|$. This inequality does not generally hold under a PL condition. Indeed, Theorem 2 in Karimi et al 2016 require $\|A(S)-w^*\|$ to be replaced by the distance between $A(S)$ and the set of minimizers.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work shows high probability excess risk bounds of $O(1/n^2)$ for several algorithms under strong convexity, smoothness, Lipschitz continuity and low noise assumptions using algorithmic stability.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The results of the paper are interesting, showing a risk bound of $O(\frac{1}{n^2})$ using algorithmic stability.

2. The paper uses a novel technique, involving the stability of gradients to demonstrate excess risk bounds and presents applications of this technique in convex optimization.

Weaknesses:
1. The problem setup of the paper is not clearly detailed before the technical section, including the assumptions used for proving the results.

2. The presentation of results and related works is somewhat lacking. It would be beneficial if the authors summarized the results from previous work and compared them to the results in the current paper, including the set of assumptions made in each work.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper achieves the high probability excess risk bounds $\mathcal{O}(1/n^2)$ for empirical risk minimization, projected gradient descent and stochastic gradient descent under strong convexity, smoothness and Lipschitz continuity assumptions.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Please see Summary.

Weaknesses:
1.The paragraph from line 52 to line 65 first analyzes the non-convex problems. Then, it suddenly mentions that this paper explores the stability of stochastic convex optimization algorithms with strongly convex losses in line 61. And, it doesn’t mention its non-convex analysis. It is so confusing. So, I suggest authors rewrite this paragraph to benefit readers’ understanding.

2.The paragraph from line 70 to line 73 is unnecessary since we can not obtain any useful information.

3.In Related Work, it is unnecessary to list the literature related to uniform convergence since it is not helpful for readers’ understanding of the contributions of this paper. It may be better that authors list the detailed literature related to high probability bound.

4.Theorem 1 in this paper is not the sharpest p-moment bound for sums of vector-valued functions. Authors demonstrate their bound is indeed tighter than Theorem 1 of [1]. However, [2] also provided a bound (Theorem 1) that is likely tighter than Theorem 1 in this paper. Note that, [2] also used Marcinkiewicz-Zygmund’s inequality to prove their bound. Besides, the paragraph from line 131 to line 136 states “On the other hand, in Section 3.2, we will carefully construct vector-valued functions which satisfies all the assumptions in Theorem 1 and ensures M = 0 at the same time. Under this condition, we can eliminate the first term.”. This point is also considered in Theorem 1 of [2]. I think that authors just consider the improvement to [1], but omit other related work.

[1]J. Fan and Y. Lei. High-probability generalization bounds for pointwise uniformly stable algorithms. Applied and Computational Harmonic Analysis, 70:101632, 2024.

[2]X. Yuan, P. Li. Exponential generalization bounds with near-optimal rates for $L_q$-stable algorithms. ICLR, 2023.

5.In Section 3.2, authors build some relationships between generalization error and stability parameter $\beta$. Authors think these relationships are under non-convex, non-smooth, non-PL conditions. These bounds are not the final generalization bounds but the relationships. After giving the stability bounds, the generalization bounds are finally determined. However, in Section 4, authors provide the stability bounds under (strongly) convex and smooth conditions. Therefore, authors didn’t remove (strongly) convex and smooth conditions for generalization analysis.

6.The symbol $M$ is repeatedly used in Theorem 1 and Theorem 2. Therefore, I suggest authors should carefully check their symbol settings.

7.In Remark 4, authors compare their Theorem 3 with the bound in [3]. It is unfair since, as mentioned in the above 5., the bound in [3] is a final result but Theorem 3 is not.

[3]Y. Xu and A. Zeevi. Towards optimal problem dependent generalization error bounds in statistical learning theory. Mathematics of Operations Research, 2024.

8.In line 633, $F(A(S)) - F(A(S))$ is wrong.

9.In line 633, Equation (31) should be an inequality.

10.In line 152, authors state “In nonconvex problems, we can only find a local minimizer by optimization algorithms which may be far away from the global minimizer. Thus the convergence does not make much sense in function values.”. So, they use uniform stability in gradients. However, in Section 4, they provide some uniform stability bounds in gradients for strongly convex problems. It is a paradox.

11.The form of the relationship in Theorem 3 is very normal. The method to obtain an excess risk bound $\mathcal{O}(1/n^2)$ is very simple. I think other normal generalization results (like [1]) in gradients can derive the excess risk bound $\mathcal{O}(1/n^2)$. The main contribution of this paper may be the simple method combining PL condition with some usual decompositions as shown in Proof of Remark 5. However, as mentioned in the above 10., the uniform stability in function values is more unreliable than the one in gradients under strongly convex condition.

Limitations:
Considering the 4. in Weaknesses, I suggest the author reconsider whether their result is the sharpest.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the standard statistical learning setting and derives $\mathcal{O}(\frac{1}{n ^ {2}})$ ($n$ denotes the number of samples) high-probability bounds for the excess risk $F(A(S)) - \inf_{w} F(w)$ ($A$ denotes the algorithm and $S$ denotes the training set) of ERM, PGD, and SGD. The best-known bounds prior to this work were $\mathcal{O}(\frac{\log n}{n})$ for ERM, PGD, that was derived using algorithmic stability, and $\mathcal{O}(\frac{1}{n ^ {2}})$  or ERM, SGD that was derived using uniform convergence. However, the latter demanded $n = \Omega(d)$ samples, thereby introducing the an undesirable dependence on $d$. The current paper shows that it is possible to obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds for the algorithms (without any dependence on $d$) under the lens of stability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The considered problem is interesting and the contributions of the paper show that sharper bounds (at par with uniform convergence, albeit without any dependence on $d$) are possible for ERM, SGD, and PGD for strongly convex and smooth stochastic convex optimization. Below I explain the roadmap taken by the authors in doing so, which also sheds light on some of the other aspects of the paper.

1) The authors first study the generalization gap via gradients, i.e. the quantity $\||\nabla F(A(S)) - \nabla F_S(A(S)\||$ for the statistical learning setting under the assumption that the function is Lipschitz and the algorithm is uniformly stable in gradients (Theorem 1 and 2). Under the nonconvex setting, the authors obtain dimension-independent bound for the generalization gap via gradients. This bound is subsequently studied under the assumption that the function is smooth and satisfies the Polyak-Lojaseiwicz (PL) condition. The obtained bound is a function of the gradient norm obtained at the end of optimization, i.e. $\|| \nabla F_S(A(S) \||$. 

2) To obtain excess risk bound for the algorithms, the (i) authors show that the algorithms are uniformly stable in gradients; (ii) translate the excess risk bound to a bound on the gradient via the PL inequality (recall from the premise that these algorithms are analyzed in the strongly convex and smooth setting of stochastic convex optimization, therefore PL holds vacuously); (iii) use triangle inequality to relate the bound on the gradient norm to the generalization gap, and use the bound on the generalization gap via gradients (see 1 above).

Weaknesses:
I found several typos in the main theorems in section 4. For example, the stability equation in Lemma 4 should be written with respect to the output at the iteration instead of the output of the ERM.  Also, why is the reference in Theorems in section 4 to Theorem 3, instead of Lemma 1? From my understanding (explained in the Strengths above), Lemma 1 is a specific instantiation of Theorem 3 to smooth + PL functions, which is exactly the premise in Section 4. What exactly is $w$ in the bound $F(w) - F(w^\star)$ in Theorem 5? I expect it to $w_{T + 1}$ (or $w_T$).

I don't see the point of Marcinkiewicz-Zygmund’s inequality with improved constants. The whole paper is about the improved dependence with respect to $n$, so I don't see a point in improving the specific constants in the inequality. 

I need some more clarification in lines 204--207. The authors state that Klochkov and Zhivotovskiy obtained $\mathcal{O}(\frac{1}{n})$ style bounds for the excess risk. What's the assumption on $f$ considered by them? The authors mention that they can obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds with an extra PL and smoothness assumption. Is this something for which Klochkov and Zhivotovskiy, 2021 could only obtain a suboptimal $\mathcal{O}(\frac{1}{n})$ bound? Earlier, my interpretation was that this work obtained $\frac{1}{n}$ bounds for ERM, and PGD for smooth and strongly convex stochastic convex optimization, but lines 204--207 made my understanding unclear. I want to make sure the authors are not invoking extra assumptions to get improved dependence.

Along similar lines as above, Lines 259--261 seem to be saying inconsistent things (is the assumption just smoothness, or strong convexity and smoothness). I would appreciate clarifications from the authors.

Minor Typos: 1) In definition 1, $\gamma$ and $\mu$ should be strictly positive; (2) Line 182: $\gamma$-smooth instead of $\gamma$-smoothness.

Based on the authors' responses, I would be happy to revise my assessment of the paper.

Limitations:
Yes, the authors have adequately addressed this.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
WxW4nZMD3D;"REVIEW 
Summary:
The authors propose to use network Lasso to learn a multi-task bandit problem with given network structure. More specifically, the network structure has pre-defined unknown clustering structure, where within each cluster all the bandit tasks share the same model. The authors propose a bandit algorithm that can learn and provide a sublinear guarantee. The key difference of this paper with GOBLin in Cesa-Bianchi et al., 2013 is that this paper uses a network Lasso (or something like a group Lasso) penalty while GOBLin use a ridge penalty.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Even though I think the authors overclaimed their contributions, which I will state below, I still feel it's meaningful to discuss network Lasso and design a bandit algorithm based on certain network structure, given the limited literature on multitask bandit. Compared to previous network bandit literature such as Cesa-Bianchi et al., 2013, this paper characterizes the network structure in more detail.

Weaknesses:
1. I think the authors need to provide more real-world examples to show why their network structure (and the correpondingly induced network Lasso bandit algorithm) is pratical, instead of stating their algorithm is good because it provides piecewise constant property in constrast to smoothness of GOBLin in Cesa-Bianchi et al., 2013. The key assumption in this paper is that the network structure is given, and the network can be splitted into connected clusters, within which the task parameters are the same. Can authors find or describe a couple of pratical examples/datasets where such a network exists, given that bandit is a very pratical problem?

2. The literature review that compare with the previous literature are not very accurate and sufficient IMO. For example, the authors mention Gentile et al., 2014 and Li et al., 2019 can cause overconfidence in constructing clusters. However, these algorithms do not have prior information about clusters such as a given network and thus they have to learn the clusters conditioned on the task similarities. In that sense, these algorithms are more pratical because oftentimes in practice network information is lacking. Here one should also add a related reference Context-Based Dynamic Pricing with Online Clustering by Miao et al., 2022. There are also robust multitask bandit algorithms (e.g., Multitask Learning and Bandits via Robust Statistics by Xu and Bastani, 2024) that can also solve the network bandit problem if the network structure follows certain assumptions; Multi-Task Learning for Contextual Bandits by Deshmukh et al., 2017 discuss a multitask bandit problem but use kernal based method. I suggest the authors add a more detailed literature review to discuss their paper's connection with these current multitask bandit algorithms. 

3. Typically, greedy algorithm (the method proposed in this paper is greedy too due to Assumption 2) has better performance in bandit simulations compared to UCB-based algorithm (see Bastani and Bayati, 2020). Therefore, it's not a fair comparison in Figure 1, where all benchmarks are UCB algorithms. I think it's necessary to add the following benchmarks: OLS-Bandit or Lasso-Bandit (Bastani and Bayati, 2020) without task sharing, and Cella et al., 2023 which use low-rank structure to do multitask bandit, and also a few others mentioned above as a benchmark to show that the network structure indeed helps, fixing the difference due to UCB and greedy algorithms.

4. I feel it's a false claim that the regret bound in Theorem 3 ""doesn't depend on the dimension"" and it's due to the concentration inequality from Hsu et al., 2012. Intuitively, the regret bound should depend on the dimension unless one assume that the number of tasks in a cluster is d-dependent so the regret bound is smaller compared to a typical single bandit regret bound. I think the reason why here the bound seems to be d-independent is because the dimension $d$ is hiden in the problem-dependent parameter $\phi$. Since the authors assume the context $x$ has norm 1, the minimum eigenvalue of $E[xx^\top]$ should scale as $1/d$, and hence $\phi^2$. I don't think a typical tail inequality in Hsu et al., 2012 can improve the bound regarding the context dimension. 

5. I think the asymptotic assumptions in Theorem 3 is incompatible with the finite-sample analysis in a typical bandit analysis and looks weird. I suggest the authors keep the isoperimetric ratio and centrality index as part of the regret bound (instead of forcing them out using the asymptotic assumptions), even though it might add an additional T-linear term due to the misspecification error caused by the inter-cluster edge connections. I feel that's the case because in the extreme case when each cluster has size 1, there will be misspecification error penalizing tasks connected the inter-cluster edges towards each other. I think it's totally fine to have such non-sublinear terms to provide a more comprehensive understanding of the limit of such network structures. 

I am willing to raise my rating if the authors can solve my questions and concerns.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, authors work under the multi-task contextual bandit settings by representing the task correlations through the graph structure. To solve this problem, authors propose an algorithm that utilizes a linear regression formulation with Lasso constraint in terms of the node connectivity. Theoretical analysis as well as experiments against several baselines are presented to demonstrate the effectiveness of proposed method.

-	The paper is generally well-written, with crisply clear descriptions of required assumptions, and the proposed solution is intuitive and well-motivated.

-	Good empirical performances. Authors compare proposed algorithm with several clustering of bandits baselines, showing the effectiveness of the proposed method. The performance gain over existing methods is impressive. 

-	Novel theoretical analysis roadmap. Overall, the theoretical analysis pipeline is novel and the looks promising to me. With the additional introduced RE assumption, authors are able to improve the regret bound to $\tilde{O}(\sqrt{\bar{T}})$ instead of the vanilla time horizon.

-	My major question is regarding the numerous assumptions required for the theoretical analysis. For instance, in Assumption 1, authors assume the candidate arms across different arounds are generated i.i.d. from a fixed distribution. This is different from existing clustering of bandits works, where the candidate arm contexts in each round is conditioned on previous observed arms. In this case, assumption 1 is somehow deviates from the actual applications of recommender systems where the candidate arms of each round are refined along with more information collected from the environment.

-	For the experiments, authors have compared against multiple clustering of bandit works. In this case, it would be good if authors can include additional discussions comparing your theoretical outcomes with those of existing clustering of bandits works, which can offer more intuitive comparison with existing approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Please see my comments above.

Weaknesses:
Please see my comments above.

Limitations:
Please see my comments above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the multi-task bandit problem using graph information. The given graph represents the relationships between tasks. Assuming that the preference vector of clustered tasks is constant, the problem is formulated as a network lasso problem to estimate the lasso estimator. 
A modified restricted eigenvalue condition, commonly used in high-dimensional statistics, is defined to derive the oracle inequality for the network lasso estimator on non-i.i.d. data. The oracle inequality of the proposed network lasso estimator is derived under the assumption that the true multi-task Gram matrix satisfies the adapted RE condition.
Based on the derived oracle inequality, a greedy-type algorithm is presented, achieving $\sqrt{T}$ regret. Numerical experiments support the theoretical performance of the proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The proposed algorithm efficiently learns task preference vectors by using graph information that encodes relationships between tasks. Specifically, it employs a network lasso estimator under the assumption that preferences within clustered tasks are constant, demonstrating its effectiveness in high-dimensional contexts.
- The paper adapts the restricted eigenvalue condition from high-dimensional statistics to the graph-based multi-task bandit setting. Based on the adapted RE condition, they established oracle inequality for network lasso estimator and showed that the proposed algorithm achieved $\sqrt{T}$ regert even though I haven't verified every proof in detail.
- The algorithm's performance seems robust even as the number of tasks and dimensions increase.

Weaknesses:
- Since I'm not very familiar with the graph-based multi-task bandit setting, it may be that the concepts explaining the restricted eigenvalue condition (Def 2) are too heavy. It would be helpful to include comparisons or examples from existing RE conditions in high-dimensional statistics or high-dimensional contextual bandits to improve understanding.

Limitations:
The authors have well-addressed the limitations in Appendix D.4 and further research directions in Section 7.

The content discussed in this paper appears to have little to no negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a multi-task contextual bandit algorithm that leverages a graph structure to model relationships between tasks. The algorithm assumes that the preference vectors of the tasks are piecewise constant over the graph, forming clusters. By solving an online network lasso problem with a time-dependent regularization parameter, the algorithm estimates the preference vectors, achieving a sublinear regret bound lower than independent task learning. Theoretical findings are supported by experimental evaluations against other graph bandit and online clustering algorithms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1) The paper introduces a approach by incorporating graph structures to model relationships between tasks.

(2) The algorithm is supported by comprehensive theoretical analysis, including a oracle inequality and a regret bound.

(3) Extensive experiments validate the proposed method, showing that it outperforms existing baselines in terms of cumulative regret, highlighting its practical applicability and effectiveness.

Weaknesses:
(1) The problem setting and algorithm presented are primarily adaptations of existing works, such as Oh et al. [2021]. The main difference is the inclusion of a graph matrix in the user preference vector, but this is not the first algorithm to incorporate a graph in contextual bandits, limiting the overall novelty.

(2)  The i.i.d. assumption in contextual bandits is quite strong. Even in clustering approaches like CLUB, a conditional i.i.d. assumption is used. The current regret upper bound complexity is \(\sqrt{VT}\). There should be special cases where the algorithm can improve over \(V\) to demonstrate a more significant advantage.

(3)  Since 2019, there have been many more works on clustering in bandits. The authors should conduct a broader survey to include these more recent works and relevant baselines. Using SCLUB, which is considered outdated, as a baseline, limits the comprehensiveness and relevance of the comparative analysis.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a multi-task contextual bandit algorithm that leverages a graph structure to model relationships between tasks. The algorithm assumes that the preference vectors of the tasks are piecewise constant over the graph, forming clusters. By solving an online network lasso problem with a time-dependent regularization parameter, the algorithm estimates the preference vectors, achieving a sublinear regret bound lower than independent task learning. Theoretical findings are supported by experimental evaluations against other graph bandit and online clustering algorithms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1) The paper introduces a approach by incorporating graph structures to model relationships between tasks.

(2) The algorithm is supported by comprehensive theoretical analysis, including a oracle inequality and a regret bound.

(3) Extensive experiments validate the proposed method, showing that it outperforms existing baselines in terms of cumulative regret, highlighting its practical applicability and effectiveness.

Weaknesses:
(1) The problem setting and algorithm presented are primarily adaptations of existing works, such as Oh et al. [2021]. The main difference is the inclusion of a graph matrix in the user preference vector, but this is not the first algorithm to incorporate a graph in contextual bandits, limiting the overall novelty.

(2)  The i.i.d. assumption in contextual bandits is quite strong. Even in clustering approaches like CLUB, a conditional i.i.d. assumption is used. The current regret upper bound complexity is \(\sqrt{VT}\). There should be special cases where the algorithm can improve over \(V\) to demonstrate a more significant advantage.

(3)  Since 2019, there have been many more works on clustering in bandits. The authors should conduct a broader survey to include these more recent works and relevant baselines. Using SCLUB, which is considered outdated, as a baseline, limits the comprehensiveness and relevance of the comparative analysis.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose to use network Lasso to learn a multi-task bandit problem with given network structure. More specifically, the network structure has pre-defined unknown clustering structure, where within each cluster all the bandit tasks share the same model. The authors propose a bandit algorithm that can learn and provide a sublinear guarantee. The key difference of this paper with GOBLin in Cesa-Bianchi et al., 2013 is that this paper uses a network Lasso (or something like a group Lasso) penalty while GOBLin use a ridge penalty.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Even though I think the authors overclaimed their contributions, which I will state below, I still feel it's meaningful to discuss network Lasso and design a bandit algorithm based on certain network structure, given the limited literature on multitask bandit. Compared to previous network bandit literature such as Cesa-Bianchi et al., 2013, this paper characterizes the network structure in more detail.

Weaknesses:
1. I think the authors need to provide more real-world examples to show why their network structure (and the correpondingly induced network Lasso bandit algorithm) is pratical, instead of stating their algorithm is good because it provides piecewise constant property in constrast to smoothness of GOBLin in Cesa-Bianchi et al., 2013. The key assumption in this paper is that the network structure is given, and the network can be splitted into connected clusters, within which the task parameters are the same. Can authors find or describe a couple of pratical examples/datasets where such a network exists, given that bandit is a very pratical problem?

2. The literature review that compare with the previous literature are not very accurate and sufficient IMO. For example, the authors mention Gentile et al., 2014 and Li et al., 2019 can cause overconfidence in constructing clusters. However, these algorithms do not have prior information about clusters such as a given network and thus they have to learn the clusters conditioned on the task similarities. In that sense, these algorithms are more pratical because oftentimes in practice network information is lacking. Here one should also add a related reference Context-Based Dynamic Pricing with Online Clustering by Miao et al., 2022. There are also robust multitask bandit algorithms (e.g., Multitask Learning and Bandits via Robust Statistics by Xu and Bastani, 2024) that can also solve the network bandit problem if the network structure follows certain assumptions; Multi-Task Learning for Contextual Bandits by Deshmukh et al., 2017 discuss a multitask bandit problem but use kernal based method. I suggest the authors add a more detailed literature review to discuss their paper's connection with these current multitask bandit algorithms. 

3. Typically, greedy algorithm (the method proposed in this paper is greedy too due to Assumption 2) has better performance in bandit simulations compared to UCB-based algorithm (see Bastani and Bayati, 2020). Therefore, it's not a fair comparison in Figure 1, where all benchmarks are UCB algorithms. I think it's necessary to add the following benchmarks: OLS-Bandit or Lasso-Bandit (Bastani and Bayati, 2020) without task sharing, and Cella et al., 2023 which use low-rank structure to do multitask bandit, and also a few others mentioned above as a benchmark to show that the network structure indeed helps, fixing the difference due to UCB and greedy algorithms.

4. I feel it's a false claim that the regret bound in Theorem 3 ""doesn't depend on the dimension"" and it's due to the concentration inequality from Hsu et al., 2012. Intuitively, the regret bound should depend on the dimension unless one assume that the number of tasks in a cluster is d-dependent so the regret bound is smaller compared to a typical single bandit regret bound. I think the reason why here the bound seems to be d-independent is because the dimension $d$ is hiden in the problem-dependent parameter $\phi$. Since the authors assume the context $x$ has norm 1, the minimum eigenvalue of $E[xx^\top]$ should scale as $1/d$, and hence $\phi^2$. I don't think a typical tail inequality in Hsu et al., 2012 can improve the bound regarding the context dimension. 

5. I think the asymptotic assumptions in Theorem 3 is incompatible with the finite-sample analysis in a typical bandit analysis and looks weird. I suggest the authors keep the isoperimetric ratio and centrality index as part of the regret bound (instead of forcing them out using the asymptotic assumptions), even though it might add an additional T-linear term due to the misspecification error caused by the inter-cluster edge connections. I feel that's the case because in the extreme case when each cluster has size 1, there will be misspecification error penalizing tasks connected the inter-cluster edges towards each other. I think it's totally fine to have such non-sublinear terms to provide a more comprehensive understanding of the limit of such network structures. 

I am willing to raise my rating if the authors can solve my questions and concerns.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, authors work under the multi-task contextual bandit settings by representing the task correlations through the graph structure. To solve this problem, authors propose an algorithm that utilizes a linear regression formulation with Lasso constraint in terms of the node connectivity. Theoretical analysis as well as experiments against several baselines are presented to demonstrate the effectiveness of proposed method.

-	The paper is generally well-written, with crisply clear descriptions of required assumptions, and the proposed solution is intuitive and well-motivated.

-	Good empirical performances. Authors compare proposed algorithm with several clustering of bandits baselines, showing the effectiveness of the proposed method. The performance gain over existing methods is impressive. 

-	Novel theoretical analysis roadmap. Overall, the theoretical analysis pipeline is novel and the looks promising to me. With the additional introduced RE assumption, authors are able to improve the regret bound to $\tilde{O}(\sqrt{\bar{T}})$ instead of the vanilla time horizon.

-	My major question is regarding the numerous assumptions required for the theoretical analysis. For instance, in Assumption 1, authors assume the candidate arms across different arounds are generated i.i.d. from a fixed distribution. This is different from existing clustering of bandits works, where the candidate arm contexts in each round is conditioned on previous observed arms. In this case, assumption 1 is somehow deviates from the actual applications of recommender systems where the candidate arms of each round are refined along with more information collected from the environment.

-	For the experiments, authors have compared against multiple clustering of bandit works. In this case, it would be good if authors can include additional discussions comparing your theoretical outcomes with those of existing clustering of bandits works, which can offer more intuitive comparison with existing approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Please see my comments above.

Weaknesses:
Please see my comments above.

Limitations:
Please see my comments above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the multi-task bandit problem using graph information. The given graph represents the relationships between tasks. Assuming that the preference vector of clustered tasks is constant, the problem is formulated as a network lasso problem to estimate the lasso estimator. 
A modified restricted eigenvalue condition, commonly used in high-dimensional statistics, is defined to derive the oracle inequality for the network lasso estimator on non-i.i.d. data. The oracle inequality of the proposed network lasso estimator is derived under the assumption that the true multi-task Gram matrix satisfies the adapted RE condition.
Based on the derived oracle inequality, a greedy-type algorithm is presented, achieving $\sqrt{T}$ regret. Numerical experiments support the theoretical performance of the proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The proposed algorithm efficiently learns task preference vectors by using graph information that encodes relationships between tasks. Specifically, it employs a network lasso estimator under the assumption that preferences within clustered tasks are constant, demonstrating its effectiveness in high-dimensional contexts.
- The paper adapts the restricted eigenvalue condition from high-dimensional statistics to the graph-based multi-task bandit setting. Based on the adapted RE condition, they established oracle inequality for network lasso estimator and showed that the proposed algorithm achieved $\sqrt{T}$ regert even though I haven't verified every proof in detail.
- The algorithm's performance seems robust even as the number of tasks and dimensions increase.

Weaknesses:
- Since I'm not very familiar with the graph-based multi-task bandit setting, it may be that the concepts explaining the restricted eigenvalue condition (Def 2) are too heavy. It would be helpful to include comparisons or examples from existing RE conditions in high-dimensional statistics or high-dimensional contextual bandits to improve understanding.

Limitations:
The authors have well-addressed the limitations in Appendix D.4 and further research directions in Section 7.

The content discussed in this paper appears to have little to no negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
WpEaUIBIWH;"REVIEW 
Summary:
The paper addresses unsupervised anomaly detection by proposing a method named UniCAD. The authors aim to enhance anomaly detection performance by establishing a theoretical connection between representation learning, clustering, and anomaly detection. They introduce a unified framework that jointly optimizes these three components, using a probabilistic mixture model and a Student's-t distribution for robust representation learning and clustering. The framework also includes an anomaly-aware data likelihood objective, which reduces the impact of anomalous data on the learning process. Additionally, the authors propose a gravity-inspired anomaly scoring method that leverages relationships between samples and clusters.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Modeling the connection between representation learning, clustering, and anomaly detection is highly relevant. This paper effectively demonstrates how these three tasks are interrelated and can be jointly optimized to improve anomaly detection performance.

2. The paper is well-written, presenting its hypothesis and method clearly. 

3. The results are impressive and demonstrate the effectiveness of UniCAD.

Weaknesses:
1. The ablation study on the hyperparameters $k$ and $l$ is insufficient. The authors only present results from a single dataset, satimage-2, where their method achieves an almost perfect score. It would be more informative to perform ablation studies across all 30 datasets or at least a subset where the model also shows lower performance. This broader analysis would demonstrate how these hyperparameters affect the average ranking of the method, similar to the results reported in the paper's table.

2. The authors introduce a $g(\Theta)$ term to prevent shortcut solutions, mentioning it in Equation 15. However, they do not discuss its importance or impact on performance after its introduction. Key questions remain unanswered, such as how the $g(\Theta)$ term affects the model's performance, what happens if it is removed, and how the autoencoder is implemented. These details are crucial, as the regularization term may significantly influence the results.

Limitations:
The authors mention some of the limitations, but they do not address the potential negative impact of the work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes UniCAD, a theoretically unified framework for representation learning, clustering, and anomaly detection. This paper first introduces the mixture of Student-t distribution $p(x|\Theta, \Phi)$ with degree of freedom $\nu=1$ based on a representation learner $f_\Theta$ using NN. Then, this paper combines with an anomaly indicator $\delta$ for maximum likelihood estimation. Parameters $(\Theta, \Phi)$ are optimized by EM algorithm and SGD. In addition, when detecting anomalies, an improved score is used with reference to gravity. The UniCAD achieved good performance on experiments with various datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well written and easy to follow.
- Good experimental results.

Weaknesses:
- We have several questions about the proposed method and experiments. Please see Qustions.
- The comparison with DeepSVDD and DIF is excellent, but I think the paper also needs to be compared with other Deep anomaly detection methods. For example, DROCC [1].

[1] Goyal, Sachin, et al. ""DROCC: Deep robust one-class classification. ""International conference on machine learning. PMLR, 2020.

Limitations:
- Hyper-parameter sensitivity seems to be one limitation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel probabilistic mixture model for unsupervised anomaly detection (UAD) that unifies representation learning, clustering, and anomaly detection into a single theoretical framework. The proposed UniCAD model addresses the lack of a unified approach in existing methods, which often consider these components separately or in pairs. The experimental results show that UniCAD consistently outperformed other methods in terms of AUC-ROC and AUC-PR. The model’s iterative optimization process using EM was also highlighted as effective and convergent.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- This paper introduces a novel integration of a probabilistic mixture model that unifies representation learning, clustering, and anomaly detection into a single theoretical framework.
- The proposed approach is well-motivated (Fig. 1) and supported by a robust theoretical foundation that maximizes anomaly-aware data likelihood, ensuring the model effectively leverages the interplay between representation learning, clustering, and anomaly detection.
- The paper is well-written, offering clear and comprehensive explanations of the proposed method, including detailed theoretical derivations and intuitive motivations for the design choices. The methodology section is particularly well-structured, logically outlining the steps and equations involved in the proposed model.
- The comprehensive evaluation design underscores the robustness of the proposed method.

Weaknesses:
- The connection between force analysis and anomaly detection, particularly between Equations 7 and 8 in Section 3.2.1, could benefit from further justification. While the analogy is interesting, it may not be immediately intuitive for all readers.
- The iterative optimization process may pose scalability issues for large datasets. An in-depth analysis and discussion of this would further strengthen the quality of this research.
- Although the model maps data to a low-dimensional representation space, the effectiveness of this mapping for very high-dimensional datasets could be explored further.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose UniCAD to jointly model representation learning,
clustering and anomaly detection.  The main objective is maximizing
the product of anomaly indicator (1 is normal, 0 is anomaly) and the
joint probability of instance x_i and cluster c_k given parameters for
representation learning theta and clustering phi.  The joint
probability is decomposed into the prior of c_k and likelihood of
p(x_i|c_k), which is modeled by a Student's-t distribution on the
distance between representation z_i and mean mu_k with covariance
Sigma_k.  p(x_i) is the marginal over c_k.  Anomaly indicator delta is
zero for p(x_i) in the lowest l percentage.  The anomaly score is
1/p(x_i).

Compared to Newton's law of Universal Gravitation, the anomaly score
function has similar components, except for the unit vector r_ik
(which indicates the directions of forces, beyond the
magnitudes). Hence, they incorporated r_ik into their anomaly score
function.

For updating the clustering parameter phi (mixture weights, means,
covariance), they use EM.  In the E-step, they estimate the posterior
p(c_k|x_i).  In the M-step, they estimate phi.  For updating
representation parameters theta, they use gradient descent to minimize
negative log likelihood of instances, together with a reconstruction
loss via an autoencoder to prevent shortcut solutions.

For empirical comparisons, they use 30 tabular data sets and 17
existing algorithms.  The proposed approach generally outperforms the
others in terms of average rank in AUCROC.  The vector version of
anomaly score function is ranked higher than the scalar version.  On
computation time, UniCAD is in the middle among 5 algorithms.  Ablation
studies indicate the contributions of the different components.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution is combining representation learning,
clustering, and anomaly detection in a unified single probabilistic
formulation, which is interesting.

The empirical results indicate that UniCAD compares favorably against
17 existing techniques on 30 tabular datasets.  Compared to four
existing algorithms, computation is not the most intensive.

The paper is generally well written.

Weaknesses:
The clustering part is similar to a typical Gaussian mixture model for clustering via EM, except for t-distribution instead of Gaussian and the scaling factor.

Two neural-network-based approaches were compared.  As UniCAD utilizes
representation learning, comparing with more approaches that utilize
representation learning would be significant.  Approaches without
representation learning have an inherent disadvantage.

Some parts could be clarified--see Questions.

Limitations:
Limitations of the proposed approach do not seem to be mentioned.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose UniCAD to jointly model representation learning,
clustering and anomaly detection.  The main objective is maximizing
the product of anomaly indicator (1 is normal, 0 is anomaly) and the
joint probability of instance x_i and cluster c_k given parameters for
representation learning theta and clustering phi.  The joint
probability is decomposed into the prior of c_k and likelihood of
p(x_i|c_k), which is modeled by a Student's-t distribution on the
distance between representation z_i and mean mu_k with covariance
Sigma_k.  p(x_i) is the marginal over c_k.  Anomaly indicator delta is
zero for p(x_i) in the lowest l percentage.  The anomaly score is
1/p(x_i).

Compared to Newton's law of Universal Gravitation, the anomaly score
function has similar components, except for the unit vector r_ik
(which indicates the directions of forces, beyond the
magnitudes). Hence, they incorporated r_ik into their anomaly score
function.

For updating the clustering parameter phi (mixture weights, means,
covariance), they use EM.  In the E-step, they estimate the posterior
p(c_k|x_i).  In the M-step, they estimate phi.  For updating
representation parameters theta, they use gradient descent to minimize
negative log likelihood of instances, together with a reconstruction
loss via an autoencoder to prevent shortcut solutions.

For empirical comparisons, they use 30 tabular data sets and 17
existing algorithms.  The proposed approach generally outperforms the
others in terms of average rank in AUCROC.  The vector version of
anomaly score function is ranked higher than the scalar version.  On
computation time, UniCAD is in the middle among 5 algorithms.  Ablation
studies indicate the contributions of the different components.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution is combining representation learning,
clustering, and anomaly detection in a unified single probabilistic
formulation, which is interesting.

The empirical results indicate that UniCAD compares favorably against
17 existing techniques on 30 tabular datasets.  Compared to four
existing algorithms, computation is not the most intensive.

The paper is generally well written.

Weaknesses:
The clustering part is similar to a typical Gaussian mixture model for clustering via EM, except for t-distribution instead of Gaussian and the scaling factor.

Two neural-network-based approaches were compared.  As UniCAD utilizes
representation learning, comparing with more approaches that utilize
representation learning would be significant.  Approaches without
representation learning have an inherent disadvantage.

Some parts could be clarified--see Questions.

Limitations:
Limitations of the proposed approach do not seem to be mentioned.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses unsupervised anomaly detection by proposing a method named UniCAD. The authors aim to enhance anomaly detection performance by establishing a theoretical connection between representation learning, clustering, and anomaly detection. They introduce a unified framework that jointly optimizes these three components, using a probabilistic mixture model and a Student's-t distribution for robust representation learning and clustering. The framework also includes an anomaly-aware data likelihood objective, which reduces the impact of anomalous data on the learning process. Additionally, the authors propose a gravity-inspired anomaly scoring method that leverages relationships between samples and clusters.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Modeling the connection between representation learning, clustering, and anomaly detection is highly relevant. This paper effectively demonstrates how these three tasks are interrelated and can be jointly optimized to improve anomaly detection performance.

2. The paper is well-written, presenting its hypothesis and method clearly. 

3. The results are impressive and demonstrate the effectiveness of UniCAD.

Weaknesses:
1. The ablation study on the hyperparameters $k$ and $l$ is insufficient. The authors only present results from a single dataset, satimage-2, where their method achieves an almost perfect score. It would be more informative to perform ablation studies across all 30 datasets or at least a subset where the model also shows lower performance. This broader analysis would demonstrate how these hyperparameters affect the average ranking of the method, similar to the results reported in the paper's table.

2. The authors introduce a $g(\Theta)$ term to prevent shortcut solutions, mentioning it in Equation 15. However, they do not discuss its importance or impact on performance after its introduction. Key questions remain unanswered, such as how the $g(\Theta)$ term affects the model's performance, what happens if it is removed, and how the autoencoder is implemented. These details are crucial, as the regularization term may significantly influence the results.

Limitations:
The authors mention some of the limitations, but they do not address the potential negative impact of the work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes UniCAD, a theoretically unified framework for representation learning, clustering, and anomaly detection. This paper first introduces the mixture of Student-t distribution $p(x|\Theta, \Phi)$ with degree of freedom $\nu=1$ based on a representation learner $f_\Theta$ using NN. Then, this paper combines with an anomaly indicator $\delta$ for maximum likelihood estimation. Parameters $(\Theta, \Phi)$ are optimized by EM algorithm and SGD. In addition, when detecting anomalies, an improved score is used with reference to gravity. The UniCAD achieved good performance on experiments with various datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well written and easy to follow.
- Good experimental results.

Weaknesses:
- We have several questions about the proposed method and experiments. Please see Qustions.
- The comparison with DeepSVDD and DIF is excellent, but I think the paper also needs to be compared with other Deep anomaly detection methods. For example, DROCC [1].

[1] Goyal, Sachin, et al. ""DROCC: Deep robust one-class classification. ""International conference on machine learning. PMLR, 2020.

Limitations:
- Hyper-parameter sensitivity seems to be one limitation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel probabilistic mixture model for unsupervised anomaly detection (UAD) that unifies representation learning, clustering, and anomaly detection into a single theoretical framework. The proposed UniCAD model addresses the lack of a unified approach in existing methods, which often consider these components separately or in pairs. The experimental results show that UniCAD consistently outperformed other methods in terms of AUC-ROC and AUC-PR. The model’s iterative optimization process using EM was also highlighted as effective and convergent.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- This paper introduces a novel integration of a probabilistic mixture model that unifies representation learning, clustering, and anomaly detection into a single theoretical framework.
- The proposed approach is well-motivated (Fig. 1) and supported by a robust theoretical foundation that maximizes anomaly-aware data likelihood, ensuring the model effectively leverages the interplay between representation learning, clustering, and anomaly detection.
- The paper is well-written, offering clear and comprehensive explanations of the proposed method, including detailed theoretical derivations and intuitive motivations for the design choices. The methodology section is particularly well-structured, logically outlining the steps and equations involved in the proposed model.
- The comprehensive evaluation design underscores the robustness of the proposed method.

Weaknesses:
- The connection between force analysis and anomaly detection, particularly between Equations 7 and 8 in Section 3.2.1, could benefit from further justification. While the analogy is interesting, it may not be immediately intuitive for all readers.
- The iterative optimization process may pose scalability issues for large datasets. An in-depth analysis and discussion of this would further strengthen the quality of this research.
- Although the model maps data to a low-dimensional representation space, the effectiveness of this mapping for very high-dimensional datasets could be explored further.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
VajjTXRj6J;"REVIEW 
Summary:
Based on the analysis of the shortcomings of DPO (Section 2.3), the authors proposed a simple reward distillation approach (Section 3.1) to align language models and a pessimistic variant (Section 3.2). These approaches outperform the vanilla DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The analysis in Section 2.3 extends the result in the IPO paper [23].
- The reward distillation approach is simple and natural, giving good empirical results.

Weaknesses:
I think that this paper is technically well done. However, it has a limited application scope.

The authors aim to address the shortcomings of offline alignment methods like DPO, while maintaining an offline approach. However, online alignment methods such as PPO or Reinforce-style algorithms naturally alleviate these offline learning issues. This raises a question: Given the authors' findings on the limitations of offline alignment, why not adopt an online alignment method like PPO directly?

Admittedly, the distilled DPO method and its pessimistic variants are less complex than PPO. However, these proposed offline variants introduce additional complexities to vanilla DPO by requiring separate reward models and separate training phases. This makes them more akin to online methods. Furthermore, the experiment section lacks a comparison against online method baselines, leaving it unclear if the distilled-DPO variants offer a better performance--compute tradeoff compared to vanilla online methods like PPO.

To be clear, the paper offers intriguing insights. However, these techniques seem niche. They benefit those who prefer robust generalization in preference optimization without using online data, an assumption not very well communicated in the paper.

In my opinion, there are ways to increase the method's applicability. While the proposed distillation approaches are positioned as offline methods, to my understanding, they are also applicable to an online regime. For example, in Equation (7), instead of sampling $(x, y_1, y_2)$ from an offline dataset, as the authors currently do, we can sample $x$ from an offline dataset and then sample $(y_1, y_2)$ from the model in training. The learned models, in this way, are directly comparable to those learned via online alignment methods like PPO, but in a simpler approach than these online baselines. If the authors could demonstrate that these models outperform PPO or require less complexity in training, they could expand the method's application scope and impact.

Limitations:
See ""Weaknesses"" section above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the limitations of DPO in LM alignment by proposing a reward model distillation approach. DPO, while efficient, often leads to overconfident and degenerate policies due to limited preference annotations. The authors introduce a method that trains LMs to match the distribution from a reward model, improving robustness and performance, particularly in biased preference datasets. Their approach also includes a pessimistic extension to handle uncertainty in reward models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper makes a considerable theoretical contribution by addressing the critical issue of degeneration in DPO, a problem of significant concern in the community. The authors present a well-structured and clearly written analysis that helps in understanding the overfitting commonly observed with the DPO. The proposed method of reward model distillation is both theoretically sound and intuitive, offering a robust solution that potentially improves upon traditional DPO methods. I appreciate this approach and its theoretical support.

Weaknesses:
The main concern about this paper is the evaluation. As discussed in Section 2.3, DPO can shift nearly all probability mass to responses that never appear in the training set, also called OOD extrapolation in other papers. This issue arises when there are few annotations per instance (x, y1, y2). Therefore, it is expected that the authors should demonstrate how their proposed approach mitigates this problem, maybe by presenting the log-probabilities of the winning and losing responses. Specifically, the log-probability margin between winning and losing responses should not keep increasing (I assume this is true?).

Additionally, I can see the paper presents results showing robustness against distribution shifts in preference data. But what is the factor that makes the proposed algorithm learn the policy whose underlying preference distribution closer to the true preference distribution in biased setting? I would be willing to increase my score if these concerns are solved.

Limitations:
I have no concerns about the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors discuss and give formal results on the limitations of DPO that have been observed in practice, and investigate reward model objectives for 1) distilling reward differences into the generator (eq. 7), and 2) pessimistic ""minimax"" distillation over a family of reward models, to mitigate these limitations. The theoretical equivalance of the ""forward"" and ""reverse"" pessimistic formulations (eq. 8 and 9) of the standard RLHF objective (eq. 1) is shown, and an approximate pessimistic objective (eq. 10) with a minimum over distillation losses for the reward model family considered in a Langrangian term. Results on TL;DR preference data that is biased to varying degrees to prefer long responses (Figure 2) shows good results for distilled models generally, and that pessimistic optimization over a family of reward models  optimized for varying response lengths improves results in irregular settings (i.e. when shorter responses are preferred).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A well written, insightful paper, with well formulated, novel objectives for preference optimization.

Weaknesses:
- The results, as the authors acknowledge, are currently quite limited. These methods should really be tested on at least one additional preference task, and for results utilizing multiple models, pessimism should be compared with other basic ensembling strategies. 
- The gamma parameter is annealed during training, suggesting that the setting is important and perhaps sensitive. Some ablations and discussion around this seems prudent.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Based on the analysis of the shortcomings of DPO (Section 2.3), the authors proposed a simple reward distillation approach (Section 3.1) to align language models and a pessimistic variant (Section 3.2). These approaches outperform the vanilla DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The analysis in Section 2.3 extends the result in the IPO paper [23].
- The reward distillation approach is simple and natural, giving good empirical results.

Weaknesses:
I think that this paper is technically well done. However, it has a limited application scope.

The authors aim to address the shortcomings of offline alignment methods like DPO, while maintaining an offline approach. However, online alignment methods such as PPO or Reinforce-style algorithms naturally alleviate these offline learning issues. This raises a question: Given the authors' findings on the limitations of offline alignment, why not adopt an online alignment method like PPO directly?

Admittedly, the distilled DPO method and its pessimistic variants are less complex than PPO. However, these proposed offline variants introduce additional complexities to vanilla DPO by requiring separate reward models and separate training phases. This makes them more akin to online methods. Furthermore, the experiment section lacks a comparison against online method baselines, leaving it unclear if the distilled-DPO variants offer a better performance--compute tradeoff compared to vanilla online methods like PPO.

To be clear, the paper offers intriguing insights. However, these techniques seem niche. They benefit those who prefer robust generalization in preference optimization without using online data, an assumption not very well communicated in the paper.

In my opinion, there are ways to increase the method's applicability. While the proposed distillation approaches are positioned as offline methods, to my understanding, they are also applicable to an online regime. For example, in Equation (7), instead of sampling $(x, y_1, y_2)$ from an offline dataset, as the authors currently do, we can sample $x$ from an offline dataset and then sample $(y_1, y_2)$ from the model in training. The learned models, in this way, are directly comparable to those learned via online alignment methods like PPO, but in a simpler approach than these online baselines. If the authors could demonstrate that these models outperform PPO or require less complexity in training, they could expand the method's application scope and impact.

Limitations:
See ""Weaknesses"" section above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the limitations of DPO in LM alignment by proposing a reward model distillation approach. DPO, while efficient, often leads to overconfident and degenerate policies due to limited preference annotations. The authors introduce a method that trains LMs to match the distribution from a reward model, improving robustness and performance, particularly in biased preference datasets. Their approach also includes a pessimistic extension to handle uncertainty in reward models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper makes a considerable theoretical contribution by addressing the critical issue of degeneration in DPO, a problem of significant concern in the community. The authors present a well-structured and clearly written analysis that helps in understanding the overfitting commonly observed with the DPO. The proposed method of reward model distillation is both theoretically sound and intuitive, offering a robust solution that potentially improves upon traditional DPO methods. I appreciate this approach and its theoretical support.

Weaknesses:
The main concern about this paper is the evaluation. As discussed in Section 2.3, DPO can shift nearly all probability mass to responses that never appear in the training set, also called OOD extrapolation in other papers. This issue arises when there are few annotations per instance (x, y1, y2). Therefore, it is expected that the authors should demonstrate how their proposed approach mitigates this problem, maybe by presenting the log-probabilities of the winning and losing responses. Specifically, the log-probability margin between winning and losing responses should not keep increasing (I assume this is true?).

Additionally, I can see the paper presents results showing robustness against distribution shifts in preference data. But what is the factor that makes the proposed algorithm learn the policy whose underlying preference distribution closer to the true preference distribution in biased setting? I would be willing to increase my score if these concerns are solved.

Limitations:
I have no concerns about the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors discuss and give formal results on the limitations of DPO that have been observed in practice, and investigate reward model objectives for 1) distilling reward differences into the generator (eq. 7), and 2) pessimistic ""minimax"" distillation over a family of reward models, to mitigate these limitations. The theoretical equivalance of the ""forward"" and ""reverse"" pessimistic formulations (eq. 8 and 9) of the standard RLHF objective (eq. 1) is shown, and an approximate pessimistic objective (eq. 10) with a minimum over distillation losses for the reward model family considered in a Langrangian term. Results on TL;DR preference data that is biased to varying degrees to prefer long responses (Figure 2) shows good results for distilled models generally, and that pessimistic optimization over a family of reward models  optimized for varying response lengths improves results in irregular settings (i.e. when shorter responses are preferred).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A well written, insightful paper, with well formulated, novel objectives for preference optimization.

Weaknesses:
- The results, as the authors acknowledge, are currently quite limited. These methods should really be tested on at least one additional preference task, and for results utilizing multiple models, pessimism should be compared with other basic ensembling strategies. 
- The gamma parameter is annealed during training, suggesting that the setting is important and perhaps sensitive. Some ablations and discussion around this seems prudent.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
VTcGX5HO19;"REVIEW 
Summary:
This paper proposes using Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate model for Bayesian optimization (BO). BKTF approximates the objective function using a low-rank tensor factorization, with Gaussian process priors placed on the latent factors to capture dependencies and enable uncertainty quantification. The key advantages are the ability to handle complex functions that are non-stationary and non-separable, and the sharing of information across dimensions to enable a more global search compared to standard GP models with local kernels. Inference is performed via MCMC sampling. Experiments on benchmark optimization functions and hyperparameter tuning tasks demonstrate improved performance over GP-based BO, especially when the initial sample size and evaluation budget are limited.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The BKTF surrogate is a novel and creative approach to extend BO to handle more complex objective functions. Modeling the objective as a tensor factorization with GP priors on the factors is an elegant way to introduce non-stationarity and multi-scale correlations in a principled Bayesian framework.

The method is grounded in a clear mathematical framework, with full details of the model specification and MCMC-based inference procedure provided. Positioning BKTF as a type of deep GP offers useful insight into its expressive power.

The paper includes extensive experiments on a range of synthetic functions and real-world hyperparameter tuning tasks. The results convincingly demonstrate the advantages of BKTF over standard GP-BO in terms of optimization performance and sample efficiency, especially in the realistic setting of a very limited evaluation budget.

The paper is clearly written, with the methodology explained in detail and the experimental setup and results presented thoroughly. The authors also discuss the limitations of their work, including the scalability challenges and the restriction to a grid-based search space.

Weaknesses:
The main weakness is that the proposed BKTF method is only compared against basic GP-based BO with standard kernels. To fully demonstrate the advantage of the BKTF surrogate, comparisons should be made to more advanced GP models such as deep kernel learning, deep GPs, and other scalable GP variants. Without these comparisons, it's difficult to assess how much of the performance gain is due to the specific BKTF approach vs. simply being a more flexible GP.

The BKTF model bears significant similarities to existing works on scalable GPs, such as ""Gaussian Processes for Big Data"" and ""Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)"", which also use inducing points on grids to obtain tractable approximations. The relationship of BKTF to these methods is not discussed, and it's unclear whether BKTF provides any substantial advantages over these existing scalable GP approaches.

The experiments on the synthetic test functions are somewhat limited in their dimensionality (only up to 10d). Given that BO is most useful for optimizing high-dimensional black-box functions, testing on some higher-dimensional benchmarks would be valuable. The scalability of BKTF as the dimensionality and grid size increase is not fully investigated.

The MCMC inference procedure may become prohibitively slow for high-dimensional spaces or large evaluation budgets. The paper does not report the wall-clock time of the experiments, which makes it hard to assess the computational feasibility of BKTF in practice, especially compared to alternative approaches.

For the hyperparameter tuning experiments in Section 5.2, the strong performance of BKTF with very few iterations seems counterintuitive and is not fully explained, since the BKTF model has many parameters and would be expected to require a substantial amount of data to train effectively. This seems to contradict the results on the synthetic test functions, where GP-EI performs equally well in the first few iterations.

Limitations:
The relationship of BKTF to existing scalable GP methods is not thoroughly discussed. The paper does not make clear how BKTF differs from or improves upon approaches like ""Gaussian Processes for Big Data"" and ""Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)"", which also exploit grid structure. A more thorough comparison is needed to justify BKTF as a novel contribution.

The cubic scaling of the covariance matrix computations in the number of grid points, which could make BKTF infeasible for high-dimensional or very fine-grained grids. Some discussion of potential ways to scale up BKTF, e.g., by exploiting grid structure or using sparse approximations, would be valuable.

The fact that BKTF relies heavily on a sensible grid specification, and may fail badly if the grid is poorly chosen. Some experiments showing the sensitivity of the results to the grid choice would help assess this risk.

The lack of comparison to a wider range of flexible surrogate models beyond standard GPs, including more sophisticated GP models as well as other probabilistic regression approaches. The current experiments are not sufficient to establish BKTF as the best choice for BO in practice.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Bayesian Kernelized Tensor Factorization (BKTF) as a surrogate for Bayesian optimization (BO). This model uses a CP decomposition to define a set of random basis functions drawn from a GP prior. These latent functions are then weighted by another set of random variables. This defines a probabilistic model that can perform uncertainty quantification, which can then be trained by performing MCMC sampling over the hyperparameters. The acquisition function is calculated by computing the first and second moments of the samples, and calculating the upper confidence bound (UCB).

This procedure results in a non-stationary, non-separable model that can capture complex functions. This model is tested on a range of synthetic and ML hyperparameter BO benchmarks, each of which are non-separable functions that exhibit high degrees of interaction between variables.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality:**
This paper is the first to use kernelized tensors in the Bayesian optimization setting. Gaussian processes are a common surrogate in this setting, and this proposes an alternate surrogate with good arguments for its adoption.

**Quality:**
The paper demonstrates the performance of the BKTF surrogate on a wide range of benchmarks. The performance is strong, justifying the claims in the paper. Further, many supplementary results are provided to further investigate the modelling decisions made.

**Clarity:**
The explanation of the BKTF model and fitting process is explained well, providing a clear description of the model. Specifically, the 2D example in Figure 1 provides a clear motivation for modelling functions that have a high degree of interaction.

**Significance:**
The proposed method is a strong surrogate for Bayesian optimization, one that can model functions with mixed input spaces and high degrees of interaction between variables. This presents a good addition to the range of available surrogates in the field.

Weaknesses:
**Comparison to other methods:**
The authors claim that a strength of their method is that their method is non-stationary and non-separable. However, I do not feel that the paper sufficiently justifies this explanation for the model's success, for the following reasons:

I do not believe that the GP ARD is separable. The authors present that the ARD kernel is the product of $D$ independent kernels in 3.2. However, this is not how these kernels are implemented. Instead, (specifically for stationary kernels) they are expressed as functions of weighted distance [5], where $d=\sqrt{\sum_{d=1}^D (x_d - x'_d)^2/l_d}$. These kernels cannot be written as products of 1-dimensional kernels, and are non-separable. The authors also suggest that these experience the curse of dimensionality as the dimension increases, but this effect is not severe in the low-dimension regime studied in the paper.

The authors claim that the flaw of using additive GP kernels is that they:
> [require] strong prior knowledge to determine the proper interactions and [involve] a large number of kernel hyperparameters to learn

I do not find this argument convincing. For low dimensional problems, additive kernels can include all interactions up to order D, and learn the weighting of each order of interaction [1]. In fact, I believe that the number of kernel hyperparameters is of a similar order to the BKTF method. Additive kernels also work well with MAP estimation of the hyperparameters (especially for the low-dimensional problems investigated here), although I do not see why MCMC could not also be used for additive kernels if the number of kernel hyperparameters is considered too large. The additive GP baseline should therefore be order D, not order 1, to provide a fair comparison against existing non-separable modelling methods.

It is unclear why the authors compare to SaaSBO, a technique designed for high dimensional (D>100) spaces that places a strong prior on the lengthscales of the inputs (with the prior belief that few of the inputs are important, which is not the case for the test functions used).

Moreover, the authors do not provide comparisons against methods that are designed for non-stationary settings e.g. [1, 2]. 

(Minor comment) I would like to see MCMC over the GP kernel hyperparameters to obtain a fully Bayesian acquisition function, as in [6].

**Hyperparameter choices with BKTF:**
I disagree with the authors that BKTF is robust to rank misspecification. Figure 13 shows that the CRPS doesn't converge until 30 observations for the rank 2 model. Including the initial 30 datapoints, this model is fit on 60 datapoints, for a 2D problem - the GP model provides a much better fit to the data for <60 datapoints.  Since these models are used in a BO setting where few datapoints is common, this behaviour suggests that rank *is* an important parameter, and further that the model does not fit well with few datapoints. 

I would also want to see this experiment repeated on higher dimension test problems, to see if the problem is exacerbated in high dimensions. Moreover, the CRPS of the rank 2 model seems to converge only to that of the GP models, suggesting the performance over GP may not be solely due to modeling quality: this should be further investigated.

**Experiments:**
It is unclear how the authors handle categorical inputs for the baselines. The authors should be using methods designed for mixed input spaces, such as [4].

Following from the discussion on CRPS, this paper would benefit from some experiments on the quality of the fit of the model.

(Minor comment) It would be interesting to see the (arguably more popular) Matern 5/2 kernel compared to the 3/2 kernel, especially for the GP baselines.


[1] Snoek et al. ""Input Warping for Bayesian Optimization of Non-stationary Functions""    
[2] Eriksson et al. ""Scalable Global Optimization via Local Bayesian Optimization""    
[3] Duvenaud et al. ""Additive Gaussian Processes""    
[4] Ru et al. ""Bayesian optimisation over multiple continuous and categorical inputs""    
[5] Williams and Rasmussen. ""Gaussian Processes for Machine Learning""    
[6] Snoek et al. ""Practical Bayesian Optimization of Machine Learning Algorithms""

Limitations:
The authors provide good discussion on the limitations of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new surrogate model for Bayesian optimization, based on a functional tensor factorization. The approach discretizes the model to a pre-specified grid and uses MCMC sampling for inference. Bayesian optimization is carried out by selecting promising points from the pre-specified grid, as quantified by an acquisition function. The paper includes experimental results on synthetic functions as well as ML hyper-parameter tuning tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Nice experimental results on the ML hyper-parameter tuning task.
- Generally well written paper.

Weaknesses:
- Optimization method appears to be restricted to an a-priori defined grid of possible candidate points.

- I am concerned about reproducibility of the benchmarks, as the code submission not complete, e.g. appears to be missing implementations of basic functions like baseline GP fitting (`fitrgp`) and predicting (`predict`), the additive GP model mentioned in line 274 of the paper, the continuous optimization of the acquisition function. 

- Grid-based GP-UCB, GP-EI baselines are missing for ML tuning tasks (Figure 3). This is notable, because these experiments contain discrete variables, which requires a rounding operation if they are relaxed to a continuous space, as is done by the paper. This rounding operation is likely to degrade the performance of ""continuous"" GP-UCB and GP-EI, as it will can be prone to sampling ""between"" integers it has already seen, reducing its sample efficiency.

Limitations:
> A limitation of BKTF is that we restrict BO to a grid search space in order to leverage tensor factorization; however, we believe that designing a compatible grid space based on prior knowledge is not a challenging task.

An important limitation to highlight here once more that it goes from ""not challenging"" to prohibitively expensive as the dimension increases.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for Bayesian optimization where the prior is a low-rank sum of tensor products of GPs. An MCMC scheme is developed for approximate updating and UCB sampling. Several experiments show strong performance relative to baselines on artificial function optimization and ML hyperparam tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This is a seemingly new approach to BO with a more flexible type of prior, which shows good empirical performance.

Weaknesses:
It's not clear what is new relative to previous BKTF papers [10,11], other than the scheme for using UQ in UCB sampling.

The clearest potential drawbacks to the approach are the memory and compute costs. These should be reported for the experiments.

Limitations:
The paper motivates the approach in part because standard methods assume the generating process is stationary, but the BKTF is also stationary. It’s nonstationary only after conditioning on values of $g$.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes using Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate model for Bayesian optimization (BO). BKTF approximates the objective function using a low-rank tensor factorization, with Gaussian process priors placed on the latent factors to capture dependencies and enable uncertainty quantification. The key advantages are the ability to handle complex functions that are non-stationary and non-separable, and the sharing of information across dimensions to enable a more global search compared to standard GP models with local kernels. Inference is performed via MCMC sampling. Experiments on benchmark optimization functions and hyperparameter tuning tasks demonstrate improved performance over GP-based BO, especially when the initial sample size and evaluation budget are limited.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The BKTF surrogate is a novel and creative approach to extend BO to handle more complex objective functions. Modeling the objective as a tensor factorization with GP priors on the factors is an elegant way to introduce non-stationarity and multi-scale correlations in a principled Bayesian framework.

The method is grounded in a clear mathematical framework, with full details of the model specification and MCMC-based inference procedure provided. Positioning BKTF as a type of deep GP offers useful insight into its expressive power.

The paper includes extensive experiments on a range of synthetic functions and real-world hyperparameter tuning tasks. The results convincingly demonstrate the advantages of BKTF over standard GP-BO in terms of optimization performance and sample efficiency, especially in the realistic setting of a very limited evaluation budget.

The paper is clearly written, with the methodology explained in detail and the experimental setup and results presented thoroughly. The authors also discuss the limitations of their work, including the scalability challenges and the restriction to a grid-based search space.

Weaknesses:
The main weakness is that the proposed BKTF method is only compared against basic GP-based BO with standard kernels. To fully demonstrate the advantage of the BKTF surrogate, comparisons should be made to more advanced GP models such as deep kernel learning, deep GPs, and other scalable GP variants. Without these comparisons, it's difficult to assess how much of the performance gain is due to the specific BKTF approach vs. simply being a more flexible GP.

The BKTF model bears significant similarities to existing works on scalable GPs, such as ""Gaussian Processes for Big Data"" and ""Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)"", which also use inducing points on grids to obtain tractable approximations. The relationship of BKTF to these methods is not discussed, and it's unclear whether BKTF provides any substantial advantages over these existing scalable GP approaches.

The experiments on the synthetic test functions are somewhat limited in their dimensionality (only up to 10d). Given that BO is most useful for optimizing high-dimensional black-box functions, testing on some higher-dimensional benchmarks would be valuable. The scalability of BKTF as the dimensionality and grid size increase is not fully investigated.

The MCMC inference procedure may become prohibitively slow for high-dimensional spaces or large evaluation budgets. The paper does not report the wall-clock time of the experiments, which makes it hard to assess the computational feasibility of BKTF in practice, especially compared to alternative approaches.

For the hyperparameter tuning experiments in Section 5.2, the strong performance of BKTF with very few iterations seems counterintuitive and is not fully explained, since the BKTF model has many parameters and would be expected to require a substantial amount of data to train effectively. This seems to contradict the results on the synthetic test functions, where GP-EI performs equally well in the first few iterations.

Limitations:
The relationship of BKTF to existing scalable GP methods is not thoroughly discussed. The paper does not make clear how BKTF differs from or improves upon approaches like ""Gaussian Processes for Big Data"" and ""Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)"", which also exploit grid structure. A more thorough comparison is needed to justify BKTF as a novel contribution.

The cubic scaling of the covariance matrix computations in the number of grid points, which could make BKTF infeasible for high-dimensional or very fine-grained grids. Some discussion of potential ways to scale up BKTF, e.g., by exploiting grid structure or using sparse approximations, would be valuable.

The fact that BKTF relies heavily on a sensible grid specification, and may fail badly if the grid is poorly chosen. Some experiments showing the sensitivity of the results to the grid choice would help assess this risk.

The lack of comparison to a wider range of flexible surrogate models beyond standard GPs, including more sophisticated GP models as well as other probabilistic regression approaches. The current experiments are not sufficient to establish BKTF as the best choice for BO in practice.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Bayesian Kernelized Tensor Factorization (BKTF) as a surrogate for Bayesian optimization (BO). This model uses a CP decomposition to define a set of random basis functions drawn from a GP prior. These latent functions are then weighted by another set of random variables. This defines a probabilistic model that can perform uncertainty quantification, which can then be trained by performing MCMC sampling over the hyperparameters. The acquisition function is calculated by computing the first and second moments of the samples, and calculating the upper confidence bound (UCB).

This procedure results in a non-stationary, non-separable model that can capture complex functions. This model is tested on a range of synthetic and ML hyperparameter BO benchmarks, each of which are non-separable functions that exhibit high degrees of interaction between variables.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality:**
This paper is the first to use kernelized tensors in the Bayesian optimization setting. Gaussian processes are a common surrogate in this setting, and this proposes an alternate surrogate with good arguments for its adoption.

**Quality:**
The paper demonstrates the performance of the BKTF surrogate on a wide range of benchmarks. The performance is strong, justifying the claims in the paper. Further, many supplementary results are provided to further investigate the modelling decisions made.

**Clarity:**
The explanation of the BKTF model and fitting process is explained well, providing a clear description of the model. Specifically, the 2D example in Figure 1 provides a clear motivation for modelling functions that have a high degree of interaction.

**Significance:**
The proposed method is a strong surrogate for Bayesian optimization, one that can model functions with mixed input spaces and high degrees of interaction between variables. This presents a good addition to the range of available surrogates in the field.

Weaknesses:
**Comparison to other methods:**
The authors claim that a strength of their method is that their method is non-stationary and non-separable. However, I do not feel that the paper sufficiently justifies this explanation for the model's success, for the following reasons:

I do not believe that the GP ARD is separable. The authors present that the ARD kernel is the product of $D$ independent kernels in 3.2. However, this is not how these kernels are implemented. Instead, (specifically for stationary kernels) they are expressed as functions of weighted distance [5], where $d=\sqrt{\sum_{d=1}^D (x_d - x'_d)^2/l_d}$. These kernels cannot be written as products of 1-dimensional kernels, and are non-separable. The authors also suggest that these experience the curse of dimensionality as the dimension increases, but this effect is not severe in the low-dimension regime studied in the paper.

The authors claim that the flaw of using additive GP kernels is that they:
> [require] strong prior knowledge to determine the proper interactions and [involve] a large number of kernel hyperparameters to learn

I do not find this argument convincing. For low dimensional problems, additive kernels can include all interactions up to order D, and learn the weighting of each order of interaction [1]. In fact, I believe that the number of kernel hyperparameters is of a similar order to the BKTF method. Additive kernels also work well with MAP estimation of the hyperparameters (especially for the low-dimensional problems investigated here), although I do not see why MCMC could not also be used for additive kernels if the number of kernel hyperparameters is considered too large. The additive GP baseline should therefore be order D, not order 1, to provide a fair comparison against existing non-separable modelling methods.

It is unclear why the authors compare to SaaSBO, a technique designed for high dimensional (D>100) spaces that places a strong prior on the lengthscales of the inputs (with the prior belief that few of the inputs are important, which is not the case for the test functions used).

Moreover, the authors do not provide comparisons against methods that are designed for non-stationary settings e.g. [1, 2]. 

(Minor comment) I would like to see MCMC over the GP kernel hyperparameters to obtain a fully Bayesian acquisition function, as in [6].

**Hyperparameter choices with BKTF:**
I disagree with the authors that BKTF is robust to rank misspecification. Figure 13 shows that the CRPS doesn't converge until 30 observations for the rank 2 model. Including the initial 30 datapoints, this model is fit on 60 datapoints, for a 2D problem - the GP model provides a much better fit to the data for <60 datapoints.  Since these models are used in a BO setting where few datapoints is common, this behaviour suggests that rank *is* an important parameter, and further that the model does not fit well with few datapoints. 

I would also want to see this experiment repeated on higher dimension test problems, to see if the problem is exacerbated in high dimensions. Moreover, the CRPS of the rank 2 model seems to converge only to that of the GP models, suggesting the performance over GP may not be solely due to modeling quality: this should be further investigated.

**Experiments:**
It is unclear how the authors handle categorical inputs for the baselines. The authors should be using methods designed for mixed input spaces, such as [4].

Following from the discussion on CRPS, this paper would benefit from some experiments on the quality of the fit of the model.

(Minor comment) It would be interesting to see the (arguably more popular) Matern 5/2 kernel compared to the 3/2 kernel, especially for the GP baselines.


[1] Snoek et al. ""Input Warping for Bayesian Optimization of Non-stationary Functions""    
[2] Eriksson et al. ""Scalable Global Optimization via Local Bayesian Optimization""    
[3] Duvenaud et al. ""Additive Gaussian Processes""    
[4] Ru et al. ""Bayesian optimisation over multiple continuous and categorical inputs""    
[5] Williams and Rasmussen. ""Gaussian Processes for Machine Learning""    
[6] Snoek et al. ""Practical Bayesian Optimization of Machine Learning Algorithms""

Limitations:
The authors provide good discussion on the limitations of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new surrogate model for Bayesian optimization, based on a functional tensor factorization. The approach discretizes the model to a pre-specified grid and uses MCMC sampling for inference. Bayesian optimization is carried out by selecting promising points from the pre-specified grid, as quantified by an acquisition function. The paper includes experimental results on synthetic functions as well as ML hyper-parameter tuning tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Nice experimental results on the ML hyper-parameter tuning task.
- Generally well written paper.

Weaknesses:
- Optimization method appears to be restricted to an a-priori defined grid of possible candidate points.

- I am concerned about reproducibility of the benchmarks, as the code submission not complete, e.g. appears to be missing implementations of basic functions like baseline GP fitting (`fitrgp`) and predicting (`predict`), the additive GP model mentioned in line 274 of the paper, the continuous optimization of the acquisition function. 

- Grid-based GP-UCB, GP-EI baselines are missing for ML tuning tasks (Figure 3). This is notable, because these experiments contain discrete variables, which requires a rounding operation if they are relaxed to a continuous space, as is done by the paper. This rounding operation is likely to degrade the performance of ""continuous"" GP-UCB and GP-EI, as it will can be prone to sampling ""between"" integers it has already seen, reducing its sample efficiency.

Limitations:
> A limitation of BKTF is that we restrict BO to a grid search space in order to leverage tensor factorization; however, we believe that designing a compatible grid space based on prior knowledge is not a challenging task.

An important limitation to highlight here once more that it goes from ""not challenging"" to prohibitively expensive as the dimension increases.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for Bayesian optimization where the prior is a low-rank sum of tensor products of GPs. An MCMC scheme is developed for approximate updating and UCB sampling. Several experiments show strong performance relative to baselines on artificial function optimization and ML hyperparam tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This is a seemingly new approach to BO with a more flexible type of prior, which shows good empirical performance.

Weaknesses:
It's not clear what is new relative to previous BKTF papers [10,11], other than the scheme for using UQ in UCB sampling.

The clearest potential drawbacks to the approach are the memory and compute costs. These should be reported for the experiments.

Limitations:
The paper motivates the approach in part because standard methods assume the generating process is stationary, but the BKTF is also stationary. It’s nonstationary only after conditioning on values of $g$.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
V5Sbh42uDe;"REVIEW 
Summary:
This paper proposes a method to improve weakly-supervised semantic segmentation using sparse annotations. The authors introduce a Potts relaxation method, which is an extension of the traditional CRF-like methods. The experiments are conducted on the PASCAL dataset.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Pros.

1.	The proposed method appears to be technically sound.

2.	The authors provide detailed explanations of their method.

Weaknesses:
Weakness:

1.	The comparisons with other methods are not comprehensive. Recent advances [1,2,3] in sparsely annotated semantic segmentation are not discussed or compared. Point-supervised is more challenging but it is ignored. The survey in the paper is limited, making it difficult to understand the contributions.

2.	The use of a small dataset like PASCAL may not demonstrate the superiority of the proposed method. Results from larger datasets like Cityscapes and ADE20K (Table 6) should be emphasized and compared with state-of-the-art methods.

3.	Self-labeling is also well employed in many weakly-supervised methods that use image-tags labels. It seems that your method can also applied to them, am I right? If so, experiments on the COCO dataset would be important.

4.	DeepLab is an old-fashioned network architecture. The authors should prove that Potts relaxation can also work on Vision Transformer since ViT has demonstrated the SOTA performances in both fully- and weakly-supervised semantic segmentation. If using ViT without Potts can already get good performances, the relaxation may be not important.

5.	In recent fully- and weakly-supervised semantic segmentation works, CRF-like methods have been discarded due to their computation cost. The efficiency of Potts relaxation (FLOPs) is not evaluated in your paper, which is very important. 

6.	With complex formulation, on the very toy dataset PASCAL, the mIoU only increases by 1% in Table 5 (77.1 to 78.1), while the increased computation cost has not yet been evaluated. 

7.	As stated in the abstract “ … can outperform full pixel-precise supervision on PASCAL”, which is not convincing to me. The fully-supervised performance should be considered as the upper bound of weakly-supervised learning. Such results may be caused by unfair settings. 

8.	The importance of relaxation should be introduced in the abstract since it is your main contribution. In Section 2.1, the two claimed reasons are not intuitive to me. “Manage the scope of this study” seems not a strong motivation. 

9.	With the development of ViT and vision pre-training, the improvements of CRF-based self-labeling become marginal. Thus, I think it is important to evaluate ViT and vision-pretraining (DINO[4]) backbone with your relaxation. I am wondering whether Potts can improve performances beyond these strong backbones.

Overall,  I think it would be better if the authors could conduct more comprehensive experiment comparisons and related work discussions. The motivation for relaxation should be introduced in the beginning.

Refers:

1.	Sparsely Annotated Semantic Segmentation With Adaptive Gaussian Mixtures. CVPR 23

2.	Label-efficient Segmentation via Affinity Propagation. NIPS 23

3.	Modeling the Label Distributions for Weakly-Supervised Semantic Segmentation. Arxiv 24

4.	DINOv2: Learning Robust Visual Features without Supervision. TMLR 24

5. CC4S: Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation. TPAMI 24

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper considers semantic segmentation under scribble supervision. The paper studies relaxations of the Potts model and proposes a framework for generating soft pseudo-labels, which benefit over hard pseudo-labels in that they can represent uncertainty. The paper highlights problem cases with two standard relaxations, the quadratic and bilinear, and proposes a normalized quadratic relaxation. Moreover, the paper proposes to use a collision cross-entropy loss between the prediction and the pseudo-labels. Different settings are evaluated experimentally, and the proposed approach is compared to the state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Overall, the paper is well written, and the proposed approach is intuitive and should be fairly easy to reproduce, even without code.

The problem under consideration is important as it aims to reduce the manual annotation challenge in image segmentation, which is otherwise costly and time consuming.

The choice of Potts relaxations and cross-entropy terms are supported by experiments, and the proposed approach is further compared to previous methods, showing strong performance.

Weaknesses:
Some details are missing or unclear in the main paper. Considering that the soft self-labeling loss in (6) is a key contribution, it would be useful to include some details regarding the optimization of the pseudo-labels in in the main paper from Appendix B. Additionally, pairwise affinities based on intensity edges are mention at line 42, but it is unclear whether they are used in the proposed approach, see questions.

The proposed approach is only evaluated on a single dataset in the main paper. The results on additional datasets in the appendix should be moved to the main paper to better communicate the empirical findings, as they are easy to miss otherwise. This also raises some confusion about Appendix C which says that three datasets are used in Section 3.5, but results are only reported on PASCAL. What was the reason to exclude these from the main paper?

No error bars. Considering that some settings have fairly similar performance, e.g. in Table 3, standard deviation or similar over multiple runs would be useful.

Some figures have excessively small fonts, e.g. Figures 3, 4, 6.

Limitations:
The paper does not discuss limitations.

Societal impacts are not discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes a soft self-labeling framework for weakly supervised semantic segmentation using scribbles. This model-agnostic framework requires only the joint optimization of network predictions and pseudo labels, guided by specific loss functions: collision cross-entropy and log-quadratic Potts relaxations. The design choices are supported by theoretical concepts and experimental results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The work systematically analyzes common loss functions for weakly supervised semantic segmentation. By investigating theoretical concepts and experimental results, it discusses the advantages and disadvantages of these loss functions. Based on this comprehensive analysis, the work concludes by recommending the use of collision cross-entropy and log-quadratic Potts relaxations for a soft self-labeling framework.

Weaknesses:
Method
- The proposed method is tested on DeepLab. Although the theoretical concept should hold for any model, its effectiveness on other segmentation models remains unknown. 
- The work explains the rationale behind design choices from a theoretical perspective, but it does not clarify why these choices lead to specific pseudo-labels from a vision perspective. For instance, in Figure 5, NN successfully segments the bicycle while DN does not. Is this because the bicycle is a minor class? 
- The work does not provide mIoU per category, leaving it unclear whether the method is effective for all categories or just a few major ones.

Minor Writing Issue:
- The legends of the figures, such as Figures 1, 4, and 6, are small.
- Figure 1 (b) contains an extra icon (house).

Related works:
- There are more image-level WSSS works than just [4, 21]. The author should consider including additional relevant works in lines 16-18.

- Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks
- Weakly supervised learning of instance segmentation with interpixel relations.
- Extracting class activation maps from on-discriminative features as well.
- Boundary-enhanced co-training for weakly supervised semantic segmentation.
- Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation
- Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation
- Group-wise semantic mining for weakly supervised semantic segmentation

Limitations:
I agree with the author that the training time is one of the limitations.
I hope the author can consider my suggestion in the above section to build a link between the design choice and the pseudo labels from the computer vision perspective.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new framework for Weakly-Supervised image Segmentation. The main contribution is to use a soft-labelling approach which is considered superior to classic hard labelling because it can keep track of the centanty of the label. Then different forms of second order potts relaxation and cross-entropy are evaluated.
Results show that the proposed approach with its best setting is able to perform better than previous approaches and comparably to a fully supervised approach with only 3% of annotated pixels.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
\+ The paper is in general well written and easy to follow

\+ Nice illustrations help to understand the proposed approaches

\+ Ablations on the most important components help to understand the significance of each component

Weaknesses:
\- It is not clear if results are significant. For instance is 1% a meaningful improvement or it can be generated by just noise? It is important to see results on multiple runs with also standard deviation.

\- The motivation about soft-labelling because it keeps information about the label certainty is not clear to me

\- In tab. 5 it is not clear what is the base model you start from. Could you report also the base model with standard cross-entropy and hard potts model? It seems that results are very good because the baseline is already quite good.

\- Authors did not say much about the computational cost of the proposed approach compared to common models.

Limitations:
Authors did not mention the possible limitations of the proposed approach.
One possible limitation is the need of additional hyper-parameters to tune which can take time. Authors should comment on that.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a method to improve weakly-supervised semantic segmentation using sparse annotations. The authors introduce a Potts relaxation method, which is an extension of the traditional CRF-like methods. The experiments are conducted on the PASCAL dataset.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Pros.

1.	The proposed method appears to be technically sound.

2.	The authors provide detailed explanations of their method.

Weaknesses:
Weakness:

1.	The comparisons with other methods are not comprehensive. Recent advances [1,2,3] in sparsely annotated semantic segmentation are not discussed or compared. Point-supervised is more challenging but it is ignored. The survey in the paper is limited, making it difficult to understand the contributions.

2.	The use of a small dataset like PASCAL may not demonstrate the superiority of the proposed method. Results from larger datasets like Cityscapes and ADE20K (Table 6) should be emphasized and compared with state-of-the-art methods.

3.	Self-labeling is also well employed in many weakly-supervised methods that use image-tags labels. It seems that your method can also applied to them, am I right? If so, experiments on the COCO dataset would be important.

4.	DeepLab is an old-fashioned network architecture. The authors should prove that Potts relaxation can also work on Vision Transformer since ViT has demonstrated the SOTA performances in both fully- and weakly-supervised semantic segmentation. If using ViT without Potts can already get good performances, the relaxation may be not important.

5.	In recent fully- and weakly-supervised semantic segmentation works, CRF-like methods have been discarded due to their computation cost. The efficiency of Potts relaxation (FLOPs) is not evaluated in your paper, which is very important. 

6.	With complex formulation, on the very toy dataset PASCAL, the mIoU only increases by 1% in Table 5 (77.1 to 78.1), while the increased computation cost has not yet been evaluated. 

7.	As stated in the abstract “ … can outperform full pixel-precise supervision on PASCAL”, which is not convincing to me. The fully-supervised performance should be considered as the upper bound of weakly-supervised learning. Such results may be caused by unfair settings. 

8.	The importance of relaxation should be introduced in the abstract since it is your main contribution. In Section 2.1, the two claimed reasons are not intuitive to me. “Manage the scope of this study” seems not a strong motivation. 

9.	With the development of ViT and vision pre-training, the improvements of CRF-based self-labeling become marginal. Thus, I think it is important to evaluate ViT and vision-pretraining (DINO[4]) backbone with your relaxation. I am wondering whether Potts can improve performances beyond these strong backbones.

Overall,  I think it would be better if the authors could conduct more comprehensive experiment comparisons and related work discussions. The motivation for relaxation should be introduced in the beginning.

Refers:

1.	Sparsely Annotated Semantic Segmentation With Adaptive Gaussian Mixtures. CVPR 23

2.	Label-efficient Segmentation via Affinity Propagation. NIPS 23

3.	Modeling the Label Distributions for Weakly-Supervised Semantic Segmentation. Arxiv 24

4.	DINOv2: Learning Robust Visual Features without Supervision. TMLR 24

5. CC4S: Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation. TPAMI 24

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper considers semantic segmentation under scribble supervision. The paper studies relaxations of the Potts model and proposes a framework for generating soft pseudo-labels, which benefit over hard pseudo-labels in that they can represent uncertainty. The paper highlights problem cases with two standard relaxations, the quadratic and bilinear, and proposes a normalized quadratic relaxation. Moreover, the paper proposes to use a collision cross-entropy loss between the prediction and the pseudo-labels. Different settings are evaluated experimentally, and the proposed approach is compared to the state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Overall, the paper is well written, and the proposed approach is intuitive and should be fairly easy to reproduce, even without code.

The problem under consideration is important as it aims to reduce the manual annotation challenge in image segmentation, which is otherwise costly and time consuming.

The choice of Potts relaxations and cross-entropy terms are supported by experiments, and the proposed approach is further compared to previous methods, showing strong performance.

Weaknesses:
Some details are missing or unclear in the main paper. Considering that the soft self-labeling loss in (6) is a key contribution, it would be useful to include some details regarding the optimization of the pseudo-labels in in the main paper from Appendix B. Additionally, pairwise affinities based on intensity edges are mention at line 42, but it is unclear whether they are used in the proposed approach, see questions.

The proposed approach is only evaluated on a single dataset in the main paper. The results on additional datasets in the appendix should be moved to the main paper to better communicate the empirical findings, as they are easy to miss otherwise. This also raises some confusion about Appendix C which says that three datasets are used in Section 3.5, but results are only reported on PASCAL. What was the reason to exclude these from the main paper?

No error bars. Considering that some settings have fairly similar performance, e.g. in Table 3, standard deviation or similar over multiple runs would be useful.

Some figures have excessively small fonts, e.g. Figures 3, 4, 6.

Limitations:
The paper does not discuss limitations.

Societal impacts are not discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes a soft self-labeling framework for weakly supervised semantic segmentation using scribbles. This model-agnostic framework requires only the joint optimization of network predictions and pseudo labels, guided by specific loss functions: collision cross-entropy and log-quadratic Potts relaxations. The design choices are supported by theoretical concepts and experimental results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The work systematically analyzes common loss functions for weakly supervised semantic segmentation. By investigating theoretical concepts and experimental results, it discusses the advantages and disadvantages of these loss functions. Based on this comprehensive analysis, the work concludes by recommending the use of collision cross-entropy and log-quadratic Potts relaxations for a soft self-labeling framework.

Weaknesses:
Method
- The proposed method is tested on DeepLab. Although the theoretical concept should hold for any model, its effectiveness on other segmentation models remains unknown. 
- The work explains the rationale behind design choices from a theoretical perspective, but it does not clarify why these choices lead to specific pseudo-labels from a vision perspective. For instance, in Figure 5, NN successfully segments the bicycle while DN does not. Is this because the bicycle is a minor class? 
- The work does not provide mIoU per category, leaving it unclear whether the method is effective for all categories or just a few major ones.

Minor Writing Issue:
- The legends of the figures, such as Figures 1, 4, and 6, are small.
- Figure 1 (b) contains an extra icon (house).

Related works:
- There are more image-level WSSS works than just [4, 21]. The author should consider including additional relevant works in lines 16-18.

- Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks
- Weakly supervised learning of instance segmentation with interpixel relations.
- Extracting class activation maps from on-discriminative features as well.
- Boundary-enhanced co-training for weakly supervised semantic segmentation.
- Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation
- Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation
- Group-wise semantic mining for weakly supervised semantic segmentation

Limitations:
I agree with the author that the training time is one of the limitations.
I hope the author can consider my suggestion in the above section to build a link between the design choice and the pseudo labels from the computer vision perspective.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new framework for Weakly-Supervised image Segmentation. The main contribution is to use a soft-labelling approach which is considered superior to classic hard labelling because it can keep track of the centanty of the label. Then different forms of second order potts relaxation and cross-entropy are evaluated.
Results show that the proposed approach with its best setting is able to perform better than previous approaches and comparably to a fully supervised approach with only 3% of annotated pixels.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
\+ The paper is in general well written and easy to follow

\+ Nice illustrations help to understand the proposed approaches

\+ Ablations on the most important components help to understand the significance of each component

Weaknesses:
\- It is not clear if results are significant. For instance is 1% a meaningful improvement or it can be generated by just noise? It is important to see results on multiple runs with also standard deviation.

\- The motivation about soft-labelling because it keeps information about the label certainty is not clear to me

\- In tab. 5 it is not clear what is the base model you start from. Could you report also the base model with standard cross-entropy and hard potts model? It seems that results are very good because the baseline is already quite good.

\- Authors did not say much about the computational cost of the proposed approach compared to common models.

Limitations:
Authors did not mention the possible limitations of the proposed approach.
One possible limitation is the need of additional hyper-parameters to tune which can take time. Authors should comment on that.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
TKozKEMKiw;"REVIEW 
Summary:
This paper formulated the problem of finding an optimal decision tree as Markov Decision Problem and solve the scalability problem using an information-theoretic test generation function. This method provides a trade-off between the train accuracy and tree sizes, the decision tree naturally offers interpretability over ML algorithms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper is well-written and well-organized, it combines RL and decision tree generation, and building MDP before constructing the decision tree.

Weaknesses:
1. The paper lacks sufficient novelty. The approach of constructing a Markov Decision Process (MDP) and using Decision Trees (DT) to generate actions is not new. Specifically, Algorithm 1 appears to still rely on Classification and Regression Trees (CART) for splitting criteria, which diminishes the originality of the proposed method.
2. The evaluated scenarios in the paper are not clearly articulated. The algorithm has not been tested against well-known benchmarks, unlike other optimal DT algorithms. This makes it difficult to assess the comparative performance and robustness of the proposed approach.
3. The advantages of using this algorithm instead of CART are not clearly demonstrated. Both algorithms control tree size and depth. However, CART is known to converge faster and offers a simpler implementation. Without clear evidence of the benefits, it is hard to justify the use of the proposed method over established techniques like CART.
4. The definition of actions generated by the tree is ambiguous. It is not clear whether the actions are discrete or continuous. If the algorithm is designed to build an MDP, it should be tested on general reinforcement learning (RL) tasks to validate its effectiveness and applicability in broader contexts.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors pose binary decision tree construction within the framework of Markov Decision Processes. They first propose methods for constructing an MDP from a decision tree construction problem, exploring varying test generating functions that trade off the coverage of the search space vs the size of the search space. They then apply Dynamic programming to solve the resulting MDP and show this learnt method can both create binary trees that minimise the loss over a dataset but that it can also be used to add additional losses a user may have over decision trees such as the prior that trees should be small, making them interpretable. They evaluate their proposed method, comparing with other high performing methods such as Quant-BnB, MurTree and a DeepRL method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Well written paper and easy to understand the method
- Clearly an important direction of research
- Thorough experiments with appropriate baselines and good range of datasets to ensure the conclusions generalise to a wide range of datasets
- Code fully provided, along with implementations of baselines used

Weaknesses:
- Various versions of Reinforcement Learning for binary tree construction have previously been explored. While the implementation in this paper is ultimately different and appears to significantly improve performance, there is limited novelty of the approach. Novelty largely comes down to the test generating functions explored and the addition of extra losses (interpretability) in addition to just the dataset accuracy.

- Small formatting issue
     - Table 1 is too small

Limitations:
- Limitations are appropriate addressed

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper models the construction of decision trees as a reinforcement learning problem. Currently SOTA algorithms for constructing decision trees have the drawbacks that 1) they take long to compute at depths > 3, and 2) the trees constructed are complex and difficult to interpret. By modelling the problem as an RL task, the authors hope to make the construction of decision trees scale to larger sizes. They present Dynamic Programming Decision Trees which models tree construction as a MDP solved using dynamic programming. They evaluate the accuracy of trees produced by their approach empirically against other commonly used approaches.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Originality:** The approach presented is a novel approach to constructing dynamic trees.

**Significance:** As datasets become larger and interpretability becomes more important, having an approach that scales DT construction to larger trees is needed now. This makes this work rather significant.

**Clarity:** The first half of the paper (up to Sec. 4) was clear. It becomes harder to understand after. Providing an intuitive explanation certain equations would be helpful. E.g., why is probability $p_l = |X_l| / |X|$?

**Quality:** The technique designed is sound and the experiments chosen were the correct ones to demonstrate their claims.

Weaknesses:
The experimental evaluation is weak. From what I understood, the algorithms being evaluated were run only once and evaluated once. The results do not statistically back up the authors' claims. Multiple runs with statistical significance testing is needed.

There is no actual analysis on the interpretability of the trees produced, only the complexity of the trees.

Limitations:
The authors mention one limitation (test generation) as a problem. It seems that that would make it difficult for DPDT to actually scale to larger trees. Is that not so? Is scalability not a limitation then?

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes to use an approach for learning interpretable
decision trees using markov decision processes. The results are shown
to be competitive with branch and bound methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
None of notice, given the listed weaknesses.

Weaknesses:
There exists extensive experimental evidence challenging the claims
about the interpretability of decision trees, while simultaneously
demonstrating the need for decision trees to be explained, since these
can otherwise exhibit arbitrary explanation redundancy.

As a result, and at present, there is no practical justification
whatsoever to learn so-called interpretable optimal decision trees.
It is absolutely unclear that optimal decision trees will provide any
advantage, regarding computed explanations, over decision trees
induced with heuristic algorithms.

Given the above, I cannot recommend acceptance.

Some references on the necessity of explaining decision trees.

Xuanxiang Huang, Yacine Izza, Alexey Ignatiev, João Marques-Silva: On
Efficiently Explaining Graph-Based Classifiers. KR 2021: 356-367

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On the Computational
Intelligibility of Boolean Classifiers. KR 2021: 74-86

Yacine Izza, Alexey Ignatiev, João Marques-Silva: On Tackling
Explanation Redundancy in Decision Trees. J. Artif. Intell. Res. 75:
261-321 (2022)

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On the explanatory power of
Boolean decision trees. Data Knowl. Eng. 142: 102088 (2022)

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On Preferred Abductive
Explanations for Decision Trees and Random Forests. IJCAI 2022:
643-650

João Marques-Silva, Alexey Ignatiev: No silver bullet: interpretable
ML models must be explained. Frontiers Artif. Intell. 6 (2023)

Limitations:
These were listed above. I believe the paper is solving a non-relevant
problem given practical and theoretical evidence regarding the
non-interpretability of decision trees, be these optimal or not.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper formulated the problem of finding an optimal decision tree as Markov Decision Problem and solve the scalability problem using an information-theoretic test generation function. This method provides a trade-off between the train accuracy and tree sizes, the decision tree naturally offers interpretability over ML algorithms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper is well-written and well-organized, it combines RL and decision tree generation, and building MDP before constructing the decision tree.

Weaknesses:
1. The paper lacks sufficient novelty. The approach of constructing a Markov Decision Process (MDP) and using Decision Trees (DT) to generate actions is not new. Specifically, Algorithm 1 appears to still rely on Classification and Regression Trees (CART) for splitting criteria, which diminishes the originality of the proposed method.
2. The evaluated scenarios in the paper are not clearly articulated. The algorithm has not been tested against well-known benchmarks, unlike other optimal DT algorithms. This makes it difficult to assess the comparative performance and robustness of the proposed approach.
3. The advantages of using this algorithm instead of CART are not clearly demonstrated. Both algorithms control tree size and depth. However, CART is known to converge faster and offers a simpler implementation. Without clear evidence of the benefits, it is hard to justify the use of the proposed method over established techniques like CART.
4. The definition of actions generated by the tree is ambiguous. It is not clear whether the actions are discrete or continuous. If the algorithm is designed to build an MDP, it should be tested on general reinforcement learning (RL) tasks to validate its effectiveness and applicability in broader contexts.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors pose binary decision tree construction within the framework of Markov Decision Processes. They first propose methods for constructing an MDP from a decision tree construction problem, exploring varying test generating functions that trade off the coverage of the search space vs the size of the search space. They then apply Dynamic programming to solve the resulting MDP and show this learnt method can both create binary trees that minimise the loss over a dataset but that it can also be used to add additional losses a user may have over decision trees such as the prior that trees should be small, making them interpretable. They evaluate their proposed method, comparing with other high performing methods such as Quant-BnB, MurTree and a DeepRL method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Well written paper and easy to understand the method
- Clearly an important direction of research
- Thorough experiments with appropriate baselines and good range of datasets to ensure the conclusions generalise to a wide range of datasets
- Code fully provided, along with implementations of baselines used

Weaknesses:
- Various versions of Reinforcement Learning for binary tree construction have previously been explored. While the implementation in this paper is ultimately different and appears to significantly improve performance, there is limited novelty of the approach. Novelty largely comes down to the test generating functions explored and the addition of extra losses (interpretability) in addition to just the dataset accuracy.

- Small formatting issue
     - Table 1 is too small

Limitations:
- Limitations are appropriate addressed

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper models the construction of decision trees as a reinforcement learning problem. Currently SOTA algorithms for constructing decision trees have the drawbacks that 1) they take long to compute at depths > 3, and 2) the trees constructed are complex and difficult to interpret. By modelling the problem as an RL task, the authors hope to make the construction of decision trees scale to larger sizes. They present Dynamic Programming Decision Trees which models tree construction as a MDP solved using dynamic programming. They evaluate the accuracy of trees produced by their approach empirically against other commonly used approaches.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Originality:** The approach presented is a novel approach to constructing dynamic trees.

**Significance:** As datasets become larger and interpretability becomes more important, having an approach that scales DT construction to larger trees is needed now. This makes this work rather significant.

**Clarity:** The first half of the paper (up to Sec. 4) was clear. It becomes harder to understand after. Providing an intuitive explanation certain equations would be helpful. E.g., why is probability $p_l = |X_l| / |X|$?

**Quality:** The technique designed is sound and the experiments chosen were the correct ones to demonstrate their claims.

Weaknesses:
The experimental evaluation is weak. From what I understood, the algorithms being evaluated were run only once and evaluated once. The results do not statistically back up the authors' claims. Multiple runs with statistical significance testing is needed.

There is no actual analysis on the interpretability of the trees produced, only the complexity of the trees.

Limitations:
The authors mention one limitation (test generation) as a problem. It seems that that would make it difficult for DPDT to actually scale to larger trees. Is that not so? Is scalability not a limitation then?

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes to use an approach for learning interpretable
decision trees using markov decision processes. The results are shown
to be competitive with branch and bound methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
None of notice, given the listed weaknesses.

Weaknesses:
There exists extensive experimental evidence challenging the claims
about the interpretability of decision trees, while simultaneously
demonstrating the need for decision trees to be explained, since these
can otherwise exhibit arbitrary explanation redundancy.

As a result, and at present, there is no practical justification
whatsoever to learn so-called interpretable optimal decision trees.
It is absolutely unclear that optimal decision trees will provide any
advantage, regarding computed explanations, over decision trees
induced with heuristic algorithms.

Given the above, I cannot recommend acceptance.

Some references on the necessity of explaining decision trees.

Xuanxiang Huang, Yacine Izza, Alexey Ignatiev, João Marques-Silva: On
Efficiently Explaining Graph-Based Classifiers. KR 2021: 356-367

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On the Computational
Intelligibility of Boolean Classifiers. KR 2021: 74-86

Yacine Izza, Alexey Ignatiev, João Marques-Silva: On Tackling
Explanation Redundancy in Decision Trees. J. Artif. Intell. Res. 75:
261-321 (2022)

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On the explanatory power of
Boolean decision trees. Data Knowl. Eng. 142: 102088 (2022)

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On Preferred Abductive
Explanations for Decision Trees and Random Forests. IJCAI 2022:
643-650

João Marques-Silva, Alexey Ignatiev: No silver bullet: interpretable
ML models must be explained. Frontiers Artif. Intell. 6 (2023)

Limitations:
These were listed above. I believe the paper is solving a non-relevant
problem given practical and theoretical evidence regarding the
non-interpretability of decision trees, be these optimal or not.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
TI7Vy90B9j;"REVIEW 
Summary:
This paper studies last-iterate convergence rates of online learning in monotone games. The main contribution is an algorithm called Gradient Ascent with Boosting Payoff Perturbation (GABP). The GABP algorithm achieves (1) $O(\log T / T)$ last-iterate convergence with full gradient feedback, which is near-optimal; (2) and $O(1/T^{1/7})$ last-iterate convergence with noisy gradient feedback (the noise is zero-mean with bounded variance). The latter result improves prior results of $O(1/T^{1/10})$. Moreover, the GABP algorithm guarantees an individual dynamic regret of $O(\log^2 T)$ under full gradient feedback, slightly worse than the state-of-the-art bound of $O(\log T)$. This paper also contains numerical experiments on small game instances to demonstrate the effectiveness of GABP.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The problem of last-iterate convergence rates of no-regret learning algorithms in monotone games is relevant and interesting. Most existing results focus on the full gradient feedback, while only a few provide concrete convergence rates under the noisy gradient or the bandit feedback. The proposed GABP algorithm has near-optimal $O(\log T / T)$ last-iterate convergence rate under full gradient feedback. It also improves the convergence rates under noisy gradient feedback from $O(1/T^{1/10})$ to $O(1/T^{1/7})$. This is a solid contribution to learning in games, although the rate for the noisy gradient feedback setting may not be tight.

Weaknesses:
1. The proposed GABP algorithm does not achieve the optimal $O(1/T)$ last-iterate convergence rate under full gradient feedback. The $O(1/T^{1/7})$ last-iterate convergence rate is also not tight for the noisy feedback.
2. The relationship between the proposed GABP algorithm and the AOG algorithm in [1] and the intuition behind the fast last-iterate convergence rates is not clearly discussed. These two algorithms are different (as shown in Appendix F) but share similar ideas. The anchoring term in both algorithms comes from the (implicit) Halpern iteration algorithm, which can not be run directly. The difference is that GABP views each step of Halpern iteration as a fixed point problem (Line 170) and uses an inner loop of $\log (1/\epsilon)$ steps to get an $\epsilon$-approximation (this is called updating the reference strategy in the paper.); In contrast, AOG directly uses optimism to approximate the implicit update. This leads to GABP being a log factor slower than AOG in the full gradient setting. However, the approximating the fixed point approach is more robust in the noisy gradient setting due to strong monotonicity. Moreover, the potential function and the approximately non-increasing potential analysis are very similar to that used in [1]. If they are inspired by [1] then this should be acknowledged. 

[1] Doubly Optimal No-Regret Learning in Monotone Games, Cai and Zheng, ICML 2023.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel algorithmic approach to enhance the convergence of first-order methods in the context of monotone games. The authors propose a payoff perturbation technique that introduces strong convexity to players' payoff functions, which is crucial for achieving last-iterate convergence. This technique is particularly designed to handle scenarios where the gradient of the payoff functions is monotone and potentially noisy. The paper presents a method called Gradient Ascent with Boosting Payoff Perturbation (GABP), which incorporates a unique perturbation into the payoff function and maintains a periodically re-initializing anchoring strategy. The authors demonstrate that GABP offers faster last-iterate convergence rates compared to existing algorithms, even in the presence of additive noise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a unique perturbation technique that addresses the challenge of last-iterate convergence in monotone games. The proposed GABP algorithm is an innovative modification of Adaptively Perturbed Mirror Descent (APMD), offering improved convergence rates.

Quality: The theoretical development is thorough, with rigorous proofs provided for the convergence rates of GABP in both full and noisy feedback settings. The paper also includes a detailed analysis of the algorithm's performance in terms of individual regret.

Clarity: The paper is well-organized, with clear explanations of the algorithm, theoretical results, and experimental setup. The use of pseudo-code for GABP aids in understanding the algorithm's implementation.

Significance: The work contributes to the field of online learning in games, providing a solution that is particularly relevant for applications such as Generative Adversarial Networks (GANs) and large language model fine-tuning, where last-iterate convergence is desirable.

Weaknesses:
Experimental Validation: While the paper provides empirical results, the experiments could be expanded to include a broader range of game types and noise levels to further validate the robustness and generalizability of GABP.

Comparison with State-of-the-Art: The paper compares GABP with APMD and Optimistic Gradient Ascent (OGA) but could benefit from a more comprehensive comparison with other existing methods in the literature to better situate its contributions.

Practical Considerations: While the paper addresses the theoretical aspects of GABP, it could provide more insights into practical considerations, such as the implementation challenges and potential modifications needed for real-world applications.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work focuses on last-iterate convergence of  game dynamics. A payoff perturbation technique is proposed by adding strong convexity to players' payoff functions. Despite it is a well studied technqiue in learning in repeated games with first-order methods, especially in last-iterate convergence, a novel perturbation scheme introduced in this paper allows on to provide faster last-iterate convergence compared to previous works.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper provides a relatively complete result containing last-iterate convergence rate of the proposed algorithm GABP in full feedback and noisy feedback. The faster rate of convergence is an improvement compared to existing works. Except for some weakness (will be stated later), the presentation of this paper is clear to understand. The authors have reviewed most related works to my best knowledge, so that the contributions claimed are easy to follow. Addition to theoretical works, this paper has provided experiments (sufficient in my opinion) showing the comparison of GABP and existing algorithms such as Adaptively perturbed gradient ascent and Optimistic gradient ascent.

Weaknesses:
One obvious spot that should be added to improve the presentation is the following. The game considered in this paper is motivated by real-life examples. But the authors only give one example motivating monotone games. Part of contributions of the paper is claimed to be the study of two feedback models: full feedback and noisy feedback, but there is not specific examples and applications illustrating the importance of these settings. For sure readers can always find related works even just by googling the keywords, but providing concrete application scenes where the gradient of payoff can be achieved perfectly or only partially achievable gradients can be obtained is important, especially ""noisy feedback"" can be just a model of many cases.

Limitations:
This is theory based paper, no potential negative impact will cause.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies first order methods to solve monotone games where the gradient of the payoff function is monotone in the strategy, along with additive noise. The authors introduce a payoff perturbation technique which introduces strong convexity to the to the payoff functions and thereby derive last iterate convergence rates.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper is well written and the method and results are interesting.

Weaknesses:
The authors should include a table which compares their paper with others in the literature. This would make it easier for the reader to place the results in context and see where improvements are made more easily.
(for example comparison to [Yoon and Ryu, 2021, Cai and Zheng, 2023] including constants)

Limitations:
See Weaknesses and Questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies last-iterate convergence rates of online learning in monotone games. The main contribution is an algorithm called Gradient Ascent with Boosting Payoff Perturbation (GABP). The GABP algorithm achieves (1) $O(\log T / T)$ last-iterate convergence with full gradient feedback, which is near-optimal; (2) and $O(1/T^{1/7})$ last-iterate convergence with noisy gradient feedback (the noise is zero-mean with bounded variance). The latter result improves prior results of $O(1/T^{1/10})$. Moreover, the GABP algorithm guarantees an individual dynamic regret of $O(\log^2 T)$ under full gradient feedback, slightly worse than the state-of-the-art bound of $O(\log T)$. This paper also contains numerical experiments on small game instances to demonstrate the effectiveness of GABP.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The problem of last-iterate convergence rates of no-regret learning algorithms in monotone games is relevant and interesting. Most existing results focus on the full gradient feedback, while only a few provide concrete convergence rates under the noisy gradient or the bandit feedback. The proposed GABP algorithm has near-optimal $O(\log T / T)$ last-iterate convergence rate under full gradient feedback. It also improves the convergence rates under noisy gradient feedback from $O(1/T^{1/10})$ to $O(1/T^{1/7})$. This is a solid contribution to learning in games, although the rate for the noisy gradient feedback setting may not be tight.

Weaknesses:
1. The proposed GABP algorithm does not achieve the optimal $O(1/T)$ last-iterate convergence rate under full gradient feedback. The $O(1/T^{1/7})$ last-iterate convergence rate is also not tight for the noisy feedback.
2. The relationship between the proposed GABP algorithm and the AOG algorithm in [1] and the intuition behind the fast last-iterate convergence rates is not clearly discussed. These two algorithms are different (as shown in Appendix F) but share similar ideas. The anchoring term in both algorithms comes from the (implicit) Halpern iteration algorithm, which can not be run directly. The difference is that GABP views each step of Halpern iteration as a fixed point problem (Line 170) and uses an inner loop of $\log (1/\epsilon)$ steps to get an $\epsilon$-approximation (this is called updating the reference strategy in the paper.); In contrast, AOG directly uses optimism to approximate the implicit update. This leads to GABP being a log factor slower than AOG in the full gradient setting. However, the approximating the fixed point approach is more robust in the noisy gradient setting due to strong monotonicity. Moreover, the potential function and the approximately non-increasing potential analysis are very similar to that used in [1]. If they are inspired by [1] then this should be acknowledged. 

[1] Doubly Optimal No-Regret Learning in Monotone Games, Cai and Zheng, ICML 2023.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel algorithmic approach to enhance the convergence of first-order methods in the context of monotone games. The authors propose a payoff perturbation technique that introduces strong convexity to players' payoff functions, which is crucial for achieving last-iterate convergence. This technique is particularly designed to handle scenarios where the gradient of the payoff functions is monotone and potentially noisy. The paper presents a method called Gradient Ascent with Boosting Payoff Perturbation (GABP), which incorporates a unique perturbation into the payoff function and maintains a periodically re-initializing anchoring strategy. The authors demonstrate that GABP offers faster last-iterate convergence rates compared to existing algorithms, even in the presence of additive noise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a unique perturbation technique that addresses the challenge of last-iterate convergence in monotone games. The proposed GABP algorithm is an innovative modification of Adaptively Perturbed Mirror Descent (APMD), offering improved convergence rates.

Quality: The theoretical development is thorough, with rigorous proofs provided for the convergence rates of GABP in both full and noisy feedback settings. The paper also includes a detailed analysis of the algorithm's performance in terms of individual regret.

Clarity: The paper is well-organized, with clear explanations of the algorithm, theoretical results, and experimental setup. The use of pseudo-code for GABP aids in understanding the algorithm's implementation.

Significance: The work contributes to the field of online learning in games, providing a solution that is particularly relevant for applications such as Generative Adversarial Networks (GANs) and large language model fine-tuning, where last-iterate convergence is desirable.

Weaknesses:
Experimental Validation: While the paper provides empirical results, the experiments could be expanded to include a broader range of game types and noise levels to further validate the robustness and generalizability of GABP.

Comparison with State-of-the-Art: The paper compares GABP with APMD and Optimistic Gradient Ascent (OGA) but could benefit from a more comprehensive comparison with other existing methods in the literature to better situate its contributions.

Practical Considerations: While the paper addresses the theoretical aspects of GABP, it could provide more insights into practical considerations, such as the implementation challenges and potential modifications needed for real-world applications.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work focuses on last-iterate convergence of  game dynamics. A payoff perturbation technique is proposed by adding strong convexity to players' payoff functions. Despite it is a well studied technqiue in learning in repeated games with first-order methods, especially in last-iterate convergence, a novel perturbation scheme introduced in this paper allows on to provide faster last-iterate convergence compared to previous works.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper provides a relatively complete result containing last-iterate convergence rate of the proposed algorithm GABP in full feedback and noisy feedback. The faster rate of convergence is an improvement compared to existing works. Except for some weakness (will be stated later), the presentation of this paper is clear to understand. The authors have reviewed most related works to my best knowledge, so that the contributions claimed are easy to follow. Addition to theoretical works, this paper has provided experiments (sufficient in my opinion) showing the comparison of GABP and existing algorithms such as Adaptively perturbed gradient ascent and Optimistic gradient ascent.

Weaknesses:
One obvious spot that should be added to improve the presentation is the following. The game considered in this paper is motivated by real-life examples. But the authors only give one example motivating monotone games. Part of contributions of the paper is claimed to be the study of two feedback models: full feedback and noisy feedback, but there is not specific examples and applications illustrating the importance of these settings. For sure readers can always find related works even just by googling the keywords, but providing concrete application scenes where the gradient of payoff can be achieved perfectly or only partially achievable gradients can be obtained is important, especially ""noisy feedback"" can be just a model of many cases.

Limitations:
This is theory based paper, no potential negative impact will cause.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies first order methods to solve monotone games where the gradient of the payoff function is monotone in the strategy, along with additive noise. The authors introduce a payoff perturbation technique which introduces strong convexity to the to the payoff functions and thereby derive last iterate convergence rates.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper is well written and the method and results are interesting.

Weaknesses:
The authors should include a table which compares their paper with others in the literature. This would make it easier for the reader to place the results in context and see where improvements are made more easily.
(for example comparison to [Yoon and Ryu, 2021, Cai and Zheng, 2023] including constants)

Limitations:
See Weaknesses and Questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
SeesCzBelI;"REVIEW 
Summary:
The authors considers methods for removing bias in RMs, specifically the bias towards long responses and the bias certain prompts might have to generate high rewards (this stems from the BardleyTerry model being underspecified.). For the second problem the authors proposed PBC which adds a linear layer to the last token of the prompt, the output of which predicts the average reward of completions from the prompt. For the first problem the authors propose to combine PBC with existing length bias correction methods which adds a correlation term to the loss. For experimental results the authors considers RLHF training LLama-7B on the RM-static dataset. They find that their method outperforms baselines on academic benchmarks (Table 2) and in head-to-head comparisons (Fig 4). They also consider hyperparameter stability and ablations in Fig 5.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Bias in RLHF can potentially have a large impact if addressed correctly.

Weaknesses:
1. Academic metrics like MMLU are not a good fit for RLHF. MT-bench is better.
2. There are no error bars, unclear how strong the signal is. 
3. The writing is rather handwavy at times, e.g. the motivation in section 3.1. is very qualitative.
4. The novelty is low.

Limitations:
na

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the prompt bias in RLHF, especially the reward modeling --- beyond the length bias that might exist.
Alleviating reward hacking is an important topic in RLHF, however, with the current paper, some details or contributions are not very clear. I'll elaborate in the following sections.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The problem studied is important. The illustrative figures are helpful.

Weaknesses:
Some notations do not make sense, for example, in Equation (5), averaging over y does not make sense. Would not it be better to average over the C rather than y.

The presentation of the problem itself is not yet clear to me. Although the authors keep using examples in the context to anchor their ideas (which I appreciate), it is still unclear what is the problem this work aims to solve. I like the general idea of Figure 1, however, what does the red color highlighting mean? This figure makes a good contrast between your RM and conventional RM, yet it fails to illustrate the problem your RM aims to solve.

The experimental results are not supportive enough.

Limitations:
Please see weakness

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the Prompt Bias Calibration (PBC) method to address prompt-template bias in reward training of RLHF. The proposed PBC method is validated through extensive empirical results and mathematical analysis, showing its effectiveness in combination with existing length bias removal methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	Good Writing: The paper is well-written and easy to follow.
2.	Innovative Methodology: Introduces Prompt Bias Calibration (PBC) to address prompt-template bias in RLHF.
3.	Strong Empirical Evidence: Demonstrates significant performance improvements through comprehensive evaluations.

Weaknesses:
see questions

Limitations:
see the above

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the issue of reward hacking in RLHF training, superficially, identifying prompt-template bias, defined as when a reward model (RM) develops a preference for responses that adhere to specific formats or templates, even when these formats are not explicitly specified or desired in the prompt and proposes Prompt Bias Calibration (PBC) method that successfully tackles this issue. PBC can also be combined with existing length debiasing methods like ODIN to mitigate both hacks in the reward signal.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The paper identifies and analyzes ""prompt-template bias"" in RLHF, a potentially impactful issue.
* PBC is easy to implement and as shown can be combined with existing approaches.
* Strong empirical validation with good coverage in the experiments and ablation.

Weaknesses:
* Choosing one specific bias - while the title claims that removing the length bias is not enough, it seems to change the scene to potentially removing length and prompt-template bias not being enough. Leading to concerns about needing to combine many methods, one for each mitigation.

Limitations:
The limitations are covered.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors considers methods for removing bias in RMs, specifically the bias towards long responses and the bias certain prompts might have to generate high rewards (this stems from the BardleyTerry model being underspecified.). For the second problem the authors proposed PBC which adds a linear layer to the last token of the prompt, the output of which predicts the average reward of completions from the prompt. For the first problem the authors propose to combine PBC with existing length bias correction methods which adds a correlation term to the loss. For experimental results the authors considers RLHF training LLama-7B on the RM-static dataset. They find that their method outperforms baselines on academic benchmarks (Table 2) and in head-to-head comparisons (Fig 4). They also consider hyperparameter stability and ablations in Fig 5.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Bias in RLHF can potentially have a large impact if addressed correctly.

Weaknesses:
1. Academic metrics like MMLU are not a good fit for RLHF. MT-bench is better.
2. There are no error bars, unclear how strong the signal is. 
3. The writing is rather handwavy at times, e.g. the motivation in section 3.1. is very qualitative.
4. The novelty is low.

Limitations:
na

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the prompt bias in RLHF, especially the reward modeling --- beyond the length bias that might exist.
Alleviating reward hacking is an important topic in RLHF, however, with the current paper, some details or contributions are not very clear. I'll elaborate in the following sections.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The problem studied is important. The illustrative figures are helpful.

Weaknesses:
Some notations do not make sense, for example, in Equation (5), averaging over y does not make sense. Would not it be better to average over the C rather than y.

The presentation of the problem itself is not yet clear to me. Although the authors keep using examples in the context to anchor their ideas (which I appreciate), it is still unclear what is the problem this work aims to solve. I like the general idea of Figure 1, however, what does the red color highlighting mean? This figure makes a good contrast between your RM and conventional RM, yet it fails to illustrate the problem your RM aims to solve.

The experimental results are not supportive enough.

Limitations:
Please see weakness

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the Prompt Bias Calibration (PBC) method to address prompt-template bias in reward training of RLHF. The proposed PBC method is validated through extensive empirical results and mathematical analysis, showing its effectiveness in combination with existing length bias removal methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	Good Writing: The paper is well-written and easy to follow.
2.	Innovative Methodology: Introduces Prompt Bias Calibration (PBC) to address prompt-template bias in RLHF.
3.	Strong Empirical Evidence: Demonstrates significant performance improvements through comprehensive evaluations.

Weaknesses:
see questions

Limitations:
see the above

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the issue of reward hacking in RLHF training, superficially, identifying prompt-template bias, defined as when a reward model (RM) develops a preference for responses that adhere to specific formats or templates, even when these formats are not explicitly specified or desired in the prompt and proposes Prompt Bias Calibration (PBC) method that successfully tackles this issue. PBC can also be combined with existing length debiasing methods like ODIN to mitigate both hacks in the reward signal.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The paper identifies and analyzes ""prompt-template bias"" in RLHF, a potentially impactful issue.
* PBC is easy to implement and as shown can be combined with existing approaches.
* Strong empirical validation with good coverage in the experiments and ablation.

Weaknesses:
* Choosing one specific bias - while the title claims that removing the length bias is not enough, it seems to change the scene to potentially removing length and prompt-template bias not being enough. Leading to concerns about needing to combine many methods, one for each mitigation.

Limitations:
The limitations are covered.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
RdAfUp4LcD;"REVIEW 
Summary:
This work extrapolates the concept of Linear Mode Connectivity (LMC) modulo model invariances to differentiable tree ensembles (DTE). The authors revealed that, in contrast to neural networks (NNs), permutation invariance is insufficient to provide LMC in DTE and propose two additional tree-specific invariances that enable LMC after taking them into account: subtree flip invariance and splitting order invariance. In addition, they provide a modified DTE architecture that does not posses these additional invariances, however still enjoys LMC with only permutation invariance akin to neural network models. This work proposes two algorithms for building LMC given two independently trained DTEs, based on similar methods from NN LMC literature. The claims are supported by a detailed empirical evaluation.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
Honestly, I enjoyed reading this paper. Although I am not specialized in tree ensembles, I have certain expertise in LMC, and was pleased to find that it is also relevant for DTE models. I think that this contribution is novel and significant.

The paper is very well-structured. It was very easy to follow despite having no significant experience in decision trees, the authors did a good job preparing the reader in Sec. 2. 

Section 3 presents the main contributions of this work, which is done very well using both detailed and intuitive text description and auxiliary images illustrating the main concepts.

Empirical evaluation is excellent, involving multiple datasets, hyperparameter options, and random seeds. The authors tackled many important questions concerning the study of LMC in DTEs and even compared with NN LMC, which I specifically liked.

Weaknesses:
It is hard for me to formulate substantial flaws in this work but a couple of remarks that I put in the next section. 

The main weakness of this work is lack of theoretical support and practical implications. However, I acknowledge that these are the same limitations that are attributed to LMC in neural networks, which is a significantly more broad and well-studied field than LMC in tree ensembles. I hope that future work will address these disadvantages in some way. 

Also, I believe that the text could be slightly polished to eliminate typos and small inaccuracies. For instance, the value $D$ in line 127 is not defined at its first occurrence.

Limitations:
The authors discuss the limitations of their methods in Section 3.2.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an analysis of types of neural networks called soft trees from the linear mode connectivity point of view. The authors enumerate 3 types of invariances inherent to soft trees and study linear mode connectivity between different solutions (by solution they understand a trained ensemble of soft tree models) after weights or activations matching that account for these invariances. They also study linear mode connectivity for a special case of soft trees - decision list-based tree - that has only one type of invariance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written
- Authors claim that it is the first paper to study linear mode connectivity for soft trees

Weaknesses:
## Insufficient contribution
- In my opinion, the main contribution of this paper is a showcase that different architectures need to account for different invariances when LMC is analyzed, e.g. MLP and soft trees have different invariances. I think that this insight alone is not enough for a paper, because it sounds quite obvious even without analysis.

## Questionable results
- It is very important to make sure that interpolation results are not computed between the models which are almost identical (that can happen if there is not enough diversity in training recipes). Could you please provide results with distances (any kind of them, e.g. L2 or cosine similarity) between the solutions in Figure. 5 for ""Naive"", ""Tree Permutation"" and ""Ours"" parameter transformations?
- I would expect decision list trees to be much weaker than soft trees because they have less parameters. Could you please report its performance or show me where I can find it?
- Model merging is mentioned as one of the applications for linear mode connectivity (LMC), however, no results for model merging are provided.
  - line 32: ""In addition, LMC also holds significant practical importance, enabling techniques such as model merging [6, 7] by weight-space parameter averaging.""

## Questionable explanation
- I could not find a related work section.
- What is ""Ours"" in Table 2?
- I did not find in the main text any explanation (even after looking into algorithms in appendix, which I found very confusing) for the operation of weights matching (WM) and activation matching (AM) in case of such invariances as ""Perm"", ""Order"" and ""Flip"" (Notation is from Table 1). Since invariances are the main part of the whole analysis, could you please elaborate more?
- Another important part of parameter transforms includes Linear Assignment Problem (LAP), but I could not find any details for it neither.

Limitations:
- There is no theoretical justification for why and in which scenarios linear mode connectivity exists for soft trees.
- The paper does not propose any practical application for the linear mode connectivity between soft trees. While it can be argued that this paper is an analysis paper, some practical applications can be useful in motivating this kind of analysis.
- I did not find the code of the project while in the survey it is written that code is provided in supplementary material.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper empirically shows that separately trained tree emsemble models can show Linear Mode Connectivity (LMC) when considering tree invariant operations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The exploration of LMC on tree emsemble models is interesting.
- The computational process is clearly stated which makes this paper easy to follow.

------

After reading author rebuttal and discussion with other reviewers, I decide to increase my rating of this paper to borderline reject.

Weaknesses:
This paper does not provide any insights into the question of LMC in neural networks, as it is exploring a totally different model. Although it is always interesting to consider LMC in another senario, I find the contribution of this paper rather insignificant and incremental, since it is basically applying the same idea of [1] to another model. I do not want to deny the author's valueable efforts in exploring symmetry in a new model and using it to achieve LMC, but I just feel that the contribution of this paper may not be sufficient for it to be accepted by this conference.

One possible direction I can suggest for the authors to enhance the current paper is, if any non-trivial theory about LMC can be made on the tree ensemble model setting, then this work will be much more exciting. The underlying reason why neural networks can be made linear connected is not yet clear, and the is hard to study due to the non-linear nature of deep NNs. If the authors can show that the tree ensemble model can be an alternative model to study LMC from a theoretical perspective, then this will make the current work more valuable and intresting.

[1] Git Re-Basin: Merging Models modulo Permutation Symmetries

**Regard writting**
The intro is kind of confusing for readers who are not familiar with the tree ensemble models. It's even unclear whether 1) it is a new model ensembling method for neural networks, or 2) it is a new model, or 3) it is a training method. Although those questions are addressed after reading the detailed definition of tree ensemble in Section 2.2, I think it is better to make it clear in the intro to avoid any confusing.

Limitations:
Authors discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to achieve LMC for soft tree ensembles. Akin to achieve LMC for neural network after accounting for permutation invariance, the authors introduce three different kinds of invariance in soft tree ensembles: tree permutation invariance, subtree flip invariance, splitting order invariance. Additionally, the authors demonstrate that better LMC can be achieved after considering all three kinds of invariance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of extending LMC from neural networks to differentiable tree ensembles is interesting. 
2. Invariances beyond permutation variance are identified for differentiable tree ensembles. The authors demonstrate the effectiveness of accounting for these invariances when doing matching.

Weaknesses:
1. I am not familiar with differentiable tree ensembles, therefore, I would suggest the authors put more efforts on explaining tree ensembles and illustrating the invariances.
2. Another concern is about the motivation. This study is motivated by the question ""Can LMC be achieved for soft tree ensembles?"" but why would we achieve LMC for the tree ensembles? I would expect more elaboration on the motivation.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work extrapolates the concept of Linear Mode Connectivity (LMC) modulo model invariances to differentiable tree ensembles (DTE). The authors revealed that, in contrast to neural networks (NNs), permutation invariance is insufficient to provide LMC in DTE and propose two additional tree-specific invariances that enable LMC after taking them into account: subtree flip invariance and splitting order invariance. In addition, they provide a modified DTE architecture that does not posses these additional invariances, however still enjoys LMC with only permutation invariance akin to neural network models. This work proposes two algorithms for building LMC given two independently trained DTEs, based on similar methods from NN LMC literature. The claims are supported by a detailed empirical evaluation.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
Honestly, I enjoyed reading this paper. Although I am not specialized in tree ensembles, I have certain expertise in LMC, and was pleased to find that it is also relevant for DTE models. I think that this contribution is novel and significant.

The paper is very well-structured. It was very easy to follow despite having no significant experience in decision trees, the authors did a good job preparing the reader in Sec. 2. 

Section 3 presents the main contributions of this work, which is done very well using both detailed and intuitive text description and auxiliary images illustrating the main concepts.

Empirical evaluation is excellent, involving multiple datasets, hyperparameter options, and random seeds. The authors tackled many important questions concerning the study of LMC in DTEs and even compared with NN LMC, which I specifically liked.

Weaknesses:
It is hard for me to formulate substantial flaws in this work but a couple of remarks that I put in the next section. 

The main weakness of this work is lack of theoretical support and practical implications. However, I acknowledge that these are the same limitations that are attributed to LMC in neural networks, which is a significantly more broad and well-studied field than LMC in tree ensembles. I hope that future work will address these disadvantages in some way. 

Also, I believe that the text could be slightly polished to eliminate typos and small inaccuracies. For instance, the value $D$ in line 127 is not defined at its first occurrence.

Limitations:
The authors discuss the limitations of their methods in Section 3.2.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an analysis of types of neural networks called soft trees from the linear mode connectivity point of view. The authors enumerate 3 types of invariances inherent to soft trees and study linear mode connectivity between different solutions (by solution they understand a trained ensemble of soft tree models) after weights or activations matching that account for these invariances. They also study linear mode connectivity for a special case of soft trees - decision list-based tree - that has only one type of invariance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written
- Authors claim that it is the first paper to study linear mode connectivity for soft trees

Weaknesses:
## Insufficient contribution
- In my opinion, the main contribution of this paper is a showcase that different architectures need to account for different invariances when LMC is analyzed, e.g. MLP and soft trees have different invariances. I think that this insight alone is not enough for a paper, because it sounds quite obvious even without analysis.

## Questionable results
- It is very important to make sure that interpolation results are not computed between the models which are almost identical (that can happen if there is not enough diversity in training recipes). Could you please provide results with distances (any kind of them, e.g. L2 or cosine similarity) between the solutions in Figure. 5 for ""Naive"", ""Tree Permutation"" and ""Ours"" parameter transformations?
- I would expect decision list trees to be much weaker than soft trees because they have less parameters. Could you please report its performance or show me where I can find it?
- Model merging is mentioned as one of the applications for linear mode connectivity (LMC), however, no results for model merging are provided.
  - line 32: ""In addition, LMC also holds significant practical importance, enabling techniques such as model merging [6, 7] by weight-space parameter averaging.""

## Questionable explanation
- I could not find a related work section.
- What is ""Ours"" in Table 2?
- I did not find in the main text any explanation (even after looking into algorithms in appendix, which I found very confusing) for the operation of weights matching (WM) and activation matching (AM) in case of such invariances as ""Perm"", ""Order"" and ""Flip"" (Notation is from Table 1). Since invariances are the main part of the whole analysis, could you please elaborate more?
- Another important part of parameter transforms includes Linear Assignment Problem (LAP), but I could not find any details for it neither.

Limitations:
- There is no theoretical justification for why and in which scenarios linear mode connectivity exists for soft trees.
- The paper does not propose any practical application for the linear mode connectivity between soft trees. While it can be argued that this paper is an analysis paper, some practical applications can be useful in motivating this kind of analysis.
- I did not find the code of the project while in the survey it is written that code is provided in supplementary material.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper empirically shows that separately trained tree emsemble models can show Linear Mode Connectivity (LMC) when considering tree invariant operations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The exploration of LMC on tree emsemble models is interesting.
- The computational process is clearly stated which makes this paper easy to follow.

------

After reading author rebuttal and discussion with other reviewers, I decide to increase my rating of this paper to borderline reject.

Weaknesses:
This paper does not provide any insights into the question of LMC in neural networks, as it is exploring a totally different model. Although it is always interesting to consider LMC in another senario, I find the contribution of this paper rather insignificant and incremental, since it is basically applying the same idea of [1] to another model. I do not want to deny the author's valueable efforts in exploring symmetry in a new model and using it to achieve LMC, but I just feel that the contribution of this paper may not be sufficient for it to be accepted by this conference.

One possible direction I can suggest for the authors to enhance the current paper is, if any non-trivial theory about LMC can be made on the tree ensemble model setting, then this work will be much more exciting. The underlying reason why neural networks can be made linear connected is not yet clear, and the is hard to study due to the non-linear nature of deep NNs. If the authors can show that the tree ensemble model can be an alternative model to study LMC from a theoretical perspective, then this will make the current work more valuable and intresting.

[1] Git Re-Basin: Merging Models modulo Permutation Symmetries

**Regard writting**
The intro is kind of confusing for readers who are not familiar with the tree ensemble models. It's even unclear whether 1) it is a new model ensembling method for neural networks, or 2) it is a new model, or 3) it is a training method. Although those questions are addressed after reading the detailed definition of tree ensemble in Section 2.2, I think it is better to make it clear in the intro to avoid any confusing.

Limitations:
Authors discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to achieve LMC for soft tree ensembles. Akin to achieve LMC for neural network after accounting for permutation invariance, the authors introduce three different kinds of invariance in soft tree ensembles: tree permutation invariance, subtree flip invariance, splitting order invariance. Additionally, the authors demonstrate that better LMC can be achieved after considering all three kinds of invariance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of extending LMC from neural networks to differentiable tree ensembles is interesting. 
2. Invariances beyond permutation variance are identified for differentiable tree ensembles. The authors demonstrate the effectiveness of accounting for these invariances when doing matching.

Weaknesses:
1. I am not familiar with differentiable tree ensembles, therefore, I would suggest the authors put more efforts on explaining tree ensembles and illustrating the invariances.
2. Another concern is about the motivation. This study is motivated by the question ""Can LMC be achieved for soft tree ensembles?"" but why would we achieve LMC for the tree ensembles? I would expect more elaboration on the motivation.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
PnSTlFUfcd;"REVIEW 
Summary:
The paper proposes an online shielding approach for safe RL that does not assume prior knowledge of environment dynamics and utilizes finite-horizon model checking with learned approximations of the environment dynamics. It specifically focuses on RL with regular safety properties provided as a PCTL formula. The authors present a framework that dynamically identifies unsafe actions and deploys a safe backup policy when necessary. The main technical contributions of the paper are:

- Definition of a constrained RL problem based on regular safety properties.
- Presentation of model checking algorithms to verify finite-horizon satisfaction probability.
- Development of sample complexity results for statistical model checking procedures.

The novelty of the paper lies in its approach to reinforcement learning with regular safety properties without requiring prior knowledge of environment dynamics. Unlike traditional shielding approaches that need full environment models or simulators, this framework uses learned approximations and finite-horizon model checking. The authors represent the synthesis problem as finding an optimal policy under a constraint that the resultant product Markov chain (from the policy) and the DFA (from the safety property) has probability $\leq p_1$ of violating the safety property in finite horizon $H$. The authors make use of the CMDP formulation to separate the reward and safety considerations by treating the safety property as a cumulative constraint in the CMDP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, this is a very strong submission and very clearly written. The authors present their problem statement precisely and also compare against many other related settings. This is challenging to do in safe RL since it is such a wide field with many parallel approaches, but I feel the authors did a commendable job here, especially in demonstrating how related and alternative formulations can be represented in their setting. 

This is a very interesting combination of shielding from temporal logic specifications with a CMDP formulation. Especially, as the framework allows for regular safety properties instead of just invariant properties, it allows for quite general specifications. 

The authors also do a good job in showing the generality of the work as it pertains to levels of model knowledge.

Weaknesses:
The main weakness I can see in this paper is perhaps a lack of experiments in settings with more complex models and safety specifications. Especially settings in which safety and optimality of the reward are in conflict, it would be interesting to see how the agent is able to achieve a trade off. 

Another consideration is perhaps motivation of using a CMDP as a framework for safe RL. I don't see this as well motivated in the paper. There are several approaches to safe RL that use MDPs and are able to use probabilistic model checking type techniques to give guarantees for cost thresholds.

Limitations:
There is some discussion of limitations in the section before the conclusion. I agree with the authors on the downsides of separating reward and safety.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a new safe RL approach, building on safety shields.
The idea is to leverage model-checking techniques during the RL training to block actions that are identified as unsafe in the shield and use a learned backup policy if this is the case. In contrast to previous approaches, the ""meta-algorithm"" presented does not require an a-priori known model of the safety aspects of the environment. The approach comes with theoretical guarantees on the satisfaction of finite-horizon PCTL specifications and the evaluation assesses the potential of the proposed algorithm.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I'd first like to pinpoint that the paper is quite clear and very well written.

The proposed approach has several key advantages. First, it doesn't necessarily require providing a shield beforehand, in contrast to previous work. Second, the properties the agent needs to enforce during training are specified via PCTL, a well-established specification formalism that is not prone to exponential blow-up in the size of the automaton for translating the formula (as is the case for LTL). Third, the approach comes with guarantees on the shield: (i) when the hyper-parameters of the optimization procedure are well-chose, one can bound the probability of failure of the system from the initial state, (ii) the optimal policy found under the PCTL constraint is ensured to be a feasible policy for standard constrained MDP objectives. Finally, the authors discuss and provide guarantees under different assumptions, namely, the access to a model of the safety-relevant aspect of the environment (as in previous work), under a black-box model, and when one has access to an approximate model such that the total variation between the true and approximate transition probabilities is bounded. Additional statistical guarantees are provided for these last two assumptions. 

Experiments successfully highlight the potential of the approach.

Weaknesses:
**Beyond tabular settings.**
The main concern I have with this paper is the fact that the guarantees seem to solely hold in the tabular setting, i.e., when the state-action space is finite and tractable. Notably, in the second round of experiments, the authors use Dreamer-v3 to learn an approximate model of the environment. Although the resulting method seems to outperform constrained RL methods, the theoretical guarantees do not hold in practice. 

**On the assumptions.**
Assumption 5.2 is confusing. Indeed, when reading it for the first time, I thought one needs to have access to a *generative model*, i.e., a black box model of $\mathcal{P}$ that can be requested at any time, under any state and action, akin to the setting of [1, 2]. Specifically, this would mean that one doesn't necessarily need to sequentially execute the environment to obtain samples (as this is the case in RL), but, at any time, for any given state-action pair $(s, a)$, one could request the model to obtain a finite number of samples from $\mathcal{P}(\cdot \mid s, a)$. Note that this is not compliant with RL. When reading further, it seems that Monte-Carlo model checking only needs to produce episodes, which is fully compliant with RL. Thus, the distinction is really important here.

On another point, Assumption 5.3 looks rather restrictive. How to ensure that the approximate model learned through Algorithm 1 (line 300) yields a bounded total variation? This should be discussed in the main text. Moreover, the linked guarantees (Proposition 5.5) are not evaluated in the experiments. It could be interesting to have an example of how the statistical guarantees can be applied in practice.

[1] Michael J. Kearns, Yishay Mansour, Andrew Y. Ng: A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes. Mach. Learn. 49(2-3): 193-208 (2002)\
[2] Yujia Jin, Aaron Sidford: Towards Tight Bounds on the Sample Complexity of Average-reward MDPs. ICML 2021: 5055-5064

Limitations:
Apart from the points raised above, the limitations of the work have been successfully addressed in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies RL with 'regular' safety properties. The constraint of safe RL is based on the satisfaction of a logic formula in probability. The action from the 'backup' policy will proactively override the potentially unsafe action from RL to ensure/optimize safety, a typical shielding mechanism in formal safe control methods. The authors demonstrated the effectiveness of their approach against CMDP and regular RL (Q-learning) in two examples.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The approach is sound, as a typical shielding method, should work well in a safe RL setting, in 2 examples shown in the paper. 
The problem studied in this paper is important.

Weaknesses:
1. Novelty. Novelty is my biggest concern for this paper. First, Problem 4.1 has already been discussed and solved in [1][2]. Second, the shielding approach is nothing new, a very standard way in formal methods. You can even trace back to simplex architecture with an advanced controller with safety back control. Third, the model-checking approach of this paper is not novel. 
2. Significance. The experiments are weak with only 2 baselines (1 as original RL, 1 as CMDP) on 2 simple examples.

[1] Wang, Yixuan, et al. ""Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments."" International Conference on Machine Learning. PMLR, 2023.
[2] Wachi, Akifumi, et al. ""Safe exploration in reinforcement learning: A generalized formulation and algorithms."" Advances in Neural Information Processing Systems 36 (2024).

You may want to extend the related works part including two papers above and more.

Limitations:
The reviewer would like to know the limitations discussion by the authors in the rebuttal phase.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an approach to online shielding for reinforcement learning agents. Namely, safety is formulated in probabilistic temporal logic with a parametric threshold as an indicator for reachability of the goal state. The proposed algorithm checks the reachability probability threshold in each state of the environment and raises a warning when the threshold is violated. A pre-trained backup policy is then proposed to be deployed which overrides the action of the agent. The approach is evaluated on tabular and visual RL benchmarks.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper addresses an interesting and valuable problem. 

Evaluation includes visual RL benchmarks, which are interesting to provide online safety for.

Weaknesses:
The paper gives a lot of choice to the reader to compose a problem setting of their interest. It does not, however, provide precise enough approach description for each of them. This makes the contributions blurred. Described problem settings have been extensively studied before and the proposed approach does not significantly improve on them. It is also claimed that the approach can be used both during training and deployment, it is, however, not clear if the authors formulate these as two distinct settings and evaluate separately or not. In the latter case, it would be a dangerous simplification. Figures in the evaluation section are not readable.

Presentation: The presentation suffers from imprecise narrative leaving multiple questions until the evaluation section. Assumptions are introduced twice and it is not clear what exact problem the authors propose to address, or to what exact problem setting it generalizes. The use of ""etc."" and ""some other"" give the impression that more problems can be addressed than presented in the evaluation.

Minor:
- p.7: ""if need be we""
- ""don't"" --> do not

Limitations:
The approach is of limited novelty and employs existing components. Technical details in terms of the exact problem setting and guarantees are insufficient to judge the contribution. The approach is not yet placed in the larger context of online safety for RL agents, which would require more precise problem formulation and discussion of limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses safety and constraint compliance in deploying reinforcement learning (RL) systems. Such issues have triggered a vast body of research in the area of safe RL over the last few years. The paper introduces a safe RL framework for so-called regular safety properties, focusing on satisfying these properties with high probability. The paper compares and places this setup to common constrained Markov decision processes (CMDP) settings and presents a meta-algorithm with provable safety guarantees to prevent violations of regular safety properties during training and deployment. The approach is evaluated in both tabular and deep RL settings.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Safe RL is of utmost importance in order to enable RL agents to be successfully deployed in the real world. This paper tackles an important problem: How to ensure safety if the safety criteria go beyond standard reward or simple reachability features. The motivation of the paper is well done, and the placement in the literature is mostly complete and extensive.

Weaknesses:
While this paper addresses an important research question, I feel it is not ready for publication at a major ML conference. I will first summarize the issues that I see and then elaborate in more detail.

1. The claims made in the intro are not properly met. This concerns particularly the claim that the approaches are most permissive in terms of prior knowledge. 

2. The evaluation is not sufficient. The experiments are very sparse and miss important information to assess the quality of the research.

3. The paper is packed with formal definitions, most of them being standard in the formal methods community. Conversely, little space is spent on describing the methods in detail and providing a proper evaluation.

I overall feel that the paper aims too high and wants to solve all kinds of aspects in model-based safe RL with shields. It would have been better to pick certain key aspects more clearly. As an example, the paper claims to 
- not need a model
- to learn a model
- to use most of the available model checking paradigms (and even introduce them)
- evaluate with tabular and deep RL

Many of these aspects are then not properly discussed. 

I will now comment on the previously listed weaknesses.

1. In the introduction, the authors claim to operate in a most permissive setting where environment dynamics are not known. Yet, then, they introduce three model-checking paradigms, which are essentially standard numerical model checking, Monte Carlo-based statistical model checking, or model checking with approximate models. In particular, the last version is shady. What is a guarantee for a learned model? How can a claim be made for a most permissive setting but still obtain hard guarantees when, as a consequence, all guarantees rely effectively on statistics or a learned model? Other approaches are simply very clear about their assumptions, such as knowing a model. The model learning procedure is standard, and if the whole 'permissiveness' of the setting depends on the fact that a model can learned, I do not see a novel approach here. 

Very importantly: While a model is learned, no safety guarantees can be given, defying the notion of shielded RL. This aspect is not discussed in the paper. A step further, even the safe fallback policy that is explained in Section 6 relies on learning. 

2. The evaluation only considers a few environments and compares a simple tabular and deel RL agent. What I would have liked to see is how the different assumptions and model checking paradigms affect the learning, the safety, the performance. I believe, with a thorough evaluation in this direction, the paper would be much stronger. If we can believe that it is feasible to learn a model, then I would like to see a comparison between the strong assumption of knowing the model, and having learned a model, and what the effects on safety guarantees are. 

3. The contributions only start at page 5, and are then still interleaved with standard definitions. It is important to have a paper safe-contained, but, as an example, why is it necessary to introduce both LTL, PCTL, DFA in detail? And then, the evaluation even uses PCTL^*. To me this seems as if the space had been filled up with long definitions, while it should have been used more on the contributions of the paper. 

Generally, I feel the paper follows a great direction, especially investigating Shielded RL with a learned model. With a stronger evaluation and emphasis of this part, the research and contribution could be much improved in my opinion. 

Minor comments:

- proposition 4.2 is obvious and seems like an overformal statement of a simple fact. 

- Def 4.3. Make clear that this is reward engineering

- Non-Markovian cost: Compare to reward machines

- The tradeoff between safety and exploration has (for instance) been investigated in 

Carr et al.: Safe Reinforcement Learning via Shielding under Partial Observability. AAAI 2023.

- Learning the model, proposition 5.5: I think the topology (graph) of the model needs to be known to estimate the probabilities.

Limitations:
The limitations of the work in terms of assumptions and their real-world relation are not properly explained in the paper.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an online shielding approach for safe RL that does not assume prior knowledge of environment dynamics and utilizes finite-horizon model checking with learned approximations of the environment dynamics. It specifically focuses on RL with regular safety properties provided as a PCTL formula. The authors present a framework that dynamically identifies unsafe actions and deploys a safe backup policy when necessary. The main technical contributions of the paper are:

- Definition of a constrained RL problem based on regular safety properties.
- Presentation of model checking algorithms to verify finite-horizon satisfaction probability.
- Development of sample complexity results for statistical model checking procedures.

The novelty of the paper lies in its approach to reinforcement learning with regular safety properties without requiring prior knowledge of environment dynamics. Unlike traditional shielding approaches that need full environment models or simulators, this framework uses learned approximations and finite-horizon model checking. The authors represent the synthesis problem as finding an optimal policy under a constraint that the resultant product Markov chain (from the policy) and the DFA (from the safety property) has probability $\leq p_1$ of violating the safety property in finite horizon $H$. The authors make use of the CMDP formulation to separate the reward and safety considerations by treating the safety property as a cumulative constraint in the CMDP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, this is a very strong submission and very clearly written. The authors present their problem statement precisely and also compare against many other related settings. This is challenging to do in safe RL since it is such a wide field with many parallel approaches, but I feel the authors did a commendable job here, especially in demonstrating how related and alternative formulations can be represented in their setting. 

This is a very interesting combination of shielding from temporal logic specifications with a CMDP formulation. Especially, as the framework allows for regular safety properties instead of just invariant properties, it allows for quite general specifications. 

The authors also do a good job in showing the generality of the work as it pertains to levels of model knowledge.

Weaknesses:
The main weakness I can see in this paper is perhaps a lack of experiments in settings with more complex models and safety specifications. Especially settings in which safety and optimality of the reward are in conflict, it would be interesting to see how the agent is able to achieve a trade off. 

Another consideration is perhaps motivation of using a CMDP as a framework for safe RL. I don't see this as well motivated in the paper. There are several approaches to safe RL that use MDPs and are able to use probabilistic model checking type techniques to give guarantees for cost thresholds.

Limitations:
There is some discussion of limitations in the section before the conclusion. I agree with the authors on the downsides of separating reward and safety.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a new safe RL approach, building on safety shields.
The idea is to leverage model-checking techniques during the RL training to block actions that are identified as unsafe in the shield and use a learned backup policy if this is the case. In contrast to previous approaches, the ""meta-algorithm"" presented does not require an a-priori known model of the safety aspects of the environment. The approach comes with theoretical guarantees on the satisfaction of finite-horizon PCTL specifications and the evaluation assesses the potential of the proposed algorithm.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I'd first like to pinpoint that the paper is quite clear and very well written.

The proposed approach has several key advantages. First, it doesn't necessarily require providing a shield beforehand, in contrast to previous work. Second, the properties the agent needs to enforce during training are specified via PCTL, a well-established specification formalism that is not prone to exponential blow-up in the size of the automaton for translating the formula (as is the case for LTL). Third, the approach comes with guarantees on the shield: (i) when the hyper-parameters of the optimization procedure are well-chose, one can bound the probability of failure of the system from the initial state, (ii) the optimal policy found under the PCTL constraint is ensured to be a feasible policy for standard constrained MDP objectives. Finally, the authors discuss and provide guarantees under different assumptions, namely, the access to a model of the safety-relevant aspect of the environment (as in previous work), under a black-box model, and when one has access to an approximate model such that the total variation between the true and approximate transition probabilities is bounded. Additional statistical guarantees are provided for these last two assumptions. 

Experiments successfully highlight the potential of the approach.

Weaknesses:
**Beyond tabular settings.**
The main concern I have with this paper is the fact that the guarantees seem to solely hold in the tabular setting, i.e., when the state-action space is finite and tractable. Notably, in the second round of experiments, the authors use Dreamer-v3 to learn an approximate model of the environment. Although the resulting method seems to outperform constrained RL methods, the theoretical guarantees do not hold in practice. 

**On the assumptions.**
Assumption 5.2 is confusing. Indeed, when reading it for the first time, I thought one needs to have access to a *generative model*, i.e., a black box model of $\mathcal{P}$ that can be requested at any time, under any state and action, akin to the setting of [1, 2]. Specifically, this would mean that one doesn't necessarily need to sequentially execute the environment to obtain samples (as this is the case in RL), but, at any time, for any given state-action pair $(s, a)$, one could request the model to obtain a finite number of samples from $\mathcal{P}(\cdot \mid s, a)$. Note that this is not compliant with RL. When reading further, it seems that Monte-Carlo model checking only needs to produce episodes, which is fully compliant with RL. Thus, the distinction is really important here.

On another point, Assumption 5.3 looks rather restrictive. How to ensure that the approximate model learned through Algorithm 1 (line 300) yields a bounded total variation? This should be discussed in the main text. Moreover, the linked guarantees (Proposition 5.5) are not evaluated in the experiments. It could be interesting to have an example of how the statistical guarantees can be applied in practice.

[1] Michael J. Kearns, Yishay Mansour, Andrew Y. Ng: A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes. Mach. Learn. 49(2-3): 193-208 (2002)\
[2] Yujia Jin, Aaron Sidford: Towards Tight Bounds on the Sample Complexity of Average-reward MDPs. ICML 2021: 5055-5064

Limitations:
Apart from the points raised above, the limitations of the work have been successfully addressed in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies RL with 'regular' safety properties. The constraint of safe RL is based on the satisfaction of a logic formula in probability. The action from the 'backup' policy will proactively override the potentially unsafe action from RL to ensure/optimize safety, a typical shielding mechanism in formal safe control methods. The authors demonstrated the effectiveness of their approach against CMDP and regular RL (Q-learning) in two examples.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The approach is sound, as a typical shielding method, should work well in a safe RL setting, in 2 examples shown in the paper. 
The problem studied in this paper is important.

Weaknesses:
1. Novelty. Novelty is my biggest concern for this paper. First, Problem 4.1 has already been discussed and solved in [1][2]. Second, the shielding approach is nothing new, a very standard way in formal methods. You can even trace back to simplex architecture with an advanced controller with safety back control. Third, the model-checking approach of this paper is not novel. 
2. Significance. The experiments are weak with only 2 baselines (1 as original RL, 1 as CMDP) on 2 simple examples.

[1] Wang, Yixuan, et al. ""Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments."" International Conference on Machine Learning. PMLR, 2023.
[2] Wachi, Akifumi, et al. ""Safe exploration in reinforcement learning: A generalized formulation and algorithms."" Advances in Neural Information Processing Systems 36 (2024).

You may want to extend the related works part including two papers above and more.

Limitations:
The reviewer would like to know the limitations discussion by the authors in the rebuttal phase.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an approach to online shielding for reinforcement learning agents. Namely, safety is formulated in probabilistic temporal logic with a parametric threshold as an indicator for reachability of the goal state. The proposed algorithm checks the reachability probability threshold in each state of the environment and raises a warning when the threshold is violated. A pre-trained backup policy is then proposed to be deployed which overrides the action of the agent. The approach is evaluated on tabular and visual RL benchmarks.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper addresses an interesting and valuable problem. 

Evaluation includes visual RL benchmarks, which are interesting to provide online safety for.

Weaknesses:
The paper gives a lot of choice to the reader to compose a problem setting of their interest. It does not, however, provide precise enough approach description for each of them. This makes the contributions blurred. Described problem settings have been extensively studied before and the proposed approach does not significantly improve on them. It is also claimed that the approach can be used both during training and deployment, it is, however, not clear if the authors formulate these as two distinct settings and evaluate separately or not. In the latter case, it would be a dangerous simplification. Figures in the evaluation section are not readable.

Presentation: The presentation suffers from imprecise narrative leaving multiple questions until the evaluation section. Assumptions are introduced twice and it is not clear what exact problem the authors propose to address, or to what exact problem setting it generalizes. The use of ""etc."" and ""some other"" give the impression that more problems can be addressed than presented in the evaluation.

Minor:
- p.7: ""if need be we""
- ""don't"" --> do not

Limitations:
The approach is of limited novelty and employs existing components. Technical details in terms of the exact problem setting and guarantees are insufficient to judge the contribution. The approach is not yet placed in the larger context of online safety for RL agents, which would require more precise problem formulation and discussion of limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses safety and constraint compliance in deploying reinforcement learning (RL) systems. Such issues have triggered a vast body of research in the area of safe RL over the last few years. The paper introduces a safe RL framework for so-called regular safety properties, focusing on satisfying these properties with high probability. The paper compares and places this setup to common constrained Markov decision processes (CMDP) settings and presents a meta-algorithm with provable safety guarantees to prevent violations of regular safety properties during training and deployment. The approach is evaluated in both tabular and deep RL settings.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Safe RL is of utmost importance in order to enable RL agents to be successfully deployed in the real world. This paper tackles an important problem: How to ensure safety if the safety criteria go beyond standard reward or simple reachability features. The motivation of the paper is well done, and the placement in the literature is mostly complete and extensive.

Weaknesses:
While this paper addresses an important research question, I feel it is not ready for publication at a major ML conference. I will first summarize the issues that I see and then elaborate in more detail.

1. The claims made in the intro are not properly met. This concerns particularly the claim that the approaches are most permissive in terms of prior knowledge. 

2. The evaluation is not sufficient. The experiments are very sparse and miss important information to assess the quality of the research.

3. The paper is packed with formal definitions, most of them being standard in the formal methods community. Conversely, little space is spent on describing the methods in detail and providing a proper evaluation.

I overall feel that the paper aims too high and wants to solve all kinds of aspects in model-based safe RL with shields. It would have been better to pick certain key aspects more clearly. As an example, the paper claims to 
- not need a model
- to learn a model
- to use most of the available model checking paradigms (and even introduce them)
- evaluate with tabular and deep RL

Many of these aspects are then not properly discussed. 

I will now comment on the previously listed weaknesses.

1. In the introduction, the authors claim to operate in a most permissive setting where environment dynamics are not known. Yet, then, they introduce three model-checking paradigms, which are essentially standard numerical model checking, Monte Carlo-based statistical model checking, or model checking with approximate models. In particular, the last version is shady. What is a guarantee for a learned model? How can a claim be made for a most permissive setting but still obtain hard guarantees when, as a consequence, all guarantees rely effectively on statistics or a learned model? Other approaches are simply very clear about their assumptions, such as knowing a model. The model learning procedure is standard, and if the whole 'permissiveness' of the setting depends on the fact that a model can learned, I do not see a novel approach here. 

Very importantly: While a model is learned, no safety guarantees can be given, defying the notion of shielded RL. This aspect is not discussed in the paper. A step further, even the safe fallback policy that is explained in Section 6 relies on learning. 

2. The evaluation only considers a few environments and compares a simple tabular and deel RL agent. What I would have liked to see is how the different assumptions and model checking paradigms affect the learning, the safety, the performance. I believe, with a thorough evaluation in this direction, the paper would be much stronger. If we can believe that it is feasible to learn a model, then I would like to see a comparison between the strong assumption of knowing the model, and having learned a model, and what the effects on safety guarantees are. 

3. The contributions only start at page 5, and are then still interleaved with standard definitions. It is important to have a paper safe-contained, but, as an example, why is it necessary to introduce both LTL, PCTL, DFA in detail? And then, the evaluation even uses PCTL^*. To me this seems as if the space had been filled up with long definitions, while it should have been used more on the contributions of the paper. 

Generally, I feel the paper follows a great direction, especially investigating Shielded RL with a learned model. With a stronger evaluation and emphasis of this part, the research and contribution could be much improved in my opinion. 

Minor comments:

- proposition 4.2 is obvious and seems like an overformal statement of a simple fact. 

- Def 4.3. Make clear that this is reward engineering

- Non-Markovian cost: Compare to reward machines

- The tradeoff between safety and exploration has (for instance) been investigated in 

Carr et al.: Safe Reinforcement Learning via Shielding under Partial Observability. AAAI 2023.

- Learning the model, proposition 5.5: I think the topology (graph) of the model needs to be known to estimate the probabilities.

Limitations:
The limitations of the work in terms of assumptions and their real-world relation are not properly explained in the paper.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
PKEmH9ZJfw;"REVIEW 
Summary:
This paper studies weakly supervised causal representation learning with soft interventions.
It assumes that pairs of observations are provided that differ by a soft intervention on one variable in the latent space.
Additionally, the intervention target (and the total number of latent variables) are given.
Identifiability up to element-wise transformations assuming linear mixing and known intervention targets (among other assumptions) is shown.
Experiments on synthetic data compare the proposed method to ILCM, beta-VAE and D-VAE.
An improved D and C score in the DCI framework is shown.
An experiment on semi-synthetic image data is shown.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method extends previous work on weakly supervised CRL and generalizes it from hard to soft interventions, which is an important step for bringing CRL closer to real-world applications, since CRL methods often suffer from restrictive assumptions.
- Also, I appreciate the effort of testing this CRL approach to datasets which are more similar to real-world image datasets.

Weaknesses:
- The mathematical notation is poorly defined and difficult to follow: objects aren't well-defined and notation inconsistently or unusually used. See questions below.
- L136-137: ""a diffeomorphic solution function [...] deterministically maps a value for exogenous variable [...] to a value for causal variable [...]"" That's incorrect. There can be such a map from the set of all exogenous to all endogenous, but not per variable. Consider $X:= U_X; Y:= X+U_Y$, where $U_X$, $U_Y$ are exogenous. In general, there is no deterministic map from $U_Y$ to $Y$.
- Even though the DCI framework for evaluation of the learned representation is cited, only two of the metrics (D and C) are used, whereas Informativeness (I) is omitted without comment. That seems suspicious to me.
- The synthetic experiments test a rather simple setting on few data generating processes. The observation dimension is 4 (same as latent dimension) and the mixing is linear (as far as I can tell). We have seen in previous work (e.g. [2]) that the complexity of the mixing function is a crucial element in the recoverability of the latents. For example, the more nonlinear the function, the harder it is to recover latents. Furthermore, only 10 data generating processes are tested. That seems quite limited, given that the dimensions of the data is so small.


**Minor:**

- L133: ""decoder function"" is a bit of an odd choice for something that relates to the ground-truth data generating process. Usually, this is a part of a model. ""Mixing function"" would be more appropriate.
- There is a period missing in L251.
- L345: There is a follow-up version of DCI that takes more aspects of the learned representations into account [1]. It could be considered for the next iteration if time permits.
- L363: ""As mentioned in [27, 25], causal graphs are sparse and in the G5 case, where the graph is fully connected, the proposed method cannot identify the causal variables well."" The two references **do not say that causal graphs are sparse**. You may refer to the sparse mechanism shift hypothesis, which says that changes between environments are assumed to stem from changes in few mechanisms of the causal graph. This is a statement about sparse changes, not sparse graphs.

Limitations:
- As far as I can tell, no latents were uncovered (theoretically or experimentally) for either nonlinear mixing or unobserved intervention targets. Therefore, it seems to me, it should be mentioned that the results are for the linear case and weakly supervised. The title and abstract make the impression as if the more general CRL problem with paired observations is solved.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new approach **ICRL-SM** that performs implicit causal representation learning (mapping from noise to latent variables) by using causal mechanism switch variable to model the soft intervention effects.

&nbsp;

### References
[1] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning with interventions. arXiv:2301.08230, 2023.

[2] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning: Linear and general transformations. arXiv:2402.00849, 2024.

[3] Julius von Kügelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Kekic ́, Elias Bareinboim, David M Blei, and Bernhard Schölkopf. Nonparametric identifiability of causal rep- resentations from unknown interventions. In Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2023.

[4] J. Brehmer, P. De Haan, P. Lippe, and T. Cohen. Weakly supervised causal representation learning. In Advances in Neural Information Processing Systems, volume 35, pages 38319– 38331, 2022.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of implicitly modeling causal effects using switch variables is very interesting. The experiments have shown promising performance on synthetic and high-dimensional image datasets compared to several baselines.

Weaknesses:
1. My most significant concern is that this paper is generally not well written and, thus, pretty hard to follow. For example, the calligraphic letter $\mathcal{Z}$ was used throughout to denote both causal variables (e.g., line 137) and its domain (e.g., line 139).
2. The assumption of a diffeomorphic causal mechanism is pretty strong. I am aware that [4] made a similar assumption. Yet, it is not a very common assumption for latent causal models in other causal representation learning literature (e.g., [1, 2, 3]).
3. Line 278: The Gaussianity assumption of the causal and exogenous variables might be hard to satisfy in realistic settings.

Limitations:
The main paper did not discuss limitations. The authors pointed them out in the checklist, but I would have appreciated it more if they had adequately addressed the limitations in the main paper.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel approach for learning implicit causal representations through switchable mechanisms, specifically designed to handle soft interventions which are more realistic but challenging compared to hard interventions. The authors introduce a causal mechanism switch variable to model the subtle effects of soft interventions and establish the identifiability of causal models under certain assumptions. Their proposed method, ICRL-SM, demonstrates improved performance in learning identifiable causal representations over baseline methods through experiments on synthetic and real-world datasets. The paper contributes a new perspective to causal representation learning with potential applications in various domains where understanding causal relationships from observational and interventional data is crucial.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A standout feature of this paper is the introduction of a switchable mechanism to model the nuanced effects of soft interventions. Traditional causal inference methods often focus on hard interventions, which are impractical in many real-world scenarios due to the need for stringent control. The paper's approach to incorporate soft interventions through a causal mechanism switch variable is groundbreaking. It allows the model to adapt to changes in causal relationships post-intervention, providing a more realistic and flexible framework for causal representation learning.
2. This paper proposes Augmented Implicit Causal Models, which is an innovative concept that extends the scope of implicit causal representation learning. By integrating the causal mechanism switch variable into the model's solution functions, AICMs can capture the intrinsic characteristics of each causal variable while accounting for intervention effects. This approach sidesteps the need for explicit parameterization of the causal graph, which is a complex and often intractable task. The innovation here lies in the model's ability to implicitly learn the causal structure while directly modeling the effects of interventions.

Weaknesses:
1. The method's effectiveness relies on several key assumptions, including knowing the targets for intervention, interventions being atomic (indivisible), and variables following a multivariate normal distribution. These assumptions might not be realistic in many real-world settings where interventions can be complex, and data may not be normally distributed.
2. While the paper's experiments on synthetic and specific real-world (Causal-Triplet) datasets demonstrate the method's potential, it's unclear how well it generalizes to other data types or different domains.

Limitations:
No

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work is in causal representation learning that utilizes interventional data. It involves two types of common interventions: hard interventions and soft interventions. It’s known that soft intervention is more general since it covers hard intervention. But it is also more challenging since parental relations remain. This work proposed identifiability results using a causal mechanism switch variable designed to change between different causal mechanisms by component-wise transformations. Adequate experiments were provided to verify they claim.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Pros:

1. Overall, the writing is clear and with good context.
2. Adequate experiments were provided, including real-data experiments. Also, they offered good observations of the comparison with baselines. This is a big plus to their contribution.

Weaknesses:
Cons:

1. It's unclear what the unique theoretical contribution is to this switch variable since both types of interventions were proposed before. Could the author clarify the theoretical contribution? Especially regarding the original technical hardness compared to [1] and also [38].

2. In the synthetic experiment, it seems that you use the linear decoder, which loses the purpose of training a neural network to handle non-linear issues. Did you try non-linear functions and see how it goes? Besides the switch variable utilizing interventional data, what is the difference between these settings and those classical linear unmixing methods (like linear ICA or its other assumption variants)?

Some minor issues/recommendations:

Line 161, the $e_i$ is not defined, if you mean causal variable, then you already have defined it as $Z_i$.

Line 187, the ‘\s’ is not defined before but later in line 195.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies weakly supervised causal representation learning with soft interventions.
It assumes that pairs of observations are provided that differ by a soft intervention on one variable in the latent space.
Additionally, the intervention target (and the total number of latent variables) are given.
Identifiability up to element-wise transformations assuming linear mixing and known intervention targets (among other assumptions) is shown.
Experiments on synthetic data compare the proposed method to ILCM, beta-VAE and D-VAE.
An improved D and C score in the DCI framework is shown.
An experiment on semi-synthetic image data is shown.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method extends previous work on weakly supervised CRL and generalizes it from hard to soft interventions, which is an important step for bringing CRL closer to real-world applications, since CRL methods often suffer from restrictive assumptions.
- Also, I appreciate the effort of testing this CRL approach to datasets which are more similar to real-world image datasets.

Weaknesses:
- The mathematical notation is poorly defined and difficult to follow: objects aren't well-defined and notation inconsistently or unusually used. See questions below.
- L136-137: ""a diffeomorphic solution function [...] deterministically maps a value for exogenous variable [...] to a value for causal variable [...]"" That's incorrect. There can be such a map from the set of all exogenous to all endogenous, but not per variable. Consider $X:= U_X; Y:= X+U_Y$, where $U_X$, $U_Y$ are exogenous. In general, there is no deterministic map from $U_Y$ to $Y$.
- Even though the DCI framework for evaluation of the learned representation is cited, only two of the metrics (D and C) are used, whereas Informativeness (I) is omitted without comment. That seems suspicious to me.
- The synthetic experiments test a rather simple setting on few data generating processes. The observation dimension is 4 (same as latent dimension) and the mixing is linear (as far as I can tell). We have seen in previous work (e.g. [2]) that the complexity of the mixing function is a crucial element in the recoverability of the latents. For example, the more nonlinear the function, the harder it is to recover latents. Furthermore, only 10 data generating processes are tested. That seems quite limited, given that the dimensions of the data is so small.


**Minor:**

- L133: ""decoder function"" is a bit of an odd choice for something that relates to the ground-truth data generating process. Usually, this is a part of a model. ""Mixing function"" would be more appropriate.
- There is a period missing in L251.
- L345: There is a follow-up version of DCI that takes more aspects of the learned representations into account [1]. It could be considered for the next iteration if time permits.
- L363: ""As mentioned in [27, 25], causal graphs are sparse and in the G5 case, where the graph is fully connected, the proposed method cannot identify the causal variables well."" The two references **do not say that causal graphs are sparse**. You may refer to the sparse mechanism shift hypothesis, which says that changes between environments are assumed to stem from changes in few mechanisms of the causal graph. This is a statement about sparse changes, not sparse graphs.

Limitations:
- As far as I can tell, no latents were uncovered (theoretically or experimentally) for either nonlinear mixing or unobserved intervention targets. Therefore, it seems to me, it should be mentioned that the results are for the linear case and weakly supervised. The title and abstract make the impression as if the more general CRL problem with paired observations is solved.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new approach **ICRL-SM** that performs implicit causal representation learning (mapping from noise to latent variables) by using causal mechanism switch variable to model the soft intervention effects.

&nbsp;

### References
[1] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning with interventions. arXiv:2301.08230, 2023.

[2] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning: Linear and general transformations. arXiv:2402.00849, 2024.

[3] Julius von Kügelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Kekic ́, Elias Bareinboim, David M Blei, and Bernhard Schölkopf. Nonparametric identifiability of causal rep- resentations from unknown interventions. In Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2023.

[4] J. Brehmer, P. De Haan, P. Lippe, and T. Cohen. Weakly supervised causal representation learning. In Advances in Neural Information Processing Systems, volume 35, pages 38319– 38331, 2022.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of implicitly modeling causal effects using switch variables is very interesting. The experiments have shown promising performance on synthetic and high-dimensional image datasets compared to several baselines.

Weaknesses:
1. My most significant concern is that this paper is generally not well written and, thus, pretty hard to follow. For example, the calligraphic letter $\mathcal{Z}$ was used throughout to denote both causal variables (e.g., line 137) and its domain (e.g., line 139).
2. The assumption of a diffeomorphic causal mechanism is pretty strong. I am aware that [4] made a similar assumption. Yet, it is not a very common assumption for latent causal models in other causal representation learning literature (e.g., [1, 2, 3]).
3. Line 278: The Gaussianity assumption of the causal and exogenous variables might be hard to satisfy in realistic settings.

Limitations:
The main paper did not discuss limitations. The authors pointed them out in the checklist, but I would have appreciated it more if they had adequately addressed the limitations in the main paper.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel approach for learning implicit causal representations through switchable mechanisms, specifically designed to handle soft interventions which are more realistic but challenging compared to hard interventions. The authors introduce a causal mechanism switch variable to model the subtle effects of soft interventions and establish the identifiability of causal models under certain assumptions. Their proposed method, ICRL-SM, demonstrates improved performance in learning identifiable causal representations over baseline methods through experiments on synthetic and real-world datasets. The paper contributes a new perspective to causal representation learning with potential applications in various domains where understanding causal relationships from observational and interventional data is crucial.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A standout feature of this paper is the introduction of a switchable mechanism to model the nuanced effects of soft interventions. Traditional causal inference methods often focus on hard interventions, which are impractical in many real-world scenarios due to the need for stringent control. The paper's approach to incorporate soft interventions through a causal mechanism switch variable is groundbreaking. It allows the model to adapt to changes in causal relationships post-intervention, providing a more realistic and flexible framework for causal representation learning.
2. This paper proposes Augmented Implicit Causal Models, which is an innovative concept that extends the scope of implicit causal representation learning. By integrating the causal mechanism switch variable into the model's solution functions, AICMs can capture the intrinsic characteristics of each causal variable while accounting for intervention effects. This approach sidesteps the need for explicit parameterization of the causal graph, which is a complex and often intractable task. The innovation here lies in the model's ability to implicitly learn the causal structure while directly modeling the effects of interventions.

Weaknesses:
1. The method's effectiveness relies on several key assumptions, including knowing the targets for intervention, interventions being atomic (indivisible), and variables following a multivariate normal distribution. These assumptions might not be realistic in many real-world settings where interventions can be complex, and data may not be normally distributed.
2. While the paper's experiments on synthetic and specific real-world (Causal-Triplet) datasets demonstrate the method's potential, it's unclear how well it generalizes to other data types or different domains.

Limitations:
No

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work is in causal representation learning that utilizes interventional data. It involves two types of common interventions: hard interventions and soft interventions. It’s known that soft intervention is more general since it covers hard intervention. But it is also more challenging since parental relations remain. This work proposed identifiability results using a causal mechanism switch variable designed to change between different causal mechanisms by component-wise transformations. Adequate experiments were provided to verify they claim.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Pros:

1. Overall, the writing is clear and with good context.
2. Adequate experiments were provided, including real-data experiments. Also, they offered good observations of the comparison with baselines. This is a big plus to their contribution.

Weaknesses:
Cons:

1. It's unclear what the unique theoretical contribution is to this switch variable since both types of interventions were proposed before. Could the author clarify the theoretical contribution? Especially regarding the original technical hardness compared to [1] and also [38].

2. In the synthetic experiment, it seems that you use the linear decoder, which loses the purpose of training a neural network to handle non-linear issues. Did you try non-linear functions and see how it goes? Besides the switch variable utilizing interventional data, what is the difference between these settings and those classical linear unmixing methods (like linear ICA or its other assumption variants)?

Some minor issues/recommendations:

Line 161, the $e_i$ is not defined, if you mean causal variable, then you already have defined it as $Z_i$.

Line 187, the ‘\s’ is not defined before but later in line 195.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
NhP8MTJzT5;"REVIEW 
Summary:
The paper introduces SyntheOcc, a framework utilizing diffusion models to synthesize photorealistic images for autonomous driving simulations. The proposed method addresses limitations in the existing 2D diffusion model to generate multi-view driving videos by integrating detailed 3D geometric data.
The authors effectively employ 3D semantic multi-plane images (MPIs) for precise geometric control, enhancing the realism and utility of generated images for training perception models. The paper also proposes re-weighting strategies to address the imbalance problem between foreground, background, and object categories. The experiments prove the effectiveness of the proposed MPI encoder and the reweighting strategies.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper introduces an innovative approach by incorporating 3D semantic Multi-Plane Images (MPIs) to capture both geometric and semantic details of a scene. This approach allows for the precise modeling of 3D environments in a 2D image synthesis context, enhancing the photorealism and depth accuracy of the generated images. 
- The design of the MPI encoder is very effective in handling the input conditions with a large number of channels while maintaining spatial consistency to the latent features of diffusion UNet.
- Additionally, SyntheOcc incorporates sophisticated reweighing strategies to address class imbalance and ensure focus on critical features. These include foreground enhancement, depth-aware reweighing, and class-balanced reweighing.
- The paper outlines a comprehensive set of evaluations to demonstrate the effectiveness of the proposed method. Qualitative evaluations visually demonstrate the photorealism and environmental accuracy of the generated images compared to real scenes from the nuScenes dataset. Quantitative analyses leverage metrics such as Frechet Inception Distance (FID) to measure image quality and evaluate perception model performance, offering solid empirical evidence of the framework's effectiveness. Ablation studies further dissect the impact of various components and design choices in the proposed method. Additional robustness tests are conducted to evaluate how changes in the MPI settings (like variations in depth or semantic labeling) affect the output quality and the training effectiveness of perception models.

Weaknesses:
The contributions for reweighing strategies seem to be minor improvements over 
existing methods (Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, and Dit-Yan 
Yeung. Integrating geometric control into text-to-image diffusion models for high-quality 
detection data generation via text prompt. arXiv preprint arXiv:2306.04607, 2023, Benjin Zhu, 
Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and sampling for point cloud 3d object detection (arXiv preprint arXiv:1908.09492), which limits the perceived novelty of the paper's contributions.

The paper focuses on scene editing capabilities, but there is a noticeable underrepresentation of object-level editing in the experiments. 

Magic drive’s data augmentation is evaluated on two perception tasks BEV segmentation and 3D object detection, with CVT (Zhou & Krahenbuhl , 2022) and BEVFusion (Liu et al., 2023a) as perception models, respectively. Hence, evaluations on same downstream tasks are encouraged for better comparisons to the state-of-the-art baseline.

The paper doesn't provide any evaluations comparing the re-weighing solution proposed by GeoDiffusion.

There are also several noticeable view inconsistencies, eg: Fig 14 - row 2 column 2-3 (clouds seem different), row 7 column 4-5 there is a mismatch in building structures, which are not discussed in the paper.

Limitations:
The authors acknowledge some key limitations in the proposed method. First, it relies heavily on existing data for generating scenes, which means it doesn’t create as much variety as it could. This limits how well it can train models to handle different driving conditions. The paper also struggles with complex scenes, like crowds, where it fails to accurately identify individual people. This is a big deal for autonomous driving, where accurate representations of the scene are crucial for predictions. The authors suggest that future improvements could include better methods for creating diverse scenes and making the model more capable of handling dynamic environments, which would help make the system more practical and effective for real-world applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces SytheOcc, a method that employs a diffusion model with 3D occupancy as conditions to generate street view images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Unlike previous methods that use box conditions, this paper proposes the use of 3D occupancy, resulting in finer geometric control ability.
2. The paper further suggests the use of 3D semantic multi-plane images to represent the 3D occupancy.
3. The text and figures are well-presented, and the provided examples are very promising.

Weaknesses:
1. The main concern is the inconsistency between views and frames. Despite using Cross-View and Cross-Frame Attention and 3D occupancy as conditions, the spatial and temporal consistency results are unsatisfactory (e.g., Fig 5 (b) and the video demos). This is not the expected outcome, as incorporating 3D occupancy as a consistent world representation should result in better spatial and temporal consistency. Additionally, it would be preferable to have metric results such as FVD for the temporal experiments.
2. In Table 1, why does SytheOcc-Aug show worse results for certain categories (e.g., bicycle, moto)?
3. Table 1 lacks experiments for ControlNet-Aug or ControlNet+depth-aug.
4. Some discussions regarding 3D occupancy as a 3D geometry condition:
   4.1. The field of view (FOV) for 3D occupancy is limited as it is generally generated using lidar, which leads to inconsistency issues for high-rise buildings when considering cameras of larger FOVs.
   4.2. The current annotation of 3D occupancy has limited category coverage. It would be beneficial to explore open-vocabulary approaches.
   4.3. When using only 2D semantic masks as conditions, the paper mentions the presence of ambiguity (i.e., Fig 6 a0). Can the use of instance-level semantic masks alleviate this problem?

Limitations:
The authors have provided a comprehensive list of limitations and future work of their paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper propose a new 3D semantic multi-plane images (MPIs) based image generation pipeline, which enables finer geometric control for 3D editing, dataset generation, and long-tailed scene generation. Through extensive experiments, the work demonstrates substantial advancement in generation quality and better alignment between condition and synthesized images.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The work explores a new 3D semantic Multi-Plane Images (MPIs) as a condition, which provides better spatial alignment compared with baselines and enables 3D editing.

2. The comparison results are comprehensive and demonstrate the effectiveness the proposed method, the ablation is relatively complete to validate the MPI Encoder and the reweighting strategy.

3. The paper is well-written, and the experimental results are presented clearly.

Weaknesses:
1. The MPI encoder, which is the major contribution, is not novel for me. Although the proposed 3D MPI enables finer control than BEVGen, but the diffusion model also operates on the 2D domain and generates each view and frame separately without strict geometry constraints.

2. The importance of reweighing is tricky and hard to tune, considering many hyperparameters. Are the m and n in Eq. 6 the same for different datasets?

3. It’s hard to decide how the method works without a supplementary video, I doubt the view-consistency of the generated video across frames and views.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors propose a new controllable diffusion-based image generation method named SyntheOcc, which takes an occupancy map as input and generates camera images. SyntheOcc enables the application of scene editing and long-tail corner case generation and shows a strong capability of data augmentation for autonomous driving systems.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Compared with previous controllable image generation methods for traffic scenarios like Panacea or MagicDrive, the occupancy map contains more 3D spatial information than the BEV layout.
2. The paper is well-organized and easy to follow.
3. The extensive experimental results demonstrate the effectiveness of the proposed data generation pipeline.

Weaknesses:
1. The control signal in Panacea or MagicDrive is BEV layout, which only contains lanes and foreground objects and is more easily acquired than occupancy. However, the SyntheOcc relies on sophisticated collected occupancy.

Limitations:
The proposed SyntheOcc faces challenges in real-world application scenarios. For instance, to generate planning-level long-tail corner cases, other methods like Panacea or MagicDrive simply require editing the object's trajectory. However, SyntheOcc demands not only inputting the background occupancy but also constructing a pseudo occupancy for the foreground object. This raises the question of whether using occupancy as a control signal is an advantage or a disadvantage.

The crux of the issue lies in the complexity of this method compared to alternatives. While other approaches need some adjustments to object trajectories, this technique necessitates providing comprehensive background and foreground occupancy data. This additional overhead prompts us to ponder whether occupancy-based control offers tangible benefits or introduces unnecessary complications.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces SyntheOcc, a framework utilizing diffusion models to synthesize photorealistic images for autonomous driving simulations. The proposed method addresses limitations in the existing 2D diffusion model to generate multi-view driving videos by integrating detailed 3D geometric data.
The authors effectively employ 3D semantic multi-plane images (MPIs) for precise geometric control, enhancing the realism and utility of generated images for training perception models. The paper also proposes re-weighting strategies to address the imbalance problem between foreground, background, and object categories. The experiments prove the effectiveness of the proposed MPI encoder and the reweighting strategies.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper introduces an innovative approach by incorporating 3D semantic Multi-Plane Images (MPIs) to capture both geometric and semantic details of a scene. This approach allows for the precise modeling of 3D environments in a 2D image synthesis context, enhancing the photorealism and depth accuracy of the generated images. 
- The design of the MPI encoder is very effective in handling the input conditions with a large number of channels while maintaining spatial consistency to the latent features of diffusion UNet.
- Additionally, SyntheOcc incorporates sophisticated reweighing strategies to address class imbalance and ensure focus on critical features. These include foreground enhancement, depth-aware reweighing, and class-balanced reweighing.
- The paper outlines a comprehensive set of evaluations to demonstrate the effectiveness of the proposed method. Qualitative evaluations visually demonstrate the photorealism and environmental accuracy of the generated images compared to real scenes from the nuScenes dataset. Quantitative analyses leverage metrics such as Frechet Inception Distance (FID) to measure image quality and evaluate perception model performance, offering solid empirical evidence of the framework's effectiveness. Ablation studies further dissect the impact of various components and design choices in the proposed method. Additional robustness tests are conducted to evaluate how changes in the MPI settings (like variations in depth or semantic labeling) affect the output quality and the training effectiveness of perception models.

Weaknesses:
The contributions for reweighing strategies seem to be minor improvements over 
existing methods (Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, and Dit-Yan 
Yeung. Integrating geometric control into text-to-image diffusion models for high-quality 
detection data generation via text prompt. arXiv preprint arXiv:2306.04607, 2023, Benjin Zhu, 
Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and sampling for point cloud 3d object detection (arXiv preprint arXiv:1908.09492), which limits the perceived novelty of the paper's contributions.

The paper focuses on scene editing capabilities, but there is a noticeable underrepresentation of object-level editing in the experiments. 

Magic drive’s data augmentation is evaluated on two perception tasks BEV segmentation and 3D object detection, with CVT (Zhou & Krahenbuhl , 2022) and BEVFusion (Liu et al., 2023a) as perception models, respectively. Hence, evaluations on same downstream tasks are encouraged for better comparisons to the state-of-the-art baseline.

The paper doesn't provide any evaluations comparing the re-weighing solution proposed by GeoDiffusion.

There are also several noticeable view inconsistencies, eg: Fig 14 - row 2 column 2-3 (clouds seem different), row 7 column 4-5 there is a mismatch in building structures, which are not discussed in the paper.

Limitations:
The authors acknowledge some key limitations in the proposed method. First, it relies heavily on existing data for generating scenes, which means it doesn’t create as much variety as it could. This limits how well it can train models to handle different driving conditions. The paper also struggles with complex scenes, like crowds, where it fails to accurately identify individual people. This is a big deal for autonomous driving, where accurate representations of the scene are crucial for predictions. The authors suggest that future improvements could include better methods for creating diverse scenes and making the model more capable of handling dynamic environments, which would help make the system more practical and effective for real-world applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces SytheOcc, a method that employs a diffusion model with 3D occupancy as conditions to generate street view images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Unlike previous methods that use box conditions, this paper proposes the use of 3D occupancy, resulting in finer geometric control ability.
2. The paper further suggests the use of 3D semantic multi-plane images to represent the 3D occupancy.
3. The text and figures are well-presented, and the provided examples are very promising.

Weaknesses:
1. The main concern is the inconsistency between views and frames. Despite using Cross-View and Cross-Frame Attention and 3D occupancy as conditions, the spatial and temporal consistency results are unsatisfactory (e.g., Fig 5 (b) and the video demos). This is not the expected outcome, as incorporating 3D occupancy as a consistent world representation should result in better spatial and temporal consistency. Additionally, it would be preferable to have metric results such as FVD for the temporal experiments.
2. In Table 1, why does SytheOcc-Aug show worse results for certain categories (e.g., bicycle, moto)?
3. Table 1 lacks experiments for ControlNet-Aug or ControlNet+depth-aug.
4. Some discussions regarding 3D occupancy as a 3D geometry condition:
   4.1. The field of view (FOV) for 3D occupancy is limited as it is generally generated using lidar, which leads to inconsistency issues for high-rise buildings when considering cameras of larger FOVs.
   4.2. The current annotation of 3D occupancy has limited category coverage. It would be beneficial to explore open-vocabulary approaches.
   4.3. When using only 2D semantic masks as conditions, the paper mentions the presence of ambiguity (i.e., Fig 6 a0). Can the use of instance-level semantic masks alleviate this problem?

Limitations:
The authors have provided a comprehensive list of limitations and future work of their paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper propose a new 3D semantic multi-plane images (MPIs) based image generation pipeline, which enables finer geometric control for 3D editing, dataset generation, and long-tailed scene generation. Through extensive experiments, the work demonstrates substantial advancement in generation quality and better alignment between condition and synthesized images.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The work explores a new 3D semantic Multi-Plane Images (MPIs) as a condition, which provides better spatial alignment compared with baselines and enables 3D editing.

2. The comparison results are comprehensive and demonstrate the effectiveness the proposed method, the ablation is relatively complete to validate the MPI Encoder and the reweighting strategy.

3. The paper is well-written, and the experimental results are presented clearly.

Weaknesses:
1. The MPI encoder, which is the major contribution, is not novel for me. Although the proposed 3D MPI enables finer control than BEVGen, but the diffusion model also operates on the 2D domain and generates each view and frame separately without strict geometry constraints.

2. The importance of reweighing is tricky and hard to tune, considering many hyperparameters. Are the m and n in Eq. 6 the same for different datasets?

3. It’s hard to decide how the method works without a supplementary video, I doubt the view-consistency of the generated video across frames and views.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors propose a new controllable diffusion-based image generation method named SyntheOcc, which takes an occupancy map as input and generates camera images. SyntheOcc enables the application of scene editing and long-tail corner case generation and shows a strong capability of data augmentation for autonomous driving systems.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Compared with previous controllable image generation methods for traffic scenarios like Panacea or MagicDrive, the occupancy map contains more 3D spatial information than the BEV layout.
2. The paper is well-organized and easy to follow.
3. The extensive experimental results demonstrate the effectiveness of the proposed data generation pipeline.

Weaknesses:
1. The control signal in Panacea or MagicDrive is BEV layout, which only contains lanes and foreground objects and is more easily acquired than occupancy. However, the SyntheOcc relies on sophisticated collected occupancy.

Limitations:
The proposed SyntheOcc faces challenges in real-world application scenarios. For instance, to generate planning-level long-tail corner cases, other methods like Panacea or MagicDrive simply require editing the object's trajectory. However, SyntheOcc demands not only inputting the background occupancy but also constructing a pseudo occupancy for the foreground object. This raises the question of whether using occupancy as a control signal is an advantage or a disadvantage.

The crux of the issue lies in the complexity of this method compared to alternatives. While other approaches need some adjustments to object trajectories, this technique necessitates providing comprehensive background and foreground occupancy data. This additional overhead prompts us to ponder whether occupancy-based control offers tangible benefits or introduces unnecessary complications.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
J8m0DEjxEO;"REVIEW 
Summary:
The main claim of this paper is that adversarial suffixes against large language models (LLMs) function by distracting the model from the original harmful goal to the suffix itself. The authors then propose a modification to GCG attack by incorporating a regularization term that increases the attention score on the adversarial suffix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
### 1. Originality and significance

The main claim of the paper is an interesting hypothesis that aims to unfold the inner workings of adversarial attacks on LLMs. This type of question can lead to a nice interpretability tool and/or a potential mitigation. Hence, the significance of this research question is clear. 

While some existing works start to look into “features” or neurons that correspond to these jailbreak attacks, the attention weights have not been deeply studied to the extent of my knowledge so it could be a nice complementary explanation.

### 2. Experiment coverage

The experiments on the attacks are relatively thorough. The authors compare their method against three existing SOTA attacks (GCG, AutoDAN, and ICA) on various open-source models. The transfer attack experiments in Section 3.4 also cover a broad range of closed-source models. The evaluation metrics are also comprehensive, including both the keyword matching and the GPT-4 evaluation.

Weaknesses:
### 1. Attention score measurement and interpretation

I first notice that in Figure 2, the attention scores on all parts (system, goal, suffix) can all go up as the optimization progresses and that the attention scores can be larger than 1 in Table 2 and 3. This suggests that the attention scores are before softmax and hence, not normalized to sum to 1. Please feel free to correct me if I’m mistaken.

1. If this is the case, it makes the score much more difficult to interpret and compare across different attacks. The absolute unnormalized value of the attention scores does not mean much because, for example, even if the score increases for the suffix portion, it may gets smaller relative to the other portions (system or goal). This is major flaw that undermines the main conclusion of the paper.
2. If that authors have not already done so, I would like to ask that all the reported attention scores be normalized (after softmax). The autoregressive generation also contributes to the attention scores, i.e., attention score of the target token $x_{n+2}$ also includes the target token $x_{t+1}$ along with all the prompt tokens $x_{1:n}$. I’m not sure what is the best way to normalize their effect. One way is to simply leave them out of the softmax, but there could be an interesting trend that we fail to capture this way. Another way is to report *difference* between average unnormalized attention score on the goal vs on the suffix portions. This also gives us a relative score but ignores the system portion.
3. In Figure 2 (left), ASR also increases along with the attention score on the goal, contradicting the main claim of the paper that higher attention score on the suffix is better.
4. It is unclear to me how Figure 5 supports the main claim of the paper. The attention pattern on ""Vanilla"" is strikingly similar to that of  ""ICA"" on the goal segment. Based on the color bar, the ICA attention score also seems higher than the Vanilla which contradicts the claim that the attack ""diverts the model’s attention away from the goal towards themselves.”

### 2. Section 3.3: Generalize AttnGCG to other attack methods

1. The purpose of this experiment is unclear to me. If the authors wish to prove their claim that higher attention weight on the suffix leads to a better attack, there should be a better controlled experiments than running GCG or AttnGCG on prompts generated by the other methods. This experiment entangles the initialization method with the attention score.
2. It might be interesting to see AttnGCG with varying values of $w_t$ and $w_a$.
3. I’d suggest an experiment where the attention loss is incorporated into AutoDAN (or other attacks) optimization objective. This would better emphasize the transferability and the usefulness of the attention loss across multiple attack algorithms.

### 3. Limited empirical improvement

While the main idea could help improve interpretability to these adversarial attacks, the attack that is inspired by this observation, AttnGCG, does not lead to significant improvement in the attack success rate, especially in the transfer setting. In the white-box setting, the improvement seems consistent across models, but the small margin suggests that attention score is not the most important factor that determines the success of the attack.

That said, it is sufficiently convincing to me that AttnGCG performs better than GCG and may replace it for evaluating the safety of LLMs.

Limitations:
Limitations and negative societal impact have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a new adversarial attack strategy on LLMs which improves over existing adversarial attacks. For this the authors propose a new regularizer that maximizes the weight of attention corresponding to suffix tokens, which naturally results in minimizing the weight for the other tokens present in the input prompt. Using this additional regularizer with GCG results in improved attack success rate. The authors also show that this attack is transferable to other attack methods like ICA and AutoDAN.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The paper is well motivated and the proposed loss follows well with the reader’s intuition.
2) The results are promising and the gains over the existing GCG attack are significant.
3) The comparison is comprehensive, involving different models.

Weaknesses:
1) It is not clear how the transferability of the same suffix tokens is for different goal prompts. This is important to investigate because GCG shows that the generated attacks are universal and can transfer on different goal prompts. I am currently a bit skeptical that the transferability on using the proposed attack might be limited because the generated suffix tokens might be more specialized for the given goal prompt. This is expected because now the generation of the suffix tokens is largely conditioned on the target target tokens due to the proposed regularizer. 

2) I believe it might be possible that using the proposed attack the model ends up outputting something potentially harmful but completely unrelated with the input prompt. This might be a possibility because the proposed approach inherently minimizes the attention on the goal tokens, which means the context of the input might become less relevant. It would be great if the authors could share some analysis on transferability of adv prompts and also share the generated text for GCG and AttnGCG.

3) It is not clear why maximizing the attention weights for suffix tokens should always lead to a stronger attack? This is also evident from tables 2 and 3 where AutoDAN has a lower value of goal attention score but stil leads to weaker attack as compared to GCG (see Table-4). Thus the argument presented in 162-163 seems questionable. 
In general, it is not clear why authors did not attempt to analyze the defenses like the ones proposed in [1]. Particularly, I believe it is important to analyze if the proposed attacks are able to bypass detection filters based on perplexity [1]. 

[1] Jain, Neel et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” ArXiv abs/2309.00614 (2023)

Limitations:
Yes, the authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a refined GCG method named AttnGCG for Large Language Model jailbreaking attacks. They focus on the attention scores of the input components, refining the loss function by adding an Attention Loss term. The attack success rates are greatly improved. Various experiments are provided to support the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1)	An interesting finding is that as the attention score on the adversarial suffix increases, the effectiveness in safeguarding LLM diminishes.

(2)	The experiments are conducted on various LLMs to prove the effectiveness of the AttnGCG.

Weaknesses:
(1)	It is unclear whether the increased success cases correspond to the 'regret' cases observed in GCG. The authors proposed AttnGCG to address the issue where the model successfully generates target tokens but then rejects the request; however, the results remain ambiguous.

(2)	In the success case illustrated in Figure 4, the attention scores at the boundary between the goal and the suffix are significantly higher than in other regions. Is this a common phenomenon in success cases? If so, why does this occur?

(3)	In Appendix A.3, the table shows that the system prompt for Llama-2 and Llama-3 is set to None, which is different from most jailbreaking papers, including the original GCG. How does this influence the attacking success rate? The authors should also report the success rate under the standard system prompt.

I will reconsider my score if all these problems are adequately addressed.

Limitations:
The authors adequately discussed the limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new jailbreak attack method against LLMs, called AttnGCG. The method integrates a loss of maximizing the attention scores of the adversarial suffix. The paper provides experimental results to show the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to follow.

Weaknesses:
My main concerns are as follows.

- Will increasing the attention scores of adversarial suffixes make the responses focus on the content in adversarial suffixes?  
- The discussion in lines 151-164 is weak. Specifically, in Figure 4, AttnGCG explicitly increases the attention scores of adversarial suffixes, so it is natural to have higher adversarial suffix attention scores. It is not convincing to say ""uncover the underlying reasons for successful attacks within the model’s attention mechanism"".  
- In Table 3, AutoDAN achieves 0.227 goal attention score, while the scores of GCG and AttnGCG are 0.8657 and 0.793. Does the observation mean that AutoDAN is better than AttnGCG?
- Some content seems to be redundant, e.g., Figure 1 and Algorithm 1.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The main claim of this paper is that adversarial suffixes against large language models (LLMs) function by distracting the model from the original harmful goal to the suffix itself. The authors then propose a modification to GCG attack by incorporating a regularization term that increases the attention score on the adversarial suffix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
### 1. Originality and significance

The main claim of the paper is an interesting hypothesis that aims to unfold the inner workings of adversarial attacks on LLMs. This type of question can lead to a nice interpretability tool and/or a potential mitigation. Hence, the significance of this research question is clear. 

While some existing works start to look into “features” or neurons that correspond to these jailbreak attacks, the attention weights have not been deeply studied to the extent of my knowledge so it could be a nice complementary explanation.

### 2. Experiment coverage

The experiments on the attacks are relatively thorough. The authors compare their method against three existing SOTA attacks (GCG, AutoDAN, and ICA) on various open-source models. The transfer attack experiments in Section 3.4 also cover a broad range of closed-source models. The evaluation metrics are also comprehensive, including both the keyword matching and the GPT-4 evaluation.

Weaknesses:
### 1. Attention score measurement and interpretation

I first notice that in Figure 2, the attention scores on all parts (system, goal, suffix) can all go up as the optimization progresses and that the attention scores can be larger than 1 in Table 2 and 3. This suggests that the attention scores are before softmax and hence, not normalized to sum to 1. Please feel free to correct me if I’m mistaken.

1. If this is the case, it makes the score much more difficult to interpret and compare across different attacks. The absolute unnormalized value of the attention scores does not mean much because, for example, even if the score increases for the suffix portion, it may gets smaller relative to the other portions (system or goal). This is major flaw that undermines the main conclusion of the paper.
2. If that authors have not already done so, I would like to ask that all the reported attention scores be normalized (after softmax). The autoregressive generation also contributes to the attention scores, i.e., attention score of the target token $x_{n+2}$ also includes the target token $x_{t+1}$ along with all the prompt tokens $x_{1:n}$. I’m not sure what is the best way to normalize their effect. One way is to simply leave them out of the softmax, but there could be an interesting trend that we fail to capture this way. Another way is to report *difference* between average unnormalized attention score on the goal vs on the suffix portions. This also gives us a relative score but ignores the system portion.
3. In Figure 2 (left), ASR also increases along with the attention score on the goal, contradicting the main claim of the paper that higher attention score on the suffix is better.
4. It is unclear to me how Figure 5 supports the main claim of the paper. The attention pattern on ""Vanilla"" is strikingly similar to that of  ""ICA"" on the goal segment. Based on the color bar, the ICA attention score also seems higher than the Vanilla which contradicts the claim that the attack ""diverts the model’s attention away from the goal towards themselves.”

### 2. Section 3.3: Generalize AttnGCG to other attack methods

1. The purpose of this experiment is unclear to me. If the authors wish to prove their claim that higher attention weight on the suffix leads to a better attack, there should be a better controlled experiments than running GCG or AttnGCG on prompts generated by the other methods. This experiment entangles the initialization method with the attention score.
2. It might be interesting to see AttnGCG with varying values of $w_t$ and $w_a$.
3. I’d suggest an experiment where the attention loss is incorporated into AutoDAN (or other attacks) optimization objective. This would better emphasize the transferability and the usefulness of the attention loss across multiple attack algorithms.

### 3. Limited empirical improvement

While the main idea could help improve interpretability to these adversarial attacks, the attack that is inspired by this observation, AttnGCG, does not lead to significant improvement in the attack success rate, especially in the transfer setting. In the white-box setting, the improvement seems consistent across models, but the small margin suggests that attention score is not the most important factor that determines the success of the attack.

That said, it is sufficiently convincing to me that AttnGCG performs better than GCG and may replace it for evaluating the safety of LLMs.

Limitations:
Limitations and negative societal impact have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a new adversarial attack strategy on LLMs which improves over existing adversarial attacks. For this the authors propose a new regularizer that maximizes the weight of attention corresponding to suffix tokens, which naturally results in minimizing the weight for the other tokens present in the input prompt. Using this additional regularizer with GCG results in improved attack success rate. The authors also show that this attack is transferable to other attack methods like ICA and AutoDAN.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The paper is well motivated and the proposed loss follows well with the reader’s intuition.
2) The results are promising and the gains over the existing GCG attack are significant.
3) The comparison is comprehensive, involving different models.

Weaknesses:
1) It is not clear how the transferability of the same suffix tokens is for different goal prompts. This is important to investigate because GCG shows that the generated attacks are universal and can transfer on different goal prompts. I am currently a bit skeptical that the transferability on using the proposed attack might be limited because the generated suffix tokens might be more specialized for the given goal prompt. This is expected because now the generation of the suffix tokens is largely conditioned on the target target tokens due to the proposed regularizer. 

2) I believe it might be possible that using the proposed attack the model ends up outputting something potentially harmful but completely unrelated with the input prompt. This might be a possibility because the proposed approach inherently minimizes the attention on the goal tokens, which means the context of the input might become less relevant. It would be great if the authors could share some analysis on transferability of adv prompts and also share the generated text for GCG and AttnGCG.

3) It is not clear why maximizing the attention weights for suffix tokens should always lead to a stronger attack? This is also evident from tables 2 and 3 where AutoDAN has a lower value of goal attention score but stil leads to weaker attack as compared to GCG (see Table-4). Thus the argument presented in 162-163 seems questionable. 
In general, it is not clear why authors did not attempt to analyze the defenses like the ones proposed in [1]. Particularly, I believe it is important to analyze if the proposed attacks are able to bypass detection filters based on perplexity [1]. 

[1] Jain, Neel et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” ArXiv abs/2309.00614 (2023)

Limitations:
Yes, the authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a refined GCG method named AttnGCG for Large Language Model jailbreaking attacks. They focus on the attention scores of the input components, refining the loss function by adding an Attention Loss term. The attack success rates are greatly improved. Various experiments are provided to support the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1)	An interesting finding is that as the attention score on the adversarial suffix increases, the effectiveness in safeguarding LLM diminishes.

(2)	The experiments are conducted on various LLMs to prove the effectiveness of the AttnGCG.

Weaknesses:
(1)	It is unclear whether the increased success cases correspond to the 'regret' cases observed in GCG. The authors proposed AttnGCG to address the issue where the model successfully generates target tokens but then rejects the request; however, the results remain ambiguous.

(2)	In the success case illustrated in Figure 4, the attention scores at the boundary between the goal and the suffix are significantly higher than in other regions. Is this a common phenomenon in success cases? If so, why does this occur?

(3)	In Appendix A.3, the table shows that the system prompt for Llama-2 and Llama-3 is set to None, which is different from most jailbreaking papers, including the original GCG. How does this influence the attacking success rate? The authors should also report the success rate under the standard system prompt.

I will reconsider my score if all these problems are adequately addressed.

Limitations:
The authors adequately discussed the limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new jailbreak attack method against LLMs, called AttnGCG. The method integrates a loss of maximizing the attention scores of the adversarial suffix. The paper provides experimental results to show the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to follow.

Weaknesses:
My main concerns are as follows.

- Will increasing the attention scores of adversarial suffixes make the responses focus on the content in adversarial suffixes?  
- The discussion in lines 151-164 is weak. Specifically, in Figure 4, AttnGCG explicitly increases the attention scores of adversarial suffixes, so it is natural to have higher adversarial suffix attention scores. It is not convincing to say ""uncover the underlying reasons for successful attacks within the model’s attention mechanism"".  
- In Table 3, AutoDAN achieves 0.227 goal attention score, while the scores of GCG and AttnGCG are 0.8657 and 0.793. Does the observation mean that AutoDAN is better than AttnGCG?
- Some content seems to be redundant, e.g., Figure 1 and Algorithm 1.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
I3kIEjoON6;"REVIEW 
Summary:
This paper presents a framework for crystal structure generation, focusing on polymorphs. The framework utilizes matrix representation of crystals and various generative models used in vision tasks, with specially designed similarity metrics and loss function. It is tested on (1) modification of given structures and (2) generation from scratch within the dataset, as well as finding new structures in the Ta–W–B system.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work develops domain-specific representation, metric, and loss, so that generative models that have proven useful in image generation can apply to crystals. The topic is timely and important.

Weaknesses:
- In the demonstrated use cases, the generation is conditioned on elements, space group, etc., but not materials properties of interest. These show limited usefulness in materials discovery and design.
- The matrix representation does not take physical constraints into account, e.g., space group determines symmetries in the lattice parameters. Besides, related previous works, e.g., [UniMat](https://openreview.net/forum?id=wm4WlHoXpC), should be discussed.
- The clarity and rigorousness need to be improved (see Questions). The mathematical notations are not unified, e.g., $x$ vs $X$.

Limitations:
Discussed in the Conclusion.
Besides, Sec. 9 contains a GitHub link that could break anonymity.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors studied the use of diffusion and flow matching approaches for the generation of crystalline materials. The authors trained UNet models on polymorphs in the AFLOW database (which has a series of DFT-computed properties for these materials) using either simple R3 regression, diffusion/flow matching. The authors then presented inference results on similarity to training structures (Section 3.4, which shows these methods can reproduce training structures to different extent), and showed that a subset of the modified generated crystals (with Ta, W, B) can have a small non-zero formation energy.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
*Originality*: The authors attempted to study the problem of crystal structure generation with no invariances/equivariances other than periodic translation invariance. 
*Quality*: The authors attempted to use DFT to validate some inference results.
*Significance*: Crystal structure generation (especially synthesizable ones) is an important problem. It seems that training from uniform noise distribution works better for CFM than training on Gaussian noise, contrary to the established results in the field.

Weaknesses:
*Originality*. The manuscript lacks originality. The diffusion/flow matching techniques are well-established in inorganic crystal structures (e.g. CDVAE cited here, DiffCSP/FlowMM that's not here). Sure, using a network architecture not designed for materials/crystals and using no invariances/equivalences is new, but it deviates from standard practices in the field without sufficient justification. I believe the implementations shown in the paper are a great exercise for practitioners interested in the field, but unfortunately, I do not see it as a NeurIPS paper.

*Quality*. The manuscript is _very_ bare-boned, making a comprehensive technical critique challenging without appearing disproportionately critical. 
- On the ML side, there are numerous large fallacies/mistakes (e.g. no consideration of bonds between atoms at all, Sec. 3.3 there is no description of the PBC loss, the generation does not consider the unit cell, no generation with atom types, and there is no investigation of any experiments observed e.g. why is uniform noise better for CFM, the result in Table 1 appears to evaluate overfitting rather than novel generation, the list can go on). 
- On the chemistry/validation side, there are again numerous problems (why would formation energy be given during the generation process, what functional did you use in DFT, there are no comparisons against existing structures and hence cannot be claimed as novel, etc.) 
- There is no comparison against _any_ known methodologies. 
- The results overall, are very weak both in ML and in chemistry (e.g. Table 3 shows most if not all materials generated have extremely large positive formation energies despite the simple elemental composition; the remaining few negative ones are at the brink of instability, in any case they likely would not be synthesizable).  

*Clarity*. The manuscript suffers from poor presentation, starting with a promotional-style title that lacks scientific descriptiveness. I unfortunately do not understand the novelty of the paper in comparison to existing methods. The paper consistently fails to provide essential explanations across both machine learning and chemical methodologies. 
- On the ML side, there are numerous things poorly presented (e.g. Figure 1 is just periodic translation invariance and in a typical manuscript would be summarized in one sentence). 
- On the chemistry side, things are greatly exaggerated (e.g. computationally making a few materials with negative formation energy can be done by undergraduate students and certainly does not warrant descriptions such as 'This significant outcome underscores the remarkable potential of our framework in uncovering thermodynamically stable materials)

Limitations:
Partially.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the inverse problem of generating crystal structures based on given properties, thereby avoiding the need for extensive computational resources typically required in traditional methods. The authors utilized the AFLOW materials database, selecting unstable and stable series of structures for two specific tasks: modifying structures to achieve stability and conditional structure generation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors experimented with various generative model approaches and evaluated two tasks in crystal generation. Additionally, they integrated the VASP software for application testing and successfully identified four previously undiscovered stable structures through conditional generation.

Weaknesses:
1. From the perspective of model application, although the authors used the AFLOW database for their study, they did not compare the data range and coverage with other significant databases like the Materials Project. This omission leaves a gap in understanding how the generative models perform across different datasets and whether the results are consistent and generalizable. Comparing the performance of the same generative models on different databases could provide valuable insights into the robustness and applicability of their approach.

2. There is a partial break of anonymity in the GitHub link on Page 9 in this paper.

Limitations:
The authors proposed two major directions: conditional generation and conditional modification. There is room for improvement in both the experimental results and the data used for conditional modification. For example, they could consider recognizing unit cells with translational and rotational transformations and introducing more ways to assess generation results.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper deals with an important application of generative models for science: generation of crystalline structures. However, I have some serious concerns. First, scope. While comparing different methods for the same objective is informative, I am not so sure what is the purpose here. Having so many different methods certainly dilutes the main message in a short paper format like neurips. Second, approach. From what I understand, there are questionable design problems with the technical approach. Third, results. The authors spend most of their space explaining various methods such that there is little room left for explaning the impact of their results, or comparing their results with existing approaches. Finally, presentation. Figure 1 is kind of trivial or at least very simple and I am not sure it is worth a separate figure. The overall typesetting looks not too professional.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The methodology selection is broad and hopefully the audience can benefit from a mini-benchmark of different generative approaches. The overall model architecture is distinctive from what I have seen in the literature.

Weaknesses:
I have a good number of questions on the technical approaches. More specifically, the model takes a specially formulated data structure that does not seem to be obviously invariant or equivariant under permutation, translation, rotation, which is concerning. For example, change the selection or ordering of the unit cell vectors and everything will change in an uncontrolled way.

The conditioning approach seems to be to provide desired properties as inputs to the generative approaches. I am not sure this always make sense. For example, if one desires a certain space group, there is no enforcing compliance with the space group. One can always easily check it. It is perhaps more suitable to enforce space group compliance using a guidance-based conditioning approach.

Limitations:
There is insufficient discussion about the limitations given my concerns shown above.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a framework for crystal structure generation, focusing on polymorphs. The framework utilizes matrix representation of crystals and various generative models used in vision tasks, with specially designed similarity metrics and loss function. It is tested on (1) modification of given structures and (2) generation from scratch within the dataset, as well as finding new structures in the Ta–W–B system.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work develops domain-specific representation, metric, and loss, so that generative models that have proven useful in image generation can apply to crystals. The topic is timely and important.

Weaknesses:
- In the demonstrated use cases, the generation is conditioned on elements, space group, etc., but not materials properties of interest. These show limited usefulness in materials discovery and design.
- The matrix representation does not take physical constraints into account, e.g., space group determines symmetries in the lattice parameters. Besides, related previous works, e.g., [UniMat](https://openreview.net/forum?id=wm4WlHoXpC), should be discussed.
- The clarity and rigorousness need to be improved (see Questions). The mathematical notations are not unified, e.g., $x$ vs $X$.

Limitations:
Discussed in the Conclusion.
Besides, Sec. 9 contains a GitHub link that could break anonymity.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors studied the use of diffusion and flow matching approaches for the generation of crystalline materials. The authors trained UNet models on polymorphs in the AFLOW database (which has a series of DFT-computed properties for these materials) using either simple R3 regression, diffusion/flow matching. The authors then presented inference results on similarity to training structures (Section 3.4, which shows these methods can reproduce training structures to different extent), and showed that a subset of the modified generated crystals (with Ta, W, B) can have a small non-zero formation energy.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
*Originality*: The authors attempted to study the problem of crystal structure generation with no invariances/equivariances other than periodic translation invariance. 
*Quality*: The authors attempted to use DFT to validate some inference results.
*Significance*: Crystal structure generation (especially synthesizable ones) is an important problem. It seems that training from uniform noise distribution works better for CFM than training on Gaussian noise, contrary to the established results in the field.

Weaknesses:
*Originality*. The manuscript lacks originality. The diffusion/flow matching techniques are well-established in inorganic crystal structures (e.g. CDVAE cited here, DiffCSP/FlowMM that's not here). Sure, using a network architecture not designed for materials/crystals and using no invariances/equivalences is new, but it deviates from standard practices in the field without sufficient justification. I believe the implementations shown in the paper are a great exercise for practitioners interested in the field, but unfortunately, I do not see it as a NeurIPS paper.

*Quality*. The manuscript is _very_ bare-boned, making a comprehensive technical critique challenging without appearing disproportionately critical. 
- On the ML side, there are numerous large fallacies/mistakes (e.g. no consideration of bonds between atoms at all, Sec. 3.3 there is no description of the PBC loss, the generation does not consider the unit cell, no generation with atom types, and there is no investigation of any experiments observed e.g. why is uniform noise better for CFM, the result in Table 1 appears to evaluate overfitting rather than novel generation, the list can go on). 
- On the chemistry/validation side, there are again numerous problems (why would formation energy be given during the generation process, what functional did you use in DFT, there are no comparisons against existing structures and hence cannot be claimed as novel, etc.) 
- There is no comparison against _any_ known methodologies. 
- The results overall, are very weak both in ML and in chemistry (e.g. Table 3 shows most if not all materials generated have extremely large positive formation energies despite the simple elemental composition; the remaining few negative ones are at the brink of instability, in any case they likely would not be synthesizable).  

*Clarity*. The manuscript suffers from poor presentation, starting with a promotional-style title that lacks scientific descriptiveness. I unfortunately do not understand the novelty of the paper in comparison to existing methods. The paper consistently fails to provide essential explanations across both machine learning and chemical methodologies. 
- On the ML side, there are numerous things poorly presented (e.g. Figure 1 is just periodic translation invariance and in a typical manuscript would be summarized in one sentence). 
- On the chemistry side, things are greatly exaggerated (e.g. computationally making a few materials with negative formation energy can be done by undergraduate students and certainly does not warrant descriptions such as 'This significant outcome underscores the remarkable potential of our framework in uncovering thermodynamically stable materials)

Limitations:
Partially.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the inverse problem of generating crystal structures based on given properties, thereby avoiding the need for extensive computational resources typically required in traditional methods. The authors utilized the AFLOW materials database, selecting unstable and stable series of structures for two specific tasks: modifying structures to achieve stability and conditional structure generation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors experimented with various generative model approaches and evaluated two tasks in crystal generation. Additionally, they integrated the VASP software for application testing and successfully identified four previously undiscovered stable structures through conditional generation.

Weaknesses:
1. From the perspective of model application, although the authors used the AFLOW database for their study, they did not compare the data range and coverage with other significant databases like the Materials Project. This omission leaves a gap in understanding how the generative models perform across different datasets and whether the results are consistent and generalizable. Comparing the performance of the same generative models on different databases could provide valuable insights into the robustness and applicability of their approach.

2. There is a partial break of anonymity in the GitHub link on Page 9 in this paper.

Limitations:
The authors proposed two major directions: conditional generation and conditional modification. There is room for improvement in both the experimental results and the data used for conditional modification. For example, they could consider recognizing unit cells with translational and rotational transformations and introducing more ways to assess generation results.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper deals with an important application of generative models for science: generation of crystalline structures. However, I have some serious concerns. First, scope. While comparing different methods for the same objective is informative, I am not so sure what is the purpose here. Having so many different methods certainly dilutes the main message in a short paper format like neurips. Second, approach. From what I understand, there are questionable design problems with the technical approach. Third, results. The authors spend most of their space explaining various methods such that there is little room left for explaning the impact of their results, or comparing their results with existing approaches. Finally, presentation. Figure 1 is kind of trivial or at least very simple and I am not sure it is worth a separate figure. The overall typesetting looks not too professional.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The methodology selection is broad and hopefully the audience can benefit from a mini-benchmark of different generative approaches. The overall model architecture is distinctive from what I have seen in the literature.

Weaknesses:
I have a good number of questions on the technical approaches. More specifically, the model takes a specially formulated data structure that does not seem to be obviously invariant or equivariant under permutation, translation, rotation, which is concerning. For example, change the selection or ordering of the unit cell vectors and everything will change in an uncontrolled way.

The conditioning approach seems to be to provide desired properties as inputs to the generative approaches. I am not sure this always make sense. For example, if one desires a certain space group, there is no enforcing compliance with the space group. One can always easily check it. It is perhaps more suitable to enforce space group compliance using a guidance-based conditioning approach.

Limitations:
There is insufficient discussion about the limitations given my concerns shown above.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
Hc2ZwCYgmB;"REVIEW 
Summary:
This work presents a zero-shot face-generation method based on diffusion models. The proposed method first extracts face features using Face2Vec and trains a network to map these features into the textual space (i.e., the prompt embedding space for diffusion’s text condition). The main difference from existing zero-shot face generation approaches is that, instead of adding conditions in the denoising UNet, the presented method incorporates the condition into the textual embedding. To prevent the subject information from overwhelming the generation (e.g., preserving only the subject ID while ignoring other descriptions), the authors propose a Composition Distillation Loss. This contrastive loss encourages the model to generate descriptions other than the subject information.

The main shortcoming of the paper lies in the experimental validation. There are multiple components proposed, but their effectiveness is not adequately demonstrated. Additionally, both qualitative and quantitative evidence fail to show that the proposed method outperforms SoTA methods.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
* The Compositional Distillation Loss is an interesting and intuitive approach to reducing the problem of subject information overwhelming the generation

Weaknesses:
* There is a lack of intuition behind using multi-timestep distillation. This approach can cause accumulated errors, and there is no evidence provided to support the benefits of adopting such a strategy.
* The section on dynamic model expansion is very unclear. It is not specified which part of the model is expanded or if the tokens are simply replicated. While adding Gaussian noise to replicated tokens is mentioned, there is no empirical validation of its effectiveness.
* The qualitative comparison, especially in Figure 7, does not support the claim that the proposed method has advantages over baselines like PuLID. Additionally, the benchmarking results in Table 1 show limited improvement in facial identity preservation, and the text alignment can be inferior to PuLID. Thus, the experiments are not comprehensive enough to convincingly demonstrate that the proposed approach is superior to existing methods like PuLID.

Limitations:
The authors have discussed the limitations.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes AdaFace, a face encoder that maps facial features from the image space to the text space through the AdaFace Prompt Inverter, utilizing the structure and pre-trained weights of the CLIP text encoder for initialization. During the face distillation phase, AdaFace employs random Gaussian face embeddings and multi-timestep distillation, enhancing the model's ability to capture subtle facial details through dynamic model expansion. In the composition distillation phase, AdaFace uses a comparative learning loss, aligning feature increments with orthogonal subtraction, while introducing an elastic face preservation loss to address the misalignment of facial features caused by different prompts.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written and easy to follow
- The proposed method requires fewer training resources
- The approach to constructing contrastive pairs during the composition distillation stage sounds reasonable

Weaknesses:
- I tried the demo provided by the authors, and the ID similarity on a few test images was relatively low; it should be far from the state-of-the-art (SOTA) level of ID similarity claimed in the paper.
- The test dataset consists of celebrities, which does not guarantee whether these IDs have appeared in the training set. Furthermore, the number of test samples is too small to be convincing.
- The upper bound of ID fidelity is constrained by the frozen Face2Image model, in this paper, Arc2Face.
- The proposed improvements like Random Gaussian Face Embeddings, Orthogonal Subtraction, etc. are not effectively validated through ablation study.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes AdaFace, a method for personalizing text-to-image diffusion models for human faces. At its core, it learns a prompt inverter that maps face embeddings from a pretrained face encoder to the text embedding space of diffusion prompts. It leverages various components including face distillation, composition distillation and elastic face preserving loss to preserve subject identity while attaining good compositionality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper designs targeted training losses and regularizations for the task at hand. The explanation of the methods is detailed. The video qualitative results show improvement over ID-Animator.

Weaknesses:
*  The quantitative metrics do not show a clear advantage of AdaFace over other existing personalization methods like PuLID. The number of qualitative examples for comparing with those methods is also limited -- just the 5 images per method in Figure 7, and not sufficient to clearly demonstrate that AdaFace outperforms existing methods. It would be helpful to show a larger number of uncurated examples comparing AdaFace and baselines to get a better comparison of their performance.
* The training of the prompt inverter involves a number of components -- such as model expansion, the inclusion of different feature types in composition distillation, orthogonal subtraction and elastic face preserving loss -- but there are no ablation studies on most of them to demonstrate their effects on the performance.

Limitations:
The authors have discussed limitations and societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes AdaFace, a test-time-tuning-free method for personalized text-to-face-image generation. Previous methods involving face features in the feature space of a face encoder, which is not flexibly composable with natural language for personalized generation. Thus, this paper proposes to map the face features into the features in the text conditioning space. Several techniques are proposed to enhance the performance, like Random Gaussian Face Embeddings, Multi-Timestep Distillation, Dynamic Model Expansion, Composition Distillation, and Elastic Face Preserving Loss. Some experiments demonstrate that the proposed method achieves good visual results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The visual results are satisfactory in general.
2. The proposed method can also be applied for personalized text-to-video generation.

Weaknesses:
1. The overall motivation is not novel enough. Finding ways to convert input images into the textual space is a fundamental goal in text-to-image personalization, which has been emphasized in the very first TextualInversion work. Even in the context of tuning-free based methods, the proposed framework is not so novel compared with ELITE [a], which also involves training a mapper from the image space into the textual space. Similar compositional distillation technique has also been explored in SuTI [b].
2. Lack of detailed studies of the proposed components, either qualitatively or quantitively. The authors propose a bag of techniques to improve the performance. Although their motivation is mentioned in the texts, there is no supportive results to illustrate how these techniques work. 
    * There is only one quantitive study in Tab. 1 regarding the compositional distillation. However, there are actually a lot of technical details in the proposed compositional distillation techniques, like the orthogonal subtraction and compositional delta loss, which lack careful experimental analysis against their alternatives. 
    * The proposed face distillation is not well supported. Can we simply train the face encoder with the simple noise prediction loss of diffusion models?
    * The analysis of the proposed Elastic Face Preserving Loss is also missing.
3. ELITE [a] mentions that using multiple token to represent an image may hurt the textual compatibility. It is necessary for the authors to provide a rationale for doing so.
4. How about the method comparing with the popular IP-Adapter (face version) [c]?
5. The overall training pipeline requires multiple stages of training, which is not so elegant.
6. The authors would like to consider merging multiple figures with similar functionalities and structures to one, like Figs. 2, 3, 4 and 5, to leave enough space for necessary experimental results.

[a] ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation, Wei et al., ICCV 2023.

[b] Subject-driven Text-to-Image Generation via Apprenticeship Learning, Chen et al., NeurIPS 2023.

[c] IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models, Ye et al..

Limitations:
The authors have discussed the limitations of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents a zero-shot face-generation method based on diffusion models. The proposed method first extracts face features using Face2Vec and trains a network to map these features into the textual space (i.e., the prompt embedding space for diffusion’s text condition). The main difference from existing zero-shot face generation approaches is that, instead of adding conditions in the denoising UNet, the presented method incorporates the condition into the textual embedding. To prevent the subject information from overwhelming the generation (e.g., preserving only the subject ID while ignoring other descriptions), the authors propose a Composition Distillation Loss. This contrastive loss encourages the model to generate descriptions other than the subject information.

The main shortcoming of the paper lies in the experimental validation. There are multiple components proposed, but their effectiveness is not adequately demonstrated. Additionally, both qualitative and quantitative evidence fail to show that the proposed method outperforms SoTA methods.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
* The Compositional Distillation Loss is an interesting and intuitive approach to reducing the problem of subject information overwhelming the generation

Weaknesses:
* There is a lack of intuition behind using multi-timestep distillation. This approach can cause accumulated errors, and there is no evidence provided to support the benefits of adopting such a strategy.
* The section on dynamic model expansion is very unclear. It is not specified which part of the model is expanded or if the tokens are simply replicated. While adding Gaussian noise to replicated tokens is mentioned, there is no empirical validation of its effectiveness.
* The qualitative comparison, especially in Figure 7, does not support the claim that the proposed method has advantages over baselines like PuLID. Additionally, the benchmarking results in Table 1 show limited improvement in facial identity preservation, and the text alignment can be inferior to PuLID. Thus, the experiments are not comprehensive enough to convincingly demonstrate that the proposed approach is superior to existing methods like PuLID.

Limitations:
The authors have discussed the limitations.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes AdaFace, a face encoder that maps facial features from the image space to the text space through the AdaFace Prompt Inverter, utilizing the structure and pre-trained weights of the CLIP text encoder for initialization. During the face distillation phase, AdaFace employs random Gaussian face embeddings and multi-timestep distillation, enhancing the model's ability to capture subtle facial details through dynamic model expansion. In the composition distillation phase, AdaFace uses a comparative learning loss, aligning feature increments with orthogonal subtraction, while introducing an elastic face preservation loss to address the misalignment of facial features caused by different prompts.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written and easy to follow
- The proposed method requires fewer training resources
- The approach to constructing contrastive pairs during the composition distillation stage sounds reasonable

Weaknesses:
- I tried the demo provided by the authors, and the ID similarity on a few test images was relatively low; it should be far from the state-of-the-art (SOTA) level of ID similarity claimed in the paper.
- The test dataset consists of celebrities, which does not guarantee whether these IDs have appeared in the training set. Furthermore, the number of test samples is too small to be convincing.
- The upper bound of ID fidelity is constrained by the frozen Face2Image model, in this paper, Arc2Face.
- The proposed improvements like Random Gaussian Face Embeddings, Orthogonal Subtraction, etc. are not effectively validated through ablation study.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes AdaFace, a method for personalizing text-to-image diffusion models for human faces. At its core, it learns a prompt inverter that maps face embeddings from a pretrained face encoder to the text embedding space of diffusion prompts. It leverages various components including face distillation, composition distillation and elastic face preserving loss to preserve subject identity while attaining good compositionality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper designs targeted training losses and regularizations for the task at hand. The explanation of the methods is detailed. The video qualitative results show improvement over ID-Animator.

Weaknesses:
*  The quantitative metrics do not show a clear advantage of AdaFace over other existing personalization methods like PuLID. The number of qualitative examples for comparing with those methods is also limited -- just the 5 images per method in Figure 7, and not sufficient to clearly demonstrate that AdaFace outperforms existing methods. It would be helpful to show a larger number of uncurated examples comparing AdaFace and baselines to get a better comparison of their performance.
* The training of the prompt inverter involves a number of components -- such as model expansion, the inclusion of different feature types in composition distillation, orthogonal subtraction and elastic face preserving loss -- but there are no ablation studies on most of them to demonstrate their effects on the performance.

Limitations:
The authors have discussed limitations and societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes AdaFace, a test-time-tuning-free method for personalized text-to-face-image generation. Previous methods involving face features in the feature space of a face encoder, which is not flexibly composable with natural language for personalized generation. Thus, this paper proposes to map the face features into the features in the text conditioning space. Several techniques are proposed to enhance the performance, like Random Gaussian Face Embeddings, Multi-Timestep Distillation, Dynamic Model Expansion, Composition Distillation, and Elastic Face Preserving Loss. Some experiments demonstrate that the proposed method achieves good visual results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The visual results are satisfactory in general.
2. The proposed method can also be applied for personalized text-to-video generation.

Weaknesses:
1. The overall motivation is not novel enough. Finding ways to convert input images into the textual space is a fundamental goal in text-to-image personalization, which has been emphasized in the very first TextualInversion work. Even in the context of tuning-free based methods, the proposed framework is not so novel compared with ELITE [a], which also involves training a mapper from the image space into the textual space. Similar compositional distillation technique has also been explored in SuTI [b].
2. Lack of detailed studies of the proposed components, either qualitatively or quantitively. The authors propose a bag of techniques to improve the performance. Although their motivation is mentioned in the texts, there is no supportive results to illustrate how these techniques work. 
    * There is only one quantitive study in Tab. 1 regarding the compositional distillation. However, there are actually a lot of technical details in the proposed compositional distillation techniques, like the orthogonal subtraction and compositional delta loss, which lack careful experimental analysis against their alternatives. 
    * The proposed face distillation is not well supported. Can we simply train the face encoder with the simple noise prediction loss of diffusion models?
    * The analysis of the proposed Elastic Face Preserving Loss is also missing.
3. ELITE [a] mentions that using multiple token to represent an image may hurt the textual compatibility. It is necessary for the authors to provide a rationale for doing so.
4. How about the method comparing with the popular IP-Adapter (face version) [c]?
5. The overall training pipeline requires multiple stages of training, which is not so elegant.
6. The authors would like to consider merging multiple figures with similar functionalities and structures to one, like Figs. 2, 3, 4 and 5, to leave enough space for necessary experimental results.

[a] ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation, Wei et al., ICCV 2023.

[b] Subject-driven Text-to-Image Generation via Apprenticeship Learning, Chen et al., NeurIPS 2023.

[c] IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models, Ye et al..

Limitations:
The authors have discussed the limitations of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
HYwfZEhyK4;"REVIEW 
Summary:
Proposed a graph based learnable multi-agent framework. The framework consists of multiple stages : Forwarding: Election (K: Answer agents; R: Reviewer) -> Review -> K Discuss till a final conclusion is reached. Proposed a mechanism to learn the graph connections dynamically. 

The major Contributions Introduced in the paper: (A)  A new swarm intelligence geo-local framework smileGeo; (B) Dynamic learning strategy; (C) A new Geo-dataset (test mainly).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The major strengths of the proposed smileGeo frameworks are: 

 (a) the learnable Graph based communication strategy seems works well empirically. In table 2, authors demonstrated that it helps achieve better acc, but lower average token costs. 

(b) The proposed method is also scalable as shown in table 3. 

(c) Used attention-based GNN to predict optimal connections and optimal election. Also empirically justified the effectiveness of attention based GNN.

(d) Also constructed Simple rules of updating edges(connections) that works well in practice.

Weaknesses:
The major weaknesses are as follows:

(a) Comparisons with baselines seems unfair. 

(b) Missing details of the evaluation setup, metrics, etc.

Limitations:
yes, the authors adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This works proposes a new visual geo-localization framework with multiple LVLM (Large Vision Language Model) agents. The agents communicate with each other to estimate the geo-location of the input image. A dynamic learning strategy is proposed to optimize the communication patterns among agents to improve efficiency. The method is evaluated on the proposed GeoGlobe dataset.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
+ The idea of tacking worldwide city-level geo-localization with multiple LVLM agents is very interesting. 
+ The result is surprisingly good with zero-shot setting, which is even better than powerful close-source models.
+ Detailed comparison with other agent-based methods is provided. The ablation study on the number of agents is also very detailed.
+ The writing is easy to follow.

Weaknesses:
- The authors could make the geo-localization setting more clear in the introduction, for example, the paper focuses on worldwide city-level geo-localization. There are lots of different settings for geo-localization problem and this could be confusing for some researchers.
- This paper provides a comparison with three traditional geo-localization methods, i.e., NetVLAD, GeM, and CosPlace. However, these three methods are either retrieval-based landmark matching methods or fine-grained classification-based place recognition methods. It would be better to provide a direct comparison with worldwide geo-localization method on city-level setting, e.g., [A]. Although I believe LVLM-based method is better at this setting, a comparison can make it more convincing.

[A] Pramanick, Shraman, et al. ""Where in the world is this image? transformer-based geo-localization in the wild."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.
- There are only two qualitative results in the appendix. Given that the accuracy is over 60%, it should be easy to find successful and failed cases to demonstrate the actual output cases of the proposed methods. It can also better illustrate how multiple agents help the geo-localization process.
- There are also some existing worldwide geo-localization datasets that could be used for more comprehensive evaluation, e.g., IM2GPS3K, YFCC4K.

Limitations:
The authors mentioned the limitations in the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces smileGeo, a novel framework for visual geo-localization, which involves identifying the geographic location of an image. The authors argue that while Large Vision-Language Models (LVLMs) show promise in this area, their individual performance is limited. SmileGeo leverages the concept of ""swarm intelligence"" by enabling multiple LVLMs to collaborate and refine their location predictions through a multi-stage review process. To enhance efficiency, the framework incorporates a dynamic learning strategy that optimizes the selection of LVLMs for each image. Furthermore, the paper introduces ""GeoGlobe,"" a new dataset designed to evaluate visual geo-localization models in open-world scenarios where many images depict locations not seen during training. Experimental results demonstrate that smileGeo outperforms existing single LVLMs and image retrieval methods, highlighting the effectiveness of collaborative learning for visual geo-localization.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* The idea of using an ensemble of networks/agents for geolocalization is interesting and novel. The authors propose a graph-based social network to enable collaboration between the agents.
* The ability to search the internet and provide the agents with relevant information is interesting and improves the performance on the task of geolocalization.
* The paper proposes GeoGlobe, a new dataset for benchmarking models on the task of geo-localizing landmarks. The dataset could be utilized in future for other learning based geospatial tasks.

Weaknesses:
* The paper only seems to tackle the problem of geolocalizing **landmark images**. While this is a challenging problem, the current literature [1, 2, 3] has already tried to address the problem of geolocalizing arbitrary ground-level images. The latter problem requires learning sophisticated geographic and visual features. I think even searching the internet cannot effectively solve the geolocalization problem for non-landmark images.
* Limited applicability: The framework is built entirely upon the capabilities of different LVLMs (e.g. GPT4, LLaVA, etc). It seems the framework cannot generalize beyond the training data used for training LLMs.
* The work fails to address the practical applications and real-life use cases of the framework. Why do we require such a framework?
* The limitation and failure cases are not adequately mentioned in the paper.

[1] Vivanco Cepeda, Vicente, Gaurav Kumar Nayak, and Mubarak Shah. ""Geoclip: Clip-inspired alignment between locations and images for effective worldwide geo-localization."" Advances in Neural Information Processing Systems 36 (2023).

[2] Haas, Lukas, Michal Skreta, Silas Alberti, and Chelsea Finn. ""Pigeon: Predicting image geolocations."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12893-12902. 2024.

[3] Berton, Gabriele, Carlo Masone, and Barbara Caputo. ""Rethinking visual geo-localization for large-scale applications."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4878-4888. 2022.

Limitations:
Limitations are insufficiently addressed in the paper. The future works mentioned in the conclusion are vague and fail to specify specific future directions for the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Proposed a graph based learnable multi-agent framework. The framework consists of multiple stages : Forwarding: Election (K: Answer agents; R: Reviewer) -> Review -> K Discuss till a final conclusion is reached. Proposed a mechanism to learn the graph connections dynamically. 

The major Contributions Introduced in the paper: (A)  A new swarm intelligence geo-local framework smileGeo; (B) Dynamic learning strategy; (C) A new Geo-dataset (test mainly).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The major strengths of the proposed smileGeo frameworks are: 

 (a) the learnable Graph based communication strategy seems works well empirically. In table 2, authors demonstrated that it helps achieve better acc, but lower average token costs. 

(b) The proposed method is also scalable as shown in table 3. 

(c) Used attention-based GNN to predict optimal connections and optimal election. Also empirically justified the effectiveness of attention based GNN.

(d) Also constructed Simple rules of updating edges(connections) that works well in practice.

Weaknesses:
The major weaknesses are as follows:

(a) Comparisons with baselines seems unfair. 

(b) Missing details of the evaluation setup, metrics, etc.

Limitations:
yes, the authors adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This works proposes a new visual geo-localization framework with multiple LVLM (Large Vision Language Model) agents. The agents communicate with each other to estimate the geo-location of the input image. A dynamic learning strategy is proposed to optimize the communication patterns among agents to improve efficiency. The method is evaluated on the proposed GeoGlobe dataset.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
+ The idea of tacking worldwide city-level geo-localization with multiple LVLM agents is very interesting. 
+ The result is surprisingly good with zero-shot setting, which is even better than powerful close-source models.
+ Detailed comparison with other agent-based methods is provided. The ablation study on the number of agents is also very detailed.
+ The writing is easy to follow.

Weaknesses:
- The authors could make the geo-localization setting more clear in the introduction, for example, the paper focuses on worldwide city-level geo-localization. There are lots of different settings for geo-localization problem and this could be confusing for some researchers.
- This paper provides a comparison with three traditional geo-localization methods, i.e., NetVLAD, GeM, and CosPlace. However, these three methods are either retrieval-based landmark matching methods or fine-grained classification-based place recognition methods. It would be better to provide a direct comparison with worldwide geo-localization method on city-level setting, e.g., [A]. Although I believe LVLM-based method is better at this setting, a comparison can make it more convincing.

[A] Pramanick, Shraman, et al. ""Where in the world is this image? transformer-based geo-localization in the wild."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.
- There are only two qualitative results in the appendix. Given that the accuracy is over 60%, it should be easy to find successful and failed cases to demonstrate the actual output cases of the proposed methods. It can also better illustrate how multiple agents help the geo-localization process.
- There are also some existing worldwide geo-localization datasets that could be used for more comprehensive evaluation, e.g., IM2GPS3K, YFCC4K.

Limitations:
The authors mentioned the limitations in the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces smileGeo, a novel framework for visual geo-localization, which involves identifying the geographic location of an image. The authors argue that while Large Vision-Language Models (LVLMs) show promise in this area, their individual performance is limited. SmileGeo leverages the concept of ""swarm intelligence"" by enabling multiple LVLMs to collaborate and refine their location predictions through a multi-stage review process. To enhance efficiency, the framework incorporates a dynamic learning strategy that optimizes the selection of LVLMs for each image. Furthermore, the paper introduces ""GeoGlobe,"" a new dataset designed to evaluate visual geo-localization models in open-world scenarios where many images depict locations not seen during training. Experimental results demonstrate that smileGeo outperforms existing single LVLMs and image retrieval methods, highlighting the effectiveness of collaborative learning for visual geo-localization.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* The idea of using an ensemble of networks/agents for geolocalization is interesting and novel. The authors propose a graph-based social network to enable collaboration between the agents.
* The ability to search the internet and provide the agents with relevant information is interesting and improves the performance on the task of geolocalization.
* The paper proposes GeoGlobe, a new dataset for benchmarking models on the task of geo-localizing landmarks. The dataset could be utilized in future for other learning based geospatial tasks.

Weaknesses:
* The paper only seems to tackle the problem of geolocalizing **landmark images**. While this is a challenging problem, the current literature [1, 2, 3] has already tried to address the problem of geolocalizing arbitrary ground-level images. The latter problem requires learning sophisticated geographic and visual features. I think even searching the internet cannot effectively solve the geolocalization problem for non-landmark images.
* Limited applicability: The framework is built entirely upon the capabilities of different LVLMs (e.g. GPT4, LLaVA, etc). It seems the framework cannot generalize beyond the training data used for training LLMs.
* The work fails to address the practical applications and real-life use cases of the framework. Why do we require such a framework?
* The limitation and failure cases are not adequately mentioned in the paper.

[1] Vivanco Cepeda, Vicente, Gaurav Kumar Nayak, and Mubarak Shah. ""Geoclip: Clip-inspired alignment between locations and images for effective worldwide geo-localization."" Advances in Neural Information Processing Systems 36 (2023).

[2] Haas, Lukas, Michal Skreta, Silas Alberti, and Chelsea Finn. ""Pigeon: Predicting image geolocations."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12893-12902. 2024.

[3] Berton, Gabriele, Carlo Masone, and Barbara Caputo. ""Rethinking visual geo-localization for large-scale applications."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4878-4888. 2022.

Limitations:
Limitations are insufficiently addressed in the paper. The future works mentioned in the conclusion are vague and fail to specify specific future directions for the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
GP30inajOt;"REVIEW 
Summary:
The authors propose a retraction-free Riemannian optimization scheme on Stiefel and oblique manifolds to perform parameter-efficient fine-tuning (PEFT) in LoRA style. The proposed approach exploits the theory of landing flows on Stiefel manifolds. Theoretical results demonstrating convergence of this iterative scheme are presented, and complemented by numerical experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method combines the advantages of Riemannian methods while avoiding the computational burden of retracting on the Stiefel manifold. The application in the context of parameter-efficient fine-tuning represents a novelty, and enhances the relevance of the numerical results. The work is well-presented, covering both algorithmic aspects and the experimental section effectively.

Weaknesses:
All the presented theory is developed on a function defined on $St(d,r)$, while LoRA fine-tuning gives rise to an objective function $f(B,A) = L(BA)$, where $B \in St(d,r)$ (or $Ob(d,r)$), and $A \in \mathbb{R}^{r \times m}$. This objective function has to be minimized over $St(d,r) \times \mathbb{R}^{r \times m}$, and the advantages of optimizing on a compact manifold are thus lost.

Unfortunately, this makes the presented theoretical results not directly useful for the practical case under consideration. 
To give a more precise statement, for example in Lemma 3, the constant $\widehat{L}$ would depend on $A$. By the mean value theorem, we would get a bound of the kind

$$
||grad_B f(A,B_1) - grad_B f(A,B_2)|| \leq C(A) ||B_1 - B_2||
$$
 (as noted in equation after line 183 for the Euclidean gradient).

 Since the space in which $A$ resides is not compact, one would need at least a uniform control on $||A_k||$ over the iterations to make the theory interesting for LoRA fine-tuning. 
It is interesting to note, however, that the authors observe exact numerical convergence to the constraint in all cases.

Limitations:
As noted in the ""weaknesses"" section, I believe there is a delicate point that is not addressed in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Retraction-free optimization algorithms on the Steifel manifold have been proposed in [1,18,19,41] etc. The motivation is that if the cost of the objective function/gradient evaluation is significantly larger than the evaluation of a retraction, then the retraction-free optimization algorithms show their advantages and efficiency. In particular, for the landing algorithm proposed in [1], the choice of the parameter in the penalty is important and may not be easy to choose. This paper gives an analysis that shows if the parameter is 1/3, the initial point is sufficiently close to the Stiefel manifold, and the step size is chosen sufficiently small, then the algorithm converges linearly to a stationary point. This result gives a concrete value of the parameter. Such a result is further merged into optimization on low-rank matrices and Manifold-LoRA is proposed. Numerical experiments show that the proposed method outperforms the baseline algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper gives a concrete value of $\mu$ and a gap between x_0 and \bar{x}_0 such that the algorithm converges under reasonable assumptions. Numerical experiments show that the proposed method is more effective than the existing approach.

Weaknesses:
(1) Though the value of mu and an upper bound of \|x_0 - \bar{x}_0\| are given concretely, the choice of step size is unknown. Theoretically, the step size needs to be sufficiently small (See Theorem 1). Any theoretical suggestion for the choice of the step size?
(2) Numerical experiments report results of the comparisons. However, the definition of ``result'' is not given. Is the result computational time or classification accuracy or a notion of correctness or something else?
(3) Problem (12) does not remove all the ambiguity. Note that if B \in St(d, r), then B A = B O O^T A = \tilde{B} \tilde{A}, where O is an orthonormal matrix and \tilde{B} = B O is still in St(d, r). Likewise for B \in Ob(d, r). Is it possible to completely remove the ambiguity by considering the quotient manifold? 
(4) Why is the numerical performance of Manifold-LoRA for using Stiefel and Oblique manifold in (12) different? The optimization problem is equivalent in the sense that the local minimizer/stationary point does not change.

Limitations:
The limitations of the paper are discussed in the conclusion section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers solving optimization problems with constraints that have orthonormal columns (i.e. the matrix belongs to the Stiefel manifold). The leading method for solving such problems is the Riemannian optimization. However, Riemannian optimization requires a costly retraction operation. The authors propose to circumvent this by introducing an additional penalty terms that stirs the optimization towards respect the manifold constraints. Indeed, the authors show that with correcting setting of parameters, the optimum will be on the manifold, and the algorithm will find it. The authors advocate that an additional advantage of their algorithm is that we know how to set the parameters for the penalty term, and so their algorithm is parameter-free.

A significant part of the paper is devoted towards motivating the study in terms of low-rank adaption in LLMs, and showing experiments in that vain.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A very elegant method for retraction free optimization on the Stiefel manifold.
- Detailed theoretical analysis showing the algorithm converges to a critical point on the manifold.
- The theoretical analysis gives explicit guidance on how to set the penalty parameter.
- The LLM applicaiton and experiments appear impressive. However, I am not an expert on this subject, so it is hard for me to asses how significant the results and evaluations are.

Weaknesses:
(The following were addressed by the authors in the rebuttal)

Major issues (affecting the recommendation):
1) Novelty: Citation [1] considers optimization with constraints on the orthogonal group (i.e. St(n,n)). It seems that the core idea on how to implement retraction free optimization already appears there. The authors mention this in ""related work"", and say that [1[ does not discuss the r<n case. Inspection of [1] reveals that this is not the entire story. In Sec 3.5 of [1] the case of r<n is discussed briefly, and it is said the results can be extended for that case. However, the authors of [1] are skeptical of the value of this, as they mention that there are fast retraction methods (i.e. Cayley) for the Stiefel manifold. 
-  Setting the penalty parameter: the authors advocate that they give an explicit value for the penalty parameter. And indeed Theorem 1 sets the parameter \mu to 1/3. However, I do not think the situation is so simple. The theorems have the additional assumption that the iterates starts close to the manifold (1/8). This is, of course, easy to achieve - just start on the manifold itself. However, for the proof to work shouldn't all iterates stay inside this bound? This necessitates for the other parameter (step size) to be small enough. And indeed, the theorem requires that the step size be small enough, and does not specify how small. Without looking in detail in the proofs, my guess is that changing the penalty step size (\mu)  affects how close you need to be to the manifold (the value 1/8), which affects how small the step size need to be. In other words, the authors load all the complexity of setting the parameters onto the step size of the main objective. Saying there is  a upper bound on its value , without specifying what that value is. You cannot call this parameter free. 

Another point is that the values of the parameter probably affect convergence rate, though the authors do not discuss this at all.

Minor comments (do not affect the recommendation):
- Line 117: If X is on the manifold, shouldn't \bar{X}, which is the projection of X on the manifold, be exactly X?
- Line 119: ""satisfies the restricted secant""
- Line 131: What is U_St (1/8)? Not defined.  Ditto line 136.
- Line 133: If the condition of twice diff is assumed, then state it earlier. 
- Eq (10): \hat{D}_f is not defined. 
- Table 1: Why metrics are changing between columns?

Limitations:
Nothing to add.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new algorithm, Manifold-LoRA, which incorporates the Stiefel manifold constraint to accelerate low-rank adaptation (LoRA) in fine-tuning LLMs. It also provides theoretical and experimental validation for the retraction-free and penalty parameter-free optimization methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper is highly technical and indicates a strong mathematical background in optimization and manifold theory. Manifold-LoRA leverages manifold geometry to reduce redundancy in LoRA fine-tuning, leading to enhanced performance and faster convergence. Furthermore, it has robust experimental validation across various datasets.

Weaknesses:
W1: Some experimental results are unclear and not well defined.

W2: Lack of the discussion about limitations of your method.

W3: Some findings of the experiments are hard to understand.

Limitations:
Limitation is not enough and is meaningless.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a retraction-free Riemannian optimization scheme on Stiefel and oblique manifolds to perform parameter-efficient fine-tuning (PEFT) in LoRA style. The proposed approach exploits the theory of landing flows on Stiefel manifolds. Theoretical results demonstrating convergence of this iterative scheme are presented, and complemented by numerical experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method combines the advantages of Riemannian methods while avoiding the computational burden of retracting on the Stiefel manifold. The application in the context of parameter-efficient fine-tuning represents a novelty, and enhances the relevance of the numerical results. The work is well-presented, covering both algorithmic aspects and the experimental section effectively.

Weaknesses:
All the presented theory is developed on a function defined on $St(d,r)$, while LoRA fine-tuning gives rise to an objective function $f(B,A) = L(BA)$, where $B \in St(d,r)$ (or $Ob(d,r)$), and $A \in \mathbb{R}^{r \times m}$. This objective function has to be minimized over $St(d,r) \times \mathbb{R}^{r \times m}$, and the advantages of optimizing on a compact manifold are thus lost.

Unfortunately, this makes the presented theoretical results not directly useful for the practical case under consideration. 
To give a more precise statement, for example in Lemma 3, the constant $\widehat{L}$ would depend on $A$. By the mean value theorem, we would get a bound of the kind

$$
||grad_B f(A,B_1) - grad_B f(A,B_2)|| \leq C(A) ||B_1 - B_2||
$$
 (as noted in equation after line 183 for the Euclidean gradient).

 Since the space in which $A$ resides is not compact, one would need at least a uniform control on $||A_k||$ over the iterations to make the theory interesting for LoRA fine-tuning. 
It is interesting to note, however, that the authors observe exact numerical convergence to the constraint in all cases.

Limitations:
As noted in the ""weaknesses"" section, I believe there is a delicate point that is not addressed in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Retraction-free optimization algorithms on the Steifel manifold have been proposed in [1,18,19,41] etc. The motivation is that if the cost of the objective function/gradient evaluation is significantly larger than the evaluation of a retraction, then the retraction-free optimization algorithms show their advantages and efficiency. In particular, for the landing algorithm proposed in [1], the choice of the parameter in the penalty is important and may not be easy to choose. This paper gives an analysis that shows if the parameter is 1/3, the initial point is sufficiently close to the Stiefel manifold, and the step size is chosen sufficiently small, then the algorithm converges linearly to a stationary point. This result gives a concrete value of the parameter. Such a result is further merged into optimization on low-rank matrices and Manifold-LoRA is proposed. Numerical experiments show that the proposed method outperforms the baseline algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper gives a concrete value of $\mu$ and a gap between x_0 and \bar{x}_0 such that the algorithm converges under reasonable assumptions. Numerical experiments show that the proposed method is more effective than the existing approach.

Weaknesses:
(1) Though the value of mu and an upper bound of \|x_0 - \bar{x}_0\| are given concretely, the choice of step size is unknown. Theoretically, the step size needs to be sufficiently small (See Theorem 1). Any theoretical suggestion for the choice of the step size?
(2) Numerical experiments report results of the comparisons. However, the definition of ``result'' is not given. Is the result computational time or classification accuracy or a notion of correctness or something else?
(3) Problem (12) does not remove all the ambiguity. Note that if B \in St(d, r), then B A = B O O^T A = \tilde{B} \tilde{A}, where O is an orthonormal matrix and \tilde{B} = B O is still in St(d, r). Likewise for B \in Ob(d, r). Is it possible to completely remove the ambiguity by considering the quotient manifold? 
(4) Why is the numerical performance of Manifold-LoRA for using Stiefel and Oblique manifold in (12) different? The optimization problem is equivalent in the sense that the local minimizer/stationary point does not change.

Limitations:
The limitations of the paper are discussed in the conclusion section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers solving optimization problems with constraints that have orthonormal columns (i.e. the matrix belongs to the Stiefel manifold). The leading method for solving such problems is the Riemannian optimization. However, Riemannian optimization requires a costly retraction operation. The authors propose to circumvent this by introducing an additional penalty terms that stirs the optimization towards respect the manifold constraints. Indeed, the authors show that with correcting setting of parameters, the optimum will be on the manifold, and the algorithm will find it. The authors advocate that an additional advantage of their algorithm is that we know how to set the parameters for the penalty term, and so their algorithm is parameter-free.

A significant part of the paper is devoted towards motivating the study in terms of low-rank adaption in LLMs, and showing experiments in that vain.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A very elegant method for retraction free optimization on the Stiefel manifold.
- Detailed theoretical analysis showing the algorithm converges to a critical point on the manifold.
- The theoretical analysis gives explicit guidance on how to set the penalty parameter.
- The LLM applicaiton and experiments appear impressive. However, I am not an expert on this subject, so it is hard for me to asses how significant the results and evaluations are.

Weaknesses:
(The following were addressed by the authors in the rebuttal)

Major issues (affecting the recommendation):
1) Novelty: Citation [1] considers optimization with constraints on the orthogonal group (i.e. St(n,n)). It seems that the core idea on how to implement retraction free optimization already appears there. The authors mention this in ""related work"", and say that [1[ does not discuss the r<n case. Inspection of [1] reveals that this is not the entire story. In Sec 3.5 of [1] the case of r<n is discussed briefly, and it is said the results can be extended for that case. However, the authors of [1] are skeptical of the value of this, as they mention that there are fast retraction methods (i.e. Cayley) for the Stiefel manifold. 
-  Setting the penalty parameter: the authors advocate that they give an explicit value for the penalty parameter. And indeed Theorem 1 sets the parameter \mu to 1/3. However, I do not think the situation is so simple. The theorems have the additional assumption that the iterates starts close to the manifold (1/8). This is, of course, easy to achieve - just start on the manifold itself. However, for the proof to work shouldn't all iterates stay inside this bound? This necessitates for the other parameter (step size) to be small enough. And indeed, the theorem requires that the step size be small enough, and does not specify how small. Without looking in detail in the proofs, my guess is that changing the penalty step size (\mu)  affects how close you need to be to the manifold (the value 1/8), which affects how small the step size need to be. In other words, the authors load all the complexity of setting the parameters onto the step size of the main objective. Saying there is  a upper bound on its value , without specifying what that value is. You cannot call this parameter free. 

Another point is that the values of the parameter probably affect convergence rate, though the authors do not discuss this at all.

Minor comments (do not affect the recommendation):
- Line 117: If X is on the manifold, shouldn't \bar{X}, which is the projection of X on the manifold, be exactly X?
- Line 119: ""satisfies the restricted secant""
- Line 131: What is U_St (1/8)? Not defined.  Ditto line 136.
- Line 133: If the condition of twice diff is assumed, then state it earlier. 
- Eq (10): \hat{D}_f is not defined. 
- Table 1: Why metrics are changing between columns?

Limitations:
Nothing to add.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new algorithm, Manifold-LoRA, which incorporates the Stiefel manifold constraint to accelerate low-rank adaptation (LoRA) in fine-tuning LLMs. It also provides theoretical and experimental validation for the retraction-free and penalty parameter-free optimization methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper is highly technical and indicates a strong mathematical background in optimization and manifold theory. Manifold-LoRA leverages manifold geometry to reduce redundancy in LoRA fine-tuning, leading to enhanced performance and faster convergence. Furthermore, it has robust experimental validation across various datasets.

Weaknesses:
W1: Some experimental results are unclear and not well defined.

W2: Lack of the discussion about limitations of your method.

W3: Some findings of the experiments are hard to understand.

Limitations:
Limitation is not enough and is meaningless.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Fm4FkfGTLu;"REVIEW 
Summary:
gRNAde is a graph neural network designed to address the RNA reverse folding problem, a significant challenge due to the potential of RNA as therapeutic modalities and their unique data properties. RNA molecules have lower thermodynamic stability compared to proteins, resulting in fewer training samples, and their increased flexibility means multiple final states are possible. gRNAde addresses these issues by proposing a custom multi-graph representation and extending message passing to operate independently on each conformer while sharing an adjacency graph. The authors thoroughly explore evaluation techniques, comparing their model performance against Rosetta by assessing the percentage of native sequence recovery on held-out sequence families. Additionally, they demonstrate slightly improved performance when utilizing multiple conformers for model training. The authors also conduct an interesting zero-shot ranking analysis on mutation data providing a refreshing evaluation against random baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is exceptionally well written presenting a thorough overview of the challenges within modality, broader field, and the importance of the problem.
- The experimental validation assesses the utility of design choices and although the improved performance in the presence of multiple conformers isn't large (the authors don't report statistical significance), the approach is promising. 
- The authors conduct a variant effect evaluation assessing whether their model is capable of learning impact of single or double mutant sequences demonstrating convincing improvement over random baselines.

Weaknesses:
- I find the arguments regarding gRNAde perplexity being correlated with recovery to have limited support in the current presentation. In figure 2 (b) color denotes perplexity instead of one of the axis making it very challenging to assess the correlation. In addition the authors don't report a correlation value or its significance. 
- The authors only use random baselines for the retrospective variant effect analysis. Including another reverse folding model or a metric from Rosetta similar to gRNAde's perplexity could strengthen the evaluation.

Limitations:
- The authors effectively discuss the current evaluation limitations and the difficulty of assessing the novelty and ground truth recovery of generated sequences.
- There is limited discussion on the data limitations in the current field. With only 4000 sequences, training points are very few, presenting a major challenge.
- The authors could include a brief statement on the broader impacts, such as the potential design of harmful molecules. As these models improve, the dual-use concern becomes legitimate.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a geometric RNA design model. Specifically, it introduces multi-stage GNN to encode multiple conformations and aggregate these candidates, and further feed decoder to predict probabilities of a set of candidate sequences.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	This work creates a new dataset for RNA inverse design, with diverse properties such as sequence length, number of structures, and structural variations.
2.	This work designs a multi-state RNA reverse design model, distinct from existing methods.

Weaknesses:
1.	The technical contribution appears to be somewhat weak. The backbone used is from existing GNN models for equivariant design.
2.	Some technical details are not claimed. For example, are the comparison baselines retained on the new dataset or simply tested on their released model.

Limitations:
This paper has no negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduce gRNAde, a geometric deep learning pipeline for RNA sequence design conditioned on one or more 3D backbone structures. gRNAde is superior to the physically based Rosetta for 3D 320 RNA inverse folding in terms of performance, inference speed, and ease of use. The method demonstrates significant superiority across various experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The authors introduce gRNAde, the first work to consider multi-state biomolecule representation. This study explores the feasibility and specific experimental results of using multi-state biomolecule representation, providing new ideas for researchers in the field.
2. The authors conduct extensive experiments and analyses on multiple datasets and experimental settings, demonstrating the model's effectiveness from various perspectives, especially regarding the ""Zero-shot ranking of RNA fitness landscape"" experiment, which is currently lacking in this field.
3. The authors present various experimental details using numerous visualizations and data tables, making the paper easier for readers to understand.

Weaknesses:
1. The model architecture proposed by the author lacks innovation. The core structure of gRNAde is directly stacked using GVP-GNN, and the handling of multi-state conformations is merely simple stacking. Additionally, the 3-beads representation method is very common in traditional RNA 3D structure modeling, which is also not an innovation by the author. Therefore, I believe the model design is lacking.
2. The baselines compared by the author in various experiments are either outdated or too simple, such as ""Rosetta(2020)"" and the ""random baseline"" in the Zero-shot experiment. This makes it difficult to demonstrate the actual performance of gRNAde. Some recent works using deep learning to model RNA 3D structures can serve as baselines, such as [1-3].
3. The author mentions that gRNAde has a significant speed improvement over Rosetta, but the author did not run the Rosetta code themselves and instead directly cited the original Rosetta paper. I believe this point is debatable because the model's running speed is also limited by GPU computational performance. The author uses an A100, whereas the GPU used by Rosetta four years ago is obviously inferior to the A100. Therefore, the author needs to rerun the Rosetta program on the A100 to provide accurate model inference times.







[1] Geometric deep learning of RNA structure, Science 2021

[2] Physics-aware Graph Neural Network for Accurate RNA 3D Structure Prediction, NIPS workshop 2022

[3] RDesign: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design, ICLR 2024

Limitations:
The authors discuss practical tradeoffs to using gRNAde in real-world RNA design scenarios 330 in Appendix B, including limitations due to the current state of 3D RNA structure prediction tools.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work designed gRNAde, a geometric deep learning pipeline for RNA sequence design conditioned on one or more 3D backbone structures. To achieve this, the authors created single-state and multi-state 3D RNA structure datasets, built a geometric graph representation, and proposed an architecture consisting of a multi-state GNN encoder, a pooling layer, and a autoregressive decoder. The single-state RNA design, multi-state RNA design, and zero-shot rank experiments were conducted and results show that gRNAde outperformed all previous methods including Rosetta.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1.	The datasets were carefully designed. Only structures with high resolution were maintained. Two kinds of clusters were used to split train, validate, and test sets where the hard samples were split into test sets.

2.	The model architecture makes use of information from multiple conformations. This is achieved by sum or average pooling.

3.	The experiments were conducted fairly. Datasets were split carefully. The results were averaged on 16 sampled sequences across 3 random seeds.

4.	The inference speed is much faster than traditional methods. This makes it possible to be used in High throughput screening. The zero-shot ranking ability is also an advantage.

Weaknesses:
1.	The model architecture has no novelty. All components are token from previous work and the overall structure is very similar to that of ProteinMPNN. The multiple conformations are processed independently and the representations are simply averaged or summed, which may not grasp all information.

2.	The model is trained on only about 4 thousand RNA sequences. These sequences are too few to cover the entire space. RNAs with no 3D structures should be exploited, as done in alphafold3.

3.	The results about single-state RNA design were reported on only 14 samples. More samples should be used to test the model. For example, the results on test sets with 100 samples should be reported.

4.	The improvements on multi-state RNA design task are limited. The native sequence recovery is obviously lower than the results of single-state design task.

Limitations:
The representation ability of gRNAde remains to be verified. Usually, the representation is extracted when all nucleotides are known, so the architecture for inverse folding problem may not suitable for representation learning.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduced a multi-state geometric graph neural network for the RNA inverse folding problems. Experiments are conducted on carefully splited structural datasets that avoid data leakage. The results have shown convincing performance improvement over the physics-based methods such has FARFAR and Rosetta which are commonly used for RNAs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is well written and pleasing to read. Explanations on key biological concepts related to RNAs, and how they motivate the model design as well as experiment setup are adequate and above all, clear.
- Evaluation metrics are well considered. The self-consistent scores on the secondary and tertiary levels are meaningful.
    - limitations on self-consistence scores are acknowledged in the main text. For RNAs, many challenges are unique especially when they are compared to proteins. Therefore, clarifications and precautions are particularly needed for RNA related tasks. I appreciate the authors’ effort for stating these limitation clearly in the main text.

Weaknesses:
- Comparison to contemporary deep learning models for RNA inverse folding is limiting
    - RDesign (https://openreview.net/forum?id=RemfXx7ebP) for example is a recent deep learning based method for 3D RNA inverse design
    - For inverse design on the secondary structure level there are many more options — a lot of them are better than RNAinverse from ViennaRNA. I would suggest checking out this survey (Design of RNAs: comparing programs for inverse RNA folding) and include a few other more competitive baselines.
- For the self-consistency scores, I personally doubt if RhoFold (also called e2efold-3d) is reliable software for RNA tertiary structure prediction, since it is from the same group that published e2efold which is a spectacularly awful RNA secondary structure predictor (I personally would avoid using any of their tools; checkout its Github issues, and also followup works on RNA secondary structure predictions that have compared with e2efold). Have the authors used more recent folding softwares such as RosettaFoldNA and AlphaFold3?
    - Would using different structure predictors significantly impact the results? This also includes EternaFold. How would gRNAde hold up against the baselines when RNAfold or LinearFold is used to compute the self-consistency scores on the secondary level?
- Data splits (train, validation and test) are carefully constructed so that the evaluation is not contaminated by data leak. But I wonder if the TM-score cut-off at 0.45 is too lenient? Is it still possible to have similar structures between training and test sets under this threshold?

Line 258 to 259. The argument would be more compelling if the inverse design operates at the quaternary level which would include information about ligand structures.

Limitations:
- Comparison to contemporary models for RNA inverse folding is a bit hollow. It would be more meaningful if some deep learning based baselines can be included into the comparison.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
gRNAde is a graph neural network designed to address the RNA reverse folding problem, a significant challenge due to the potential of RNA as therapeutic modalities and their unique data properties. RNA molecules have lower thermodynamic stability compared to proteins, resulting in fewer training samples, and their increased flexibility means multiple final states are possible. gRNAde addresses these issues by proposing a custom multi-graph representation and extending message passing to operate independently on each conformer while sharing an adjacency graph. The authors thoroughly explore evaluation techniques, comparing their model performance against Rosetta by assessing the percentage of native sequence recovery on held-out sequence families. Additionally, they demonstrate slightly improved performance when utilizing multiple conformers for model training. The authors also conduct an interesting zero-shot ranking analysis on mutation data providing a refreshing evaluation against random baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is exceptionally well written presenting a thorough overview of the challenges within modality, broader field, and the importance of the problem.
- The experimental validation assesses the utility of design choices and although the improved performance in the presence of multiple conformers isn't large (the authors don't report statistical significance), the approach is promising. 
- The authors conduct a variant effect evaluation assessing whether their model is capable of learning impact of single or double mutant sequences demonstrating convincing improvement over random baselines.

Weaknesses:
- I find the arguments regarding gRNAde perplexity being correlated with recovery to have limited support in the current presentation. In figure 2 (b) color denotes perplexity instead of one of the axis making it very challenging to assess the correlation. In addition the authors don't report a correlation value or its significance. 
- The authors only use random baselines for the retrospective variant effect analysis. Including another reverse folding model or a metric from Rosetta similar to gRNAde's perplexity could strengthen the evaluation.

Limitations:
- The authors effectively discuss the current evaluation limitations and the difficulty of assessing the novelty and ground truth recovery of generated sequences.
- There is limited discussion on the data limitations in the current field. With only 4000 sequences, training points are very few, presenting a major challenge.
- The authors could include a brief statement on the broader impacts, such as the potential design of harmful molecules. As these models improve, the dual-use concern becomes legitimate.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a geometric RNA design model. Specifically, it introduces multi-stage GNN to encode multiple conformations and aggregate these candidates, and further feed decoder to predict probabilities of a set of candidate sequences.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	This work creates a new dataset for RNA inverse design, with diverse properties such as sequence length, number of structures, and structural variations.
2.	This work designs a multi-state RNA reverse design model, distinct from existing methods.

Weaknesses:
1.	The technical contribution appears to be somewhat weak. The backbone used is from existing GNN models for equivariant design.
2.	Some technical details are not claimed. For example, are the comparison baselines retained on the new dataset or simply tested on their released model.

Limitations:
This paper has no negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduce gRNAde, a geometric deep learning pipeline for RNA sequence design conditioned on one or more 3D backbone structures. gRNAde is superior to the physically based Rosetta for 3D 320 RNA inverse folding in terms of performance, inference speed, and ease of use. The method demonstrates significant superiority across various experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The authors introduce gRNAde, the first work to consider multi-state biomolecule representation. This study explores the feasibility and specific experimental results of using multi-state biomolecule representation, providing new ideas for researchers in the field.
2. The authors conduct extensive experiments and analyses on multiple datasets and experimental settings, demonstrating the model's effectiveness from various perspectives, especially regarding the ""Zero-shot ranking of RNA fitness landscape"" experiment, which is currently lacking in this field.
3. The authors present various experimental details using numerous visualizations and data tables, making the paper easier for readers to understand.

Weaknesses:
1. The model architecture proposed by the author lacks innovation. The core structure of gRNAde is directly stacked using GVP-GNN, and the handling of multi-state conformations is merely simple stacking. Additionally, the 3-beads representation method is very common in traditional RNA 3D structure modeling, which is also not an innovation by the author. Therefore, I believe the model design is lacking.
2. The baselines compared by the author in various experiments are either outdated or too simple, such as ""Rosetta(2020)"" and the ""random baseline"" in the Zero-shot experiment. This makes it difficult to demonstrate the actual performance of gRNAde. Some recent works using deep learning to model RNA 3D structures can serve as baselines, such as [1-3].
3. The author mentions that gRNAde has a significant speed improvement over Rosetta, but the author did not run the Rosetta code themselves and instead directly cited the original Rosetta paper. I believe this point is debatable because the model's running speed is also limited by GPU computational performance. The author uses an A100, whereas the GPU used by Rosetta four years ago is obviously inferior to the A100. Therefore, the author needs to rerun the Rosetta program on the A100 to provide accurate model inference times.







[1] Geometric deep learning of RNA structure, Science 2021

[2] Physics-aware Graph Neural Network for Accurate RNA 3D Structure Prediction, NIPS workshop 2022

[3] RDesign: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design, ICLR 2024

Limitations:
The authors discuss practical tradeoffs to using gRNAde in real-world RNA design scenarios 330 in Appendix B, including limitations due to the current state of 3D RNA structure prediction tools.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work designed gRNAde, a geometric deep learning pipeline for RNA sequence design conditioned on one or more 3D backbone structures. To achieve this, the authors created single-state and multi-state 3D RNA structure datasets, built a geometric graph representation, and proposed an architecture consisting of a multi-state GNN encoder, a pooling layer, and a autoregressive decoder. The single-state RNA design, multi-state RNA design, and zero-shot rank experiments were conducted and results show that gRNAde outperformed all previous methods including Rosetta.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1.	The datasets were carefully designed. Only structures with high resolution were maintained. Two kinds of clusters were used to split train, validate, and test sets where the hard samples were split into test sets.

2.	The model architecture makes use of information from multiple conformations. This is achieved by sum or average pooling.

3.	The experiments were conducted fairly. Datasets were split carefully. The results were averaged on 16 sampled sequences across 3 random seeds.

4.	The inference speed is much faster than traditional methods. This makes it possible to be used in High throughput screening. The zero-shot ranking ability is also an advantage.

Weaknesses:
1.	The model architecture has no novelty. All components are token from previous work and the overall structure is very similar to that of ProteinMPNN. The multiple conformations are processed independently and the representations are simply averaged or summed, which may not grasp all information.

2.	The model is trained on only about 4 thousand RNA sequences. These sequences are too few to cover the entire space. RNAs with no 3D structures should be exploited, as done in alphafold3.

3.	The results about single-state RNA design were reported on only 14 samples. More samples should be used to test the model. For example, the results on test sets with 100 samples should be reported.

4.	The improvements on multi-state RNA design task are limited. The native sequence recovery is obviously lower than the results of single-state design task.

Limitations:
The representation ability of gRNAde remains to be verified. Usually, the representation is extracted when all nucleotides are known, so the architecture for inverse folding problem may not suitable for representation learning.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduced a multi-state geometric graph neural network for the RNA inverse folding problems. Experiments are conducted on carefully splited structural datasets that avoid data leakage. The results have shown convincing performance improvement over the physics-based methods such has FARFAR and Rosetta which are commonly used for RNAs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is well written and pleasing to read. Explanations on key biological concepts related to RNAs, and how they motivate the model design as well as experiment setup are adequate and above all, clear.
- Evaluation metrics are well considered. The self-consistent scores on the secondary and tertiary levels are meaningful.
    - limitations on self-consistence scores are acknowledged in the main text. For RNAs, many challenges are unique especially when they are compared to proteins. Therefore, clarifications and precautions are particularly needed for RNA related tasks. I appreciate the authors’ effort for stating these limitation clearly in the main text.

Weaknesses:
- Comparison to contemporary deep learning models for RNA inverse folding is limiting
    - RDesign (https://openreview.net/forum?id=RemfXx7ebP) for example is a recent deep learning based method for 3D RNA inverse design
    - For inverse design on the secondary structure level there are many more options — a lot of them are better than RNAinverse from ViennaRNA. I would suggest checking out this survey (Design of RNAs: comparing programs for inverse RNA folding) and include a few other more competitive baselines.
- For the self-consistency scores, I personally doubt if RhoFold (also called e2efold-3d) is reliable software for RNA tertiary structure prediction, since it is from the same group that published e2efold which is a spectacularly awful RNA secondary structure predictor (I personally would avoid using any of their tools; checkout its Github issues, and also followup works on RNA secondary structure predictions that have compared with e2efold). Have the authors used more recent folding softwares such as RosettaFoldNA and AlphaFold3?
    - Would using different structure predictors significantly impact the results? This also includes EternaFold. How would gRNAde hold up against the baselines when RNAfold or LinearFold is used to compute the self-consistency scores on the secondary level?
- Data splits (train, validation and test) are carefully constructed so that the evaluation is not contaminated by data leak. But I wonder if the TM-score cut-off at 0.45 is too lenient? Is it still possible to have similar structures between training and test sets under this threshold?

Line 258 to 259. The argument would be more compelling if the inverse design operates at the quaternary level which would include information about ligand structures.

Limitations:
- Comparison to contemporary models for RNA inverse folding is a bit hollow. It would be more meaningful if some deep learning based baselines can be included into the comparison.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
DsN7tHNo78;"REVIEW 
Summary:
This paper focuses on Zero-Shot Composed Image Retrieval (ZS-CIR), which requires retrieving an image matching a reference image while incorporating specified textual modifications. The authors argue that a key challenge in ZS-CIR is training models on limited intention-relevant datasets to understand human intention implicitly expressed in textual modifications for accurately retrieving target images. Therefore, they introduce an image-text dataset incorporated with pseudo-manipulation intentions to enhance the training of ZS-CIR models in understanding human manipulation intents, based on LLaVA. They also propose to use Q-former to compress the features generated by CLIP for retrieval. The experimental results show the improvements of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors propose a large-scale pretraining dataset for ZS-CIR.
2. The experimental results show the improvements of the proposed method.

Weaknesses:
1. The concept of ""intention"" discussed throughout the whole paper is unclear. Based on Figure 1, the authors haven't explained what's ""intention"" to the MLLM. Moreover, the pseudo-manipulation description is the same as the rewritten caption in semantics. I can't find any ""intention"" added into this pseudo-manipulation description. The key novelty of considering human ""intention"" is farfetched.
2. The proposed model lacks novelty. In the model architecture, the authors just add a Q-Former [1] after the CLIP encoder, which is prevalent in existing research based on CLIP-like models. And the authors even do not cite any relevant work.
3. Existing work on CLIP-based ZS-CIR generally compares the experimental results with different CLIP variants, and different methods may be superior with different CLIP variants. The authors only experimented with one CLIP variant, which is insufficient.
4. In Figure 4, the compared method also accurately captures the ""intention"" in the modification text, which cannot show the superiority of the proposed method in capturing ""intention"".

References:

[1] Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.

Limitations:
The authors addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces De-MINDS, a novel framework for Zero-Shot Composed Image Retrieval (ZS-CIR) that aims to bridge the gap between pre-training and retrieval by incorporating intention-based pseudo-manipulation descriptions. The authors propose intent-CC3M, a dataset featuring these descriptions generated through chain-of-thought prompting by a Multi-modal Large Language Model (MLLM). They also introduce a manipulation intention understanding network that uses learnable queries to enhance the model's ability to understand user intentions from manipulation descriptions. The paper demonstrates significant performance improvements across four ZS-CIR tasks compared to state-of-the-art models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The introduction of intent-CC3M as a dataset for training mapping networks to align intention-relevant visual information is innovative and potentially impactful.
- The proposed De-MINDS framework shows significant performance improvements over state-of-the-art models across multiple ZS-CIR tasks.
- The approach addresses the challenge of understanding manipulation intentions in user descriptions, which is crucial for accurate image retrieval.
- The ablation studies provide insights into the contributions of different components of the proposed method.

Weaknesses:
Major Weaknesses:

1. Experimental Gaps:
   - The paper lacks experimental evidence to support the claim that caption redundancy leads to inaccurate retrieval, as mentioned in the introduction.
   - There's no evaluation of the method's performance with longer text encoders like LongCLIP, which could potentially address some of the stated limitations of CLIP.
   - The comparison with a baseline (other than CIRR and Fashion-IQ) using only f_theta (trained on Intent-CC3M) without De-MINDS (ablation model '4') is missing, which would provide a fairer comparison.

2. Methodological Concerns:
   - The justification for using CC3M as the base dataset for creating intent-CC3M is not clearly explained.
   - There's no exploration of De-MINDS' performance when prompt options are mismatched with their intended tasks or in scenarios where the task is not known in advance.

3. Incomplete Ablation Studies:
   - The ablation study for the T sampling ratios (50%, 30%, 20%) is missing, and there's no explanation why concatenation of them wasn't considered as an alternative.
   - The ablation study lacks an exploration of the impact of the number of learnable queries, despite its apparent significance.

Minor Weaknesses:

1. Presentation Issues:
   - The prompt types (a), (b), and (c) are not clearly explained in the context they are introduced, requiring readers to refer back to previous sections.
   - There are inconsistencies between the notation in the text and figures (e.g., X vs q in Figure 2).

2. Comparative Analysis:
   - The paper doesn't include evaluations on CIRCO and GeneCIS datasets, which were used in baseline studies.

3. Clarity:
   - More details are needed on certain aspects, such as the ""cos distill"" mentioned in ablation model '9'.

Limitations:
The authors acknowledge the computational intensity of generating pseudo-manipulation descriptions using MLLMs and the potential introduction of irrelevant details in these descriptions. However, they could further discuss the implications of these limitations on the practical applicability of their method in real-world scenarios.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an image-text dataset (intent-CC3M) for Zero-Shot Composed Image Retrieval (ZS-CIR) models to make better understanding of human manipulation intentions. Specifically, captions are re-written with LLaVA model to provide more details, and additional manipulation reasoning prompt is applied to make pseudo-manipulation description. With this dataset, the paper proposes De-MINDS framework (unDErstanding of Manipulation INtention from target Description before Searching), which utilizes pseudo-manipulation descriptions. The model training involves reasoning distillation and cross-modal alignment. The method shows state-of-the-art performance with ViT-L backbone comparisons.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes to leverage LLaVA model to elaborate the image caption and further utilize LLaVA's reasoning capability to build a pseudo manipulation. The proposal is intuitive and clear, and the presentation of this paper is also clear. Extensive results including various ablations and qualitative results demonstrate the proposed method.

Weaknesses:
The proposed method of utilizing LLM, referred to as MLLM, is not entirely novel, as it has been previously addressed in works [1, 2] (please also refer to [1]). Furthermore, the evaluation of the proposed method is limited to the ViT-L backbone, which raises concerns about its effectiveness with other, more robust backbones (such as ViT-G).

[1] Jang, et al. Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval, CVPR2024
[2] Karthik, et al, Vision-by-Language for Training-Free Compositional Image Retrieval, ICLR2024

Limitations:
The paper handles possible limitations properly.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper focuses on Zero-Shot Composed Image Retrieval (ZS-CIR), which requires retrieving an image matching a reference image while incorporating specified textual modifications. The authors argue that a key challenge in ZS-CIR is training models on limited intention-relevant datasets to understand human intention implicitly expressed in textual modifications for accurately retrieving target images. Therefore, they introduce an image-text dataset incorporated with pseudo-manipulation intentions to enhance the training of ZS-CIR models in understanding human manipulation intents, based on LLaVA. They also propose to use Q-former to compress the features generated by CLIP for retrieval. The experimental results show the improvements of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors propose a large-scale pretraining dataset for ZS-CIR.
2. The experimental results show the improvements of the proposed method.

Weaknesses:
1. The concept of ""intention"" discussed throughout the whole paper is unclear. Based on Figure 1, the authors haven't explained what's ""intention"" to the MLLM. Moreover, the pseudo-manipulation description is the same as the rewritten caption in semantics. I can't find any ""intention"" added into this pseudo-manipulation description. The key novelty of considering human ""intention"" is farfetched.
2. The proposed model lacks novelty. In the model architecture, the authors just add a Q-Former [1] after the CLIP encoder, which is prevalent in existing research based on CLIP-like models. And the authors even do not cite any relevant work.
3. Existing work on CLIP-based ZS-CIR generally compares the experimental results with different CLIP variants, and different methods may be superior with different CLIP variants. The authors only experimented with one CLIP variant, which is insufficient.
4. In Figure 4, the compared method also accurately captures the ""intention"" in the modification text, which cannot show the superiority of the proposed method in capturing ""intention"".

References:

[1] Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.

Limitations:
The authors addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces De-MINDS, a novel framework for Zero-Shot Composed Image Retrieval (ZS-CIR) that aims to bridge the gap between pre-training and retrieval by incorporating intention-based pseudo-manipulation descriptions. The authors propose intent-CC3M, a dataset featuring these descriptions generated through chain-of-thought prompting by a Multi-modal Large Language Model (MLLM). They also introduce a manipulation intention understanding network that uses learnable queries to enhance the model's ability to understand user intentions from manipulation descriptions. The paper demonstrates significant performance improvements across four ZS-CIR tasks compared to state-of-the-art models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The introduction of intent-CC3M as a dataset for training mapping networks to align intention-relevant visual information is innovative and potentially impactful.
- The proposed De-MINDS framework shows significant performance improvements over state-of-the-art models across multiple ZS-CIR tasks.
- The approach addresses the challenge of understanding manipulation intentions in user descriptions, which is crucial for accurate image retrieval.
- The ablation studies provide insights into the contributions of different components of the proposed method.

Weaknesses:
Major Weaknesses:

1. Experimental Gaps:
   - The paper lacks experimental evidence to support the claim that caption redundancy leads to inaccurate retrieval, as mentioned in the introduction.
   - There's no evaluation of the method's performance with longer text encoders like LongCLIP, which could potentially address some of the stated limitations of CLIP.
   - The comparison with a baseline (other than CIRR and Fashion-IQ) using only f_theta (trained on Intent-CC3M) without De-MINDS (ablation model '4') is missing, which would provide a fairer comparison.

2. Methodological Concerns:
   - The justification for using CC3M as the base dataset for creating intent-CC3M is not clearly explained.
   - There's no exploration of De-MINDS' performance when prompt options are mismatched with their intended tasks or in scenarios where the task is not known in advance.

3. Incomplete Ablation Studies:
   - The ablation study for the T sampling ratios (50%, 30%, 20%) is missing, and there's no explanation why concatenation of them wasn't considered as an alternative.
   - The ablation study lacks an exploration of the impact of the number of learnable queries, despite its apparent significance.

Minor Weaknesses:

1. Presentation Issues:
   - The prompt types (a), (b), and (c) are not clearly explained in the context they are introduced, requiring readers to refer back to previous sections.
   - There are inconsistencies between the notation in the text and figures (e.g., X vs q in Figure 2).

2. Comparative Analysis:
   - The paper doesn't include evaluations on CIRCO and GeneCIS datasets, which were used in baseline studies.

3. Clarity:
   - More details are needed on certain aspects, such as the ""cos distill"" mentioned in ablation model '9'.

Limitations:
The authors acknowledge the computational intensity of generating pseudo-manipulation descriptions using MLLMs and the potential introduction of irrelevant details in these descriptions. However, they could further discuss the implications of these limitations on the practical applicability of their method in real-world scenarios.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an image-text dataset (intent-CC3M) for Zero-Shot Composed Image Retrieval (ZS-CIR) models to make better understanding of human manipulation intentions. Specifically, captions are re-written with LLaVA model to provide more details, and additional manipulation reasoning prompt is applied to make pseudo-manipulation description. With this dataset, the paper proposes De-MINDS framework (unDErstanding of Manipulation INtention from target Description before Searching), which utilizes pseudo-manipulation descriptions. The model training involves reasoning distillation and cross-modal alignment. The method shows state-of-the-art performance with ViT-L backbone comparisons.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes to leverage LLaVA model to elaborate the image caption and further utilize LLaVA's reasoning capability to build a pseudo manipulation. The proposal is intuitive and clear, and the presentation of this paper is also clear. Extensive results including various ablations and qualitative results demonstrate the proposed method.

Weaknesses:
The proposed method of utilizing LLM, referred to as MLLM, is not entirely novel, as it has been previously addressed in works [1, 2] (please also refer to [1]). Furthermore, the evaluation of the proposed method is limited to the ViT-L backbone, which raises concerns about its effectiveness with other, more robust backbones (such as ViT-G).

[1] Jang, et al. Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval, CVPR2024
[2] Karthik, et al, Vision-by-Language for Training-Free Compositional Image Retrieval, ICLR2024

Limitations:
The paper handles possible limitations properly.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
D80tRH9CXF;"REVIEW 
Summary:
The paper examines prediction and estimation risk of ridgeless least squares estimator in the setting of a general error structure. The iid assumption on the error structure is often not valid in settings such as time series data , panel data, grouped data etc. The current paper introduces a theoretical framework which investigates the variance component estimation of both prediction and estimation risks in the above mentioned data settings. The benefits of overparametrization which has been seen in iid context has been shown to exist in the dependent error structure context as well.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Following are the strengths of the paper-

- Investigation of prediction and estimation risk under non i.i.d. regressor errors with specific focus on time series and cluster data

- Explicit quantification of the variance component of both the risks (as mentioned above) which depends on the trace of the error covariance matrix and the trace of a function of design matrix as a separable product.

- Explicit analysis of the variance and bias term of both the risks (as mentioned above) in the high-dimensional asymptotics

- Well constructed numerical experiments to support the theory

Weaknesses:
Following are the weaknesses of the paper

- The theoretical results particularly the bias component analysis section could have been more rigorous and better written. There are some notational discrepancies and theoretical inconsistencies.

- Some remarks following theorem 3.4 and 3.5 where the design matrix $X$ has a known distribution say Gaussian would have been useful  examples to get insight on the results proved in the theorems

- Some notations such as $a(X)$ and $b$ used in theorem 3.4 have been clarified later in the appendix. It would be better to introduce them in the sketch of the proof if you are using them anyway there.

Limitations:
The authors have adequately addressed the limitations of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper explores the prediction risk and estimation risk of the ridgeless least squares estimator under more general assumptions on regression errors. It highlights the benefits of overparameterization in a realistic setting that allows for clustered or serial dependence. The paper establishes that the estimation difficulties associated with the variance components of both risks can be summarized through the trace of the variance-covariance matrix of the regression errors. The findings suggest that the benefits of overparameterization can extend to time series, panel, and grouped data. The paper is a theoretical work that discusses various aspects of linear regression models, providing details on the assumptions and proofs for the theoretical results presented. It also includes information on the experimental setting and provides code and instructions for reproducing the main results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This study addresses an important research gap by considering more realistic assumptions on regression errors. It provides exact finite-sample characterizations of the variance components of prediction and estimation risks, includes numerical experiments that validate the theoretical results, and demonstrates the relationship between the expected variance and the covariance of the regression errors. Additionally, it analyzes the bias components of prediction and estimation risks, offers a comprehensive overview of linear regression models covering various theoretical aspects, and provides detailed proofs for the theoretical results, ensuring the validity of their claims.

Weaknesses:
Is it possible to provide validation on large-scale data?

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the properties of minimum norm (ridgeless) interpolation least squares estimators, analyzing prediction risk and estimation risk under broader regression error assumptions, including clustered or serial dependence. This diverges from the typical assumption of i.i.d. errors with zero mean and common variance. The paper shows that the challenges in estimating the variance components of prediction and estimation risks can be captured by the trace of the variance-covariance matrix of the regression errors.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a more general theoretical analysis of minimum norm interpolation least squares estimators, going beyond the restrictive i.i.d. error assumption.

2. The paper suggests that the benefits of overparameterization can extend to a wider range of regression settings, including time series, panel, and grouped data.

Weaknesses:
While the paper examines broader error structures, it might not fully grasp the complexity of real-world regression challenges, which could involve even more intricate patterns of error dependence.

Limitations:
The authors have addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the ridgeless least-squares estimator, and derives its prediction and estimation risk. One of the assumptions used is that the expectation of the noise variance matrix is finite and positive-definite. This is more general than the assumption that this expectation is some positive multiple of the identity matrix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper has an easy-to-follow introduction that motivates the need to derive theoretical results under general assumptions on regression errors.
- Related works are sufficiently discussed. The most relevant papers are those of Chinot et al. [9] and Chinot and Lerasle [8], which are based on different noise assumptions that this paper makes.
- The technical presentation is clear with examples and figures to help the reader understand the notations and results.

Weaknesses:
The major concern I have is whether the paper makes sufficient technical contributions. Even with the more general assumption on noise (Assumption 2.1), the technical change in the proofs seems very small compared to prior work. For example, the proof of Theorem 3.4 is short and relatively straightforward (and this might further simplify if we make Gaussian assumptions on data rather than left-spherical assumptions. Gaussian assumptions are what I like to make personally). It is always nice to have short and concise proofs whenever possible, but this might also indicate that the paper is not very technically solid.

Limitations:
N.A.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper examines prediction and estimation risk of ridgeless least squares estimator in the setting of a general error structure. The iid assumption on the error structure is often not valid in settings such as time series data , panel data, grouped data etc. The current paper introduces a theoretical framework which investigates the variance component estimation of both prediction and estimation risks in the above mentioned data settings. The benefits of overparametrization which has been seen in iid context has been shown to exist in the dependent error structure context as well.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Following are the strengths of the paper-

- Investigation of prediction and estimation risk under non i.i.d. regressor errors with specific focus on time series and cluster data

- Explicit quantification of the variance component of both the risks (as mentioned above) which depends on the trace of the error covariance matrix and the trace of a function of design matrix as a separable product.

- Explicit analysis of the variance and bias term of both the risks (as mentioned above) in the high-dimensional asymptotics

- Well constructed numerical experiments to support the theory

Weaknesses:
Following are the weaknesses of the paper

- The theoretical results particularly the bias component analysis section could have been more rigorous and better written. There are some notational discrepancies and theoretical inconsistencies.

- Some remarks following theorem 3.4 and 3.5 where the design matrix $X$ has a known distribution say Gaussian would have been useful  examples to get insight on the results proved in the theorems

- Some notations such as $a(X)$ and $b$ used in theorem 3.4 have been clarified later in the appendix. It would be better to introduce them in the sketch of the proof if you are using them anyway there.

Limitations:
The authors have adequately addressed the limitations of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper explores the prediction risk and estimation risk of the ridgeless least squares estimator under more general assumptions on regression errors. It highlights the benefits of overparameterization in a realistic setting that allows for clustered or serial dependence. The paper establishes that the estimation difficulties associated with the variance components of both risks can be summarized through the trace of the variance-covariance matrix of the regression errors. The findings suggest that the benefits of overparameterization can extend to time series, panel, and grouped data. The paper is a theoretical work that discusses various aspects of linear regression models, providing details on the assumptions and proofs for the theoretical results presented. It also includes information on the experimental setting and provides code and instructions for reproducing the main results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This study addresses an important research gap by considering more realistic assumptions on regression errors. It provides exact finite-sample characterizations of the variance components of prediction and estimation risks, includes numerical experiments that validate the theoretical results, and demonstrates the relationship between the expected variance and the covariance of the regression errors. Additionally, it analyzes the bias components of prediction and estimation risks, offers a comprehensive overview of linear regression models covering various theoretical aspects, and provides detailed proofs for the theoretical results, ensuring the validity of their claims.

Weaknesses:
Is it possible to provide validation on large-scale data?

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the properties of minimum norm (ridgeless) interpolation least squares estimators, analyzing prediction risk and estimation risk under broader regression error assumptions, including clustered or serial dependence. This diverges from the typical assumption of i.i.d. errors with zero mean and common variance. The paper shows that the challenges in estimating the variance components of prediction and estimation risks can be captured by the trace of the variance-covariance matrix of the regression errors.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a more general theoretical analysis of minimum norm interpolation least squares estimators, going beyond the restrictive i.i.d. error assumption.

2. The paper suggests that the benefits of overparameterization can extend to a wider range of regression settings, including time series, panel, and grouped data.

Weaknesses:
While the paper examines broader error structures, it might not fully grasp the complexity of real-world regression challenges, which could involve even more intricate patterns of error dependence.

Limitations:
The authors have addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the ridgeless least-squares estimator, and derives its prediction and estimation risk. One of the assumptions used is that the expectation of the noise variance matrix is finite and positive-definite. This is more general than the assumption that this expectation is some positive multiple of the identity matrix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper has an easy-to-follow introduction that motivates the need to derive theoretical results under general assumptions on regression errors.
- Related works are sufficiently discussed. The most relevant papers are those of Chinot et al. [9] and Chinot and Lerasle [8], which are based on different noise assumptions that this paper makes.
- The technical presentation is clear with examples and figures to help the reader understand the notations and results.

Weaknesses:
The major concern I have is whether the paper makes sufficient technical contributions. Even with the more general assumption on noise (Assumption 2.1), the technical change in the proofs seems very small compared to prior work. For example, the proof of Theorem 3.4 is short and relatively straightforward (and this might further simplify if we make Gaussian assumptions on data rather than left-spherical assumptions. Gaussian assumptions are what I like to make personally). It is always nice to have short and concise proofs whenever possible, but this might also indicate that the paper is not very technically solid.

Limitations:
N.A.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Cth1PyCwZt;"REVIEW 
Summary:
This paper shows the use of psychometric modeling techniques to measure the reasoning ability of LLMs on human exams. Specifically, the author(s) use Item Response Theory (IRT) to evaluate a Brazilian college-entrance exam, and demonstrate that IRT can provide a more informative evaluation of LLMs , including: the ability to distinguish human-like vs non-human-like response patterns, and to determine whether an exam can reliably measure an LLM's abilities. The empirical results suggest that traditional accuracy metrics are insufficient to assess the abilities of LLMs, and advocate for using IRT/psychometric theory to evaluate them.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Provides more comprehensive evaluation methods for LLM performance, I personally agree that accuracy metrics often do not give a complete picture of LLM ability.
2. The results section is methodological, it evaluates not only the IRT scores but how reliable they are based on several metrics (increases reliability of the evaluations)

Weaknesses:
1. The results analysis would benefit from a more detailed and clearer/deeper analysis, some statements made (eg. L293-298) are high level observations based on the results, but lack further insight into why certain LLM behaviors occur. Performing more detailed analyses into the specific subset of questions that contribute to scores could help to further understand the limitations of the LLM (L328-331 alludes to this, but very briefly).

2.  All the evaluations were done on variations of the ENEM exam dataset, showing that these psychometric method would also work on other datasets would make this approach more convincing that it will work for wider applications - I understand that there is limited time to run more experiments, so this is more so just a comment.

Limitations:
As mentioned above, as experiments are done on variations of one dataset, there are doubts about the generalizability of these methods on other datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on evaluating LLM abilities on a dataset of 8 college-entrance exams in Brazil (translated to English) measuring Item Response Theory instead of Accuracy. It highlights how such metric is useful to better understand models' performance.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I have found the work very well structured and appreciated the amount of care the authors have been given to the preparation of the dataset for the experiments (PDF processing, translation to English, use of exams designed for blind people in order to address questions based on images, etc). The experiments and results are discussed in details, with clear comparisons with human performance, discussing clear differences (e.g. in Mathematics).

Weaknesses:
While the paper is well structured, I felt it was missing a ""what now?"" message. The authors wrote a convincing argument in favour of using IRT, how do we convince now the field of ML / AI to use it more extensively? What are its limitations in comparison with accuracy-based metrics (given there are many, for instance you need information on overall human performance) and how do we overcome them?

Limitations:
I think the work should have discussed more about the specificities of ENEM - I agree with the authors that this is a relevant test-bed for this sort of evaluation, but in which ways are they specific / tailored to Brazil? Is there anything researchers should know about ENEM, which would make future testing / applications more challenging? For instance which topics are covered in Humanities or Languages, how specific are they about the country cultural context?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper initiates the empirical study of the performance of LLMs using Item Response Theory (IRT) models from a large college-entrance exam.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The question of construct validity of LLM evaluations (based on scores in exams designed for humans) is very important. This paper addresses this question in the earnest, by leveraging the primary tool developed in the education and psychometrics field, IRT. As far as I know, this is an original contribution as no previous work has used IRT in the same way to tackle the construct validity issue of LLM evaluations.

- The paper is largely well-written and the concepts (e.g. section 3) are explained clearly.

- Relative strength of empirical work. The experiments are well-designed and there is some breadth in the range of hypotheses explored, e.g. English vs. Portuguese effect on performance, Different topics of exams, response patterns in LLMs vs. humans with questions sorted by IRT difficulty. Seven different LLMs were evaluated.

- Significance. The method of this paper (i.e. using IRT in LLM evals) is an important first step to understanding what LLM evals are trying to measure. The paper already observes interesting phenomena, e.g. 
(1) the Fisher information of the math exam for the LLM test response distributions is low compared to other exams (although this is a somewhat obvious corollary of the p_i's being close to random for the LLMs performance on the math exam, the FI is a metric that points in the right direction).
(2) the joint distribution of IRT scores and CTT scores for LLMs is meaningfully different from that of the human test takers.

Weaknesses:
1. Some of the conclusions drawn by the paper appear unscientific/not well-substantiated. To me, the empirical results are subtle and require more thoughtful interpretations. Most of the interpretations of the experiments are confusing to me (i.e. I'm skeptical the conclusions follow), given the actual plots shown. For example, 

(a) What are ""outlier models"" (line 237)? We cannot see from Figure 1 that ""outlier models ... have higher accuracy and/or lower IRT scores..."" - how is this statement supported?

(b) line 223-224. The scale of IRT scores and CTT scores is not comparable. How can you conclude there is ""greater variability"" in the latter than in the IRT score? This is not scientific.

(c) line 264-265. The statement ""...questions that are easy for humans but difficult for LLMs"" is again inaccurate. The questions are relatively easier for humans but may not be ""easier"" than the other questions for humans, if easier means for humans anyway.

(d) Why is the math exam not meaningful for evaluating LLMs? Doesn't it suggest that the models are randomly guessing and therefore bad? I don't agree with this interpretation.

2. A clarity issue with the math writing. Line 154-155: This sentence ""...j has a more likely response vector than indicated by their ability"" is mathematically wrong. It is not possible to have a random draw from a Multinomial distribution that is ""more likely"" (i.e. higher probability) than the expectation vector (which is not even in the space of possible draws).

3. Experiment section writing missing some details and figures are somewhat difficult to interpret (esp Figure 1). I have several unanswered questions. How was the closed curve generated from the 30 points (of random shuffles)? The caption for Figure 1 could be more informative, e.g. was the exam answered in English or Portuguese by the LLM. If English, are the IRT models fit still valid? - I don't think so. 

4. Typo in lines 232-233, ""Natural sciences"" appears twice. and the sentence contradicts the graph.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a fresh perspective to evaluating LLMs by arguing for a stronger emphasis on psychometric methods particularly Item Response Theory (IRT) when evaluating them on exams designed for humans, rather than the reliance on traditional metrics such as accuracy. The authors postulate that IRT provides a more comprehensive evaluation by considering not just the number of correct answers but also the difficulty of the questions and the patterns of responses. The authors utilize the Brazilian college entrance exam ENEM for their case study and compare how various LLMs fare against human test-takers. They show how psychometric methods can be leveraged to distinguish between human like and non-human like responses. Furthermore, they demonstrate how IRT can be used to assess the suitability of an exam for making meaningful measurements of an LLM's abilities in the given area.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is very well written. It provides a comprehensive literature review and does a good job at covering the background information. The experimental analysis is sound with sufficient supporting materials. The paper makes significant novel contributions to evaluation of LLMs. The application of psychometric methods and the insights that can be mined through them when used to compare LLMs can be of significant interest to the research community. The experimental results on assessing whether an exam is a good indicator of an LLM's ability are particularly interesting and open up significant opportunities for future research.

Weaknesses:
The error analysis can be more detailed especially in areas where the results are surprising. This would better help support the conclusions.       For instance for the questions in Math and Natural Sciences wherein the models show fluctuating performance it would be useful to know what those questions aim to test. Are LLMs not able to solve the problems due to calculation errors or do these problems involve more complex multi-step reasoning or is it just linked to knowledge cutoff (e.g questions involving current events)?

Limitations:
The pre-requisite for this type of evaluation seems to be the existence of a strong IRT model which in turn requires the existence of large amount of carefully annotated human data.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper shows the use of psychometric modeling techniques to measure the reasoning ability of LLMs on human exams. Specifically, the author(s) use Item Response Theory (IRT) to evaluate a Brazilian college-entrance exam, and demonstrate that IRT can provide a more informative evaluation of LLMs , including: the ability to distinguish human-like vs non-human-like response patterns, and to determine whether an exam can reliably measure an LLM's abilities. The empirical results suggest that traditional accuracy metrics are insufficient to assess the abilities of LLMs, and advocate for using IRT/psychometric theory to evaluate them.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Provides more comprehensive evaluation methods for LLM performance, I personally agree that accuracy metrics often do not give a complete picture of LLM ability.
2. The results section is methodological, it evaluates not only the IRT scores but how reliable they are based on several metrics (increases reliability of the evaluations)

Weaknesses:
1. The results analysis would benefit from a more detailed and clearer/deeper analysis, some statements made (eg. L293-298) are high level observations based on the results, but lack further insight into why certain LLM behaviors occur. Performing more detailed analyses into the specific subset of questions that contribute to scores could help to further understand the limitations of the LLM (L328-331 alludes to this, but very briefly).

2.  All the evaluations were done on variations of the ENEM exam dataset, showing that these psychometric method would also work on other datasets would make this approach more convincing that it will work for wider applications - I understand that there is limited time to run more experiments, so this is more so just a comment.

Limitations:
As mentioned above, as experiments are done on variations of one dataset, there are doubts about the generalizability of these methods on other datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on evaluating LLM abilities on a dataset of 8 college-entrance exams in Brazil (translated to English) measuring Item Response Theory instead of Accuracy. It highlights how such metric is useful to better understand models' performance.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I have found the work very well structured and appreciated the amount of care the authors have been given to the preparation of the dataset for the experiments (PDF processing, translation to English, use of exams designed for blind people in order to address questions based on images, etc). The experiments and results are discussed in details, with clear comparisons with human performance, discussing clear differences (e.g. in Mathematics).

Weaknesses:
While the paper is well structured, I felt it was missing a ""what now?"" message. The authors wrote a convincing argument in favour of using IRT, how do we convince now the field of ML / AI to use it more extensively? What are its limitations in comparison with accuracy-based metrics (given there are many, for instance you need information on overall human performance) and how do we overcome them?

Limitations:
I think the work should have discussed more about the specificities of ENEM - I agree with the authors that this is a relevant test-bed for this sort of evaluation, but in which ways are they specific / tailored to Brazil? Is there anything researchers should know about ENEM, which would make future testing / applications more challenging? For instance which topics are covered in Humanities or Languages, how specific are they about the country cultural context?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper initiates the empirical study of the performance of LLMs using Item Response Theory (IRT) models from a large college-entrance exam.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The question of construct validity of LLM evaluations (based on scores in exams designed for humans) is very important. This paper addresses this question in the earnest, by leveraging the primary tool developed in the education and psychometrics field, IRT. As far as I know, this is an original contribution as no previous work has used IRT in the same way to tackle the construct validity issue of LLM evaluations.

- The paper is largely well-written and the concepts (e.g. section 3) are explained clearly.

- Relative strength of empirical work. The experiments are well-designed and there is some breadth in the range of hypotheses explored, e.g. English vs. Portuguese effect on performance, Different topics of exams, response patterns in LLMs vs. humans with questions sorted by IRT difficulty. Seven different LLMs were evaluated.

- Significance. The method of this paper (i.e. using IRT in LLM evals) is an important first step to understanding what LLM evals are trying to measure. The paper already observes interesting phenomena, e.g. 
(1) the Fisher information of the math exam for the LLM test response distributions is low compared to other exams (although this is a somewhat obvious corollary of the p_i's being close to random for the LLMs performance on the math exam, the FI is a metric that points in the right direction).
(2) the joint distribution of IRT scores and CTT scores for LLMs is meaningfully different from that of the human test takers.

Weaknesses:
1. Some of the conclusions drawn by the paper appear unscientific/not well-substantiated. To me, the empirical results are subtle and require more thoughtful interpretations. Most of the interpretations of the experiments are confusing to me (i.e. I'm skeptical the conclusions follow), given the actual plots shown. For example, 

(a) What are ""outlier models"" (line 237)? We cannot see from Figure 1 that ""outlier models ... have higher accuracy and/or lower IRT scores..."" - how is this statement supported?

(b) line 223-224. The scale of IRT scores and CTT scores is not comparable. How can you conclude there is ""greater variability"" in the latter than in the IRT score? This is not scientific.

(c) line 264-265. The statement ""...questions that are easy for humans but difficult for LLMs"" is again inaccurate. The questions are relatively easier for humans but may not be ""easier"" than the other questions for humans, if easier means for humans anyway.

(d) Why is the math exam not meaningful for evaluating LLMs? Doesn't it suggest that the models are randomly guessing and therefore bad? I don't agree with this interpretation.

2. A clarity issue with the math writing. Line 154-155: This sentence ""...j has a more likely response vector than indicated by their ability"" is mathematically wrong. It is not possible to have a random draw from a Multinomial distribution that is ""more likely"" (i.e. higher probability) than the expectation vector (which is not even in the space of possible draws).

3. Experiment section writing missing some details and figures are somewhat difficult to interpret (esp Figure 1). I have several unanswered questions. How was the closed curve generated from the 30 points (of random shuffles)? The caption for Figure 1 could be more informative, e.g. was the exam answered in English or Portuguese by the LLM. If English, are the IRT models fit still valid? - I don't think so. 

4. Typo in lines 232-233, ""Natural sciences"" appears twice. and the sentence contradicts the graph.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a fresh perspective to evaluating LLMs by arguing for a stronger emphasis on psychometric methods particularly Item Response Theory (IRT) when evaluating them on exams designed for humans, rather than the reliance on traditional metrics such as accuracy. The authors postulate that IRT provides a more comprehensive evaluation by considering not just the number of correct answers but also the difficulty of the questions and the patterns of responses. The authors utilize the Brazilian college entrance exam ENEM for their case study and compare how various LLMs fare against human test-takers. They show how psychometric methods can be leveraged to distinguish between human like and non-human like responses. Furthermore, they demonstrate how IRT can be used to assess the suitability of an exam for making meaningful measurements of an LLM's abilities in the given area.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is very well written. It provides a comprehensive literature review and does a good job at covering the background information. The experimental analysis is sound with sufficient supporting materials. The paper makes significant novel contributions to evaluation of LLMs. The application of psychometric methods and the insights that can be mined through them when used to compare LLMs can be of significant interest to the research community. The experimental results on assessing whether an exam is a good indicator of an LLM's ability are particularly interesting and open up significant opportunities for future research.

Weaknesses:
The error analysis can be more detailed especially in areas where the results are surprising. This would better help support the conclusions.       For instance for the questions in Math and Natural Sciences wherein the models show fluctuating performance it would be useful to know what those questions aim to test. Are LLMs not able to solve the problems due to calculation errors or do these problems involve more complex multi-step reasoning or is it just linked to knowledge cutoff (e.g questions involving current events)?

Limitations:
The pre-requisite for this type of evaluation seems to be the existence of a strong IRT model which in turn requires the existence of large amount of carefully annotated human data.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
C3t6GMPnC5;"REVIEW 
Summary:
This paper explores the capabilities of Mamba state-space models (SSMs) in comparison to Transformer large language models (LLMs) in various downstream learning tasks. Despite Mamba's success in some areas, the paper identifies challenges and limitations in achieving performance parity with Transformers on standard benchmarks, particularly in in-context learning (ICL), mixed-precision fine-tuning (MPFT), and parameter-efficient fine-tuning (PEFT). The study demonstrates that while Mamba models have robust recurrent dynamics and can achieve significant speed and memory efficiency gains through fine-tuning techniques, their downstream learning improvements still lag behind those of Transformers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to read.

2. The study shows that Mamba’s recurrent dynamics are robust to small input changes, which is validated both theoretically and empirically. This robustness ensures stability in training and fine-tuning processes.

3. Despite initial shortcomings in ICL performance, extensive experiments demonstrate that mamba models exhibit strong potential for improvement through efficient fine-tuning. The models can achieve up to 81.5% of the ICL performance improvement, highlighting their adaptability with appropriate tuning methods.

Weaknesses:
I appreciate the authors for providing a theoretical analysis to demonstrate the controllability of implementing AMP on Mamba blocks, and the experiments indicate that PEFT is also suitable for Mamba. However, I have several concerns:

1. The authors define the Mamba process as a generalized operation: $x_t=F_{\theta}(x_{t-1},u_t)$, but the actual output of Mamba is $y_t = \bar{C_t}x_t$.Therefore, the theoretical analysis provided in the paper pertains to the stability of the hidden state under small perturbations. Is it possible to extend this analysis directly to the output $y_t$? Since the stability of the hidden state does not necessarily imply the stability of the output.
2. Theorem 1 ensures the feasibility of implementing LoRA on Mamba blocks but focuses on the $W$ matrix, neglecting the consideration of the most crucial transition matrix $\bar{A_t}$ in Mamba. Does this mean that the $\bar{A_t}$ matrix was not subjected to LoRA during fine-tuning? If so, is it possible to consider applying PEFT to the $\bar{A_t}$ matrix as well?
3. As an empirically-driven paper, would it be possible to include more backbones for comparison in future versions? Currently, the only baseline for comparison is Pythia.

Limitations:
The authors raise some limitaions, for example, the lower-precision method is not explored in this paper, but the authors claime to solve them in the future.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores Mamba's downstream learning capabilities through two primary aspects: (i) fine-tuning and (ii) in-context learning. Specifically, it examines the training stability and robustness of fine-tuning when mixed precision is applied, as well as Mamba's ability to perform in-context learning. The contributions of this paper include:

- Theoretical analysis of the stable dynamics of Mamba.
- The theoretical analysis is corroborated by the experiments.
- Experimental demonstration of Mamba's limitations on real datasets in terms of ICL.
- ICL performance improvement of Mamba through fine-tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The theoretical analysis section, although I did not verify the proofs, offers valuable insights and paves the way for more complex analyses in future work.
- This paper is well-motivated.

Weaknesses:
The weakness is majorly from the ICL part. 

In fact, the authors show that pretrained Mamba cannot learn well via ICL, but can learn well after fine-tuning. This fact indicates that this limitation does not come from Mamba architecture itself, which is also consistent with the observation of other works such as [1]. Therefore, the limitations observed in this paper is just a general limitation caused by training recipes, which is not Mamba-specific and has been studied in many works. The solution is also standard, and the improvements are also expected since once trains well, Mamba should be able to perform in-context learning as shown in [1]. 

Moreover, many questions are still unclear. For instance, why does Mamba suffer from such limitations? 

Therefore, the study in terms of the ICL part is lack of depth, novelty, and technical contribution. 

In terms of the mixed precision part, there are also many places that are unclear to me. I suggest the authors to use more space in discussing the speciality of Mamba compared to Transformers, and what structure of Mamba caused this problem. If recurrence is the main cause of the problem, having more experiments of similar models such as GLA, linear attention, etc would also help readers to understand more about the phenomenon. 

---

References

[1] Park, Jongho, et al. ""Can mamba learn how to learn? a comparative study on in-context learning tasks."" ICML 2024.

Limitations:
The work is well-motivated, but the study lacks depth.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper looks at improving start space models or Mamba by enabling mixed precision handling to improve inference and fine-tuning. The results show similar performance with a significantly reduced memory requirement

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
There are extensive results compared to full-precision models
The authors provide a proof of the theorem to back up their claim
The change to the mamba block is clear and easy to implement by others

Weaknesses:
The actual change is relatively minor in quantity but does deliver the author's required memory reductions.
The works don't use the larger models available due to limitations on memory requirements still

Limitations:
the limitations are well discussed at the end of the paper and generally relate to LLM or transformers in general too

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the capabilities of Mamba state-space models (SSMs) in comparison to Transformer large language models (LLMs) in various downstream learning tasks. Despite Mamba's success in some areas, the paper identifies challenges and limitations in achieving performance parity with Transformers on standard benchmarks, particularly in in-context learning (ICL), mixed-precision fine-tuning (MPFT), and parameter-efficient fine-tuning (PEFT). The study demonstrates that while Mamba models have robust recurrent dynamics and can achieve significant speed and memory efficiency gains through fine-tuning techniques, their downstream learning improvements still lag behind those of Transformers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to read.

2. The study shows that Mamba’s recurrent dynamics are robust to small input changes, which is validated both theoretically and empirically. This robustness ensures stability in training and fine-tuning processes.

3. Despite initial shortcomings in ICL performance, extensive experiments demonstrate that mamba models exhibit strong potential for improvement through efficient fine-tuning. The models can achieve up to 81.5% of the ICL performance improvement, highlighting their adaptability with appropriate tuning methods.

Weaknesses:
I appreciate the authors for providing a theoretical analysis to demonstrate the controllability of implementing AMP on Mamba blocks, and the experiments indicate that PEFT is also suitable for Mamba. However, I have several concerns:

1. The authors define the Mamba process as a generalized operation: $x_t=F_{\theta}(x_{t-1},u_t)$, but the actual output of Mamba is $y_t = \bar{C_t}x_t$.Therefore, the theoretical analysis provided in the paper pertains to the stability of the hidden state under small perturbations. Is it possible to extend this analysis directly to the output $y_t$? Since the stability of the hidden state does not necessarily imply the stability of the output.
2. Theorem 1 ensures the feasibility of implementing LoRA on Mamba blocks but focuses on the $W$ matrix, neglecting the consideration of the most crucial transition matrix $\bar{A_t}$ in Mamba. Does this mean that the $\bar{A_t}$ matrix was not subjected to LoRA during fine-tuning? If so, is it possible to consider applying PEFT to the $\bar{A_t}$ matrix as well?
3. As an empirically-driven paper, would it be possible to include more backbones for comparison in future versions? Currently, the only baseline for comparison is Pythia.

Limitations:
The authors raise some limitaions, for example, the lower-precision method is not explored in this paper, but the authors claime to solve them in the future.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores Mamba's downstream learning capabilities through two primary aspects: (i) fine-tuning and (ii) in-context learning. Specifically, it examines the training stability and robustness of fine-tuning when mixed precision is applied, as well as Mamba's ability to perform in-context learning. The contributions of this paper include:

- Theoretical analysis of the stable dynamics of Mamba.
- The theoretical analysis is corroborated by the experiments.
- Experimental demonstration of Mamba's limitations on real datasets in terms of ICL.
- ICL performance improvement of Mamba through fine-tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The theoretical analysis section, although I did not verify the proofs, offers valuable insights and paves the way for more complex analyses in future work.
- This paper is well-motivated.

Weaknesses:
The weakness is majorly from the ICL part. 

In fact, the authors show that pretrained Mamba cannot learn well via ICL, but can learn well after fine-tuning. This fact indicates that this limitation does not come from Mamba architecture itself, which is also consistent with the observation of other works such as [1]. Therefore, the limitations observed in this paper is just a general limitation caused by training recipes, which is not Mamba-specific and has been studied in many works. The solution is also standard, and the improvements are also expected since once trains well, Mamba should be able to perform in-context learning as shown in [1]. 

Moreover, many questions are still unclear. For instance, why does Mamba suffer from such limitations? 

Therefore, the study in terms of the ICL part is lack of depth, novelty, and technical contribution. 

In terms of the mixed precision part, there are also many places that are unclear to me. I suggest the authors to use more space in discussing the speciality of Mamba compared to Transformers, and what structure of Mamba caused this problem. If recurrence is the main cause of the problem, having more experiments of similar models such as GLA, linear attention, etc would also help readers to understand more about the phenomenon. 

---

References

[1] Park, Jongho, et al. ""Can mamba learn how to learn? a comparative study on in-context learning tasks."" ICML 2024.

Limitations:
The work is well-motivated, but the study lacks depth.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper looks at improving start space models or Mamba by enabling mixed precision handling to improve inference and fine-tuning. The results show similar performance with a significantly reduced memory requirement

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
There are extensive results compared to full-precision models
The authors provide a proof of the theorem to back up their claim
The change to the mamba block is clear and easy to implement by others

Weaknesses:
The actual change is relatively minor in quantity but does deliver the author's required memory reductions.
The works don't use the larger models available due to limitations on memory requirements still

Limitations:
the limitations are well discussed at the end of the paper and generally relate to LLM or transformers in general too

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
BQEOJZ0aSD;"REVIEW 
Summary:
This paper presents a new method which directly encourages ensemble diversification on selected ID datapoints without the need for a separate OOD dataset. They also introduce a new measure of epistemic uncertainty which measures the diversity of the final predictions of each model, and suggest a speedup of comparing pairwise disagreement via random sampling.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written, and the presentation of the method is easy to understand. 
- The experiments cover a wide range of OOD datasets.
- The methods are intuitive and can be inexpensively applied to existing ensemble diversification algorithms.
- SED-A2D outperforms other baselines when using uniform soup or prediction ensembles for OOD generalization, and also achieves the highest AUROC for OOD detection.

Weaknesses:
- It appears that utilizing this new training objective leads to a loss in ID accuracy, since it encourages members of the ensembles to diverge. This tradeoff between ID accuracy and OOD accuracy may not be desirable in many settings. Overall, the paper emphasizes the improved OOD performance but does not show its impact on ID data for many experiments, such as the ablation studies for OOD detection, model diversity, etc.
- The stochastic computation of pairwise disagreement seems incremental, and there is no work comparing this stochastic implementation with the traditional expensive one. It would be helpful to include an ablation study to understand the accuracy vs performance tradeoff.
- There are many other methods for OOD detection beyond MSP with BMA (eg [1]). How PDS does compare against other baselines?
- Deep ensembles remain competitive in many settings, and the best values for C-1 and C-5 OOD generalization are still achieved using ensembles. 

[1] Xia and Bouganis: On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution Detection https://arxiv.org/abs/2207.07517

Limitations:
- The approach sacrifices ID accuracy for OOD generalization/detection.
- The experiments only showed the result of finetuning the last two layers.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper aims to train a diverse ensemble of models via a framework called Scalable Ensemble Diversification. This framework does not require an additional dataset of OOD inputs, as it identifies OOD samples from a given ID dataset. It then encourages the ensemble to return diverse predictions (disagreement) on these OOD samples. Furthermore, the framework makes use of stochastic summation to speed up the disagreement computation. Results are shown for different tasks like generalisation, OOD detection on different OOD datasets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The high level idea of removing the need for a separate OOD dataset and speeding up the diversification computation can be useful in practice.
- The writing is clear and easy to follow.

Weaknesses:
1. It is not clear why the method works, additional ablations studies would be useful.   
    - Naive A2D (and DivDis) uses IN-R data to compute the disagreement loss, which could give the method an advantage as it has access to the OOD data. However, it has a lower accuracy on IN-R. There seems to be two methodological differences between A2D and SED-A2D, the OOD data and use of stochastic sum. Given that A2D computes the full pairwise disagreement and stochastic sum is meant to reduce cost rather than improve performance, why does SED-A2D perform better? It would have been useful to compare two methods that only differs on the OOD data used. E.g., SED-A2D without the stochastic sum.   
    - From eqn 6, it looks like the two terms have contradicting objectives. For a “OOD” point, the first term encourages all models to classify the point correctly, but the second term encourages models to have different predictions on the same point. These objectives can be challenging to balance.   
2. The writing clearly explains the method or setup, but sometimes stops short of giving further insights. For example,  
    - Further analysis of experimental results   
        - Table 2 why does having more ensemble component (5→50) make the SED-A2D results worse? Similar trends can also be seen in Tab 4 for C-1 or C-5. Could it be because the stochastic sum does not scale with more models?   
        - Why was #unique used in Tab 1 to measure diversity when the Predictive Diversity Score was just introduced?  
        - Why does oracle selection perform worse compared to simple average in Tab 2? I would expect otherwise given that there is privilege information.   
    - Components of the method can be better motivated   
        - Why was the A2D loss chosen instead of other losses e.g. DivDis?   
        - Why is optimizing Eqn 6 preferable to e.g., forming an OOD dataset from the ID data based on the errors of DeiT or even from the errors from an ensemble of models, similar to imagnet-a, and using existing techniques like [23,28].   
    - “collecting a separate OOD dataset can be very costly, if not impossible”. 
        - There are cheap ways to introduce OOD samples to an ID dataset, e.g., simple augmentations/transformations to the input. Why are these methods not preferable?  
4. One of the main contributions involves speeding up the disagreement computation. There does not seem to be experimental details or results on this. E.g., a subset of models is chosen, what is the size of this subset? How does performance for generalization/detection change with and without this speedup?

Limitations:
Yes the limitations were adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents SED, a method for scaling up existing diversification methods to large-scale datasets and tasks. SED identifies the OOD-like samples from a single dataset, bypassing the need to prepare a separate OOD dataset. Experimental results demonstrated good performances by SED on the OOD generalization and detection tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. SED scales up existing ensemble methods.
2. According to the author's experimental results, SED demonstrates its application to OOD generalization and detection at ImageNet level.
3. Simple method, and easy to understand.

Weaknesses:
1. I am very confused about the dataset division for OOD detection task in the paper. The distribution should refer to “label distribution” in OOD detection [1], which means that OOD samples should not have overlapping labels w.r.t. training data. In the paper, the ID dataset is ImageNet-1K, while the OOD dataset for the OOD detection task includes ImageNet-C (Table 3). Their label spaces overlap, which is clearly incorrect. I don't believe the experiments conducted in this paper fall under the category of OOD detection. I suggest the authors refer to relevant literature on OOD detection, such as OpenOOD [1].

2. The ablation studies are insufficient. For instance, the number of layers being diversified is a hyperparameter. I believe conducting ablation experiments on this would make the paper more solid.

3. I think the experiments in the paper are not comprehensive enough. For example, how does it perform on small-scale datasets? Although it may not be fair to compare with methods using real OOD datasets, this could provide insights into SED's performance from multiple perspectives.

4. The comparative methods in the paper are not comprehensive enough. How does it perform compared to existing OOD generalization and OOD detection methods? If SED is complementary to existing methods, how much improvement can it bring?

5. The paper claims to speed up pairwise divergence computation, but no results are shown. Could authors demonstrate specifically how much speedup was achieved?

6. typos:
6.1 Line 61: ""We verify that SEDdiversifies a model...""
6.2 Line 68: ""In all three cases, SEDachieves a superior generalization...""

[1] Yang et al, OpenOOD: Benchmarking Generalized Out-of-Distribution Detection, IJCV 2024.

Limitations:
I think the author should discuss more about the limitations of the method proposed in the article, such as the computational time required for the method proposed in the article compared with other methods.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Ensembles of diverse models have shown promising signs for out-of-distribution (OOD) generalization.
To boost diversity, some methods require a set of OOD examples for measuring the disagreement among models.
The desired OOD examples, however, can be difficult to obtain in practice.
This paper proposes to dynamically draw OOD samples from the training data during training.
This is done by assigning a higher OOD score to examples with a greater loss in each mini-batch.
To make the diversification process across multiple models more efficient, the authors propose a stochastic approach that only diversifies a small sample of models at each iteration.
The resulting diversified models give rise to the notion of a diversity score for uncertainty estimation and can be used for OOD detection.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper introduces several reasonable improvements to a state-of-the-art method, A2D, making it more scalable and practically feasible.
- The empirical performance looks good. It is a bit surprising that the proposed method can outperform A2D which has access to “true” OOD datasets.

Weaknesses:
- The notion of “OOD samples” in an ID dataset is confusing. The actual implementation, i.e. assigning a higher “OOD-ness” weight to training examples with a greater loss, is more like identifying “hard” training examples rather than just OOD samples. Calling them “OOD samples” somewhat obfuscate their nature. They are not arbitrary OOD samples but hard samples within the support of the ID dataset. It is not obvious why diversifying models’ predictions on such samples would help. Is such prediction diversification always conducive to OOD generalization? If not, when would the proposed method work or break? These relevant theoretical questions are not answered satisfactorily in the current manuscript.
- The connection between SED and PDS is weak; PDS is not well justified. The A2D diversification loss can also be seen as a measure for prediction diversity, like PDS. Why choose PDS instead for OOD detection? Furthermore, is PDS really a good measure for epistemic uncertainty? Imagine two cases. In the first case, two models confidently (with probability 1) predict the same class for an input example, while in the second case, the two models assign uniform probability to all classes for another example. The PDS for these two examples are exactly the same, yet the models are much less confident (or more uncertain) in the second case. Meanwhile, BMA does not have this issue.
- The baselines are relatively limited. There are many other diversification methods which do not require a separate OOD dataset [1, 2, 3]. How does the proposed method compare with these methods? Can the authors also comment on why BMS is the only considered baseline for OOD detection?
- The definition of #unique values is not very clear. Table 1 shows SED-A2D has extremely large #unique values. On C-1 dataset, the value is 5, the maximum possible value. If my understanding is correct, does this suggest that all the 5 models disagree with each other on every C-1 example? If so, this suggests that for many examples, 4 out of 5 models are probably wrong. Why is this more of a good sign than a bad one?

[1] Rame, Alexandre, et al. ""Diverse weight averaging for out-of-distribution generalization."" Advances in Neural Information Processing Systems 35 (2022): 10821-10836.  
[2] Chu, Xu, et al. ""Dna: Domain generalization with diversified neural averaging."" International conference on machine learning. PMLR, 2022.  
[3] Lin, Yong, et al. ""Spurious feature diversification improves out-of-distribution generalization."" arXiv preprint arXiv:2309.17230 (2023).

Limitations:
The authors only briefly mentioned two limitations of the work. I don't notice any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes Scalable Ensemble Diversification (SED) to extend existing diversification methods to large-scale datasets and tasks where ID-OOD separation may not be possible, and also propose Predictive Diversity Score (PDS) as a novel measure for epistemic uncertainty. Extensive analysis and experiments support the effectiveness of the proposed modules.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The logic of this paper is very clear, the motivation is reasonable, and the proposed method has been proven to be effective in analysis and experiments. The figures and tables in the paper are also relatively clear.

Weaknesses:
1. Although the experiments are diverse, I am not sure if the comparison is comprehensive. Can more explanation and discussion be added?

2. The feature extractor used is frozen. Is the proposed method robust enough to different feature extractors? What will the performance be if the feature extractor is also involved in the training?

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a new method which directly encourages ensemble diversification on selected ID datapoints without the need for a separate OOD dataset. They also introduce a new measure of epistemic uncertainty which measures the diversity of the final predictions of each model, and suggest a speedup of comparing pairwise disagreement via random sampling.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written, and the presentation of the method is easy to understand. 
- The experiments cover a wide range of OOD datasets.
- The methods are intuitive and can be inexpensively applied to existing ensemble diversification algorithms.
- SED-A2D outperforms other baselines when using uniform soup or prediction ensembles for OOD generalization, and also achieves the highest AUROC for OOD detection.

Weaknesses:
- It appears that utilizing this new training objective leads to a loss in ID accuracy, since it encourages members of the ensembles to diverge. This tradeoff between ID accuracy and OOD accuracy may not be desirable in many settings. Overall, the paper emphasizes the improved OOD performance but does not show its impact on ID data for many experiments, such as the ablation studies for OOD detection, model diversity, etc.
- The stochastic computation of pairwise disagreement seems incremental, and there is no work comparing this stochastic implementation with the traditional expensive one. It would be helpful to include an ablation study to understand the accuracy vs performance tradeoff.
- There are many other methods for OOD detection beyond MSP with BMA (eg [1]). How PDS does compare against other baselines?
- Deep ensembles remain competitive in many settings, and the best values for C-1 and C-5 OOD generalization are still achieved using ensembles. 

[1] Xia and Bouganis: On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution Detection https://arxiv.org/abs/2207.07517

Limitations:
- The approach sacrifices ID accuracy for OOD generalization/detection.
- The experiments only showed the result of finetuning the last two layers.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper aims to train a diverse ensemble of models via a framework called Scalable Ensemble Diversification. This framework does not require an additional dataset of OOD inputs, as it identifies OOD samples from a given ID dataset. It then encourages the ensemble to return diverse predictions (disagreement) on these OOD samples. Furthermore, the framework makes use of stochastic summation to speed up the disagreement computation. Results are shown for different tasks like generalisation, OOD detection on different OOD datasets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The high level idea of removing the need for a separate OOD dataset and speeding up the diversification computation can be useful in practice.
- The writing is clear and easy to follow.

Weaknesses:
1. It is not clear why the method works, additional ablations studies would be useful.   
    - Naive A2D (and DivDis) uses IN-R data to compute the disagreement loss, which could give the method an advantage as it has access to the OOD data. However, it has a lower accuracy on IN-R. There seems to be two methodological differences between A2D and SED-A2D, the OOD data and use of stochastic sum. Given that A2D computes the full pairwise disagreement and stochastic sum is meant to reduce cost rather than improve performance, why does SED-A2D perform better? It would have been useful to compare two methods that only differs on the OOD data used. E.g., SED-A2D without the stochastic sum.   
    - From eqn 6, it looks like the two terms have contradicting objectives. For a “OOD” point, the first term encourages all models to classify the point correctly, but the second term encourages models to have different predictions on the same point. These objectives can be challenging to balance.   
2. The writing clearly explains the method or setup, but sometimes stops short of giving further insights. For example,  
    - Further analysis of experimental results   
        - Table 2 why does having more ensemble component (5→50) make the SED-A2D results worse? Similar trends can also be seen in Tab 4 for C-1 or C-5. Could it be because the stochastic sum does not scale with more models?   
        - Why was #unique used in Tab 1 to measure diversity when the Predictive Diversity Score was just introduced?  
        - Why does oracle selection perform worse compared to simple average in Tab 2? I would expect otherwise given that there is privilege information.   
    - Components of the method can be better motivated   
        - Why was the A2D loss chosen instead of other losses e.g. DivDis?   
        - Why is optimizing Eqn 6 preferable to e.g., forming an OOD dataset from the ID data based on the errors of DeiT or even from the errors from an ensemble of models, similar to imagnet-a, and using existing techniques like [23,28].   
    - “collecting a separate OOD dataset can be very costly, if not impossible”. 
        - There are cheap ways to introduce OOD samples to an ID dataset, e.g., simple augmentations/transformations to the input. Why are these methods not preferable?  
4. One of the main contributions involves speeding up the disagreement computation. There does not seem to be experimental details or results on this. E.g., a subset of models is chosen, what is the size of this subset? How does performance for generalization/detection change with and without this speedup?

Limitations:
Yes the limitations were adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents SED, a method for scaling up existing diversification methods to large-scale datasets and tasks. SED identifies the OOD-like samples from a single dataset, bypassing the need to prepare a separate OOD dataset. Experimental results demonstrated good performances by SED on the OOD generalization and detection tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. SED scales up existing ensemble methods.
2. According to the author's experimental results, SED demonstrates its application to OOD generalization and detection at ImageNet level.
3. Simple method, and easy to understand.

Weaknesses:
1. I am very confused about the dataset division for OOD detection task in the paper. The distribution should refer to “label distribution” in OOD detection [1], which means that OOD samples should not have overlapping labels w.r.t. training data. In the paper, the ID dataset is ImageNet-1K, while the OOD dataset for the OOD detection task includes ImageNet-C (Table 3). Their label spaces overlap, which is clearly incorrect. I don't believe the experiments conducted in this paper fall under the category of OOD detection. I suggest the authors refer to relevant literature on OOD detection, such as OpenOOD [1].

2. The ablation studies are insufficient. For instance, the number of layers being diversified is a hyperparameter. I believe conducting ablation experiments on this would make the paper more solid.

3. I think the experiments in the paper are not comprehensive enough. For example, how does it perform on small-scale datasets? Although it may not be fair to compare with methods using real OOD datasets, this could provide insights into SED's performance from multiple perspectives.

4. The comparative methods in the paper are not comprehensive enough. How does it perform compared to existing OOD generalization and OOD detection methods? If SED is complementary to existing methods, how much improvement can it bring?

5. The paper claims to speed up pairwise divergence computation, but no results are shown. Could authors demonstrate specifically how much speedup was achieved?

6. typos:
6.1 Line 61: ""We verify that SEDdiversifies a model...""
6.2 Line 68: ""In all three cases, SEDachieves a superior generalization...""

[1] Yang et al, OpenOOD: Benchmarking Generalized Out-of-Distribution Detection, IJCV 2024.

Limitations:
I think the author should discuss more about the limitations of the method proposed in the article, such as the computational time required for the method proposed in the article compared with other methods.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Ensembles of diverse models have shown promising signs for out-of-distribution (OOD) generalization.
To boost diversity, some methods require a set of OOD examples for measuring the disagreement among models.
The desired OOD examples, however, can be difficult to obtain in practice.
This paper proposes to dynamically draw OOD samples from the training data during training.
This is done by assigning a higher OOD score to examples with a greater loss in each mini-batch.
To make the diversification process across multiple models more efficient, the authors propose a stochastic approach that only diversifies a small sample of models at each iteration.
The resulting diversified models give rise to the notion of a diversity score for uncertainty estimation and can be used for OOD detection.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper introduces several reasonable improvements to a state-of-the-art method, A2D, making it more scalable and practically feasible.
- The empirical performance looks good. It is a bit surprising that the proposed method can outperform A2D which has access to “true” OOD datasets.

Weaknesses:
- The notion of “OOD samples” in an ID dataset is confusing. The actual implementation, i.e. assigning a higher “OOD-ness” weight to training examples with a greater loss, is more like identifying “hard” training examples rather than just OOD samples. Calling them “OOD samples” somewhat obfuscate their nature. They are not arbitrary OOD samples but hard samples within the support of the ID dataset. It is not obvious why diversifying models’ predictions on such samples would help. Is such prediction diversification always conducive to OOD generalization? If not, when would the proposed method work or break? These relevant theoretical questions are not answered satisfactorily in the current manuscript.
- The connection between SED and PDS is weak; PDS is not well justified. The A2D diversification loss can also be seen as a measure for prediction diversity, like PDS. Why choose PDS instead for OOD detection? Furthermore, is PDS really a good measure for epistemic uncertainty? Imagine two cases. In the first case, two models confidently (with probability 1) predict the same class for an input example, while in the second case, the two models assign uniform probability to all classes for another example. The PDS for these two examples are exactly the same, yet the models are much less confident (or more uncertain) in the second case. Meanwhile, BMA does not have this issue.
- The baselines are relatively limited. There are many other diversification methods which do not require a separate OOD dataset [1, 2, 3]. How does the proposed method compare with these methods? Can the authors also comment on why BMS is the only considered baseline for OOD detection?
- The definition of #unique values is not very clear. Table 1 shows SED-A2D has extremely large #unique values. On C-1 dataset, the value is 5, the maximum possible value. If my understanding is correct, does this suggest that all the 5 models disagree with each other on every C-1 example? If so, this suggests that for many examples, 4 out of 5 models are probably wrong. Why is this more of a good sign than a bad one?

[1] Rame, Alexandre, et al. ""Diverse weight averaging for out-of-distribution generalization."" Advances in Neural Information Processing Systems 35 (2022): 10821-10836.  
[2] Chu, Xu, et al. ""Dna: Domain generalization with diversified neural averaging."" International conference on machine learning. PMLR, 2022.  
[3] Lin, Yong, et al. ""Spurious feature diversification improves out-of-distribution generalization."" arXiv preprint arXiv:2309.17230 (2023).

Limitations:
The authors only briefly mentioned two limitations of the work. I don't notice any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes Scalable Ensemble Diversification (SED) to extend existing diversification methods to large-scale datasets and tasks where ID-OOD separation may not be possible, and also propose Predictive Diversity Score (PDS) as a novel measure for epistemic uncertainty. Extensive analysis and experiments support the effectiveness of the proposed modules.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The logic of this paper is very clear, the motivation is reasonable, and the proposed method has been proven to be effective in analysis and experiments. The figures and tables in the paper are also relatively clear.

Weaknesses:
1. Although the experiments are diverse, I am not sure if the comparison is comprehensive. Can more explanation and discussion be added?

2. The feature extractor used is frozen. Is the proposed method robust enough to different feature extractors? What will the performance be if the feature extractor is also involved in the training?

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
B3jt0Ran2t;"REVIEW 
Summary:
The paper describes the problem of matching students to daycare centers, with each family allowed to express preferences about the joint allocation of all siblings within the family. The authors present a modified notion of a stable matching, in which a family may choose to withdraw one of its children from a daycare in favor of a different child, so long as the daycare still prefers this assignment among alternatives with the first child removed. Under this stronger notion of stability, they show that the existing SDA algorithm may produce unstable outputs. They present an extension to the algorithm ESDA, whose successful outputs meet the new stability condition. They also show that the algorithm will be successful with high probability for a particular distribution over problem instances. In this distribution, children populate a daycare preference order by selecting from a fixed distribution over the daycares, and families aggregate these preferences into preferences over joint allocations, using an arbitrary aggregation function. Daycares sample a preference order over children from a Mallows model, with low dispersion. 
Finally, the authors show some empirical results from real Japanese municipalities in which ESDA produces stable matchings in all cases.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The algorithm is a heuristic (for good reasons of computational hardness of the problem). Hence, I characterize the contributions as follows:
* The authors define the problem, generalizing from couple-matching instances that may be expressed as families of size at most 2
* The authors present a new notion of stability in the matching which seems to be justified, given that families are generally empowered to prevent one of their children from attending a particular daycare.
* The authors present the ESDA algorithm, which internalizes the new stability notion in the details of the algorithm execution, and produces stable matchings whenever the algorithm succeeds
* Empirical analysis shows the algorithm succeeding on real-world instances
* Finally, the authors show that real-world instances have some strong properties in terms of the similarity across daycares of the preference ordering for students. They also incorporate this observation into the algorithm design, and are able to show that under a certain random model of problem instances, the algorithm succeeds with high probability. In the real-world examples, the preferences of the daycare are largely provided by the municipality, so the assumption is very likely to hold.
* As a smaller point, I appreciate that the authors presented some analysis of the behavior of the algorithm when the dispersion of the mallows models becomes high.
* An additional smaller point: earlier results that operate with a vanishing fraction of couples in the population seem unsatisfying. The theoretical results in this paper instead allow a constant fraction of the families to have siblings, but place stronger constraints on the similarity of orderings of the daycares, which seems better justified.

Weaknesses:
My first question is about goodness of fit of the paper to NeurIPS. The best fit from the CFP is:
* Social and economic aspects of machine learning (e.g., fairness, interpretability, human-AI interaction, privacy, safety, strategic behavior)
specifically for strategic behavior. However, I'm not sure this should be called economic aspects *of machine learning* specifically. I'll leave this issue with the area chair---my personal view is that it's not a great match. There are some related papers that have appeared in past conference instances (for instance, on deferred acceptance variants, but with more of a focus on computational complexity of an algorithmic approach, such as https://papers.nips.cc/paper_files/paper/2019/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html).

Second, I'm concerned about the family preference model in section 4.1. In particular, the title of the paper says ""Large Markets,"" and as the size of the market grows beyond a small geographic area, it seems like that geographic preferences (for nearby daycares) will play a role. However, the random model is based on a single global distribution of preferences that applies to all families across all locations. This distribution is then further constrained to place similar probabilities on all daycares. There is no analysis of the empirical data to justify this uniformity assumption. Additionally, the model assigns independent preferences to two siblings of the same family, which seems to miss a) the fact that a family may have certain specific desires, and b) the family's geo preferences will apply similarly to all children, and c) sending multiple siblings to the same daycare may provide complementarities such as less logistic overhead for transport.

Limitations:
I think the authors have done a good job here.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study a variant of the many-to-one matching problem called the stable matching problem with siblings, which generalizes the stable matching problem with couples. In this problem, some families $f \in F$ may have more than one and at most $k$ siblings, ordered by age $(c_1,\dots,c_k)$. Each family $f = (c_1,\dots,c_k)$ expresses a joint linear preference order for daycares, denoted as 
$>_f \subseteq D \times D \dots \times D$,

 where $>_{f,j} = (d_1,\dots,d_k)$ represents the $j$th preference of family $f$, and $d_i$ corresponds to the preference for child $c_i$. Note that $>_f$ is an ordered set—a tuple. Each daycare $d \in D$ expresses a linear preference order $>_d \subseteq C$ for a subset of children and a maximum capacity $Q(d)$.

The objective is to find a (stable) matching such that no blocked pair exists. In essence, a blocked pair is a tuple (of edges) $(x_1,\dots,x_\ell)$ and $(y_1,\dots,y_\ell)$ such that swapping $x_i$ with $y_i$ results in a new matching that assigns children to daycares with higher priority for at least one family while not negatively impacting the assignment of any other family or daycare preferences.

A stable matching might not exist for restrictive settings of the problem, as the authors illustrate with a simple example in Appendix B.3. My understanding is that if the preferences form cycles, it becomes impossible to find a feasible solution that satisfies these preference constraints. However, in a daycare market where priorities are generated from a specific distribution, particularly random, the authors demonstrate that the probability of a stable matching existing converges to $1$ as the number of children $n$ approaches infinity.They present algorithms to solve the problem and conduct experiments on synthetic and real-world datasets, demonstrating that they can find feasible solutions in most instances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses a challenging and relevant variant of the many-to-one matching problem, the stable matching problem with siblings. The authors provide a comprehensive approach by defining the problem, presenting algorithms to solve it, and conducting thorough experiments on both synthetic and real-world datasets. Their work not only demonstrates the feasibility of finding stable matchings under specific conditions but also highlights the practical applications and implications for real-world daycare allocation scenarios.

Weaknesses:
While the paper makes significant contributions, there are some areas that could be improved. The writing is occasionally imprecise, making it challenging to follow the arguments and understand the definitions clearly. In particular, the choice of notation can be confusing (see detailed comments and questions). The structure of the paper is somewhat disorganized, with most of the proofs deferred to the appendix. Considering the strict page limits, this may be reasonable. However, Sections 3 and 4 could be compressed and written more concisely, and some proofs (or at least proof sketches) can be included in the main paper. I have only reviewed the proofs at a high level and have not verified the claims in sufficient detail. Given the strict reviewing timeline, this is the best I can do.

Limitations:
The authors do not discuss limitations and potential negative social impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the problem of daycare matching with siblings, an extension of matching with couples. Here, children in families (of size 1 or larger) are matched to daycares. Families have ranked preferences over the tuples of daycares their children end up at (since their preference for one child at one daycare may affect their preference of another child at some daycare), and daycares have preferences over children. In most cases, daycares do not differentiate between children within a family. This is an important problem to solve in Japan, and the authors actually worked with the Japanese daycare matching market in order to produce this work.

Their contributions are: 1) introduce the problem along with notions of rationality/stability/assumptions/etc, 2) propose an extended sorted deferred acceptance algorithm and prove that it will only return stable matchings and will fail to recognize a possible stable matching with probability approaching 1 as the problem grows, and 3) run experiments on their algorithm.

Their model is defined in a pretty standard way according to stable matching literature. The novelty, of course, is the introduction of families generalizing the size of couples. Their stability definition uniquely allows children in the same family to pass along seats to each other, so that a family may use that to their advantage in forming a blocking coalition. They assume that daycares have similar rankings over children and that they are drawn according to the Mallows Model, and that families only have few daycares they are interested in.

The algorithm itself works much like deferred acceptance. First, single children can propose to daycares per usual. Then, families with multiple children begin proposing, presumably according to their full ranking of matching tuples. When a single child is unseated from a daycare, they can simply propose to their next choice. When a family f has an unseated child when family f' is processed, the algorithm attempts again under a new order where f' goes before f. This can cause many iterations.

In the experiments, they use real datasets from Japan as well as larger synthetically-generated datasets. They compare their algorithm to a baseline constraint programming solution, showing that their algorithm returns the same solution faster.

Quick note: diameter is introduced in the main body but only used in the appendix. Perhaps move it to the appendix.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Stable matching is a very well-respected area of research, and this seems like a very natural formulation of the problem. It is particularly interesting that the authors are working directly with the market in need and they seem to have been given positive feedback about their work, so this work will almost definitely have a valuable use case. For the most part, the paper is written very well and it is very easy to get a high level understanding of most aspects of the project. Overall, I am very pleased with this paper and would be excited to see it at NeurIPS.

Weaknesses:
I am a bit concerned about the literature review provided. I am aware there is much more research that has been conducted on matching markets with complementaries (I am not knowledgeable enough to know what papers would be most useful), and I know there are various papers in this field. However, very few previous works are cited in this paper. It would be great if the authors could clarify the place of their work in the context of current literature and give confidence that this problem or a generalization of it has not already been studied. In fact, this is very important to motivate the paper.

Otherwise, there are a few points in the paper that are unclear. Much of it is very high level and lacks details, which is okay because it writes a narrative, but it comes at a cost of understanding the details of the proofs. More notably, I think the authors didn't spend enough time explaining their algorithm. I found it somewhat vague and I was uncertain about how it worked, and yet it is an integral part of the paper. This definitely needs to be improved.

Limitations:
Everything seems adequate.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the existence of a stable daycare-children matching in the presence of siblings from the same families with same preferences over the daycares. The authors particularly study the case when the daycares have similar preferences over the set of children, and the market size is large. They propose a variant of the Sorted Deferred Acceptance algorithm to compute the stable matchings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The problem is well motivated by the real-world observation that stable matchings exist in the markets as opposed to what the theory suggests. This observation allowed the authors to make necessary adjustments to the assumptions that are sufficient for the theory to work out.
2. The authors take a systematic approach to the problem. They first define a new notion of stability that takes siblings into consideration and show that stable matchings may not exist in the presence of siblings and that the previous algorithms do not work for this new notion of stability. They then consider a specific random daycare market, mention the drawbacks of the existing methods of computing stable matchings, and then prove that a modification to the existing algorithm can find stable matchings with the new definition of stability.
3. The results by themselves are quite interesting; that stable matchings exist even in the presence of siblings with complementaries.
4. The analogy is drawn between the related work in stable matchings with couples and stable matchings with siblings

Weaknesses:
1. The assumption that day cares have similar priorities over children is slightly unrealistic.
2. The random daycare market for which the results are derived is somewhat restrictive.

Limitations:
Limitations sufficiently addressed by the authors.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper describes the problem of matching students to daycare centers, with each family allowed to express preferences about the joint allocation of all siblings within the family. The authors present a modified notion of a stable matching, in which a family may choose to withdraw one of its children from a daycare in favor of a different child, so long as the daycare still prefers this assignment among alternatives with the first child removed. Under this stronger notion of stability, they show that the existing SDA algorithm may produce unstable outputs. They present an extension to the algorithm ESDA, whose successful outputs meet the new stability condition. They also show that the algorithm will be successful with high probability for a particular distribution over problem instances. In this distribution, children populate a daycare preference order by selecting from a fixed distribution over the daycares, and families aggregate these preferences into preferences over joint allocations, using an arbitrary aggregation function. Daycares sample a preference order over children from a Mallows model, with low dispersion. 
Finally, the authors show some empirical results from real Japanese municipalities in which ESDA produces stable matchings in all cases.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The algorithm is a heuristic (for good reasons of computational hardness of the problem). Hence, I characterize the contributions as follows:
* The authors define the problem, generalizing from couple-matching instances that may be expressed as families of size at most 2
* The authors present a new notion of stability in the matching which seems to be justified, given that families are generally empowered to prevent one of their children from attending a particular daycare.
* The authors present the ESDA algorithm, which internalizes the new stability notion in the details of the algorithm execution, and produces stable matchings whenever the algorithm succeeds
* Empirical analysis shows the algorithm succeeding on real-world instances
* Finally, the authors show that real-world instances have some strong properties in terms of the similarity across daycares of the preference ordering for students. They also incorporate this observation into the algorithm design, and are able to show that under a certain random model of problem instances, the algorithm succeeds with high probability. In the real-world examples, the preferences of the daycare are largely provided by the municipality, so the assumption is very likely to hold.
* As a smaller point, I appreciate that the authors presented some analysis of the behavior of the algorithm when the dispersion of the mallows models becomes high.
* An additional smaller point: earlier results that operate with a vanishing fraction of couples in the population seem unsatisfying. The theoretical results in this paper instead allow a constant fraction of the families to have siblings, but place stronger constraints on the similarity of orderings of the daycares, which seems better justified.

Weaknesses:
My first question is about goodness of fit of the paper to NeurIPS. The best fit from the CFP is:
* Social and economic aspects of machine learning (e.g., fairness, interpretability, human-AI interaction, privacy, safety, strategic behavior)
specifically for strategic behavior. However, I'm not sure this should be called economic aspects *of machine learning* specifically. I'll leave this issue with the area chair---my personal view is that it's not a great match. There are some related papers that have appeared in past conference instances (for instance, on deferred acceptance variants, but with more of a focus on computational complexity of an algorithmic approach, such as https://papers.nips.cc/paper_files/paper/2019/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html).

Second, I'm concerned about the family preference model in section 4.1. In particular, the title of the paper says ""Large Markets,"" and as the size of the market grows beyond a small geographic area, it seems like that geographic preferences (for nearby daycares) will play a role. However, the random model is based on a single global distribution of preferences that applies to all families across all locations. This distribution is then further constrained to place similar probabilities on all daycares. There is no analysis of the empirical data to justify this uniformity assumption. Additionally, the model assigns independent preferences to two siblings of the same family, which seems to miss a) the fact that a family may have certain specific desires, and b) the family's geo preferences will apply similarly to all children, and c) sending multiple siblings to the same daycare may provide complementarities such as less logistic overhead for transport.

Limitations:
I think the authors have done a good job here.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study a variant of the many-to-one matching problem called the stable matching problem with siblings, which generalizes the stable matching problem with couples. In this problem, some families $f \in F$ may have more than one and at most $k$ siblings, ordered by age $(c_1,\dots,c_k)$. Each family $f = (c_1,\dots,c_k)$ expresses a joint linear preference order for daycares, denoted as 
$>_f \subseteq D \times D \dots \times D$,

 where $>_{f,j} = (d_1,\dots,d_k)$ represents the $j$th preference of family $f$, and $d_i$ corresponds to the preference for child $c_i$. Note that $>_f$ is an ordered set—a tuple. Each daycare $d \in D$ expresses a linear preference order $>_d \subseteq C$ for a subset of children and a maximum capacity $Q(d)$.

The objective is to find a (stable) matching such that no blocked pair exists. In essence, a blocked pair is a tuple (of edges) $(x_1,\dots,x_\ell)$ and $(y_1,\dots,y_\ell)$ such that swapping $x_i$ with $y_i$ results in a new matching that assigns children to daycares with higher priority for at least one family while not negatively impacting the assignment of any other family or daycare preferences.

A stable matching might not exist for restrictive settings of the problem, as the authors illustrate with a simple example in Appendix B.3. My understanding is that if the preferences form cycles, it becomes impossible to find a feasible solution that satisfies these preference constraints. However, in a daycare market where priorities are generated from a specific distribution, particularly random, the authors demonstrate that the probability of a stable matching existing converges to $1$ as the number of children $n$ approaches infinity.They present algorithms to solve the problem and conduct experiments on synthetic and real-world datasets, demonstrating that they can find feasible solutions in most instances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses a challenging and relevant variant of the many-to-one matching problem, the stable matching problem with siblings. The authors provide a comprehensive approach by defining the problem, presenting algorithms to solve it, and conducting thorough experiments on both synthetic and real-world datasets. Their work not only demonstrates the feasibility of finding stable matchings under specific conditions but also highlights the practical applications and implications for real-world daycare allocation scenarios.

Weaknesses:
While the paper makes significant contributions, there are some areas that could be improved. The writing is occasionally imprecise, making it challenging to follow the arguments and understand the definitions clearly. In particular, the choice of notation can be confusing (see detailed comments and questions). The structure of the paper is somewhat disorganized, with most of the proofs deferred to the appendix. Considering the strict page limits, this may be reasonable. However, Sections 3 and 4 could be compressed and written more concisely, and some proofs (or at least proof sketches) can be included in the main paper. I have only reviewed the proofs at a high level and have not verified the claims in sufficient detail. Given the strict reviewing timeline, this is the best I can do.

Limitations:
The authors do not discuss limitations and potential negative social impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the problem of daycare matching with siblings, an extension of matching with couples. Here, children in families (of size 1 or larger) are matched to daycares. Families have ranked preferences over the tuples of daycares their children end up at (since their preference for one child at one daycare may affect their preference of another child at some daycare), and daycares have preferences over children. In most cases, daycares do not differentiate between children within a family. This is an important problem to solve in Japan, and the authors actually worked with the Japanese daycare matching market in order to produce this work.

Their contributions are: 1) introduce the problem along with notions of rationality/stability/assumptions/etc, 2) propose an extended sorted deferred acceptance algorithm and prove that it will only return stable matchings and will fail to recognize a possible stable matching with probability approaching 1 as the problem grows, and 3) run experiments on their algorithm.

Their model is defined in a pretty standard way according to stable matching literature. The novelty, of course, is the introduction of families generalizing the size of couples. Their stability definition uniquely allows children in the same family to pass along seats to each other, so that a family may use that to their advantage in forming a blocking coalition. They assume that daycares have similar rankings over children and that they are drawn according to the Mallows Model, and that families only have few daycares they are interested in.

The algorithm itself works much like deferred acceptance. First, single children can propose to daycares per usual. Then, families with multiple children begin proposing, presumably according to their full ranking of matching tuples. When a single child is unseated from a daycare, they can simply propose to their next choice. When a family f has an unseated child when family f' is processed, the algorithm attempts again under a new order where f' goes before f. This can cause many iterations.

In the experiments, they use real datasets from Japan as well as larger synthetically-generated datasets. They compare their algorithm to a baseline constraint programming solution, showing that their algorithm returns the same solution faster.

Quick note: diameter is introduced in the main body but only used in the appendix. Perhaps move it to the appendix.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Stable matching is a very well-respected area of research, and this seems like a very natural formulation of the problem. It is particularly interesting that the authors are working directly with the market in need and they seem to have been given positive feedback about their work, so this work will almost definitely have a valuable use case. For the most part, the paper is written very well and it is very easy to get a high level understanding of most aspects of the project. Overall, I am very pleased with this paper and would be excited to see it at NeurIPS.

Weaknesses:
I am a bit concerned about the literature review provided. I am aware there is much more research that has been conducted on matching markets with complementaries (I am not knowledgeable enough to know what papers would be most useful), and I know there are various papers in this field. However, very few previous works are cited in this paper. It would be great if the authors could clarify the place of their work in the context of current literature and give confidence that this problem or a generalization of it has not already been studied. In fact, this is very important to motivate the paper.

Otherwise, there are a few points in the paper that are unclear. Much of it is very high level and lacks details, which is okay because it writes a narrative, but it comes at a cost of understanding the details of the proofs. More notably, I think the authors didn't spend enough time explaining their algorithm. I found it somewhat vague and I was uncertain about how it worked, and yet it is an integral part of the paper. This definitely needs to be improved.

Limitations:
Everything seems adequate.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the existence of a stable daycare-children matching in the presence of siblings from the same families with same preferences over the daycares. The authors particularly study the case when the daycares have similar preferences over the set of children, and the market size is large. They propose a variant of the Sorted Deferred Acceptance algorithm to compute the stable matchings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The problem is well motivated by the real-world observation that stable matchings exist in the markets as opposed to what the theory suggests. This observation allowed the authors to make necessary adjustments to the assumptions that are sufficient for the theory to work out.
2. The authors take a systematic approach to the problem. They first define a new notion of stability that takes siblings into consideration and show that stable matchings may not exist in the presence of siblings and that the previous algorithms do not work for this new notion of stability. They then consider a specific random daycare market, mention the drawbacks of the existing methods of computing stable matchings, and then prove that a modification to the existing algorithm can find stable matchings with the new definition of stability.
3. The results by themselves are quite interesting; that stable matchings exist even in the presence of siblings with complementaries.
4. The analogy is drawn between the related work in stable matchings with couples and stable matchings with siblings

Weaknesses:
1. The assumption that day cares have similar priorities over children is slightly unrealistic.
2. The random daycare market for which the results are derived is somewhat restrictive.

Limitations:
Limitations sufficiently addressed by the authors.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
B1tCaKP5nB;"REVIEW 
Summary:
The authors proposes a method for testing conditional independence in presence of discretisation.
They assume the variables to be jointly Gaussian, and that some of them are accessible only after discretisation; thus the data contain a mix of discrete and continuous variables.
Discretization might remove some conditional independencies. Assume X1 to be independent from X2 given X3.
It might be that X1 becomes instead dependent on X2 given \tilde{X3}, where \tilde{X3} is the discretised  X3.
The authors develop a way to infer the latent correlation on the real value variables and they propose a novel test for conditional independence for  the setting mixed continuous and discrete variables.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Testing conditional independence with mixed types of variables is an interesting topic and the work is original. 
The presentation is good, even though I could not follow the development of the bridge equations (this might be because I am not familiar with the adopted techniques).

Weaknesses:
* I am skeptical about the specific research question addressed.
X1 and X3 are independent given X2; yet they might be not independent given the discretised version of X2.

For instance, with ref to  Fig 1a,   X1 and X3 are formally dependent given \tilde{X2}; yet the induced dependence might be very weak.
I argue that the strength of the induced dependence depends on how the discretisation is done.
Example: X2 is human height, discretized into bins of few centimeters; then \tilde{X2}  is practically as informative as X2 and the induced dependence is likely to be negligible, in which case it might be sensible not rejecting H0.

The author did not discuss the impact  of the adopted discretization approach on the induced dependence.
Also, there is no compelling example in which discretisation induces a strong dependence.


* In the first set of experiments the test is better calibrated than the competitors, but it has by far less power. Overall, these results are not very strong.


* The competitor tests (Z-test and chi-square) are not modern.
There is no comparison against  existing tests for mixed variables; I can cite for instance Bayesian Independence Test with Mixed-type Variables, Benavoli et al. 2021.
Another simple baseline which I think should be present: test conditional independence having discretized all variables and use modern test for discrete variables (as a starting point, I suggest  those available in bnlearn https://www.bnlearn.com/documentation/man/conditional.independence.tests.html )

Limitations:
No potential negative societal impact.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Authors propose a test for conditional independence in case of discretized variables, i.e. variables that originally were defined over a continuous domain and are then mapped to a discrete domain. In this case, a binary domain. Authors propose to bridge the unobserved continuous variables with the observed discretized variables with equations modeling the original covariance/precision matrix coefficients. Both theoretical and experimental evidence support the proposed testing  methods. 

Typo at page 13, line 491, equation 17, missing closed bracket.

Citation 3, 4 are the same reference.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper contributes in a significant way on a topic which is crucial in the context of structure learning/causal discovery. Specifically, the proposed theoretical framework is solid and sound, explaining the logical steps that lead to the conditional independence test. The major strength points of this contribution are:

- The self-contained graphical representation of the discretized variables and their original continuous ones,
- The flexibility of the bridge equations, that can be adapted to specific cases without compromising the theoretical soundness.
- The performance of both the unconditional and the conditional independence test.

Weaknesses:
The only weakness is that I would do more experiments.

Limitations:
The only limitation, that is also discussed by authors, is that the ""discretized"" variables are in fact ""binary"" variables, which limits the applicability of the proposed test.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a novel statistical method for testing conditional independence (CI) when some of the data is discretized. Initially, the authors introduce bridge equations to estimate covariance and establish asymptotic normality, facilitating an unconditional independence test. For the conditional independence test, they employ nodewise regression to recover precision coefficients. Theoretical analysis and empirical validation are provided to showcase the method’s effectiveness.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper introduces a conditional independence test tailored for scenarios with discretized data, which often encountered in financial analysis and healthcare due to data collection or measurement constraints. The CI test is highly adaptable, capable of handling situations where both variables are discretized, both are continuous, or one is discretized. Numerical experiments on both synthetic and real-world datasets demonstrate superior performance in various scenarios.

Weaknesses:
The development relies on the assumption of a multivariate Gaussian distribution, which is rather stringent.

Limitations:
The development relies on the assumption of a multivariate Gaussian distribution, which is rather stringent.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses a critical issue in Conditional Independence (CI) testing methods, specifically when the available data is a discretized version of the original continuous data. Traditional CI testing methods often assume that discretized observations can directly substitute for continuous variables, leading to erroneous conclusions. To overcome this limitation, the authors introduce a novel CI test tailored for discretized data. The key innovation lies in using a bridge equation and nodewise regression to estimate the precision coefficients that reflect CI relationships among latent continuous variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper tackles a highly relevant and important problem within the realm of statistical analysis and CI testing.
The paper is well-written and presents the concepts clearly.
The proposed method is novel, and the theoretical contributions are solid, providing a robust foundation for CI testing in discretized data settings.

Weaknesses:
Assumption of Multivariate Normality: A primary limitation is the assumption that the data follow a multivariate normal distribution. This assumption simplifies the derivation of bridge equations for unconditional independence testing and the use of nodewise regression for the CI test. However, it restricts the applicability of the method to this specific class of variables. It is unclear how the method would perform with unknown or non-normal variables.

Discretization Modeling: The paper models discretization as a binarization operation applied to observed variables. This assumption may not hold in all practical scenarios. The performance of the proposed method on datasets with different types of discretization (beyond binarization) remains unexamined and is an important consideration for real-world applications.

Empirical Results: According to the empirical results, the proposed Discretized CI Test (DCT) shows smaller power compared to baseline methods. This indicates that while the method is innovative, its practical effectiveness in terms of power may be limited in some scenarios.

Limitations:
See above comments.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors proposes a method for testing conditional independence in presence of discretisation.
They assume the variables to be jointly Gaussian, and that some of them are accessible only after discretisation; thus the data contain a mix of discrete and continuous variables.
Discretization might remove some conditional independencies. Assume X1 to be independent from X2 given X3.
It might be that X1 becomes instead dependent on X2 given \tilde{X3}, where \tilde{X3} is the discretised  X3.
The authors develop a way to infer the latent correlation on the real value variables and they propose a novel test for conditional independence for  the setting mixed continuous and discrete variables.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Testing conditional independence with mixed types of variables is an interesting topic and the work is original. 
The presentation is good, even though I could not follow the development of the bridge equations (this might be because I am not familiar with the adopted techniques).

Weaknesses:
* I am skeptical about the specific research question addressed.
X1 and X3 are independent given X2; yet they might be not independent given the discretised version of X2.

For instance, with ref to  Fig 1a,   X1 and X3 are formally dependent given \tilde{X2}; yet the induced dependence might be very weak.
I argue that the strength of the induced dependence depends on how the discretisation is done.
Example: X2 is human height, discretized into bins of few centimeters; then \tilde{X2}  is practically as informative as X2 and the induced dependence is likely to be negligible, in which case it might be sensible not rejecting H0.

The author did not discuss the impact  of the adopted discretization approach on the induced dependence.
Also, there is no compelling example in which discretisation induces a strong dependence.


* In the first set of experiments the test is better calibrated than the competitors, but it has by far less power. Overall, these results are not very strong.


* The competitor tests (Z-test and chi-square) are not modern.
There is no comparison against  existing tests for mixed variables; I can cite for instance Bayesian Independence Test with Mixed-type Variables, Benavoli et al. 2021.
Another simple baseline which I think should be present: test conditional independence having discretized all variables and use modern test for discrete variables (as a starting point, I suggest  those available in bnlearn https://www.bnlearn.com/documentation/man/conditional.independence.tests.html )

Limitations:
No potential negative societal impact.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Authors propose a test for conditional independence in case of discretized variables, i.e. variables that originally were defined over a continuous domain and are then mapped to a discrete domain. In this case, a binary domain. Authors propose to bridge the unobserved continuous variables with the observed discretized variables with equations modeling the original covariance/precision matrix coefficients. Both theoretical and experimental evidence support the proposed testing  methods. 

Typo at page 13, line 491, equation 17, missing closed bracket.

Citation 3, 4 are the same reference.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper contributes in a significant way on a topic which is crucial in the context of structure learning/causal discovery. Specifically, the proposed theoretical framework is solid and sound, explaining the logical steps that lead to the conditional independence test. The major strength points of this contribution are:

- The self-contained graphical representation of the discretized variables and their original continuous ones,
- The flexibility of the bridge equations, that can be adapted to specific cases without compromising the theoretical soundness.
- The performance of both the unconditional and the conditional independence test.

Weaknesses:
The only weakness is that I would do more experiments.

Limitations:
The only limitation, that is also discussed by authors, is that the ""discretized"" variables are in fact ""binary"" variables, which limits the applicability of the proposed test.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a novel statistical method for testing conditional independence (CI) when some of the data is discretized. Initially, the authors introduce bridge equations to estimate covariance and establish asymptotic normality, facilitating an unconditional independence test. For the conditional independence test, they employ nodewise regression to recover precision coefficients. Theoretical analysis and empirical validation are provided to showcase the method’s effectiveness.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper introduces a conditional independence test tailored for scenarios with discretized data, which often encountered in financial analysis and healthcare due to data collection or measurement constraints. The CI test is highly adaptable, capable of handling situations where both variables are discretized, both are continuous, or one is discretized. Numerical experiments on both synthetic and real-world datasets demonstrate superior performance in various scenarios.

Weaknesses:
The development relies on the assumption of a multivariate Gaussian distribution, which is rather stringent.

Limitations:
The development relies on the assumption of a multivariate Gaussian distribution, which is rather stringent.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses a critical issue in Conditional Independence (CI) testing methods, specifically when the available data is a discretized version of the original continuous data. Traditional CI testing methods often assume that discretized observations can directly substitute for continuous variables, leading to erroneous conclusions. To overcome this limitation, the authors introduce a novel CI test tailored for discretized data. The key innovation lies in using a bridge equation and nodewise regression to estimate the precision coefficients that reflect CI relationships among latent continuous variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper tackles a highly relevant and important problem within the realm of statistical analysis and CI testing.
The paper is well-written and presents the concepts clearly.
The proposed method is novel, and the theoretical contributions are solid, providing a robust foundation for CI testing in discretized data settings.

Weaknesses:
Assumption of Multivariate Normality: A primary limitation is the assumption that the data follow a multivariate normal distribution. This assumption simplifies the derivation of bridge equations for unconditional independence testing and the use of nodewise regression for the CI test. However, it restricts the applicability of the method to this specific class of variables. It is unclear how the method would perform with unknown or non-normal variables.

Discretization Modeling: The paper models discretization as a binarization operation applied to observed variables. This assumption may not hold in all practical scenarios. The performance of the proposed method on datasets with different types of discretization (beyond binarization) remains unexamined and is an important consideration for real-world applications.

Empirical Results: According to the empirical results, the proposed Discretized CI Test (DCT) shows smaller power compared to baseline methods. This indicates that while the method is innovative, its practical effectiveness in terms of power may be limited in some scenarios.

Limitations:
See above comments.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
AZuONuYzKl;"REVIEW 
Summary:
The paper addresses a major challenge in biology: identifying evolutionary traits, which are features common to a group of species with a shared ancestor in the phylogenetic tree. Compared to the existing works, this submission proposes new architectures and loss to avoid the over-specification problems. In the experiments, the authors demonstrate that the proposed method improves existing works and set up ablation studies to show the impact of different components of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
[+] The paper introduces HComP-Net, a new architecture designed to discover evolutionary traits from images in a hierarchical manner. This addresses the limitations of current prototype-based methods that operate over a flat structure of classes.
[+] Together with the architecture, the paper proposes contrastive loss and several additional losses to improve the performance.
[+] The inclusion of a novel masking module allows for the exclusion of over-specific prototypes at higher levels of the tree without compromising classification performance. This helps maintain the accuracy and effectiveness of the model.
[+] The proposed method not only improves the accuracy and other metrics, but also shows the generalizability to unseen species.

Weaknesses:
[-] More background: For most of the machine learning conference readers, I guess the proposed problem background is required. Therefore, more related work and background sections should be useful. 
[-] I wonder whether the proposed framework can address ""Convergent evolution"" and other similarity cases. Since these species can have similar features but should not be very close in the evolutionary trees. I suggest the authors to include more details and discussions about the background knowledge.
[-] While the framework has shown promising results on datasets of birds and other animals, I wonder whether the method can show its scalability to larger and more diverse datasets.

Limitations:
I do not think this work has potential negative social impact. The problems sounds very interesting.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a novel deep learning based algorithm named HComP-Net that can detect evolutionary traits common to groups of species with shared ancestors. Based on earlier studies, they aim to build a model that can accurately isolate common traits of specific species and reject over-specific features.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors presented their aims and methods quite clearly. While inspired by the earlier studies, they point out how their study is different from the earlier studies. To identify common visual features (i.e., evolutionary traits), they 1) combined two novel loss functions with a previously proposed loss and 2) used a novel masking module. Their results are compelling, which suggest the learning power of HComp-Net and its utility in detecting evolutionary traits. As HComp-Net may be used in other domains, this study can be of interest to other researchers.

Weaknesses:
HComp-Net was tested with only 3 datasets, which is understandable, as proper datasets may not be readily available. Still, a more thorough evaluation is desirable in the future.

Limitations:
The authors provided the limitations in the appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a method that automatically learns multiple orthogonal embeddings to act as prototypes. This approach helps the discovery of hierarchical similarities by representing data in a structured space.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The writing is clear and easy to follow.   
The authors conduct experiments that with other methods, and the visualization of prototypes in feature maps, They also perform an ablation study on different parts of the loss functions.

Weaknesses:
In the comparison with HPNet, the authors modify HComP-Net by removing the final two max pooling layers, resulting in a more detailed 26x26 feature map. In contrast, HPNet produces only a 7x7 feature map as shown in figure 4(a). Since the architecture and effectiveness of these networks heavily depend on the resolution of feature maps, this discrepancy raises concerns about the fairness of the comparison. To ensure a fair comparison:
  * HPNet should also be adjusted to generate a larger feature map. 
  * This adjustment and its impact on performance should also be included in the ablation study section.

In the generalizing to unseen species section, the evaluation method used by the authors could be extended to include comparisons with non-hierarchical methods. This would provide a more comprehensive evaluation of the method's effectiveness across different types of classification challenges.

Limitations:
There are several limitations concerning the comparison with other methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors investigate the use of prototype-based explainability (as in ProtoPNet) for the visual discovery of evolutionary traits in biology image repositories.
In particular, the authors aim to find traits that apply to group of species in a hierarchical fashion, according ot the tree-of-life hierarchy. 
The authors identify three challenges with state-of-the-art prototype methods such as learning over-specific prototypes that do not apply to all species in a given group, and prototypes that do not descriminate between the group and other groups of species in the hierarchy. Ther main contribution is the design of a loss and a masking mechanism to mitigate those issues.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ Interesting application and qualitative results in evolutionary biology.
+ Dedicated focus on learning discriminative hierarchy-level features in an interpretable way.
+ The authors provide their source code.

Weaknesses:
- The results are limited to three relatively small datasets. Why are there no result on the iNaturalist dataset which is at least 57x larger than CUB-200- 2011?
- It was hard to judge the effectiveness of the approach from the provided figures. The images are quite small.
- It was also hard to assess the effectiveness of masking. The figures did not illustrate how it helps.

Minor: I encountered several language issues. Below are ones I noted:
- hiearchy
- seperation
- Futhermore
- scenarious => scenarios 
- overlayed => overlaid
- indicating to difference => to a difference
- hasn’t => has not [avoid abbreviations in a scientific text]

Limitations:
A fundamental limitation in the application domain of interpretable biological traits is discussed in section I. Beyond a few ablation studies focusing on the introduced losses, I missed a discussion on the limitations of the approach, in particular the effectiveness of masking.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses a major challenge in biology: identifying evolutionary traits, which are features common to a group of species with a shared ancestor in the phylogenetic tree. Compared to the existing works, this submission proposes new architectures and loss to avoid the over-specification problems. In the experiments, the authors demonstrate that the proposed method improves existing works and set up ablation studies to show the impact of different components of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
[+] The paper introduces HComP-Net, a new architecture designed to discover evolutionary traits from images in a hierarchical manner. This addresses the limitations of current prototype-based methods that operate over a flat structure of classes.
[+] Together with the architecture, the paper proposes contrastive loss and several additional losses to improve the performance.
[+] The inclusion of a novel masking module allows for the exclusion of over-specific prototypes at higher levels of the tree without compromising classification performance. This helps maintain the accuracy and effectiveness of the model.
[+] The proposed method not only improves the accuracy and other metrics, but also shows the generalizability to unseen species.

Weaknesses:
[-] More background: For most of the machine learning conference readers, I guess the proposed problem background is required. Therefore, more related work and background sections should be useful. 
[-] I wonder whether the proposed framework can address ""Convergent evolution"" and other similarity cases. Since these species can have similar features but should not be very close in the evolutionary trees. I suggest the authors to include more details and discussions about the background knowledge.
[-] While the framework has shown promising results on datasets of birds and other animals, I wonder whether the method can show its scalability to larger and more diverse datasets.

Limitations:
I do not think this work has potential negative social impact. The problems sounds very interesting.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a novel deep learning based algorithm named HComP-Net that can detect evolutionary traits common to groups of species with shared ancestors. Based on earlier studies, they aim to build a model that can accurately isolate common traits of specific species and reject over-specific features.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors presented their aims and methods quite clearly. While inspired by the earlier studies, they point out how their study is different from the earlier studies. To identify common visual features (i.e., evolutionary traits), they 1) combined two novel loss functions with a previously proposed loss and 2) used a novel masking module. Their results are compelling, which suggest the learning power of HComp-Net and its utility in detecting evolutionary traits. As HComp-Net may be used in other domains, this study can be of interest to other researchers.

Weaknesses:
HComp-Net was tested with only 3 datasets, which is understandable, as proper datasets may not be readily available. Still, a more thorough evaluation is desirable in the future.

Limitations:
The authors provided the limitations in the appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a method that automatically learns multiple orthogonal embeddings to act as prototypes. This approach helps the discovery of hierarchical similarities by representing data in a structured space.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The writing is clear and easy to follow.   
The authors conduct experiments that with other methods, and the visualization of prototypes in feature maps, They also perform an ablation study on different parts of the loss functions.

Weaknesses:
In the comparison with HPNet, the authors modify HComP-Net by removing the final two max pooling layers, resulting in a more detailed 26x26 feature map. In contrast, HPNet produces only a 7x7 feature map as shown in figure 4(a). Since the architecture and effectiveness of these networks heavily depend on the resolution of feature maps, this discrepancy raises concerns about the fairness of the comparison. To ensure a fair comparison:
  * HPNet should also be adjusted to generate a larger feature map. 
  * This adjustment and its impact on performance should also be included in the ablation study section.

In the generalizing to unseen species section, the evaluation method used by the authors could be extended to include comparisons with non-hierarchical methods. This would provide a more comprehensive evaluation of the method's effectiveness across different types of classification challenges.

Limitations:
There are several limitations concerning the comparison with other methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors investigate the use of prototype-based explainability (as in ProtoPNet) for the visual discovery of evolutionary traits in biology image repositories.
In particular, the authors aim to find traits that apply to group of species in a hierarchical fashion, according ot the tree-of-life hierarchy. 
The authors identify three challenges with state-of-the-art prototype methods such as learning over-specific prototypes that do not apply to all species in a given group, and prototypes that do not descriminate between the group and other groups of species in the hierarchy. Ther main contribution is the design of a loss and a masking mechanism to mitigate those issues.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ Interesting application and qualitative results in evolutionary biology.
+ Dedicated focus on learning discriminative hierarchy-level features in an interpretable way.
+ The authors provide their source code.

Weaknesses:
- The results are limited to three relatively small datasets. Why are there no result on the iNaturalist dataset which is at least 57x larger than CUB-200- 2011?
- It was hard to judge the effectiveness of the approach from the provided figures. The images are quite small.
- It was also hard to assess the effectiveness of masking. The figures did not illustrate how it helps.

Minor: I encountered several language issues. Below are ones I noted:
- hiearchy
- seperation
- Futhermore
- scenarious => scenarios 
- overlayed => overlaid
- indicating to difference => to a difference
- hasn’t => has not [avoid abbreviations in a scientific text]

Limitations:
A fundamental limitation in the application domain of interpretable biological traits is discussed in section I. Beyond a few ablation studies focusing on the introduced losses, I missed a discussion on the limitations of the approach, in particular the effectiveness of masking.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
8bExkmfLCr;"REVIEW 
Summary:
The paper introduces the JOBCD (J-Orthogonal Block Coordinate Descent) algorithm, a novel method designed to tackle optimization problems under J-orthogonality constraints. JOBCD includes two variants: GS-JOBCD (Gauss-Seidel strategy) and VR-J-JOBCD (Jacobi strategy with variance reduction). Theoretical analyses establish the algorithms' complexity and convergence, while extensive experiments show JOBCD's superior performance compared to state-of-the-art methods in various applications.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The strengths of this paper are listed as follows:

Originality: This paper introduces JOBCD as a novel approach to handling J-orthogonality constraints. It offers GS-JOBCD and VR-J-JOBCD, showcasing flexibility and innovation in optimization strategies.

Quality: This paper provides comprehensive complexity and convergence analyses. Extensive experiments demonstrate superior performance on real-world and synthetic data.

Clarity: The structure is logical.

Significance: This work is relevant to various statistical learning and data science fields.

Weaknesses:
Some proofs for Section 4 are hard to follow.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes two Block Coordinate gradient descent methods(BCD) for solving J-orthogonal constrained problem. One is Gauss-Seidel type, the other one is Jocobi type as well as addressing finite sum problem using variance reduction strategies. Convergence guarantees are proved with KL conditions. Numerical experiments show the advantages of the  proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed algorithm decomposes the matrix variable into row block structure, yielding a block coordinate descent algorithm with a small size subproblem. The numerical performance is very impressive.

Weaknesses:
1. This paper is based on the paper "" [51] Ganzhao Yuan. A block coordinate descent method for nonsmooth composite optimization
under orthogonality constraints. ArXiv, abs/2304.03641, 2023.""
The main difference is the constraint in this paper becomes J-orthogonality constraint. However, the framework follows almost the same as [51]. The authors should highlight the novelty of the algorithm or difficulty in the extension.

2. The authors of the reference [31] may be wrong. Besides, the UMCM algorithm in [31] solves orthogonal constrained problem. Is there any difference in implmenting in solving J-orthogonality problem? The objective value of UMCM is far from the JOBCD method. I'm curious about the reasons.

3. In numerical experiment, how do you select subset from the dataset, see line 309.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes two block coordinate descent methods for minimization of a finite-sum subject to the J-Orthogonality constraints — one based on Gauss-Seidel strategy, the other based on variance reduction and Jacobi strategy. The convergence is proved, with a global convergence rate of O(N/\epsilon) and O(\sqrt{N}/\epsilon) respectively, and a local convergence rate that depends on the desingularization in the KL-condition assumption.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The algorithms proposed are novel and might be useful in practice: the update rules involve solving a small size problem thereby is very simple, and the convergence is proved theoretically under reasonable assumptions.

Weaknesses:
The paper is relatively dense, and I find it a bit hard to keep track of all the terms introduced. For instance, the parameter theta is used in the algorithms but I’m not sure where it is introduced; in Assumption 4.8 KL function is mentioned but it’s not defined…

Limitations:
Yes, the paper discusses the assumptions of the theorems.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a block coordinate descent method for solving optimization problems with J-orthogonality constraints. Several variants of the method are introduced within this framework, and convergence results are established. Extensive numerical results are also presented to demonstrate the efficiency of the proposed methods. However, I have some concerns regarding the novelty of this paper as well as the numerical results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
It appears that optimization with J-orthogonality constraints has not been thoroughly studied in the literature. This paper proposes an efficient method for addressing this problem.

Weaknesses:
1. My major concern is that the novelty of this paper might be insufficient since the row-based approach is very similar to that in [51], even though the two papers tackle different problems.

2.  For the numerical results shown in Table 1, the proposed method fails to return a feasible solution for some instances, such as randn(10-10-5) and w1a (2470-290-145), as well as some other instances in the appendix. This is strange since the paper describes a BCD-type method, which should always return a feasible solution.

3. The information of the reference [31] might be incorrect. 

4. For the GS-JOBCD method, there are two options for choosing $Q$, whereas J-JOBCD only has one option. The authors should provide an explanation for this difference.

5. The presentation could be further improved. Here are a few examples: the formulation of $P_i$  after equation (12) could be simplified by removing the notation $\mathrm{mat}$; it is unclear if the requirement on $\underline{Q}$ in equation (4) is sufficient to guarantee convergence (probably not, since $\underline{Q} = 0$  also satisfies this condition).

Limitations:
At the beginning of the paper, the authors claim that equation (2) can imply 
$\\|\nabla f_i(X) - \nabla_i f(X^+)\\| \leq L_f \\|X - X^+\\|$, which is incorrect. Note that the converse is correct. The other assumptions in Assumptions 4.1 and 4.2 essentially assume the compactness of the iterates.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces the JOBCD (J-Orthogonal Block Coordinate Descent) algorithm, a novel method designed to tackle optimization problems under J-orthogonality constraints. JOBCD includes two variants: GS-JOBCD (Gauss-Seidel strategy) and VR-J-JOBCD (Jacobi strategy with variance reduction). Theoretical analyses establish the algorithms' complexity and convergence, while extensive experiments show JOBCD's superior performance compared to state-of-the-art methods in various applications.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The strengths of this paper are listed as follows:

Originality: This paper introduces JOBCD as a novel approach to handling J-orthogonality constraints. It offers GS-JOBCD and VR-J-JOBCD, showcasing flexibility and innovation in optimization strategies.

Quality: This paper provides comprehensive complexity and convergence analyses. Extensive experiments demonstrate superior performance on real-world and synthetic data.

Clarity: The structure is logical.

Significance: This work is relevant to various statistical learning and data science fields.

Weaknesses:
Some proofs for Section 4 are hard to follow.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes two Block Coordinate gradient descent methods(BCD) for solving J-orthogonal constrained problem. One is Gauss-Seidel type, the other one is Jocobi type as well as addressing finite sum problem using variance reduction strategies. Convergence guarantees are proved with KL conditions. Numerical experiments show the advantages of the  proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed algorithm decomposes the matrix variable into row block structure, yielding a block coordinate descent algorithm with a small size subproblem. The numerical performance is very impressive.

Weaknesses:
1. This paper is based on the paper "" [51] Ganzhao Yuan. A block coordinate descent method for nonsmooth composite optimization
under orthogonality constraints. ArXiv, abs/2304.03641, 2023.""
The main difference is the constraint in this paper becomes J-orthogonality constraint. However, the framework follows almost the same as [51]. The authors should highlight the novelty of the algorithm or difficulty in the extension.

2. The authors of the reference [31] may be wrong. Besides, the UMCM algorithm in [31] solves orthogonal constrained problem. Is there any difference in implmenting in solving J-orthogonality problem? The objective value of UMCM is far from the JOBCD method. I'm curious about the reasons.

3. In numerical experiment, how do you select subset from the dataset, see line 309.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes two block coordinate descent methods for minimization of a finite-sum subject to the J-Orthogonality constraints — one based on Gauss-Seidel strategy, the other based on variance reduction and Jacobi strategy. The convergence is proved, with a global convergence rate of O(N/\epsilon) and O(\sqrt{N}/\epsilon) respectively, and a local convergence rate that depends on the desingularization in the KL-condition assumption.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The algorithms proposed are novel and might be useful in practice: the update rules involve solving a small size problem thereby is very simple, and the convergence is proved theoretically under reasonable assumptions.

Weaknesses:
The paper is relatively dense, and I find it a bit hard to keep track of all the terms introduced. For instance, the parameter theta is used in the algorithms but I’m not sure where it is introduced; in Assumption 4.8 KL function is mentioned but it’s not defined…

Limitations:
Yes, the paper discusses the assumptions of the theorems.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a block coordinate descent method for solving optimization problems with J-orthogonality constraints. Several variants of the method are introduced within this framework, and convergence results are established. Extensive numerical results are also presented to demonstrate the efficiency of the proposed methods. However, I have some concerns regarding the novelty of this paper as well as the numerical results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
It appears that optimization with J-orthogonality constraints has not been thoroughly studied in the literature. This paper proposes an efficient method for addressing this problem.

Weaknesses:
1. My major concern is that the novelty of this paper might be insufficient since the row-based approach is very similar to that in [51], even though the two papers tackle different problems.

2.  For the numerical results shown in Table 1, the proposed method fails to return a feasible solution for some instances, such as randn(10-10-5) and w1a (2470-290-145), as well as some other instances in the appendix. This is strange since the paper describes a BCD-type method, which should always return a feasible solution.

3. The information of the reference [31] might be incorrect. 

4. For the GS-JOBCD method, there are two options for choosing $Q$, whereas J-JOBCD only has one option. The authors should provide an explanation for this difference.

5. The presentation could be further improved. Here are a few examples: the formulation of $P_i$  after equation (12) could be simplified by removing the notation $\mathrm{mat}$; it is unclear if the requirement on $\underline{Q}$ in equation (4) is sufficient to guarantee convergence (probably not, since $\underline{Q} = 0$  also satisfies this condition).

Limitations:
At the beginning of the paper, the authors claim that equation (2) can imply 
$\\|\nabla f_i(X) - \nabla_i f(X^+)\\| \leq L_f \\|X - X^+\\|$, which is incorrect. Note that the converse is correct. The other assumptions in Assumptions 4.1 and 4.2 essentially assume the compactness of the iterates.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
8JmUmTgKiY;"REVIEW 
Summary:
This paper first generalizes the Kolmogorov–Smirnov (KS) distance from one-dimensional spaces to multidimensional spaces and proposes the Kolmogorov-Smirnov GAN, which formulates the generative model by minimizing the Kolmogorov-Smirnov (KS) distance. Theoretical results are also given in this paper and the experiments also show the superiority of stability during training, resistance to mode dropping and collapse, and tolerance to variations in hyperparameter settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I like the idea of generalizing the one-dimensional KS distance to multi-dimensional, which I had thought about before but failed to achieve. The motivation and writing are good, and the experiments of KSGAN seem to have achieved good results.

Weaknesses:
1. I'm skeptical about some parts of the theory. (See questions for details)
2. It seems that the advantages of KS distance over JS divergence and Wasserstein Distance are not explained.
3. The idea of reformulating a distance between distributions to a GAN model seems to be old and is now unlikely to attract readers' interest.
4. The experimental setup is relatively simple, only comparing with vanilla GAN and WGAN on Synthetic, MNIST, and CIFAR10 datasets
5. According to the experimental results, the advantages of KSGAN lie in the stability of training and resistance to mode dropping. A significant issue is that with current network architectures and training techniques, these two problems are rarely encountered.

Limitations:
The authors have discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel variant of the generative adversarial network that uses the Kolmogorov-Smirnov distance to align the generated distribution with the target distribution. This distance is calculated using the quantile function, which acts as the critic in the adversarial training process. Experiments are conducted on synthetic distributions and small image datasets to show that the proposed KSGAN performs on par with the existing adversarial methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The paper is well-presented and easy to follow.

2. The claims and methodology designs are well supported by theoretical analysis.

Weaknesses:
1. It is still unclear why we need another adversarial design based on KS distance. The vanilla GAN paper shows that the designed bi-level optimization process can already be seen as optimizing the distance between the generated and the target distribution. Then, what are the specific advantages KS distance can bring within the adversarial framework?

2. The experiments are merely conducted on synthetic datasets and small image datasets. It is unclear whether the proposed method can be adapted to larger-scale datasets or incorporated into more advanced frameworks like StyleGAN. Moreover, the compared baselines are limited to early works, and the experimental results of KSGAN are worse than those of WGAN-GP. Thus, I do not see many advantages of KSGAN in terms of the presented experiments.

Limitations:
Please see the discussions above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a generalized KS distance applicable to high dimensional spaces, formulate the corresponding dual problem, and use adversarial training to construct a generative model that minimizes the GKS between data and generated distributions.

The paper is well presented and appears technically correct through what I've seen, though I didn't check the proofs in details.

The main problem is that there's no clear motivation for why using the GKS is beneficial at all (either theoretically or practically). As such, despite being novel, I don't see any clear impact from the paper. Furthermore, the final algorithm is quite complicated and the results are fairly underwhelming, so at the end of the day the cons dramatically outweigh the pros of the newly introduced algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is well written. It reads easily and it is clear what they want to do. The contribution of a generalized KS distance to multidimensional spaces and the algorithm to approximate it are to the best of my knowledge, novel.

Weaknesses:
The main problem I have with the paper is that I don't see any clear advantages of using the KS distance (gan) as a replacement of other distances like the Wasserstein one, or their GAN equivalent. The only mention of this, which should arguably be the most important thing in a paper introducing a new GAN, is in lines 224-228 of page 7. The authors claim there that they don't need to maximize the supremum in (5) which is false depending on how to interpret it, if you just take any set C in (5) you end up with |P_F(C) - P_G(C)| which is just measuring one moment for a given characteristic function, and far from being anything meaningful (and the same holding true for most IPMs). The results are also not particularly interesting to merit the claim that there's anything particularly different or benefitial on using this new formulation.

Limitations:
No clear motivation or benefit from using their algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed a new kind of GAN training method called KS-GAN. The method is based on minimizing the Kolmogorov–Smirnov distance. The KSGAN updates the generator by minimizing an upper bound of the generalized KS distance. It updates the discriminator (or the critic network) by using energy-based model training with regularization terms. The KSGAN is a novel attempt to explore new approaches to train generative adversarial networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* (1) The paper studied using generalized KS distance to train GAN generators is a novel attempt to extend the GAN literature. 

* (2) Some theoretical arguments about implementing the empirical KS distance using neural networks is novel yet constructive.

Weaknesses:
My main concern about the paper is its weak evaluation baselines and questionable practical usage:

* (1) Though the idea of using new objectives for training GAN generators is attractive, the practical usage of KSGAN seems questionable, especially for high-dimensional data. For instance, in the CIFAR10 generation experiment, the author compares KSGAN with WGAN-GP and Vanilla GAN, which have shown weak empirical performances. However, it is well-known that, for CIFAR10 data, the StyleGAN2-ADA[1] model is a strong baseline GAN model. I think it would strengthen the paper a lot if the authors could somehow show strong performances of KSGAN using StyleGAN2's architectures and implementation techniques. However, I do admit that such a requirement may be too tough for new methods.

* (2) The KSGAN's critic function is constructed with EBMs. However, even with regularization terms, energy-based models are well-known for poor scaling ability to high-dimensional data. This may prevent the practical usage of KSGAN for real-world high-dimensional data.

Limitations:
The author has addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper first generalizes the Kolmogorov–Smirnov (KS) distance from one-dimensional spaces to multidimensional spaces and proposes the Kolmogorov-Smirnov GAN, which formulates the generative model by minimizing the Kolmogorov-Smirnov (KS) distance. Theoretical results are also given in this paper and the experiments also show the superiority of stability during training, resistance to mode dropping and collapse, and tolerance to variations in hyperparameter settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I like the idea of generalizing the one-dimensional KS distance to multi-dimensional, which I had thought about before but failed to achieve. The motivation and writing are good, and the experiments of KSGAN seem to have achieved good results.

Weaknesses:
1. I'm skeptical about some parts of the theory. (See questions for details)
2. It seems that the advantages of KS distance over JS divergence and Wasserstein Distance are not explained.
3. The idea of reformulating a distance between distributions to a GAN model seems to be old and is now unlikely to attract readers' interest.
4. The experimental setup is relatively simple, only comparing with vanilla GAN and WGAN on Synthetic, MNIST, and CIFAR10 datasets
5. According to the experimental results, the advantages of KSGAN lie in the stability of training and resistance to mode dropping. A significant issue is that with current network architectures and training techniques, these two problems are rarely encountered.

Limitations:
The authors have discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel variant of the generative adversarial network that uses the Kolmogorov-Smirnov distance to align the generated distribution with the target distribution. This distance is calculated using the quantile function, which acts as the critic in the adversarial training process. Experiments are conducted on synthetic distributions and small image datasets to show that the proposed KSGAN performs on par with the existing adversarial methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The paper is well-presented and easy to follow.

2. The claims and methodology designs are well supported by theoretical analysis.

Weaknesses:
1. It is still unclear why we need another adversarial design based on KS distance. The vanilla GAN paper shows that the designed bi-level optimization process can already be seen as optimizing the distance between the generated and the target distribution. Then, what are the specific advantages KS distance can bring within the adversarial framework?

2. The experiments are merely conducted on synthetic datasets and small image datasets. It is unclear whether the proposed method can be adapted to larger-scale datasets or incorporated into more advanced frameworks like StyleGAN. Moreover, the compared baselines are limited to early works, and the experimental results of KSGAN are worse than those of WGAN-GP. Thus, I do not see many advantages of KSGAN in terms of the presented experiments.

Limitations:
Please see the discussions above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a generalized KS distance applicable to high dimensional spaces, formulate the corresponding dual problem, and use adversarial training to construct a generative model that minimizes the GKS between data and generated distributions.

The paper is well presented and appears technically correct through what I've seen, though I didn't check the proofs in details.

The main problem is that there's no clear motivation for why using the GKS is beneficial at all (either theoretically or practically). As such, despite being novel, I don't see any clear impact from the paper. Furthermore, the final algorithm is quite complicated and the results are fairly underwhelming, so at the end of the day the cons dramatically outweigh the pros of the newly introduced algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is well written. It reads easily and it is clear what they want to do. The contribution of a generalized KS distance to multidimensional spaces and the algorithm to approximate it are to the best of my knowledge, novel.

Weaknesses:
The main problem I have with the paper is that I don't see any clear advantages of using the KS distance (gan) as a replacement of other distances like the Wasserstein one, or their GAN equivalent. The only mention of this, which should arguably be the most important thing in a paper introducing a new GAN, is in lines 224-228 of page 7. The authors claim there that they don't need to maximize the supremum in (5) which is false depending on how to interpret it, if you just take any set C in (5) you end up with |P_F(C) - P_G(C)| which is just measuring one moment for a given characteristic function, and far from being anything meaningful (and the same holding true for most IPMs). The results are also not particularly interesting to merit the claim that there's anything particularly different or benefitial on using this new formulation.

Limitations:
No clear motivation or benefit from using their algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed a new kind of GAN training method called KS-GAN. The method is based on minimizing the Kolmogorov–Smirnov distance. The KSGAN updates the generator by minimizing an upper bound of the generalized KS distance. It updates the discriminator (or the critic network) by using energy-based model training with regularization terms. The KSGAN is a novel attempt to explore new approaches to train generative adversarial networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* (1) The paper studied using generalized KS distance to train GAN generators is a novel attempt to extend the GAN literature. 

* (2) Some theoretical arguments about implementing the empirical KS distance using neural networks is novel yet constructive.

Weaknesses:
My main concern about the paper is its weak evaluation baselines and questionable practical usage:

* (1) Though the idea of using new objectives for training GAN generators is attractive, the practical usage of KSGAN seems questionable, especially for high-dimensional data. For instance, in the CIFAR10 generation experiment, the author compares KSGAN with WGAN-GP and Vanilla GAN, which have shown weak empirical performances. However, it is well-known that, for CIFAR10 data, the StyleGAN2-ADA[1] model is a strong baseline GAN model. I think it would strengthen the paper a lot if the authors could somehow show strong performances of KSGAN using StyleGAN2's architectures and implementation techniques. However, I do admit that such a requirement may be too tough for new methods.

* (2) The KSGAN's critic function is constructed with EBMs. However, even with regularization terms, energy-based models are well-known for poor scaling ability to high-dimensional data. This may prevent the practical usage of KSGAN for real-world high-dimensional data.

Limitations:
The author has addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
6qJKrOulTr;"REVIEW 
Summary:
The authors propose a mathematically rigorous methodology based on Riemannian geometry for attributing network importance of tokens in a transformer models input space (e.g. image patches, or ~words in the textual domain). The proposed methodology—whilst based on sound theory—translates into an intuitive algorithm involving what appears to be a relatively inexpensive eigendecomposition. Experiments on 3 datasets across both the image and NLP domains explore how the features correlate with ground-truth inputs in the text domain, in addition to first steps towards exploring how the features affect the networks’ output logits.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A major strength of the paper is the mathematically solid approach in attempting to identify regions of the **input** space that explain transformers’ model decisions. This is an important area of study: in contrast to many recent mechanistic interpretability methods finding latent network representations (that are intrinsically hard for humans to interpret by default), salient features in the pixel/text input space are much more readily interpreted by humans.
- Whilst I am unfamiliar with geometric deep learning, the authors do a fantastic job of presenting the technical content in a digestible manner without sacrificing depth or rigor.

Weaknesses:
# [W1] Feature importance comparisons

Feature importance-based explanations are motivated on [L303] as quantifying the contribution of features `""to a model prediction""`. More concretely, around [L180], the authors motivate the eigenvalues of the pullback metric found using their method as ultimately deducing the importance of each segment (e.g. image patch)  `“with respect to the the final prediction”`. 

Consequently, a major weakness of the paper is how there is no comparison with related work for how well the proposed method’s identified important features alter the **output** logits (e.g. upon ablation).

I am slightly confused by why the authors did not adopt the established “perturbation test” experimental protocol in the baseline [1] against which they compared, to provide experimental evidence in favor of this. Currently, the only comparisons made around [L274] measure the features’ importance as they correlate to the *input’s* labels.

Concretely, the authors could, for example, ablate particular patches of MNIST and observe that the resulting performance drops correlate with the pullback metric’s eigenvalues. This would provide stronger evidence of the authors’ claims about the features affecting the networks’ output, and (crucially) ground the results in contrast to those achievable by existing methods.

# [W2] Limited experimental results & improvements

There is a lot of interesting theory here, but ultimately this is a paper with a concrete applied goal of feature attribution in transformer models. With such a new methodology with many technical details, I believe there is an extra burden of proof on the authors to demonstrate this somehow leads to additional insights / practical gains. As such, it is a relative weakness of the paper that so few experiments are performed to justify the methodology.

Beyond toy datasets, it would be interesting to see how the method performs on more complex ones (not necessarily larger ones), such as TinyImageNET. Here, we could visualize much more easily if the method helps identify salient features of animals’ body parts (for example) as being important features for classification. MNIST experiments alone in the image domain are hard to interpret given the similarity of all the input data. 

Furthermore, the method provides an almost insignificant increase of just `0.07` cosine similarity (over the baseline in [1]), on just a single dataset (and with just two baselines—for example, how does GradCAM perform here?). This is not sufficient evidence to convince me as a reader that the proposed methodology should be adopted. 

---

- [1]: Chefer, Hila et al. “Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021): 387-396.

Limitations:
Some limitations are indeed addressed throughout. However, (unless I have missed something, in which case I apologise!) I can only find the limitations of the small number of datasets used stated in the NeurIPS checklist. This needs to be stated explicitly in the main paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work attempts to find the set of inputs that generate the same neural network predictions. To this end, the authors interpret the layers of the network as transformations of the input manifold. This interpretation is used to defined equivalence classes over the inputs and to define feature importance. Finally, the tools are used to identify equivalence classes for MNIST digits and for hate speech detection with BERT.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
Section 2 does a thorough job of introducing a manifold interpretation to neural networks.  This introduction is then used to motivate multiple algorithms for finding equivalence classes of the inputs---or the setup of inputs that result in the same prediction---and identify features that are important.

Weaknesses:
The main contribution of this work is to introduce a tool for analyzing which set of inputs produce the same output. However, this is exactly the Fisher information matrix (with respect to the inputs) and has also been introduced in prior work (https://arxiv.org/abs/2104.13289). Could the authors clarify what the differences are and what the additional novelty. If the ""local data matrix"" introduced in https://arxiv.org/abs/2104.13289 is identical to the tools in this work, I think it severely diminishes the contributions of this work. Furthermore the experiments are extremely similar (such as Figure 1). 

The second weakness is the limited number of experiments. The work does not show any quantitative results: Figure 1, Figure 2 and Figure 3 is just 1 example and is not indicative of why the tools are useful. The experiments in section 4 primarily discuss wall-clock time. It would significantly help if claims such as ""(Line 266) we notice that the perturbation-based algorithm ends up producing monochrome ..."" are substantiated quantitatively. Overall, the work doesn't provide novel tools and the experiments lack a novel usage of these tools and do not reveal any new insights.

Limitations:
The authors address limitations of their work but it can be expanded upon. For example, the authors can discuss the time required to compute Eigenvalues, and other limitations such as not having any Eigenvalues to be 0 in Algorithms 3 / 4. Furthermore, their algorithm should work (in theory) for infinitesimal steps in the input manifold.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a method for exploring equivalence classes in the input space of Transformer models using a solid mathematical theory. By analyzing the Jacobian of the model, the method reconstructs and navigates these classes, offering a powerful tool for understanding Transformer interpretations and enhancing explainability. The proposed method is expected to solve problems in Computer Vision and Natural Language Processing tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
I write both strength and weakness. 

First, I must disclose that I have no prior study or background in both Transformers and manifolds. While I am conceptually aware of them, my knowledge is limited to that extent, and I lack confidence in reviewing the technical details. Therefore, please consider my review comments as feedback from a layperson in this field, focusing on the overall mathematical consistency and readability of the paper.

This paper describes mathematics in a clear and understandable manner that even a layperson like myself can grasp. Each definition and theorem is stated accurately, and I believe that the general concepts can be understood with basic knowledge.

I personally feel that the objective of this paper is not clearly conveyed. While the paper claims to contribute to explainability and sensitivity analysis through the analysis of input manifolds, the logic behind this was not clear to me in the Introduction and Preliminaries. Although the concepts of explainability and sensitivity analysis become clearer in the later chapters, it might be beneficial to provide a bit more explanation in the Introduction.

Additionally, it might be helpful to clearly define the equivalence class mathematically.

Since I am not familiar with the existing literature, I was unable to judge the novelty of this work.

Weaknesses:
See above.

Limitations:
N/A.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper develops a novel theoretical framework grounded in Riemannian geometry for analyzing the input space of Transformer models, and introduce two algorithms, SiMEC and SiMExp, which facilitate the exploration and interpretation of equivalence classes within this input space. These methods offer new insights the internal mechanisms of Transformers, and provide new understanding of how these models perceive and process input data which can be very useful in the field of explainable AI.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1 novelty: This paper provide an innovative application of Riemannian geometry to analyze the input spaces of Transformer models, which is very novel in the area.

2 Theory: This paper establishes a solid mathematical theory on how Riemannian geometry is applied to Transformer models. Based on this theory, SiMEC and SiMExp are developed to explore the input spaces of Transformer models.

Weaknesses:
In experiment, the MNIST dataset is a little bit trivial, as the pixels of the background is essentially zero. It is nice to see the application of the proposed algorithm on natural images like CIFAR.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a mathematically rigorous methodology based on Riemannian geometry for attributing network importance of tokens in a transformer models input space (e.g. image patches, or ~words in the textual domain). The proposed methodology—whilst based on sound theory—translates into an intuitive algorithm involving what appears to be a relatively inexpensive eigendecomposition. Experiments on 3 datasets across both the image and NLP domains explore how the features correlate with ground-truth inputs in the text domain, in addition to first steps towards exploring how the features affect the networks’ output logits.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A major strength of the paper is the mathematically solid approach in attempting to identify regions of the **input** space that explain transformers’ model decisions. This is an important area of study: in contrast to many recent mechanistic interpretability methods finding latent network representations (that are intrinsically hard for humans to interpret by default), salient features in the pixel/text input space are much more readily interpreted by humans.
- Whilst I am unfamiliar with geometric deep learning, the authors do a fantastic job of presenting the technical content in a digestible manner without sacrificing depth or rigor.

Weaknesses:
# [W1] Feature importance comparisons

Feature importance-based explanations are motivated on [L303] as quantifying the contribution of features `""to a model prediction""`. More concretely, around [L180], the authors motivate the eigenvalues of the pullback metric found using their method as ultimately deducing the importance of each segment (e.g. image patch)  `“with respect to the the final prediction”`. 

Consequently, a major weakness of the paper is how there is no comparison with related work for how well the proposed method’s identified important features alter the **output** logits (e.g. upon ablation).

I am slightly confused by why the authors did not adopt the established “perturbation test” experimental protocol in the baseline [1] against which they compared, to provide experimental evidence in favor of this. Currently, the only comparisons made around [L274] measure the features’ importance as they correlate to the *input’s* labels.

Concretely, the authors could, for example, ablate particular patches of MNIST and observe that the resulting performance drops correlate with the pullback metric’s eigenvalues. This would provide stronger evidence of the authors’ claims about the features affecting the networks’ output, and (crucially) ground the results in contrast to those achievable by existing methods.

# [W2] Limited experimental results & improvements

There is a lot of interesting theory here, but ultimately this is a paper with a concrete applied goal of feature attribution in transformer models. With such a new methodology with many technical details, I believe there is an extra burden of proof on the authors to demonstrate this somehow leads to additional insights / practical gains. As such, it is a relative weakness of the paper that so few experiments are performed to justify the methodology.

Beyond toy datasets, it would be interesting to see how the method performs on more complex ones (not necessarily larger ones), such as TinyImageNET. Here, we could visualize much more easily if the method helps identify salient features of animals’ body parts (for example) as being important features for classification. MNIST experiments alone in the image domain are hard to interpret given the similarity of all the input data. 

Furthermore, the method provides an almost insignificant increase of just `0.07` cosine similarity (over the baseline in [1]), on just a single dataset (and with just two baselines—for example, how does GradCAM perform here?). This is not sufficient evidence to convince me as a reader that the proposed methodology should be adopted. 

---

- [1]: Chefer, Hila et al. “Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021): 387-396.

Limitations:
Some limitations are indeed addressed throughout. However, (unless I have missed something, in which case I apologise!) I can only find the limitations of the small number of datasets used stated in the NeurIPS checklist. This needs to be stated explicitly in the main paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work attempts to find the set of inputs that generate the same neural network predictions. To this end, the authors interpret the layers of the network as transformations of the input manifold. This interpretation is used to defined equivalence classes over the inputs and to define feature importance. Finally, the tools are used to identify equivalence classes for MNIST digits and for hate speech detection with BERT.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
Section 2 does a thorough job of introducing a manifold interpretation to neural networks.  This introduction is then used to motivate multiple algorithms for finding equivalence classes of the inputs---or the setup of inputs that result in the same prediction---and identify features that are important.

Weaknesses:
The main contribution of this work is to introduce a tool for analyzing which set of inputs produce the same output. However, this is exactly the Fisher information matrix (with respect to the inputs) and has also been introduced in prior work (https://arxiv.org/abs/2104.13289). Could the authors clarify what the differences are and what the additional novelty. If the ""local data matrix"" introduced in https://arxiv.org/abs/2104.13289 is identical to the tools in this work, I think it severely diminishes the contributions of this work. Furthermore the experiments are extremely similar (such as Figure 1). 

The second weakness is the limited number of experiments. The work does not show any quantitative results: Figure 1, Figure 2 and Figure 3 is just 1 example and is not indicative of why the tools are useful. The experiments in section 4 primarily discuss wall-clock time. It would significantly help if claims such as ""(Line 266) we notice that the perturbation-based algorithm ends up producing monochrome ..."" are substantiated quantitatively. Overall, the work doesn't provide novel tools and the experiments lack a novel usage of these tools and do not reveal any new insights.

Limitations:
The authors address limitations of their work but it can be expanded upon. For example, the authors can discuss the time required to compute Eigenvalues, and other limitations such as not having any Eigenvalues to be 0 in Algorithms 3 / 4. Furthermore, their algorithm should work (in theory) for infinitesimal steps in the input manifold.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a method for exploring equivalence classes in the input space of Transformer models using a solid mathematical theory. By analyzing the Jacobian of the model, the method reconstructs and navigates these classes, offering a powerful tool for understanding Transformer interpretations and enhancing explainability. The proposed method is expected to solve problems in Computer Vision and Natural Language Processing tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
I write both strength and weakness. 

First, I must disclose that I have no prior study or background in both Transformers and manifolds. While I am conceptually aware of them, my knowledge is limited to that extent, and I lack confidence in reviewing the technical details. Therefore, please consider my review comments as feedback from a layperson in this field, focusing on the overall mathematical consistency and readability of the paper.

This paper describes mathematics in a clear and understandable manner that even a layperson like myself can grasp. Each definition and theorem is stated accurately, and I believe that the general concepts can be understood with basic knowledge.

I personally feel that the objective of this paper is not clearly conveyed. While the paper claims to contribute to explainability and sensitivity analysis through the analysis of input manifolds, the logic behind this was not clear to me in the Introduction and Preliminaries. Although the concepts of explainability and sensitivity analysis become clearer in the later chapters, it might be beneficial to provide a bit more explanation in the Introduction.

Additionally, it might be helpful to clearly define the equivalence class mathematically.

Since I am not familiar with the existing literature, I was unable to judge the novelty of this work.

Weaknesses:
See above.

Limitations:
N/A.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper develops a novel theoretical framework grounded in Riemannian geometry for analyzing the input space of Transformer models, and introduce two algorithms, SiMEC and SiMExp, which facilitate the exploration and interpretation of equivalence classes within this input space. These methods offer new insights the internal mechanisms of Transformers, and provide new understanding of how these models perceive and process input data which can be very useful in the field of explainable AI.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1 novelty: This paper provide an innovative application of Riemannian geometry to analyze the input spaces of Transformer models, which is very novel in the area.

2 Theory: This paper establishes a solid mathematical theory on how Riemannian geometry is applied to Transformer models. Based on this theory, SiMEC and SiMExp are developed to explore the input spaces of Transformer models.

Weaknesses:
In experiment, the MNIST dataset is a little bit trivial, as the pixels of the background is essentially zero. It is nice to see the application of the proposed algorithm on natural images like CIFAR.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
5tOVh81aze;"REVIEW 
Summary:
While existing scaling law studies look at compute-optimal pretraining, this paper considers scaling laws in the context of both pretraining and downstream performance. They perform scaling experiments and find that performance is predictable even in overtraining, and average downstream performance is also predictable.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I think this is a solid paper that attempts to answer an important question. While I’m concerned about the lack of novelty (see the weaknesses), I overall lean towards accepting the paper. I think that the fact the paper reproduces findings from other papers using a different methodology is a good sign that the overall results are correct, and is valuable information (e.g. [Owen 2024](https://arxiv.org/pdf/2401.04757) also finds that average downstream performance is more predictable than for individual downstream tasks). To me, this is the primary contribution of the paper, and an important one as such.

Weaknesses:
To my mind, the largest weakness of this paper is the lack of novelty. In particular, my understanding is that the most important findings are that (1) performance is predictable in overtraining, and (2) average downstream performance is predictable. I’m not sure why (1) should be surprising – doesn’t a parametric scaling law, such as method 3 from the Chinchilla paper, also give the ability to predict loss when overtraining? I think the authors could improve the motivation for this consideration by providing a back of the envelope calculation: for instance, I plugged in the model size and dataset size for Gopher 280B into the Chinchilla scaling law and got a predicted test ppl of ~7.3 on MassiveText. However, Gopher actually had a (validation) perplexity of ~8.1, this constitutes a relative error of around 10% – substantially larger than the relative errors obtained by the authors of this paper. If the authors can provide an argument of this sort I'd find that helpful. 

I thought (2) was a more interesting claim, but I’ve seen this analyzed in Owen 2024 (https://arxiv.org/pdf/2401.04757), albeit with a different methodology. As such, I felt that the core results of the paper weren’t very novel. However, I’d be happy to update my assessment if the authors can provide evidence that my understanding is incorrect. 

One interesting point that the authors mentioned is that performance on individual tasks is less predictable. But this is only mentioned in passing, and I felt that it could be expanded upon a fair bit. What are the implications of this observation? Are there any patterns for which individual tasks are or aren’t predictable? 

I’m slightly concerned about data leakage being an issue for the downstream tasks, given that the training data (The Pile and RedPajama) covers a wide swath of the internet, and some of the downstream benchmarks have been criticized for data leakage or having label errors. 

Minor comment: The last paragraph of section 5 is a bit confusing: “There has been a rise in over-trained models [113, 114] and accompanying massive datasets [112, 82, 104, 3]. For example, Chinchilla 70B [45] is trained with a token multiplier of 20, while LLaMA-2 7B [114] uses a token multiplier of 290.” This makes it sound a bit like Chinchilla is overtrained, which I don’t think the authors are trying to say, so I’d suggest something like the following instead: “For example, while Chinchilla 70B is trained compute-optimally with a token multiplier of 20, LLaMA-2 7B…”

Limitations:
I felt that the authors did a good job describing some of the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a scaling law for the “Chinchilla over-trained” regime where models are trained on many more tokens (in this paper, up to 30x) than Chinchilla-optimal. They motivate a scaling law relating pre-training compute and “over-training” to validation loss. They empirically demonstrate that the proposed scaling law accurately predicts the validation loss of a 1.4B 32x over-trained model and a 6.9B Chinchilla-optimal model. They then study a simple scaling law relating perplexity to downstream benchmark error. They select a subset of 17 benchmarks for which a 154M parameter models performs 10% above random chance accuracy, and show show that average downstream error of the 1.4B and 6.9B models is predictable.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The setting considered by the paper is very relevant at the moment, as major model releases in the past year fall precisely in the “Chinchilla over-trained” regime. The work is also novel, as I am not aware of prior work that proposes and empirically validates scaling laws tailored for the Chinchilla over-trained regime.

The proposed scaling law in the over-trained regime is well-motivated both by prior scaling laws and by further empirical observations in the over-trained regime (Figure 2). While the models trained (5-8 1e21 FLOPs) are at least two orders of magnitude smaller than the latest open models (e.g., Gemma 2, Llama 3, Qwen 2), the experiments are of a large enough scale for a proof of concept.

The paper is clear and it gives sufficient details on the experimental set-up.

Weaknesses:
My main concerns are twofold: authors do not compare with the standard scaling laws of Kaplan et al. (fitted on their own testbed), and it is unclear how authors choose the models used to fit their scaling laws (Table 1).

Authors do not compare their over-trained scaling law with that proposed by Kaplan et al. The authors could fit this scaling law as in Hoffman et al. , Section 3.3, without any additional model training. Specifically, how well can the standard Kaplan et al. law predict the validation loss of the 1.4B over-trained model, when fitted on the model testbed with N < 1B described in Section 3.2?

Regarding the claim that validation loss (resp accuracy) is predictable with 300x (resp. 20x) less compute, I find this misleading, since authors train and evaluate a reasonably large model testbed, but only report the compute required for the 5 (resp. 6) models that they ultimately choose for the fit.  The authors do not discuss how this “train”/“test” split was chosen. Clearly, the train/test split should be chosen before seeing the evaluation results for any of the models, rather than including models until the fit seems ""good enough"", or choosing the smallest subset for which the fit is “good enough”. Otherwise, both the claim of 300x/20x compute as well as Figure 5 are misleading. Similarly, Figure 1 would be misleading, and it should include all models with N < 1.4B. 

The authors consider token multiplier M <= 640, however current models are even more overtrained. If am not mistaken, for Llama 3 8B, M ~= 2000. Demonstrating the validity of the proposed scaling laws for the amount of over-training of current models would have been ideal, even at smaller model scales.

Lastly, the proposed scaling laws are validated at substantially lower compute scales than current models with publicly available weights. It would have been ideal to at least see results for over-trained 7B models. I understand that the experiments presented in the paper already require a substantially amount of compute, and more closely matching the compute scales of recent models would be unfeasible for most research labs — therefore I am not taking this point into consideration when scoring the paper.

Limitations:
Limitations are adequately discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the power laws (scaling laws) of neural language models, particularly from the perspective of over-training and the relationship between validation loss (perplexity) and NLP downstream tasks. The authors define over-training as the situation where runs consume large amounts of computational resources, and they introduce a token multiplier, M. It is computed by D / N, where D is the number of training data tokens and N is the number of parameters. Through various model training setups and three different training corpora, the authors demonstrate that the validation loss can be computed and predicted using an equation that includes M. They also introduce another equation that illustrates the relationship between validation loss and downstream task error.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution of this paper is the exploration of the scaling laws in language models concerning over-training and NLP downstream tasks. These results, including equations and practical outcomes, are beneficial for researchers and engineers developing large language models. As highlighted by the authors, these insights are valuable for researchers in their future work.

Weaknesses:
The authors mentioned several limitations and future work in the paper. I agree with them, and especially the ‘scaling up’ part is the primary concern of this paper. The model sizes range from 0.011B to 6.9B, but open-source models are larger than these sizes - for instance, Llama 2 starts at 7B, and Llama 3 starts at 8B [1]. Furthermore, model size is crucial for techniques such as CoT [2]. I hope to hear the authors' opinions on this concern.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
While existing scaling law studies look at compute-optimal pretraining, this paper considers scaling laws in the context of both pretraining and downstream performance. They perform scaling experiments and find that performance is predictable even in overtraining, and average downstream performance is also predictable.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I think this is a solid paper that attempts to answer an important question. While I’m concerned about the lack of novelty (see the weaknesses), I overall lean towards accepting the paper. I think that the fact the paper reproduces findings from other papers using a different methodology is a good sign that the overall results are correct, and is valuable information (e.g. [Owen 2024](https://arxiv.org/pdf/2401.04757) also finds that average downstream performance is more predictable than for individual downstream tasks). To me, this is the primary contribution of the paper, and an important one as such.

Weaknesses:
To my mind, the largest weakness of this paper is the lack of novelty. In particular, my understanding is that the most important findings are that (1) performance is predictable in overtraining, and (2) average downstream performance is predictable. I’m not sure why (1) should be surprising – doesn’t a parametric scaling law, such as method 3 from the Chinchilla paper, also give the ability to predict loss when overtraining? I think the authors could improve the motivation for this consideration by providing a back of the envelope calculation: for instance, I plugged in the model size and dataset size for Gopher 280B into the Chinchilla scaling law and got a predicted test ppl of ~7.3 on MassiveText. However, Gopher actually had a (validation) perplexity of ~8.1, this constitutes a relative error of around 10% – substantially larger than the relative errors obtained by the authors of this paper. If the authors can provide an argument of this sort I'd find that helpful. 

I thought (2) was a more interesting claim, but I’ve seen this analyzed in Owen 2024 (https://arxiv.org/pdf/2401.04757), albeit with a different methodology. As such, I felt that the core results of the paper weren’t very novel. However, I’d be happy to update my assessment if the authors can provide evidence that my understanding is incorrect. 

One interesting point that the authors mentioned is that performance on individual tasks is less predictable. But this is only mentioned in passing, and I felt that it could be expanded upon a fair bit. What are the implications of this observation? Are there any patterns for which individual tasks are or aren’t predictable? 

I’m slightly concerned about data leakage being an issue for the downstream tasks, given that the training data (The Pile and RedPajama) covers a wide swath of the internet, and some of the downstream benchmarks have been criticized for data leakage or having label errors. 

Minor comment: The last paragraph of section 5 is a bit confusing: “There has been a rise in over-trained models [113, 114] and accompanying massive datasets [112, 82, 104, 3]. For example, Chinchilla 70B [45] is trained with a token multiplier of 20, while LLaMA-2 7B [114] uses a token multiplier of 290.” This makes it sound a bit like Chinchilla is overtrained, which I don’t think the authors are trying to say, so I’d suggest something like the following instead: “For example, while Chinchilla 70B is trained compute-optimally with a token multiplier of 20, LLaMA-2 7B…”

Limitations:
I felt that the authors did a good job describing some of the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a scaling law for the “Chinchilla over-trained” regime where models are trained on many more tokens (in this paper, up to 30x) than Chinchilla-optimal. They motivate a scaling law relating pre-training compute and “over-training” to validation loss. They empirically demonstrate that the proposed scaling law accurately predicts the validation loss of a 1.4B 32x over-trained model and a 6.9B Chinchilla-optimal model. They then study a simple scaling law relating perplexity to downstream benchmark error. They select a subset of 17 benchmarks for which a 154M parameter models performs 10% above random chance accuracy, and show show that average downstream error of the 1.4B and 6.9B models is predictable.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The setting considered by the paper is very relevant at the moment, as major model releases in the past year fall precisely in the “Chinchilla over-trained” regime. The work is also novel, as I am not aware of prior work that proposes and empirically validates scaling laws tailored for the Chinchilla over-trained regime.

The proposed scaling law in the over-trained regime is well-motivated both by prior scaling laws and by further empirical observations in the over-trained regime (Figure 2). While the models trained (5-8 1e21 FLOPs) are at least two orders of magnitude smaller than the latest open models (e.g., Gemma 2, Llama 3, Qwen 2), the experiments are of a large enough scale for a proof of concept.

The paper is clear and it gives sufficient details on the experimental set-up.

Weaknesses:
My main concerns are twofold: authors do not compare with the standard scaling laws of Kaplan et al. (fitted on their own testbed), and it is unclear how authors choose the models used to fit their scaling laws (Table 1).

Authors do not compare their over-trained scaling law with that proposed by Kaplan et al. The authors could fit this scaling law as in Hoffman et al. , Section 3.3, without any additional model training. Specifically, how well can the standard Kaplan et al. law predict the validation loss of the 1.4B over-trained model, when fitted on the model testbed with N < 1B described in Section 3.2?

Regarding the claim that validation loss (resp accuracy) is predictable with 300x (resp. 20x) less compute, I find this misleading, since authors train and evaluate a reasonably large model testbed, but only report the compute required for the 5 (resp. 6) models that they ultimately choose for the fit.  The authors do not discuss how this “train”/“test” split was chosen. Clearly, the train/test split should be chosen before seeing the evaluation results for any of the models, rather than including models until the fit seems ""good enough"", or choosing the smallest subset for which the fit is “good enough”. Otherwise, both the claim of 300x/20x compute as well as Figure 5 are misleading. Similarly, Figure 1 would be misleading, and it should include all models with N < 1.4B. 

The authors consider token multiplier M <= 640, however current models are even more overtrained. If am not mistaken, for Llama 3 8B, M ~= 2000. Demonstrating the validity of the proposed scaling laws for the amount of over-training of current models would have been ideal, even at smaller model scales.

Lastly, the proposed scaling laws are validated at substantially lower compute scales than current models with publicly available weights. It would have been ideal to at least see results for over-trained 7B models. I understand that the experiments presented in the paper already require a substantially amount of compute, and more closely matching the compute scales of recent models would be unfeasible for most research labs — therefore I am not taking this point into consideration when scoring the paper.

Limitations:
Limitations are adequately discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the power laws (scaling laws) of neural language models, particularly from the perspective of over-training and the relationship between validation loss (perplexity) and NLP downstream tasks. The authors define over-training as the situation where runs consume large amounts of computational resources, and they introduce a token multiplier, M. It is computed by D / N, where D is the number of training data tokens and N is the number of parameters. Through various model training setups and three different training corpora, the authors demonstrate that the validation loss can be computed and predicted using an equation that includes M. They also introduce another equation that illustrates the relationship between validation loss and downstream task error.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution of this paper is the exploration of the scaling laws in language models concerning over-training and NLP downstream tasks. These results, including equations and practical outcomes, are beneficial for researchers and engineers developing large language models. As highlighted by the authors, these insights are valuable for researchers in their future work.

Weaknesses:
The authors mentioned several limitations and future work in the paper. I agree with them, and especially the ‘scaling up’ part is the primary concern of this paper. The model sizes range from 0.011B to 6.9B, but open-source models are larger than these sizes - for instance, Llama 2 starts at 7B, and Llama 3 starts at 8B [1]. Furthermore, model size is crucial for techniques such as CoT [2]. I hope to hear the authors' opinions on this concern.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
5KEb1mqZRl;"REVIEW 
Summary:
The authors proposed a novel compression strategy of transformer based trackers. Unlike previous works, it divides the teacher network into multiple segments, each segment corresponds a single transformer layer of student network, then train each student layer separately. It also introduced some training strategies to enhance performance including (progressive) replacement training, prediction guidance and feature mimicking. Such compression framework is insensitive to the change of architecture of teacher network.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Effectiveness. The experiment results clearly demonstrated significant improvement of inference speed while reserving the majority of tracking accuracy.
2. Flexibility. The proposed compression strategy is insensitive to the change of architecture of tracking models, making it easy to apply on almost any transformer based trackers. The segmentation strategy and the size of student network also supports user customization, which enables the user to design student network according to their unique demands. Such flexibility shows excellent application prospects in end-side scenarios.

Weaknesses:
The detailed strategy of dividing the teacher network is not stated clearly in the paper. Base on the pseudo code provided in page 13, it seems that the segmentation strategy is simply mapping the list of transformer blocks of student network to that of the teacher network base on the lengths of the two lists. This could be too simple.

For example, assume teacher network has 8 transformer blocks in module 1 and 2 blocks in module 2, while student network consists of 2 blocks, then the second student block would have to emulate the last 3 blocks of module 1 and the 2 blocks of module 2, while module 1 and  module 2 might have been trained separately and possess different knowledge. Empirically, this would result in sub-optical performance.

A brief discuss on the divide strategy could help this paper become more informative.

Limitations:
The paper clearly addressed its limitations including inefficient training process and the performance gap between teacher and student network.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to  distill knowledge from larger teacher models into more compact student trackers. Three techniques are proposed: A stage division strategy that segments the transformer layers of the teacher model. Replacement training technique. Prediction guidance and stage-wise feature mimicking. Experiment verifys the effectiveness of the method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The proposed techniques are comprehensive and include a bunch of methods to improve the performance and efficiency of the trackers, 
2.	The experiments are extensive which includes 5 VOT benchmarks. 
3.	The speed is fast when applying the 2 layer tracker variants.

Weaknesses:
1.	The most obvious weakness is that the whole method consists of many distilling techniques, including training strategies, feature mimicking, and loss guidance. It is hard to see the inherent consistency between those techniques. This may harm the generalization ability and transferability of the proposed framework, as the author claims the framework is general. 
2.	The overall method is complex. I am worried about its application to other researchers.
3.	When applied to the Mixformer v2, which has only 2 layers, performance can be improved marginally while speed is unchanged. This may indicate the method's shortcomings. Complex techniques only bring a little improvement.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces CompressTracker, a novel general model compression framework that enhances the efficiency of transformer-based object tracking models. It innovatively segments transformer layers into stages, enabling a more effective emulation of complex teacher models by lightweight student models. The framework incorporates a unique replacement training technique, prediction guidance, and feature mimicking to refine the student model's performance. Extensive experiments demonstrate CompressTracker's effectiveness in significantly speeding up tracking models with minimal loss of accuracy, showcasing its potential for real-time applications on resource-constrained devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1）	Innovative Approach: The paper presents a novel compression framework, CompressTracker, which innovatively addresses the challenge of deploying transformer-based trackers on resource-limited devices by significantly reducing model size and computation cost without substantial loss of accuracy.

2）Structural Flexibility: A key advantage of the proposed framework is its structural agnosticism, allowing it to be compatible with any transformer architecture. This flexibility enables the adaptation of CompressTracker to various student model configurations, catering to diverse deployment environments and computational constraints.

3）Efficiency and Performance: The paper demonstrates through extensive experiments that CompressTracker achieves a remarkable balance between inference speed and tracking accuracy. It notably accelerates the tracking process while maintaining high performance levels, as evidenced by the nearly 96% retention of original accuracy with a 2.17× speedup.

Weaknesses:
1）The concept of ""prediction guidance and stage-wise feature mimicking"" and the idea of BEVDistill [1] seem somewhat similar.

2）Despite the model's efficiency in inference, the training process for CompressTracker is relatively inefficient.

3）While the paper shows promising results on certain benchmarks, there may be concerns about how well these findings generalize across different types of tracking tasks and real-world scenarios.

4）The paper does not compare with other model compression techniques, such as knowledge distillation, model quantization, and pruning.

5）According to the results in Table 3, I observed that the outcomes of CompressTracker-2 are inferior to those of MixFormerV2-S. What could be the reason for this?

6）It is necessary to apply compression to other tracking models in order to further validate the efficacy of the CompressTracker presented in this paper.

7）The authors lack a sufficiently comprehensive review of the related work. The authors should give more reasonable related work by carefully introducing the recent approaches to tracking with compression, such as [2].

[1] BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection, ICLR 2023.

[2] Distilled Siamese Networks for Visual Tracking, TPAMI 2021.

Limitations:
Please refer to weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors proposed a general model compression framework for efficient Transformer object tracking, named CompressTracker. The method adopts a novel stage partitioning strategy to divide the Transformer layers of the teacher model into different stages, enabling the student model to more effectively simulate each corresponding teacher stage. The authors also designed a unique replacement training technique, which involves randomly replacing specific stages in the student model with specific stages in the teacher model. Replacement training enhances the student model's ability to replicate the behavior of the teacher model. To further force the student model to simulate the teacher model, we combine predictive guidance and staged feature imitation to provide additional supervision during the compression process of the teacher model. The authors conducted a series of experiments to verify the effectiveness and generality of CompressTracker.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The author has clear ideas and the article is easy to understand. He proposes a general compression framework for single object tracking. This method can efficiently compress large object tracking models into small models. The author has conducted a large number of experiments to prove the effectiveness of this method.

Weaknesses:
The font size of the pictures in the article is too small. The author can adjust the font size appropriately to facilitate reading. The training time line in Figure 1a is blocked, resulting in incomplete display. The font size of the tables is inconsistent, for example, the font size of Tables 5, 6, 7, and 8 is too large. The abstract is redundant and can be appropriately deleted.

Limitations:
For lightweight tracking models, the training time is too long. The author can try to find new ways to reduce the time spent on training.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces CompressTracker, a general model compression framework for efficient transformer-based object tracking. CompressTracker divides the teacher model into stages corresponding to student model layers and randomly replaces student stages with teacher stages during training. It also aligns the teacher and student models using prediction guidance and feature mimicking. The framework gradually increases the probability of using student stages throughout training. CompressTracker achieves significant speed improvements while maintaining high accuracy. For example, CompressTracker-4 accelerates OSTrack by 2.17x while preserving 96% of its accuracy on LaSOT.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Versatility: Compatible with various transformer architectures for student models.
- Efficiency: Achieves a good balance between inference speed and tracking accuracy.
- Streamlined training: Offers a single-step, end-to-end training process, simplifying the compression pipeline.

Weaknesses:
- Limited theoretical analysis: The paper focuses on empirical results without providing much theoretical justification for the proposed methods.
- Lack of ablation on some components: Some components of the framework are not thoroughly explored. For instance, the impact of different feature mimicking strategies is not extensively analyzed.
- Performance and Efficiency Trade-off: While CompressTracker maintains high accuracy, there's a slight performance drop compared to the original model. Training time for CompressTracker-4 (with only 4 blocks) exceeds that of the original OSTrack. This trade-off between training efficiency, inference speed, and model performance requires further optimization.
- The core idea of reducing the number of Transformer blocks is not new. Similar approaches have been used in other models like TinyViT[1] and MiniViT[2].


[1] Wu K, Zhang J, Peng H, et al. Tinyvit: Fast pretraining distillation for small vision transformers[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 68-85.
[2] Zhang J, Peng H, Wu K, et al. Minivit: Compressing vision transformers with weight multiplexing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 12145-12154.

Limitations:
No

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors proposed a novel compression strategy of transformer based trackers. Unlike previous works, it divides the teacher network into multiple segments, each segment corresponds a single transformer layer of student network, then train each student layer separately. It also introduced some training strategies to enhance performance including (progressive) replacement training, prediction guidance and feature mimicking. Such compression framework is insensitive to the change of architecture of teacher network.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Effectiveness. The experiment results clearly demonstrated significant improvement of inference speed while reserving the majority of tracking accuracy.
2. Flexibility. The proposed compression strategy is insensitive to the change of architecture of tracking models, making it easy to apply on almost any transformer based trackers. The segmentation strategy and the size of student network also supports user customization, which enables the user to design student network according to their unique demands. Such flexibility shows excellent application prospects in end-side scenarios.

Weaknesses:
The detailed strategy of dividing the teacher network is not stated clearly in the paper. Base on the pseudo code provided in page 13, it seems that the segmentation strategy is simply mapping the list of transformer blocks of student network to that of the teacher network base on the lengths of the two lists. This could be too simple.

For example, assume teacher network has 8 transformer blocks in module 1 and 2 blocks in module 2, while student network consists of 2 blocks, then the second student block would have to emulate the last 3 blocks of module 1 and the 2 blocks of module 2, while module 1 and  module 2 might have been trained separately and possess different knowledge. Empirically, this would result in sub-optical performance.

A brief discuss on the divide strategy could help this paper become more informative.

Limitations:
The paper clearly addressed its limitations including inefficient training process and the performance gap between teacher and student network.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to  distill knowledge from larger teacher models into more compact student trackers. Three techniques are proposed: A stage division strategy that segments the transformer layers of the teacher model. Replacement training technique. Prediction guidance and stage-wise feature mimicking. Experiment verifys the effectiveness of the method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The proposed techniques are comprehensive and include a bunch of methods to improve the performance and efficiency of the trackers, 
2.	The experiments are extensive which includes 5 VOT benchmarks. 
3.	The speed is fast when applying the 2 layer tracker variants.

Weaknesses:
1.	The most obvious weakness is that the whole method consists of many distilling techniques, including training strategies, feature mimicking, and loss guidance. It is hard to see the inherent consistency between those techniques. This may harm the generalization ability and transferability of the proposed framework, as the author claims the framework is general. 
2.	The overall method is complex. I am worried about its application to other researchers.
3.	When applied to the Mixformer v2, which has only 2 layers, performance can be improved marginally while speed is unchanged. This may indicate the method's shortcomings. Complex techniques only bring a little improvement.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces CompressTracker, a novel general model compression framework that enhances the efficiency of transformer-based object tracking models. It innovatively segments transformer layers into stages, enabling a more effective emulation of complex teacher models by lightweight student models. The framework incorporates a unique replacement training technique, prediction guidance, and feature mimicking to refine the student model's performance. Extensive experiments demonstrate CompressTracker's effectiveness in significantly speeding up tracking models with minimal loss of accuracy, showcasing its potential for real-time applications on resource-constrained devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1）	Innovative Approach: The paper presents a novel compression framework, CompressTracker, which innovatively addresses the challenge of deploying transformer-based trackers on resource-limited devices by significantly reducing model size and computation cost without substantial loss of accuracy.

2）Structural Flexibility: A key advantage of the proposed framework is its structural agnosticism, allowing it to be compatible with any transformer architecture. This flexibility enables the adaptation of CompressTracker to various student model configurations, catering to diverse deployment environments and computational constraints.

3）Efficiency and Performance: The paper demonstrates through extensive experiments that CompressTracker achieves a remarkable balance between inference speed and tracking accuracy. It notably accelerates the tracking process while maintaining high performance levels, as evidenced by the nearly 96% retention of original accuracy with a 2.17× speedup.

Weaknesses:
1）The concept of ""prediction guidance and stage-wise feature mimicking"" and the idea of BEVDistill [1] seem somewhat similar.

2）Despite the model's efficiency in inference, the training process for CompressTracker is relatively inefficient.

3）While the paper shows promising results on certain benchmarks, there may be concerns about how well these findings generalize across different types of tracking tasks and real-world scenarios.

4）The paper does not compare with other model compression techniques, such as knowledge distillation, model quantization, and pruning.

5）According to the results in Table 3, I observed that the outcomes of CompressTracker-2 are inferior to those of MixFormerV2-S. What could be the reason for this?

6）It is necessary to apply compression to other tracking models in order to further validate the efficacy of the CompressTracker presented in this paper.

7）The authors lack a sufficiently comprehensive review of the related work. The authors should give more reasonable related work by carefully introducing the recent approaches to tracking with compression, such as [2].

[1] BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection, ICLR 2023.

[2] Distilled Siamese Networks for Visual Tracking, TPAMI 2021.

Limitations:
Please refer to weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors proposed a general model compression framework for efficient Transformer object tracking, named CompressTracker. The method adopts a novel stage partitioning strategy to divide the Transformer layers of the teacher model into different stages, enabling the student model to more effectively simulate each corresponding teacher stage. The authors also designed a unique replacement training technique, which involves randomly replacing specific stages in the student model with specific stages in the teacher model. Replacement training enhances the student model's ability to replicate the behavior of the teacher model. To further force the student model to simulate the teacher model, we combine predictive guidance and staged feature imitation to provide additional supervision during the compression process of the teacher model. The authors conducted a series of experiments to verify the effectiveness and generality of CompressTracker.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The author has clear ideas and the article is easy to understand. He proposes a general compression framework for single object tracking. This method can efficiently compress large object tracking models into small models. The author has conducted a large number of experiments to prove the effectiveness of this method.

Weaknesses:
The font size of the pictures in the article is too small. The author can adjust the font size appropriately to facilitate reading. The training time line in Figure 1a is blocked, resulting in incomplete display. The font size of the tables is inconsistent, for example, the font size of Tables 5, 6, 7, and 8 is too large. The abstract is redundant and can be appropriately deleted.

Limitations:
For lightweight tracking models, the training time is too long. The author can try to find new ways to reduce the time spent on training.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces CompressTracker, a general model compression framework for efficient transformer-based object tracking. CompressTracker divides the teacher model into stages corresponding to student model layers and randomly replaces student stages with teacher stages during training. It also aligns the teacher and student models using prediction guidance and feature mimicking. The framework gradually increases the probability of using student stages throughout training. CompressTracker achieves significant speed improvements while maintaining high accuracy. For example, CompressTracker-4 accelerates OSTrack by 2.17x while preserving 96% of its accuracy on LaSOT.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Versatility: Compatible with various transformer architectures for student models.
- Efficiency: Achieves a good balance between inference speed and tracking accuracy.
- Streamlined training: Offers a single-step, end-to-end training process, simplifying the compression pipeline.

Weaknesses:
- Limited theoretical analysis: The paper focuses on empirical results without providing much theoretical justification for the proposed methods.
- Lack of ablation on some components: Some components of the framework are not thoroughly explored. For instance, the impact of different feature mimicking strategies is not extensively analyzed.
- Performance and Efficiency Trade-off: While CompressTracker maintains high accuracy, there's a slight performance drop compared to the original model. Training time for CompressTracker-4 (with only 4 blocks) exceeds that of the original OSTrack. This trade-off between training efficiency, inference speed, and model performance requires further optimization.
- The core idea of reducing the number of Transformer blocks is not new. Similar approaches have been used in other models like TinyViT[1] and MiniViT[2].


[1] Wu K, Zhang J, Peng H, et al. Tinyvit: Fast pretraining distillation for small vision transformers[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 68-85.
[2] Zhang J, Peng H, Wu K, et al. Minivit: Compressing vision transformers with weight multiplexing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 12145-12154.

Limitations:
No

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
3zYmlmkIuK;"REVIEW 
Summary:
In this paper, the authors study multi-agent reinforcement learning where agents cooperate through asynchronous communications with a central server to learn a shared environment. They consider the following two settings: multi-agent contextual bandits with general function approximation, and multi-agent RL with general function approximation. For both settings, they propose provably efficient algorithms with low regret and low communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem of asynchronous MARL with general function approximation is interesting and important.

2. This paper is the first to consider the setting with general function approximation. The results are solid and the proof looks good to me.

3. For both settings, the authors propose provably efficient algorithms. The results generalize previous results under the linear setting.

Weaknesses:
1. It seems that part of the techniques is from previous results, such as the bonus function oracle. It will be helpful if there is a section discussing technical novelty.

2. It seems that the setting is closely related to low switching RL and RL with delayed feedback. It will be interesting if the authors could briefly discuss about the connections.

3. For the communication complexity bound in theorem 5.1, should it be $/\alpha$ instead of $\alpha$? In addition, why not choose $\alpha=1/M$ in both theorems? In this way, the communication cost can be improved. (Please correct me if I misunderstood anything) 

4. Line ?? in line 214 of page 6. Please correct it.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose two algorithms for asynchronous communication in multi-agent reinforcement learning with generalized value function approximation: Asynchronous-NLin-UCB for context bandit scenarios and Asynchronous-NLSVI-UCB for episodic MDP scenarios. These algorithms achieve near-optimal regret with low communication complexity. The authors theoretically show the trade-off between regret and communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors provide a detailed background on the related literature concerning regret, communication complexity, and the presence of asynchronous update, which is greatly helpful in understanding the contributions of the proposed algorithms. 
- The theoretical foundations and proofs regarding the communication criterion are important and interesting. Also, the trade-off between regret and communication complexity via parameter $\alpha$ offers valuable insights. 
- The approach of receiving decisions and bonus functions from a central server instead of historical data is intuitive and appears crucial from a privacy perspective.

Weaknesses:
While I have studied general value function approximation, I do not have research background in this field for multi-agent scenarios. Therefore, my critique may not have captured the weaknesses of this paper. 
I’m open to revising my score based on the authors' responses.

As far as I know, MARL often adopts the Centralized Training Decentralized Execution(CTDE) framework to avoid the action space growing exponentially with the number of users. However, it is unclear whether the proposed scenario follows ""decentralized execution"". Agents are supposed to execute based on partial observations in a decentralized manner, but the proposed approach appears to involve a central server consistently during execution. If the proposed scenario is inconsistent with CTDE, I would be interested to hear from the authors what the distinct advantages or necessity of this scenario is.

Typos:

- Line 214: Reference to the label is not correctly written.
- Theorems 4.3 and 5.1: $\tilde{\beta}$ is not properly defined in the statement, and $\beta_t$ should be fixed to $\tilde{\beta}$.
- Theorem 5.1: Total communication complexity should be fixed to $O((1+M\alpha)^2 / \alpha)$.

Limitations:
The requirement for global state instead of partial observation may limit the practical applicability of the proposed methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the asynchronous multi-agent bandit and RL problem with general function approximation (measured by Elude dimension). The main contribution is to establish $\tilde{O}(\sqrt{\text{dim} T})$ regret bound with $\tilde{O}(M^2 \text{dim})$ communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well written and the contribution is solid.

Weaknesses:
I think the major concern is non-optimal complexity bounds. Although it seems unreasonable to ask for a matching upper\&lower regret bound for the contextual bandit problem, the part about RL could be possibly improved (at least, the dependence on $H$ is not tight). Also I am curious that what is the current best lower bound for the communication cost to reach an $\sqrt{T}$ regret bound. It would be an interesting problem to study the exact trade-off between the communication cost and regret.

A minor concern might be about the technical novelty given previous methods on measuring the uncertainty.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studied the distributed federated contextual bandit and federated reinforcement learning (FRL) in the presence of a trusted server. In both problems, nonlinearity and asynchronous communications are explored. Similar algorithms for contextual bandit and FRL that encourage exploration via bonus functions are proposed. Finite-time convergence results in terms of regrets are established for both algorithms and communication complexities are also characterized.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This paper studied the asynchronous federated learning problem where only one agent is activated to sample data and infrequently communicate with server.
* The trigger-based communication is an interesting approach in multi-agent or multi-learner problems.

Weaknesses:
* The clarity of some of the important quantities are not well defined or explained. For example, 
1)	In the sample complexity result of Theorem 4.3, $\tilde{\beta}_1$ is used. However, it was not defined. It’s unclear what this notation is referring to. Similarly, in Theorem 5.1, $\tilde{\beta}_2$ is used.
2)	The oracle for to compute bonus term bk+1,h is crucial in understanding the algorithms. However, it was not very well-explained or shown anywhere in the main paper.	
3)	The two sentences from Line 300 to Line 302 are confusing. Please clarify them.
* Typos:
1)	An extra closing parenthesis appeared in Line 141.
2)	Line 214, ?? -> 12
3)	In Line 1 of algorithm 3, $k=[K]$ -> $k\in [K]$.
4)	In Line 154, the trajectory should be $(s_h, a_h, \cdots, s_H, a_H)$.

Limitations:
Please see weaknesses and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors study multi-agent reinforcement learning where agents cooperate through asynchronous communications with a central server to learn a shared environment. They consider the following two settings: multi-agent contextual bandits with general function approximation, and multi-agent RL with general function approximation. For both settings, they propose provably efficient algorithms with low regret and low communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem of asynchronous MARL with general function approximation is interesting and important.

2. This paper is the first to consider the setting with general function approximation. The results are solid and the proof looks good to me.

3. For both settings, the authors propose provably efficient algorithms. The results generalize previous results under the linear setting.

Weaknesses:
1. It seems that part of the techniques is from previous results, such as the bonus function oracle. It will be helpful if there is a section discussing technical novelty.

2. It seems that the setting is closely related to low switching RL and RL with delayed feedback. It will be interesting if the authors could briefly discuss about the connections.

3. For the communication complexity bound in theorem 5.1, should it be $/\alpha$ instead of $\alpha$? In addition, why not choose $\alpha=1/M$ in both theorems? In this way, the communication cost can be improved. (Please correct me if I misunderstood anything) 

4. Line ?? in line 214 of page 6. Please correct it.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose two algorithms for asynchronous communication in multi-agent reinforcement learning with generalized value function approximation: Asynchronous-NLin-UCB for context bandit scenarios and Asynchronous-NLSVI-UCB for episodic MDP scenarios. These algorithms achieve near-optimal regret with low communication complexity. The authors theoretically show the trade-off between regret and communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors provide a detailed background on the related literature concerning regret, communication complexity, and the presence of asynchronous update, which is greatly helpful in understanding the contributions of the proposed algorithms. 
- The theoretical foundations and proofs regarding the communication criterion are important and interesting. Also, the trade-off between regret and communication complexity via parameter $\alpha$ offers valuable insights. 
- The approach of receiving decisions and bonus functions from a central server instead of historical data is intuitive and appears crucial from a privacy perspective.

Weaknesses:
While I have studied general value function approximation, I do not have research background in this field for multi-agent scenarios. Therefore, my critique may not have captured the weaknesses of this paper. 
I’m open to revising my score based on the authors' responses.

As far as I know, MARL often adopts the Centralized Training Decentralized Execution(CTDE) framework to avoid the action space growing exponentially with the number of users. However, it is unclear whether the proposed scenario follows ""decentralized execution"". Agents are supposed to execute based on partial observations in a decentralized manner, but the proposed approach appears to involve a central server consistently during execution. If the proposed scenario is inconsistent with CTDE, I would be interested to hear from the authors what the distinct advantages or necessity of this scenario is.

Typos:

- Line 214: Reference to the label is not correctly written.
- Theorems 4.3 and 5.1: $\tilde{\beta}$ is not properly defined in the statement, and $\beta_t$ should be fixed to $\tilde{\beta}$.
- Theorem 5.1: Total communication complexity should be fixed to $O((1+M\alpha)^2 / \alpha)$.

Limitations:
The requirement for global state instead of partial observation may limit the practical applicability of the proposed methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the asynchronous multi-agent bandit and RL problem with general function approximation (measured by Elude dimension). The main contribution is to establish $\tilde{O}(\sqrt{\text{dim} T})$ regret bound with $\tilde{O}(M^2 \text{dim})$ communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well written and the contribution is solid.

Weaknesses:
I think the major concern is non-optimal complexity bounds. Although it seems unreasonable to ask for a matching upper\&lower regret bound for the contextual bandit problem, the part about RL could be possibly improved (at least, the dependence on $H$ is not tight). Also I am curious that what is the current best lower bound for the communication cost to reach an $\sqrt{T}$ regret bound. It would be an interesting problem to study the exact trade-off between the communication cost and regret.

A minor concern might be about the technical novelty given previous methods on measuring the uncertainty.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studied the distributed federated contextual bandit and federated reinforcement learning (FRL) in the presence of a trusted server. In both problems, nonlinearity and asynchronous communications are explored. Similar algorithms for contextual bandit and FRL that encourage exploration via bonus functions are proposed. Finite-time convergence results in terms of regrets are established for both algorithms and communication complexities are also characterized.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This paper studied the asynchronous federated learning problem where only one agent is activated to sample data and infrequently communicate with server.
* The trigger-based communication is an interesting approach in multi-agent or multi-learner problems.

Weaknesses:
* The clarity of some of the important quantities are not well defined or explained. For example, 
1)	In the sample complexity result of Theorem 4.3, $\tilde{\beta}_1$ is used. However, it was not defined. It’s unclear what this notation is referring to. Similarly, in Theorem 5.1, $\tilde{\beta}_2$ is used.
2)	The oracle for to compute bonus term bk+1,h is crucial in understanding the algorithms. However, it was not very well-explained or shown anywhere in the main paper.	
3)	The two sentences from Line 300 to Line 302 are confusing. Please clarify them.
* Typos:
1)	An extra closing parenthesis appeared in Line 141.
2)	Line 214, ?? -> 12
3)	In Line 1 of algorithm 3, $k=[K]$ -> $k\in [K]$.
4)	In Line 154, the trajectory should be $(s_h, a_h, \cdots, s_H, a_H)$.

Limitations:
Please see weaknesses and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
4FwlejUlg5;"REVIEW 
Summary:
This paper investigates the problem of latent post-treatment bias in causal models where there exists some proxy variables of the latent confounder and post-treatment variables. The authors first derive a general form of latent post-treatment bias which is intractable in most situations (except in special cases such as linear SCM). The authors state that the latent post-treatment bias can be arbitrarily bad for existing proxy-based causal inference methods. They then propose an identifiable VAE-based causal inference algorithm under the assumption that at least one dimension of each sufficient statistic of the latent prior is invertible. The proposed method is evaluated on both synthetic and real-world datasets to demonstrate its causal effect estimation capability with the presence of both latent confounders and post-treatment variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
• Causal reasoning in the context of latent confounder and post-treatment variables is an important topic especially with observational data.

• The authors clearly state the necessary assumptions for the identifiability of true latent variables, and the logic of determining the dimensions of $\boldsymbol{C}$ and $\boldsymbol{M}$ is well presented.

• The paper has a well-established theoretical basis.

Weaknesses:
•	For the illustrative example in the introduction, it might be better to explicitly specify what the post-treatment variable is.

•	Other existing works [1-3] on identifying latent confounder/mediators based on the iVAE architecture should also be included in the related work.

•	The role of post-treatment variables $\boldsymbol{M}$ seems to be a bit ambiguous. To be specific, is Theorem 4.1 valid for all types of relationships between $\boldsymbol{M}$ and $Y$?

•	The illustration of (iv) in Assumption 3 is a little confusing, as it assumes one extra degree of freedom on the prior parameters of $\boldsymbol{Z}$ and is critical to the identifiability of $\boldsymbol{Z}$ from $\boldsymbol{X}$. More explanation on this point will be appreciated.

•	The empirical evaluation consists of only one real-world dataset, which somehow limits the applicability of the proposed method.

References:

[1]. Zhou, D., & Wei, X. X. (2020). Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE. Advances in Neural Information Processing Systems, 33, 7234-7247.

[2]. Sorrenson, P., Rother, C., & Köthe, U. (2020). Disentanglement by nonlinear ica with general incompressible-flow networks (gin). arXiv preprint arXiv:2001.04872.

[3]. Jiang, Z., Liu, Y., Klein, M. H., Aloui, A., Ren, Y., Li, K., ... & Carlson, D. (2023). Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators. arXiv preprint arXiv:2306.07918.

Limitations:
The authors do not include a paragraph discussing the limitations and potential societal impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors deal with latent post-treatment bias for proxy-based methods which are employed for causal effect estimation.
They show that post-treatment variables can be latent and mixed into the observed covariates along with the latent confounders.
The authors transform the confounder-identifiability problem into a tractable pair-wise conditional independence test problem.
They prove that the latent confounders and latent post-treatment variables can be identified up to bijective transformations. Finally, they provide experimental analysis for their approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper deals with a very interesting problem.	 The proposed method appeared to be theoretically robust. The method is evaluated with proper experimental analysis on synthetic and real-world datasets and compared with multiple benchmarks.

Weaknesses:
Here I provide some weaknesses of the paper:
* Bi-directed edges in Figure 1 are not defined properly.
* Do-operator in equation 3 is not defined in detail.
* Assumptions in Assumption 2 should be described in more detail.
* The proposed method seems to depend on a lot of assumptions. Assumptions 1,2,3 each contain multiple assumptions. The authors should explain how their assumptions hold for the real-world scenarios they considered in their experiment section.

Limitations:
The authors discussed a very few limitations of their paper but more discussion should be done.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of causal inference with observational data, particularly when direct measurement of confounders is infeasible. The authors propose a new method, Confounder-identifiable Variational Autoencoder (CiVAE), to mitigate post-treatment bias using observed proxies for both latent confounders and latent post-treatment variables. The paper provides a theoretical analysis under specific assumptions and validates the proposed approach through experiments on both simulated and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper investigates a critical question concerning the mitigation of post-treatment bias, which is essential in various practical scenarios.
* The ideas presented in the paper are clear and easy to follow, and the theoretical analysis is well-established.

Weaknesses:
* In practical scenarios, interactions among latent factors are often present and can significantly impact the estimation. It would be beneficial if the authors could elaborate on how their method addresses these interactions and whether there are any theoretical guarantees regarding their handling in the proposed approach.

* The theoretical guarantees rely on strong assumptions, and the assumptions are hard to verify in practice. In assumption 1, the paper assumes an injective function of latent confounders and latent post-treatment variables into the observed proxy. This is a strong assumption,  and it will be much harder to meet the assumption in general when the function is nonlinear. The specific setup with strong assumptions limits the practical applicability of the proposed approach. It would be helpful if the authors could provide examples where these assumptions hold and demonstrate how they can be verified.

* The experiment lacks sufficient details on setup and implementation. Could the authors provide more specific information to enhance understanding of the empirical results?

Limitations:
* The proposed method relies on very strong assumptions to ensure identifiability, which can be challenging to verify in practical applications.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors investigated the issue of latent post-treatment bias in causal inference from observational data. They showed that estimator of existing proxy-of-confounder-based methd, i.e., DEV (f(X)), is an arbitrarily biased estimator of the Average Treatment Effect (ATE), when the selected proxy of confounders X accidentally mixes in latent post-treatment variables (Theorem 3.2). To address this issue, they proposed the Confounder-identifiable VAE (CiVAE), which identifies latent confounders up to bijective transformations under a mild assumption regarding the prior of latent factors. They showed that controlling for latent confounders inferred by CiVAE can provide an unbiased estimation of the ATE. Experiments on both simulated and real-world datasets demonstrate that CiVAE exhibits superior robustness to latent post-treatment bias compared to state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Being able to recover latent variables (cofounders, post-treatment variables, or others) from observations is challenging and important. Ignoring latent variables or assuming non-existence of latent variables is unrealistic and can lead to the wrong conclusion and decisions. The authors further motivated the importance of recovering latent cofounders, post-treatment variables and the consequence of not doing so  (Theorem 3.2). The solution provided shows originality and quality.

Weaknesses:
The presentation can be improved.

Limitations:
n.a.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the problem of latent post-treatment bias in causal models where there exists some proxy variables of the latent confounder and post-treatment variables. The authors first derive a general form of latent post-treatment bias which is intractable in most situations (except in special cases such as linear SCM). The authors state that the latent post-treatment bias can be arbitrarily bad for existing proxy-based causal inference methods. They then propose an identifiable VAE-based causal inference algorithm under the assumption that at least one dimension of each sufficient statistic of the latent prior is invertible. The proposed method is evaluated on both synthetic and real-world datasets to demonstrate its causal effect estimation capability with the presence of both latent confounders and post-treatment variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
• Causal reasoning in the context of latent confounder and post-treatment variables is an important topic especially with observational data.

• The authors clearly state the necessary assumptions for the identifiability of true latent variables, and the logic of determining the dimensions of $\boldsymbol{C}$ and $\boldsymbol{M}$ is well presented.

• The paper has a well-established theoretical basis.

Weaknesses:
•	For the illustrative example in the introduction, it might be better to explicitly specify what the post-treatment variable is.

•	Other existing works [1-3] on identifying latent confounder/mediators based on the iVAE architecture should also be included in the related work.

•	The role of post-treatment variables $\boldsymbol{M}$ seems to be a bit ambiguous. To be specific, is Theorem 4.1 valid for all types of relationships between $\boldsymbol{M}$ and $Y$?

•	The illustration of (iv) in Assumption 3 is a little confusing, as it assumes one extra degree of freedom on the prior parameters of $\boldsymbol{Z}$ and is critical to the identifiability of $\boldsymbol{Z}$ from $\boldsymbol{X}$. More explanation on this point will be appreciated.

•	The empirical evaluation consists of only one real-world dataset, which somehow limits the applicability of the proposed method.

References:

[1]. Zhou, D., & Wei, X. X. (2020). Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE. Advances in Neural Information Processing Systems, 33, 7234-7247.

[2]. Sorrenson, P., Rother, C., & Köthe, U. (2020). Disentanglement by nonlinear ica with general incompressible-flow networks (gin). arXiv preprint arXiv:2001.04872.

[3]. Jiang, Z., Liu, Y., Klein, M. H., Aloui, A., Ren, Y., Li, K., ... & Carlson, D. (2023). Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators. arXiv preprint arXiv:2306.07918.

Limitations:
The authors do not include a paragraph discussing the limitations and potential societal impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors deal with latent post-treatment bias for proxy-based methods which are employed for causal effect estimation.
They show that post-treatment variables can be latent and mixed into the observed covariates along with the latent confounders.
The authors transform the confounder-identifiability problem into a tractable pair-wise conditional independence test problem.
They prove that the latent confounders and latent post-treatment variables can be identified up to bijective transformations. Finally, they provide experimental analysis for their approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper deals with a very interesting problem.	 The proposed method appeared to be theoretically robust. The method is evaluated with proper experimental analysis on synthetic and real-world datasets and compared with multiple benchmarks.

Weaknesses:
Here I provide some weaknesses of the paper:
* Bi-directed edges in Figure 1 are not defined properly.
* Do-operator in equation 3 is not defined in detail.
* Assumptions in Assumption 2 should be described in more detail.
* The proposed method seems to depend on a lot of assumptions. Assumptions 1,2,3 each contain multiple assumptions. The authors should explain how their assumptions hold for the real-world scenarios they considered in their experiment section.

Limitations:
The authors discussed a very few limitations of their paper but more discussion should be done.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of causal inference with observational data, particularly when direct measurement of confounders is infeasible. The authors propose a new method, Confounder-identifiable Variational Autoencoder (CiVAE), to mitigate post-treatment bias using observed proxies for both latent confounders and latent post-treatment variables. The paper provides a theoretical analysis under specific assumptions and validates the proposed approach through experiments on both simulated and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper investigates a critical question concerning the mitigation of post-treatment bias, which is essential in various practical scenarios.
* The ideas presented in the paper are clear and easy to follow, and the theoretical analysis is well-established.

Weaknesses:
* In practical scenarios, interactions among latent factors are often present and can significantly impact the estimation. It would be beneficial if the authors could elaborate on how their method addresses these interactions and whether there are any theoretical guarantees regarding their handling in the proposed approach.

* The theoretical guarantees rely on strong assumptions, and the assumptions are hard to verify in practice. In assumption 1, the paper assumes an injective function of latent confounders and latent post-treatment variables into the observed proxy. This is a strong assumption,  and it will be much harder to meet the assumption in general when the function is nonlinear. The specific setup with strong assumptions limits the practical applicability of the proposed approach. It would be helpful if the authors could provide examples where these assumptions hold and demonstrate how they can be verified.

* The experiment lacks sufficient details on setup and implementation. Could the authors provide more specific information to enhance understanding of the empirical results?

Limitations:
* The proposed method relies on very strong assumptions to ensure identifiability, which can be challenging to verify in practical applications.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors investigated the issue of latent post-treatment bias in causal inference from observational data. They showed that estimator of existing proxy-of-confounder-based methd, i.e., DEV (f(X)), is an arbitrarily biased estimator of the Average Treatment Effect (ATE), when the selected proxy of confounders X accidentally mixes in latent post-treatment variables (Theorem 3.2). To address this issue, they proposed the Confounder-identifiable VAE (CiVAE), which identifies latent confounders up to bijective transformations under a mild assumption regarding the prior of latent factors. They showed that controlling for latent confounders inferred by CiVAE can provide an unbiased estimation of the ATE. Experiments on both simulated and real-world datasets demonstrate that CiVAE exhibits superior robustness to latent post-treatment bias compared to state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Being able to recover latent variables (cofounders, post-treatment variables, or others) from observations is challenging and important. Ignoring latent variables or assuming non-existence of latent variables is unrealistic and can lead to the wrong conclusion and decisions. The authors further motivated the importance of recovering latent cofounders, post-treatment variables and the consequence of not doing so  (Theorem 3.2). The solution provided shows originality and quality.

Weaknesses:
The presentation can be improved.

Limitations:
n.a.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
3mzFmBPFIX;"REVIEW 
Summary:
This work presents a method for learning metriplectic systems from data. Metriplectic systems are a model which conserve energy and produce entropy, two desirable features. Their method, termed “neural metriplectic systems” (NMS), is based on a more efficient system parametrization. The authors also prove universal approximation results on non-degenerate systems, and generalization error bounds. They verify that their method outperforms other metriplectic-learning baselines, GNODE and GFINN, on two physical experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality: Although I am not at all an expert in this field and therefore cannot properly judge, it seems that the main theorem (Theorem 3.4) is novel and non-trivial.

Quality: The proposed method outperforms baselines in two experimental settings, verifying the expected gain from having a more efficient parametrization. The corresponding theoretical results, on universality and generalization, provide a fairly thorough picture of the method. 

Significance: Within the field of learning metriplectic systems, this paper seems to make a valuable contribution and improve on prior work.

Clarity: The paper is very well-written, and mathematically rigorous. Although the details are not accessible to someone without a background that matches the subject material rather closely, the high-level ideas about the benefits of metriplectic systems, what past work has done, and the advantages of their new method, are conveyed well.

Weaknesses:
Clarity:
1. Although well-written, the paper is not accessible to most machine learning audiences, and seemingly requires the reader to already have a physics background in phenomenological modeling, or exterior algebra.

2. Mathematical terms such as algebraic brackets, Poisson brackets, degenerate metric brackets, etc. should be defined in the beginning of the paper, or with a reference to a textbook or other paper defining them. The “exterior algebra” background is suitable for only those with a strong mathematical background already, using terms like “wedge product” and “graded algebra” without definition. (Admittedly, it would be impossible to fully explain all of these concepts in only 9 pages — perhaps a citation to a textbook would be helpful here, but in practice if the reader needs to understand the decomposition result properly to grasp the contribution, then this work may be more suitable for a venue other than a machine learning conference.)

Quality: The baselines in experiments, as well as the methods discussed in the exposition, are all metriplectic. However, it seems like other methods (e.g. which preserve energy but do not increase entropy, or even those which are not physics-informed at all), should be included too.

Significance: I am not sure how widely applicable metriplectic learning systems are, or what alternative (non-metriplectic) methods can be used for the same problems. The paper would be improved by providing more of this background/motivation.

Overall, as a non-expert, my main concern is with the suitability of this work for a machine learning conference - I defer to the AC on this point. It seems that the machine learning techniques used within NMS are fairly straightforward, while it is the parametrization in Theorem 3.4 that seemingly constitutes the crux of the method. However, the statement and proof of Theorem 3.4 would be more accessible to a physics or math audience, than an ML audience.

Limitations:
Limitations are discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new parameterization for neural metriplectic systems which explicitly incorporates structural information about the degeneracy conditions $\{ S, \cdot \} = 0, [E,\cdot ] = 0$ into the model. The model requires $\sim O(n^2)$ learnable parameters for a problem with $n$ state variables instead of some prior methods which need $O(n^3)$. Further, it also encodes this degeneracy condition in a hard constraint, leading to models which will by construction respect these desired physical conditions. The authors provide a deep learning implementation scheme for their method which involves learning $E(x), S(x)$ and using $\nabla E, \nabla S$ to construct the matrices $L, M$ needed for the bracket from observed trajectories of the physical system. The gradients $\nabla E, \nabla S$ needed for the brackets are computed with autodifferentiation. The authors show that this system is trained end-to-end on simple physical systems including a two-gas system and a thermo-elastic pendulum and can outperform existing methods on these benchmarks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper motivates the need for metriplectic systems which can be efficiently implemented and that incorporate the dynamical constraints required for these systems (energy conservation and entropy production). This captures a potentially interesting class of physical systems that could be modeled by machine learning methods like those employed in the present work. The method that they present as Algorithm 1 is straightforward and improves upon the cubic time complexity of GNODE or GFINN. The authors also provide an approximation result for their algorithm and support their claims with some experiments.

Weaknesses:
While the authors improve the scaling from cubic to quadratic in the number of state variables, the total complexity (quadratic) still scales poorly with size of the problem (number of state variables / dimensions). Further, the current experiments and comparisons were performed on small benchmarks. However, since this paper is the first to point out that the cubic scaling can be improved by reflecting constraints due to degeneracy, I think the experimental component of the contribution is not the most important.

Limitations:
The authors do mention the primary limitations of this present work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a parameter efficient parameterization for neural networks simulating metripletic systems, which are differential equations that have both an energy dissipative piece and an energy conservative piece. The method works by learning several of the required quantities (L and M, which trade off dissipation and conservation, I believe), while also using a small neural network to estimate the dissipation and conservation pieces (E(x) and S(x) ). As not all quantities in the state, x, can be observed, they use a time based diffusion model to emulate the hidden states (e.g. entropy) to develop initial conditions for these. Experiments are performed on two systems of this class, where it seems like the method performs better (probably due to having better inductive biases).


Unfortunately, due to not having a strong physics background, I feel somewhat unqualified to judge many of the technical strengths although things seem reasonable from a skim. I don’t know if I can properly assess novelty and significance as a result.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Significance:
-	Building better emulators of physical systems that are complicated is a good first step in what the authors term “phenomenonological” understanding of these systems.

-	Even quicker training time (and demonstrated both practically and theoretically) is quite helpful. I remember one of the original issues with NODE was that it took a very long time to converge.

Clarity: 
-	Overall, the paper is pretty well written, even if quite dense, and okay to follow for a non expert physicist. I was able to follow at least the ML pieces and the experiments section quite well.

-	The relevant literature is reasonably well signposted; I learned a fair bit about the state of this field by checking the references.

Novelty:

-	The approach seems to have a clear inductive bias win over the prior works GFINN and GNODE due to better parameterization of the system.

Weaknesses:
Unfortunately, the writing ends up being quite dense and technical with minimal outside applications. 

Sure, emulating these physical systems in the experiments is quite nice, but what types of applications does this lead to? This is more of a writing based thing and the paper could be refactored around one of these applications if possible.

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents a method for learning metriplectic systems from data. Metriplectic systems are a model which conserve energy and produce entropy, two desirable features. Their method, termed “neural metriplectic systems” (NMS), is based on a more efficient system parametrization. The authors also prove universal approximation results on non-degenerate systems, and generalization error bounds. They verify that their method outperforms other metriplectic-learning baselines, GNODE and GFINN, on two physical experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality: Although I am not at all an expert in this field and therefore cannot properly judge, it seems that the main theorem (Theorem 3.4) is novel and non-trivial.

Quality: The proposed method outperforms baselines in two experimental settings, verifying the expected gain from having a more efficient parametrization. The corresponding theoretical results, on universality and generalization, provide a fairly thorough picture of the method. 

Significance: Within the field of learning metriplectic systems, this paper seems to make a valuable contribution and improve on prior work.

Clarity: The paper is very well-written, and mathematically rigorous. Although the details are not accessible to someone without a background that matches the subject material rather closely, the high-level ideas about the benefits of metriplectic systems, what past work has done, and the advantages of their new method, are conveyed well.

Weaknesses:
Clarity:
1. Although well-written, the paper is not accessible to most machine learning audiences, and seemingly requires the reader to already have a physics background in phenomenological modeling, or exterior algebra.

2. Mathematical terms such as algebraic brackets, Poisson brackets, degenerate metric brackets, etc. should be defined in the beginning of the paper, or with a reference to a textbook or other paper defining them. The “exterior algebra” background is suitable for only those with a strong mathematical background already, using terms like “wedge product” and “graded algebra” without definition. (Admittedly, it would be impossible to fully explain all of these concepts in only 9 pages — perhaps a citation to a textbook would be helpful here, but in practice if the reader needs to understand the decomposition result properly to grasp the contribution, then this work may be more suitable for a venue other than a machine learning conference.)

Quality: The baselines in experiments, as well as the methods discussed in the exposition, are all metriplectic. However, it seems like other methods (e.g. which preserve energy but do not increase entropy, or even those which are not physics-informed at all), should be included too.

Significance: I am not sure how widely applicable metriplectic learning systems are, or what alternative (non-metriplectic) methods can be used for the same problems. The paper would be improved by providing more of this background/motivation.

Overall, as a non-expert, my main concern is with the suitability of this work for a machine learning conference - I defer to the AC on this point. It seems that the machine learning techniques used within NMS are fairly straightforward, while it is the parametrization in Theorem 3.4 that seemingly constitutes the crux of the method. However, the statement and proof of Theorem 3.4 would be more accessible to a physics or math audience, than an ML audience.

Limitations:
Limitations are discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new parameterization for neural metriplectic systems which explicitly incorporates structural information about the degeneracy conditions $\{ S, \cdot \} = 0, [E,\cdot ] = 0$ into the model. The model requires $\sim O(n^2)$ learnable parameters for a problem with $n$ state variables instead of some prior methods which need $O(n^3)$. Further, it also encodes this degeneracy condition in a hard constraint, leading to models which will by construction respect these desired physical conditions. The authors provide a deep learning implementation scheme for their method which involves learning $E(x), S(x)$ and using $\nabla E, \nabla S$ to construct the matrices $L, M$ needed for the bracket from observed trajectories of the physical system. The gradients $\nabla E, \nabla S$ needed for the brackets are computed with autodifferentiation. The authors show that this system is trained end-to-end on simple physical systems including a two-gas system and a thermo-elastic pendulum and can outperform existing methods on these benchmarks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper motivates the need for metriplectic systems which can be efficiently implemented and that incorporate the dynamical constraints required for these systems (energy conservation and entropy production). This captures a potentially interesting class of physical systems that could be modeled by machine learning methods like those employed in the present work. The method that they present as Algorithm 1 is straightforward and improves upon the cubic time complexity of GNODE or GFINN. The authors also provide an approximation result for their algorithm and support their claims with some experiments.

Weaknesses:
While the authors improve the scaling from cubic to quadratic in the number of state variables, the total complexity (quadratic) still scales poorly with size of the problem (number of state variables / dimensions). Further, the current experiments and comparisons were performed on small benchmarks. However, since this paper is the first to point out that the cubic scaling can be improved by reflecting constraints due to degeneracy, I think the experimental component of the contribution is not the most important.

Limitations:
The authors do mention the primary limitations of this present work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a parameter efficient parameterization for neural networks simulating metripletic systems, which are differential equations that have both an energy dissipative piece and an energy conservative piece. The method works by learning several of the required quantities (L and M, which trade off dissipation and conservation, I believe), while also using a small neural network to estimate the dissipation and conservation pieces (E(x) and S(x) ). As not all quantities in the state, x, can be observed, they use a time based diffusion model to emulate the hidden states (e.g. entropy) to develop initial conditions for these. Experiments are performed on two systems of this class, where it seems like the method performs better (probably due to having better inductive biases).


Unfortunately, due to not having a strong physics background, I feel somewhat unqualified to judge many of the technical strengths although things seem reasonable from a skim. I don’t know if I can properly assess novelty and significance as a result.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Significance:
-	Building better emulators of physical systems that are complicated is a good first step in what the authors term “phenomenonological” understanding of these systems.

-	Even quicker training time (and demonstrated both practically and theoretically) is quite helpful. I remember one of the original issues with NODE was that it took a very long time to converge.

Clarity: 
-	Overall, the paper is pretty well written, even if quite dense, and okay to follow for a non expert physicist. I was able to follow at least the ML pieces and the experiments section quite well.

-	The relevant literature is reasonably well signposted; I learned a fair bit about the state of this field by checking the references.

Novelty:

-	The approach seems to have a clear inductive bias win over the prior works GFINN and GNODE due to better parameterization of the system.

Weaknesses:
Unfortunately, the writing ends up being quite dense and technical with minimal outside applications. 

Sure, emulating these physical systems in the experiments is quite nice, but what types of applications does this lead to? This is more of a writing based thing and the paper could be refactored around one of these applications if possible.

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
2vhkjOdlc8;"REVIEW 
Summary:
The paper ""Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection"" introduces a minimalist reconstruction-based framework for unsupervised anomaly detection (UAD) in multi-class settings. The framework focuses on four main components: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets show that Dinomaly achieves superior performance compared to state-of-the-art multi-class and even some class-separated UAD methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Using simple components like Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction to achieve superior performance is highly original. This is a significant departure from traditional methods that rely on complex designs and multiple modules. It challenges the conventional views: more complex architectures are necessary for better performance in anomaly detection tasks.
2. The methodology is well-detailed, and the experimental design is robust. The authors conduct extensive experiments on three well-known datasets (MVTec-AD, VisA, and Real-IAD), providing comprehensive performance metrics and comparisons with SOTA methods. The result is convincing, showing that Dinomaly not only outperforms existing MUAD methods but also surpasses some of the best class-separated UAD methods. 
3. The paper is generally clear, well-organized, and relatively reproducible.
4. The significance of this work is substantial, and makes a valuable contribution to anomaly detection, as it addresses a major challenge in UAD—achieving high performance in multi-class settings without resorting to complex, specialized architectures, and is potentially scalable.

Weaknesses:
1. The paper provides a detailed explanation of the proposed framework but lacks important justification and discussion, it is difficult for readers to realize the novelty and improvements brought by Dinomaly. The author may need to compare Dinomaly to specific previous methods, highlighting the differences and improvements. Discuss how the minimalist approach contrasts with more complex architectures and why this improvement is significant.
2. The motivations for choosing Noisy Bottleneck and Loose Reconstruction are not deeply explored. For instance, explain in more detail why Noisy Bottleneck helps prevent identity mapping.
3. The paper claims simplicity but there was no discussion of parameter number, computational complexity, or time complexity in the experiment. 
4. In Loose Constraint, the author claims that 1-group LC mixes low-level and high-level features which is harmful for anomaly localization. How to group the features into the low-semantic-level group and high-semantic-level group in 2-group LC?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the Multi-class Unsupervised Anomaly Detection task and proposes a minimalistic reconstruction-based anomaly detection framework — Dinomaly that consists of only vanilla Transformer blocks. In this framework, four key components (Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction) are introduced to alleviate the performance gap between multi-class and class-separated models. The paper conducts extensive experiments on three major datasets: MVTec-AD, VisA, and Real-IAD. Results show that Dinomaly outperforms current state-of-the-art methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
(1)The paper is well-written and has clear statements which make it easy to understand. 

(2)The design of Dinomaly is straightforward but innovative. The use of foundation transformers, noisy bottleneck, linear attention, and loose reconstruction is well-justified. 

(3)The paper generally outperformed existing SOTA methods and did enough experiments and comparisons.

Weaknesses:
(1)The method relies heavily on transformer architectures, which might limit its applicability to other types of models.

(2)Transformers can be resource-intensive, and the paper does not fully address the computational cost of training and inference.

(3)The method's generalization to other domains or types of anomaly detection is not fully explored.

Limitations:
Limitations are discussed in Supplementary Sec. A.4.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Dinomaly, a simple yet effective anomaly detection framework using pure Transformer architectures. It identifies four key components essential for multi-class anomaly detection: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets show that Dinomaly achieves superior performance, surpassing both state-of-the-art multi-class and class-separated anomaly detection methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors have conducted extensive experiments to validate the effectiveness of their method across multiple anomaly detection tasks.
2. The authors have proposed a simple yet effective framework that approaches and even surpasses the results of state-of-the-art methods in single-class anomaly detection tasks.

Weaknesses:
1. L53-55 'In addition, previous...': The authors should provide relevant evidence rather than subjective assumptions.
2. L77: Placing the Related Works section in the appendix is unconventional.
3. The first component proposed by the authors, Foundation Transformers, was already introduced in the ViTAD paper, which diminishes the overall contribution of the paper. 
4. The input resolution used in the authors' experiments is 448x448, while other comparison methods use 256x256 or 224x224. This is an extremely unfair comparison. Please include results with a 256 resolution in table for a fair comparison.
5. In the proposed Loose Loss, 90% of the feature points were selected. How was this 90% hyperparameter determined? Please provide ablation study results.
6. The authors have employed Linear Attention to reduce computational load while maintaining similar performance. It is recommended that the authors compare the parameter count and FLOPs of their method with those of the baseline methods to demonstrate its efficiency. Additionally, it is suggested to conduct ablation studies to verify the computational efficiency of Linear Attention. 
7. Other methods, such as RD4AD, SimpleNet, and UniAD, perform under the proposed settings. The authors can conduct a more equitable comparison.

Limitations:
It is recommended that the authors evaluate the performance of different methods under fair settings.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces Dinomaly, a minimalistic unsupervised anomaly detection (UAD) method designed to bridge the performance gap between multi-class UAD and class-separated UAD. Utilizing pure Transformer architectures with key components such as Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction, Dinomaly achieves superior performance on MVTec-AD, VisA, and Real-IAD benchmarks, surpassing state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Dinomaly effectively bridges the performance gap between multi-class and class-separated UAD, achieving superior results on popular benchmarks such as MVTec-AD, VisA, and Real-IAD. 
2.  It utilizes a simple, straightforward approach with pure Transformer architectures, avoiding complex modules or specialized tricks.
3. The detailed ablation study demonstrates the effectiveness of each component—Noisy Bottleneck, Linear Attention, Loose Constraint, and Loose Loss—in enhancing anomaly detection.

Weaknesses:
1. The method might be perceived as too application-oriented, lacking broader theoretical contributions.

Limitations:
See the weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Dinomaly simplifies the anomaly detection process by eliminating the need for complex designs, additional modules, or specialized techniques. It relies solely on basic Transformer components such as self attention mechanisms and multi-layer perceptrons (MLPs) to perform anomaly detection for multi class images.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper has a clear motivation and contribution. The paper effectively proposes the viewpoint of ""less is more"" in multi class unsupervised anomaly detection, emphasizing how the simplicity of model architecture can achieve or surpass the performance of more complex systems.

Weaknesses:
I hope to provide a specific explanation of the information provided by the decision-making process for identifying anomalies in the model.

Limitations:
The author candidly acknowledged the limitations of the work and provided the problems that need to be addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper ""Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection"" introduces a minimalist reconstruction-based framework for unsupervised anomaly detection (UAD) in multi-class settings. The framework focuses on four main components: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets show that Dinomaly achieves superior performance compared to state-of-the-art multi-class and even some class-separated UAD methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Using simple components like Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction to achieve superior performance is highly original. This is a significant departure from traditional methods that rely on complex designs and multiple modules. It challenges the conventional views: more complex architectures are necessary for better performance in anomaly detection tasks.
2. The methodology is well-detailed, and the experimental design is robust. The authors conduct extensive experiments on three well-known datasets (MVTec-AD, VisA, and Real-IAD), providing comprehensive performance metrics and comparisons with SOTA methods. The result is convincing, showing that Dinomaly not only outperforms existing MUAD methods but also surpasses some of the best class-separated UAD methods. 
3. The paper is generally clear, well-organized, and relatively reproducible.
4. The significance of this work is substantial, and makes a valuable contribution to anomaly detection, as it addresses a major challenge in UAD—achieving high performance in multi-class settings without resorting to complex, specialized architectures, and is potentially scalable.

Weaknesses:
1. The paper provides a detailed explanation of the proposed framework but lacks important justification and discussion, it is difficult for readers to realize the novelty and improvements brought by Dinomaly. The author may need to compare Dinomaly to specific previous methods, highlighting the differences and improvements. Discuss how the minimalist approach contrasts with more complex architectures and why this improvement is significant.
2. The motivations for choosing Noisy Bottleneck and Loose Reconstruction are not deeply explored. For instance, explain in more detail why Noisy Bottleneck helps prevent identity mapping.
3. The paper claims simplicity but there was no discussion of parameter number, computational complexity, or time complexity in the experiment. 
4. In Loose Constraint, the author claims that 1-group LC mixes low-level and high-level features which is harmful for anomaly localization. How to group the features into the low-semantic-level group and high-semantic-level group in 2-group LC?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the Multi-class Unsupervised Anomaly Detection task and proposes a minimalistic reconstruction-based anomaly detection framework — Dinomaly that consists of only vanilla Transformer blocks. In this framework, four key components (Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction) are introduced to alleviate the performance gap between multi-class and class-separated models. The paper conducts extensive experiments on three major datasets: MVTec-AD, VisA, and Real-IAD. Results show that Dinomaly outperforms current state-of-the-art methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
(1)The paper is well-written and has clear statements which make it easy to understand. 

(2)The design of Dinomaly is straightforward but innovative. The use of foundation transformers, noisy bottleneck, linear attention, and loose reconstruction is well-justified. 

(3)The paper generally outperformed existing SOTA methods and did enough experiments and comparisons.

Weaknesses:
(1)The method relies heavily on transformer architectures, which might limit its applicability to other types of models.

(2)Transformers can be resource-intensive, and the paper does not fully address the computational cost of training and inference.

(3)The method's generalization to other domains or types of anomaly detection is not fully explored.

Limitations:
Limitations are discussed in Supplementary Sec. A.4.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Dinomaly, a simple yet effective anomaly detection framework using pure Transformer architectures. It identifies four key components essential for multi-class anomaly detection: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets show that Dinomaly achieves superior performance, surpassing both state-of-the-art multi-class and class-separated anomaly detection methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors have conducted extensive experiments to validate the effectiveness of their method across multiple anomaly detection tasks.
2. The authors have proposed a simple yet effective framework that approaches and even surpasses the results of state-of-the-art methods in single-class anomaly detection tasks.

Weaknesses:
1. L53-55 'In addition, previous...': The authors should provide relevant evidence rather than subjective assumptions.
2. L77: Placing the Related Works section in the appendix is unconventional.
3. The first component proposed by the authors, Foundation Transformers, was already introduced in the ViTAD paper, which diminishes the overall contribution of the paper. 
4. The input resolution used in the authors' experiments is 448x448, while other comparison methods use 256x256 or 224x224. This is an extremely unfair comparison. Please include results with a 256 resolution in table for a fair comparison.
5. In the proposed Loose Loss, 90% of the feature points were selected. How was this 90% hyperparameter determined? Please provide ablation study results.
6. The authors have employed Linear Attention to reduce computational load while maintaining similar performance. It is recommended that the authors compare the parameter count and FLOPs of their method with those of the baseline methods to demonstrate its efficiency. Additionally, it is suggested to conduct ablation studies to verify the computational efficiency of Linear Attention. 
7. Other methods, such as RD4AD, SimpleNet, and UniAD, perform under the proposed settings. The authors can conduct a more equitable comparison.

Limitations:
It is recommended that the authors evaluate the performance of different methods under fair settings.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces Dinomaly, a minimalistic unsupervised anomaly detection (UAD) method designed to bridge the performance gap between multi-class UAD and class-separated UAD. Utilizing pure Transformer architectures with key components such as Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction, Dinomaly achieves superior performance on MVTec-AD, VisA, and Real-IAD benchmarks, surpassing state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Dinomaly effectively bridges the performance gap between multi-class and class-separated UAD, achieving superior results on popular benchmarks such as MVTec-AD, VisA, and Real-IAD. 
2.  It utilizes a simple, straightforward approach with pure Transformer architectures, avoiding complex modules or specialized tricks.
3. The detailed ablation study demonstrates the effectiveness of each component—Noisy Bottleneck, Linear Attention, Loose Constraint, and Loose Loss—in enhancing anomaly detection.

Weaknesses:
1. The method might be perceived as too application-oriented, lacking broader theoretical contributions.

Limitations:
See the weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Dinomaly simplifies the anomaly detection process by eliminating the need for complex designs, additional modules, or specialized techniques. It relies solely on basic Transformer components such as self attention mechanisms and multi-layer perceptrons (MLPs) to perform anomaly detection for multi class images.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper has a clear motivation and contribution. The paper effectively proposes the viewpoint of ""less is more"" in multi class unsupervised anomaly detection, emphasizing how the simplicity of model architecture can achieve or surpass the performance of more complex systems.

Weaknesses:
I hope to provide a specific explanation of the information provided by the decision-making process for identifying anomalies in the model.

Limitations:
The author candidly acknowledged the limitations of the work and provided the problems that need to be addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
xymhWyiZOp;"REVIEW 
Summary:
This paper identifies a major problem with anchored training, that the performance of anchored training does not increase with increasing reference set size, and proposes a simple regularization approach to overcome this problem. This approach is evaluated on OOD generalization, calibration and anomaly rejection, and task adaptation, and various facets of anchored training are analyzed.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper makes the interesting finding that the performance of anchored training does not increase with increasing reference set size, and that this problem is not alleviated by more sophisticated inference strategies. The paper also proposes a simple reference-masking regularization technique to help alleviate this problem. The experiments show the effectiveness of the proposed approach, and there is also analysis of how the method interacts with data augmentation and noisy labels. An ablation study of the $\alpha$ parameter is also performed. Training recipes are also provided, making the paper easier to reproduce.

Weaknesses:
One weakness is that the reference set selection strategy and reference set sizes are not explained for the experiments. 

The impact/novelty is a bit limited because of the lack of comparisons to non-anchored training works.

Minor points: in the tables, decreases in performance could be colored in a color other than pink. Figure 1 could be improved with error bars. One highlighting was missed in Table 3. The abbreviation LP is not defined.

Limitations:
The limitations of this work are discussed by the authors at the end of the paper. Negative societal impact is probably not a concern for this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors propose a new strategy to train anchoring-based models, significantly improving performance, training efficiency, and model generalization compared to previous approaches. The key to the method is the added masking strategy that allows the model to better profit from anchoring-based training. The authors demonstrate that modifications only in inference (using several samples or searching for the best references) or the number of used references do not improve model performance, while the application of the masking procedure significantly improves it, as shown on various image classification datasets, specifically CIFAR-10, CIFAR-100, and ImageNet, using different architectures (both CNN and attention-based). The experiments demonstrate the effectiveness of the proposed method and the significant benefit of using it for improved generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is clearly written and easy to follow. The idea is intuitive and easy to grasp. The related work section provides an adequate discussion of existing approaches to anchoring-based training. The analysis narrative, with the presented drawbacks of existing methods, is very clear and easy to understand.

* The idea of masking the reference input argument is very clear and logical. The intuition behind why the problem could occur: 1) argument size grows combinatorially, and therefore 2) the model could learn to ignore the reference argument; seems correct, which is further clearly supported by the experiments.

* The authors provided an extensive evaluation of their approach, spanning different datasets and architectures, which provides a solid grounding to support the proposed method.

Weaknesses:
* It seems that the evaluation could benefit from an additional comparison with other existing state-of-the-art OOD/uncertainty methods to better represent the quality of the results (not just in comparison with former anchoring-based approaches, but overall).

* From the perspective of the experimental evaluation, I would be curious to see evidence that the behavior demonstrated in the paper would hold in other domains, such as texts, graphs, more complicated vision tasks (e.g. segmentation), not limiting to image classification task.

Limitations:
No

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a thorough discussion on the use of anchoring for training vision models. In particular, the paper tackles 1) the problem of reference diversity when training with anchoring to explain how superior generalization can be achieved 2) addresses the problem of spurious correlations learnt between the residual and 3) how different inference-time strategies can enable greater out-of-support generalization. Overall, this comprehensive study of anchoring provides useful guidelines for how anchoring should be applied to extract maximum performance. The paper empirically confirms this via the proposed anchoring scheme outperforming prior work noticeably.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1) Clarity: The paper is very clearly written and easy to follow. Readers unfamiliar with the literature like myself are able to understand what anchoring is, how it can be useful for (out of support) generalization and how current methods fail to apply anchoring in the most effective way. 

2) Thoroughness of Evaluation: The paper conducts thorough ablations on several components of the anchoring pipeline. Reference diversity, reference masking, inference procedure etc. More

Weaknesses:
No obvious weaknesses.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors analyze the effect of anchored training through a series of small experiments and find that, contrary to claims in prior works, increasing the size of the reference set is not beneficial and that this shortcoming cannot be mitigated through existing inference strategies. The authors provide a simple yet efficient fix by randomly masking out the reference during training, and forcing the model to make high entropy predictions in those cases. This solution does not incur any training overhead, and the authors demonstrate in extensive experiments that the fix is applicable to different models and datasets, yields improvements for OOD performance over various distribution shifts, and improves calibration and anomaly resilience.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper is very well written and structured and is overall easy to follow. The initial experiments highlight the studied problem well.
2. The authors showcase an important limitation to existing anchoring techniques that was unknown to the community.
3. The proposed solution is simple and is demonstrated to consistently improve performance across models and datasets.
4. The experiment section is extensive and covers both OOD performance as well as safety-relevant metrics. The results convincingly demonstrate the effectiveness of the proposed method.

Weaknesses:
The paper is very well written, I don't see any major weaknesses that would prevent an accept.

Minor weakness: The optimal $\alpha$ is determined when using the entire dataset as a reference set. However, as is clear from the motivation, risk of spurious shortcuts is larger with a smaller reference set. Wouldn't this imply that the optimal $\alpha$ would be larger for smaller reference sets? How should this value be chosen in practice and for datasets larger than ImageNet-1k?

Limitations:
Limitations were sufficiently addressed, especially the empirical nature of the work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper identifies a major problem with anchored training, that the performance of anchored training does not increase with increasing reference set size, and proposes a simple regularization approach to overcome this problem. This approach is evaluated on OOD generalization, calibration and anomaly rejection, and task adaptation, and various facets of anchored training are analyzed.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper makes the interesting finding that the performance of anchored training does not increase with increasing reference set size, and that this problem is not alleviated by more sophisticated inference strategies. The paper also proposes a simple reference-masking regularization technique to help alleviate this problem. The experiments show the effectiveness of the proposed approach, and there is also analysis of how the method interacts with data augmentation and noisy labels. An ablation study of the $\alpha$ parameter is also performed. Training recipes are also provided, making the paper easier to reproduce.

Weaknesses:
One weakness is that the reference set selection strategy and reference set sizes are not explained for the experiments. 

The impact/novelty is a bit limited because of the lack of comparisons to non-anchored training works.

Minor points: in the tables, decreases in performance could be colored in a color other than pink. Figure 1 could be improved with error bars. One highlighting was missed in Table 3. The abbreviation LP is not defined.

Limitations:
The limitations of this work are discussed by the authors at the end of the paper. Negative societal impact is probably not a concern for this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors propose a new strategy to train anchoring-based models, significantly improving performance, training efficiency, and model generalization compared to previous approaches. The key to the method is the added masking strategy that allows the model to better profit from anchoring-based training. The authors demonstrate that modifications only in inference (using several samples or searching for the best references) or the number of used references do not improve model performance, while the application of the masking procedure significantly improves it, as shown on various image classification datasets, specifically CIFAR-10, CIFAR-100, and ImageNet, using different architectures (both CNN and attention-based). The experiments demonstrate the effectiveness of the proposed method and the significant benefit of using it for improved generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is clearly written and easy to follow. The idea is intuitive and easy to grasp. The related work section provides an adequate discussion of existing approaches to anchoring-based training. The analysis narrative, with the presented drawbacks of existing methods, is very clear and easy to understand.

* The idea of masking the reference input argument is very clear and logical. The intuition behind why the problem could occur: 1) argument size grows combinatorially, and therefore 2) the model could learn to ignore the reference argument; seems correct, which is further clearly supported by the experiments.

* The authors provided an extensive evaluation of their approach, spanning different datasets and architectures, which provides a solid grounding to support the proposed method.

Weaknesses:
* It seems that the evaluation could benefit from an additional comparison with other existing state-of-the-art OOD/uncertainty methods to better represent the quality of the results (not just in comparison with former anchoring-based approaches, but overall).

* From the perspective of the experimental evaluation, I would be curious to see evidence that the behavior demonstrated in the paper would hold in other domains, such as texts, graphs, more complicated vision tasks (e.g. segmentation), not limiting to image classification task.

Limitations:
No

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a thorough discussion on the use of anchoring for training vision models. In particular, the paper tackles 1) the problem of reference diversity when training with anchoring to explain how superior generalization can be achieved 2) addresses the problem of spurious correlations learnt between the residual and 3) how different inference-time strategies can enable greater out-of-support generalization. Overall, this comprehensive study of anchoring provides useful guidelines for how anchoring should be applied to extract maximum performance. The paper empirically confirms this via the proposed anchoring scheme outperforming prior work noticeably.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1) Clarity: The paper is very clearly written and easy to follow. Readers unfamiliar with the literature like myself are able to understand what anchoring is, how it can be useful for (out of support) generalization and how current methods fail to apply anchoring in the most effective way. 

2) Thoroughness of Evaluation: The paper conducts thorough ablations on several components of the anchoring pipeline. Reference diversity, reference masking, inference procedure etc. More

Weaknesses:
No obvious weaknesses.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors analyze the effect of anchored training through a series of small experiments and find that, contrary to claims in prior works, increasing the size of the reference set is not beneficial and that this shortcoming cannot be mitigated through existing inference strategies. The authors provide a simple yet efficient fix by randomly masking out the reference during training, and forcing the model to make high entropy predictions in those cases. This solution does not incur any training overhead, and the authors demonstrate in extensive experiments that the fix is applicable to different models and datasets, yields improvements for OOD performance over various distribution shifts, and improves calibration and anomaly resilience.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper is very well written and structured and is overall easy to follow. The initial experiments highlight the studied problem well.
2. The authors showcase an important limitation to existing anchoring techniques that was unknown to the community.
3. The proposed solution is simple and is demonstrated to consistently improve performance across models and datasets.
4. The experiment section is extensive and covers both OOD performance as well as safety-relevant metrics. The results convincingly demonstrate the effectiveness of the proposed method.

Weaknesses:
The paper is very well written, I don't see any major weaknesses that would prevent an accept.

Minor weakness: The optimal $\alpha$ is determined when using the entire dataset as a reference set. However, as is clear from the motivation, risk of spurious shortcuts is larger with a smaller reference set. Wouldn't this imply that the optimal $\alpha$ would be larger for smaller reference sets? How should this value be chosen in practice and for datasets larger than ImageNet-1k?

Limitations:
Limitations were sufficiently addressed, especially the empirical nature of the work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xxY8d4rnSb;"REVIEW 
Summary:
This paper presents a method to estimate 3D human keypoints from a sequence of monocular 2D keypoints observations. It builds upon an existing sequence-to-sequence architecture (MixSTE), with a different output parameterization exploiting a kinematic skeletton prior, and different training losses. Lengths of the skeletton bones are predicted for the whole sequence to ensure consistency across frames (and maybe also left/right symmetry of the skeletton), and five 3D pose hypotheses with associated scores are predicted for each frame, parameterized as a list of 3D relative orientation for each bone with respect to its parent in the kinematic tree.

The authors develop theoretical arguments regarding the benefits of enforcing such structural priors in the predictions, and illustrate with a toy example the interest of having multiple predictions in case of ambiguous multimodal output. They validate their approach on Human3.6M and MPI-INF-3DHP datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The motivation for exploiting bone lengths constraints is well expressed, with a clear and detailed discussion provided in Section 4. The discussion of experimental and ablation results is insightful and shows – in a setting dependent on an oracle – benefits of the proposed approach.

Weaknesses:
The idea of enforcing body priors (constant bone length here) is not novel and has actually been heavily exploited in a whole line of work relying on more advanced parametric models such as SMPL [100]. This line of work would deserve being considered in the paper, as it encompass approaches suitable for 2D-to-3D sequence lifting such as e.g. [101].

The authors present a pose space consisting in 3D coordinates of joints linked by some rigid segments. Based on this definition, a natural pose parameterization would consist in the 3D direction of each segment, yet the authors chose to overparameterize poses by using relative 3D bone orientation instead. I understand that such choice can have practical benefits in term of biomechanical constraints and additional supervision signal when ground truth data is available, but such choice should be properly motivated, discussed and ablated in the paper.


The authors describe two ways of aggregating results L247 but do not state which one they use for MPI-INF-3DHP, and they only report oracle results on Human3.6M and for the ablations.

In my understanding, pose hypotheses are selected independently for each frame and there are no temporal terms in the training objectives or aggregation method. Since the proposed approach deals with temporal sequences, it would be worth evaluating the temporal consistency of the predictions, through qualitative video examples and quantitatively e.g. using joint acceleration metrics. Having multiple hypotheses for each frame brings combinatorial questions worth discussing in my opinion.

References:
- [100] Loper at al., “SMPL: A Skinned Multi-Person Linear Model”, at SIGGRAPH Asia 2015.
- [101] Baradel et al., “PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling”, in TPAMI 2022.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a MCL-based framework for multi-hypothesis 3D human pose estimation. This framework predicts skeletal parameters so that the predicted 3D poses in a sequence are constrained to one smooth manifold. To prove the superiority of such a framework, the paper presents detailed theoretical analysis on the drawback of unconstrained single-hypothesis HPE and why MPJPE alone is not enough for pose evaluation. The experiments show the proposed framework is capable of keeping the consistency of predicted poses and achieving state-of-the-art MPJPE in the meantime.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* Simple and reasonable manifold representation. The proposed framework keeps the predicted human pose on the target manifold by representing the human pose with bone lengths and orientations, and the 3D pose is a direct inference from forward kinematics. The manifold is represented by the kinematics itself.
  
* Inspiring theoretical analysis on basic problems in 3D HPE. The paper arrives at some theoretical conclusions (line178-183), along with detailed proofs. They can provide some refreshing ideas on the innate drawbacks of traditional loss functions and MPJPE metrics.
  
* Good performance under both MPJPE and consistency measures, as validated in Table 2 and 3.

Weaknesses:
* Theoretical analysis on the advantage of multi-hypothesis methods over single-hypothesis ones could be added. Specifically, why a **constrained multi-hypothesis** method performs better than an **unconstrained single-hypothesis** method in MPJPE? Though this is already validated by the experiments, I personally believe it would make the paper more solid if the authors could make this analysis.

Minor problem:
* In Fig.4 (C) and (D), it is not quite clear how the estimations (crosses and triangles) correspond with the inputs (black dots). There might be some unexpected shifts, as the projections of the predicitons do not strictly align with the inputs (like in B).

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a new method to estimate 3D human pose from 2D observations (lifting). To ensure the body symmetry and temporal consistency, the authors disentangle human skeleton to two parts: temporally consistency bone scales and temporally variable bone rotations. The authors use fancy formulas to prove that, minimizing MSE loss could not gurantee manifold consistency. The quantitative and qualitative results on Human3.6m and MPI-INF-3DHP datasets show the superiority of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The evalution results in this paper is quite impressive, especially the newly proposed consistency metric. Figure 1 clearly shows the superiority of the proposed method. 

2. The authors try to prove the theoretical optimal of the proposed method, which is worth encouraging.

Weaknesses:
I am not an expert in manifold theory, therefore my questions only relate to human pose estimation. 

1. How to constrain the rotation space during training? 

2. The pose lifting method is quite similar to Anatomy3D (bone length + rotations). Can I view this paper an multi-hypothesis extension of Anatomy3D? Why? 

3. Previous paper ""POSE-NDF: MODELING HUMAN POSE MANIFOLDS WITH NEURAL DISTANCE FIELDS"" is similar to this paper in concepts. SMPL naturally guarantees bone length symmetry, and the learnable parameters (rotations and shape parameters) are similar to this paper in its functionality. It would be better to cite it. 

4. Suppose that, there is a virtual dataset, all 2D human joints are rendered (projected) from strictly symmetric 3D joints, then, could learning the lifting function on this virtual dataset using MSE loss guarantee the results all lie on manifold? 

5. (An optional question) The ground truth 3D joints of Human3.6M datasets come from the marker tracking on body surface, which naturally could not guarantee skeleton length consistency. Why learning symmetric bones yields better results (both Anatomy3D and the proposed methods)?

Limitations:
The authors addressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper propose ManiPose, a manifold-constrained multi-hypothesis model for 3D human pose lifting. The authors provide empirical and experimental evidence to show that joint position regression leads to inconsistent skeleton lengths. And they propose to predict globally consistent pose scale and individual joint rotations per frame (rather than joint positions) to constrain the predictions to the pose manifold. Empirical results demonstrates that the proposed ManiPose framework improves the pose consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper provides valuable theoretical analysis to support their arguments and provides intuitive toy examples to illustrate the ambiguity in pose lifting.
* The paper conducts extensive experiments on H36M and MPI-INF-3DHP datasets.

Weaknesses:
* The paper uses a multi-head design to predict multiple hypotheses. This design loses the flexibility of sampling different numbers of hypotheses and limits the maximum number of hypotheses to a small number. This often results in limited hypothesis diversity. In the experimental section, the authors do not provide numerical of visual measurements of hypothesis diversity.
* According to the comparison in Table 4, the manifold constraint proposed in this paper sacrifices MPJPE to improve pose consistency, serving as a trade-off approach between accuracy and consistency. Although the consistency is improved, it lags behind the traditional position regression or manifold regularization in accuracy, and does not bring essential improvement (improve both in accuracy and consistency) compared with these two methods.
* Missing comparison with two recent multi-hypothesis methods. [1] GFPose: Learning 3D Human Pose Prior with Gradient Fields. [2] DiffPose: Toward More Reliable 3D Pose Estimation.

Limitations:
As the authors discussed in the Limitations Section, they used forward kinematics to obtain joint positions, which can lead to error accumulation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method to estimate 3D human keypoints from a sequence of monocular 2D keypoints observations. It builds upon an existing sequence-to-sequence architecture (MixSTE), with a different output parameterization exploiting a kinematic skeletton prior, and different training losses. Lengths of the skeletton bones are predicted for the whole sequence to ensure consistency across frames (and maybe also left/right symmetry of the skeletton), and five 3D pose hypotheses with associated scores are predicted for each frame, parameterized as a list of 3D relative orientation for each bone with respect to its parent in the kinematic tree.

The authors develop theoretical arguments regarding the benefits of enforcing such structural priors in the predictions, and illustrate with a toy example the interest of having multiple predictions in case of ambiguous multimodal output. They validate their approach on Human3.6M and MPI-INF-3DHP datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The motivation for exploiting bone lengths constraints is well expressed, with a clear and detailed discussion provided in Section 4. The discussion of experimental and ablation results is insightful and shows – in a setting dependent on an oracle – benefits of the proposed approach.

Weaknesses:
The idea of enforcing body priors (constant bone length here) is not novel and has actually been heavily exploited in a whole line of work relying on more advanced parametric models such as SMPL [100]. This line of work would deserve being considered in the paper, as it encompass approaches suitable for 2D-to-3D sequence lifting such as e.g. [101].

The authors present a pose space consisting in 3D coordinates of joints linked by some rigid segments. Based on this definition, a natural pose parameterization would consist in the 3D direction of each segment, yet the authors chose to overparameterize poses by using relative 3D bone orientation instead. I understand that such choice can have practical benefits in term of biomechanical constraints and additional supervision signal when ground truth data is available, but such choice should be properly motivated, discussed and ablated in the paper.


The authors describe two ways of aggregating results L247 but do not state which one they use for MPI-INF-3DHP, and they only report oracle results on Human3.6M and for the ablations.

In my understanding, pose hypotheses are selected independently for each frame and there are no temporal terms in the training objectives or aggregation method. Since the proposed approach deals with temporal sequences, it would be worth evaluating the temporal consistency of the predictions, through qualitative video examples and quantitatively e.g. using joint acceleration metrics. Having multiple hypotheses for each frame brings combinatorial questions worth discussing in my opinion.

References:
- [100] Loper at al., “SMPL: A Skinned Multi-Person Linear Model”, at SIGGRAPH Asia 2015.
- [101] Baradel et al., “PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling”, in TPAMI 2022.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a MCL-based framework for multi-hypothesis 3D human pose estimation. This framework predicts skeletal parameters so that the predicted 3D poses in a sequence are constrained to one smooth manifold. To prove the superiority of such a framework, the paper presents detailed theoretical analysis on the drawback of unconstrained single-hypothesis HPE and why MPJPE alone is not enough for pose evaluation. The experiments show the proposed framework is capable of keeping the consistency of predicted poses and achieving state-of-the-art MPJPE in the meantime.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* Simple and reasonable manifold representation. The proposed framework keeps the predicted human pose on the target manifold by representing the human pose with bone lengths and orientations, and the 3D pose is a direct inference from forward kinematics. The manifold is represented by the kinematics itself.
  
* Inspiring theoretical analysis on basic problems in 3D HPE. The paper arrives at some theoretical conclusions (line178-183), along with detailed proofs. They can provide some refreshing ideas on the innate drawbacks of traditional loss functions and MPJPE metrics.
  
* Good performance under both MPJPE and consistency measures, as validated in Table 2 and 3.

Weaknesses:
* Theoretical analysis on the advantage of multi-hypothesis methods over single-hypothesis ones could be added. Specifically, why a **constrained multi-hypothesis** method performs better than an **unconstrained single-hypothesis** method in MPJPE? Though this is already validated by the experiments, I personally believe it would make the paper more solid if the authors could make this analysis.

Minor problem:
* In Fig.4 (C) and (D), it is not quite clear how the estimations (crosses and triangles) correspond with the inputs (black dots). There might be some unexpected shifts, as the projections of the predicitons do not strictly align with the inputs (like in B).

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a new method to estimate 3D human pose from 2D observations (lifting). To ensure the body symmetry and temporal consistency, the authors disentangle human skeleton to two parts: temporally consistency bone scales and temporally variable bone rotations. The authors use fancy formulas to prove that, minimizing MSE loss could not gurantee manifold consistency. The quantitative and qualitative results on Human3.6m and MPI-INF-3DHP datasets show the superiority of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The evalution results in this paper is quite impressive, especially the newly proposed consistency metric. Figure 1 clearly shows the superiority of the proposed method. 

2. The authors try to prove the theoretical optimal of the proposed method, which is worth encouraging.

Weaknesses:
I am not an expert in manifold theory, therefore my questions only relate to human pose estimation. 

1. How to constrain the rotation space during training? 

2. The pose lifting method is quite similar to Anatomy3D (bone length + rotations). Can I view this paper an multi-hypothesis extension of Anatomy3D? Why? 

3. Previous paper ""POSE-NDF: MODELING HUMAN POSE MANIFOLDS WITH NEURAL DISTANCE FIELDS"" is similar to this paper in concepts. SMPL naturally guarantees bone length symmetry, and the learnable parameters (rotations and shape parameters) are similar to this paper in its functionality. It would be better to cite it. 

4. Suppose that, there is a virtual dataset, all 2D human joints are rendered (projected) from strictly symmetric 3D joints, then, could learning the lifting function on this virtual dataset using MSE loss guarantee the results all lie on manifold? 

5. (An optional question) The ground truth 3D joints of Human3.6M datasets come from the marker tracking on body surface, which naturally could not guarantee skeleton length consistency. Why learning symmetric bones yields better results (both Anatomy3D and the proposed methods)?

Limitations:
The authors addressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper propose ManiPose, a manifold-constrained multi-hypothesis model for 3D human pose lifting. The authors provide empirical and experimental evidence to show that joint position regression leads to inconsistent skeleton lengths. And they propose to predict globally consistent pose scale and individual joint rotations per frame (rather than joint positions) to constrain the predictions to the pose manifold. Empirical results demonstrates that the proposed ManiPose framework improves the pose consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper provides valuable theoretical analysis to support their arguments and provides intuitive toy examples to illustrate the ambiguity in pose lifting.
* The paper conducts extensive experiments on H36M and MPI-INF-3DHP datasets.

Weaknesses:
* The paper uses a multi-head design to predict multiple hypotheses. This design loses the flexibility of sampling different numbers of hypotheses and limits the maximum number of hypotheses to a small number. This often results in limited hypothesis diversity. In the experimental section, the authors do not provide numerical of visual measurements of hypothesis diversity.
* According to the comparison in Table 4, the manifold constraint proposed in this paper sacrifices MPJPE to improve pose consistency, serving as a trade-off approach between accuracy and consistency. Although the consistency is improved, it lags behind the traditional position regression or manifold regularization in accuracy, and does not bring essential improvement (improve both in accuracy and consistency) compared with these two methods.
* Missing comparison with two recent multi-hypothesis methods. [1] GFPose: Learning 3D Human Pose Prior with Gradient Fields. [2] DiffPose: Toward More Reliable 3D Pose Estimation.

Limitations:
As the authors discussed in the Limitations Section, they used forward kinematics to obtain joint positions, which can lead to error accumulation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xvYI7TCiU6;"REVIEW 
Summary:
The authors study MARL in heterogeneous settings, where agents are not allowed to share their parameters, and make use of the sequential updating scheme under the CTDE schema. They propose a method which exploits the preceding information to improve exploration and heterogeneity sequentially. This method is equipped with a mutual policy divergence maximization framework, which utilizes the discrepancies between episodes to enhance exploration and between agents to heterogenize agents. Interestingly, the authors propose the conditional Cauchy-Schwarz divergence to provide entropy-guided exploration incentives.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The problem of exploration in settings with heterogeneous agents is important in MARL and not well-explored in literature. 

- The paper is the first to study the effectiveness of policy divergence maximization in the sequential updating schema, upon which important related work has been built.

- The paper proposed the conditional Cauchy-Schwarz (CS) divergence as an alternative to the popular KL-divergence in MARL. Such an alternation may be interesting to the broader RL community. Interestingly, unlike KL-divergence which can expload for small values of the denominator, CS divergence has a provable good lower bound ($-\log(n)$) only dependent on the number of finite actions.

- The proposed method displays good performance, in comparison to strong SOTA methods (including MAPPO, HAPPO), on benchmarks with heterogeneous agents.

- The proposed framework is simple and easy-to-implement.

- The paper is generally well-written and easy-to-follow.

Weaknesses:
- The improvement over the baselines (standard KL-divergence, entropy term, no incentive) does not seem to be quite consistent in the ablation study, due to (a) high variance in the results of the no incentive, and (b) very close improvement over the KL divergence baseline in terms of best episodic reward in 2 out 3 tasks.

- Since the CS divergence is new in MARL and RL, a table containing the running times of the evaluated algorithms is missing. How costly is the CS divergence?

Limitations:
The authors provide limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel training objective where it encourages the policies to diverge from each other and from the previous policy under heterogeneous multi-agent tasks based on sequential recently proposed sequential policy update. It utilizes CS divergence for calculation of ""distance"" between policies for tractable and stable optimization compared to KL divergence. The evaluation is done in high-dimensional multi-agent mujoco and and bi-dexhands environments, outperforming existing state-of-the-art sequential algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written and easy to understand; Fig. 1 is very informative.
- The problem of exploration under agent heterogeneity is an important problem in multi-agent learning
- The proposed method is sound and is backed by theory

Weaknesses:
- The evaluation is hard to judge whether the proposed method is actually performs better than the baselines, this is a deal breaker. I suggest the authors also incorporate aggregate quantities from https://agarwl.github.io/rliable/

I'm willing to increase the score if the authors show that the improvement is statistically significant

Limitations:
Minor comments
- line 36, wrong citation format
- line 193 and line 217, Ep. 5 should be Eq. 4

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper is situated in the problem setting of heterogeneous cooperative agents, under the sequential update framework. The paper introduces the novel MADPO algorithm, in which agents maximize the Cauchy Schwarz divergence between agents and between episodes of data gathered by the same agent, to improve exploration. Empirical validation is performed on the Multi-Agent Mujoco and Bi-DexHands benchmark suites, demonstrating that the MADPO outperforms baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the paper is clear, succinct, and the main idea is clear and easy to understand. The format, and figures are good, with all expected components included. The idea of maximizing the inter/intra agent divergences is intuitively appealing. Further, the authors address the pitfalls of naively maximizing intra-agent divergences by adopting the Cauchy Schwarz divergence. It's especially nice that maximizing the CS divergence implies maximizing the policy entropy as well. Experiments are done on a large number of tasks, with comparisons against expected baselines and parameter sensitivity analyses all present.

Weaknesses:
1. The motivation of the paper is not altogether clear to me. The paper seems to suggest that exploration is more challenging in the sequential update setting, necessitating devoted algorithms. Why would this be the case? 
2. In many of the presented domains, the improvement of MADPO over the next best method is not very large. Sometimes, confidence intervals of MADPO overlap those of the next best method. Can the authors provide statistical significance tests for the main results in Figures 2 and 3, comparing MADPO to the next best method?
3. Some minor suggestions:
- Please check your paper carefully for typos, as there are quite a few: 
    - Line 89: ""connecting link dimension curse""? Not sure what this is
    - No period after Figure 4
    - Trust interval -> confidence interval 
    - Lacking 'and' at line 174
    - Line 204: conditoned -> conditioned
    - Line 216: extra ""of"" 
- Please be sure to state the number of trials in the main text. It is mentioned in the Neurips checklist, but I could not find it  in  the main text
- Please make the colors of the methods the same for both domains (i.e. pick 1 color for MADPO and be consistent with it)

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel multi-agent reinforcement learning (MARL) method called Multi-Agent Divergence Policy Optimization (MADPO), which enhances exploration and heterogeneity through a mutual policy divergence maximization framework. MADPO leverages a sequential updating scheme and quantifies discrepancies between episodes and agents, termed intra-agent divergence and inter-agent divergence, respectively. To address the instability and lack of directionality in traditional divergence measurements, the paper proposes using conditional Cauchy-Schwarz divergence to provide entropy-guided exploration incentives. Experiments demonstrate that the proposed method outperforms state-of-the-art sequential updating approaches in two challenging multi-agent tasks with various heterogeneous scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. **Innovation**: The paper introduces MADPO, a novel MARL method that enhances agent exploration and heterogeneity through mutual policy divergence maximization. 
   
2. **Theoretical Foundation**: The use of conditional Cauchy-Schwarz divergence to address instability and directionality in traditional divergence measurements is a contribution.
   
3. **Experimental Validation**: The experiments conducted on two challenging multi-agent tasks with different heterogeneous scenarios convincingly demonstrate the effectiveness and superiority of MADPO in enhancing exploration and heterogeneity.

Weaknesses:
1.  The paper lacks analysis and comparison with relevant literature on sequential decision-making, such as:
   - Liu J, Zhong Y, Hu S, et al. Maximum Entropy Heterogeneous-Agent Reinforcement Learning[C]//The Twelfth International Conference on Learning Representations.  (This paper extends SAC to heterogeneous sequential decision-making scenarios, and the relationship between this work and the current paper remains unclear.)

2. It is unclear whether the intrinsic reward method proposed in this paper can ensure that the resulting trained policies are consistent with the original policies.

Limitations:
The authors have addressed the limitations of their work and discussed potential negative societal impacts in accordance with the guidelines.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study MARL in heterogeneous settings, where agents are not allowed to share their parameters, and make use of the sequential updating scheme under the CTDE schema. They propose a method which exploits the preceding information to improve exploration and heterogeneity sequentially. This method is equipped with a mutual policy divergence maximization framework, which utilizes the discrepancies between episodes to enhance exploration and between agents to heterogenize agents. Interestingly, the authors propose the conditional Cauchy-Schwarz divergence to provide entropy-guided exploration incentives.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The problem of exploration in settings with heterogeneous agents is important in MARL and not well-explored in literature. 

- The paper is the first to study the effectiveness of policy divergence maximization in the sequential updating schema, upon which important related work has been built.

- The paper proposed the conditional Cauchy-Schwarz (CS) divergence as an alternative to the popular KL-divergence in MARL. Such an alternation may be interesting to the broader RL community. Interestingly, unlike KL-divergence which can expload for small values of the denominator, CS divergence has a provable good lower bound ($-\log(n)$) only dependent on the number of finite actions.

- The proposed method displays good performance, in comparison to strong SOTA methods (including MAPPO, HAPPO), on benchmarks with heterogeneous agents.

- The proposed framework is simple and easy-to-implement.

- The paper is generally well-written and easy-to-follow.

Weaknesses:
- The improvement over the baselines (standard KL-divergence, entropy term, no incentive) does not seem to be quite consistent in the ablation study, due to (a) high variance in the results of the no incentive, and (b) very close improvement over the KL divergence baseline in terms of best episodic reward in 2 out 3 tasks.

- Since the CS divergence is new in MARL and RL, a table containing the running times of the evaluated algorithms is missing. How costly is the CS divergence?

Limitations:
The authors provide limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel training objective where it encourages the policies to diverge from each other and from the previous policy under heterogeneous multi-agent tasks based on sequential recently proposed sequential policy update. It utilizes CS divergence for calculation of ""distance"" between policies for tractable and stable optimization compared to KL divergence. The evaluation is done in high-dimensional multi-agent mujoco and and bi-dexhands environments, outperforming existing state-of-the-art sequential algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written and easy to understand; Fig. 1 is very informative.
- The problem of exploration under agent heterogeneity is an important problem in multi-agent learning
- The proposed method is sound and is backed by theory

Weaknesses:
- The evaluation is hard to judge whether the proposed method is actually performs better than the baselines, this is a deal breaker. I suggest the authors also incorporate aggregate quantities from https://agarwl.github.io/rliable/

I'm willing to increase the score if the authors show that the improvement is statistically significant

Limitations:
Minor comments
- line 36, wrong citation format
- line 193 and line 217, Ep. 5 should be Eq. 4

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper is situated in the problem setting of heterogeneous cooperative agents, under the sequential update framework. The paper introduces the novel MADPO algorithm, in which agents maximize the Cauchy Schwarz divergence between agents and between episodes of data gathered by the same agent, to improve exploration. Empirical validation is performed on the Multi-Agent Mujoco and Bi-DexHands benchmark suites, demonstrating that the MADPO outperforms baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the paper is clear, succinct, and the main idea is clear and easy to understand. The format, and figures are good, with all expected components included. The idea of maximizing the inter/intra agent divergences is intuitively appealing. Further, the authors address the pitfalls of naively maximizing intra-agent divergences by adopting the Cauchy Schwarz divergence. It's especially nice that maximizing the CS divergence implies maximizing the policy entropy as well. Experiments are done on a large number of tasks, with comparisons against expected baselines and parameter sensitivity analyses all present.

Weaknesses:
1. The motivation of the paper is not altogether clear to me. The paper seems to suggest that exploration is more challenging in the sequential update setting, necessitating devoted algorithms. Why would this be the case? 
2. In many of the presented domains, the improvement of MADPO over the next best method is not very large. Sometimes, confidence intervals of MADPO overlap those of the next best method. Can the authors provide statistical significance tests for the main results in Figures 2 and 3, comparing MADPO to the next best method?
3. Some minor suggestions:
- Please check your paper carefully for typos, as there are quite a few: 
    - Line 89: ""connecting link dimension curse""? Not sure what this is
    - No period after Figure 4
    - Trust interval -> confidence interval 
    - Lacking 'and' at line 174
    - Line 204: conditoned -> conditioned
    - Line 216: extra ""of"" 
- Please be sure to state the number of trials in the main text. It is mentioned in the Neurips checklist, but I could not find it  in  the main text
- Please make the colors of the methods the same for both domains (i.e. pick 1 color for MADPO and be consistent with it)

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel multi-agent reinforcement learning (MARL) method called Multi-Agent Divergence Policy Optimization (MADPO), which enhances exploration and heterogeneity through a mutual policy divergence maximization framework. MADPO leverages a sequential updating scheme and quantifies discrepancies between episodes and agents, termed intra-agent divergence and inter-agent divergence, respectively. To address the instability and lack of directionality in traditional divergence measurements, the paper proposes using conditional Cauchy-Schwarz divergence to provide entropy-guided exploration incentives. Experiments demonstrate that the proposed method outperforms state-of-the-art sequential updating approaches in two challenging multi-agent tasks with various heterogeneous scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. **Innovation**: The paper introduces MADPO, a novel MARL method that enhances agent exploration and heterogeneity through mutual policy divergence maximization. 
   
2. **Theoretical Foundation**: The use of conditional Cauchy-Schwarz divergence to address instability and directionality in traditional divergence measurements is a contribution.
   
3. **Experimental Validation**: The experiments conducted on two challenging multi-agent tasks with different heterogeneous scenarios convincingly demonstrate the effectiveness and superiority of MADPO in enhancing exploration and heterogeneity.

Weaknesses:
1.  The paper lacks analysis and comparison with relevant literature on sequential decision-making, such as:
   - Liu J, Zhong Y, Hu S, et al. Maximum Entropy Heterogeneous-Agent Reinforcement Learning[C]//The Twelfth International Conference on Learning Representations.  (This paper extends SAC to heterogeneous sequential decision-making scenarios, and the relationship between this work and the current paper remains unclear.)

2. It is unclear whether the intrinsic reward method proposed in this paper can ensure that the resulting trained policies are consistent with the original policies.

Limitations:
The authors have addressed the limitations of their work and discussed potential negative societal impacts in accordance with the guidelines.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xvVeSZoVJO;"REVIEW 
Summary:
In this paper, the authors proposed an essential problems: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? The authors presented a robust camera-insensitivity collaborative perception with a novel dynamic feature-based 3d neural modeling mechanism to address the issue. Moreover, to verify the effectiveness of the model, the authors also provided a new large-scale dataset, OPV2V-N for this field. The experiments result showcase the model’s robustness in proposed dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Strength:
1.	The paper presents an interesting viewpoint that is to recover noisy camera perceptual information from other agents’ views by modeling the collaborative neural rendering field representation, in which the model is divided into two stages: a time-invariant static background and time-varying dynamic foreground.s
2.	The paper develops a new dataset to fill the gap of the lack of a comprehensive collaborative perception dataset that accounts for different camera noise scenarios.
3.	The paper is well-organized and interesting to read.

Weaknesses:
1.	From my perspective, the paper lacks the theory analysis for the proposed method. Moreover, the authors fail to introduce the motivation of each sub-module in the presented model. For example, can the authors showcase the motivation of using Nerf for the static and dynamic fields, are there any dominant advantages of nerf, compared to other 3d reconstruction methods in this method?
2.	It is necessary to give more rigorous mathematic analysis of equations in this paper. Furthermore, the authors are required to introduce the details of each networks, including the training parameters, learning rate, weight values in eq. 12.

Limitations:
The current work focuses on addressing the camera-insensitivity problem in collaborative perception. It is evident that accurate reconstruction can compensate for the negative impact of noisy camera features on collaborative perception.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? Therefore, RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism is introduced. To validate the new method, the authors also provide a new dataset: OPV2V-N. RCDN serves as baseline here. Ablation Study shows for 5 models (F-cooper, Att-Fuse, Disco-Net, V2VNet, CoBEVT a significant improvement over their baselines, w/o RCDN.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper builds up on three pillars: single perception, collaborative perception and neural rendering. The base idea is novel to the best of my knowledge. The problem formulation is clear and well sounded, easy to follow. The System architecture is strong. The authors also focus on the differentiation between static and dynamic scenarios, especially for the neural fields both based on the BEV volume feature space. This differention is very important, not very often in detail discussed. The ablation study especially table 5.1 shows very accurate an increase of performance for different tasks static (lanes, free space) and dynamic perception. The experimentsl part introduces a new dataset, which is necessray for the investigation.

Weaknesses:
The overall system architecture sounds good. However, there are some open points for me, the impact of section 4.3 and 4.4, i.e. the neural fields part, seems open in terms of clarification. Example: What is difference between sf w , sbw in equation (7)?
The experimental section is a bit too short. I feel its not finished yet. However, there is limited space. The overall approach is not usable for realtime.

Limitations:
The most relevant limitation is the missibg real-time applicability.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents RCDN, a method to aggregate multi-sensor perception signals in dynamic environment.
The key idea, is to improve the aggregated multi-agent feature with the multi-view rendering loss.
At its core, RCDN gathers input streams at varying timesteps of multiple agents. The gathered images are fused into Birds Eye view (BEV) then further decoded into volume. 
The volumetric features are learned into static scene and dynamic scene components with NGP based representation.
Overall procedure is supervised with rendering loss, (cyclic) optical flow consistency.


The method is evaluated on new dataset, OPV2V-N, which is an updated version of OPV2V, with additional masking and optical flow. 
The results show that RCDN helps BEV segmentation with various backbones, compared to the model used without RCDN.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
1: poor

Strengths:
The main benefit of the RCDN, is that it is fairly easy to apply into different existing feature backbones, as it is the post-processing step built on top of BEV features. 
Experimentally, the usage of RCDN significantly improves the segmentations which implies that the features are better aligned throughout the noisy signals. 
This makes the work to be a great off-line data augmentation / preparation pipeline for generating BEV segmentation features. 
The paper additionally proposes OPV2V-N dataset, which may be somewhat valuable addition to the community.

Aside from technical perspective, the paper is easy to follow and well-written.

Weaknesses:
The paper's main weaknesses are two folds. 
1. The paper does not evaluate on tasks other than BEV segmentation. 
While I believe that the pixel-aligned features from NGP would give benefits over various vision tasks, the paper only demonstrates on smaller domain of work which undermines its actual potential. It would have been more interesting to compare how it impacts in different downstreaming tasks, such as detection / tracking.

2. Technical contribution seems to lack novelty. 
The paper is a mix of two known-to-work solutions; BEV feature decoding for segmentation (used with various baselines in the experiments), and NGP (or radiance field based) multi-view pixel / density alignment through rendering loss. Usage of rendering loss to improve segmentation map is well-investigated in different literatures in the NeRF community (e.g, semantic-nerf).

Limitations:
No concerning limitations are found.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposed Bird Eye View (BEV) semantic segmentation pipeline from collaborative perception, robust to motion blur, sensor noise, occlusion and even failure. The proposed a pipeline that adapts neural rendering techniques to overcome the noise/malfunction in camera capture and occlusion. With the proposed method combined with prior methods, performances on OPV2V-N (the proposed BEV semantic segmentation dataset) are improved.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper proposed to apply neural rendering concept for ‘robust’ collaborative-perception BEV segmentation. It is natural way of thinking to overcome noise/malfunction in the caption system but the way the paper adapts neural rendering to BEV segmentation is novel. And, the performance is verified with OPV2V-N dataset.

Weaknesses:
Evaluation is only performed with OPV2V-N dataset which may result in overfitting. More evaluation with different dataset is required. The author may need to compare methodologies on other dataset although the existing dataset do not have noise. The author also may add random noise to the prior dataset and run experiments.

The manuscript was uneasy to read and understand. The paper should re-written. The comments below are without understanding supplemental materials fully.
- The way proposed algorithm is combined with prior method is unclear. The reviewer guessed that the MCP module can be replaced with prior methods, but it is not stated.
- Many abbreviations are not explained sufficiently and terminologies the author defined are ambiguous and may be incorrect. 
- MCP is short for the multi-agents collaborative perception process but the paper did not explain MCP module in details with no reference
- BEV, no full name, no reference.
- “Camera-insensitivity” can be understood terminologies related to camera sensor sensitivity (how much the camera sensor accept photon…).
- Robust Camera-Insensitivity: Robust == Camera-sensitivity? The latter one may be redundant
- Line 6. introduce a new robust camera-insensitivity problem: cam be replaced “introduce BEV segmentation when the camera capture are unreliable (or noisy)?” Should be more concrete without ambiguous words
- Line19 “Ported to” mean?
- There are more unclear sentences.

Limitations:
.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces RCDN, a novel method for robust camera-insensitivity collaborative perception. This method aims to overcome challenges associated with noisy, obscured, or failed camera perspectives by using dynamic feature-based 3D neural modeling. RCDN constructs collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. The proposed system consists of two collaborative field phases: a time-invariant static background field and a time-varying dynamic field. To validate RCDN, a new dataset called OPV2V-N was created. The paper demonstrates that RCDN improves the robustness of baseline methods in extreme camera-insensitivity settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
*Innovative Problem Addressing*: The paper tackles a significant real-world problem of camera insensitivity in multi-agent collaborative perception, which is crucial for autonomous systems.

*Novel Methodology*: The introduction of dynamic feature-based 3D neural modeling and the construction of collaborative neural rendering field representations are innovative approaches.

*Comprehensive Dataset*: The creation of the OPV2V-N dataset, which includes various camera failure scenarios, provides a robust platform for testing and validating the proposed method.

*Performance Improvement*: The extensive experiments and quantitative evaluations show significant improvements in robustness and performance over baseline methods.

*Detailed Evaluation*: The paper includes both quantitative and qualitative evaluations, along with ablation studies, which thoroughly demonstrate the effectiveness of RCDN.

Weaknesses:
*Complexity and Computation*: The proposed method involves complex modeling and multiple steps. The author should provide the latency.

Generalizability: The performance of RCDN is primarily validated on the OPV2V-N dataset, which may limit the generalizability of the results to other datasets or real-world scenarios.


*Failure Cases*: It would be nice if the authors provide failure cases, which is important.

Limitations:
Please see weakness and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors proposed an essential problems: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? The authors presented a robust camera-insensitivity collaborative perception with a novel dynamic feature-based 3d neural modeling mechanism to address the issue. Moreover, to verify the effectiveness of the model, the authors also provided a new large-scale dataset, OPV2V-N for this field. The experiments result showcase the model’s robustness in proposed dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Strength:
1.	The paper presents an interesting viewpoint that is to recover noisy camera perceptual information from other agents’ views by modeling the collaborative neural rendering field representation, in which the model is divided into two stages: a time-invariant static background and time-varying dynamic foreground.s
2.	The paper develops a new dataset to fill the gap of the lack of a comprehensive collaborative perception dataset that accounts for different camera noise scenarios.
3.	The paper is well-organized and interesting to read.

Weaknesses:
1.	From my perspective, the paper lacks the theory analysis for the proposed method. Moreover, the authors fail to introduce the motivation of each sub-module in the presented model. For example, can the authors showcase the motivation of using Nerf for the static and dynamic fields, are there any dominant advantages of nerf, compared to other 3d reconstruction methods in this method?
2.	It is necessary to give more rigorous mathematic analysis of equations in this paper. Furthermore, the authors are required to introduce the details of each networks, including the training parameters, learning rate, weight values in eq. 12.

Limitations:
The current work focuses on addressing the camera-insensitivity problem in collaborative perception. It is evident that accurate reconstruction can compensate for the negative impact of noisy camera features on collaborative perception.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? Therefore, RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism is introduced. To validate the new method, the authors also provide a new dataset: OPV2V-N. RCDN serves as baseline here. Ablation Study shows for 5 models (F-cooper, Att-Fuse, Disco-Net, V2VNet, CoBEVT a significant improvement over their baselines, w/o RCDN.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper builds up on three pillars: single perception, collaborative perception and neural rendering. The base idea is novel to the best of my knowledge. The problem formulation is clear and well sounded, easy to follow. The System architecture is strong. The authors also focus on the differentiation between static and dynamic scenarios, especially for the neural fields both based on the BEV volume feature space. This differention is very important, not very often in detail discussed. The ablation study especially table 5.1 shows very accurate an increase of performance for different tasks static (lanes, free space) and dynamic perception. The experimentsl part introduces a new dataset, which is necessray for the investigation.

Weaknesses:
The overall system architecture sounds good. However, there are some open points for me, the impact of section 4.3 and 4.4, i.e. the neural fields part, seems open in terms of clarification. Example: What is difference between sf w , sbw in equation (7)?
The experimental section is a bit too short. I feel its not finished yet. However, there is limited space. The overall approach is not usable for realtime.

Limitations:
The most relevant limitation is the missibg real-time applicability.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents RCDN, a method to aggregate multi-sensor perception signals in dynamic environment.
The key idea, is to improve the aggregated multi-agent feature with the multi-view rendering loss.
At its core, RCDN gathers input streams at varying timesteps of multiple agents. The gathered images are fused into Birds Eye view (BEV) then further decoded into volume. 
The volumetric features are learned into static scene and dynamic scene components with NGP based representation.
Overall procedure is supervised with rendering loss, (cyclic) optical flow consistency.


The method is evaluated on new dataset, OPV2V-N, which is an updated version of OPV2V, with additional masking and optical flow. 
The results show that RCDN helps BEV segmentation with various backbones, compared to the model used without RCDN.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
1: poor

Strengths:
The main benefit of the RCDN, is that it is fairly easy to apply into different existing feature backbones, as it is the post-processing step built on top of BEV features. 
Experimentally, the usage of RCDN significantly improves the segmentations which implies that the features are better aligned throughout the noisy signals. 
This makes the work to be a great off-line data augmentation / preparation pipeline for generating BEV segmentation features. 
The paper additionally proposes OPV2V-N dataset, which may be somewhat valuable addition to the community.

Aside from technical perspective, the paper is easy to follow and well-written.

Weaknesses:
The paper's main weaknesses are two folds. 
1. The paper does not evaluate on tasks other than BEV segmentation. 
While I believe that the pixel-aligned features from NGP would give benefits over various vision tasks, the paper only demonstrates on smaller domain of work which undermines its actual potential. It would have been more interesting to compare how it impacts in different downstreaming tasks, such as detection / tracking.

2. Technical contribution seems to lack novelty. 
The paper is a mix of two known-to-work solutions; BEV feature decoding for segmentation (used with various baselines in the experiments), and NGP (or radiance field based) multi-view pixel / density alignment through rendering loss. Usage of rendering loss to improve segmentation map is well-investigated in different literatures in the NeRF community (e.g, semantic-nerf).

Limitations:
No concerning limitations are found.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposed Bird Eye View (BEV) semantic segmentation pipeline from collaborative perception, robust to motion blur, sensor noise, occlusion and even failure. The proposed a pipeline that adapts neural rendering techniques to overcome the noise/malfunction in camera capture and occlusion. With the proposed method combined with prior methods, performances on OPV2V-N (the proposed BEV semantic segmentation dataset) are improved.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper proposed to apply neural rendering concept for ‘robust’ collaborative-perception BEV segmentation. It is natural way of thinking to overcome noise/malfunction in the caption system but the way the paper adapts neural rendering to BEV segmentation is novel. And, the performance is verified with OPV2V-N dataset.

Weaknesses:
Evaluation is only performed with OPV2V-N dataset which may result in overfitting. More evaluation with different dataset is required. The author may need to compare methodologies on other dataset although the existing dataset do not have noise. The author also may add random noise to the prior dataset and run experiments.

The manuscript was uneasy to read and understand. The paper should re-written. The comments below are without understanding supplemental materials fully.
- The way proposed algorithm is combined with prior method is unclear. The reviewer guessed that the MCP module can be replaced with prior methods, but it is not stated.
- Many abbreviations are not explained sufficiently and terminologies the author defined are ambiguous and may be incorrect. 
- MCP is short for the multi-agents collaborative perception process but the paper did not explain MCP module in details with no reference
- BEV, no full name, no reference.
- “Camera-insensitivity” can be understood terminologies related to camera sensor sensitivity (how much the camera sensor accept photon…).
- Robust Camera-Insensitivity: Robust == Camera-sensitivity? The latter one may be redundant
- Line 6. introduce a new robust camera-insensitivity problem: cam be replaced “introduce BEV segmentation when the camera capture are unreliable (or noisy)?” Should be more concrete without ambiguous words
- Line19 “Ported to” mean?
- There are more unclear sentences.

Limitations:
.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces RCDN, a novel method for robust camera-insensitivity collaborative perception. This method aims to overcome challenges associated with noisy, obscured, or failed camera perspectives by using dynamic feature-based 3D neural modeling. RCDN constructs collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. The proposed system consists of two collaborative field phases: a time-invariant static background field and a time-varying dynamic field. To validate RCDN, a new dataset called OPV2V-N was created. The paper demonstrates that RCDN improves the robustness of baseline methods in extreme camera-insensitivity settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
*Innovative Problem Addressing*: The paper tackles a significant real-world problem of camera insensitivity in multi-agent collaborative perception, which is crucial for autonomous systems.

*Novel Methodology*: The introduction of dynamic feature-based 3D neural modeling and the construction of collaborative neural rendering field representations are innovative approaches.

*Comprehensive Dataset*: The creation of the OPV2V-N dataset, which includes various camera failure scenarios, provides a robust platform for testing and validating the proposed method.

*Performance Improvement*: The extensive experiments and quantitative evaluations show significant improvements in robustness and performance over baseline methods.

*Detailed Evaluation*: The paper includes both quantitative and qualitative evaluations, along with ablation studies, which thoroughly demonstrate the effectiveness of RCDN.

Weaknesses:
*Complexity and Computation*: The proposed method involves complex modeling and multiple steps. The author should provide the latency.

Generalizability: The performance of RCDN is primarily validated on the OPV2V-N dataset, which may limit the generalizability of the results to other datasets or real-world scenarios.


*Failure Cases*: It would be nice if the authors provide failure cases, which is important.

Limitations:
Please see weakness and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xoc4QOvbDs;"REVIEW 
Summary:
This paper studies multi-view clustering and seeks to investigate the view cooperation issue. The authors consider DMVC as an unsupervised cooperative game and regard each view as a participant. Compared with the existing methods, this consideration is new and interesting. Based on the novel idea, the authors proposed SCE-MVC, a novel shapley-based cooperation enhancing multi-view clustering method. The paper is well-organized. The experiments are convincing.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper proposes a new point also an interesting point for multi-view clustering tasks, i.e., considering the multi-view collaboration as a cooperative game. 

2. The experiments are sufficient and convincing. The authors validate the method from many aspects. The proposed SCE-MVC obtains much better performance on six diverse datasets.

Weaknesses:
1. Figure 2 is confusing. The specific structure of View Cooperation Enhancing Module is not clearly presented.

2. There are many formulas and symbols. It is suggested to add a notation table.

3. Although the authors try to explain model (1), it is still difficult to understand Shapley Value from the model. In addition, many variables are not clearly explained. The authors should present more information about the model and explain all variables used in this model, such as S_i, {i}, s\{i}， etc.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author introduces a Shapley-based cooperation enhancement framework aimed at fostering collaboration among different views. The SCE-MVC method incorporates cooperative game theory, considering each view as a participant in the model and assessing their contributions using the Shapley Value.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Viewing each view as an individual player within game theory represents a fresh perspective in multi-view clustering. Also, enhancing clustering performance through balancing view contribution is both well-founded and innovative.

Weaknesses:
1. Using the SCE module in an alignment-based framework only provides a marginal improvement to the model. Does this imply that the SCE module is ineffective in the alignment-based framework? 

2. The view contributions of alignment-based method is much balanced than view contributions of joint methods. Does this imply that the alignment-based method is much better than the joint method? It's not reasonable since the clustering performance of alignment-based methods may not necessarily be better than that of joint methods.

3. Is the complexity of computing Shapley values truly O(n!)? When dealing with a larger number of views, can this evaluation framework still be utilized for computation?

4. Are the loss functions L in Eqs (15) and (16) on page 6 the same? If so, there is a problem of inconsistent dependent variables. In addition, $D_ij$ in Eq. (9) is a scalar and should not be bolded.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The study centers on improving task performance via deep multi-view clustering (DMVC) and fostering cooperation among different views. Specifically, the study evaluates view contributions, emphasizing the significance of strengthening cooperation among views.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Considering multi-view tasks from a collaborative standpoint represents a novel approach, with the paper's motivation being notably fresh. Moreover, the paper elucidates potential contribution imbalances in the joint method and addresses them through the SCE method, thereby enhancing cooperation among views.

Weaknesses:
When dealing with datasets comprising more than two views, such as three views, how can one assess whether the contribution of the views has become more evenly distributed after employing SCE? While the paper visually presents the contributions of the views, could a quantitative method be provided for this evaluation?

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This research merges game theory with multi-view clustering by introducing the Shapley-based Cooperation Enhancing (SCE) approach. It features a module to systematically evaluate each view's contribution. The approach promotes view cooperation by adjusting the training convergence rate of view parameters based on their contributions. Extensive experiments on various datasets demonstrate the method's effectiveness when applied to different MVC frameworks.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1) The paper integrates the Shapley value from game theory into DMVC, allowing for precise assessment of each view's contribution.
2) Theoretical analysis is thorough, with clear and intuitive figures.
3) The manuscript is well-organized and clearly written.

Weaknesses:
The article categorizes DMVC into alignment-based and joint methods. What criteria were used for this classification? Furthermore, only one DMJC method is used as a representative for joint methods.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper firstly considered DMVC as an unsupervised cooperative game where each view can be regarded as a participant. Then, the authors introduced the shapley value and propose a novel MVC framework termed Shapley-based Cooperation Enhancing Multi-view Clustering (SCE-MVC), which evaluates view cooperation with game theory. In summary, this paper was well written with obvious superiority.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
-- A MVC framework was designed that utilizeD game theory and Shapley values to evaluate and elevate inter-view cooperation. 
-- The experiments were sufficient, and the analysis of the experimental results was adequate.

Weaknesses:
-- In this paper, why utilize $\phi_i$ to measure the contribution of views instead of the view weight $w_i$? The article's explanation on this is not clear enough, and there is a lack of experiments to demonstrate the relationship between $\phi_i$ and $w_i$.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This research merges game theory with multi-view clustering by introducing the Shapley-based Cooperation Enhancing (SCE) approach. It features a module to systematically evaluate each view's contribution. The approach promotes view cooperation by adjusting the training convergence rate of view parameters based on their contributions. Extensive experiments on various datasets demonstrate the method's effectiveness when applied to different MVC frameworks.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1) The paper integrates the Shapley value from game theory into DMVC, allowing for precise assessment of each view's contribution.
2) Theoretical analysis is thorough, with clear and intuitive figures.
3) The manuscript is well-organized and clearly written.

Weaknesses:
The article categorizes DMVC into alignment-based and joint methods. What criteria were used for this classification? Furthermore, only one DMJC method is used as a representative for joint methods.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper firstly considered DMVC as an unsupervised cooperative game where each view can be regarded as a participant. Then, the authors introduced the shapley value and propose a novel MVC framework termed Shapley-based Cooperation Enhancing Multi-view Clustering (SCE-MVC), which evaluates view cooperation with game theory. In summary, this paper was well written with obvious superiority.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
-- A MVC framework was designed that utilizeD game theory and Shapley values to evaluate and elevate inter-view cooperation. 
-- The experiments were sufficient, and the analysis of the experimental results was adequate.

Weaknesses:
-- In this paper, why utilize $\phi_i$ to measure the contribution of views instead of the view weight $w_i$? The article's explanation on this is not clear enough, and there is a lack of experiments to demonstrate the relationship between $\phi_i$ and $w_i$.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies multi-view clustering and seeks to investigate the view cooperation issue. The authors consider DMVC as an unsupervised cooperative game and regard each view as a participant. Compared with the existing methods, this consideration is new and interesting. Based on the novel idea, the authors proposed SCE-MVC, a novel shapley-based cooperation enhancing multi-view clustering method. The paper is well-organized. The experiments are convincing.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper proposes a new point also an interesting point for multi-view clustering tasks, i.e., considering the multi-view collaboration as a cooperative game. 

2. The experiments are sufficient and convincing. The authors validate the method from many aspects. The proposed SCE-MVC obtains much better performance on six diverse datasets.

Weaknesses:
1. Figure 2 is confusing. The specific structure of View Cooperation Enhancing Module is not clearly presented.

2. There are many formulas and symbols. It is suggested to add a notation table.

3. Although the authors try to explain model (1), it is still difficult to understand Shapley Value from the model. In addition, many variables are not clearly explained. The authors should present more information about the model and explain all variables used in this model, such as S_i, {i}, s\{i}， etc.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author introduces a Shapley-based cooperation enhancement framework aimed at fostering collaboration among different views. The SCE-MVC method incorporates cooperative game theory, considering each view as a participant in the model and assessing their contributions using the Shapley Value.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Viewing each view as an individual player within game theory represents a fresh perspective in multi-view clustering. Also, enhancing clustering performance through balancing view contribution is both well-founded and innovative.

Weaknesses:
1. Using the SCE module in an alignment-based framework only provides a marginal improvement to the model. Does this imply that the SCE module is ineffective in the alignment-based framework? 

2. The view contributions of alignment-based method is much balanced than view contributions of joint methods. Does this imply that the alignment-based method is much better than the joint method? It's not reasonable since the clustering performance of alignment-based methods may not necessarily be better than that of joint methods.

3. Is the complexity of computing Shapley values truly O(n!)? When dealing with a larger number of views, can this evaluation framework still be utilized for computation?

4. Are the loss functions L in Eqs (15) and (16) on page 6 the same? If so, there is a problem of inconsistent dependent variables. In addition, $D_ij$ in Eq. (9) is a scalar and should not be bolded.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The study centers on improving task performance via deep multi-view clustering (DMVC) and fostering cooperation among different views. Specifically, the study evaluates view contributions, emphasizing the significance of strengthening cooperation among views.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Considering multi-view tasks from a collaborative standpoint represents a novel approach, with the paper's motivation being notably fresh. Moreover, the paper elucidates potential contribution imbalances in the joint method and addresses them through the SCE method, thereby enhancing cooperation among views.

Weaknesses:
When dealing with datasets comprising more than two views, such as three views, how can one assess whether the contribution of the views has become more evenly distributed after employing SCE? While the paper visually presents the contributions of the views, could a quantitative method be provided for this evaluation?

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
vt2qkE1Oax;"REVIEW 
Summary:
The authors propose a loss function that seeks to group the trajectories into low-rank matrices where the motion of object points can be approximately explained as a linear combination of other point tracks. Experiments on the synthetic MOVi-F variant of
the Kubric dataset  and the real datasets DAVIS 2016, SegTrackv2 and FBMS show that the proposed method outperforms single-sequence methods, single-stage end-to-end methods and multi-stage methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1) The authors address key issues in the field and the contribution is original even if somewhat incremental.
2) The proposed method is detailed and reproducible.
3) Experiments are relatively well conducted on synthetic and real datasets showing the superiority of the proposed method.

Weaknesses:
About the presentation, please clearly state a name/acronym to the proposed method and replace ""ours"" by it in the comparison tables.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a method for training a segmentation network using long-term point trajectories as a supervisory signal to enhance optical flow. It proposes a novel loss function aimed at grouping these trajectories into low-rank matrices, allowing the motion of object points to be approximately represented as a linear combination of other point tracks. The proposed approach surpasses previous methods in motion-based segmentation, demonstrating the value of long-term motion and the effectiveness of the new formulation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The introduction describes the problem in more detail when introducing the issue.
2. The structure of the article is good.
3. The experimental results of the method proposed in this paper show a significant improvement.

Weaknesses:
1. The main contribution of this paper is the proposal of two losses, but the loss seems to be effective in the experiments of other segmentation methods.
2. The contribution of the paper in Subspace Clustering is not described clearly.
3. The resolution of Fig 3 is relatively low.
4. There is a lack of comparison in terms of inference speed.

Limitations:
The authors have acknowledged some limitations of their work. I suggest the authors could further describe the limitations in the generalizability of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel loss function that allows training image object segmentation
models based on object motion in videos. Motivated by recent work on self-supervised
learning of segmentation using optical flow, the authors propose to use longer point
trajectories as additional self-supervision signal. Related to subspace clustering, the
proposed loss function encourages to predict segments whose trajectories can be well
explained by few basis trajectories. The predicted segments are merged into a binary
segmentation mask and evaluated on standard, real-world segmentation benchmarks.
Previous methods based only on optical flow are consistently outperformed, demonstrating
the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Unsupervised learning of segmentation is an important problem. Several recent methods
  approached this task using optical flow as a self-supervision signal, extending this
  line of research to trajectories is a well-motivated idea.
- The mathematical motivation of the loss is very well explained. Without having a deep
  mathematical background, I could follow the derivation of the loss function without
  issues.
- Standard benchmarks and modelling components are used for evaluation, which makes it
  easy to compare the proposed method to previous approaches.

Weaknesses:
1. It is not described clearly enough what kind of segmentation task is targeted. From
   the introduction and method section it seems to me that multi-object segmentation is
   adressed, only at the very end of the method section it is mentioned that the
   predicted segments are merged into a binary segmentation in some cases.
   - To my understanding the task is multi-object segmentation for MOVi and binary
     segmentation for all other datasets. This should be clearly stated in the
     experiment section.
   - It should be stated in the introduction and method section more clearly that the
     main task is binary segmentation.

2. The proposed method is not compared to models that do not use optical flow for
   self-supervision. It would be interesting to see how the proposed method compares to
   other self-supervised segmentation approaches. For example
   - CutLER ([Wang et al. 2023](https://arxiv.org/abs/2301.11320)) and VideoCutLER ([Wang et al. 2023](https://arxiv.org/abs/2308.14710))
   - DINOSAUR ([Seitzer et al. 2023](https://www.amazon.science/publications/bridging-the-gap-to-real-world-object-centric-learning)) and VideoSAUR ([Zadaianchuk et al. 2023](https://proceedings.neurips.cc/paper_files/paper/2023/hash/c1fdec0d7ea1affa15bd09dd0fd3af05-Abstract-Conference.html))
   
    The masks predicted by these models could be merged to obtain a binary segmentation
   in the same way as for the proposed method.

Limitations:
The authors address limitations of their work in a dedicated paragraph. Their
discussion is brief but adequate in my view.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a model to process long-term motion and short-term motion simultaneously to achieve motion-based segmentation. Specifically, motivated by subspace clustering, this work proposes a loss function that enables training a neural network to learn motion grouping from both optical flows and point trajectories. It outperforms the previous method in the unsupervised video segmentation task. The qualitative comparison also shows obvious improvement, giving a clearer boundary.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation and method explanation seems to be clear. The paper writing is easy to follow.
2. Using a simple sample to introduce the low-rank intuition is convincing and reasonable. Based on this core idea, other smoothing losses and regular loss from optical flow make learning more effective.
3. Experiments show the strength of the proposed strategy. A comprehensive ablation study has been performed to illustrate the impact of each factor.

Weaknesses:
1. As mentioned in the limitation, the paper's principle assumes that the object is rigid. However, the task that this paper works on not only includes rigid objects -- it's a general video segmentation task. Then it seems that the low-rank theory can not extend to a general setting. And why not consider local rigid like ARAP loss? (SpatialTracker)
2. Do not give some corner cases or failure cases, especially for non-rigid objects. I hope to see some corner cases like multiple objects, where they behave similarly in the short term but different in the long term. Then it can better demonstrate the motivation of the paper.

Limitations:
As mentioned in the weakness, the principle the paper proposed is reasonable, but seems like it does not fully support the motivation and the ultimate goal of the task. More analysis and experiments are needed to show the right practice when applying the proposed to real-world videos.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tackles video object segmentation by incorporating into the loss function not only instantaneous optical flow information but also long term pixel tracking information.  Raft was used for optical flow and CoTracker was used for long term pixel tracking in the experiments.  The experiments show a marginal improvement in performance when combining the two information sources in the loss function.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper flows quite well, it addresses that video object segmentation is the problem space, the focus is on loss function, Figure 2, the layout appears clear as well.  There are a handful of datasets and comparing methods used in the experiments.

Weaknesses:
Table 2 where the experimental results are presented lists a collection of methods categorized into different groupings.  Perhaps these groupings and methods could be better discussed in the lit review, it appears that the categories in the lit review do not correlate nicely and I do not know the difference of these methods unless I look at the references and read the papers myself.
The improvement is incremental.  IT is expected that there would be some improvement however what cases do we actually get the improvement in, a bit of more depth in the analysis would make this a better paper.
I assume that the camera is static?, correct? if not, perhaps making this clearer would help.
I have no idea how long the long term point trajectories were, perhaps analyzing this would help.  Also depending on the trajectories, were there occlusions or other interesting factors that contribute to the loss function would be interesting.

Limitations:
I am not sure that the examples actually illustrate that motion segmentation is necessary for these cases.  I would focus on cases where appearance information is not enough.
Can this system deal with a moving camera or does the camera have to be static?
How well does the system work under occlusion?
Different motions of the object of interest will results in different performance, perhaps diving into this analysis would be informative.
Both sources of info, optical flow and long term pixel tracking info are based on 2D info, the projection of 3D info.  This has limitations.  The paper should have explored different object movements.  It does state that non rigid objects when dealing with multiple objects is an issue however an in depth exploration for a single non rigid object would be informative.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors propose a loss function that seeks to group the trajectories into low-rank matrices where the motion of object points can be approximately explained as a linear combination of other point tracks. Experiments on the synthetic MOVi-F variant of
the Kubric dataset  and the real datasets DAVIS 2016, SegTrackv2 and FBMS show that the proposed method outperforms single-sequence methods, single-stage end-to-end methods and multi-stage methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1) The authors address key issues in the field and the contribution is original even if somewhat incremental.
2) The proposed method is detailed and reproducible.
3) Experiments are relatively well conducted on synthetic and real datasets showing the superiority of the proposed method.

Weaknesses:
About the presentation, please clearly state a name/acronym to the proposed method and replace ""ours"" by it in the comparison tables.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a method for training a segmentation network using long-term point trajectories as a supervisory signal to enhance optical flow. It proposes a novel loss function aimed at grouping these trajectories into low-rank matrices, allowing the motion of object points to be approximately represented as a linear combination of other point tracks. The proposed approach surpasses previous methods in motion-based segmentation, demonstrating the value of long-term motion and the effectiveness of the new formulation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The introduction describes the problem in more detail when introducing the issue.
2. The structure of the article is good.
3. The experimental results of the method proposed in this paper show a significant improvement.

Weaknesses:
1. The main contribution of this paper is the proposal of two losses, but the loss seems to be effective in the experiments of other segmentation methods.
2. The contribution of the paper in Subspace Clustering is not described clearly.
3. The resolution of Fig 3 is relatively low.
4. There is a lack of comparison in terms of inference speed.

Limitations:
The authors have acknowledged some limitations of their work. I suggest the authors could further describe the limitations in the generalizability of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel loss function that allows training image object segmentation
models based on object motion in videos. Motivated by recent work on self-supervised
learning of segmentation using optical flow, the authors propose to use longer point
trajectories as additional self-supervision signal. Related to subspace clustering, the
proposed loss function encourages to predict segments whose trajectories can be well
explained by few basis trajectories. The predicted segments are merged into a binary
segmentation mask and evaluated on standard, real-world segmentation benchmarks.
Previous methods based only on optical flow are consistently outperformed, demonstrating
the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Unsupervised learning of segmentation is an important problem. Several recent methods
  approached this task using optical flow as a self-supervision signal, extending this
  line of research to trajectories is a well-motivated idea.
- The mathematical motivation of the loss is very well explained. Without having a deep
  mathematical background, I could follow the derivation of the loss function without
  issues.
- Standard benchmarks and modelling components are used for evaluation, which makes it
  easy to compare the proposed method to previous approaches.

Weaknesses:
1. It is not described clearly enough what kind of segmentation task is targeted. From
   the introduction and method section it seems to me that multi-object segmentation is
   adressed, only at the very end of the method section it is mentioned that the
   predicted segments are merged into a binary segmentation in some cases.
   - To my understanding the task is multi-object segmentation for MOVi and binary
     segmentation for all other datasets. This should be clearly stated in the
     experiment section.
   - It should be stated in the introduction and method section more clearly that the
     main task is binary segmentation.

2. The proposed method is not compared to models that do not use optical flow for
   self-supervision. It would be interesting to see how the proposed method compares to
   other self-supervised segmentation approaches. For example
   - CutLER ([Wang et al. 2023](https://arxiv.org/abs/2301.11320)) and VideoCutLER ([Wang et al. 2023](https://arxiv.org/abs/2308.14710))
   - DINOSAUR ([Seitzer et al. 2023](https://www.amazon.science/publications/bridging-the-gap-to-real-world-object-centric-learning)) and VideoSAUR ([Zadaianchuk et al. 2023](https://proceedings.neurips.cc/paper_files/paper/2023/hash/c1fdec0d7ea1affa15bd09dd0fd3af05-Abstract-Conference.html))
   
    The masks predicted by these models could be merged to obtain a binary segmentation
   in the same way as for the proposed method.

Limitations:
The authors address limitations of their work in a dedicated paragraph. Their
discussion is brief but adequate in my view.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a model to process long-term motion and short-term motion simultaneously to achieve motion-based segmentation. Specifically, motivated by subspace clustering, this work proposes a loss function that enables training a neural network to learn motion grouping from both optical flows and point trajectories. It outperforms the previous method in the unsupervised video segmentation task. The qualitative comparison also shows obvious improvement, giving a clearer boundary.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation and method explanation seems to be clear. The paper writing is easy to follow.
2. Using a simple sample to introduce the low-rank intuition is convincing and reasonable. Based on this core idea, other smoothing losses and regular loss from optical flow make learning more effective.
3. Experiments show the strength of the proposed strategy. A comprehensive ablation study has been performed to illustrate the impact of each factor.

Weaknesses:
1. As mentioned in the limitation, the paper's principle assumes that the object is rigid. However, the task that this paper works on not only includes rigid objects -- it's a general video segmentation task. Then it seems that the low-rank theory can not extend to a general setting. And why not consider local rigid like ARAP loss? (SpatialTracker)
2. Do not give some corner cases or failure cases, especially for non-rigid objects. I hope to see some corner cases like multiple objects, where they behave similarly in the short term but different in the long term. Then it can better demonstrate the motivation of the paper.

Limitations:
As mentioned in the weakness, the principle the paper proposed is reasonable, but seems like it does not fully support the motivation and the ultimate goal of the task. More analysis and experiments are needed to show the right practice when applying the proposed to real-world videos.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tackles video object segmentation by incorporating into the loss function not only instantaneous optical flow information but also long term pixel tracking information.  Raft was used for optical flow and CoTracker was used for long term pixel tracking in the experiments.  The experiments show a marginal improvement in performance when combining the two information sources in the loss function.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper flows quite well, it addresses that video object segmentation is the problem space, the focus is on loss function, Figure 2, the layout appears clear as well.  There are a handful of datasets and comparing methods used in the experiments.

Weaknesses:
Table 2 where the experimental results are presented lists a collection of methods categorized into different groupings.  Perhaps these groupings and methods could be better discussed in the lit review, it appears that the categories in the lit review do not correlate nicely and I do not know the difference of these methods unless I look at the references and read the papers myself.
The improvement is incremental.  IT is expected that there would be some improvement however what cases do we actually get the improvement in, a bit of more depth in the analysis would make this a better paper.
I assume that the camera is static?, correct? if not, perhaps making this clearer would help.
I have no idea how long the long term point trajectories were, perhaps analyzing this would help.  Also depending on the trajectories, were there occlusions or other interesting factors that contribute to the loss function would be interesting.

Limitations:
I am not sure that the examples actually illustrate that motion segmentation is necessary for these cases.  I would focus on cases where appearance information is not enough.
Can this system deal with a moving camera or does the camera have to be static?
How well does the system work under occlusion?
Different motions of the object of interest will results in different performance, perhaps diving into this analysis would be informative.
Both sources of info, optical flow and long term pixel tracking info are based on 2D info, the projection of 3D info.  This has limitations.  The paper should have explored different object movements.  It does state that non rigid objects when dealing with multiple objects is an issue however an in depth exploration for a single non rigid object would be informative.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xtK3gZjQDC;"REVIEW 
Summary:
The paper analyzes decision support systems based on prediction set algorithms. The authors show that: (i) the usage of conformal prediction techniques is generally sub-optimal in terms of accuracy; (ii) the problem of finding the optimal prediction sets under human assistance is NP-hard. Moreover, they provide (iii) a greedy algorithm that is guaranteed to find prediction sets that are better than those provided by conformal predictors. Experimental evaluation on synthetic and real data show the effectiveness of the considered approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main strengths of the paper are:

1. the actual paper contribution is well framed;
2. the theoretical analysis is sound;
3. the proposed algorithm improves over existing approaches.

Weaknesses:
I think this work is a good paper, without major weaknesses, as it provides solid theoretical insights.
The concerns I have are mainly due to typos/details missing. I will point out here these and a few remarks that might be considered for the final version of the paper.

1. It seems to me that Table 2 and Figure 3 are missing the BRUTE FORCE baseline.
2. regarding the style of the paper, I found lines 135-146 very dense. Maybe providing a more concrete example (e.g., what could 1,2,3 represent?) might help the reader getting through it.
3. In Algorihm 1, I think adding a comment to the pseudo-code (from lines 4 to 13) could be useful
4. regarding the limitation section (evaluation) a useful reference might be [Stutz et al., 2023], where the authors evaluate the possibility that human experts might not be approximating the true probability distribution 
5. the experimental analysis (on real data) could be enriched with other popular Learning-to-Defer datasets, such as Cifar10H or hatespeech.

[Stutz et al., 2023] - Stutz, David, Abhijit Guha Roy, Tatiana Matejovicova, Patricia Strachan, Ali Taylan Cemgil, and Arnaud Doucet. ""Conformal prediction under ambiguous ground truth."" Transactions on Machine Learning Research (2023).

Limitations:
The paper adequately discussed the limitations and societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors first show that conformal prediction sets may not lead to human decision optimality.  The authors then introduce a greedy algorithm to generate candidate prediction sets that improve human decisions regarding the accuracy metric.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors find the sub-optimality of conformal prediction sets on providing candidates for human decisions. Thereby, they propose a novel method to produce prediction sets that helps to improve human prediction.

Weaknesses:
* The presentation somewhere is unclear: 
  * Line 86. Please break the sentence properly.	
  * Line 40/43/48: It is unclear for readers when the authors mention “optimal” multiple times but delay its explicit definition later.
  * Line 197: It is confusing when the authors refer to the role of $a$. What is the value of $a$?

* The authors claim they propose an efficient algorithm. However, I am not sure which part is efficient. Are there any numerical metrics, e.g., running time, supporting this contribution? Additionally, how should we understand this restriction of “for a large class of non-conformity scores and expert models” in line 51?

* Line 90:  But you also miss the possibility outside the prediction set, especially when the prediction set is not that good. I think the authors need to discuss the exploitation-exploration dilemma.

* The authors use the scores related to softmax and APS. Other papers propose alternative scores like RAPS and SAPS. I think they should be included.

Limitations:
Please see the above sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper shows the conformal prediction set may not be the optimal set recommendation to humans if humans follow certain choice models. The authors then propose a greedy algorithm by modeling $P(y|x)$ and the choice model of humans assuming it follows MNL model. Authors compare the proposed method against the standard conformal prediction set under synthetic human experts and the proposed method has a slightly better performance compared to traditional conformal sets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors consider conformal prediction in the human-in-the-loop setting, which is an important problem. The first part of the paper shows the conformal prediction set may not be the best recommendation set for humans, which is easy to understand since most conformal sets arrange the set in a ranked order and we can play with the human choice models to create an example that conformal sets may not be the best recommendation set.

Weaknesses:
The problem setting is not realistic: The authors do not allow humans to select outside the conformal prediction set. However, in the setups of most empirical successes of human-AI collaboration with conformal prediction, this is allowed. Similarly, if the authors do not allow humans to select outside the conformal prediction set, humans' value is greatly reduced and the optimal thing to do may be just to use fully automated AI prediction and in all the toy examples the authors provided, kicking humans out of the loop is the optimal system (humans only make things worse). 

The theoretical analysis seems useless: I think the theoretical analysis is useless for two reasons: 1) while identifying the optimal set is NP-hard, in practice the metric we care about is $\mathbb{E} g(S|x)$, not identifying the optimal set. If an algorithm can get a good rate of convergence for this regret, then this problem is not hopeless, so I think authors need to show for all conformal prediction algorithms, what is the regret lower bound for $\mathbb{E} g(S|x)$; 2) while I can see that sometimes the label set can be large. In practice, the theoretical results may not be a big issue for many problems since most problems have small label set (binary or three classes). This negative results may not seem that severe as the authors presented in the paper. 

The solution is disconnected and not useful in human-AI collaboration: 1) The proposed solution does not enjoy the distributionally-free guarantee, which is the main reason why people use conformal prediction. I would expect authors to provide a conformal prediction algorithm that is human-centered, rather than directly switch lanes to traditional prediction methods. 2) The proposed solution requires $P(y|x)$ and the true human choice model, which is too strong to be realistic. If I know $P(y|x)$, why should I involve humans in the loop anymore (recall that authors can restrict humans only select from prediction set so humans are not necessary in the system). The optimal strategy would be directly use $P(y|x)$ to select actions. 

Baselines: For human-AI collaboration tasks, I expect to see the proposed solution is better than human working alone or AI working alone. The authors should compare with AI only baseline using $P(y|x)$. Based on the toy example and my current understanding of the paper, the proposed solution cannot beat AI only baseline.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to construct optimal prediction sets under which experts can achieve the highest accuracy. The authors claim that human experts cannot attain maximum accuracy with the prediction sets generated by conformal predictors. To address this issue, the paper proposes an efficient greedy algorithm based on maximum marginal gain to find prediction sets that outperform those generated by conformal predictors. The paper offers two main theoretical contributions: the first proves that finding the optimal prediction set is an NP-hard problem, while the second demonstrates that the proposed method enables experts to achieve higher accuracy than conformal predictors. Empirical results further validate the effectiveness of the proposed approach.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-motivated and easy to follow.
    
2. The authors provide a theoretical analysis for their motivation and offer a theoretical guarantee for the superior performance of the proposed greedy algorithm.
    
3. The paper presents an extensive set of experiments, including both synthetic and real data.

Weaknesses:
1. Further validation on more realistic datasets, such as ImageNet and CIFAR100, could strengthen the main points of the paper.
    
2. The experiments lack comparison with other classical score functions, such as Regularized Adaptive Prediction Sets.

Limitations:
They are adequately discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper shows the conformal prediction set may not be the optimal set recommendation to humans if humans follow certain choice models. The authors then propose a greedy algorithm by modeling $P(y|x)$ and the choice model of humans assuming it follows MNL model. Authors compare the proposed method against the standard conformal prediction set under synthetic human experts and the proposed method has a slightly better performance compared to traditional conformal sets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors consider conformal prediction in the human-in-the-loop setting, which is an important problem. The first part of the paper shows the conformal prediction set may not be the best recommendation set for humans, which is easy to understand since most conformal sets arrange the set in a ranked order and we can play with the human choice models to create an example that conformal sets may not be the best recommendation set.

Weaknesses:
The problem setting is not realistic: The authors do not allow humans to select outside the conformal prediction set. However, in the setups of most empirical successes of human-AI collaboration with conformal prediction, this is allowed. Similarly, if the authors do not allow humans to select outside the conformal prediction set, humans' value is greatly reduced and the optimal thing to do may be just to use fully automated AI prediction and in all the toy examples the authors provided, kicking humans out of the loop is the optimal system (humans only make things worse). 

The theoretical analysis seems useless: I think the theoretical analysis is useless for two reasons: 1) while identifying the optimal set is NP-hard, in practice the metric we care about is $\mathbb{E} g(S|x)$, not identifying the optimal set. If an algorithm can get a good rate of convergence for this regret, then this problem is not hopeless, so I think authors need to show for all conformal prediction algorithms, what is the regret lower bound for $\mathbb{E} g(S|x)$; 2) while I can see that sometimes the label set can be large. In practice, the theoretical results may not be a big issue for many problems since most problems have small label set (binary or three classes). This negative results may not seem that severe as the authors presented in the paper. 

The solution is disconnected and not useful in human-AI collaboration: 1) The proposed solution does not enjoy the distributionally-free guarantee, which is the main reason why people use conformal prediction. I would expect authors to provide a conformal prediction algorithm that is human-centered, rather than directly switch lanes to traditional prediction methods. 2) The proposed solution requires $P(y|x)$ and the true human choice model, which is too strong to be realistic. If I know $P(y|x)$, why should I involve humans in the loop anymore (recall that authors can restrict humans only select from prediction set so humans are not necessary in the system). The optimal strategy would be directly use $P(y|x)$ to select actions. 

Baselines: For human-AI collaboration tasks, I expect to see the proposed solution is better than human working alone or AI working alone. The authors should compare with AI only baseline using $P(y|x)$. Based on the toy example and my current understanding of the paper, the proposed solution cannot beat AI only baseline.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper analyzes decision support systems based on prediction set algorithms. The authors show that: (i) the usage of conformal prediction techniques is generally sub-optimal in terms of accuracy; (ii) the problem of finding the optimal prediction sets under human assistance is NP-hard. Moreover, they provide (iii) a greedy algorithm that is guaranteed to find prediction sets that are better than those provided by conformal predictors. Experimental evaluation on synthetic and real data show the effectiveness of the considered approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main strengths of the paper are:

1. the actual paper contribution is well framed;
2. the theoretical analysis is sound;
3. the proposed algorithm improves over existing approaches.

Weaknesses:
I think this work is a good paper, without major weaknesses, as it provides solid theoretical insights.
The concerns I have are mainly due to typos/details missing. I will point out here these and a few remarks that might be considered for the final version of the paper.

1. It seems to me that Table 2 and Figure 3 are missing the BRUTE FORCE baseline.
2. regarding the style of the paper, I found lines 135-146 very dense. Maybe providing a more concrete example (e.g., what could 1,2,3 represent?) might help the reader getting through it.
3. In Algorihm 1, I think adding a comment to the pseudo-code (from lines 4 to 13) could be useful
4. regarding the limitation section (evaluation) a useful reference might be [Stutz et al., 2023], where the authors evaluate the possibility that human experts might not be approximating the true probability distribution 
5. the experimental analysis (on real data) could be enriched with other popular Learning-to-Defer datasets, such as Cifar10H or hatespeech.

[Stutz et al., 2023] - Stutz, David, Abhijit Guha Roy, Tatiana Matejovicova, Patricia Strachan, Ali Taylan Cemgil, and Arnaud Doucet. ""Conformal prediction under ambiguous ground truth."" Transactions on Machine Learning Research (2023).

Limitations:
The paper adequately discussed the limitations and societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors first show that conformal prediction sets may not lead to human decision optimality.  The authors then introduce a greedy algorithm to generate candidate prediction sets that improve human decisions regarding the accuracy metric.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors find the sub-optimality of conformal prediction sets on providing candidates for human decisions. Thereby, they propose a novel method to produce prediction sets that helps to improve human prediction.

Weaknesses:
* The presentation somewhere is unclear: 
  * Line 86. Please break the sentence properly.	
  * Line 40/43/48: It is unclear for readers when the authors mention “optimal” multiple times but delay its explicit definition later.
  * Line 197: It is confusing when the authors refer to the role of $a$. What is the value of $a$?

* The authors claim they propose an efficient algorithm. However, I am not sure which part is efficient. Are there any numerical metrics, e.g., running time, supporting this contribution? Additionally, how should we understand this restriction of “for a large class of non-conformity scores and expert models” in line 51?

* Line 90:  But you also miss the possibility outside the prediction set, especially when the prediction set is not that good. I think the authors need to discuss the exploitation-exploration dilemma.

* The authors use the scores related to softmax and APS. Other papers propose alternative scores like RAPS and SAPS. I think they should be included.

Limitations:
Please see the above sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to construct optimal prediction sets under which experts can achieve the highest accuracy. The authors claim that human experts cannot attain maximum accuracy with the prediction sets generated by conformal predictors. To address this issue, the paper proposes an efficient greedy algorithm based on maximum marginal gain to find prediction sets that outperform those generated by conformal predictors. The paper offers two main theoretical contributions: the first proves that finding the optimal prediction set is an NP-hard problem, while the second demonstrates that the proposed method enables experts to achieve higher accuracy than conformal predictors. Empirical results further validate the effectiveness of the proposed approach.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-motivated and easy to follow.
    
2. The authors provide a theoretical analysis for their motivation and offer a theoretical guarantee for the superior performance of the proposed greedy algorithm.
    
3. The paper presents an extensive set of experiments, including both synthetic and real data.

Weaknesses:
1. Further validation on more realistic datasets, such as ImageNet and CIFAR100, could strengthen the main points of the paper.
    
2. The experiments lack comparison with other classical score functions, such as Regularized Adaptive Prediction Sets.

Limitations:
They are adequately discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xse8QMGnyM;"REVIEW 
Summary:
Existing data in 3D human pose estimation are typically collected indoors with human actors. To address this scalability issue, the authors propose to synthesize 3D human pose data via an Osteo-kinematic model and introduce biochemical constraints for better physical plausibility. Additionally, to deal with the inherent ambiguity in single-view depth estimation, the authors introduce Binary Depth Coordinates to explicitly model the relative spatial relation between adjacent joints. Extensive experiments verify the effectiveness of the proposed approach.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Leveraging biomechanical prior knowledge to synthesize physically plausible human data is $\textbf{intuitive}$ and $\textbf{interesting}$.
2. Comprehensive experiments verify the effectiveness of the proposed data augmentation approach (BPG) and Binary Depth Coordinates (BDC). Specifically, BDC can be applied to different methods, e.g., image-based and lifting-based, showing superior generalization ability.

Weaknesses:
1. $\textbf{Repeated text}$: The first paragraph of Sec.2 appears to be a copy-paste from the abstract, which is highly discouraged.
2. $\textbf{Requirement of camera intrinsics}$: While BDC shows notable performance gains to baselines, solving depth requires camera intrinsics (principal point and focal length), typically not required by current 3D HPE methods. This requirement may introduce additional constraints for in-the-wild inference.

Limitations:
The authors have included the limitations in Sec.6. However, the requirement for camera intrinsics may also be considered a limitation and discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces two components aimed at addressing challenges in 3D human pose estimation, specifically in terms of scalability and generalization. The authors propose a Biomechanical Pose Generator (BPG), which incorporates biomechanical principles to generate plausible new 3D poses. They also introduce Binary Depth Coordinates (BDC), a component designed to mitigate the depth ambiguity encountered when lifting a 2D pose to 3D. The paper includes ablation studies to demonstrate the impact of each component, and compares these new approaches to existing pose augmentation methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper’s focus on addressing the challenge of limited datasets and enhancing the generalizability of the method is interesting and to the best of my knowledge the idea of biomechanical pose generator which does not rely on a source dataset is novel. Also, the authors’ attention to the depth ambiguity in 3D pose estimation from a single image adds a value to the field. The authors have conducted comprehensive experiments and ablation studies, which provide valuable insights into the effectiveness of the proposed components. The inclusion of cross-dataset evaluation is crucial, as it allows for a robust assessment of the Biomechanical Pose Generator (BPG) component’s effectiveness.

Weaknesses:
1- The paper is generally well-written, but some parts could be clearer. Including a figure to illustrate the entire system could significantly help reader comprehension. For example, a diagram showing the VPose (or any baseline) architecture and the integration of the BDC component might be more effective than a text-only description. Additionally, including some implementation details about the BDC component in the main paper could improve the flow of information.

2- There are some ambiguities in the experiment section that need clarification. When referring to the “source-dataset”, it would be helpful to specify whether this refers to the Human 3.6M dataset or the newly synthesized poses. Similarly, when discussing evaluations on 3DHP and 3DPW, it would be beneficial to mention the specific subset used, such as the test set.

3- There appears to be some confusion between Table 1 and the results in Figure 4 (left). While Table 1 shows improvements in the Human 3.6M results when adding new poses generated from BPG, Figure 4 (left) indicates that adding more data increases the MPJPE error (without integrating BDC). This seems contradictory and could benefit from further explanation.

4- Typos: There are a couple of typographical errors that need correction. On Line 177, (xi) is repeated twice instead of yi. On Line 287, BDC should be corrected to BPG.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a 3D human pose estimation framework that incorporates data augmentation and depth ordering information. The main contributions are two-fold: First, the proposed Biomechanical Pose Generator (BPG) generates plausible body poses based on kinematic constraints, which is used for data augmentation. Second, the Binary Depth Coordinates (BDC) disambiguate the projective depth of each joint by classifying whether the joints are positioned towards or away from the camera. The proposed framework achieved state-of-the-art performance in single-frame 3D human pose estimation settings.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The proposed method achieves state-of-the-art results in various 3D HPE datasets.
- The effect of data augmentation is validated in cross-domain learning settings.

Weaknesses:
My major concern lies on the novelty of the contribution.

- There are numerous research papers that regularize 3D human pose based on kinematic constraints. The authors did not clarify the distinctiveness of BPG from these conventional works, except for stating that BPG achieved better performance. An analysis showing how the proposed BPG generates more plausible poses compared to previous augmentation methods is required, either by displaying the generated poses or by showing qualitative estimation results.
- The concept of BDC is similar to [1] which learns ordinal depth information. The authors should cite the paper and discuss the difference.

The paper also contains	ambiguously explained parts or lacks details about their methods. Please refer to Questions section.

[1] G. Pavlakos et al., ""Ordinal depth supervision for 3d human pose estimation"", CVPR 2018

Limitations:
Suggestions
- In Fig. 4, it should be clearly stated what * means. It would be better to use GFpose+BDC instead of GFpose*.
- More detailed explanation about how $T$ in Eq (1) and ${d}_{m,n}$ in Eq (2) are formulated would clarify the methods.

Typos
- Line 185, by the projection from -> by back-projecting a ray from
- Line 212, duplicated sentences.
- Line 115, to peed -> of

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper address the task of 3D Human Pose Estimation from monocular RGB. The authors make two main contributions: The Biomechanical Pose Generator (BPG) and the Binary Depth Coordinates (BDC). BPG is a 3D human pose generator that leverages the ""Normal Range of Motion"" (NROM) that is used in the medical field to describe standard biomechanical limitations. With it, BPG is capable of generating biomechanically sound 3D human poses by randomly sampling joint angles and bones that lie within a certain ratio to each other.
BDC is a coordinate system that decompose a 3D pose into constituents. Specifically, it decomposes it into the 2D coordinate, bone length, a binary depth parameter indicating the closeness to the image plane as well as the 3D coordinates of the parent joint. This decomposition, so the authors claim, allows models to better deal with depth ambiguity. 
Experimental results demonstrate that the proposed approach achieves better performance over the compared related work on a variety of datasets (cf. Tbl 1-4). Ablative studies demonstrate that BDC helps keep performance steady even in the face of larger depth ambiguity (Tbl. 5) and that related work can benefit as well from switching to the proposed coordinates (Tbl 6.)

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors properly motivate and evaluate their approach. Depth ambiguity in monocular RGB is a challenging problem to address. I particularly liked Tbl. 5 that demonstrated that BDC is capable of handling even larger depth ambiguities.
- The paper was easy to digest and understand.
- One of the main strength of this paper is that BDC can be combined with other related work, yielding improvements (Tbl. 6)

Weaknesses:
- My biggest concern about the paper is that BDC is very similar conceptually to ""Hand Pose Estimation via Latent 2.5D Heatmap
Regression"", Iqbal et al., ECCV'18. Yet there is no mention of the paper, let alone any comparisons. The mentioned paper also addresses with depth ambiguity by decomposing the 3D pose into 2D pose and a root-relative depth vector. Addressing the differences, performing comparisons with this approach would better contextualize as well as strengthen the contribution of the paper.
- BPG shows to improve performance by improving the 2D to 3D lifting component. Yet, it's contribution is rather sparse, as it essentially amount to performing forward kinematics on bounded joint angle and bone lengths. It does not take into consideration statistics on poses. Certain poses are more common, due to them corresponding to actual human movement patterns (such as walking) that are affected by gravity. Randomly sampling poses without taking such statistics into consideration may generate a range of synthetic poses that are unrealistic, leading to non-optimal improvements.

Limitations:
The authors address limitations of their methods, such as not taking temporal dynamics into account.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Existing data in 3D human pose estimation are typically collected indoors with human actors. To address this scalability issue, the authors propose to synthesize 3D human pose data via an Osteo-kinematic model and introduce biochemical constraints for better physical plausibility. Additionally, to deal with the inherent ambiguity in single-view depth estimation, the authors introduce Binary Depth Coordinates to explicitly model the relative spatial relation between adjacent joints. Extensive experiments verify the effectiveness of the proposed approach.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Leveraging biomechanical prior knowledge to synthesize physically plausible human data is $\textbf{intuitive}$ and $\textbf{interesting}$.
2. Comprehensive experiments verify the effectiveness of the proposed data augmentation approach (BPG) and Binary Depth Coordinates (BDC). Specifically, BDC can be applied to different methods, e.g., image-based and lifting-based, showing superior generalization ability.

Weaknesses:
1. $\textbf{Repeated text}$: The first paragraph of Sec.2 appears to be a copy-paste from the abstract, which is highly discouraged.
2. $\textbf{Requirement of camera intrinsics}$: While BDC shows notable performance gains to baselines, solving depth requires camera intrinsics (principal point and focal length), typically not required by current 3D HPE methods. This requirement may introduce additional constraints for in-the-wild inference.

Limitations:
The authors have included the limitations in Sec.6. However, the requirement for camera intrinsics may also be considered a limitation and discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces two components aimed at addressing challenges in 3D human pose estimation, specifically in terms of scalability and generalization. The authors propose a Biomechanical Pose Generator (BPG), which incorporates biomechanical principles to generate plausible new 3D poses. They also introduce Binary Depth Coordinates (BDC), a component designed to mitigate the depth ambiguity encountered when lifting a 2D pose to 3D. The paper includes ablation studies to demonstrate the impact of each component, and compares these new approaches to existing pose augmentation methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper’s focus on addressing the challenge of limited datasets and enhancing the generalizability of the method is interesting and to the best of my knowledge the idea of biomechanical pose generator which does not rely on a source dataset is novel. Also, the authors’ attention to the depth ambiguity in 3D pose estimation from a single image adds a value to the field. The authors have conducted comprehensive experiments and ablation studies, which provide valuable insights into the effectiveness of the proposed components. The inclusion of cross-dataset evaluation is crucial, as it allows for a robust assessment of the Biomechanical Pose Generator (BPG) component’s effectiveness.

Weaknesses:
1- The paper is generally well-written, but some parts could be clearer. Including a figure to illustrate the entire system could significantly help reader comprehension. For example, a diagram showing the VPose (or any baseline) architecture and the integration of the BDC component might be more effective than a text-only description. Additionally, including some implementation details about the BDC component in the main paper could improve the flow of information.

2- There are some ambiguities in the experiment section that need clarification. When referring to the “source-dataset”, it would be helpful to specify whether this refers to the Human 3.6M dataset or the newly synthesized poses. Similarly, when discussing evaluations on 3DHP and 3DPW, it would be beneficial to mention the specific subset used, such as the test set.

3- There appears to be some confusion between Table 1 and the results in Figure 4 (left). While Table 1 shows improvements in the Human 3.6M results when adding new poses generated from BPG, Figure 4 (left) indicates that adding more data increases the MPJPE error (without integrating BDC). This seems contradictory and could benefit from further explanation.

4- Typos: There are a couple of typographical errors that need correction. On Line 177, (xi) is repeated twice instead of yi. On Line 287, BDC should be corrected to BPG.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a 3D human pose estimation framework that incorporates data augmentation and depth ordering information. The main contributions are two-fold: First, the proposed Biomechanical Pose Generator (BPG) generates plausible body poses based on kinematic constraints, which is used for data augmentation. Second, the Binary Depth Coordinates (BDC) disambiguate the projective depth of each joint by classifying whether the joints are positioned towards or away from the camera. The proposed framework achieved state-of-the-art performance in single-frame 3D human pose estimation settings.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The proposed method achieves state-of-the-art results in various 3D HPE datasets.
- The effect of data augmentation is validated in cross-domain learning settings.

Weaknesses:
My major concern lies on the novelty of the contribution.

- There are numerous research papers that regularize 3D human pose based on kinematic constraints. The authors did not clarify the distinctiveness of BPG from these conventional works, except for stating that BPG achieved better performance. An analysis showing how the proposed BPG generates more plausible poses compared to previous augmentation methods is required, either by displaying the generated poses or by showing qualitative estimation results.
- The concept of BDC is similar to [1] which learns ordinal depth information. The authors should cite the paper and discuss the difference.

The paper also contains	ambiguously explained parts or lacks details about their methods. Please refer to Questions section.

[1] G. Pavlakos et al., ""Ordinal depth supervision for 3d human pose estimation"", CVPR 2018

Limitations:
Suggestions
- In Fig. 4, it should be clearly stated what * means. It would be better to use GFpose+BDC instead of GFpose*.
- More detailed explanation about how $T$ in Eq (1) and ${d}_{m,n}$ in Eq (2) are formulated would clarify the methods.

Typos
- Line 185, by the projection from -> by back-projecting a ray from
- Line 212, duplicated sentences.
- Line 115, to peed -> of

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper address the task of 3D Human Pose Estimation from monocular RGB. The authors make two main contributions: The Biomechanical Pose Generator (BPG) and the Binary Depth Coordinates (BDC). BPG is a 3D human pose generator that leverages the ""Normal Range of Motion"" (NROM) that is used in the medical field to describe standard biomechanical limitations. With it, BPG is capable of generating biomechanically sound 3D human poses by randomly sampling joint angles and bones that lie within a certain ratio to each other.
BDC is a coordinate system that decompose a 3D pose into constituents. Specifically, it decomposes it into the 2D coordinate, bone length, a binary depth parameter indicating the closeness to the image plane as well as the 3D coordinates of the parent joint. This decomposition, so the authors claim, allows models to better deal with depth ambiguity. 
Experimental results demonstrate that the proposed approach achieves better performance over the compared related work on a variety of datasets (cf. Tbl 1-4). Ablative studies demonstrate that BDC helps keep performance steady even in the face of larger depth ambiguity (Tbl. 5) and that related work can benefit as well from switching to the proposed coordinates (Tbl 6.)

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors properly motivate and evaluate their approach. Depth ambiguity in monocular RGB is a challenging problem to address. I particularly liked Tbl. 5 that demonstrated that BDC is capable of handling even larger depth ambiguities.
- The paper was easy to digest and understand.
- One of the main strength of this paper is that BDC can be combined with other related work, yielding improvements (Tbl. 6)

Weaknesses:
- My biggest concern about the paper is that BDC is very similar conceptually to ""Hand Pose Estimation via Latent 2.5D Heatmap
Regression"", Iqbal et al., ECCV'18. Yet there is no mention of the paper, let alone any comparisons. The mentioned paper also addresses with depth ambiguity by decomposing the 3D pose into 2D pose and a root-relative depth vector. Addressing the differences, performing comparisons with this approach would better contextualize as well as strengthen the contribution of the paper.
- BPG shows to improve performance by improving the 2D to 3D lifting component. Yet, it's contribution is rather sparse, as it essentially amount to performing forward kinematics on bounded joint angle and bone lengths. It does not take into consideration statistics on poses. Certain poses are more common, due to them corresponding to actual human movement patterns (such as walking) that are affected by gravity. Randomly sampling poses without taking such statistics into consideration may generate a range of synthetic poses that are unrealistic, leading to non-optimal improvements.

Limitations:
The authors address limitations of their methods, such as not taking temporal dynamics into account.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xrbgXJomJp;"REVIEW 
Summary:
This paper extends the IQ-Learn method to cooperative multi-agent settings. The main insight is to use mixing networks to enable centralized training via decentralized Q functions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is quite relevant to NeurIPS and it is indeed important to extend IQ-Learn (or similar inverse learning algorithms) to multi-agent systems.

Weaknesses:
- The major concern that I have is that, if my understanding is correct, the paper assumes access to the global state information. This is not realistic. In real application, this will never be the case. So the algorithm does not seem useful in practice.
- Typo: In line 62, it should be ""generalization"" instead of ""generation"",
- In line 72, \citet should be used instead of \cite or \citep so that the author names will become a part of the sentence.
- In line 162, \eqref should be used instead of \ref so that the parenthesis will appear around the equation number.
- The architecture figure is in page 7. It would significantly increase the readability if it came earlier. 
- By the time the reader reads line 191, the IGC principle is still undefined. This makes reading very difficult.
- The same thing is true at line 203, too.
- Typo: In line 241, it should be ""makes"" instead of ""make"".
- Typo: In line 242, it should be ""yields"" instead of ""yield"".

Limitations:
The paper does not discuss the broader impacts. I disagree that there is no potential societal impact. I invite the authors to think about the applications their algorithm may have and then consider how their algorithm would affect those applications (both positively and negatively).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the problem of extending a single-agent imitation learning algorithm, inverse soft-Q learning (IQ-learn, Garg et al. Neurips 21) to the multi-agent cooperative setting. The proposed algorithm, MIFQ, leverages the ideas of mixing networks and the individual-global-max (IGM) principle, to perform the extension. Experimental evaluations of MIFQ are conducted on SMAC-v2, MPE, and Gold Miner, and demonstrate that MIFQ improves over baselines across various domains and with varying numbers of demonstrations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses the challenge of generalizing a key imitation learning (IL) algorithm from single-agent to multi-agent settings, offering a novel approach with MIFQ. The problem is clearly specified and represents an important contribution to the MARL literature. 

The empirical results are robust:
- MIFQ outperforms most baselines with various demonstrations.
- Extensive experiments across multiple domains and tasks confirm MIFQ's superior performance.
- Comprehensive comparisons with baselines (BC, independent IQ learning, alternative Q-factorization methods, etc.) highlight MIFQ's advantages.

Weaknesses:
1. Some aspects of the method do not seem fully justified to me: 
    - The authors claim in lines 143-148 that a shortcoming of the IQ learn method is that the objective depends on the centralized state and joint action. However, Section 5.4 of the IQ Learn paper presents a state-only objective (independent from the actions). I wonder if the authors could discuss whether a simple state-only extension of IQ Learn, where critic depends on the centralized state as usual, but the actor depends on the observations, would be sufficient to sidestep many of the concerns addressed by IQ Learn? 
    - The authors also claim in Section 4.1.2 that the straightforward Independent Inverse Q-learning is not a satisfactory solution because the method ""…has limitations in addressing the interdependence between agents and the global information available during the training process."". Can the authors more explicitly discuss what the shortcomings of an independent version of IQ-learn is not satisfactory? Does it suffer from convergence problems? 
					
2. The current experimental analysis is somewhat shallow, and essentially amounts to a description of the plots. The authors could improve the analysis of MIFQ by considering the following additional questions: 
    - The original IQ learn paper plots the rewards to validate that their method recovers the ground truth reward. Can the same be done here? 
    - Why does MIFQ perform worse than BC on MPE, particularly the reference and spread tasks?
3. There are some issues with how the experimental results have been reported. 
    - What is the number of trials for each of the results? Please include this in the main paper. 
    - The caption of Figure 2 is missing key information to understand the figure. What is the number of demonstrations used to train each of the methods? What does the shaded region mean? Based on the std devs reported in the Appendix, I assume it is the standard deviation; please see the note below and instead compute 95% confidence intervals. 
    - No measurements of uncertainty are provided in Table 2, and standard deviations are provided only in the Appendix. Standard deviations reflect the underlying variance in models learned by the algorithm, rather than providing a measure of statistical significance. Please also compute 95% confidence intervals to enable readers to judge the statistical significance of the gaps in mean test returns -- ideally, bootstrapped confidence intervals. See this paper for a reference on best practices:  https://arxiv.org/abs/2304.01315
3. There are also some minor clarity issues: 
    - IGC is used in line 192, but is only explained in the following Section 4.2.2
    - Definition 4.2 - this definition is not specific enough to be useful. It handwaves  by only requiring that the joint policy be 'equivalent' to the collection of individual optimal policies. Equivalent in what sense?

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel algorithm, Multi-agent Inverse Factorized Q-learning (MIFQ), for cooperative multi-agent imitation learning (IL). It extends the inverse soft-Q learning framework to multi-agent settings by introducing a mixing network architecture for centralized training with decentralized execution. This enables learning local and joint value functions effectively. The authors conducted extensive experiments across multiple challenging environments, demonstrating that their approach outperforms existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-  The introduction of a multi-agent extension of inverse soft-Q learning using factorized networks is a significant and novel contribution to the field of IL.  
- This paper is well-written and organized, and provides a sound theoretical analysis.
- The empirical results across three different environments, including a complex version of the StarCraft multi-agent challenge, are impressive. The proposed method outperforms existing baselines.

Weaknesses:
As someone who is not an expert in the field of imitation learning, I perceive no significant weaknesses in this paper from my perspective.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the imitation problem in cooperative Multi-Agent Reinforcement Learning (MARL). It extends inverse soft-Q learning to the multi-agent domain by leveraging value factorizations under the Centralized Training with Decentralized Execution (CTDE) paradigm. Experimental results demonstrate the effectiveness of the proposed approach across several environments.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The study of imitation learning in MARL is a valuable and relevant research problem, and the paper provides promising solutions.
- The experimental results are robust and convincingly support the proposed method's effectiveness.

Weaknesses:
- The paper's organization could be improved. The current structure alternates between theory and architecture without a clear flow.

- The similarity between IGC and IGO[1] requires further clarification.

- The objective function (6) introduces sub-optimality compared to the original objective (3) due to the restriction that $Q^{tot}$ and $V^{tot}$ must be monotonic. Additionally, since $Q^{tot}$ and $V^{tot}$ use different mixing networks, the relationship between them violates Equation (2). This indicates that Equation (6) does not represent the same objective as Equation (3), even without considering the sub-optimality introduced by factorization. These issues need further theoretical exploration and discussion.

- Although the experimental results are promising, the superior performance seems to stem from the QMIX algorithm's advantage over other MARL algorithms. An important missing baseline is the soft actor-critic version of IQ-Learn, which uses a centralized Q function with decentralized critics and does not seem to violate the original objective.

[1] Zhang, et al., FOP: Factorizing Optimal Joint Policy of Maximum-Entropy Multi-Agent Reinforcement Learning, ICML 2021.

Limitations:
Limitations are discussed in conclusion section.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper extends the IQ-Learn method to cooperative multi-agent settings. The main insight is to use mixing networks to enable centralized training via decentralized Q functions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is quite relevant to NeurIPS and it is indeed important to extend IQ-Learn (or similar inverse learning algorithms) to multi-agent systems.

Weaknesses:
- The major concern that I have is that, if my understanding is correct, the paper assumes access to the global state information. This is not realistic. In real application, this will never be the case. So the algorithm does not seem useful in practice.
- Typo: In line 62, it should be ""generalization"" instead of ""generation"",
- In line 72, \citet should be used instead of \cite or \citep so that the author names will become a part of the sentence.
- In line 162, \eqref should be used instead of \ref so that the parenthesis will appear around the equation number.
- The architecture figure is in page 7. It would significantly increase the readability if it came earlier. 
- By the time the reader reads line 191, the IGC principle is still undefined. This makes reading very difficult.
- The same thing is true at line 203, too.
- Typo: In line 241, it should be ""makes"" instead of ""make"".
- Typo: In line 242, it should be ""yields"" instead of ""yield"".

Limitations:
The paper does not discuss the broader impacts. I disagree that there is no potential societal impact. I invite the authors to think about the applications their algorithm may have and then consider how their algorithm would affect those applications (both positively and negatively).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the problem of extending a single-agent imitation learning algorithm, inverse soft-Q learning (IQ-learn, Garg et al. Neurips 21) to the multi-agent cooperative setting. The proposed algorithm, MIFQ, leverages the ideas of mixing networks and the individual-global-max (IGM) principle, to perform the extension. Experimental evaluations of MIFQ are conducted on SMAC-v2, MPE, and Gold Miner, and demonstrate that MIFQ improves over baselines across various domains and with varying numbers of demonstrations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses the challenge of generalizing a key imitation learning (IL) algorithm from single-agent to multi-agent settings, offering a novel approach with MIFQ. The problem is clearly specified and represents an important contribution to the MARL literature. 

The empirical results are robust:
- MIFQ outperforms most baselines with various demonstrations.
- Extensive experiments across multiple domains and tasks confirm MIFQ's superior performance.
- Comprehensive comparisons with baselines (BC, independent IQ learning, alternative Q-factorization methods, etc.) highlight MIFQ's advantages.

Weaknesses:
1. Some aspects of the method do not seem fully justified to me: 
    - The authors claim in lines 143-148 that a shortcoming of the IQ learn method is that the objective depends on the centralized state and joint action. However, Section 5.4 of the IQ Learn paper presents a state-only objective (independent from the actions). I wonder if the authors could discuss whether a simple state-only extension of IQ Learn, where critic depends on the centralized state as usual, but the actor depends on the observations, would be sufficient to sidestep many of the concerns addressed by IQ Learn? 
    - The authors also claim in Section 4.1.2 that the straightforward Independent Inverse Q-learning is not a satisfactory solution because the method ""…has limitations in addressing the interdependence between agents and the global information available during the training process."". Can the authors more explicitly discuss what the shortcomings of an independent version of IQ-learn is not satisfactory? Does it suffer from convergence problems? 
					
2. The current experimental analysis is somewhat shallow, and essentially amounts to a description of the plots. The authors could improve the analysis of MIFQ by considering the following additional questions: 
    - The original IQ learn paper plots the rewards to validate that their method recovers the ground truth reward. Can the same be done here? 
    - Why does MIFQ perform worse than BC on MPE, particularly the reference and spread tasks?
3. There are some issues with how the experimental results have been reported. 
    - What is the number of trials for each of the results? Please include this in the main paper. 
    - The caption of Figure 2 is missing key information to understand the figure. What is the number of demonstrations used to train each of the methods? What does the shaded region mean? Based on the std devs reported in the Appendix, I assume it is the standard deviation; please see the note below and instead compute 95% confidence intervals. 
    - No measurements of uncertainty are provided in Table 2, and standard deviations are provided only in the Appendix. Standard deviations reflect the underlying variance in models learned by the algorithm, rather than providing a measure of statistical significance. Please also compute 95% confidence intervals to enable readers to judge the statistical significance of the gaps in mean test returns -- ideally, bootstrapped confidence intervals. See this paper for a reference on best practices:  https://arxiv.org/abs/2304.01315
3. There are also some minor clarity issues: 
    - IGC is used in line 192, but is only explained in the following Section 4.2.2
    - Definition 4.2 - this definition is not specific enough to be useful. It handwaves  by only requiring that the joint policy be 'equivalent' to the collection of individual optimal policies. Equivalent in what sense?

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel algorithm, Multi-agent Inverse Factorized Q-learning (MIFQ), for cooperative multi-agent imitation learning (IL). It extends the inverse soft-Q learning framework to multi-agent settings by introducing a mixing network architecture for centralized training with decentralized execution. This enables learning local and joint value functions effectively. The authors conducted extensive experiments across multiple challenging environments, demonstrating that their approach outperforms existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-  The introduction of a multi-agent extension of inverse soft-Q learning using factorized networks is a significant and novel contribution to the field of IL.  
- This paper is well-written and organized, and provides a sound theoretical analysis.
- The empirical results across three different environments, including a complex version of the StarCraft multi-agent challenge, are impressive. The proposed method outperforms existing baselines.

Weaknesses:
As someone who is not an expert in the field of imitation learning, I perceive no significant weaknesses in this paper from my perspective.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the imitation problem in cooperative Multi-Agent Reinforcement Learning (MARL). It extends inverse soft-Q learning to the multi-agent domain by leveraging value factorizations under the Centralized Training with Decentralized Execution (CTDE) paradigm. Experimental results demonstrate the effectiveness of the proposed approach across several environments.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The study of imitation learning in MARL is a valuable and relevant research problem, and the paper provides promising solutions.
- The experimental results are robust and convincingly support the proposed method's effectiveness.

Weaknesses:
- The paper's organization could be improved. The current structure alternates between theory and architecture without a clear flow.

- The similarity between IGC and IGO[1] requires further clarification.

- The objective function (6) introduces sub-optimality compared to the original objective (3) due to the restriction that $Q^{tot}$ and $V^{tot}$ must be monotonic. Additionally, since $Q^{tot}$ and $V^{tot}$ use different mixing networks, the relationship between them violates Equation (2). This indicates that Equation (6) does not represent the same objective as Equation (3), even without considering the sub-optimality introduced by factorization. These issues need further theoretical exploration and discussion.

- Although the experimental results are promising, the superior performance seems to stem from the QMIX algorithm's advantage over other MARL algorithms. An important missing baseline is the soft actor-critic version of IQ-Learn, which uses a centralized Q function with decentralized critics and does not seem to violate the original objective.

[1] Zhang, et al., FOP: Factorizing Optimal Joint Policy of Maximum-Entropy Multi-Agent Reinforcement Learning, ICML 2021.

Limitations:
Limitations are discussed in conclusion section.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xqrlhsbcwN;"REVIEW 
Summary:
The paper proposes a novel training framework for regression tasks called the Approximated Orthogonal Projection Unit (AOPU), optimized using truncated natural gradients. The authors utilize the Rank Rate (RR) of the augmented data covariance matrix as a metric. They demonstrate that their method offers more stable training than existing architectures and optimizers, which is crucial for industrial applications requiring online training during production. Additionally, the authors provide a comprehensive analysis of their setup's convergence.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Detailed introduction on the background and intuition.
2. The method is very simple.
3. A thorough theoretical analysis of the method was provided.

Weaknesses:
1. Poorly arranged paper; conclusions are at the end of the appendix.
2. In the introduction, the authors claim that their methods improve interpretability, but they do not explain later why that matters. Also, many existing works explain the behavior at a neuron level; it is not clear why one has to track the parameter itself.
3. Experimental qualities are not good; no hyperparameter search is mentioned in the paper, which is essential when the authors claim that their method improves training stability.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the Approximated Orthogonal Projection Unit, the basis for a new neural network, designed to enhance the stability and interpretability of regression models, particularly in industrial soft sensor applications. The primary aim is to address the need for stable and immediate optimization in online settings, where traditional NN training techniques fall short. The paper introduces the theoretical background and demonstrates the effectiveness on two tasks, while also introducing ablations and comparisons to several other techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method appears novel and straightforward.
- The paper provides a solid theoretical foundation.
- The paper imrpoves interpretability of the neural network's behavior and training dynamics by differentiatiating between trackable and untrackable parameters, enhancing the interpretability.
- The authors demonstrate superior performance of AOPU in experiments with two chemical process datasets, showcasing its practical effectiveness in achieving stable convergence compared to existing models.
- Practical Relevance: Tailors the AOPU framework specifically for industrial soft sensor applications, addressing the need for immediate optimization and stability in online settings.
- Limitations, such as numerical stability issues during matrix inversion in the training process, are discussed.

Weaknesses:
- Code not published. The justification provided is somewhat questionable, since easy reproducibility should also enable the authors to provide code (possibly mirrored from the code implemented at the company).
- While the page limit is formally met, the authors make extensive use of the Appendix, including core elements of the paper. The Conclusion and Limitations, for example, are in the appendix.
- There is no mention of thorough hyperparametertuning and its results.

Limitations:
The limitations have been addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a new model for soft sensor tasks, the Approximated Orthogonal Projection Unit (AOPU), to enhance the stability and interpretability of regression networks. AOPU incorporates trackable and dual parameters, which are treated differently during the inference and training processes.  AOPU truncates the gradient backpropagation at dual parameters, optimizes the trackable parameters updates, and enhances the robustness of training. The paper provides theoretical proof that AOPU is an approximation of both MVE and Natural Gradient Descent (NGD).Experimental results on two chemical process datasets demonstrat that AOPU outperforms other models in achieving stable convergence.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method is novel and has a strong theoretical basis. The authors provide detailed proofs of theorems in the appendix.
2. If the contents in the appendix are considered, this paper analyses the proposed AOPU from many aspects, and provide sufficient experimental results and ablation study to validate the advantage of AOPU.

Weaknesses:
1. Due to the limitation of paper length, the contents in the formal paper is incomplete. Many important content like quantitative analysis and ablation study are put in the appendix. The formal contents also lacks a conclusion section. For the quality of publishing, I suggest submitting the paper to other platforms like a IEEE Transaction, where the paper length can be longer.

2. The proposed method are not incorporated into DNN structures, therefore its expressive power is limited in more complicated tasks. Considering the requirements of industrial soft sensor tasks, this is not a critical flaw, but it still hinders AOPU from challenging AI applications.

Limitations:
The limitations of AOPU is discussed in the paper, specifically, the need for an understanding of the RR distribution to guide the selection of hyperparameters. I do not see potential negative societal impact of the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel training framework for regression tasks called the Approximated Orthogonal Projection Unit (AOPU), optimized using truncated natural gradients. The authors utilize the Rank Rate (RR) of the augmented data covariance matrix as a metric. They demonstrate that their method offers more stable training than existing architectures and optimizers, which is crucial for industrial applications requiring online training during production. Additionally, the authors provide a comprehensive analysis of their setup's convergence.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Detailed introduction on the background and intuition.
2. The method is very simple.
3. A thorough theoretical analysis of the method was provided.

Weaknesses:
1. Poorly arranged paper; conclusions are at the end of the appendix.
2. In the introduction, the authors claim that their methods improve interpretability, but they do not explain later why that matters. Also, many existing works explain the behavior at a neuron level; it is not clear why one has to track the parameter itself.
3. Experimental qualities are not good; no hyperparameter search is mentioned in the paper, which is essential when the authors claim that their method improves training stability.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the Approximated Orthogonal Projection Unit, the basis for a new neural network, designed to enhance the stability and interpretability of regression models, particularly in industrial soft sensor applications. The primary aim is to address the need for stable and immediate optimization in online settings, where traditional NN training techniques fall short. The paper introduces the theoretical background and demonstrates the effectiveness on two tasks, while also introducing ablations and comparisons to several other techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method appears novel and straightforward.
- The paper provides a solid theoretical foundation.
- The paper imrpoves interpretability of the neural network's behavior and training dynamics by differentiatiating between trackable and untrackable parameters, enhancing the interpretability.
- The authors demonstrate superior performance of AOPU in experiments with two chemical process datasets, showcasing its practical effectiveness in achieving stable convergence compared to existing models.
- Practical Relevance: Tailors the AOPU framework specifically for industrial soft sensor applications, addressing the need for immediate optimization and stability in online settings.
- Limitations, such as numerical stability issues during matrix inversion in the training process, are discussed.

Weaknesses:
- Code not published. The justification provided is somewhat questionable, since easy reproducibility should also enable the authors to provide code (possibly mirrored from the code implemented at the company).
- While the page limit is formally met, the authors make extensive use of the Appendix, including core elements of the paper. The Conclusion and Limitations, for example, are in the appendix.
- There is no mention of thorough hyperparametertuning and its results.

Limitations:
The limitations have been addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a new model for soft sensor tasks, the Approximated Orthogonal Projection Unit (AOPU), to enhance the stability and interpretability of regression networks. AOPU incorporates trackable and dual parameters, which are treated differently during the inference and training processes.  AOPU truncates the gradient backpropagation at dual parameters, optimizes the trackable parameters updates, and enhances the robustness of training. The paper provides theoretical proof that AOPU is an approximation of both MVE and Natural Gradient Descent (NGD).Experimental results on two chemical process datasets demonstrat that AOPU outperforms other models in achieving stable convergence.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method is novel and has a strong theoretical basis. The authors provide detailed proofs of theorems in the appendix.
2. If the contents in the appendix are considered, this paper analyses the proposed AOPU from many aspects, and provide sufficient experimental results and ablation study to validate the advantage of AOPU.

Weaknesses:
1. Due to the limitation of paper length, the contents in the formal paper is incomplete. Many important content like quantitative analysis and ablation study are put in the appendix. The formal contents also lacks a conclusion section. For the quality of publishing, I suggest submitting the paper to other platforms like a IEEE Transaction, where the paper length can be longer.

2. The proposed method are not incorporated into DNN structures, therefore its expressive power is limited in more complicated tasks. Considering the requirements of industrial soft sensor tasks, this is not a critical flaw, but it still hinders AOPU from challenging AI applications.

Limitations:
The limitations of AOPU is discussed in the paper, specifically, the need for an understanding of the RR distribution to guide the selection of hyperparameters. I do not see potential negative societal impact of the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xpRUi8amtC;"REVIEW 
Summary:
This paper proposes SDSGG, a novel open vocabulary scene graph generation(OVSGG) algorithm that leverages the reasoning capability of a LLM to better determine the relations between objects in the scene. It achieves this goal by first prompting a LLM with multiple persona prompts to expand a simple relational predicative to a list of detailed visual descriptions, which are subsequently used to augment the classification process.  It also introduces a novel mutual visual adapter, which better captures the interaction between subjects and objects.  Experiments show that these proposed designs are effective.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Incorporating a LLM to augment the predicate labels for scene graph generation is a novel idea.  This paper provides meaningful insight to future works in this area.
2. The experiment results (table 1-2) are strong, significantly outperforming previous methods.
3. The authors conducted extensive ablation studies on various design elements.

Weaknesses:
1. Prompting the LLM is a key element of the method, however some crucial details are missing. For example, how are the prompts constructed? While the author provided the prompt in Appendix Fig 5, it is unclear how the  ""{scene content to be discussed}"" is generated. The author did show some examples throughout the paper, but they are not sufficient for the reader to understand the underlying process. In particular, in L167, the author showed example #1 ""Imagine there is an animal that is eating, "". In Fig 1c, there is example #2 ""Assuming that the scene has a man riding a horse.""  These two descriptions have two different granularity, as one only includes the generic concept of ""an animal that is eating"" while the other has specific class names ""man"" and ""horse"". The authors should clearly describe what information is included into the prompt, and discuss the scalability and cost of generating such prompts. I suppose if the prompts are like example #1, they can be generated offline based on predicative label sets. However, if the prompts are like example #2, they need to be generated for every possible triple of (subject, predicative, object) over the label space, or be generated online over possible objects in a scene. It is unclear which is the case.

2. Additional discussions and experiments are required to justify some of the design choices. For example,

  2.1 in eq 8, the loss of descriptions marked by possible coexistence is to make the prediction ""close to those of CLIP."" (L255). If this is the case, why not directly use CLIP results for these possible coexistence descriptions at inference time (eq 2)?

  2.2 some discussion is needed on if CLIP is good at classifying the generated descriptions. What are the nature of these descriptions and do they fit well with CLIP's pretraining pipeline (i.e. object-level image caption)? As a concrete example, can CLIP properly distinguish descriptions involving counting, such as ""with four legs"", and ""with two legs"", mentioned in the examples?

 2.3 what happens if we discard ""possible coexistence"" descriptions and only use definite coexistence and contradiction? Table8 shows that it is ideal to have a low weight for ""possible coexistence""  loss. What happens if we set the weight to 0 and remove it at inference pipeline?

Limitations:
The authors discussed limitations and societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to solve the open-vocabulary scene graph generation problem. Previous methods mainly adopt scene-agnostic prompts as text classifiers. The authors argue that using the fixed text classifiers not only struggles to model visual relations with high variance, but also falls short in adapting to distinct contexts. Therefore, the authors propose the scene-specific description based OVSGG framework. They employ an LLM and ask it to play different roles. Besides, they design the mutual visual adapter to encode visual features. Extensive experiments show that the proposed method significantly outperforms top-leading methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The motivation and idea of this paper are innovative and interesting. Simply applying LLM to SGG cannot effectively reason the relationships. The authors consider employing the context and introducing multiple roles of LLM, which is shown to be effective for solving the OVSGG problem.

Besides, the experiments are convincing. Plenty of ablation studies are provided.

Weaknesses:
My main concern is Computational Complexity: The proposed framework involves multiple stages, including generating descriptions, renormalizing them, and applying mutual visual adapters. This multi-step process could be computationally intensive, making it less practical for real-time applications or scenarios with limited computational resources.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper starts by discussing methods for Open-vocabulary Scene Graph Generation (OVSGG) based on the CLIP model, highlighting the issue that current OVSGG methods do not differentiate between various scenes, which limits their effectiveness. The authors introduce SDSGG, a scene-specific description-based OVSGG framework that improves both the textual and visual parts, enhancing the model's open-vocabulary relationship prediction capabilities.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The novelty of this paper lies in its analysis of the issues present in current OVSGG methods, leading to the conclusion that differentiating between scenes is necessary to enhance the performance of OVSGG. The proposed Scene-specific Descriptions are particularly insightful.
2. The paper validates its findings on two datasets, VG and GQA, with experimental results showing significant performance improvements over previous state-of-the-art methods.

Weaknesses:
1. The description in Sec3.1, Scene-specific Text Classifiers, of the paper is somewhat confusing. This confusion arises primarily because the text section includes multiple different naming conventions and several distinct modules. It is recommended that this section be rewritten to make it easier for readers to understand. Additionally, the terminology used in this section is inconsistent with that in lines 64~77, leading to comprehension difficulties.
2. For the OVSGG method, it is suggested to also train the model on a full set of relations and compare its performance with conventional SGG methods to ensure that it achieves good performance under standard settings.
3. Is the model robust to different base/novel splits? It is recommended to train and test the model on different base/novel dataset divisions to assess its robustness.
4. It is advised to train and test the model on the PSG dataset as well.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes SDSGG, a novel open vocabulary scene graph generation(OVSGG) algorithm that leverages the reasoning capability of a LLM to better determine the relations between objects in the scene. It achieves this goal by first prompting a LLM with multiple persona prompts to expand a simple relational predicative to a list of detailed visual descriptions, which are subsequently used to augment the classification process.  It also introduces a novel mutual visual adapter, which better captures the interaction between subjects and objects.  Experiments show that these proposed designs are effective.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Incorporating a LLM to augment the predicate labels for scene graph generation is a novel idea.  This paper provides meaningful insight to future works in this area.
2. The experiment results (table 1-2) are strong, significantly outperforming previous methods.
3. The authors conducted extensive ablation studies on various design elements.

Weaknesses:
1. Prompting the LLM is a key element of the method, however some crucial details are missing. For example, how are the prompts constructed? While the author provided the prompt in Appendix Fig 5, it is unclear how the  ""{scene content to be discussed}"" is generated. The author did show some examples throughout the paper, but they are not sufficient for the reader to understand the underlying process. In particular, in L167, the author showed example #1 ""Imagine there is an animal that is eating, "". In Fig 1c, there is example #2 ""Assuming that the scene has a man riding a horse.""  These two descriptions have two different granularity, as one only includes the generic concept of ""an animal that is eating"" while the other has specific class names ""man"" and ""horse"". The authors should clearly describe what information is included into the prompt, and discuss the scalability and cost of generating such prompts. I suppose if the prompts are like example #1, they can be generated offline based on predicative label sets. However, if the prompts are like example #2, they need to be generated for every possible triple of (subject, predicative, object) over the label space, or be generated online over possible objects in a scene. It is unclear which is the case.

2. Additional discussions and experiments are required to justify some of the design choices. For example,

  2.1 in eq 8, the loss of descriptions marked by possible coexistence is to make the prediction ""close to those of CLIP."" (L255). If this is the case, why not directly use CLIP results for these possible coexistence descriptions at inference time (eq 2)?

  2.2 some discussion is needed on if CLIP is good at classifying the generated descriptions. What are the nature of these descriptions and do they fit well with CLIP's pretraining pipeline (i.e. object-level image caption)? As a concrete example, can CLIP properly distinguish descriptions involving counting, such as ""with four legs"", and ""with two legs"", mentioned in the examples?

 2.3 what happens if we discard ""possible coexistence"" descriptions and only use definite coexistence and contradiction? Table8 shows that it is ideal to have a low weight for ""possible coexistence""  loss. What happens if we set the weight to 0 and remove it at inference pipeline?

Limitations:
The authors discussed limitations and societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to solve the open-vocabulary scene graph generation problem. Previous methods mainly adopt scene-agnostic prompts as text classifiers. The authors argue that using the fixed text classifiers not only struggles to model visual relations with high variance, but also falls short in adapting to distinct contexts. Therefore, the authors propose the scene-specific description based OVSGG framework. They employ an LLM and ask it to play different roles. Besides, they design the mutual visual adapter to encode visual features. Extensive experiments show that the proposed method significantly outperforms top-leading methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The motivation and idea of this paper are innovative and interesting. Simply applying LLM to SGG cannot effectively reason the relationships. The authors consider employing the context and introducing multiple roles of LLM, which is shown to be effective for solving the OVSGG problem.

Besides, the experiments are convincing. Plenty of ablation studies are provided.

Weaknesses:
My main concern is Computational Complexity: The proposed framework involves multiple stages, including generating descriptions, renormalizing them, and applying mutual visual adapters. This multi-step process could be computationally intensive, making it less practical for real-time applications or scenarios with limited computational resources.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper starts by discussing methods for Open-vocabulary Scene Graph Generation (OVSGG) based on the CLIP model, highlighting the issue that current OVSGG methods do not differentiate between various scenes, which limits their effectiveness. The authors introduce SDSGG, a scene-specific description-based OVSGG framework that improves both the textual and visual parts, enhancing the model's open-vocabulary relationship prediction capabilities.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The novelty of this paper lies in its analysis of the issues present in current OVSGG methods, leading to the conclusion that differentiating between scenes is necessary to enhance the performance of OVSGG. The proposed Scene-specific Descriptions are particularly insightful.
2. The paper validates its findings on two datasets, VG and GQA, with experimental results showing significant performance improvements over previous state-of-the-art methods.

Weaknesses:
1. The description in Sec3.1, Scene-specific Text Classifiers, of the paper is somewhat confusing. This confusion arises primarily because the text section includes multiple different naming conventions and several distinct modules. It is recommended that this section be rewritten to make it easier for readers to understand. Additionally, the terminology used in this section is inconsistent with that in lines 64~77, leading to comprehension difficulties.
2. For the OVSGG method, it is suggested to also train the model on a full set of relations and compare its performance with conventional SGG methods to ensure that it achieves good performance under standard settings.
3. Is the model robust to different base/novel splits? It is recommended to train and test the model on different base/novel dataset divisions to assess its robustness.
4. It is advised to train and test the model on the PSG dataset as well.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xjyU6zmZD7;"REVIEW 
Summary:
The paper trains SNNs using surrogate gradient learning. In order to mitigate the gradient vanishing problem, the paper proposed the Shortcut Back-propagation method and utilizes an evolutionary algorithm framework to balance the training of shallow and deep layers. The effectiveness of the proposed method is demonstrated through many experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1)	The shortcut backpropagation method and the evolutionary training method are novel. 
2)	This paper can well handle the gradient vanishing problem.
3)	The paper is well-written.
4)	The paper shows the effectiveness of the proposed methods through many experiments.

Weaknesses:
1)	The author should add more mathematical proof to demonstrate that the mentioned residual structure in SNN is not very effective? The introduction of shortcut branches might add complexity to the network architecture, which could affect the interpretability of the model.
2)	Some recent SOTA works should be compared with too.  The authors can also compare with paper [1][2] which obtains really good results by MS-ResNet-18 backbone with 1 or 6 timesteps on large imageNet datasets.


[1]Yao M, Zhao G, Zhang H, et al. Attention spiking neural networks[J]. IEEE transactions on pattern analysis and machine intelligence, 2023.

[2] Qiu X, Zhu R J, Chou Y, et al. Gated attention coding for training high-performance and efficient spiking neural networks[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(1): 601-610.

Limitations:
I find no limitation about the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a simple method to mitigate the gradient vanishing problem in the training of SNNs. This method introduces some early classification heads (including a pooling layer and a fully connected layer) to the SNN. Because the gradients from the early classification heads pass fewer surrogate gradients, this method aids the SNN in addressing the gradient vanishing problem. The authors also suggest an evolutionary training framework that changes the loss function to gradually adjust how important early classification head outputs are during the training phase. The proposed methods are only alive in the training phase and will not affect the inference phase of SNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This proposed method partially alleviates the gradient vanishing problem in the training of SNN with surrogate gradients. Furthermore, the method has demonstrated excellent performance across multiple datasets. The Short-BP method can be easily integrated into the SNN training process without introducing excessive computational overhead. Furthermore, the evolutionary training framework effectively mitigates the short-BP problem, which may make the network pay more attention to early classification heads than the final SNN output. The writing in this paper is clear and concise.

Weaknesses:
1. In this paper, the author only demonstrates a change in gradient distribution in the first layer. Presenting the changes in the men and variance of the absolute gradients for each layer would provide a more direct proof of their argument.
2. The author should provide a more detailed mathematical proof to explain why the use of surrogate gradients in deep SNN would lead to gradient vanishing, as well as why direct use of residual learning will not address the problem.
3. The author has not demonstrated their method on much deeper network architectures where the gradient vanishing problem is more severe.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes shortcut connections between layers to mitigate the gradient vanishing problem in SNNs. Additionally, the authors present a way to phase out the shortcut connections over training so that inference can be done without these additional connections. The experiments show that this method improves training performance in several image classification tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.The idea is small, but interesting and effective enough.

2.The performance improvement over the existing SNN methods is noticeable.

3.The paper is well-written.

Weaknesses:
1.The proposed method will increase the training time.

2.In the experimental section, some newer methods should be compared with this method.

3.Figure 2 lacks horizontal and vertical coordinates, and the readability and comprehensibility of the picture need to be improved.

Limitations:
None.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper trains SNNs using surrogate gradient learning. In order to mitigate the gradient vanishing problem, the paper proposed the Shortcut Back-propagation method and utilizes an evolutionary algorithm framework to balance the training of shallow and deep layers. The effectiveness of the proposed method is demonstrated through many experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1)	The shortcut backpropagation method and the evolutionary training method are novel. 
2)	This paper can well handle the gradient vanishing problem.
3)	The paper is well-written.
4)	The paper shows the effectiveness of the proposed methods through many experiments.

Weaknesses:
1)	The author should add more mathematical proof to demonstrate that the mentioned residual structure in SNN is not very effective? The introduction of shortcut branches might add complexity to the network architecture, which could affect the interpretability of the model.
2)	Some recent SOTA works should be compared with too.  The authors can also compare with paper [1][2] which obtains really good results by MS-ResNet-18 backbone with 1 or 6 timesteps on large imageNet datasets.


[1]Yao M, Zhao G, Zhang H, et al. Attention spiking neural networks[J]. IEEE transactions on pattern analysis and machine intelligence, 2023.

[2] Qiu X, Zhu R J, Chou Y, et al. Gated attention coding for training high-performance and efficient spiking neural networks[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(1): 601-610.

Limitations:
I find no limitation about the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a simple method to mitigate the gradient vanishing problem in the training of SNNs. This method introduces some early classification heads (including a pooling layer and a fully connected layer) to the SNN. Because the gradients from the early classification heads pass fewer surrogate gradients, this method aids the SNN in addressing the gradient vanishing problem. The authors also suggest an evolutionary training framework that changes the loss function to gradually adjust how important early classification head outputs are during the training phase. The proposed methods are only alive in the training phase and will not affect the inference phase of SNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This proposed method partially alleviates the gradient vanishing problem in the training of SNN with surrogate gradients. Furthermore, the method has demonstrated excellent performance across multiple datasets. The Short-BP method can be easily integrated into the SNN training process without introducing excessive computational overhead. Furthermore, the evolutionary training framework effectively mitigates the short-BP problem, which may make the network pay more attention to early classification heads than the final SNN output. The writing in this paper is clear and concise.

Weaknesses:
1. In this paper, the author only demonstrates a change in gradient distribution in the first layer. Presenting the changes in the men and variance of the absolute gradients for each layer would provide a more direct proof of their argument.
2. The author should provide a more detailed mathematical proof to explain why the use of surrogate gradients in deep SNN would lead to gradient vanishing, as well as why direct use of residual learning will not address the problem.
3. The author has not demonstrated their method on much deeper network architectures where the gradient vanishing problem is more severe.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes shortcut connections between layers to mitigate the gradient vanishing problem in SNNs. Additionally, the authors present a way to phase out the shortcut connections over training so that inference can be done without these additional connections. The experiments show that this method improves training performance in several image classification tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.The idea is small, but interesting and effective enough.

2.The performance improvement over the existing SNN methods is noticeable.

3.The paper is well-written.

Weaknesses:
1.The proposed method will increase the training time.

2.In the experimental section, some newer methods should be compared with this method.

3.Figure 2 lacks horizontal and vertical coordinates, and the readability and comprehensibility of the picture need to be improved.

Limitations:
None.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xjXYgdFM5M;"REVIEW 
Summary:
This paper addresses the challenges associated with the decline in performance of LLMs after undergoing knowledge editing. The study identifies the primary factors contributing to performance degradation from both data and model perspectives. By constructing a Multi-Question Dataset (MQD) and analyzing the impact of editing objectives, token length, and diversity, the paper finds that perplexity associated with editing objectives significantly affects model performance. From the model perspective, a strong correlation was observed between the L1 norm of parameter layers and editing accuracy. The paper proposes a novel method called Dump for sequence (D4C), which effectively manages the parameter growth and improves model performance post-editing.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Innovative Methodological Approach: The study introduces a new method, D4C, which addresses the explosive growth in parameter norms and optimizes model performance post-editing. This approach is both innovative and practical for managing edited models.
- Comprehensive Data Analysis: The construction of the Multi-Question Dataset and detailed analysis of how different types of data affect model performance provide valuable insights into the mechanics of model editing.
- Clear Identification of Problems and Solutions: The paper clearly identifies specific problems associated with knowledge editing in LLMs, such as catastrophic forgetting and performance bottlenecks, and provides targeted solutions to these issues.
- Empirical Validation: The experiments conducted in this paper offer empirical evidence supporting the proposed methods, enhancing the credibility and applicability of the findings.

Weaknesses:
- Generalizability of Findings: The study focuses on specific scenarios and datasets, which may limit the generalizability of the findings across different types of LLMs or editing tasks.
- Potential Overfitting to Edited Scenarios: There is a risk that the model may become overly optimized for the edited scenarios, potentially affecting its performance on unedited or unrelated tasks.
- Complexity of Implementation: The proposed D4C method, while effective, may be complex to implement and integrate into existing systems due to its sophisticated handling of parameter layers.
- Unsuitable Citation Format: The citations in this paper are in the format of “XXX et al. [YEAR]”, which are not suitable enough, and had better change into the format of [1], [2], [3], ……

Limitations:
- Dependency on Specific Data Characteristics: The effectiveness of the proposed solutions may depend heavily on the characteristics of the data used for training and testing, which might not be consistent across different domains or applications.
- Evaluation Metrics: While the paper introduces new evaluation methods, the reliance on perplexity (PPL) and L1 norm metrics might not completely capture all aspects of model performance and health, especially in nuanced or context-dependent scenarios.
- Limited Experimentation: The experiments (Section 5) in this paper are too limited to demonstrate the conclusion. 
- Scope of Editing Objectives: The study might not fully capture the impact of highly diverse or complex editing objectives that could be encountered in real-world scenarios.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Recent research has shown varying degrees of decline in model performance following small changes made by certain model editing methods. This paper is the first to comprehensively analyze the reasons behind such performance declines. Through extensive experiments, it identifies two main factors: data and model. For data-specific factors, the paper finds that perplexity and token length significantly influence performance. For model-specific factors, the L1 norm of the edited layer is identified as a key influence. Building upon these insights, the paper proposes a method named Dump for sequence (D4C), which significantly improves model performance.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-motivated: Exploring the reasons behind and impact of small changes made by model editing techniques on the performance of unedited samples is of great significance.
- The analysis of the data-specific and model-specific factors is supported with diverse datasets and comprehensive experiments. The model-specific analysis, in particular, is evaluated rigorously, addressing the forgetting issue that prior works often overlooked

- The observation of the influence of editing on the model norm is intriguing. High-norm parameters can be sensitive to noise and numerically unstable. It would be beneficial if the authors could also provide an L2-norm plot for comparison.

- The experimental results are impressive, demonstrating significant improvements and validating the effectiveness of the proposed method.

Weaknesses:
- My main concern with the data-specific analysis is whether the conclusion is about correlation or causation. Many variables can be changed about the input data. Plotting a single Figure 3 might be insufficient to justify that perplexity and token length are the main reasons for the decline in model performance after editing.

- Unfortunately, the constructed dataset is not open-sourced. 

- Recent research [1] has shown that model editing methods (e.g. ROME, MEMIT) are not good at handling multi-hop questions, how would D4C perform in such more challenging scenarios?

- Some theoretical analysis can be conducted to demonstrate that D4C does not lead to an increase in norms.

[1] Mquake: Assessing knowledge editing in language models via multi-hop questions. EMNLP 2023

Limitations:
The limitations are discussed in the paper.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates the reasons behind performance decline in sequential model editing approaches that selectively update parameters based on both data and model factors. To address the issues causing this decline, the authors propose a method to save editing history, thereby transforming sequential editing into batch editing with minimal computational overhead.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Extensive experimentation is conducted to empirically demonstrate how factors such as dataset characteristics, editing objectives, and model-specific properties affect performance in sequential model editing.

A simple matrix storage solution is introduced, which enables the conversion of sequential editing into batch editing.

Weaknesses:
The study is restricted to two closely related editing approaches.

Experimentation is limited in demonstrating the efficacy of the D4C method. Different datasets and a larger number of edits for a more thorough evaluation are needed.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the reasons and solutions for the decline in model performance of model editing.  The authors conduct experiments from two perspectives: data and model. Specifically, to clarify the impact of data on the performance of edited models, the authors first evaluate how editing different types of data affects model performance. Then, the authors construct a Multi-Question Dataset (MQD) and identified that the performance of  the edited models is primarily influenced by the diversity of the editing objectives  and the length of the tokens. Secondly, the authors explore the factors that affect model  performance from a model perspective. Experiments revealed a strong correlation between the L1 norm of the edited model layers and the editing accuracy, and  identified an editing quantity bottleneck. To enhance the performance of edited  models, the authors propose a Dump for sequence (D4C) method that effectively improves  the performance of edited models and overcomes the previous editing bottleneck issue. This method allows for multiple effective edits with minimal impact on  model performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper investigates the impact of data on the performance of edited models. Evaluations are conducted across multiple tasks, revealing that the editing objective is the primary factor influencing model performance.

The authors found that the decline in edited model performance is correlated with the explosive growth of the L1 norm of parameter layers during the editing process.

This paper proposes a caching sequence edit method that leverages O(1) space complexity to retain past knowledge and regulate the explosive growth of the parameter layer norm.

Weaknesses:
The writing of this paper should be improved. There is no overview of this paper, which makes it hard to follow the details of Section 3 and 4.

The motivation of the proposed method  is not clear.

There are many typos such as line 182.

There are many missing references such as: 

Knowledge Editing for Large Language Models: A Survey

Stable Knowledge Editing in Large Language Models

A Comprehensive Study of Knowledge Editing for Large Language Models

Editing Large Language Models: Problems, Methods, and Opportunities

Limitations:
Yes

The limitations are on page ten. I am unsure if this counts as exceeding the page limit.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenges associated with the decline in performance of LLMs after undergoing knowledge editing. The study identifies the primary factors contributing to performance degradation from both data and model perspectives. By constructing a Multi-Question Dataset (MQD) and analyzing the impact of editing objectives, token length, and diversity, the paper finds that perplexity associated with editing objectives significantly affects model performance. From the model perspective, a strong correlation was observed between the L1 norm of parameter layers and editing accuracy. The paper proposes a novel method called Dump for sequence (D4C), which effectively manages the parameter growth and improves model performance post-editing.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Innovative Methodological Approach: The study introduces a new method, D4C, which addresses the explosive growth in parameter norms and optimizes model performance post-editing. This approach is both innovative and practical for managing edited models.
- Comprehensive Data Analysis: The construction of the Multi-Question Dataset and detailed analysis of how different types of data affect model performance provide valuable insights into the mechanics of model editing.
- Clear Identification of Problems and Solutions: The paper clearly identifies specific problems associated with knowledge editing in LLMs, such as catastrophic forgetting and performance bottlenecks, and provides targeted solutions to these issues.
- Empirical Validation: The experiments conducted in this paper offer empirical evidence supporting the proposed methods, enhancing the credibility and applicability of the findings.

Weaknesses:
- Generalizability of Findings: The study focuses on specific scenarios and datasets, which may limit the generalizability of the findings across different types of LLMs or editing tasks.
- Potential Overfitting to Edited Scenarios: There is a risk that the model may become overly optimized for the edited scenarios, potentially affecting its performance on unedited or unrelated tasks.
- Complexity of Implementation: The proposed D4C method, while effective, may be complex to implement and integrate into existing systems due to its sophisticated handling of parameter layers.
- Unsuitable Citation Format: The citations in this paper are in the format of “XXX et al. [YEAR]”, which are not suitable enough, and had better change into the format of [1], [2], [3], ……

Limitations:
- Dependency on Specific Data Characteristics: The effectiveness of the proposed solutions may depend heavily on the characteristics of the data used for training and testing, which might not be consistent across different domains or applications.
- Evaluation Metrics: While the paper introduces new evaluation methods, the reliance on perplexity (PPL) and L1 norm metrics might not completely capture all aspects of model performance and health, especially in nuanced or context-dependent scenarios.
- Limited Experimentation: The experiments (Section 5) in this paper are too limited to demonstrate the conclusion. 
- Scope of Editing Objectives: The study might not fully capture the impact of highly diverse or complex editing objectives that could be encountered in real-world scenarios.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Recent research has shown varying degrees of decline in model performance following small changes made by certain model editing methods. This paper is the first to comprehensively analyze the reasons behind such performance declines. Through extensive experiments, it identifies two main factors: data and model. For data-specific factors, the paper finds that perplexity and token length significantly influence performance. For model-specific factors, the L1 norm of the edited layer is identified as a key influence. Building upon these insights, the paper proposes a method named Dump for sequence (D4C), which significantly improves model performance.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-motivated: Exploring the reasons behind and impact of small changes made by model editing techniques on the performance of unedited samples is of great significance.
- The analysis of the data-specific and model-specific factors is supported with diverse datasets and comprehensive experiments. The model-specific analysis, in particular, is evaluated rigorously, addressing the forgetting issue that prior works often overlooked

- The observation of the influence of editing on the model norm is intriguing. High-norm parameters can be sensitive to noise and numerically unstable. It would be beneficial if the authors could also provide an L2-norm plot for comparison.

- The experimental results are impressive, demonstrating significant improvements and validating the effectiveness of the proposed method.

Weaknesses:
- My main concern with the data-specific analysis is whether the conclusion is about correlation or causation. Many variables can be changed about the input data. Plotting a single Figure 3 might be insufficient to justify that perplexity and token length are the main reasons for the decline in model performance after editing.

- Unfortunately, the constructed dataset is not open-sourced. 

- Recent research [1] has shown that model editing methods (e.g. ROME, MEMIT) are not good at handling multi-hop questions, how would D4C perform in such more challenging scenarios?

- Some theoretical analysis can be conducted to demonstrate that D4C does not lead to an increase in norms.

[1] Mquake: Assessing knowledge editing in language models via multi-hop questions. EMNLP 2023

Limitations:
The limitations are discussed in the paper.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates the reasons behind performance decline in sequential model editing approaches that selectively update parameters based on both data and model factors. To address the issues causing this decline, the authors propose a method to save editing history, thereby transforming sequential editing into batch editing with minimal computational overhead.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Extensive experimentation is conducted to empirically demonstrate how factors such as dataset characteristics, editing objectives, and model-specific properties affect performance in sequential model editing.

A simple matrix storage solution is introduced, which enables the conversion of sequential editing into batch editing.

Weaknesses:
The study is restricted to two closely related editing approaches.

Experimentation is limited in demonstrating the efficacy of the D4C method. Different datasets and a larger number of edits for a more thorough evaluation are needed.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the reasons and solutions for the decline in model performance of model editing.  The authors conduct experiments from two perspectives: data and model. Specifically, to clarify the impact of data on the performance of edited models, the authors first evaluate how editing different types of data affects model performance. Then, the authors construct a Multi-Question Dataset (MQD) and identified that the performance of  the edited models is primarily influenced by the diversity of the editing objectives  and the length of the tokens. Secondly, the authors explore the factors that affect model  performance from a model perspective. Experiments revealed a strong correlation between the L1 norm of the edited model layers and the editing accuracy, and  identified an editing quantity bottleneck. To enhance the performance of edited  models, the authors propose a Dump for sequence (D4C) method that effectively improves  the performance of edited models and overcomes the previous editing bottleneck issue. This method allows for multiple effective edits with minimal impact on  model performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper investigates the impact of data on the performance of edited models. Evaluations are conducted across multiple tasks, revealing that the editing objective is the primary factor influencing model performance.

The authors found that the decline in edited model performance is correlated with the explosive growth of the L1 norm of parameter layers during the editing process.

This paper proposes a caching sequence edit method that leverages O(1) space complexity to retain past knowledge and regulate the explosive growth of the parameter layer norm.

Weaknesses:
The writing of this paper should be improved. There is no overview of this paper, which makes it hard to follow the details of Section 3 and 4.

The motivation of the proposed method  is not clear.

There are many typos such as line 182.

There are many missing references such as: 

Knowledge Editing for Large Language Models: A Survey

Stable Knowledge Editing in Large Language Models

A Comprehensive Study of Knowledge Editing for Large Language Models

Editing Large Language Models: Problems, Methods, and Opportunities

Limitations:
Yes

The limitations are on page ten. I am unsure if this counts as exceeding the page limit.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xgiurUq0ss;"REVIEW 
Summary:
This paper proposes DDK, a knowledge distillation (KD) framework that distills large language models (LMs) into small LMs. Unlike previous KD methods, DDK dynamically adjusts the domain weights during distillation. Experiments show that DDK outperforms other KD baselines across various tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well written and the method is easy to follow.
2. The experiments show that DDK outperforms other KD baselines on various tasks.

Weaknesses:
The extra computation introduced by KKD should be considered. It seems KKD requires the inference of a large LM during the training of the small LM. When the teacher model is much larger than the student model (QWen-1.5 14B v.s. QWen-1.5 1.8B), the inference cost of the teacher model would be even larger than training the student model. Therefore, it is more reasonable to compare the performance of the distilled model and the baselines given the same FLOPs.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new framework called Dynamic Domain Knowledge Distillation (DDK) to enhance the efficiency of knowledge distillation for large language models (LLMs). Unlike traditional methods that overlook domain performance differences between student and teacher models, DDK dynamically adjusts the distillation dataset composition based on these differences, ensuring a more stable and effective knowledge transfer. This approach addresses the issue of excessive focus on domains with minimal performance gaps and enhances overall model performance. Extensive evaluations demonstrate that DDK significantly outperforms existing knowledge distillation methods and continuously pretrained baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed dynamic dataloader for KD is technically sound. 
- Numerical experiments well validate the efficacy of the method.

Weaknesses:
- Dynamic dataloader requires knowing the training data distribution and category beforehand. 
- Missing references. Similar ideas have been explored in pruning LLMs, such as ShearedLLaMA, LoRAShear to recover the knowledge . The paper needs to discuss with them in the related work section due to the closed relation between pruning and KD. 

Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning

LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The work introduces a novel framework for knowledge distillation (KD) for LLMs. The key innovation of DDK is its dynamic adjustment of the distillation dataset composition based on domain performance differences between the teacher and student models. The paper presents extensive evaluations demonstrating that DDK significantly improves performance in various KD settings, outperforming both continuous training baselines and existing KD methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.  The authors provide extensive empirical evidence demonstrating the effectiveness of DDK in improving the performance of student models across various benchmarks.
2.  As the computational and storage demands of LLMs are significant barriers to their widespread deployment, KD is a promising solution.  The proposed KD method is simple and easy to follow.

Weaknesses:
1. Discuss the difference between DDK and the Dynamic Batch Loading proposed by Sheared LLaMA[1], which is also 
 proposed to adjust domain proportions for dynamically training smaller models. They also identify discrepancies in loss between smaller and larger models across various domains, and accordingly, they sample more data from domains where the discrepancy is more pronounced. While they concentrate on structural pruning, it is akin to the DDK. Consequently, I perceive the novelty of DDK as being somewhat limited.
2. The results of Qwen 1.5 in Table 1 are not significantly convincing. The MMLU/HumanEval of Qwen 1.5 1.8B in the Qwen official blog are  46.8/20.1 while the authors' report is 44.5/11.9. In addition, compared to the official results, we can see that the DDK fails to improve the model of the students on MMLU. The authors need to check this and provide **more robust results of baselines**.

[1] SHEARED LLAMA: ACCELERATING LANGUAGE MODEL PRE-TRAINING VIA STRUCTURED PRUNING. Xia et al., 2023

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposed a KD strategy for LLMs. Specifically, with assess to the domain-specific performance of both the teacher and student LLMs, DDK uses domain knowledge guided sampling to dynamically update the data mixture. In addition the paper also conducts a statistical analysis of the domain distribution of the datasets involved. The training process is relatively straightforward and easy to generalize. The experimental results also show that DDK's training method improves the performance average of different data sets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A complete training algorithm is designed, and the process is explained clearly. The process of the DDK algorithm is easy to extend to the training process of other models.
2. The authors conducted a comprehensive knowledge distillation experiment on two large model families and a comprehensive ablation study.

Weaknesses:
1. Although the method proposed in this paper is easy to understand and effective, I doubt that the method in this paper is limited to LLMs. In other words, this paper does not mention (or needs to explain) how previous researchers (before LLMs) performed domain-enhanced distillation for domain-biased datasets, and why these previous methods cannot be applied to the distillation of LLMs to achieve similar results. The advantages and novelty of this paper's domain sampling method over previous work that may be transferable to LLMs need further explanation.
2. In the experimental part, there is a lack of key comparison between DDK and other methods that focus on similar domain sampling. The baseline actually involves the work that focuses on domain in KD (cited as [60], etc.), but the subsequent analysis only compares the total average score of DDK and these works, which seems to lack comparison and analysis of similar works. As far as I know, other baselines are more general KDs, and do not focus on domain information.
It is certainly worth noting that DDK performs better than baselines such as MiniLLM, but I think what can better illustrate the effectiveness and novelty of this paper is the comparison with similar domain data sampling, including experimental analysis.
3. In the experimental section, you can add experiments on the dataset and the scale property of the teacher model. This is a possible suggestion.

Limitations:
The method proposed in this paper is very complete and solves the problem well, but there are perhaps two points to note:
1. The distillation method designed in this paper does not seem to be necessarily limited to LLMs. One of the main difficulties of KD on LLM may be that the distribution difference between the teacher and student models is too large, but the key points that this paper focuses on and solves seem to be orthogonal to this point. So it also leads to a similar question: why previous similar methods that focus on domain sampling cannot be migrated to LLMs, and what are the advantages and novelties of this paper's design.
2. Following point 1, in the experiment, what are the specific advantages of this method over the predecessors in the domain problem (not just the overall average, of which result shows DDK outperformed others). This may be what I am very curious about after reading the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes DDK, a knowledge distillation (KD) framework that distills large language models (LMs) into small LMs. Unlike previous KD methods, DDK dynamically adjusts the domain weights during distillation. Experiments show that DDK outperforms other KD baselines across various tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well written and the method is easy to follow.
2. The experiments show that DDK outperforms other KD baselines on various tasks.

Weaknesses:
The extra computation introduced by KKD should be considered. It seems KKD requires the inference of a large LM during the training of the small LM. When the teacher model is much larger than the student model (QWen-1.5 14B v.s. QWen-1.5 1.8B), the inference cost of the teacher model would be even larger than training the student model. Therefore, it is more reasonable to compare the performance of the distilled model and the baselines given the same FLOPs.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new framework called Dynamic Domain Knowledge Distillation (DDK) to enhance the efficiency of knowledge distillation for large language models (LLMs). Unlike traditional methods that overlook domain performance differences between student and teacher models, DDK dynamically adjusts the distillation dataset composition based on these differences, ensuring a more stable and effective knowledge transfer. This approach addresses the issue of excessive focus on domains with minimal performance gaps and enhances overall model performance. Extensive evaluations demonstrate that DDK significantly outperforms existing knowledge distillation methods and continuously pretrained baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed dynamic dataloader for KD is technically sound. 
- Numerical experiments well validate the efficacy of the method.

Weaknesses:
- Dynamic dataloader requires knowing the training data distribution and category beforehand. 
- Missing references. Similar ideas have been explored in pruning LLMs, such as ShearedLLaMA, LoRAShear to recover the knowledge . The paper needs to discuss with them in the related work section due to the closed relation between pruning and KD. 

Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning

LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The work introduces a novel framework for knowledge distillation (KD) for LLMs. The key innovation of DDK is its dynamic adjustment of the distillation dataset composition based on domain performance differences between the teacher and student models. The paper presents extensive evaluations demonstrating that DDK significantly improves performance in various KD settings, outperforming both continuous training baselines and existing KD methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.  The authors provide extensive empirical evidence demonstrating the effectiveness of DDK in improving the performance of student models across various benchmarks.
2.  As the computational and storage demands of LLMs are significant barriers to their widespread deployment, KD is a promising solution.  The proposed KD method is simple and easy to follow.

Weaknesses:
1. Discuss the difference between DDK and the Dynamic Batch Loading proposed by Sheared LLaMA[1], which is also 
 proposed to adjust domain proportions for dynamically training smaller models. They also identify discrepancies in loss between smaller and larger models across various domains, and accordingly, they sample more data from domains where the discrepancy is more pronounced. While they concentrate on structural pruning, it is akin to the DDK. Consequently, I perceive the novelty of DDK as being somewhat limited.
2. The results of Qwen 1.5 in Table 1 are not significantly convincing. The MMLU/HumanEval of Qwen 1.5 1.8B in the Qwen official blog are  46.8/20.1 while the authors' report is 44.5/11.9. In addition, compared to the official results, we can see that the DDK fails to improve the model of the students on MMLU. The authors need to check this and provide **more robust results of baselines**.

[1] SHEARED LLAMA: ACCELERATING LANGUAGE MODEL PRE-TRAINING VIA STRUCTURED PRUNING. Xia et al., 2023

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposed a KD strategy for LLMs. Specifically, with assess to the domain-specific performance of both the teacher and student LLMs, DDK uses domain knowledge guided sampling to dynamically update the data mixture. In addition the paper also conducts a statistical analysis of the domain distribution of the datasets involved. The training process is relatively straightforward and easy to generalize. The experimental results also show that DDK's training method improves the performance average of different data sets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A complete training algorithm is designed, and the process is explained clearly. The process of the DDK algorithm is easy to extend to the training process of other models.
2. The authors conducted a comprehensive knowledge distillation experiment on two large model families and a comprehensive ablation study.

Weaknesses:
1. Although the method proposed in this paper is easy to understand and effective, I doubt that the method in this paper is limited to LLMs. In other words, this paper does not mention (or needs to explain) how previous researchers (before LLMs) performed domain-enhanced distillation for domain-biased datasets, and why these previous methods cannot be applied to the distillation of LLMs to achieve similar results. The advantages and novelty of this paper's domain sampling method over previous work that may be transferable to LLMs need further explanation.
2. In the experimental part, there is a lack of key comparison between DDK and other methods that focus on similar domain sampling. The baseline actually involves the work that focuses on domain in KD (cited as [60], etc.), but the subsequent analysis only compares the total average score of DDK and these works, which seems to lack comparison and analysis of similar works. As far as I know, other baselines are more general KDs, and do not focus on domain information.
It is certainly worth noting that DDK performs better than baselines such as MiniLLM, but I think what can better illustrate the effectiveness and novelty of this paper is the comparison with similar domain data sampling, including experimental analysis.
3. In the experimental section, you can add experiments on the dataset and the scale property of the teacher model. This is a possible suggestion.

Limitations:
The method proposed in this paper is very complete and solves the problem well, but there are perhaps two points to note:
1. The distillation method designed in this paper does not seem to be necessarily limited to LLMs. One of the main difficulties of KD on LLM may be that the distribution difference between the teacher and student models is too large, but the key points that this paper focuses on and solves seem to be orthogonal to this point. So it also leads to a similar question: why previous similar methods that focus on domain sampling cannot be migrated to LLMs, and what are the advantages and novelties of this paper's design.
2. Following point 1, in the experiment, what are the specific advantages of this method over the predecessors in the domain problem (not just the overall average, of which result shows DDK outperformed others). This may be what I am very curious about after reading the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xgP5ynlZWf;"REVIEW 
Summary:
For real-world images corrupted by multiple simultaneous degradations, this paper first analyzes the limitations of using all-in-one restoration models and various task-specific models. The authors then introduce RestoreAgent, which automatically identifies the types of degradation in a degraded image, determines the sequence of restoration tasks, and selects suitable models from the model pool. RestoreAgent presents an automated restoration pipeline that requires only an input image and a general human instruction, without any prior knowledge of the involved degradation tasks or manually predefined task sequences.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper comprehensively analyzes the challenges and limitations of employing all-in-one models and multiple task-specific expert models with fixed or random task sequences, as well as fixed or random models for each task.
2. The authors evaluate various configurations of RestoreAgent using diverse objective image quality metrics (PSNR, SSIM, LPIPS, DISTS, and their combinations), all of which outperform the human expert model on the corresponding metric. 
3. RestoreAgent exhibits the scalability by extending to new tasks and models with minimal computational resource. 
4. The presentation, including writing, analysis, and visualization, is clear and easy to follow.

Weaknesses:
1. Incomplete descriptions about data construction. 

- Authors randomly select up to four types of degradation from a degradation set (noise, blur, JPEG, rain, haze, and low-light) to construct paired training data. According to data synthesis strategies in [1,2], JPEG compression is typically performed after noise and blur, and in the final order. Is the degradation order of JPEG compression in this paper the same? If not, the authors should discuss the reasonableness of random sampling.

- What are the components of 23k paired data? One degraded image for each high-quality image or many degraded versions for each high-quality image? 

- What is the configuration in ablation studies about training data amount? Simultaneously scaling up low & high-quality images or synthesizing more low-quality images for each high-quality image? If it’s the former, will increasing the number of degraded images while keeping the number of high-quality images unchanged improve performance?

2. Inference time for input images with diverse resolution.
- The authors are suggested to report the running time for input images of various resolutions. This should include the total time, the running time for the RestoreAgent, and the running time for the subsequent restoration models. The reviewer is curious whether the agent's response time exceeds that of the restoration models when processing high-resolution images, such as those with 4K resolution.

3. Scalability for new tasks and models. 
- Section 4.5 demonstrates that the proposed RestoreAgent can extend to new tasks and models in just half an hour, surpassing human expert-level performance on the new task. However, it is unclear whether adaptation to the new task results in performance degradation on prior tasks, similar to the catastrophic forgetting problem in continual learning. The authors are encouraged to report the performance of the fine-tuned model on the previous tasks to address this concern.

[1] Wang X, Xie L, Dong C, et al. Real-esrgan: Training real-world blind super-resolution with pure synthetic data[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 1905-1914.

[2] Zhang K, Liang J, Van Gool L, et al. Designing a practical degradation model for deep blind image super-resolution[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 4791-4800.

Limitations:
The manuscript includes the checklist guidelines.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new pipeline to address multiple degradation, like noise, blur and low light. Besides, a RestoreAgent with multimodal large language models is introduced to assess the type and extent of degradations in the input images and perform dynamic restorations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and well-organised. 
2. The whole pipeline seems to be novel and reasonable. 
3. The method achieves SOTA performance on several benchmarks and different degradation tasks.

Weaknesses:
The overall motivation of this paper is commendable, but I have a few concerns:

1. The author mentions that RestoreAgent can autonomously assess the type and extent of degradation in input images and perform restoration. This strategy is interesting. However, I am wondering how the order of different enhancement techniques is defined. For example, if the input has noise and rain streaks, how is the order of dehazing and denoising techniques determined? Will this affect performance?

2. In contrast to other image enhancement techniques, the proposed RestoreAgent should first find a suitable restoration task and then select the most appropriate model to enhance the quality of the input. Therefore, I am concerned whether this process will increase the inference time. The authors should provide some computational analysis.

3. The enhancement capabilities of this work rely heavily on existing enhancement frameworks. If existing frameworks cannot work well in some cases, such as extreme noise effects, I guess the proposed RestoreAgent may also fail. Is this true? If so, I suggest the authors mention this in the limitations section.

4. The explanation of ""ranking"" and ""balanced"" in Table 1 is still unclear. The authors should clarify the definitions of these terms.

5. It would be better to show more visual comparisons of the RestoreAgent.

Limitations:
Limitations are not mentioned in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an image restoration pipeline designed to handle various degradation types and levels by leveraging MLLM’s capabilities to select the appropriate model and determine the execution order. It begins with an analysis of why execution order and utilizing multiple models for different degradation levels are crucial for restoring complexly degraded images. The paper then constructs an instruction dataset and fine-tunes the MLLM. Experimental results demonstrate the effectiveness of the proposed restoration pipeline.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1.
This work presents a compelling analysis of complex image restoration. This insight is valuable given that degraded images in real-world scenarios often involve multiple types of degradation.
2.
This approach leverages the strengths of different models for handling specific noise levels, thereby eliminating the trade-off between generalization and performance.
3.
This paper formally defines the problem of handling multiple degradations and model selection in image restoration.
4.
Extensive experiments demonstrate superiority of such pipeline in processing degraded images with multiple degradations.

Weaknesses:
1.
In the introduction, it would be helpful to explain how the Multi-Level Learning Model (MLLM) excels at understanding different types and levels of image degradation. This will show why MLLM is well-suited for handling complex combinations of image degradation. Providing this clarity will make the benefits of using MLLM for image restoration more evident.
2.
When incorporating a new type of degradation, the cost extends beyond merely training the MLLM. Please also discuss the process of constructing training data for the newly added degradation and how it integrates with previously trained data.
3.
In lines 211-212, please clarify what the mean and standard deviation are calculated over. The subscript ""i"" is already used for degradation type and it might be clearer to use another character.

Limitations:
The authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces RestoreAgent, an innovative image restoration system that leverages multimodal large language models to autonomously handle images with multiple types of degradation. The system addresses limitations of existing all-in-one models and fixed task sequences by dynamically adapting to each image's specific degradations. RestoreAgent can identify degradation types, determine appropriate restoration tasks, optimize the execution sequence, select the most suitable models, and execute the restoration process autonomously. The authors present a method for constructing training data and demonstrate that RestoreAgent outperforms existing methods and human experts in handling complex image degradations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper represents a innovation and a good contribution in image restoration and potentially opens up a new research direction for this area.
2. The motivation is strong. The authors effectively demonstrate the importance of task execution order and model selection in multi-task scenarios. The designed system adeptly addresses these issues.
3. Experimental results indicate that RestoreAgent's decision-making capabilities in handling complex degradations surpass those of human experts. This kind of pipeline also surpass all-in-one models.
4. The paper is generally well written and clear to understand.

Weaknesses:
1. The paper constructs a training dataset for training the multimodal large language model and a testing dataset as a benchmark for evaluating performance across multiple tasks. More details and explanations regarding the construction methods of these datasets would be beneficial.
2. Table 1 presents performance rankings using both ordinal and percentage forms. The definitions and explanations for these ranking forms are somewhat lacking, which might require readers to spend extra time understanding them. Clearer explanations would facilitate better comprehension.
3. The proposed Autonomous Restoration Agent represents a novel paradigm that is likely to encounter numerous new challenges. Beyond the issues already mentioned in the paper, the authors could consider discussing additional limitations and future research directions for this paradigm. This would help future researchers better follow and improve upon this work.

Limitations:
See Weaknesses

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
For real-world images corrupted by multiple simultaneous degradations, this paper first analyzes the limitations of using all-in-one restoration models and various task-specific models. The authors then introduce RestoreAgent, which automatically identifies the types of degradation in a degraded image, determines the sequence of restoration tasks, and selects suitable models from the model pool. RestoreAgent presents an automated restoration pipeline that requires only an input image and a general human instruction, without any prior knowledge of the involved degradation tasks or manually predefined task sequences.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper comprehensively analyzes the challenges and limitations of employing all-in-one models and multiple task-specific expert models with fixed or random task sequences, as well as fixed or random models for each task.
2. The authors evaluate various configurations of RestoreAgent using diverse objective image quality metrics (PSNR, SSIM, LPIPS, DISTS, and their combinations), all of which outperform the human expert model on the corresponding metric. 
3. RestoreAgent exhibits the scalability by extending to new tasks and models with minimal computational resource. 
4. The presentation, including writing, analysis, and visualization, is clear and easy to follow.

Weaknesses:
1. Incomplete descriptions about data construction. 

- Authors randomly select up to four types of degradation from a degradation set (noise, blur, JPEG, rain, haze, and low-light) to construct paired training data. According to data synthesis strategies in [1,2], JPEG compression is typically performed after noise and blur, and in the final order. Is the degradation order of JPEG compression in this paper the same? If not, the authors should discuss the reasonableness of random sampling.

- What are the components of 23k paired data? One degraded image for each high-quality image or many degraded versions for each high-quality image? 

- What is the configuration in ablation studies about training data amount? Simultaneously scaling up low & high-quality images or synthesizing more low-quality images for each high-quality image? If it’s the former, will increasing the number of degraded images while keeping the number of high-quality images unchanged improve performance?

2. Inference time for input images with diverse resolution.
- The authors are suggested to report the running time for input images of various resolutions. This should include the total time, the running time for the RestoreAgent, and the running time for the subsequent restoration models. The reviewer is curious whether the agent's response time exceeds that of the restoration models when processing high-resolution images, such as those with 4K resolution.

3. Scalability for new tasks and models. 
- Section 4.5 demonstrates that the proposed RestoreAgent can extend to new tasks and models in just half an hour, surpassing human expert-level performance on the new task. However, it is unclear whether adaptation to the new task results in performance degradation on prior tasks, similar to the catastrophic forgetting problem in continual learning. The authors are encouraged to report the performance of the fine-tuned model on the previous tasks to address this concern.

[1] Wang X, Xie L, Dong C, et al. Real-esrgan: Training real-world blind super-resolution with pure synthetic data[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 1905-1914.

[2] Zhang K, Liang J, Van Gool L, et al. Designing a practical degradation model for deep blind image super-resolution[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 4791-4800.

Limitations:
The manuscript includes the checklist guidelines.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new pipeline to address multiple degradation, like noise, blur and low light. Besides, a RestoreAgent with multimodal large language models is introduced to assess the type and extent of degradations in the input images and perform dynamic restorations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and well-organised. 
2. The whole pipeline seems to be novel and reasonable. 
3. The method achieves SOTA performance on several benchmarks and different degradation tasks.

Weaknesses:
The overall motivation of this paper is commendable, but I have a few concerns:

1. The author mentions that RestoreAgent can autonomously assess the type and extent of degradation in input images and perform restoration. This strategy is interesting. However, I am wondering how the order of different enhancement techniques is defined. For example, if the input has noise and rain streaks, how is the order of dehazing and denoising techniques determined? Will this affect performance?

2. In contrast to other image enhancement techniques, the proposed RestoreAgent should first find a suitable restoration task and then select the most appropriate model to enhance the quality of the input. Therefore, I am concerned whether this process will increase the inference time. The authors should provide some computational analysis.

3. The enhancement capabilities of this work rely heavily on existing enhancement frameworks. If existing frameworks cannot work well in some cases, such as extreme noise effects, I guess the proposed RestoreAgent may also fail. Is this true? If so, I suggest the authors mention this in the limitations section.

4. The explanation of ""ranking"" and ""balanced"" in Table 1 is still unclear. The authors should clarify the definitions of these terms.

5. It would be better to show more visual comparisons of the RestoreAgent.

Limitations:
Limitations are not mentioned in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an image restoration pipeline designed to handle various degradation types and levels by leveraging MLLM’s capabilities to select the appropriate model and determine the execution order. It begins with an analysis of why execution order and utilizing multiple models for different degradation levels are crucial for restoring complexly degraded images. The paper then constructs an instruction dataset and fine-tunes the MLLM. Experimental results demonstrate the effectiveness of the proposed restoration pipeline.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1.
This work presents a compelling analysis of complex image restoration. This insight is valuable given that degraded images in real-world scenarios often involve multiple types of degradation.
2.
This approach leverages the strengths of different models for handling specific noise levels, thereby eliminating the trade-off between generalization and performance.
3.
This paper formally defines the problem of handling multiple degradations and model selection in image restoration.
4.
Extensive experiments demonstrate superiority of such pipeline in processing degraded images with multiple degradations.

Weaknesses:
1.
In the introduction, it would be helpful to explain how the Multi-Level Learning Model (MLLM) excels at understanding different types and levels of image degradation. This will show why MLLM is well-suited for handling complex combinations of image degradation. Providing this clarity will make the benefits of using MLLM for image restoration more evident.
2.
When incorporating a new type of degradation, the cost extends beyond merely training the MLLM. Please also discuss the process of constructing training data for the newly added degradation and how it integrates with previously trained data.
3.
In lines 211-212, please clarify what the mean and standard deviation are calculated over. The subscript ""i"" is already used for degradation type and it might be clearer to use another character.

Limitations:
The authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces RestoreAgent, an innovative image restoration system that leverages multimodal large language models to autonomously handle images with multiple types of degradation. The system addresses limitations of existing all-in-one models and fixed task sequences by dynamically adapting to each image's specific degradations. RestoreAgent can identify degradation types, determine appropriate restoration tasks, optimize the execution sequence, select the most suitable models, and execute the restoration process autonomously. The authors present a method for constructing training data and demonstrate that RestoreAgent outperforms existing methods and human experts in handling complex image degradations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper represents a innovation and a good contribution in image restoration and potentially opens up a new research direction for this area.
2. The motivation is strong. The authors effectively demonstrate the importance of task execution order and model selection in multi-task scenarios. The designed system adeptly addresses these issues.
3. Experimental results indicate that RestoreAgent's decision-making capabilities in handling complex degradations surpass those of human experts. This kind of pipeline also surpass all-in-one models.
4. The paper is generally well written and clear to understand.

Weaknesses:
1. The paper constructs a training dataset for training the multimodal large language model and a testing dataset as a benchmark for evaluating performance across multiple tasks. More details and explanations regarding the construction methods of these datasets would be beneficial.
2. Table 1 presents performance rankings using both ordinal and percentage forms. The definitions and explanations for these ranking forms are somewhat lacking, which might require readers to spend extra time understanding them. Clearer explanations would facilitate better comprehension.
3. The proposed Autonomous Restoration Agent represents a novel paradigm that is likely to encounter numerous new challenges. Beyond the issues already mentioned in the paper, the authors could consider discussing additional limitations and future research directions for this paradigm. This would help future researchers better follow and improve upon this work.

Limitations:
See Weaknesses

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xeviQPXTMU;"REVIEW 
Summary:
This paper investigated the problem of watermarking the Federated Graph Learning (FGL) models. This paper proposed the first backdoor-based FGL watermarking framework, called FedGMark. Specifically, to tackle the issues of ineffectiveness and vulnerability of existing methods, FedGMark designed two modules respectively. One is a Customized Watermark Generator (CWG). CWG aimed to generate the watermarked trigger samples (graphs) using each client's secret key. The other is the Robust Model Loader (RML). RML guaranteed that the watermarked models were certifiably robust against layer perturbation attacks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The first attempt to watermark federated graph learning models.
- The watermarked models are certifiably robust against attacks.
- Experiments on various datasets and models validate the effectiveness of FedGMark.

Weaknesses:
My major concerns are as follows.
1. Unclear threat model: The threat model and the problem formulation of this paper is unclear. What's the capability of the adversary and the defender? And more importantly, who is the adversary to steal the FGL model? This paper proposed to watermark the FGL model from the client side, which means the clients should be trustworthy. Is the central server an adversary in this paper? To my best knowledge, the typical threat model of various attacks in FL (e.g., backdoor attacks or Byzantine attacks) assumes that some of the clients may be malicious. The author should add a section on the threat model or problem formulation and clarify why they make these assumptions. This may be helpful to better understand the problem the authors tried to solve.
2. Privacy concern: I also worry that utilizing FedGMark may raise privacy concerns. In Section 3.4, the watermarked client needs to use a subset of its training graphs as the watermarked graphs. However, in FL, the client's graphs are privacy-sensitive, and using them to verify ownership may lead to privacy leakage. This is contrary to the original purpose (preserve privacy) of FL.
3. Missing experiments on the robustness against backdoor defense: This paper considers three different watermark removal attacks. However, since FedGMark utilizes backdoor-based watermarking methods, it is important to validate whether FedGMark is robust against backdoor defenses.
4. Missing introduction to ownership verification: This paper lacks an important section to introduce the ownership verification procedure of FedGMark.

Limitations:
This paper does not include a discussion of the limitations. However, I think there is a strong assumption that the clients need to be trustworthy in FedGMark. A discussion on this assumption is necessitated.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript introduces FedGMark, a backdoor-based watermarking method specifically designed to protect Federated Graph Learning (FedGL) models from illegal copying and model theft. They claim that the proposed FedGMark is the first method to safeguard the intellectual property of FedGL models, offering certified robustness against watermark removal attacks, leveraging unique graph structures and client information to create customized and diverse watermarks. Experiments demonstrate its effectiveness and robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper introduces FedGMark to address the overlooked vulnerability of FedGL model ownership and identifies three main challenges in current watermarking techniques: inapplicability to graph data, vulnerability to removal attacks, and lack of formal guarantees. The proposed method, including CWG and RML, is clear and intuitive, and the authors have provided comprehensive experiments to support their approach.

Weaknesses:
1.	I strongly recommend setting a ""Threat Model"" subsection to clarify the potential security threats to FedGL. In my opinion, since the authors consider watermark removal attacks like distillation and finetuning, FedGL operates under a white-box setting.
2.	The paper assumes attackers know the internal information of the target watermarked model, enabling distillation, finetuning, and layer-perturbation attacks. However, I find the white-box setting narrow and trivial. The authors should consider black-box attacks, which are more challenging and meaningful. Many studies on black-box attacks can be found.
3.	In watermarking-related literature, robustness and fidelity are more frequently used terms than watermark accuracy and task accuracy.
4.	In the ""Inapplicable or Ineffective"" item, the authors state, ""For instance, they require input data to have the same size, while graphs can have varying sizes,"" which is not entirely accurate. For example, some Wavelet and DCT-based watermarking methods can be scalable.

Limitations:
Please refer to Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem of protecting model ownership in the emerging domain of Federated Graph Learning (FedGL) by proposing FedGMark, a backdoor-based watermarking technique. The authors argue that existing watermarking approaches are either inapplicable to graph data or exhibit weaknesses in terms of robustness against removal attacks and lack of formal guarantees. FedGMark aims to overcome these limitations by leveraging graph structure and client information to learn customized watermarks, employing a novel graph learning (GL) architecture that enhances robustness, and providing certified robustness guarantees against layer-perturbation attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper clearly outlines the limitations of existing watermarking techniques and presents a well-motivated approach to address them. The design of FedGMark, with its CWG and RML modules, is tailored to the specific challenges of watermarking in FedGL.
-  FedGMark demonstrates promising empirical performance in terms of both main task accuracy and watermark accuracy. It outperforms the baseline approach (random graph-based watermarking) significantly, especially under watermark removal attacks.
- The paper provides theoretical guarantees for the robustness of FedGMark against layer-perturbation attacks, a unique and valuable contribution in the watermarking literature.

Weaknesses:
1. The reliance on pre-defined private keys for watermark generation may not be practical in all scenarios, and alternative key management methods should be explored.
2. The assumption of limited attacker knowledge about the watermarked model may not hold in practice. Evaluating FedGMark against more knowledgeable adversaries would provide a more realistic assessment.
3. The focus on FedAvg for model aggregation limits the exploration of other aggregation methods and their impact on watermark robustness.

Limitations:
1. FedGMark's evaluation focuses solely on FedAvg for aggregating client models. The impact of alternative aggregation methods (e.g., those prioritizing clients based on data quality or model performance) on both watermark robustness and overall FedGL model performance remains unexplored.
2. The paper acknowledges the increased computational cost of using more submodels (S) in RML but doesn't fully analyze the scalability of FedGMark. Further investigation is needed to understand how performance scales with different numbers of clients.
3. FedGMark relies heavily on structural modifications of the graph as the watermark.  The effectiveness and robustness of alternative trigger designs, such as feature-based triggers, hybrid triggers, or combinations of different trigger types, have not been explored. 
4. The paper lacks specific details about the hyperparameters used for training the GL models on the client-side. The impact of client training dynamics, particularly the choice of learning rate and the number of local epochs, on the watermarking performance and robustness of FedGMark remains unclear and requires further investigation.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work studies watermarking for federated graph learning (FGL) to protect the ownership of participants. It proposes a customized watermark generator for local clients that can capture the local graph structure and private client information, and a robust model loader consisting of multiple GL submodels and a majority-voting-based ensemble classifier, which can defend against the proposed layer-perturbation attack.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work claims to be the first to study watermarking for FGL models.

2. The method can leverage local graph and client information to generate customized watermarks.

3. The paper introduces a layer-perturbation attack to further demonstrate the certifiably robustness of the proposed backdoor-based watermarking for FGL.

4. The work is well-motivated with preliminary studies.

Weaknesses:
1. The concept of ownership in FGL can be confusing and is not well-defined in this paper. For example, can every client claim ownership of the federated trained model? Since the watermarks from different clients are different, can any single client claim entire ownership? Additionally, for clients who participate in the FL but do not have watermarks, how can they claim ownership?

2. The motivation for using local customized watermarks is not clear. The following problems arise: (1) It is unclear how to conduct ownership verification. Should it use the global watermark or the local watermarks? (2) If using a global watermark, what is the necessity of employing customized watermarks, or what is the adequate way to aggregate the global watermark from customized watermarks? If using local watermarks, how can the customized watermarks be used across clients?

3. The method requires specific GL models (to be split to multiple submodels), which can be hard to adapt to existing FGL methods, especially for advanced FGL methods.

4. The motivation for incorporating submodels for GL is missing. Why is this design necessary?

5. (1) What does “layer indexes” for splitting GL models mean? From section 3.3, it is not clear how the submodels are split and how the split submodels are decoupled from each other regarding cascaded structures. (2) Additionally, structural information can be important for graph learning. How would discarding such structural information impact in this setting?

6. The global model is obtained by simply averaging uploaded clients’ models (not weighted by data size, or applying proxy terms for regularization). Can this method address the potential heterogeneity issue when local watermarks are highly disparate from each other?

7. The proposed method can introduce efficiency issues, as it significantly increases the number of parameters and computation time.

Limitations:
Please see Weaknesses above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigated the problem of watermarking the Federated Graph Learning (FGL) models. This paper proposed the first backdoor-based FGL watermarking framework, called FedGMark. Specifically, to tackle the issues of ineffectiveness and vulnerability of existing methods, FedGMark designed two modules respectively. One is a Customized Watermark Generator (CWG). CWG aimed to generate the watermarked trigger samples (graphs) using each client's secret key. The other is the Robust Model Loader (RML). RML guaranteed that the watermarked models were certifiably robust against layer perturbation attacks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The first attempt to watermark federated graph learning models.
- The watermarked models are certifiably robust against attacks.
- Experiments on various datasets and models validate the effectiveness of FedGMark.

Weaknesses:
My major concerns are as follows.
1. Unclear threat model: The threat model and the problem formulation of this paper is unclear. What's the capability of the adversary and the defender? And more importantly, who is the adversary to steal the FGL model? This paper proposed to watermark the FGL model from the client side, which means the clients should be trustworthy. Is the central server an adversary in this paper? To my best knowledge, the typical threat model of various attacks in FL (e.g., backdoor attacks or Byzantine attacks) assumes that some of the clients may be malicious. The author should add a section on the threat model or problem formulation and clarify why they make these assumptions. This may be helpful to better understand the problem the authors tried to solve.
2. Privacy concern: I also worry that utilizing FedGMark may raise privacy concerns. In Section 3.4, the watermarked client needs to use a subset of its training graphs as the watermarked graphs. However, in FL, the client's graphs are privacy-sensitive, and using them to verify ownership may lead to privacy leakage. This is contrary to the original purpose (preserve privacy) of FL.
3. Missing experiments on the robustness against backdoor defense: This paper considers three different watermark removal attacks. However, since FedGMark utilizes backdoor-based watermarking methods, it is important to validate whether FedGMark is robust against backdoor defenses.
4. Missing introduction to ownership verification: This paper lacks an important section to introduce the ownership verification procedure of FedGMark.

Limitations:
This paper does not include a discussion of the limitations. However, I think there is a strong assumption that the clients need to be trustworthy in FedGMark. A discussion on this assumption is necessitated.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript introduces FedGMark, a backdoor-based watermarking method specifically designed to protect Federated Graph Learning (FedGL) models from illegal copying and model theft. They claim that the proposed FedGMark is the first method to safeguard the intellectual property of FedGL models, offering certified robustness against watermark removal attacks, leveraging unique graph structures and client information to create customized and diverse watermarks. Experiments demonstrate its effectiveness and robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper introduces FedGMark to address the overlooked vulnerability of FedGL model ownership and identifies three main challenges in current watermarking techniques: inapplicability to graph data, vulnerability to removal attacks, and lack of formal guarantees. The proposed method, including CWG and RML, is clear and intuitive, and the authors have provided comprehensive experiments to support their approach.

Weaknesses:
1.	I strongly recommend setting a ""Threat Model"" subsection to clarify the potential security threats to FedGL. In my opinion, since the authors consider watermark removal attacks like distillation and finetuning, FedGL operates under a white-box setting.
2.	The paper assumes attackers know the internal information of the target watermarked model, enabling distillation, finetuning, and layer-perturbation attacks. However, I find the white-box setting narrow and trivial. The authors should consider black-box attacks, which are more challenging and meaningful. Many studies on black-box attacks can be found.
3.	In watermarking-related literature, robustness and fidelity are more frequently used terms than watermark accuracy and task accuracy.
4.	In the ""Inapplicable or Ineffective"" item, the authors state, ""For instance, they require input data to have the same size, while graphs can have varying sizes,"" which is not entirely accurate. For example, some Wavelet and DCT-based watermarking methods can be scalable.

Limitations:
Please refer to Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem of protecting model ownership in the emerging domain of Federated Graph Learning (FedGL) by proposing FedGMark, a backdoor-based watermarking technique. The authors argue that existing watermarking approaches are either inapplicable to graph data or exhibit weaknesses in terms of robustness against removal attacks and lack of formal guarantees. FedGMark aims to overcome these limitations by leveraging graph structure and client information to learn customized watermarks, employing a novel graph learning (GL) architecture that enhances robustness, and providing certified robustness guarantees against layer-perturbation attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper clearly outlines the limitations of existing watermarking techniques and presents a well-motivated approach to address them. The design of FedGMark, with its CWG and RML modules, is tailored to the specific challenges of watermarking in FedGL.
-  FedGMark demonstrates promising empirical performance in terms of both main task accuracy and watermark accuracy. It outperforms the baseline approach (random graph-based watermarking) significantly, especially under watermark removal attacks.
- The paper provides theoretical guarantees for the robustness of FedGMark against layer-perturbation attacks, a unique and valuable contribution in the watermarking literature.

Weaknesses:
1. The reliance on pre-defined private keys for watermark generation may not be practical in all scenarios, and alternative key management methods should be explored.
2. The assumption of limited attacker knowledge about the watermarked model may not hold in practice. Evaluating FedGMark against more knowledgeable adversaries would provide a more realistic assessment.
3. The focus on FedAvg for model aggregation limits the exploration of other aggregation methods and their impact on watermark robustness.

Limitations:
1. FedGMark's evaluation focuses solely on FedAvg for aggregating client models. The impact of alternative aggregation methods (e.g., those prioritizing clients based on data quality or model performance) on both watermark robustness and overall FedGL model performance remains unexplored.
2. The paper acknowledges the increased computational cost of using more submodels (S) in RML but doesn't fully analyze the scalability of FedGMark. Further investigation is needed to understand how performance scales with different numbers of clients.
3. FedGMark relies heavily on structural modifications of the graph as the watermark.  The effectiveness and robustness of alternative trigger designs, such as feature-based triggers, hybrid triggers, or combinations of different trigger types, have not been explored. 
4. The paper lacks specific details about the hyperparameters used for training the GL models on the client-side. The impact of client training dynamics, particularly the choice of learning rate and the number of local epochs, on the watermarking performance and robustness of FedGMark remains unclear and requires further investigation.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work studies watermarking for federated graph learning (FGL) to protect the ownership of participants. It proposes a customized watermark generator for local clients that can capture the local graph structure and private client information, and a robust model loader consisting of multiple GL submodels and a majority-voting-based ensemble classifier, which can defend against the proposed layer-perturbation attack.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work claims to be the first to study watermarking for FGL models.

2. The method can leverage local graph and client information to generate customized watermarks.

3. The paper introduces a layer-perturbation attack to further demonstrate the certifiably robustness of the proposed backdoor-based watermarking for FGL.

4. The work is well-motivated with preliminary studies.

Weaknesses:
1. The concept of ownership in FGL can be confusing and is not well-defined in this paper. For example, can every client claim ownership of the federated trained model? Since the watermarks from different clients are different, can any single client claim entire ownership? Additionally, for clients who participate in the FL but do not have watermarks, how can they claim ownership?

2. The motivation for using local customized watermarks is not clear. The following problems arise: (1) It is unclear how to conduct ownership verification. Should it use the global watermark or the local watermarks? (2) If using a global watermark, what is the necessity of employing customized watermarks, or what is the adequate way to aggregate the global watermark from customized watermarks? If using local watermarks, how can the customized watermarks be used across clients?

3. The method requires specific GL models (to be split to multiple submodels), which can be hard to adapt to existing FGL methods, especially for advanced FGL methods.

4. The motivation for incorporating submodels for GL is missing. Why is this design necessary?

5. (1) What does “layer indexes” for splitting GL models mean? From section 3.3, it is not clear how the submodels are split and how the split submodels are decoupled from each other regarding cascaded structures. (2) Additionally, structural information can be important for graph learning. How would discarding such structural information impact in this setting?

6. The global model is obtained by simply averaging uploaded clients’ models (not weighted by data size, or applying proxy terms for regularization). Can this method address the potential heterogeneity issue when local watermarks are highly disparate from each other?

7. The proposed method can introduce efficiency issues, as it significantly increases the number of parameters and computation time.

Limitations:
Please see Weaknesses above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xeXRhTUmcf;"REVIEW 
Summary:
This paper introduces a new method for Out-of-Distribution detection based on the concepts of Lens Depth and Fermat distance. This method is used to see whether a sample has a similar representation in the penultimate layer of a Neural Network as the samples in the training data. The method is subjected to various tests of Out-of-Distribution detection and is shown to be on-par or exceeding alternative methods. However, the proposed method does not intrude on the training process of the model, and therefore cannot have a negative impact on the classification performance. Alternative methods assume a Gaussian Distribution in the hidden representation, but the use of (a modification of) Lens Depth allows estimating the “similarity” of the sample without assuming a certain distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The application of Femat Distance and Lens Depth introduces mathematical concepts that are not common knowledge and not obvious to a Machine Learning audience. The application of these methods in OoD detection is new (originality)
-  Previous literature is well cited, and the mathematical concepts are clearly and intuitively introduced, with clearly stated relevance (clarity)
The claims made follow naturally from the evidence and are not overstated. The evaluation is in line with common practice in the field of OoD detection (quality)
- The paper is well written and consistently builds a clear argumentation (clarity)
- Mathematical concepts are introduced with both formalism, and an intuitive explanation (clarity).
- The proposed method is competitive with other methods, and is minimally invasive to the training process. This could be helpful when then training process is outside of the control, for example for large pre-trained models (significance)

Weaknesses:
- Small claims are not entirely accurate. Line 4 says there are “no assumptions” about the form of the distribution, but there are only minimal assumptions (see question 3). Line 262 claims that the proposed measure is a good measure of “uncertainty estimation”, but it’s only evaluated for OoD detection, so it may be wildly over/underconfident and behave poorly on aleatoric uncertainty. Line 323 conjects that OoD detection may ensure fairness, but I see no reason why. Line 5 claims that the proposed method is applicable to any classification model, but the performance is only tested for Neural Networks (quality/clarity)
- The explanation of Lens Depth may be made more intuitive with a visualisation to support Lines 94-99 (clarity)
- Presented results are not substantially better than previous methods. Authors argue that the main benefit is that the proposed method is minimally invasive to the training process, but the authors do not make a strong case on why this is necessary (significance)

Limitations:
The authors claim that their method works on all classification models, and without any assumptions on the distribution of the data. However, this is missing evidence. Authors only demonstrate effectiveness in Neural Networks on Computer Vision data. While it is true that the method may be applied to other models and other data, more research is needed to establish its effectiveness there. Other limitations are demonstrated and addressed. The positive conclusions are appropriately based on the findings and are not over-optimistic. 

The authors discuss the high computational cost and demonstrate methods to make it more efficient, but it’s not clear what the remaining computational cost is.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a non-parametric approach to out-of-distribution (OOD) detection. Given a trained neural network classifier, it is proposed to combine the Lens Depth (LD) with the Fermat distance (in an improved form) to capture the geometry and density of the data in feature space. Without assuming any prior distribution, the paper classifies OOD samples for toys and small scale benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The combination of the Lens Depth with the sample Fermat distance for the out-of-distribution problem is a solid and interesting contribution. 
- The paper is well written and easy to follow. In general, the approach is clearly described.
- The results on small scale experiments are convincing. 
- The approach presented does not include the training process of the model.

Weaknesses:
- An extension of the related work to include papers on OOD would be necessary for the content of the paper. 
- An additional evaluation metric would be helpful, e.g. FPR-95, ECE. This point should be addressed. 
- A large-scale evaluation, e.g. ImageNet, is also missing. This is the main limitation of the paper.

Limitations:
The paper has a broader impact statement to discuss the idea of robust decision making.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new method for OOD detection/scoring based on the lens depth and Fermat distance, arguing that it has advantages over prior methods by being non-parametric, non-invasive, (almost) tuning-parameter-free, and quite effective in adapting to the unknown structure of the data to identify OOD points.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Subject matter is important
2. I found the paper really easy and fun to read.
3. 4.2 is a nice, simple, and practical modification—very natural and clearly successful!
4. Both the Lens Depth and Fermat Distance are nice, intuitive notions, and it is natural and fun to think about their combination!
5. I raise a number of conceptual issues below, but at the end of the day the demonstration of the method on standard data sets, comparing it to state-of-the-art methods, is fairly compelling, hence my high score.

Weaknesses:
1. LD is interesting and intuitive but what happens when the data falls into two disjoint clusters? Then won’t LD (with basically any distance I can think of, including Fermat distance) consider points in between those two clusters to be extremely central, despite the fact that, since they lie in neither distribution, they could reasonably be considered very OOD? Related: it seems the FD is infinite (whenever \beta>0) between two points separated by a region of zero density, suggesting that the sample version will be highly unstable in this setting, as it is should not converge at all but instead diverge to infinity. I see this is addressed in 4.4 by computing sample FD separately per cluster, but how were the clusters computed? Clustering is no trivial task, and given that things go wrong without clustering, I imagine S(x) in eq (4.2) depends rather heavily on the clustering. This (seems to me important) aspect of the proposed method seems underexplored/underexplained in the paper.
2. How does the convergence of the sample FD to the population FD depend on dimension? It’s a bit hard to believe it doesn’t suffer from some sort of curse of dimensionality, since it depends on a density and density estimation very much suffers from the curse of dimensionality. It seems many of the nice demonstrations of it in this paper occur in 2 dimensions (with the data lying nearly on a set of dimension 1), which doesn’t seem very representative of NN feature spaces.
3. Claim of “no trainable parameter” in the abstract is rather misleading, given the need for choosing both \alpha (ok there is a case made that maybe this isn’t too important) and the clustering.
4. Lit review is well-organized, but very focused on methods for NN OOD detection. The paper makes a big deal out of the method being non-intrusive, but another way of saying this is just that the proposed method is a way of scoring a point being OOD with respect to a distribution, which is a problem that, in general, has nothing to do with NNs or their feature representations. Surely there is a large body of work on outlier detection in statistics that could be considered in a similar light to this method, where one takes an off-the-shelf outlier detection method’s score and just applies it to the data transformed to be in the feature space of the NN? That is essentially what this paper is doing (though for a novel method, and I am not questioning its novelty). I just wonder what other existing methods are out there that could be doing something similar, even if they haven’t been explicitly applied to NNs.
5. Section 4.5 and Appendix E: choices II and III seem like they would rather seriously break the connection between the estimated LD and the true LD, since the k-means clustering will in general (and in typical circumstances) have clusters with very different numbers of points in them, so by reducing to the cluster centers (or center+’s), you are representing very different numbers of points with different centers. Another way to say it is that the density of the n points via methods II and III is quite different from that of the original N points (or via method I), and hence using them to compute the LD will be quite different in nature from using method I or the original N points. I would expect these methods (II and III) to not even have any kind of consistency property to the true LD of the original points, given their change in the density. 
6. I appreciated the authors’ honesty in reporting LL ratio results as being better than their method (of course, it comes with a more complex process), but it seems worth noting that it is substantially better. Since all the AUROC scores are close to 1, it is natural to look at. 1-AUROC (so smaller is better), in which case the LL ratio gets 0.006 and LD gets 0.029, almost 5x higher. I don’t think the authors were misleading in presenting these results, but I found the two sentences (lines 252-254) highlighting the challenges associated with the LL ratio to be a bit vague, and the results might be more convincing if those challenges were made more explicit (possibly in an appendix if there isn’t room in the main paper).
7. I don’t find Fig 5.2 very convincing, since the monotonicity here is a pretty weak property and no comparison is made with other methods—my guess would be that many methods satisfy monotonicity. Is that not the case?

Limitations:
I guess some of my points listed under “weaknesses” could be interpreted as limitations, and I would like to see them better addressed/discussed. If they are (even if the authors don’t change their method at all), that would raise my score.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors address the problem of out of distribution detection in supervised learning with particular focus on neural networks models. The developed method worj in some feature (embedding) space by measuring the statistical depth of the query point with respect to some reference set of points. The particular implementation combines lens depth function with Fermat distance. The authors validate the proposed approach in a series of experiments on simulated and real-world data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is very well-written and easy to follow.
* The considered problem is relevant for practice as there is a significant demand in efficient and non-intrusive methods for uncertainty quantification. 
* The proposed approach is solid with all the steps being properly motivated.
* The authors did a significant effort to do a comprehensive literature review, experimental evaluation and analysis, though all the steps were not fully successful (see Weaknesses and Questions below).

[After rebuttal comment] I appreciate the answer by the authors and increase my score to 6. My main concerns were addressed.

Weaknesses:
* While usage of statistical depth functions and distribution/manifold related distances looks logical, it is not clear why the particular choices of Lens Depth and Fermat distance were made.

* The baselines considered are not comprehensive enough and some of the baselines are not interpreted correctly by the authors of the present paper. In particular:
a. Non-Gaussianity of embedding distribution was directly considered in [1] aiming to improve over GDA. I think that is worth comparing with this method as the present paper target the same issue though with the completely different approach.
b. I believe that the authors incorrectly say that the difference between papers [2] and [3] is only in usage of spectral normalization. In my opinion, even more important is that [2] uses Mahalanobis distance as uncertainty measure while [3] considers the density of Gaussian mixture instead.

* The experiments are done with relatively simple datasets like CIFAR-10 for in-distribution data and SVHN/CIFAR-100/TinyImageNet as OOD. With the proposed approach being relatively lightweight, it is not clear why not to consider CIFAR-100/ImageNet as in-distribution with corresponding OOD choices (like ImageNet-R or ImageNet-O as OOD for ImageNet).

References 
[1] Kotelevskii, Nikita, et al. Nonparametric uncertainty quantification for single deterministic neural network. Advances in Neural Information Processing Systems 35 (2022): 36308-36323.
[2] K. Lee, K. Lee, H. Lee, and J. Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.
[3] J. Mukhoti, 362 A. Kirsch, J. van Amersfoort, P. H. Torr, and Y. Gal. Deep deterministic uncertainty: A new simple baseline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24384–24394, 2023.

Limitations:
Limitations are adequately addressed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new method for Out-of-Distribution detection based on the concepts of Lens Depth and Fermat distance. This method is used to see whether a sample has a similar representation in the penultimate layer of a Neural Network as the samples in the training data. The method is subjected to various tests of Out-of-Distribution detection and is shown to be on-par or exceeding alternative methods. However, the proposed method does not intrude on the training process of the model, and therefore cannot have a negative impact on the classification performance. Alternative methods assume a Gaussian Distribution in the hidden representation, but the use of (a modification of) Lens Depth allows estimating the “similarity” of the sample without assuming a certain distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The application of Femat Distance and Lens Depth introduces mathematical concepts that are not common knowledge and not obvious to a Machine Learning audience. The application of these methods in OoD detection is new (originality)
-  Previous literature is well cited, and the mathematical concepts are clearly and intuitively introduced, with clearly stated relevance (clarity)
The claims made follow naturally from the evidence and are not overstated. The evaluation is in line with common practice in the field of OoD detection (quality)
- The paper is well written and consistently builds a clear argumentation (clarity)
- Mathematical concepts are introduced with both formalism, and an intuitive explanation (clarity).
- The proposed method is competitive with other methods, and is minimally invasive to the training process. This could be helpful when then training process is outside of the control, for example for large pre-trained models (significance)

Weaknesses:
- Small claims are not entirely accurate. Line 4 says there are “no assumptions” about the form of the distribution, but there are only minimal assumptions (see question 3). Line 262 claims that the proposed measure is a good measure of “uncertainty estimation”, but it’s only evaluated for OoD detection, so it may be wildly over/underconfident and behave poorly on aleatoric uncertainty. Line 323 conjects that OoD detection may ensure fairness, but I see no reason why. Line 5 claims that the proposed method is applicable to any classification model, but the performance is only tested for Neural Networks (quality/clarity)
- The explanation of Lens Depth may be made more intuitive with a visualisation to support Lines 94-99 (clarity)
- Presented results are not substantially better than previous methods. Authors argue that the main benefit is that the proposed method is minimally invasive to the training process, but the authors do not make a strong case on why this is necessary (significance)

Limitations:
The authors claim that their method works on all classification models, and without any assumptions on the distribution of the data. However, this is missing evidence. Authors only demonstrate effectiveness in Neural Networks on Computer Vision data. While it is true that the method may be applied to other models and other data, more research is needed to establish its effectiveness there. Other limitations are demonstrated and addressed. The positive conclusions are appropriately based on the findings and are not over-optimistic. 

The authors discuss the high computational cost and demonstrate methods to make it more efficient, but it’s not clear what the remaining computational cost is.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a non-parametric approach to out-of-distribution (OOD) detection. Given a trained neural network classifier, it is proposed to combine the Lens Depth (LD) with the Fermat distance (in an improved form) to capture the geometry and density of the data in feature space. Without assuming any prior distribution, the paper classifies OOD samples for toys and small scale benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The combination of the Lens Depth with the sample Fermat distance for the out-of-distribution problem is a solid and interesting contribution. 
- The paper is well written and easy to follow. In general, the approach is clearly described.
- The results on small scale experiments are convincing. 
- The approach presented does not include the training process of the model.

Weaknesses:
- An extension of the related work to include papers on OOD would be necessary for the content of the paper. 
- An additional evaluation metric would be helpful, e.g. FPR-95, ECE. This point should be addressed. 
- A large-scale evaluation, e.g. ImageNet, is also missing. This is the main limitation of the paper.

Limitations:
The paper has a broader impact statement to discuss the idea of robust decision making.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new method for OOD detection/scoring based on the lens depth and Fermat distance, arguing that it has advantages over prior methods by being non-parametric, non-invasive, (almost) tuning-parameter-free, and quite effective in adapting to the unknown structure of the data to identify OOD points.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Subject matter is important
2. I found the paper really easy and fun to read.
3. 4.2 is a nice, simple, and practical modification—very natural and clearly successful!
4. Both the Lens Depth and Fermat Distance are nice, intuitive notions, and it is natural and fun to think about their combination!
5. I raise a number of conceptual issues below, but at the end of the day the demonstration of the method on standard data sets, comparing it to state-of-the-art methods, is fairly compelling, hence my high score.

Weaknesses:
1. LD is interesting and intuitive but what happens when the data falls into two disjoint clusters? Then won’t LD (with basically any distance I can think of, including Fermat distance) consider points in between those two clusters to be extremely central, despite the fact that, since they lie in neither distribution, they could reasonably be considered very OOD? Related: it seems the FD is infinite (whenever \beta>0) between two points separated by a region of zero density, suggesting that the sample version will be highly unstable in this setting, as it is should not converge at all but instead diverge to infinity. I see this is addressed in 4.4 by computing sample FD separately per cluster, but how were the clusters computed? Clustering is no trivial task, and given that things go wrong without clustering, I imagine S(x) in eq (4.2) depends rather heavily on the clustering. This (seems to me important) aspect of the proposed method seems underexplored/underexplained in the paper.
2. How does the convergence of the sample FD to the population FD depend on dimension? It’s a bit hard to believe it doesn’t suffer from some sort of curse of dimensionality, since it depends on a density and density estimation very much suffers from the curse of dimensionality. It seems many of the nice demonstrations of it in this paper occur in 2 dimensions (with the data lying nearly on a set of dimension 1), which doesn’t seem very representative of NN feature spaces.
3. Claim of “no trainable parameter” in the abstract is rather misleading, given the need for choosing both \alpha (ok there is a case made that maybe this isn’t too important) and the clustering.
4. Lit review is well-organized, but very focused on methods for NN OOD detection. The paper makes a big deal out of the method being non-intrusive, but another way of saying this is just that the proposed method is a way of scoring a point being OOD with respect to a distribution, which is a problem that, in general, has nothing to do with NNs or their feature representations. Surely there is a large body of work on outlier detection in statistics that could be considered in a similar light to this method, where one takes an off-the-shelf outlier detection method’s score and just applies it to the data transformed to be in the feature space of the NN? That is essentially what this paper is doing (though for a novel method, and I am not questioning its novelty). I just wonder what other existing methods are out there that could be doing something similar, even if they haven’t been explicitly applied to NNs.
5. Section 4.5 and Appendix E: choices II and III seem like they would rather seriously break the connection between the estimated LD and the true LD, since the k-means clustering will in general (and in typical circumstances) have clusters with very different numbers of points in them, so by reducing to the cluster centers (or center+’s), you are representing very different numbers of points with different centers. Another way to say it is that the density of the n points via methods II and III is quite different from that of the original N points (or via method I), and hence using them to compute the LD will be quite different in nature from using method I or the original N points. I would expect these methods (II and III) to not even have any kind of consistency property to the true LD of the original points, given their change in the density. 
6. I appreciated the authors’ honesty in reporting LL ratio results as being better than their method (of course, it comes with a more complex process), but it seems worth noting that it is substantially better. Since all the AUROC scores are close to 1, it is natural to look at. 1-AUROC (so smaller is better), in which case the LL ratio gets 0.006 and LD gets 0.029, almost 5x higher. I don’t think the authors were misleading in presenting these results, but I found the two sentences (lines 252-254) highlighting the challenges associated with the LL ratio to be a bit vague, and the results might be more convincing if those challenges were made more explicit (possibly in an appendix if there isn’t room in the main paper).
7. I don’t find Fig 5.2 very convincing, since the monotonicity here is a pretty weak property and no comparison is made with other methods—my guess would be that many methods satisfy monotonicity. Is that not the case?

Limitations:
I guess some of my points listed under “weaknesses” could be interpreted as limitations, and I would like to see them better addressed/discussed. If they are (even if the authors don’t change their method at all), that would raise my score.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors address the problem of out of distribution detection in supervised learning with particular focus on neural networks models. The developed method worj in some feature (embedding) space by measuring the statistical depth of the query point with respect to some reference set of points. The particular implementation combines lens depth function with Fermat distance. The authors validate the proposed approach in a series of experiments on simulated and real-world data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is very well-written and easy to follow.
* The considered problem is relevant for practice as there is a significant demand in efficient and non-intrusive methods for uncertainty quantification. 
* The proposed approach is solid with all the steps being properly motivated.
* The authors did a significant effort to do a comprehensive literature review, experimental evaluation and analysis, though all the steps were not fully successful (see Weaknesses and Questions below).

[After rebuttal comment] I appreciate the answer by the authors and increase my score to 6. My main concerns were addressed.

Weaknesses:
* While usage of statistical depth functions and distribution/manifold related distances looks logical, it is not clear why the particular choices of Lens Depth and Fermat distance were made.

* The baselines considered are not comprehensive enough and some of the baselines are not interpreted correctly by the authors of the present paper. In particular:
a. Non-Gaussianity of embedding distribution was directly considered in [1] aiming to improve over GDA. I think that is worth comparing with this method as the present paper target the same issue though with the completely different approach.
b. I believe that the authors incorrectly say that the difference between papers [2] and [3] is only in usage of spectral normalization. In my opinion, even more important is that [2] uses Mahalanobis distance as uncertainty measure while [3] considers the density of Gaussian mixture instead.

* The experiments are done with relatively simple datasets like CIFAR-10 for in-distribution data and SVHN/CIFAR-100/TinyImageNet as OOD. With the proposed approach being relatively lightweight, it is not clear why not to consider CIFAR-100/ImageNet as in-distribution with corresponding OOD choices (like ImageNet-R or ImageNet-O as OOD for ImageNet).

References 
[1] Kotelevskii, Nikita, et al. Nonparametric uncertainty quantification for single deterministic neural network. Advances in Neural Information Processing Systems 35 (2022): 36308-36323.
[2] K. Lee, K. Lee, H. Lee, and J. Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.
[3] J. Mukhoti, 362 A. Kirsch, J. van Amersfoort, P. H. Torr, and Y. Gal. Deep deterministic uncertainty: A new simple baseline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24384–24394, 2023.

Limitations:
Limitations are adequately addressed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xcqSOfHt4g;"REVIEW 
Summary:
This paper proposes a new framework for masked diffusion models for generative modeling of discrete data. Masked diffusion models offer an alternative to autoregressive models for discrete data but have faced challenges due to complex formulations and unclear relationships between different approaches. This paper presents a simplified and generalized framework to address these issues, enhancing the performance and training of masked diffusion models.

The key contributions includes:

1. Simplification of Model Formulation: The paper establishes properties for the forward process and its time reversal using elementary arguments, provides a simple expression for the Evidence Lower Bound (ELBO), demonstrating it as a weighted integral over time of cross-entropy losses, and shows invariance properties similar to continuous space diffusions.

2. Re-derivation of Training Objectives: The paper demonstrates how various previously proposed discrete diffusion training objectives can be derived from the ELBO objective by altering parameterization, relaxing constraints, or modifying loss weighting.

3. Performance Improvements: The paper demonstrates state-of-the-art likelihood and zero-shot transfer results on text and image tasks using the proposed ELBO objective.

4. Generalized Masked Diffusion Model: The paper proposes a generalized masked diffusion model that allows state-dependent masking schedules, further improving predictive performance on test likelihoods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper makes a notable contribution to the field of generative modeling for discrete data by introducing a simplified and generalized framework for masked diffusion models.
2. The quality of the paper is reflected in the thoroughness of its methodology and the robustness of its experimental validation.
3. The paper is well-written and clearly structured, making it accessible to both experts and those new to the field.
4. The significance of the paper lies in its potential to impact a wide range of applications in generative modeling for discrete data.

Weaknesses:
1. While the paper provides a robust theoretical foundation, there could be more emphasis on practical applicability. The paper could benefit from additional practical guidelines for implementing the proposed framework, such as more detailed pseudocode and specific implementation challenges.
2. The experimental results presented are strong, but the range of tasks and datasets could be expanded, such as VQ-Diffusion [1] for token-based text-to-image.
3. I am unfamiliar with diffusion models for text generation. For image generation, the paper has reported likelihood results, missing some other common metrics, such as FID and IS.

[1] Vector Quantized Diffusion Model for Text-to-Image Synthesis

Limitations:
The authors have adequately described the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper simplifies the mathematical formula for the absorbing state diffusion process. By doing so, the authors derive a continuous-time ELBO for masked diffusion models. Their method, MD4, achieves better perplexity scores than SEDD on text8 and zero-shot perplexity on numerous datasets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
Simplifies the complex mathematical formulations for the absorbing state diffusion for D3PM.

Weaknesses:
Weaknesses:

1. Weak empirical results
    1. The zero shot numbers for D3PM in Table 1 look fishy.  There are only 2 differences between Md4 and Absorbing State D3PM:
        1. Mathematical simplification. In the discrete case (Eqn. 6), even though MD4 features a Simplified functional form for the ELBO, it shouldn't give it any performance benefits in terms of perplexity since it is mathematically equivalent to D3PM.
        2. The improvement in ELBO could be because of the continuous time formulation. However, VDM [1]  has shown that for gaussian diffusion, improvement from discrete (T=1000) to continuous time (T = $\infty$) barely improves the likelihood by less than 1%. For this reason, I request the authors to perform eval on an already trained model and report the  perplexity numbers on text8 or OWT using Eqn (6) with T=100, 1000, 10000. If the numbers reported for D3PM in Table 1 are indeed correct, and if the entire improvement is coming from the continuous time formulation, then the discrete time MD4 should get a number that's comparable to D3PM's zero shot ppl numbers. 
        
        Questions: How did they retrain D3PM? Did they use the same transformer backbone as MD4? Did they use the same model size and data pre-processing scheme? Did they use  uniform state or absorbing state diffusion process? The authors need to clarify this.
        
    2. CIFAR10 Experiments. The AR baselines use old transformer models hence the comparison isn't quite fair. Current SOTA diffusion models on Imagenet 32 achieve a NLL of 2.55 [2] which is far better than the absorbing state diffusion models. So, I'm unsure about the takeaway from Table 3. In the conclusion section, the authors claim that ""… on text and image data, the resulting masked diffusions outperform existing discrete and continuous diffusion models …"" which is factually incorrect given that their method largely underperforms against gaussian diffusion [1, 2].
2. Limited evaluation of GenMD4. The authors mention that GenMD4 performs poorly on zero-shot tasks. I request the authors to quantify this poor performance by providing 
    1. Validation ppl numbers on OWT
    2. zero-shot ppl numbers.

[1] Kingma, D., Salimans, T., Poole, B. and Ho, J., 2021. Variational diffusion models. *Advances in neural information processing systems*, *34*, pp.21696-21707.

[2] Sahoo, S., Gokaslan, A., Sa, C., Kuleshov, V., 2024.  Diffusion Models With Learned Adaptive Noise. arXiv:2406.07524

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a streamlined and generalized framework for masked diffusion models, addressing the complexities and inefficiencies of existing models, including those based on Score Entropy Discrete Diffusion (SEDD). It introduces a continuous-time variational objective for masked diffusion models, simplifying the evidence lower bound (ELBO) to a weighted integral of cross-entropy losses. Additionally, the paper presents state-dependent masking schedules, enhancing the flexibility and performance of these models. The proposed methods demonstrate state-of-the-art results in text and image tasks, significantly improving likelihood and zero-shot transfer performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper offers a novel theoretical formulation of the continuous-time variational objective for masked diffusion models, simplifying the training process and ensuring consistency between forward and reverse processes.
- The introduction of state-dependent masking schedules provides a more adaptable approach, catering to the specific characteristics of the data and improving model performance.
- The proposed methods achieve state-of-the-art performance in both text and image generative tasks, significantly enhancing likelihood and zero-shot transfer capabilities.
- By reducing the ELBO to a weighted integral of cross-entropy losses, the paper makes the training and understanding of masked diffusion models more accessible and potentially more stable.
- The paper includes comprehensive experimental validation on various datasets, demonstrating the robustness and superiority of the proposed methods.

Weaknesses:
-  Despite the theoretical simplifications, the practical implementation of state-dependent masking schedules can still be complex and computationally demanding. Specifically, obtaining the starting x_T is challenging, and since the sampling process lacks stochasticity, sampling cannot be done from the completely masked state.
- The state-dependent models have a tendency to overfit to dataset statistics, which can limit their effectiveness in zero-shot transfer tasks.
- While the paper demonstrates superior performance, a more detailed comparative analysis with other state-of-the-art methods, particularly regarding computational efficiency and training times, would provide a clearer picture of the advantages.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Summary: This paper introduces a framework for masked diffusions that consolidates previous research on the topic and organizes it into a cohesive structure. The authors also present a generalized model within this framework, which enables the use of state-dependent masking schedules and optimization of scheduler parameters.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The GenMD4 framework offers a valuable approach to optimize the forward process. In earlier studies, forward processes were typically manually designed and set within the model. However, GenMD4 adjusts the forward distribution to align with the estimated distribution, thereby improving the forward process. This innovation may serve as a source of inspiration for developing more effective forward processes.

2. This paper summarizes previous formulations of masked diffusion models and establishes the connections between them.

Weaknesses:
1. In line 90. The handling of $p(x _0|x _{t(1)})$ could be enhanced. Assuming $p(x _0|x _{t(1)}) \propto q(x _{t(1)} | x _0)$ is equivalent to assuming that $q(x _0)$ is uniformly distributed. In reality, it should be treated the same as other $p(x _s|x _t)$.
2. In line 114. When discussing multidimensional data, it is not straightforward to assume that the backward process factorizes across tokens. This is because the distribution $p(x _0)$ does not factorize across tokens. Achieving factorization necessitates a small time step dt, which may not be easily observable. Additionally, in the previous single-token scenario, dt dose not need to be small, indicating that one step is sufficient to model the distribution $p(x _0 | x _1)$. This aspect is crucial for multidimensional data and should be emphasized in a fundamental paper like this.
3. In append F. The presence of a non-zero $\alpha _1$ may result in the ""medium brightness problem"" [1]. However, there is no singularity when $\alpha _1$ is zero if log-SNR is not introduced, and the time interval can be extended to [0, 1].
4. In append G2. When applied to masked diffusion, $R_{kj}$ is zero when $ j \ne k$ and $j \ne m$. Given that $R_{kk} + R_{km} = 0$, $\tilde{q}$ can only take on one value (m), resulting in no additional variance.
5. In image experiments, MD4 employs masked noise, while $\tau$LDR uses Gaussian noise. We recommend conducting experiments with the same noise scheduler to demonstrate conclusively that MD4 is superior. If the goal of this paper is solely to establish that masked noise outperforms Gaussian noise, we recommend explicitly stating this claim. Additionally, we advise detailing the sampling method, as variations in methodology can influence the quality of generated samples.

[1] Common Diffusion Noise Schedules and Sample Steps are Flawed, Lin et al., 2024

Limitations:
The method is only applied to masked diffusions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new framework for masked diffusion models for generative modeling of discrete data. Masked diffusion models offer an alternative to autoregressive models for discrete data but have faced challenges due to complex formulations and unclear relationships between different approaches. This paper presents a simplified and generalized framework to address these issues, enhancing the performance and training of masked diffusion models.

The key contributions includes:

1. Simplification of Model Formulation: The paper establishes properties for the forward process and its time reversal using elementary arguments, provides a simple expression for the Evidence Lower Bound (ELBO), demonstrating it as a weighted integral over time of cross-entropy losses, and shows invariance properties similar to continuous space diffusions.

2. Re-derivation of Training Objectives: The paper demonstrates how various previously proposed discrete diffusion training objectives can be derived from the ELBO objective by altering parameterization, relaxing constraints, or modifying loss weighting.

3. Performance Improvements: The paper demonstrates state-of-the-art likelihood and zero-shot transfer results on text and image tasks using the proposed ELBO objective.

4. Generalized Masked Diffusion Model: The paper proposes a generalized masked diffusion model that allows state-dependent masking schedules, further improving predictive performance on test likelihoods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper makes a notable contribution to the field of generative modeling for discrete data by introducing a simplified and generalized framework for masked diffusion models.
2. The quality of the paper is reflected in the thoroughness of its methodology and the robustness of its experimental validation.
3. The paper is well-written and clearly structured, making it accessible to both experts and those new to the field.
4. The significance of the paper lies in its potential to impact a wide range of applications in generative modeling for discrete data.

Weaknesses:
1. While the paper provides a robust theoretical foundation, there could be more emphasis on practical applicability. The paper could benefit from additional practical guidelines for implementing the proposed framework, such as more detailed pseudocode and specific implementation challenges.
2. The experimental results presented are strong, but the range of tasks and datasets could be expanded, such as VQ-Diffusion [1] for token-based text-to-image.
3. I am unfamiliar with diffusion models for text generation. For image generation, the paper has reported likelihood results, missing some other common metrics, such as FID and IS.

[1] Vector Quantized Diffusion Model for Text-to-Image Synthesis

Limitations:
The authors have adequately described the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper simplifies the mathematical formula for the absorbing state diffusion process. By doing so, the authors derive a continuous-time ELBO for masked diffusion models. Their method, MD4, achieves better perplexity scores than SEDD on text8 and zero-shot perplexity on numerous datasets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
Simplifies the complex mathematical formulations for the absorbing state diffusion for D3PM.

Weaknesses:
Weaknesses:

1. Weak empirical results
    1. The zero shot numbers for D3PM in Table 1 look fishy.  There are only 2 differences between Md4 and Absorbing State D3PM:
        1. Mathematical simplification. In the discrete case (Eqn. 6), even though MD4 features a Simplified functional form for the ELBO, it shouldn't give it any performance benefits in terms of perplexity since it is mathematically equivalent to D3PM.
        2. The improvement in ELBO could be because of the continuous time formulation. However, VDM [1]  has shown that for gaussian diffusion, improvement from discrete (T=1000) to continuous time (T = $\infty$) barely improves the likelihood by less than 1%. For this reason, I request the authors to perform eval on an already trained model and report the  perplexity numbers on text8 or OWT using Eqn (6) with T=100, 1000, 10000. If the numbers reported for D3PM in Table 1 are indeed correct, and if the entire improvement is coming from the continuous time formulation, then the discrete time MD4 should get a number that's comparable to D3PM's zero shot ppl numbers. 
        
        Questions: How did they retrain D3PM? Did they use the same transformer backbone as MD4? Did they use the same model size and data pre-processing scheme? Did they use  uniform state or absorbing state diffusion process? The authors need to clarify this.
        
    2. CIFAR10 Experiments. The AR baselines use old transformer models hence the comparison isn't quite fair. Current SOTA diffusion models on Imagenet 32 achieve a NLL of 2.55 [2] which is far better than the absorbing state diffusion models. So, I'm unsure about the takeaway from Table 3. In the conclusion section, the authors claim that ""… on text and image data, the resulting masked diffusions outperform existing discrete and continuous diffusion models …"" which is factually incorrect given that their method largely underperforms against gaussian diffusion [1, 2].
2. Limited evaluation of GenMD4. The authors mention that GenMD4 performs poorly on zero-shot tasks. I request the authors to quantify this poor performance by providing 
    1. Validation ppl numbers on OWT
    2. zero-shot ppl numbers.

[1] Kingma, D., Salimans, T., Poole, B. and Ho, J., 2021. Variational diffusion models. *Advances in neural information processing systems*, *34*, pp.21696-21707.

[2] Sahoo, S., Gokaslan, A., Sa, C., Kuleshov, V., 2024.  Diffusion Models With Learned Adaptive Noise. arXiv:2406.07524

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a streamlined and generalized framework for masked diffusion models, addressing the complexities and inefficiencies of existing models, including those based on Score Entropy Discrete Diffusion (SEDD). It introduces a continuous-time variational objective for masked diffusion models, simplifying the evidence lower bound (ELBO) to a weighted integral of cross-entropy losses. Additionally, the paper presents state-dependent masking schedules, enhancing the flexibility and performance of these models. The proposed methods demonstrate state-of-the-art results in text and image tasks, significantly improving likelihood and zero-shot transfer performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper offers a novel theoretical formulation of the continuous-time variational objective for masked diffusion models, simplifying the training process and ensuring consistency between forward and reverse processes.
- The introduction of state-dependent masking schedules provides a more adaptable approach, catering to the specific characteristics of the data and improving model performance.
- The proposed methods achieve state-of-the-art performance in both text and image generative tasks, significantly enhancing likelihood and zero-shot transfer capabilities.
- By reducing the ELBO to a weighted integral of cross-entropy losses, the paper makes the training and understanding of masked diffusion models more accessible and potentially more stable.
- The paper includes comprehensive experimental validation on various datasets, demonstrating the robustness and superiority of the proposed methods.

Weaknesses:
-  Despite the theoretical simplifications, the practical implementation of state-dependent masking schedules can still be complex and computationally demanding. Specifically, obtaining the starting x_T is challenging, and since the sampling process lacks stochasticity, sampling cannot be done from the completely masked state.
- The state-dependent models have a tendency to overfit to dataset statistics, which can limit their effectiveness in zero-shot transfer tasks.
- While the paper demonstrates superior performance, a more detailed comparative analysis with other state-of-the-art methods, particularly regarding computational efficiency and training times, would provide a clearer picture of the advantages.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Summary: This paper introduces a framework for masked diffusions that consolidates previous research on the topic and organizes it into a cohesive structure. The authors also present a generalized model within this framework, which enables the use of state-dependent masking schedules and optimization of scheduler parameters.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The GenMD4 framework offers a valuable approach to optimize the forward process. In earlier studies, forward processes were typically manually designed and set within the model. However, GenMD4 adjusts the forward distribution to align with the estimated distribution, thereby improving the forward process. This innovation may serve as a source of inspiration for developing more effective forward processes.

2. This paper summarizes previous formulations of masked diffusion models and establishes the connections between them.

Weaknesses:
1. In line 90. The handling of $p(x _0|x _{t(1)})$ could be enhanced. Assuming $p(x _0|x _{t(1)}) \propto q(x _{t(1)} | x _0)$ is equivalent to assuming that $q(x _0)$ is uniformly distributed. In reality, it should be treated the same as other $p(x _s|x _t)$.
2. In line 114. When discussing multidimensional data, it is not straightforward to assume that the backward process factorizes across tokens. This is because the distribution $p(x _0)$ does not factorize across tokens. Achieving factorization necessitates a small time step dt, which may not be easily observable. Additionally, in the previous single-token scenario, dt dose not need to be small, indicating that one step is sufficient to model the distribution $p(x _0 | x _1)$. This aspect is crucial for multidimensional data and should be emphasized in a fundamental paper like this.
3. In append F. The presence of a non-zero $\alpha _1$ may result in the ""medium brightness problem"" [1]. However, there is no singularity when $\alpha _1$ is zero if log-SNR is not introduced, and the time interval can be extended to [0, 1].
4. In append G2. When applied to masked diffusion, $R_{kj}$ is zero when $ j \ne k$ and $j \ne m$. Given that $R_{kk} + R_{km} = 0$, $\tilde{q}$ can only take on one value (m), resulting in no additional variance.
5. In image experiments, MD4 employs masked noise, while $\tau$LDR uses Gaussian noise. We recommend conducting experiments with the same noise scheduler to demonstrate conclusively that MD4 is superior. If the goal of this paper is solely to establish that masked noise outperforms Gaussian noise, we recommend explicitly stating this claim. Additionally, we advise detailing the sampling method, as variations in methodology can influence the quality of generated samples.

[1] Common Diffusion Noise Schedules and Sample Steps are Flawed, Lin et al., 2024

Limitations:
The method is only applied to masked diffusions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xcF2VbyZts;"REVIEW 
Summary:
Paper proposes a pipeline method of orchestration pre-trained foundation models to solve the social relationship classification problem. It uses vision models to extract information in text about the scene in the form of caption. Relevant information, i.e. age, gender, general description, of individual persons and objects are also extracted in text form via instance segmentation + masking + captioning. The generated text are then further converted to Social Story with a LLM. With the novel prompt engineering method, GSPO, another LLM will then generate the social relationship from the Social Story.

Experimental results on the challenging benchmarks, PIPA and PISC, indicates its strong performance with zero-shot setup. Extensive ablation studies were also done to evaluate the contributions of the various components. In particular, it clearly showed the merits of the ""Social Story"" design.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Paper proposed a novel method to solve the challenging social relationship classification problem. The proposed method cleverly combine several state-of-the-art foundation models in a logical, intuitive, and yet non-obvious design to achieve state-of-the-arts experimental results.

Weaknesses:
1. Besides the clever design of the pipeline, the direct technical contributions is slightly on the weaker side as there is no obvious technical breakthrough. The proposed GSPO appears to be the main new technique introduced. However, I am not an expert in this area and will defer to other reviewers on its technical novelty and merits.

2. (minor) The use of the generic semantic segmentation model (SAM) may not be the optimal choice. There are much stronger Human Instance Segmentation methods which can replace the paper's custom SAM method. Such methods are specifically trained on person dataset to handle various challenging scenarios unique to human segmentation, e.g. heavy occulsion, human-like objects (e.g. maniquinn).

Ling, E., Huang, D., & Hur, M. (2022). Humans need not label more humans: Occlusion copy & paste for occluded human instance segmentation. BMVC.

Limitations:
(minor) There may be some unintended negative consequences of automatic classification of social relationship.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces SocialGPT, a modular framework for social relation reasoning that integrates the perception capabilities of Vision Foundation Models (VFMs) with the reasoning capabilities of Large Language Models (LLMs). To optimize prompts, the authors propose GSPO, a segment-based optimization algorithm for automated prompt tuning. Extensive empirical results validate the effectiveness of SocialGPT both quantitatively and qualitatively. GSPO consistently enhances SocialGPT's performance across various LLMs, and case studies demonstrate the framework's generalizability and interpretability.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-organized, with a logical flow and clear explanations of each step.
- The proposed SocialGPT framework innovatively combines perception from VFMs with reasoning from LLMs, achieving competitive zero-shot performance and offering potential explanations for its reasoning process.
- Extensive experiments, ablation studies, and case studies comprehensively evaluate the framework's effectiveness.

Weaknesses:
Section 3.2 mentions that using precise coordinates can pose challenges for LLM numerical reasoning. However, it appears in Figure 3 that the objects' positional relations in the social story are inferred from numeric coordinates provided in the dense captions with symbols. Does this coordinate-based inference lead to similar numerical reasoning challenges? Additionally, how are relative positional relations conveyed here using referral symbols?

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a framework called SocialGPT for social relation reasoning, which combines vision foundation models and large language models. A greeedy segment prompt optimization methods is also proposed to prompt LLM. Experimental results show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
---The paper is well organized and written. 

---The idea of combining VFMs and LLMs is reasonable.

Weaknesses:
--- The paradigm of using VLMs for perceiving and LLMs for reasoning is currently a common solution for multimodal tasks. The main difference of this paper seems to be the use of a generated social story as the representation of visual content. As stated by the authors, LLMs perform best when working with human-readable natural language and often struggle with arithmetic reasoning tasks, which is why they design an additional process to generate social stories. However, the generation of social stories is also done by LLMs, which also suffer from the above difficulties. 

--- The authors propose a candidate set consisting of alternative prompts for each segment and select the best-performing prompt from their combination. The final prompt is obtained by selection rather than generation, which limits the upper bound of the performance on the manually collected candidate set. 

--- The function of SAM is to distinguish individuals in the image and obtain their coordinates. However, in the social story generation phase, the LLM (Large Language Model) discards the coordinates, retaining only the semantic information and losing the positional information. Conducting social relationship reasoning purely based on semantics may be insufficient. For example, in Figure 2, the social relationship is identified as a sibling relationship (brother and sister), but there are two boys in the image, both fitting the given description of ""stands out in his vibrant red and green striped pajamas,"" making it unclear which individual P1 refers to.

Limitations:
Please see the weakness and limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This manuscript introduces SocialGPT, a modular framework designed to enhance social relation reasoning by combining Vision Foundation Models (VFMs) and Large Language Models (LLMs). SocialGPT utilizes VFMs to convert image content into a textual social story, followed by LLMs performing text-based reasoning. The paper further introduces the Greedy Segment Prompt Optimization (GSPO) algorithm to optimize prompts for LLMs, addressing the challenges of long prompt optimization. The proposed method achieves competitive zero-shot results on social relation recognition tasks and offers interpretable answers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The GSPO algorithm provides an efficient method for optimizing long prompts, significantly improving the performance of LLMs in social relation reasoning tasks.
- SocialGPT achieves competitive zero-shot results on PIPA and PISC datasets, demonstrating the effectiveness of the proposed approach without additional model training.
- By leveraging LLMs for reasoning, SocialGPT can generate language-based explanations for its decisions, enhancing the interpretability of the results.

Weaknesses:
- The approach involves substantial computational resources for both the perception and reasoning phases, potentially limiting accessibility and scalability for some users.
- The experiments, while promising, are primarily conducted on two datasets. Further testing on a broader range of datasets and tasks would strengthen the generalizability of the findings.
- The method assumes that the visual context provided by VFMs is sufficiently detailed and accurate, which might not always hold true in diverse real-world scenarios.
- The compatibility of the proposed method seems to be limited; Table 5 implies that LLaMA2-based SocialGPT performs very poorly compared to Vicuna. The proposed framework may work only for specific types of models.

Limitations:
See the weakness section and question above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Paper proposes a pipeline method of orchestration pre-trained foundation models to solve the social relationship classification problem. It uses vision models to extract information in text about the scene in the form of caption. Relevant information, i.e. age, gender, general description, of individual persons and objects are also extracted in text form via instance segmentation + masking + captioning. The generated text are then further converted to Social Story with a LLM. With the novel prompt engineering method, GSPO, another LLM will then generate the social relationship from the Social Story.

Experimental results on the challenging benchmarks, PIPA and PISC, indicates its strong performance with zero-shot setup. Extensive ablation studies were also done to evaluate the contributions of the various components. In particular, it clearly showed the merits of the ""Social Story"" design.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Paper proposed a novel method to solve the challenging social relationship classification problem. The proposed method cleverly combine several state-of-the-art foundation models in a logical, intuitive, and yet non-obvious design to achieve state-of-the-arts experimental results.

Weaknesses:
1. Besides the clever design of the pipeline, the direct technical contributions is slightly on the weaker side as there is no obvious technical breakthrough. The proposed GSPO appears to be the main new technique introduced. However, I am not an expert in this area and will defer to other reviewers on its technical novelty and merits.

2. (minor) The use of the generic semantic segmentation model (SAM) may not be the optimal choice. There are much stronger Human Instance Segmentation methods which can replace the paper's custom SAM method. Such methods are specifically trained on person dataset to handle various challenging scenarios unique to human segmentation, e.g. heavy occulsion, human-like objects (e.g. maniquinn).

Ling, E., Huang, D., & Hur, M. (2022). Humans need not label more humans: Occlusion copy & paste for occluded human instance segmentation. BMVC.

Limitations:
(minor) There may be some unintended negative consequences of automatic classification of social relationship.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces SocialGPT, a modular framework for social relation reasoning that integrates the perception capabilities of Vision Foundation Models (VFMs) with the reasoning capabilities of Large Language Models (LLMs). To optimize prompts, the authors propose GSPO, a segment-based optimization algorithm for automated prompt tuning. Extensive empirical results validate the effectiveness of SocialGPT both quantitatively and qualitatively. GSPO consistently enhances SocialGPT's performance across various LLMs, and case studies demonstrate the framework's generalizability and interpretability.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-organized, with a logical flow and clear explanations of each step.
- The proposed SocialGPT framework innovatively combines perception from VFMs with reasoning from LLMs, achieving competitive zero-shot performance and offering potential explanations for its reasoning process.
- Extensive experiments, ablation studies, and case studies comprehensively evaluate the framework's effectiveness.

Weaknesses:
Section 3.2 mentions that using precise coordinates can pose challenges for LLM numerical reasoning. However, it appears in Figure 3 that the objects' positional relations in the social story are inferred from numeric coordinates provided in the dense captions with symbols. Does this coordinate-based inference lead to similar numerical reasoning challenges? Additionally, how are relative positional relations conveyed here using referral symbols?

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a framework called SocialGPT for social relation reasoning, which combines vision foundation models and large language models. A greeedy segment prompt optimization methods is also proposed to prompt LLM. Experimental results show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
---The paper is well organized and written. 

---The idea of combining VFMs and LLMs is reasonable.

Weaknesses:
--- The paradigm of using VLMs for perceiving and LLMs for reasoning is currently a common solution for multimodal tasks. The main difference of this paper seems to be the use of a generated social story as the representation of visual content. As stated by the authors, LLMs perform best when working with human-readable natural language and often struggle with arithmetic reasoning tasks, which is why they design an additional process to generate social stories. However, the generation of social stories is also done by LLMs, which also suffer from the above difficulties. 

--- The authors propose a candidate set consisting of alternative prompts for each segment and select the best-performing prompt from their combination. The final prompt is obtained by selection rather than generation, which limits the upper bound of the performance on the manually collected candidate set. 

--- The function of SAM is to distinguish individuals in the image and obtain their coordinates. However, in the social story generation phase, the LLM (Large Language Model) discards the coordinates, retaining only the semantic information and losing the positional information. Conducting social relationship reasoning purely based on semantics may be insufficient. For example, in Figure 2, the social relationship is identified as a sibling relationship (brother and sister), but there are two boys in the image, both fitting the given description of ""stands out in his vibrant red and green striped pajamas,"" making it unclear which individual P1 refers to.

Limitations:
Please see the weakness and limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This manuscript introduces SocialGPT, a modular framework designed to enhance social relation reasoning by combining Vision Foundation Models (VFMs) and Large Language Models (LLMs). SocialGPT utilizes VFMs to convert image content into a textual social story, followed by LLMs performing text-based reasoning. The paper further introduces the Greedy Segment Prompt Optimization (GSPO) algorithm to optimize prompts for LLMs, addressing the challenges of long prompt optimization. The proposed method achieves competitive zero-shot results on social relation recognition tasks and offers interpretable answers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The GSPO algorithm provides an efficient method for optimizing long prompts, significantly improving the performance of LLMs in social relation reasoning tasks.
- SocialGPT achieves competitive zero-shot results on PIPA and PISC datasets, demonstrating the effectiveness of the proposed approach without additional model training.
- By leveraging LLMs for reasoning, SocialGPT can generate language-based explanations for its decisions, enhancing the interpretability of the results.

Weaknesses:
- The approach involves substantial computational resources for both the perception and reasoning phases, potentially limiting accessibility and scalability for some users.
- The experiments, while promising, are primarily conducted on two datasets. Further testing on a broader range of datasets and tasks would strengthen the generalizability of the findings.
- The method assumes that the visual context provided by VFMs is sufficiently detailed and accurate, which might not always hold true in diverse real-world scenarios.
- The compatibility of the proposed method seems to be limited; Table 5 implies that LLaMA2-based SocialGPT performs very poorly compared to Vicuna. The proposed framework may work only for specific types of models.

Limitations:
See the weakness section and question above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xbuaSTqAEz;"REVIEW 
Summary:
This paper incorporates a multi-model subspace proxy learning (Multi-Sub) to design a novel end-to-end multiple clustering approach and utilizes the synergistic capabilities of CLIP and GPT-4 to align textual prompts expressing user preferences with corresponding visual representations. The main contributions of Multi-Sub can be summarized as follows:
1. Capturing user’s clustering interest: Existing works struggle to adapt to diverse user-specific needs in data grouping. To overcome this limitation, Multi-Sub explicitly captures a user’s clustering interest by learning the desired clustering proxy under a user’s interest and aligning textual interest with corresponding visual features.
2. Simultaneous optimized framework: The previous methods separated the representation learning and clustering stages. Different from them, Multi-Sub obtains both the desired representations and clustering simultaneously, which significantly improve the clustering performance and efficiency.
3. Extensive experimental validation: Extensive experiments on all public multiple clustering tasks demonstrate that Multi-Sub outperform other methods. Moreover, a series of ablation studies further verify the effectiveness of Multi-Sub.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. In real world, data may have multiple aspects that they can be grouped into different clusters. However, existing methods solely consider a single partition. So, it is meaningful to propose an effective algorithm to overcome this problem.
2. The authors leveraged large language models (LLMs), including GPT-4 and CLIP, to align image and textual representations in the same subspace. Then, multi-modal subspace proxy learning is introduced to allow for the customized representation of data in terms specific to the user’s interests.
3. Experimental results on public datasets show that the Multi-Sub method has a significant improvement, indicating the effectiveness of the propose method.

Weaknesses:
1. To change the two-stage learning approach of previous works, Multi-Sub aims to learn representation and clustering simultaneously. However, Multi-Sub employs a two-phase iterative approach to align and cluster images in training process, including (1) Phase I: Learning and Alignment; (2) Phase II: Clustering and Optimization. I wonder if this is another form of two-stage task.
2. The description of Clustering Loss is not very clear in Section 3.4, how to determine that samples belong to the same class? By pseudo-label? Where did the pseudo-label come from?
3. In this paper, the authors introduced large language models (LLMs) to learn representations and bridge the gap of textual and image features. But does the direct use of a pre-trained large language model introduce a priori information about the category, which can lead to unsupervised scenarios being corrupted?

Limitations:
The authors have discussed social impacts and limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an innovative approach for addressing the limitations of existing multiple clustering methods. By leveraging the synergistic capabilities of CLIP and GPT-4, Multi-Sub aligns textual prompts with visual representations to cater to diverse user-specific clustering needs. This method introduces a novel multi-modal subspace proxy learning framework, which automatically generates proxy words from large language models to represent data in terms specific to user interests. The experimental results demonstrate that Multi-Sub consistently outperforms existing baselines across various datasets. Overall, I believe this paper makes a substantial contribution to the field of deep clustering and holds significant practical application value.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper offers several notable strengths that contribute to its overall impact and significance in the field of multiple clustering: 
1.	The integration of CLIP and GPT-4 for multi-modal subspace proxy learning is novel and effectively addresses the limitations of traditional multiple clustering methods.
2.	Multi-Sub excels in capturing and responding to diverse user interests, providing tailored clustering results without requiring extensive manual interpretation. Moreover, the performance gains come at a low cost and seem relatively easy to achieve. 
3.	The writing is clear and easy to follow. The figures are well-drawn, allowing for a quick understanding of the research motivation and methodological design.
4.	Extensive experiments on a wide range of publicly available datasets demonstrate the robustness and generalizability of the proposed method.

Weaknesses:
Despite its strengths, there are some areas where the paper could be improved to enhance its clarity and applicability:
1. Although the paper mentions the hyperparameters used, a more detailed analysis and discussion on the sensitivity of the method to these parameters would be beneficial.
2. Given the method's iterative nature and the use of large models, there is a risk of overfitting, especially on smaller datasets. I am curious whether regularization techniques were used to address this issue?
3. Table 3 compares the impact of different text encoders on performance. Clearly, there are significant performance differences when using different encoders, and the authors have indeed analyzed this issue. However, I believe the reasons behind this phenomenon could be explored in depth. Intuitively, given that the input text is quite simple, the overall performance should not be particularly sensitive to the choice of text encoder.

Limitations:
The limitations are discussed in the paper by the authors. There is no potential negative societal impact

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper is about Multiple Clustering, which is an interesting topic. The authors propose a novel end-to-end multiple clustering approach that incorporates a multi-modal subspace proxy learning framework. The paper is well written and well organized. However, there are several concerns in the current version of the paper that addressing them will increase the quality of this paper.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1 The authors' idea of using large models to aid clustering is novel.
2 The paper is clearly structured and easy to understand.
3 The paper has sufficient experiments to support its point of view.

Weaknesses:
1 The authors point out that different clustering results can be given for different customization needs of users. Then it will bring several associations (not necessarily accurate): a. What should be done if the user's demand is exactly opposite to the potential clustering distribution? b. The experiments do give different clustering results for different demand types, if the user proposes a new type of demand, can the model also adaptively adjust?
2 Figure 2 is well drawn but could be further improved, some icons and fonts need to be adjusted.
3 The authors point out that their model is capable of outputting clustering results directly, and then there should be a corresponding formula to represent this. In addition, it is hoped that the authors will discuss further why, if it is not a difficult task to output clustering results directly, few previous methods have done so.
4 Authors should add details about the dataset, such as data size, feature types, etc.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an end-to-end multi-clustering method that integrates a multimodal subspace proxy learning framework. It combines text prompts expressing user preferences with corresponding visual representations to achieve clustering based on user interests.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The clustering task, driven by user interests, aligns better with user preferences and is more applicable to real-world scenarios.
2.The experimental results are promising, and the methodology is clear and logical.

Weaknesses:
1.The contributions of the paper are not very clear. At first glance, it appears to merely combine CLIP and GPT, lacking innovative architecture.
2.The baseline methods chosen for comparison are neither cited nor introduced.
3.Section 5 discusses only limitations, lacking a discussion on broader impacts.

Limitations:
The authors only address the limitations of their work and do not discuss broader impacts.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper incorporates a multi-model subspace proxy learning (Multi-Sub) to design a novel end-to-end multiple clustering approach and utilizes the synergistic capabilities of CLIP and GPT-4 to align textual prompts expressing user preferences with corresponding visual representations. The main contributions of Multi-Sub can be summarized as follows:
1. Capturing user’s clustering interest: Existing works struggle to adapt to diverse user-specific needs in data grouping. To overcome this limitation, Multi-Sub explicitly captures a user’s clustering interest by learning the desired clustering proxy under a user’s interest and aligning textual interest with corresponding visual features.
2. Simultaneous optimized framework: The previous methods separated the representation learning and clustering stages. Different from them, Multi-Sub obtains both the desired representations and clustering simultaneously, which significantly improve the clustering performance and efficiency.
3. Extensive experimental validation: Extensive experiments on all public multiple clustering tasks demonstrate that Multi-Sub outperform other methods. Moreover, a series of ablation studies further verify the effectiveness of Multi-Sub.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. In real world, data may have multiple aspects that they can be grouped into different clusters. However, existing methods solely consider a single partition. So, it is meaningful to propose an effective algorithm to overcome this problem.
2. The authors leveraged large language models (LLMs), including GPT-4 and CLIP, to align image and textual representations in the same subspace. Then, multi-modal subspace proxy learning is introduced to allow for the customized representation of data in terms specific to the user’s interests.
3. Experimental results on public datasets show that the Multi-Sub method has a significant improvement, indicating the effectiveness of the propose method.

Weaknesses:
1. To change the two-stage learning approach of previous works, Multi-Sub aims to learn representation and clustering simultaneously. However, Multi-Sub employs a two-phase iterative approach to align and cluster images in training process, including (1) Phase I: Learning and Alignment; (2) Phase II: Clustering and Optimization. I wonder if this is another form of two-stage task.
2. The description of Clustering Loss is not very clear in Section 3.4, how to determine that samples belong to the same class? By pseudo-label? Where did the pseudo-label come from?
3. In this paper, the authors introduced large language models (LLMs) to learn representations and bridge the gap of textual and image features. But does the direct use of a pre-trained large language model introduce a priori information about the category, which can lead to unsupervised scenarios being corrupted?

Limitations:
The authors have discussed social impacts and limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an innovative approach for addressing the limitations of existing multiple clustering methods. By leveraging the synergistic capabilities of CLIP and GPT-4, Multi-Sub aligns textual prompts with visual representations to cater to diverse user-specific clustering needs. This method introduces a novel multi-modal subspace proxy learning framework, which automatically generates proxy words from large language models to represent data in terms specific to user interests. The experimental results demonstrate that Multi-Sub consistently outperforms existing baselines across various datasets. Overall, I believe this paper makes a substantial contribution to the field of deep clustering and holds significant practical application value.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper offers several notable strengths that contribute to its overall impact and significance in the field of multiple clustering: 
1.	The integration of CLIP and GPT-4 for multi-modal subspace proxy learning is novel and effectively addresses the limitations of traditional multiple clustering methods.
2.	Multi-Sub excels in capturing and responding to diverse user interests, providing tailored clustering results without requiring extensive manual interpretation. Moreover, the performance gains come at a low cost and seem relatively easy to achieve. 
3.	The writing is clear and easy to follow. The figures are well-drawn, allowing for a quick understanding of the research motivation and methodological design.
4.	Extensive experiments on a wide range of publicly available datasets demonstrate the robustness and generalizability of the proposed method.

Weaknesses:
Despite its strengths, there are some areas where the paper could be improved to enhance its clarity and applicability:
1. Although the paper mentions the hyperparameters used, a more detailed analysis and discussion on the sensitivity of the method to these parameters would be beneficial.
2. Given the method's iterative nature and the use of large models, there is a risk of overfitting, especially on smaller datasets. I am curious whether regularization techniques were used to address this issue?
3. Table 3 compares the impact of different text encoders on performance. Clearly, there are significant performance differences when using different encoders, and the authors have indeed analyzed this issue. However, I believe the reasons behind this phenomenon could be explored in depth. Intuitively, given that the input text is quite simple, the overall performance should not be particularly sensitive to the choice of text encoder.

Limitations:
The limitations are discussed in the paper by the authors. There is no potential negative societal impact

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper is about Multiple Clustering, which is an interesting topic. The authors propose a novel end-to-end multiple clustering approach that incorporates a multi-modal subspace proxy learning framework. The paper is well written and well organized. However, there are several concerns in the current version of the paper that addressing them will increase the quality of this paper.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1 The authors' idea of using large models to aid clustering is novel.
2 The paper is clearly structured and easy to understand.
3 The paper has sufficient experiments to support its point of view.

Weaknesses:
1 The authors point out that different clustering results can be given for different customization needs of users. Then it will bring several associations (not necessarily accurate): a. What should be done if the user's demand is exactly opposite to the potential clustering distribution? b. The experiments do give different clustering results for different demand types, if the user proposes a new type of demand, can the model also adaptively adjust?
2 Figure 2 is well drawn but could be further improved, some icons and fonts need to be adjusted.
3 The authors point out that their model is capable of outputting clustering results directly, and then there should be a corresponding formula to represent this. In addition, it is hoped that the authors will discuss further why, if it is not a difficult task to output clustering results directly, few previous methods have done so.
4 Authors should add details about the dataset, such as data size, feature types, etc.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an end-to-end multi-clustering method that integrates a multimodal subspace proxy learning framework. It combines text prompts expressing user preferences with corresponding visual representations to achieve clustering based on user interests.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The clustering task, driven by user interests, aligns better with user preferences and is more applicable to real-world scenarios.
2.The experimental results are promising, and the methodology is clear and logical.

Weaknesses:
1.The contributions of the paper are not very clear. At first glance, it appears to merely combine CLIP and GPT, lacking innovative architecture.
2.The baseline methods chosen for comparison are neither cited nor introduced.
3.Section 5 discusses only limitations, lacking a discussion on broader impacts.

Limitations:
The authors only address the limitations of their work and do not discuss broader impacts.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xavWvnJTST;"REVIEW 
Summary:
The authors explore the relationship between feedback control and learning with recurrent neural networks (RNN). Specifically, they enforce a control signal onto a RNN that is used to generate a trajectory for a outreaching task, and then propose to use local learning rules on the neurons in the RNN. They show that with feedback control the network can adapt faster to perturbations, of the task and show that the local (in time) gradients are better aligned with the global ones.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The claims are all very reasonable and well illustrated. I this is the first time such feedback-based learning used in proper control settings, which is surprising given that it is based on control theory.

Weaknesses:
Main problem:
My main concern is that I the task chosen consists on bringing a system to a desired static target, so it is possible that there is no ""out of equilibrium dynamics"", rather the learning simply consists on bringing the ""arm"" to the required target and it just so happens that the shortest trajectory aligns with the velocity profile. While it could be that the trajectory is indeed learned (and with some implicit or explicit regularization it should be the case), the current task is not conclusive. If the point is to really learn a trajectory, the authors should have picked a task where the trajectory is a bit more complex than going to equilibrium. Maybe a limit cycle? Otherwise the work might be a minor modification of Meulemans et al.
Also, I fail to see the ""biological circuits"". If we are talking about recurrent neural networks, this is fine, but usually when we talk about circuits in biology we would refer to cell types (and this has a lot of constraints). In fact the authors themselves state that they are agnostic to the biological implementation, which is hardly in compatible with the title. I would replace it by recurrent neural networks.

Other issues:
- The key findings are not clear in the introduction. The term ""inference learning"" is only used there and in one of the figure, but it is not clearly defined. If the authors mean that feedback control can train an RNN then this has already been shown. For the second finding, ""increased accuracy of approximate local learning rules"" it would be better stated as increased accuracy WITH local learning rules (or something similar). For the third, the second order gradient is not really injected (this would suggest that the gradient is imposed on purpose); rather, the feedback control is implicitly related to second order optimization methods.
- Line 142: it seems natural that if the network is perturbed from its trajectory the feedback would be stronger to compensate for the perturbation. I don't see why this is ""suggested"". Also, the sentence is badly written ""suggest that the during task... activity is increasingly by feedback"").
- LInes 164 and 165. The authors say that "" using a local learning rule without feedback control show an increasing performance gap compared to those trained with feedback control and BPTT"". The sentence could be interpreted as if the network is trained with feedback control AND BPTT (combined). A better wording would replace AND by OR 
- In 3.4 it is a bit hard to follow. It seems as if the authors are using an eligibility trace to train the RNN through BPTT. But this intermediate step might not be real BPTT as it is commonly used. 


Literature issues:
- The work of Meulemans et al 2022b is credited with alleviating the temporal storage required by BPTT. While they did do that (and it is a good paper), I think that they based the memory load decrease on previous work (Bai et al., Deep Equilibrium models 2019), which if memory serves does use BP. The logic of my comment is that by training the equlibrium point of the network one can avoid the memory load, regardless of the training method.
- The connection between feedback-based learning and second order optimization has been is very closely related to Meulemans, et al. ""A theoretical framework for target propagation"" 2020. That paper mentions target propagation, but it is very similar to feedback based learning (as the authors probably can infer).
- This is a personal opinion, the authors do not need to take it into consideration: The biological plausibility claims seem to rely on the locality of the learning rules. While it's a requirement that learning rules should not break the laws of physics (or in this case basic biological knowledge), learning rules should at least have some basis on biology, which I am missing here. A brief mention of why would one think that the learning rules are close to biological ones would be welcome. My guess for this feedback-based work would be something with a temporal component such as  temporal hebbian rules (ex: Aceituno et al. ""Learning cortical hierarchies with temporal Hebbian updates."" 2020)

Limitations:
They did mention some limitations, but the key issue of whether this is general motor control (or shaping attractors) is not addressed.
Also, if the paper is about how feedback control guides credit assignment on biological circuits, being agnostic to the biological circuit is a problem, rather than a strength. To make a biological statement there should be some non trivial biological predictions, or some mention of what exactly does this bring to neuroscience that wasn't already known.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Feedback controllers are ubiquitous in neuroscience but their functions are not fully understood. This paper studies how feedback control interplays with biologically plausible online learning on a standard motor control task. The authors show that:
- feedback control enables to adapt to task variations without any plasticity, by approximating the gradient of the loss with respect to the hidden states.
- it makes tractable approximations to RTRL more reliable by shrinking the recurrent Jacobian eigenvalues.
- it incorporates some second-order information in the weight updates, leading to faster learning.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper studies an important understudied question, that is the interplay between feedback and learning.
The paper is overall well-written and is easy to read. The message is clearly delivered.
The experiments are carefully designed and well executed, and support the claims of the paper.

Overall, the paper will be an insightful read to the community.

Weaknesses:
While the experiments are overall well executed, there are a few points that should be improved to make the paper's claims more robust:
- In the appendix, it is written that the learning rate is taken to be constant. To make claims about e.g. learning speed, the optimizer, in particular its learning rate, has to be tuned.
- Figure 5b: it is not clear from this experiment that RFLO-c contains some second-order information. The alignment with the 2nd-order gradient result is not convincing as the estimated gradient is more aligned to the first-order gradient than to the second-order one. This experiment needs to be improved for it to support its claim. The BPTT-c baseline that I mention below may be a good starting point for further analysis as the gradient of a ""controlled loss"" (which is not the case for RFLO).
- A BPTT-c/RTRL-c baseline would be an interesting add to disambiguate between the role of feedback control and approximate gradient estimation through RFLO. This baseline would include feedback control in the recurrent neural network dynamics and optimize for the MSE loss at the output. This would be useful in e.g. Fig3b and Fig5b.

Limitations:
The paper is theoretical and its limitations have been properly addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Recent work has shown that feedback signals can be critical to rapid adaptation in control tasks, and may explain how biological intelligence can make rapid adjustments when solving such tasks. This paper studies how feedback control achieves this. To do so, the authors train an RNN enhanced with feedback control on a common control task, and study how the feedback signal lead the network to achieve more rapid adjustments when perturbations are introduced. The 3 main findings are that the feedback signals align well with the optimal global gradient of the error, that they help the network better weigh current information (vs. less relevant past information) during perturbations, and that they indirectly inject second-order information to the RNN.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- This work focuses on improving the theoretical understanding of an important method. Given that our understanding of many deep learning methods are woefully inadequate, such work is critically important for the field's development.
- The method and the results are clearly presented, the figures are excellent, and the writing is easy to follow.

Weaknesses:
I am not familiar with feedback control and motor tasks; hence, I ask the AC to please take this into consideration to appropriately weigh my review. My remarks on the methods could be wrong or trivial. That said, I'll do my best to provide feedback.

- Several sections of the paper seem to just present results from previous work, including section 3.1 and the entirety of the methods section. This makes the contributions of this paper seem rather thin.

- I may be missing something, but some of the results seem minimally surprising. For example, in section 3.2, the authors state ""...the feedback contribution to the overall network output increases during perturbation."" But how could it not increase during perturbation? Isn't the network explicitly trained to use the feedback information to make corrections during perturbation? The same goes for the alignment between the feedback signal and the optimal global gradient, and the indirect introduction of second-order information-- is it not by design that the network use feedback to make corrections, and thus the larger the correction needed (i.e. the larger the optimal gradient) the larger the feedback signal? And is it not by design that second-order information gets introduced via the recurrent connections that enables the network to ""save"" information from previous timesteps in the hidden state?

- The authors claim that feedback control guides credit assignment in biological circuits, but uses BPTT during the pretraining phase of the RNN, which they acknowledge is not biologically plausible. 
It seems to me that backprop is still doing much of the heavy lifting in terms of solving credit assignment, thus I'm not sure this claim is sufficiently justifiable. A more defensible claim given the current results may be that feedback control may guide motor adaptation in biological circuits.
Similarly, some parts of the intro and abstract strongly suggest that the presented method would perform credit assignment without suffering from the biological implausibilities of backpropagation (e.g. the abstract sets up the problem as ""backpropagation is known to perform accurate credit assignment of error, how a similarly powerful process can be realized within the constraints of biological circuits remains largely unclear""), yet the actual method relies heavily on backpropagation.

- The experiments are performed on a single task, using a small single layer RNN with 400 hidden units, and therefore it's unclear whether the findings would scale to other tasks and larger architectures. Given that the primary goal of this paper is to improve understanding of an existing learning algorithm, and most of the analysis are performed via empirical testing, I believe it's important for the authors to demonstrate that their conclusions are robust over a wider range of tasks and hyperparameters/architectures.

Limitations:
I'd like to see an expanded discussion in the limitations section regarding the remaining aspects of the feedback-enhanced RNN that remain biologically implausible. Particularly, the usage of BPTT in a work that aims to explain how biological credit assignment is performed is quite troubling for me, given its significant biological implausibility. Ideally, I think the authors should show that their results hold on a network trained using a biologically-plausible learning rule enhanced with feedback control.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the effect of feedback control on motor learning in recurrent neural networks, finding that feedback control improves learning performance and better aligns with the true gradient w.r.t. the task.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Alignment with the true gradient is an interesting result and helps explain why feedback works
- The authors study alignment from different perspectives (e.g. step-wise/full gradients, Newton method)
- The task the authors consider is widely used in monkey experiments, therefore it should be possible to adapt the conclusions to real data or use them to guide new experiments

Weaknesses:
- The training setup is rather limited; it would be interesting to see training done for other tasks and architectures (or RNN sizes).
- The paper might benefit from some theoretical analysis of why the feedback signal alings with the true gradient, although it’s not clear if that can be easily done.

Limitations:
The authors have addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors explore the relationship between feedback control and learning with recurrent neural networks (RNN). Specifically, they enforce a control signal onto a RNN that is used to generate a trajectory for a outreaching task, and then propose to use local learning rules on the neurons in the RNN. They show that with feedback control the network can adapt faster to perturbations, of the task and show that the local (in time) gradients are better aligned with the global ones.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The claims are all very reasonable and well illustrated. I this is the first time such feedback-based learning used in proper control settings, which is surprising given that it is based on control theory.

Weaknesses:
Main problem:
My main concern is that I the task chosen consists on bringing a system to a desired static target, so it is possible that there is no ""out of equilibrium dynamics"", rather the learning simply consists on bringing the ""arm"" to the required target and it just so happens that the shortest trajectory aligns with the velocity profile. While it could be that the trajectory is indeed learned (and with some implicit or explicit regularization it should be the case), the current task is not conclusive. If the point is to really learn a trajectory, the authors should have picked a task where the trajectory is a bit more complex than going to equilibrium. Maybe a limit cycle? Otherwise the work might be a minor modification of Meulemans et al.
Also, I fail to see the ""biological circuits"". If we are talking about recurrent neural networks, this is fine, but usually when we talk about circuits in biology we would refer to cell types (and this has a lot of constraints). In fact the authors themselves state that they are agnostic to the biological implementation, which is hardly in compatible with the title. I would replace it by recurrent neural networks.

Other issues:
- The key findings are not clear in the introduction. The term ""inference learning"" is only used there and in one of the figure, but it is not clearly defined. If the authors mean that feedback control can train an RNN then this has already been shown. For the second finding, ""increased accuracy of approximate local learning rules"" it would be better stated as increased accuracy WITH local learning rules (or something similar). For the third, the second order gradient is not really injected (this would suggest that the gradient is imposed on purpose); rather, the feedback control is implicitly related to second order optimization methods.
- Line 142: it seems natural that if the network is perturbed from its trajectory the feedback would be stronger to compensate for the perturbation. I don't see why this is ""suggested"". Also, the sentence is badly written ""suggest that the during task... activity is increasingly by feedback"").
- LInes 164 and 165. The authors say that "" using a local learning rule without feedback control show an increasing performance gap compared to those trained with feedback control and BPTT"". The sentence could be interpreted as if the network is trained with feedback control AND BPTT (combined). A better wording would replace AND by OR 
- In 3.4 it is a bit hard to follow. It seems as if the authors are using an eligibility trace to train the RNN through BPTT. But this intermediate step might not be real BPTT as it is commonly used. 


Literature issues:
- The work of Meulemans et al 2022b is credited with alleviating the temporal storage required by BPTT. While they did do that (and it is a good paper), I think that they based the memory load decrease on previous work (Bai et al., Deep Equilibrium models 2019), which if memory serves does use BP. The logic of my comment is that by training the equlibrium point of the network one can avoid the memory load, regardless of the training method.
- The connection between feedback-based learning and second order optimization has been is very closely related to Meulemans, et al. ""A theoretical framework for target propagation"" 2020. That paper mentions target propagation, but it is very similar to feedback based learning (as the authors probably can infer).
- This is a personal opinion, the authors do not need to take it into consideration: The biological plausibility claims seem to rely on the locality of the learning rules. While it's a requirement that learning rules should not break the laws of physics (or in this case basic biological knowledge), learning rules should at least have some basis on biology, which I am missing here. A brief mention of why would one think that the learning rules are close to biological ones would be welcome. My guess for this feedback-based work would be something with a temporal component such as  temporal hebbian rules (ex: Aceituno et al. ""Learning cortical hierarchies with temporal Hebbian updates."" 2020)

Limitations:
They did mention some limitations, but the key issue of whether this is general motor control (or shaping attractors) is not addressed.
Also, if the paper is about how feedback control guides credit assignment on biological circuits, being agnostic to the biological circuit is a problem, rather than a strength. To make a biological statement there should be some non trivial biological predictions, or some mention of what exactly does this bring to neuroscience that wasn't already known.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Feedback controllers are ubiquitous in neuroscience but their functions are not fully understood. This paper studies how feedback control interplays with biologically plausible online learning on a standard motor control task. The authors show that:
- feedback control enables to adapt to task variations without any plasticity, by approximating the gradient of the loss with respect to the hidden states.
- it makes tractable approximations to RTRL more reliable by shrinking the recurrent Jacobian eigenvalues.
- it incorporates some second-order information in the weight updates, leading to faster learning.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper studies an important understudied question, that is the interplay between feedback and learning.
The paper is overall well-written and is easy to read. The message is clearly delivered.
The experiments are carefully designed and well executed, and support the claims of the paper.

Overall, the paper will be an insightful read to the community.

Weaknesses:
While the experiments are overall well executed, there are a few points that should be improved to make the paper's claims more robust:
- In the appendix, it is written that the learning rate is taken to be constant. To make claims about e.g. learning speed, the optimizer, in particular its learning rate, has to be tuned.
- Figure 5b: it is not clear from this experiment that RFLO-c contains some second-order information. The alignment with the 2nd-order gradient result is not convincing as the estimated gradient is more aligned to the first-order gradient than to the second-order one. This experiment needs to be improved for it to support its claim. The BPTT-c baseline that I mention below may be a good starting point for further analysis as the gradient of a ""controlled loss"" (which is not the case for RFLO).
- A BPTT-c/RTRL-c baseline would be an interesting add to disambiguate between the role of feedback control and approximate gradient estimation through RFLO. This baseline would include feedback control in the recurrent neural network dynamics and optimize for the MSE loss at the output. This would be useful in e.g. Fig3b and Fig5b.

Limitations:
The paper is theoretical and its limitations have been properly addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Recent work has shown that feedback signals can be critical to rapid adaptation in control tasks, and may explain how biological intelligence can make rapid adjustments when solving such tasks. This paper studies how feedback control achieves this. To do so, the authors train an RNN enhanced with feedback control on a common control task, and study how the feedback signal lead the network to achieve more rapid adjustments when perturbations are introduced. The 3 main findings are that the feedback signals align well with the optimal global gradient of the error, that they help the network better weigh current information (vs. less relevant past information) during perturbations, and that they indirectly inject second-order information to the RNN.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- This work focuses on improving the theoretical understanding of an important method. Given that our understanding of many deep learning methods are woefully inadequate, such work is critically important for the field's development.
- The method and the results are clearly presented, the figures are excellent, and the writing is easy to follow.

Weaknesses:
I am not familiar with feedback control and motor tasks; hence, I ask the AC to please take this into consideration to appropriately weigh my review. My remarks on the methods could be wrong or trivial. That said, I'll do my best to provide feedback.

- Several sections of the paper seem to just present results from previous work, including section 3.1 and the entirety of the methods section. This makes the contributions of this paper seem rather thin.

- I may be missing something, but some of the results seem minimally surprising. For example, in section 3.2, the authors state ""...the feedback contribution to the overall network output increases during perturbation."" But how could it not increase during perturbation? Isn't the network explicitly trained to use the feedback information to make corrections during perturbation? The same goes for the alignment between the feedback signal and the optimal global gradient, and the indirect introduction of second-order information-- is it not by design that the network use feedback to make corrections, and thus the larger the correction needed (i.e. the larger the optimal gradient) the larger the feedback signal? And is it not by design that second-order information gets introduced via the recurrent connections that enables the network to ""save"" information from previous timesteps in the hidden state?

- The authors claim that feedback control guides credit assignment in biological circuits, but uses BPTT during the pretraining phase of the RNN, which they acknowledge is not biologically plausible. 
It seems to me that backprop is still doing much of the heavy lifting in terms of solving credit assignment, thus I'm not sure this claim is sufficiently justifiable. A more defensible claim given the current results may be that feedback control may guide motor adaptation in biological circuits.
Similarly, some parts of the intro and abstract strongly suggest that the presented method would perform credit assignment without suffering from the biological implausibilities of backpropagation (e.g. the abstract sets up the problem as ""backpropagation is known to perform accurate credit assignment of error, how a similarly powerful process can be realized within the constraints of biological circuits remains largely unclear""), yet the actual method relies heavily on backpropagation.

- The experiments are performed on a single task, using a small single layer RNN with 400 hidden units, and therefore it's unclear whether the findings would scale to other tasks and larger architectures. Given that the primary goal of this paper is to improve understanding of an existing learning algorithm, and most of the analysis are performed via empirical testing, I believe it's important for the authors to demonstrate that their conclusions are robust over a wider range of tasks and hyperparameters/architectures.

Limitations:
I'd like to see an expanded discussion in the limitations section regarding the remaining aspects of the feedback-enhanced RNN that remain biologically implausible. Particularly, the usage of BPTT in a work that aims to explain how biological credit assignment is performed is quite troubling for me, given its significant biological implausibility. Ideally, I think the authors should show that their results hold on a network trained using a biologically-plausible learning rule enhanced with feedback control.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the effect of feedback control on motor learning in recurrent neural networks, finding that feedback control improves learning performance and better aligns with the true gradient w.r.t. the task.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Alignment with the true gradient is an interesting result and helps explain why feedback works
- The authors study alignment from different perspectives (e.g. step-wise/full gradients, Newton method)
- The task the authors consider is widely used in monkey experiments, therefore it should be possible to adapt the conclusions to real data or use them to guide new experiments

Weaknesses:
- The training setup is rather limited; it would be interesting to see training done for other tasks and architectures (or RNN sizes).
- The paper might benefit from some theoretical analysis of why the feedback signal alings with the true gradient, although it’s not clear if that can be easily done.

Limitations:
The authors have addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xaqPAkJnAS;"REVIEW 
Summary:
- This paper presents an information theory approach to obtain a single graph fused from a multiplex graph, which preserves 
   - sufficient task-relevant information 
   - while removing task-irrelevant noise. 
- A learnable graph augmentation strategy is also developed. 
   - The learned graph and representation can be applied to different types of tasks. 
- The effectiveness is supported by extensive experimental results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is well-motivated. 
   - The authors find that each graph contains much unique task-relevant information, which is ignored by mainstream contrastive learning-based methods.
- This paper develops multiple graphs non-redundancy principle, which lays the foundation for multiplex graph data process. 
   - Two random and generative graph augmentation strategies are accordingly built to capture view-unique task information.
- The experimental results are promising. 
   - The framework demonstrates a clear advantage over existing methods, including advanced supervised approaches, highlighting its potential for broad application.
- This paper provides the code and all experimental settings for reproducing the results.

Weaknesses:
- The difference between the existing non-redundancy principle and multiplex graph non-redundancy is unclear. Please clarify it.
- The proposed InfoMGF-LA runs out-of-memory on MAG data. The reason should be given.
- It is possible that the proposed method cannot handle real-world large-scale graph. It should be addressed in the future and discussed in the conclusion part.
- The difference between the proposed method and DGM is unclear.

Limitations:
Some imitations are addressed in $\S5$.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces InfoMGF (Information-aware Unsupervised Multiplex Graph Fusion), a novel framework aimed at addressing the issue of graph structure reliability in Multiplex Graphs. The primary goal is to refine graph structures to eliminate noise and maximize task-relevant information. Theoretical analysis and comprehensive experimental results validate its effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	Originality: The paper addresses a critical gap in Unsupervised Multiplex Graph Learning (UMGL) by focusing on the reliability of graph structures, which is often overlooked in existing research.
2.	Quality: The proposed InfoMGF framework effectively refines graph structures to eliminate noise and maximizes both view-shared and view-unique task-relevant information. Theoretical analyses provided in the paper validate the effectiveness of InfoMGF in capturing task-relevant information and improving graph fusion. Extensive experiments demonstrate that InfoMGF outperforms various baselines and even sophisticated supervised approaches in different downstream tasks.
3.	Clarity: The paper is generally clearly written and well organized.

Weaknesses:
1.	Scalability: The framework involves several steps. Though the paper provides the complexity analysis in Appendix for each step, it is still unclear what is the overall complexity.
2.	Reproducibility: The authors share the code for reproducibility. However, I didn’t see the datasets.
3.	Accuracy: The authors should check for the few grammatical and spelling errors that occur in the text.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces InfoMGF, an innovative framework for Unsupervised Multiplex Graph Learning (UMGL) that addresses the often-overlooked issue of graph structure reliability. InfoMGF refines graph structures by removing task-irrelevant noise and maximizing task-relevant information through mutual information maximization. Extensive experiments demonstrate its superior performance over various baselines and even some supervised methods, validating its effectiveness in enhancing node representation learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- New Problem Formulation: The paper pioneers the investigation of graph structure reliability in multiplex graphs, which is a significant advancement in the field. Multiplex graphs enrich the representation of real-world systems and its analysis is very difficult inherently.
- Theoretical Analysis: The several theorems are quite interesting and provide a solid foundation for the proposed method. In particular, Theorem 3 proves the necessity of fusing multiplex graphs.
- Extensive Evaluation: The framework is thoroughly tested against various state-of-the-art methods on both node clustering and classification tasks, showcasing its robustness and effectiveness across different tasks. The comparison methods are representative and new.

Weaknesses:
- Robustness: Fig.4 shows that the proposed method is very robust to structure noise. However, more analysis is needed. Both InfoMGF and SUBLIME are structure learning methods. Compared to InfoMGF，Why does the performance of SUBLIME degrade rapidly in the case of edge deletions?
- Clarity: The paper develops two algorithms in this paper: InfoMGF-RA and InfoMGF-LA. However, it is a little confusion that what is the difference in their objective functions.

Limitations:
The paper discusses the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a novel approach to improve Unsupervised Multiplex Graph Learning by refining graph structures to eliminate noise and maximize relevant information. The method utilizes mutual information maximization to integrate multiple graph views effectively. Theoretical validation and comprehensive experiments show that the proposed method outperforms existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	Multiplex graph provides an efficient representation of complex systems. This paper focuses on non-redundancy issue, which is a new perspective and opens up a new avenue for future research.
2.	The proposed method adopts an unsupervised and generalized approach. Its performance surpasses several supervised approaches, underscoring its potential for practical applications. 
3.	The framework’s performance is validated through comprehensive experiments and compared with more than 20 methods.
4.	Visualization is also a strong point of this paper. The figures of node correlation, heatmaps of the subgraph, and unique relevant edge ratio are very illustrative.

Weaknesses:
1.	According to Table 1 and 2, it seems that the proposed method improves more on clustering than classification.
2.	Overall, this paper is well-organized. However, the writing could be improved in terms of tone and words.
3.	There are too many notations, which are confusing.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
- This paper presents an information theory approach to obtain a single graph fused from a multiplex graph, which preserves 
   - sufficient task-relevant information 
   - while removing task-irrelevant noise. 
- A learnable graph augmentation strategy is also developed. 
   - The learned graph and representation can be applied to different types of tasks. 
- The effectiveness is supported by extensive experimental results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is well-motivated. 
   - The authors find that each graph contains much unique task-relevant information, which is ignored by mainstream contrastive learning-based methods.
- This paper develops multiple graphs non-redundancy principle, which lays the foundation for multiplex graph data process. 
   - Two random and generative graph augmentation strategies are accordingly built to capture view-unique task information.
- The experimental results are promising. 
   - The framework demonstrates a clear advantage over existing methods, including advanced supervised approaches, highlighting its potential for broad application.
- This paper provides the code and all experimental settings for reproducing the results.

Weaknesses:
- The difference between the existing non-redundancy principle and multiplex graph non-redundancy is unclear. Please clarify it.
- The proposed InfoMGF-LA runs out-of-memory on MAG data. The reason should be given.
- It is possible that the proposed method cannot handle real-world large-scale graph. It should be addressed in the future and discussed in the conclusion part.
- The difference between the proposed method and DGM is unclear.

Limitations:
Some imitations are addressed in $\S5$.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces InfoMGF (Information-aware Unsupervised Multiplex Graph Fusion), a novel framework aimed at addressing the issue of graph structure reliability in Multiplex Graphs. The primary goal is to refine graph structures to eliminate noise and maximize task-relevant information. Theoretical analysis and comprehensive experimental results validate its effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	Originality: The paper addresses a critical gap in Unsupervised Multiplex Graph Learning (UMGL) by focusing on the reliability of graph structures, which is often overlooked in existing research.
2.	Quality: The proposed InfoMGF framework effectively refines graph structures to eliminate noise and maximizes both view-shared and view-unique task-relevant information. Theoretical analyses provided in the paper validate the effectiveness of InfoMGF in capturing task-relevant information and improving graph fusion. Extensive experiments demonstrate that InfoMGF outperforms various baselines and even sophisticated supervised approaches in different downstream tasks.
3.	Clarity: The paper is generally clearly written and well organized.

Weaknesses:
1.	Scalability: The framework involves several steps. Though the paper provides the complexity analysis in Appendix for each step, it is still unclear what is the overall complexity.
2.	Reproducibility: The authors share the code for reproducibility. However, I didn’t see the datasets.
3.	Accuracy: The authors should check for the few grammatical and spelling errors that occur in the text.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces InfoMGF, an innovative framework for Unsupervised Multiplex Graph Learning (UMGL) that addresses the often-overlooked issue of graph structure reliability. InfoMGF refines graph structures by removing task-irrelevant noise and maximizing task-relevant information through mutual information maximization. Extensive experiments demonstrate its superior performance over various baselines and even some supervised methods, validating its effectiveness in enhancing node representation learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- New Problem Formulation: The paper pioneers the investigation of graph structure reliability in multiplex graphs, which is a significant advancement in the field. Multiplex graphs enrich the representation of real-world systems and its analysis is very difficult inherently.
- Theoretical Analysis: The several theorems are quite interesting and provide a solid foundation for the proposed method. In particular, Theorem 3 proves the necessity of fusing multiplex graphs.
- Extensive Evaluation: The framework is thoroughly tested against various state-of-the-art methods on both node clustering and classification tasks, showcasing its robustness and effectiveness across different tasks. The comparison methods are representative and new.

Weaknesses:
- Robustness: Fig.4 shows that the proposed method is very robust to structure noise. However, more analysis is needed. Both InfoMGF and SUBLIME are structure learning methods. Compared to InfoMGF，Why does the performance of SUBLIME degrade rapidly in the case of edge deletions?
- Clarity: The paper develops two algorithms in this paper: InfoMGF-RA and InfoMGF-LA. However, it is a little confusion that what is the difference in their objective functions.

Limitations:
The paper discusses the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a novel approach to improve Unsupervised Multiplex Graph Learning by refining graph structures to eliminate noise and maximize relevant information. The method utilizes mutual information maximization to integrate multiple graph views effectively. Theoretical validation and comprehensive experiments show that the proposed method outperforms existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	Multiplex graph provides an efficient representation of complex systems. This paper focuses on non-redundancy issue, which is a new perspective and opens up a new avenue for future research.
2.	The proposed method adopts an unsupervised and generalized approach. Its performance surpasses several supervised approaches, underscoring its potential for practical applications. 
3.	The framework’s performance is validated through comprehensive experiments and compared with more than 20 methods.
4.	Visualization is also a strong point of this paper. The figures of node correlation, heatmaps of the subgraph, and unique relevant edge ratio are very illustrative.

Weaknesses:
1.	According to Table 1 and 2, it seems that the proposed method improves more on clustering than classification.
2.	Overall, this paper is well-organized. However, the writing could be improved in terms of tone and words.
3.	There are too many notations, which are confusing.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xabStWAUtr;"REVIEW 
Summary:
This paper distinguishes two forms of knowledge learning in the model: 
1. co-occurrence statistics: from modeling the co-occurrence of entities in the text.
2. factual associations: from modeling entity relations established through implicit associations.

They synthesize two datasets where knowledge is represented in the above two ways. They show that models that learn factual associations can generalize better than models that learn co-occurrence statistics. They also show that models that learn from factual associations can utilize the knowledge better for reasoning.

They further study where the knowledge of these two different representations is stored in the model. They show that co-occurrence statistics are stored in the middle layer, while factual associations are stored in the lower layers. Accordingly, they propose to reset the middle layer while training the model. They show that this approach makes models generalize and do reasoning better.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The identification of the two forms of knowledge learning shed valuable insight on how models generalize from learned from the training data.
2. They create a dataset and an associated experiment, which can be used for further studies in the same direction.
3. They study where the knowledge is stored in the model. According to the findings, they propose a simple but effective approach to improve the models’ generalization ability and utilization of the knowledge for reasoning.
4. Their experiment is comprehensive. They utilize a benchmark dataset MQuAKE-T and they include fine-tuning only the lower layers as a baseline.

Because studying how language models acquire knowledge from training data is crucial for developing a better training paradigm and I found this paper solid and well-presented, I highly recommend this paper.

Weaknesses:
1. They only experiment with MQuAKE-T where each sentence encodes a piece of knowledge (subject-relation-object). The authors could experiment with some more realistic settings where a single sentence contains more than one piece of knowledge.
2. It would be interesting to see how model scaling affects the behavior. The authors could experiment with models of different sizes in the same model family.

Limitations:
Yes, it's addressed in the last section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies how language models acquires factual knowledge during finetuning. It shows that narrative input tends to teach a model co-occurrence between entities, while referencing input teaches more about factual association. Models that learn factual association generalizes better to various question answering tasks than models that learn co-occurrence, especially for multi-hop reasoning tasks. By resetting different layers to the pretrained weights in models, the authors show that co-occurrence is mostly learned by the middle layers, while factual association is mostly learned by the lower layers. Based on this observation, the authors propose to reset the upper 2/3 layers to learn factual association when finetuning models on narrative input.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper studies how factual knowledge is learned by language models training on pure textual data, which is novel to my knowledge. The authors delivered clear lessons based on synthetic data and per-layer parameter ablation, and provided two solid solutions for real-world reasoning datasets. These lessons are important to the community of language models and reasoning.
- The paper is well structured and very easy to read. There are no typos and grammar errors.

Weaknesses:
- The analysis of this paper is limited to triplets, which do not represent all kinds of knowledge in reasoning tasks. Can you extend the conclusions to more general knowledge forms?
- The authors do not provide enough insights why narrative input tends to teach co-occurrence statistics. The only insight I can find in the paper is that co-occurrence statistics can be learned faster (Line 245-247). I would suggest the authors discussing this more in Section 3.

Limitations:
Yes. The authors have thoroughly discussed limitations of their analysis in the conclusion section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work investigates the deficiencies of pretrained language models in learning factual knowledge, highlighting that these models tend to learn word co-occurrence statistics rather than true factual associations. The authors find that language models, when dealing with explicit relationships, are prone to merely memorize word co-occurrences and perform poorly on tasks that require reasoning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This work shows that language models tend to learn word co-occurrence statistics instead of true factual associations. This finding is important for improving the knowledge learning of language models.
* The authors propose two methods to improve the learning of factual associations. First, by using text with implicit rather than explicit factual associations, they force the model to learn these associations. Second, by actively forgetting the learned co-occurrence statistics, they allow the model to better learn and retain factual associations.
* The proposed strategies significantly improve the model's performance in multi-hop reasoning tasks on both synthetic and real-world datasets, proving their effectiveness.

Weaknesses:
* The generalization across different domains. This work synthesizes Country-City-Animal data, which is somewhat limited.
* Reasoning or memory? The purpose of implicit training is to force the model to understand factual associations through indirect connections, thereby enhancing its reasoning abilities. This approach will help the model perform better on complex, multi-step reasoning questions rather than simple memory tasks because of their training pattern. While, it can’t directly prove that referencing method can bring better memory than Co-occurrence. Moreover, for simple QA tasks, the Referencing method performs worse than the Narrative method. Different test tasks should be designed to verify knowledge retention. For instance, adding more noise and interference during simple QA tests to evaluate the robustness of memory. Design memory retrieval tasks that do not require complex reasoning to ensure that the tests only assess the model's ability to recall facts.
* Although it mentions that co-occurrence statistics and factual associations are parameterized in different layers of the Transformer model, it lacks a deep explanation of the specific mechanisms and reasons behind these phenomena.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the learning of factual knowledge in pretrained language models, distinguishing between knowledge represented as word co-occurrence statistics and true factual associations. The authors find that language models tend to learn co-occurrence statistics, which do not generalize well to reasoning tasks, while factual associations, which generalize better, can be harder to learn. They propose two strategies to improve the learning of factual associations: training on text with implicit associations and using a method called active forgetting to discard learned co-occurrence statistics. Their experiments on synthetic and real-world datasets demonstrate that these strategies significantly enhance the models' ability to generalize factual knowledge in various reasoning scenarios. The paper includes a thorough layer-wise analysis of knowledge parameterization in transformer models finding different localization for co-occurence statistics vs factual knowledge in model weights.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I think the strengths of this paper are in the following contribtions 

- Identification of Knowledge Representations: The paper clearly distinguishes between two forms of knowledge representation in language models: co-occurrence statistics and true factual associations. This distinction is crucial for understanding the limitations of current models. Additionally, the detailed analysis of how co-occurrence statistics and factual associations are parameterized across different layers of transformer models provides valuable insights into the internal workings of pretrained models.

- Empirical Validation: The authors conduct comprehensive experiments using synthetic and real-world datasets to validate their claims. They show that models trained on implicit associations generalize better to reasoning tasks than those trained on explicit co-occurrence.

- Novel Training Strategies: They propose a training strategies to improve factual learning are innovative. Training on text with implicit associations and a method of actively forgetting learned co-occurrence statistics to unblock factual learning.

- Public Release of Resources: Finally, the release of the synthetic corpus and code to reproduce their reulsts can facilitate further research and experimentation in this domain.

Weaknesses:
I did not find any major weaknesses in this paper.

The main ones, which are mentioned by the authors when addressing current limitations of their work are the following:

- Synthetic data split: how are you splitting your synthetic data? Are you evaluating on an unseen subset for both synthetic as well as natural dataset? I understood you are testing on unseen data for natural dataset and I am unsure if that's also the case for the synthetic dataset. Please clarify. This is the reason why I am, at the moment, giving a score of 6 for what would otherwise be a clear 7.

- Overhead in Data Preparation: Converting general text to forms with implicit associations for real-word data may require significant effort and sophisticated rewriting techniques, potentially limiting practical applicability.

- Limited Scope of Text Variations: The paper only considers two forms of text (narrative and implicit association). There is a need to explore more diverse textual representations to validate the findings comprehensively.

- Focus on a single type of reasoning: While the claims that learning implicit knowledge improve performance on complex reasoning tasks, the paper focuses on a specific type of reasoning. Other type of reasoning like logical or mathematical should be validated. Additionally, it is unclear whether the proposed finetuning method and data harm existing model performance on standard LLM benchmark. It would a nice addition to show whether the method in the paper do not conflict with existing model knowledge in other domains.
 
- Evaluation information: Taken from the appendix ""For evaluation on question answering tasks, we report 5-shot exact match accuracy unless otherwise specified."" Please add this in the main body of the paper and mention why you use this metric instead of others like F1 for QA tasks. Is it because all your tasks require a single word as gold label? Is this true also for the real-world dataset in table 3 (MQuAKE-T and 2WikiMultiHopQA)? Please add this info together with your generation parameters used at inference time (number of generated tokens/sampling parameters etc.)

- 
---

Minor

- Missing reference: De Cao et al. Editing Factual Knowledge in Language Models, EMNLP 2021. This is an important reference when discussing model editing since it was among the first contribution in this area.

- line 200 the reference to Appendix 3.3 is wrong

----

### Final Recommendation

Overall, I think the claims are backed by well-presented empirical evidence and I vote for the inclusion of this paper to NeurIPS.

### Update post rebuttal

I increase my score from 6 to 7

Limitations:
Yes in the limitations section after the conclusion on page 9

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper distinguishes two forms of knowledge learning in the model: 
1. co-occurrence statistics: from modeling the co-occurrence of entities in the text.
2. factual associations: from modeling entity relations established through implicit associations.

They synthesize two datasets where knowledge is represented in the above two ways. They show that models that learn factual associations can generalize better than models that learn co-occurrence statistics. They also show that models that learn from factual associations can utilize the knowledge better for reasoning.

They further study where the knowledge of these two different representations is stored in the model. They show that co-occurrence statistics are stored in the middle layer, while factual associations are stored in the lower layers. Accordingly, they propose to reset the middle layer while training the model. They show that this approach makes models generalize and do reasoning better.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The identification of the two forms of knowledge learning shed valuable insight on how models generalize from learned from the training data.
2. They create a dataset and an associated experiment, which can be used for further studies in the same direction.
3. They study where the knowledge is stored in the model. According to the findings, they propose a simple but effective approach to improve the models’ generalization ability and utilization of the knowledge for reasoning.
4. Their experiment is comprehensive. They utilize a benchmark dataset MQuAKE-T and they include fine-tuning only the lower layers as a baseline.

Because studying how language models acquire knowledge from training data is crucial for developing a better training paradigm and I found this paper solid and well-presented, I highly recommend this paper.

Weaknesses:
1. They only experiment with MQuAKE-T where each sentence encodes a piece of knowledge (subject-relation-object). The authors could experiment with some more realistic settings where a single sentence contains more than one piece of knowledge.
2. It would be interesting to see how model scaling affects the behavior. The authors could experiment with models of different sizes in the same model family.

Limitations:
Yes, it's addressed in the last section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies how language models acquires factual knowledge during finetuning. It shows that narrative input tends to teach a model co-occurrence between entities, while referencing input teaches more about factual association. Models that learn factual association generalizes better to various question answering tasks than models that learn co-occurrence, especially for multi-hop reasoning tasks. By resetting different layers to the pretrained weights in models, the authors show that co-occurrence is mostly learned by the middle layers, while factual association is mostly learned by the lower layers. Based on this observation, the authors propose to reset the upper 2/3 layers to learn factual association when finetuning models on narrative input.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper studies how factual knowledge is learned by language models training on pure textual data, which is novel to my knowledge. The authors delivered clear lessons based on synthetic data and per-layer parameter ablation, and provided two solid solutions for real-world reasoning datasets. These lessons are important to the community of language models and reasoning.
- The paper is well structured and very easy to read. There are no typos and grammar errors.

Weaknesses:
- The analysis of this paper is limited to triplets, which do not represent all kinds of knowledge in reasoning tasks. Can you extend the conclusions to more general knowledge forms?
- The authors do not provide enough insights why narrative input tends to teach co-occurrence statistics. The only insight I can find in the paper is that co-occurrence statistics can be learned faster (Line 245-247). I would suggest the authors discussing this more in Section 3.

Limitations:
Yes. The authors have thoroughly discussed limitations of their analysis in the conclusion section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work investigates the deficiencies of pretrained language models in learning factual knowledge, highlighting that these models tend to learn word co-occurrence statistics rather than true factual associations. The authors find that language models, when dealing with explicit relationships, are prone to merely memorize word co-occurrences and perform poorly on tasks that require reasoning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This work shows that language models tend to learn word co-occurrence statistics instead of true factual associations. This finding is important for improving the knowledge learning of language models.
* The authors propose two methods to improve the learning of factual associations. First, by using text with implicit rather than explicit factual associations, they force the model to learn these associations. Second, by actively forgetting the learned co-occurrence statistics, they allow the model to better learn and retain factual associations.
* The proposed strategies significantly improve the model's performance in multi-hop reasoning tasks on both synthetic and real-world datasets, proving their effectiveness.

Weaknesses:
* The generalization across different domains. This work synthesizes Country-City-Animal data, which is somewhat limited.
* Reasoning or memory? The purpose of implicit training is to force the model to understand factual associations through indirect connections, thereby enhancing its reasoning abilities. This approach will help the model perform better on complex, multi-step reasoning questions rather than simple memory tasks because of their training pattern. While, it can’t directly prove that referencing method can bring better memory than Co-occurrence. Moreover, for simple QA tasks, the Referencing method performs worse than the Narrative method. Different test tasks should be designed to verify knowledge retention. For instance, adding more noise and interference during simple QA tests to evaluate the robustness of memory. Design memory retrieval tasks that do not require complex reasoning to ensure that the tests only assess the model's ability to recall facts.
* Although it mentions that co-occurrence statistics and factual associations are parameterized in different layers of the Transformer model, it lacks a deep explanation of the specific mechanisms and reasons behind these phenomena.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the learning of factual knowledge in pretrained language models, distinguishing between knowledge represented as word co-occurrence statistics and true factual associations. The authors find that language models tend to learn co-occurrence statistics, which do not generalize well to reasoning tasks, while factual associations, which generalize better, can be harder to learn. They propose two strategies to improve the learning of factual associations: training on text with implicit associations and using a method called active forgetting to discard learned co-occurrence statistics. Their experiments on synthetic and real-world datasets demonstrate that these strategies significantly enhance the models' ability to generalize factual knowledge in various reasoning scenarios. The paper includes a thorough layer-wise analysis of knowledge parameterization in transformer models finding different localization for co-occurence statistics vs factual knowledge in model weights.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I think the strengths of this paper are in the following contribtions 

- Identification of Knowledge Representations: The paper clearly distinguishes between two forms of knowledge representation in language models: co-occurrence statistics and true factual associations. This distinction is crucial for understanding the limitations of current models. Additionally, the detailed analysis of how co-occurrence statistics and factual associations are parameterized across different layers of transformer models provides valuable insights into the internal workings of pretrained models.

- Empirical Validation: The authors conduct comprehensive experiments using synthetic and real-world datasets to validate their claims. They show that models trained on implicit associations generalize better to reasoning tasks than those trained on explicit co-occurrence.

- Novel Training Strategies: They propose a training strategies to improve factual learning are innovative. Training on text with implicit associations and a method of actively forgetting learned co-occurrence statistics to unblock factual learning.

- Public Release of Resources: Finally, the release of the synthetic corpus and code to reproduce their reulsts can facilitate further research and experimentation in this domain.

Weaknesses:
I did not find any major weaknesses in this paper.

The main ones, which are mentioned by the authors when addressing current limitations of their work are the following:

- Synthetic data split: how are you splitting your synthetic data? Are you evaluating on an unseen subset for both synthetic as well as natural dataset? I understood you are testing on unseen data for natural dataset and I am unsure if that's also the case for the synthetic dataset. Please clarify. This is the reason why I am, at the moment, giving a score of 6 for what would otherwise be a clear 7.

- Overhead in Data Preparation: Converting general text to forms with implicit associations for real-word data may require significant effort and sophisticated rewriting techniques, potentially limiting practical applicability.

- Limited Scope of Text Variations: The paper only considers two forms of text (narrative and implicit association). There is a need to explore more diverse textual representations to validate the findings comprehensively.

- Focus on a single type of reasoning: While the claims that learning implicit knowledge improve performance on complex reasoning tasks, the paper focuses on a specific type of reasoning. Other type of reasoning like logical or mathematical should be validated. Additionally, it is unclear whether the proposed finetuning method and data harm existing model performance on standard LLM benchmark. It would a nice addition to show whether the method in the paper do not conflict with existing model knowledge in other domains.
 
- Evaluation information: Taken from the appendix ""For evaluation on question answering tasks, we report 5-shot exact match accuracy unless otherwise specified."" Please add this in the main body of the paper and mention why you use this metric instead of others like F1 for QA tasks. Is it because all your tasks require a single word as gold label? Is this true also for the real-world dataset in table 3 (MQuAKE-T and 2WikiMultiHopQA)? Please add this info together with your generation parameters used at inference time (number of generated tokens/sampling parameters etc.)

- 
---

Minor

- Missing reference: De Cao et al. Editing Factual Knowledge in Language Models, EMNLP 2021. This is an important reference when discussing model editing since it was among the first contribution in this area.

- line 200 the reference to Appendix 3.3 is wrong

----

### Final Recommendation

Overall, I think the claims are backed by well-presented empirical evidence and I vote for the inclusion of this paper to NeurIPS.

### Update post rebuttal

I increase my score from 6 to 7

Limitations:
Yes in the limitations section after the conclusion on page 9

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vpEq2bzsS0;"REVIEW 
Summary:
The paper introduces a novel framework called MoTE. This framework addresses the trade-off between zero-shot generalization and close-set performance in video recognition tasks by tuning a mixture of temporal experts. The key contributions include:

- Introducing Weight Merging Regularization to balance generalization and specialization.
- Proposing temporal feature modulation to improve generalization during inference.
- Demonstrating state-of-the-art or competitive results on various video datasets such as Kinetics-400, Kinetics-600, UCF-101, and HMDB-51.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The introduction of Weight Merging Regularization and temporal feature modulation provides a novel approach to balancing generalization and specialization in video recognition.
- The experimental results are thorough, demonstrating the effectiveness of the proposed methods on multiple datasets.

Weaknesses:
- The framework's text space is confined to video category names, which limits the richness of textual representations. Expanding the semantic space using large-scale generative models could enhance performance.
- The method currently explores limited forms of additional parameters. Extending the approach to other forms could improve generality and versatility.
- While results on certain benchmarks are promising, the model's performance on more diverse and challenging datasets needs further validation.
- The additional complexity from Weight Merging Regularization and other components can slightly increase training time, which may be a barrier for real-time applications.
- Extensive fine-tuning required for different tasks can be computationally expensive and time-consuming.

Limitations:
The following work is recommended for citation & discussion:

Oh, C., Lim, H., Kim, M., Han, D., Yun, S., Choo, J., Hauptmann, A., Cheng, Z.-Q., & Song, K. (2023). Towards calibrated robust fine-tuning of vision-language models. In NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models.

Tu, S., Dai, Q., Wu, Z., Cheng, Z.-Q., Hu, H., & Jiang, Y.-G. (2023). Implicit temporal modeling with learnable alignment for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 19936-19947).

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issue of Video-Language Models (VLMs), such as CLIP, experiencing reduced generalization performance to unseen categories when learning domain-specific knowledge for video understanding tasks. The authors propose the MoTE framework, which introduces temporal experts and employs a Mixture of Experts (MoE) approach to effectively learn domain-specific knowledge for videos. Additionally, a soft stochastic routing policy is utilized to further enhance the learning efficiency of the experts. To guarantee the discrepancy in knowledge learned by different experts while maintaining a flat loss landscape, the paper incorporates weight merging regularization, which improves the generalization performance of the learned features. Moreover, the paper presents a temporal feature modulation method that leverages the semantic relevance confidence of proxy text features to modulate features.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper introduces the Mixture of Experts (MoE) approach in zero-shot video classification tasks based on Video-Language Models (VLMs). By utilizing weight merging regularization and other methods, the approach ensures effective learning of domain-specific knowledge in videos while maintaining strong model generalization.

2. The study effectively combines temporal modeling of visual content with the MoE approach. During downstream task adaptation, it leverages multi-perspective data bias learning to avoid overfitting, thus enhancing the learning effectiveness of domain-specific knowledge in videos.

3. The paper analyzes model generalization from the perspective of loss landscape flatness. By improving the flatness, weight merging regularization enhances the generalization performance of the learned features.

Weaknesses:
1. There is ambiguity in the use of certain symbols within the paper. For example, the symbol L is used to represent both the loss function of CLIP and the number of layers in the Transformer introduced in MoTE. This issue is particularly evident in Equations (4) and (7). The paper should consider adjusting the usage of these symbols to avoid confusion.

2. There seems to be a problem with the calculation in Equation (5). The notation ""exp"" typically represents the exponential function of e, but this is not clearly explained. According to the equation, the probability of selecting an expert increases with i, which seems to contradict the intended randomness of stochastic. This requires clarification or correction.

3. In the Introduction and Section 3.4, the paper emphasizes the plug-and-play characteristic of the modulation module. However, the subsequent experiments only demonstrate the improvement in model performance without introducing additional training parameters (Play). They do not showcase the flexibility and usability of the module regardless of the upper model structure (Plug). Therefore, it would be beneficial to add experiments validating the plug-and-play effect or adjust the relevant descriptions in the paper.

Limitations:
The paper does not adequately explain the connection of data bias view and MoE in Section 3.2. For reading-friendly, there should be additional descriptions of the relationship between experts and data bias views, and how the MoE approach leverages multiple data bias views to improve model performance.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces MoTE (Mixture-of-Temporal-Experts) to improve the generalization and specialization capabilities of visual-language models (VLMs) when adapting to video tasks. MoTE addresses two main questions: how to enhance the generalization of additional parameters during fine-tuning, and how to balance generalization and specialization in a unified model. The approach uses multiple feedforward network (FFN) experts in each Transformer layer to capture various data bias views, improving generalization. A routing algorithm based on multinomial distribution maximizes knowledge diversity among experts, while Weight Merging Regularization effectively combines generalized and specialized knowledge in the final model.

To further improve generalization at test time, MoTE incorporates a Temporal Feature Modulation module. Notably, the approach maintains the same computational cost and final structure as conventional methods. The paper contributes to the field by offering a new perspective on enhancing parameter generalization and balancing it with specialization in the context of adapting VLMs to video tasks. Extensive experiments demonstrate that MoTE achieves an optimal trade-off between zero-shot and close-set performance, with thorough ablation studies showing the scalability and effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The manuscript is well-written and easy to follow.

- It is interesting to observe that the introduction of a mixture of experts can enhance the balance between acquiring generalizable knowledge and learning video-specific features. The motivation is intuitive, and the extensive experiments effectively validate the method’s efficacy.

- The design of weight merging regularization and temporal feature modulation harmonizes the pursuit of the two learning objectives. The temporal feature modulation is particularly noteworthy, as it takes into account the categorical relationships between the training and test sets to inform the integration of features.

Weaknesses:
- The primary motivation for this study stems from two objectives: (1) mitigating the catastrophic forgetting that emerges with the integration of trainable parameters, and (2) striking a balance between generalizable knowledge and video-specific learning within one single model. However, these objectives bear considerable resemblance to the work presented in the paper FROSTER (ICLR 2024), which has not been discussed by the authors. While I acknowledge that the current paper and FROSTER employ distinct methodologies to address these issues, their close relevance necessitates a thorough discussion and a direct performance comparison.

- According to the description in the paper, the baseline model utilizes a clip encoder equipped with several temporal transformer layers. This leads me to question whether the model can be effectively integrated with alternative network architectures, such as adapter-based networks, X-CLIP, and ST-adapter, particularly given their noted efficiency in training.

- I would also request that the authors provide details regarding the additional computational and training time costs associated with implementing their method in conjunction with the baseline model.

- I believe it would be beneficial to delve deeper into the specific types of actions that each expert excels at recognizing. Providing a more detailed analysis in this area would enhance our comprehension of the distinct roles played by various experts, as well as the unique temporal knowledge they contribute in comparison to one another.

[1] FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition. ICLR 2024.

Limitations:
The authors have not sufficiently addressed the limitations of their methodology, as it has been applied exclusively to a specific type of adapted network without demonstrating broader applicability. It would be advantageous to see an exploration of the method’s versatility across different network architectures.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To preserve the generalization ability of the model trained on general visual-language model (VLM) with task-specific data, while boost the performance on specific task, this paper propose a new framework and training strategy to learn a unified model with specific performance and generalization ability. Three techniques are introduced. Mixture temporal experts to avoid overfitting on the task-specific data. A weight merging regularization to enlarge the loss flat region such that optimization on generalization ability will not introduce perturbation that drops the close-set performance. A temporal feature modulation to reuse the feature of VLM model when the target category label is not fitted during task-specific finetuning. The proposed method is evaluated on four benchmark datasets. K400 for close-set finetuning and UCF-101, HMDB-51and K600 for zero-shot evaluation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	To train a model with both task-specific performance and zero-shot generalization ability is a interesting topic, and it is less explored in the community. 
2.	The proposed method achieves competitive performance compared with the similar methods.
3.	Balancing between the zero-shot and the task-specific ability is always hard to handle. Considering the wide application of general VLM, this method bears practical value in the industry.

Weaknesses:
1.	The experimental setting may hide the weakness of the proposed method. The method is only trained on K400 and evaluated its zero-shot ability on UCF-101, HMDB-51and K600. Considering K400 is already a large-scale dataset, the MoTE may still have good performance on UCF-101 and HMDB-51. Besides, K600 is an extension of K400, therefore they may have similar data distribution. It would be great to also finetune the model on small-scale dataset and evaluated generalization ability on large-scale dataset, for example, train the model on UCF-101 and evaluate it on K400.
2.	A simple solution to handle the zero-shot / task-specific balancing issue is to use a finetuned model such as Text4Vis for specific task and to use its temporally mean-pooled clip feature when facing out-of-distribution task. This baseline is missing in the comparison. If the performance of this baseline is acceptable, is it really necessary to train a unified model with such much cost?

Limitations:
The limitation has been discussed in the suplemental material.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel framework called MoTE. This framework addresses the trade-off between zero-shot generalization and close-set performance in video recognition tasks by tuning a mixture of temporal experts. The key contributions include:

- Introducing Weight Merging Regularization to balance generalization and specialization.
- Proposing temporal feature modulation to improve generalization during inference.
- Demonstrating state-of-the-art or competitive results on various video datasets such as Kinetics-400, Kinetics-600, UCF-101, and HMDB-51.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The introduction of Weight Merging Regularization and temporal feature modulation provides a novel approach to balancing generalization and specialization in video recognition.
- The experimental results are thorough, demonstrating the effectiveness of the proposed methods on multiple datasets.

Weaknesses:
- The framework's text space is confined to video category names, which limits the richness of textual representations. Expanding the semantic space using large-scale generative models could enhance performance.
- The method currently explores limited forms of additional parameters. Extending the approach to other forms could improve generality and versatility.
- While results on certain benchmarks are promising, the model's performance on more diverse and challenging datasets needs further validation.
- The additional complexity from Weight Merging Regularization and other components can slightly increase training time, which may be a barrier for real-time applications.
- Extensive fine-tuning required for different tasks can be computationally expensive and time-consuming.

Limitations:
The following work is recommended for citation & discussion:

Oh, C., Lim, H., Kim, M., Han, D., Yun, S., Choo, J., Hauptmann, A., Cheng, Z.-Q., & Song, K. (2023). Towards calibrated robust fine-tuning of vision-language models. In NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models.

Tu, S., Dai, Q., Wu, Z., Cheng, Z.-Q., Hu, H., & Jiang, Y.-G. (2023). Implicit temporal modeling with learnable alignment for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 19936-19947).

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issue of Video-Language Models (VLMs), such as CLIP, experiencing reduced generalization performance to unseen categories when learning domain-specific knowledge for video understanding tasks. The authors propose the MoTE framework, which introduces temporal experts and employs a Mixture of Experts (MoE) approach to effectively learn domain-specific knowledge for videos. Additionally, a soft stochastic routing policy is utilized to further enhance the learning efficiency of the experts. To guarantee the discrepancy in knowledge learned by different experts while maintaining a flat loss landscape, the paper incorporates weight merging regularization, which improves the generalization performance of the learned features. Moreover, the paper presents a temporal feature modulation method that leverages the semantic relevance confidence of proxy text features to modulate features.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper introduces the Mixture of Experts (MoE) approach in zero-shot video classification tasks based on Video-Language Models (VLMs). By utilizing weight merging regularization and other methods, the approach ensures effective learning of domain-specific knowledge in videos while maintaining strong model generalization.

2. The study effectively combines temporal modeling of visual content with the MoE approach. During downstream task adaptation, it leverages multi-perspective data bias learning to avoid overfitting, thus enhancing the learning effectiveness of domain-specific knowledge in videos.

3. The paper analyzes model generalization from the perspective of loss landscape flatness. By improving the flatness, weight merging regularization enhances the generalization performance of the learned features.

Weaknesses:
1. There is ambiguity in the use of certain symbols within the paper. For example, the symbol L is used to represent both the loss function of CLIP and the number of layers in the Transformer introduced in MoTE. This issue is particularly evident in Equations (4) and (7). The paper should consider adjusting the usage of these symbols to avoid confusion.

2. There seems to be a problem with the calculation in Equation (5). The notation ""exp"" typically represents the exponential function of e, but this is not clearly explained. According to the equation, the probability of selecting an expert increases with i, which seems to contradict the intended randomness of stochastic. This requires clarification or correction.

3. In the Introduction and Section 3.4, the paper emphasizes the plug-and-play characteristic of the modulation module. However, the subsequent experiments only demonstrate the improvement in model performance without introducing additional training parameters (Play). They do not showcase the flexibility and usability of the module regardless of the upper model structure (Plug). Therefore, it would be beneficial to add experiments validating the plug-and-play effect or adjust the relevant descriptions in the paper.

Limitations:
The paper does not adequately explain the connection of data bias view and MoE in Section 3.2. For reading-friendly, there should be additional descriptions of the relationship between experts and data bias views, and how the MoE approach leverages multiple data bias views to improve model performance.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces MoTE (Mixture-of-Temporal-Experts) to improve the generalization and specialization capabilities of visual-language models (VLMs) when adapting to video tasks. MoTE addresses two main questions: how to enhance the generalization of additional parameters during fine-tuning, and how to balance generalization and specialization in a unified model. The approach uses multiple feedforward network (FFN) experts in each Transformer layer to capture various data bias views, improving generalization. A routing algorithm based on multinomial distribution maximizes knowledge diversity among experts, while Weight Merging Regularization effectively combines generalized and specialized knowledge in the final model.

To further improve generalization at test time, MoTE incorporates a Temporal Feature Modulation module. Notably, the approach maintains the same computational cost and final structure as conventional methods. The paper contributes to the field by offering a new perspective on enhancing parameter generalization and balancing it with specialization in the context of adapting VLMs to video tasks. Extensive experiments demonstrate that MoTE achieves an optimal trade-off between zero-shot and close-set performance, with thorough ablation studies showing the scalability and effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The manuscript is well-written and easy to follow.

- It is interesting to observe that the introduction of a mixture of experts can enhance the balance between acquiring generalizable knowledge and learning video-specific features. The motivation is intuitive, and the extensive experiments effectively validate the method’s efficacy.

- The design of weight merging regularization and temporal feature modulation harmonizes the pursuit of the two learning objectives. The temporal feature modulation is particularly noteworthy, as it takes into account the categorical relationships between the training and test sets to inform the integration of features.

Weaknesses:
- The primary motivation for this study stems from two objectives: (1) mitigating the catastrophic forgetting that emerges with the integration of trainable parameters, and (2) striking a balance between generalizable knowledge and video-specific learning within one single model. However, these objectives bear considerable resemblance to the work presented in the paper FROSTER (ICLR 2024), which has not been discussed by the authors. While I acknowledge that the current paper and FROSTER employ distinct methodologies to address these issues, their close relevance necessitates a thorough discussion and a direct performance comparison.

- According to the description in the paper, the baseline model utilizes a clip encoder equipped with several temporal transformer layers. This leads me to question whether the model can be effectively integrated with alternative network architectures, such as adapter-based networks, X-CLIP, and ST-adapter, particularly given their noted efficiency in training.

- I would also request that the authors provide details regarding the additional computational and training time costs associated with implementing their method in conjunction with the baseline model.

- I believe it would be beneficial to delve deeper into the specific types of actions that each expert excels at recognizing. Providing a more detailed analysis in this area would enhance our comprehension of the distinct roles played by various experts, as well as the unique temporal knowledge they contribute in comparison to one another.

[1] FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition. ICLR 2024.

Limitations:
The authors have not sufficiently addressed the limitations of their methodology, as it has been applied exclusively to a specific type of adapted network without demonstrating broader applicability. It would be advantageous to see an exploration of the method’s versatility across different network architectures.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To preserve the generalization ability of the model trained on general visual-language model (VLM) with task-specific data, while boost the performance on specific task, this paper propose a new framework and training strategy to learn a unified model with specific performance and generalization ability. Three techniques are introduced. Mixture temporal experts to avoid overfitting on the task-specific data. A weight merging regularization to enlarge the loss flat region such that optimization on generalization ability will not introduce perturbation that drops the close-set performance. A temporal feature modulation to reuse the feature of VLM model when the target category label is not fitted during task-specific finetuning. The proposed method is evaluated on four benchmark datasets. K400 for close-set finetuning and UCF-101, HMDB-51and K600 for zero-shot evaluation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	To train a model with both task-specific performance and zero-shot generalization ability is a interesting topic, and it is less explored in the community. 
2.	The proposed method achieves competitive performance compared with the similar methods.
3.	Balancing between the zero-shot and the task-specific ability is always hard to handle. Considering the wide application of general VLM, this method bears practical value in the industry.

Weaknesses:
1.	The experimental setting may hide the weakness of the proposed method. The method is only trained on K400 and evaluated its zero-shot ability on UCF-101, HMDB-51and K600. Considering K400 is already a large-scale dataset, the MoTE may still have good performance on UCF-101 and HMDB-51. Besides, K600 is an extension of K400, therefore they may have similar data distribution. It would be great to also finetune the model on small-scale dataset and evaluated generalization ability on large-scale dataset, for example, train the model on UCF-101 and evaluate it on K400.
2.	A simple solution to handle the zero-shot / task-specific balancing issue is to use a finetuned model such as Text4Vis for specific task and to use its temporally mean-pooled clip feature when facing out-of-distribution task. This baseline is missing in the comparison. If the performance of this baseline is acceptable, is it really necessary to train a unified model with such much cost?

Limitations:
The limitation has been discussed in the suplemental material.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xXRnUU7xTL;"REVIEW 
Summary:
- The paper introduces SelfCodeAlign, a fully transparent and permissive self-alignment pipeline for code generation in LLMs without relying on extensive human annotations or distillation from larger models. SelfCodeAlign generates instruction-response pairs from seed snippets, evaluates responses with test cases, and fine-tunes models based on successful executions. The approach shows superior performance over state-of-the-art methods, including GPT-3.5-Turbo-based distillation, particularly in HumanEval+ benchmark. The pipeline demonstrates effectiveness across various model sizes, emphasizing the benefits of self-generated data over teacher models with smaller performance gaps. 
- Overall, I feel that SelfCodeAlign is a very easy workflow to follow and I see much potential for such pipelines that do not depend on distillation or human annotations. I recommend an accept.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
## Originality
- The paper adequately cites related work, clearly identifying gaps such as the lack of transparency in existing methods, which is a key motivation for their work. 

## Quality
- The submission is technically sound with both quantitative and qualitative analysis.
- The authors provide detailed experimental results, demonstrating significant performance improvements over baselines.
- The inclusion of both large-scale and small-scale model evaluations further strengthens the quality of the research.
- In terms of ethical considerations, they have considered all terms of use as well as the data in code snippets.

## Clarity
- Well organized paper, except for appendix.

## Significance
- The results are highly significant, as SelfCodeAlign achieves performance improvements, notably surpassing models that are an order of magnitude larger in size. This work addresses the challenge of instruction tuning without human annotations or distillation, offering a scalable and transparent solution that advances the state of the art in code generation.

Weaknesses:
## Originality
- Perhaps similar to this paper [Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models](https://arxiv.org/pdf/2312.06585) ? Even if it is different, I think that this should also be part of your baseline comparison as well.

## Quality
- The qualitative examples provided in the appendix are excessively long, which may overwhelm the reader and obscure the main differences and contributions of the methodology. It would be beneficial to reduce the number of examples or to shorten them, focusing on highlighting the key differences and improvements over baseline methods. Additionally, the examples are presented in black and white with no descriptions or annotations, making it difficult to discern their significance. Providing clearer, annotated examples with concise explanations would enhance the readability and impact of this section.
- I do not see any weaknesses discussed in this work, for example, in what scenario do you think does this methodology not work? Why is the score still not perfect? (or for eg, below 80% accuracy)

Limitations:
Limitations are stated. However, I think the authors miss one important point – the reliance on the seed snippets. The performance of the model is based on what it was finetuned on, so if the seed snippets are not sufficiently diverse or representative of the target tasks, then it might have resulted in a large drop in accuracy.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors proposed SelfCodeAlign that finetunes the model based on the filtered data generated by the same model itself. The authors conduct experiments to show that SelfCodeAlign outperforms most open-sourced models that were finetuned on public code dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The code generation problem is important and the results (compared to models trained on public dataset) are promising.

Weaknesses:
Compare to models that are distilled/trained on non-disclosed data, the performance of SelfCodeAlign is not as competitive. The presentation can be improved, see **questions**.

Limitations:
Limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces SelfCodeAlign, an entirely transparent and permissive pipeline designed for self-aligning code large language models without the need for human annotations or distillation. By applying SelfCodeAlign to CodeQwen1.5-7B, the authors generated a dataset containing 74k instruction-response pairs. They then fine-tuned CodeQwen1.5-7B using this dataset, resulting in SelfCodeAlign-CQ-7B, which demonstrates robust performance on the HumanEval+ benchmark.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The performance is satisfactory: SelfCodeAlign-CQ-7B achieves a pass@1 score of 67.1 on HumanEval+, outperforming larger models like CodeLlama-70B-Instruct (65.2), which is a significant achievement.
2. The process is auto-mated: This paper introduces a novel self-alignment pipeline including concept extraction from seed code, task generation, multiple response generation, and execution validation. This approach is independent of human annotations or large model distillation, making it easy to be applied.
3. Scalability: Experiments demonstrate the method's applicability to models ranging from 3B to 33B parameters, showing good scalability across different model sizes.

Weaknesses:
1. Lack of Diversity in Generated Tasks: While the method aims to produce a variety of coding tasks, it is unclear how this diversity is achieved or measured. There is a risk that the generated tasks may be biased towards certain types of coding problems, which could limit the model's ability to generalize effectively.
2. Overreliance on Self-Generated Tests: The method relies heavily on tests generated by the model itself to validate responses. This self-validation approach could result in a feedback loop where the model learns to create tests that are easy to pass, rather than generating truly challenging or comprehensive tests. The paper does not address how this potential issue is mitigated.

Limitations:
Refer to the weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a pipeline for generating synthetic instruction tuning data. The method consists of the following steps: 1. data filtering is applied to seed coding data to select high quality examples; 2. base LLM is used to generate a set of coding concept and category based on the seed data; 3. base LLM is used to generate coding instruction, response and test; 4. generated examples are selected based on the code execution result.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. the paper focuses on using base model to generate synthetic data to self-improve, which is an interesting and useful angle for synthetic data generation
2. the method is evaluated on several different coding LLM benchmarks which shows the effectiveness of the method
3. there are also ablation experiments verifying the contribution of specific design choices in the framework.

Weaknesses:
While using base model to self-improve is an interesting and useful direction, synthetic data generation could be improved by using a stronger LLM than the base model. It is not clear from the paper whether the proposed framework would be effective compared to previous methods if we use a stronger LLM to synthesize the data. The synthetic data generation could also be potentially improved by having multiple rounds of data generation process.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors proposed SelfCodeAlign that finetunes the model based on the filtered data generated by the same model itself. The authors conduct experiments to show that SelfCodeAlign outperforms most open-sourced models that were finetuned on public code dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The code generation problem is important and the results (compared to models trained on public dataset) are promising.

Weaknesses:
Compare to models that are distilled/trained on non-disclosed data, the performance of SelfCodeAlign is not as competitive. The presentation can be improved, see **questions**.

Limitations:
Limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
- The paper introduces SelfCodeAlign, a fully transparent and permissive self-alignment pipeline for code generation in LLMs without relying on extensive human annotations or distillation from larger models. SelfCodeAlign generates instruction-response pairs from seed snippets, evaluates responses with test cases, and fine-tunes models based on successful executions. The approach shows superior performance over state-of-the-art methods, including GPT-3.5-Turbo-based distillation, particularly in HumanEval+ benchmark. The pipeline demonstrates effectiveness across various model sizes, emphasizing the benefits of self-generated data over teacher models with smaller performance gaps. 
- Overall, I feel that SelfCodeAlign is a very easy workflow to follow and I see much potential for such pipelines that do not depend on distillation or human annotations. I recommend an accept.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
## Originality
- The paper adequately cites related work, clearly identifying gaps such as the lack of transparency in existing methods, which is a key motivation for their work. 

## Quality
- The submission is technically sound with both quantitative and qualitative analysis.
- The authors provide detailed experimental results, demonstrating significant performance improvements over baselines.
- The inclusion of both large-scale and small-scale model evaluations further strengthens the quality of the research.
- In terms of ethical considerations, they have considered all terms of use as well as the data in code snippets.

## Clarity
- Well organized paper, except for appendix.

## Significance
- The results are highly significant, as SelfCodeAlign achieves performance improvements, notably surpassing models that are an order of magnitude larger in size. This work addresses the challenge of instruction tuning without human annotations or distillation, offering a scalable and transparent solution that advances the state of the art in code generation.

Weaknesses:
## Originality
- Perhaps similar to this paper [Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models](https://arxiv.org/pdf/2312.06585) ? Even if it is different, I think that this should also be part of your baseline comparison as well.

## Quality
- The qualitative examples provided in the appendix are excessively long, which may overwhelm the reader and obscure the main differences and contributions of the methodology. It would be beneficial to reduce the number of examples or to shorten them, focusing on highlighting the key differences and improvements over baseline methods. Additionally, the examples are presented in black and white with no descriptions or annotations, making it difficult to discern their significance. Providing clearer, annotated examples with concise explanations would enhance the readability and impact of this section.
- I do not see any weaknesses discussed in this work, for example, in what scenario do you think does this methodology not work? Why is the score still not perfect? (or for eg, below 80% accuracy)

Limitations:
Limitations are stated. However, I think the authors miss one important point – the reliance on the seed snippets. The performance of the model is based on what it was finetuned on, so if the seed snippets are not sufficiently diverse or representative of the target tasks, then it might have resulted in a large drop in accuracy.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces SelfCodeAlign, an entirely transparent and permissive pipeline designed for self-aligning code large language models without the need for human annotations or distillation. By applying SelfCodeAlign to CodeQwen1.5-7B, the authors generated a dataset containing 74k instruction-response pairs. They then fine-tuned CodeQwen1.5-7B using this dataset, resulting in SelfCodeAlign-CQ-7B, which demonstrates robust performance on the HumanEval+ benchmark.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The performance is satisfactory: SelfCodeAlign-CQ-7B achieves a pass@1 score of 67.1 on HumanEval+, outperforming larger models like CodeLlama-70B-Instruct (65.2), which is a significant achievement.
2. The process is auto-mated: This paper introduces a novel self-alignment pipeline including concept extraction from seed code, task generation, multiple response generation, and execution validation. This approach is independent of human annotations or large model distillation, making it easy to be applied.
3. Scalability: Experiments demonstrate the method's applicability to models ranging from 3B to 33B parameters, showing good scalability across different model sizes.

Weaknesses:
1. Lack of Diversity in Generated Tasks: While the method aims to produce a variety of coding tasks, it is unclear how this diversity is achieved or measured. There is a risk that the generated tasks may be biased towards certain types of coding problems, which could limit the model's ability to generalize effectively.
2. Overreliance on Self-Generated Tests: The method relies heavily on tests generated by the model itself to validate responses. This self-validation approach could result in a feedback loop where the model learns to create tests that are easy to pass, rather than generating truly challenging or comprehensive tests. The paper does not address how this potential issue is mitigated.

Limitations:
Refer to the weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a pipeline for generating synthetic instruction tuning data. The method consists of the following steps: 1. data filtering is applied to seed coding data to select high quality examples; 2. base LLM is used to generate a set of coding concept and category based on the seed data; 3. base LLM is used to generate coding instruction, response and test; 4. generated examples are selected based on the code execution result.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. the paper focuses on using base model to generate synthetic data to self-improve, which is an interesting and useful angle for synthetic data generation
2. the method is evaluated on several different coding LLM benchmarks which shows the effectiveness of the method
3. there are also ablation experiments verifying the contribution of specific design choices in the framework.

Weaknesses:
While using base model to self-improve is an interesting and useful direction, synthetic data generation could be improved by using a stronger LLM than the base model. It is not clear from the paper whether the proposed framework would be effective compared to previous methods if we use a stronger LLM to synthesize the data. The synthetic data generation could also be potentially improved by having multiple rounds of data generation process.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xW6ga9i4eA;"REVIEW 
Summary:
The authors address a key issue in personalized federated learning, which enables clients with heterogeneous model structures to participate in federated learning with consideration of effectiveness and efficiency. This method is based on model assembly and reassembly, in which the blocks and layers can be treated as modules. After that, the server selects the personalized models and assigns them to the clients. The received models will be used as the teacher to guide the local update. The authors run extensive experiments to demonstrate the effectiveness of their algorithm.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-organized and clearly motivated. Its logical structure and presentation aid comprehension, while the clear and accessible framework and figures enhance readability. Experiments, discussions, or analyses robustly support each claim.

2. The focus on controllability renders the algorithm more applicable in real-world scenarios, allowing for greater human involvement in the model generation process. The authors effectively demonstrate the utility of their design through experimental results.

3. The authors have performed extensive experiments, including principal studies on image datasets, ablation studies, hyperparameter evaluations, and thorough discussions. These efforts confirm the validity of the techniques and provide deep insights into the paper's contributions.

Weaknesses:
1. Based on the algorithm itself, it includes the reassembly, assembly, matching, and other operations. The reviewer may be concerned about the computational burden compared with the one without any controllability. 

2. How to select the anchor block and why needs to be stated clearly.

3. According to the experiment results, the reviewer is wondering about how this approach can be used with the public data with/without labels and the possible reason why it is robust to the public data with or without labels.

Limitations:
No negative social impact to the reviewer’s best knowledge. The study of K should be put in the main content as that is an important part of the algorithm.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a controllable model reassembly approach to enable heterogeneous model cooperation in federated learning. The designed CMSR algorithm provides the control of the space to save the computational cost.  Furthermore, the approach also achieves model personalization for each local client. They test the proposed approach on benchmark datasets and compare with other baselines under different settings.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1, This paper targets one of the challenges in federated learning, which is the model heterogeneity. To the best knowledge, most existing related works are based on knowledge distillation.  This work presents a controllable approach to conduct block assembly and reassembly from local models to achieve heterogeneous model cooperation and model personalization. The idea itself is interesting and practical.
2, They take efficiency, generalization, and personalization into consideration. They provide comprehensive analysis and provide detailed discussion under various settings, which support their statement soundly. 
3, Their presentation and logic are both easy to follow and understand. The framework, experiment results, and discussion are clearly presented.

Weaknesses:
Weakness
1, In their approach, the authors employ K-means clustering. The reviewer is curious about how the value of K is selected and how this selection influences the results.

2, One of the main contributions compared to pFedHR is the enhanced controllability. I am interested in understanding the nature of this controllability, specifically the extent to which the generated models can be controlled.

3, The paper focuses solely on image classification. Adhering to the review guidelines, the reviewer is not requesting additional experiments, but the reviewer is interested in exploring whether the existing methodology could be applicable to other tasks.

Limitations:
There are no potential negative societal impacts of the work. There are two limitations as follows:
(1)	This approach still raises extra computational cost at the server side. 
(2)	That would be great if this approach can be extended to other tasks and other domains.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a `pFedClub` method for personalized federated learning that enables controllable heterogeneous model aggregation, addressing limitations of existing approaches such as lack of personalization, privacy concerns, and uncontrolled model size growth.

Extensive experiments conducted on three benchmark datasets using various CNN-based model structures validate the effectiveness of the proposed method under both IID and non-IID settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- They conduct extensive experiment including the discussion about the hyparameter $K$ to validate the controllability of the proposed method and computational efficiency on the server.

Weaknesses:
1. The writing and structure of the paper need improvement, particularly in the ""Order-constrained Block Search"" paragraph. The concept of order is unclear, especially the meaning of $q < u$ in line 177. It's not evident whether this refers to a similarity score or another metric. The author should provide a clearer explanation of this constraint.
2. In equation (1) on line 141, the meaning of 'CKA' is not defined. The authors should explain what CKA stands for and how it's calculated. Additionally, it's unclear whether this computation occurs on the server. If clients must transmit input $x_{m,i}^t$ and output to the server, this raises privacy concerns that should be addressed.
3. The paper doesn't specify whether the features $x_{m,i}^t$ and $x_{n,j}^t$ in equation (1) have the same dimensions. This should be clarified to ensure a proper understanding of the similarity calculation.
4. The sampling process for the Anchor Block selection is ambiguous. The probability distribution over all models for this selection is not clearly defined.

Overall, the authors should formulate the proposed method more rigorously, using well-defined notations and providing clear explanations for each step of the algorithm. This would significantly improve the paper's readability and reproducibility.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses heterogeneous model aggregation in federated learning. To this end, the authors introduce pFedClub, which aims to generate personalized models for federated clients while ensuring that the models remain within size constraints. Specifically, pFedClub consists of three main steps: first, it decomposes models into multiple blocks and clusters them using the K-means algorithm; second, it replaces original blocks with others from the same clusters to create a set of candidate models; third, it selects the optimal personalized model for each client using a public dataset and an initial model transferred to the server. Extensive experiments illustrate its significant improvement over existing methods in this field.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The work is well motivated and explores an interesting problem in federated learning. 
2. The presentation of this paper is clear, and the authors comprehensively and intuitively describe the proposed pFedClub. 
3. The paper conducts sufficient experiments and compares the proposed method with previous works. The numerical results demonstrate the superiority of pFedClub.

Weaknesses:
1. The proposed work requires a public dataset, which is unsuitable in federated learning due to the privacy concerns. Is this work applicable to a public dataset different from the training data distribution? For example, the clients collaboratively train a model for CIFAR-10, while the server holds a public dataset from ImageNet. 
2. Although the proposed work achieves remarkable under convolutional neural networks, it is unclear how pFedClub performs under transformers. Is the proposed work suitable for a setting where clients hold three different sizes of LLM, i.e., LLaMA-7B, LLaMA-13B, and LLaMA-70B?

Limitations:
See **Weaknesses**

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors address a key issue in personalized federated learning, which enables clients with heterogeneous model structures to participate in federated learning with consideration of effectiveness and efficiency. This method is based on model assembly and reassembly, in which the blocks and layers can be treated as modules. After that, the server selects the personalized models and assigns them to the clients. The received models will be used as the teacher to guide the local update. The authors run extensive experiments to demonstrate the effectiveness of their algorithm.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-organized and clearly motivated. Its logical structure and presentation aid comprehension, while the clear and accessible framework and figures enhance readability. Experiments, discussions, or analyses robustly support each claim.

2. The focus on controllability renders the algorithm more applicable in real-world scenarios, allowing for greater human involvement in the model generation process. The authors effectively demonstrate the utility of their design through experimental results.

3. The authors have performed extensive experiments, including principal studies on image datasets, ablation studies, hyperparameter evaluations, and thorough discussions. These efforts confirm the validity of the techniques and provide deep insights into the paper's contributions.

Weaknesses:
1. Based on the algorithm itself, it includes the reassembly, assembly, matching, and other operations. The reviewer may be concerned about the computational burden compared with the one without any controllability. 

2. How to select the anchor block and why needs to be stated clearly.

3. According to the experiment results, the reviewer is wondering about how this approach can be used with the public data with/without labels and the possible reason why it is robust to the public data with or without labels.

Limitations:
No negative social impact to the reviewer’s best knowledge. The study of K should be put in the main content as that is an important part of the algorithm.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a controllable model reassembly approach to enable heterogeneous model cooperation in federated learning. The designed CMSR algorithm provides the control of the space to save the computational cost.  Furthermore, the approach also achieves model personalization for each local client. They test the proposed approach on benchmark datasets and compare with other baselines under different settings.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1, This paper targets one of the challenges in federated learning, which is the model heterogeneity. To the best knowledge, most existing related works are based on knowledge distillation.  This work presents a controllable approach to conduct block assembly and reassembly from local models to achieve heterogeneous model cooperation and model personalization. The idea itself is interesting and practical.
2, They take efficiency, generalization, and personalization into consideration. They provide comprehensive analysis and provide detailed discussion under various settings, which support their statement soundly. 
3, Their presentation and logic are both easy to follow and understand. The framework, experiment results, and discussion are clearly presented.

Weaknesses:
Weakness
1, In their approach, the authors employ K-means clustering. The reviewer is curious about how the value of K is selected and how this selection influences the results.

2, One of the main contributions compared to pFedHR is the enhanced controllability. I am interested in understanding the nature of this controllability, specifically the extent to which the generated models can be controlled.

3, The paper focuses solely on image classification. Adhering to the review guidelines, the reviewer is not requesting additional experiments, but the reviewer is interested in exploring whether the existing methodology could be applicable to other tasks.

Limitations:
There are no potential negative societal impacts of the work. There are two limitations as follows:
(1)	This approach still raises extra computational cost at the server side. 
(2)	That would be great if this approach can be extended to other tasks and other domains.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a `pFedClub` method for personalized federated learning that enables controllable heterogeneous model aggregation, addressing limitations of existing approaches such as lack of personalization, privacy concerns, and uncontrolled model size growth.

Extensive experiments conducted on three benchmark datasets using various CNN-based model structures validate the effectiveness of the proposed method under both IID and non-IID settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- They conduct extensive experiment including the discussion about the hyparameter $K$ to validate the controllability of the proposed method and computational efficiency on the server.

Weaknesses:
1. The writing and structure of the paper need improvement, particularly in the ""Order-constrained Block Search"" paragraph. The concept of order is unclear, especially the meaning of $q < u$ in line 177. It's not evident whether this refers to a similarity score or another metric. The author should provide a clearer explanation of this constraint.
2. In equation (1) on line 141, the meaning of 'CKA' is not defined. The authors should explain what CKA stands for and how it's calculated. Additionally, it's unclear whether this computation occurs on the server. If clients must transmit input $x_{m,i}^t$ and output to the server, this raises privacy concerns that should be addressed.
3. The paper doesn't specify whether the features $x_{m,i}^t$ and $x_{n,j}^t$ in equation (1) have the same dimensions. This should be clarified to ensure a proper understanding of the similarity calculation.
4. The sampling process for the Anchor Block selection is ambiguous. The probability distribution over all models for this selection is not clearly defined.

Overall, the authors should formulate the proposed method more rigorously, using well-defined notations and providing clear explanations for each step of the algorithm. This would significantly improve the paper's readability and reproducibility.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses heterogeneous model aggregation in federated learning. To this end, the authors introduce pFedClub, which aims to generate personalized models for federated clients while ensuring that the models remain within size constraints. Specifically, pFedClub consists of three main steps: first, it decomposes models into multiple blocks and clusters them using the K-means algorithm; second, it replaces original blocks with others from the same clusters to create a set of candidate models; third, it selects the optimal personalized model for each client using a public dataset and an initial model transferred to the server. Extensive experiments illustrate its significant improvement over existing methods in this field.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The work is well motivated and explores an interesting problem in federated learning. 
2. The presentation of this paper is clear, and the authors comprehensively and intuitively describe the proposed pFedClub. 
3. The paper conducts sufficient experiments and compares the proposed method with previous works. The numerical results demonstrate the superiority of pFedClub.

Weaknesses:
1. The proposed work requires a public dataset, which is unsuitable in federated learning due to the privacy concerns. Is this work applicable to a public dataset different from the training data distribution? For example, the clients collaboratively train a model for CIFAR-10, while the server holds a public dataset from ImageNet. 
2. Although the proposed work achieves remarkable under convolutional neural networks, it is unclear how pFedClub performs under transformers. Is the proposed work suitable for a setting where clients hold three different sizes of LLM, i.e., LLaMA-7B, LLaMA-13B, and LLaMA-70B?

Limitations:
See **Weaknesses**

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xUoNgR1Byy;"REVIEW 
Summary:
This submission tries to tackle one big question in the field of interpreting the data-driven preference learnt by RLHF in human language. The technical path this submission took is to train probe on SAE features to distinguish between good and bad RLHF features.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
+ The attempt to interpret what happens during RLHF training is a good direction to pursue.
+ Releasing the SAE direction and training code could be an excellent news to the community.

Weaknesses:
+ Unclear why have to probe on top of SAE feature. SAE greatly increase the dimensions of the features, leading to overfitting---you can find a separating plane for whatever classification task in this high-D space. Lacking comparison to normal probing. 
+ Considering the problem from a dynamical perspective can be fruitful. Noted that the authors did ablate the features and observe a performance drop on preference dataset. But it's also interesting to see the progress of RLHF training, how it warps the features spaces, even the SAE features' relative importances.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The goal of this paper is to predict where patterns in LLM activations learned from RLHF diverge from the human preferences used for the RLHF training. 
Given a base model and an RLHF tuned version of it, the method involves first identifying the 5 layers with highest parameter difference according to an L2 norm. Then two auto-encoders are trained over the activations from these layers. The encoder and decoder weights of the autoencoder are tied, and the output from these is preferred for studying the activations as they are expected to be more sparse, condensed and interpretable than the raw activations.
At inference time, for each input, the activations from the high divergence layers are computed, passed through the autoencoder and then aggregated. Given a pair of contrasting inputs, a linear probe is trained to predict activation deltas using the above aggregated autoencoder output as input. The output of the probe is meant to be a predicted feedback signal that can be compared to the ground truth fine tuning feedback. For sentiment analysis, a strong correlation is observed with the Pythia-160m model but this is weaker for Pythia-70m and GPT-Neo-125m.  

For another validation the probes, GPT-4 is used to generate explanations of the features in the decoder weights of the autoencoders that get activated when the predicted feedback is positive. GPT4 is then prompted to predict whether or not these are relevant to the fine tuning task, based on a language description of the task. It is found that a feature identified by GPT-4 as relevant to the fine-tuning task is between twice and thrice as likely to be correlated with predicted positive feedback.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is quite accessible for a reader whose area of focus is not interpretability.

Weaknesses:
As a reviewer not particularly experienced with work on interpretability, the takeaways of this paper are somewhat unclear. For example, if we finetuned a new model on one of the datasets used in this paper and trained probes in a similar way from its activations, what would that tell us about the the difference between the base and RLHF versions of that model? Alternately, is the goal to discover information about a model where the base and RLHF-tuned versions are available but the data is not, and hence we do not know what factors might have influenced the preference annotations that guided the annotation. 

I did not fully understand how the activation deltas are calculated. While most of the paper is fairly readable to a reviewer with a different area of focus, this aspect could be improved.

Limitations:
The paper has discussed limitations but not broader impact of their method.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose an approach for measuring and interpreting the divergence between learned feedback patterns (LFPs, or simply the model's activation patterns) and the feedback reward distribution of the preference training dataset. To do so, they identify layers whose activations have moved the most during RLHF training and input these layers' activations into a sparse auto-encoder (SAE) that is trained to provide sparse representations of the LLM's activations. Then, they train probes to predict the feedback signal (e.g. reward, sentiment label) from the SAE's outputs. They use these probes both to measure the divergence of the LFPs from the actual feedback signals and to interpret which features are most important for the LFPs.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The authors ask an interesting question of whether we can measure and interpret the difference between a trained model's activation patterns and the preference distribution it has been trained on. The interpretability aspect of this question is interesting, since it can help us better understand what exactly the model has learned (or not learned) from its training dataset.
- The authors provide a good explanation of why sparse auto-encoders are being used for this task (rather than interpreting the raw model activations), as well as the limitations thereof.

Weaknesses:
- The effectiveness of this probing method seems to rely on many key assumptions being true, such as (i) sparse autoencoder outputs being more interpretable than the original model's outputs, (ii) sparse autoencoder output representations being faithful to the original model's representations, (iii) the probes being accurate, and (iv) GPT-4 being accurate/faithful when giving descriptions of each feature. There is very little experimental evidence provided for confirming that any of these assumptions are true, and these claims are difficult to test in the first place.
   - In fact, the authors mention that a likely reason for the low correlation between the probe's predictions and the VADER lexicon (for some models) is ""the complexity of the probe's task...a linear regression model is unlikely to recover such granular rewards accurately from just the activations"" (L265-266). Although they do find a high correlation for one model, the insufficiency of this probe implies that it is not effective for accurately measuring the divergence between the model's activation patterns and the feedback label distribution. If the correlation is low, we cannot tell whether that is the probe's failure, or if the model has not  acquired strong LFPs, or some combination of the two. Since this probing technique is a central contribution of the paper, I would expect stronger probes and more rigorous evaluation of the effectiveness of the probes.
   - How can one ensure that GPT-4's interpretations of the features are accurate or faithful?
- Table 5 purports to check whether the predicted LFP-related features were actually important and useful to the LLM, but the numbers before and after ablation are often very close together (or identical, in the case of GPT-Neo-125m). It would be helpful to report confidence intervals or standard errors to check whether these differences are significant. But as it currently stands, this table's results does not seem to strongly support the claim that the predicted LFP-related features are indeed relevant to/critical for LFPs.

- Lack of clarity in explaining methods:
    - Much of the writing about the methods is unclear, contradictory, or omits many details. For instance, the explanation of the logistic regression probe in L233-234 says ""we label the concatenated activations as positive or negative based on the averaged activation deltas for each token over the entire input sequence, and train a logistic regression model to classify the activations,"" which would suggest that this probe's inputs are the activations. But L493 (in the appendix) says ""...we give a positive or negative label to concatenated autoencoder outputs based on the sign of their activation delta. We then train a logistic regression model to predict the labels from the concatenated autoencoder outputs,"" which suggests that the inputs are actually the autoencoder outputs, not the original model's activations. Which is it?
   - In Section 3.4, how is GPT-4 prompted to provide the explanations?
   - Given how confusing and verbose the methodology is, I would encourage the authors to write out some of the procedures in equation form, rather than long paragraphs of text.

Limitations:
The limitations section was well-written and thorough, and covered many of the concerns I had myself. An additional limitation is that this method is computationally expensive and requires both training another model and running inference on a sufficiently powerful LLM (e.g. GPT-4) to interpret the features. In this paper, most of the results were for smaller models (under 1B params), and it is unclear whether this method would be scalable to larger models.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates how large language models (LLMs) learn preferences from human feedback during fine-tuning using reinforcement learning (RLHF). The authors introduce the concept of Learned Feedback Patterns (LFPs) to describe activation patterns in LLMs that align with human feedback. They aim to measure the accuracy of these patterns in capturing human preferences by training probes on condensed representations of LLM activations. The probes predict the implicit feedback signal in these activations and compare it to true feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The introduction of LFPs provides a new perspective on understanding how LLMs learn from human feedback. This concept helps in quantifying and interpreting the alignment between LLM activations and human preferences.

- The authors validate their probes by comparing neural features correlated with positive feedback against GPT-4’s descriptions of relevant features. This cross-validation strengthens the reliability of their findings.

- The use of synthetic datasets to elicit specific activation patterns in LLMs adds to the reproducibility and robustness of the study. These datasets are also made publicly available for further research.

Weaknesses:
- The study primarily focuses on a few specific models (e.g., Pythia-70m, GPT-Neo-125m) and tasks (sentiment generation, toxicity), which might limit the generalizability of the findings across different LLMs and applications. More recently released models are of more value for studying RLHF patterns and verify that the method can be generalized. The patterns are easy to extract because that the used data are quite obvious to encode and decode. 

- While the probes show significant accuracy for certain tasks, the paper notes weaker correlations for more granular reward predictions, suggesting that the approach might struggle with highly detailed feedback signals. The issue of feature superposition in dense, high-dimensional activation spaces poses a challenge to fully interpreting the learned features. Although sparse autoencoders mitigate this to some extent, the problem remains a significant obstacle.

- The validation process relies on GPT-4’s ability to describe neural features, which introduces a dependency on another model’s interpretability. This could introduce biases or inaccuracies if GPT-4’s descriptions are not perfectly reliable.

- The paper acknowledges that while their method identifies features involved in feedback signals, it does not provide a mechanistic explanation of how these features interact or influence the expected feedback signal. This limits the depth of interpretability.

Limitations:
discussed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates how large language models (LLMs) learn preferences from human feedback during fine-tuning using reinforcement learning (RLHF). The authors introduce the concept of Learned Feedback Patterns (LFPs) to describe activation patterns in LLMs that align with human feedback. They aim to measure the accuracy of these patterns in capturing human preferences by training probes on condensed representations of LLM activations. The probes predict the implicit feedback signal in these activations and compare it to true feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The introduction of LFPs provides a new perspective on understanding how LLMs learn from human feedback. This concept helps in quantifying and interpreting the alignment between LLM activations and human preferences.

- The authors validate their probes by comparing neural features correlated with positive feedback against GPT-4’s descriptions of relevant features. This cross-validation strengthens the reliability of their findings.

- The use of synthetic datasets to elicit specific activation patterns in LLMs adds to the reproducibility and robustness of the study. These datasets are also made publicly available for further research.

Weaknesses:
- The study primarily focuses on a few specific models (e.g., Pythia-70m, GPT-Neo-125m) and tasks (sentiment generation, toxicity), which might limit the generalizability of the findings across different LLMs and applications. More recently released models are of more value for studying RLHF patterns and verify that the method can be generalized. The patterns are easy to extract because that the used data are quite obvious to encode and decode. 

- While the probes show significant accuracy for certain tasks, the paper notes weaker correlations for more granular reward predictions, suggesting that the approach might struggle with highly detailed feedback signals. The issue of feature superposition in dense, high-dimensional activation spaces poses a challenge to fully interpreting the learned features. Although sparse autoencoders mitigate this to some extent, the problem remains a significant obstacle.

- The validation process relies on GPT-4’s ability to describe neural features, which introduces a dependency on another model’s interpretability. This could introduce biases or inaccuracies if GPT-4’s descriptions are not perfectly reliable.

- The paper acknowledges that while their method identifies features involved in feedback signals, it does not provide a mechanistic explanation of how these features interact or influence the expected feedback signal. This limits the depth of interpretability.

Limitations:
discussed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This submission tries to tackle one big question in the field of interpreting the data-driven preference learnt by RLHF in human language. The technical path this submission took is to train probe on SAE features to distinguish between good and bad RLHF features.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
+ The attempt to interpret what happens during RLHF training is a good direction to pursue.
+ Releasing the SAE direction and training code could be an excellent news to the community.

Weaknesses:
+ Unclear why have to probe on top of SAE feature. SAE greatly increase the dimensions of the features, leading to overfitting---you can find a separating plane for whatever classification task in this high-D space. Lacking comparison to normal probing. 
+ Considering the problem from a dynamical perspective can be fruitful. Noted that the authors did ablate the features and observe a performance drop on preference dataset. But it's also interesting to see the progress of RLHF training, how it warps the features spaces, even the SAE features' relative importances.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The goal of this paper is to predict where patterns in LLM activations learned from RLHF diverge from the human preferences used for the RLHF training. 
Given a base model and an RLHF tuned version of it, the method involves first identifying the 5 layers with highest parameter difference according to an L2 norm. Then two auto-encoders are trained over the activations from these layers. The encoder and decoder weights of the autoencoder are tied, and the output from these is preferred for studying the activations as they are expected to be more sparse, condensed and interpretable than the raw activations.
At inference time, for each input, the activations from the high divergence layers are computed, passed through the autoencoder and then aggregated. Given a pair of contrasting inputs, a linear probe is trained to predict activation deltas using the above aggregated autoencoder output as input. The output of the probe is meant to be a predicted feedback signal that can be compared to the ground truth fine tuning feedback. For sentiment analysis, a strong correlation is observed with the Pythia-160m model but this is weaker for Pythia-70m and GPT-Neo-125m.  

For another validation the probes, GPT-4 is used to generate explanations of the features in the decoder weights of the autoencoders that get activated when the predicted feedback is positive. GPT4 is then prompted to predict whether or not these are relevant to the fine tuning task, based on a language description of the task. It is found that a feature identified by GPT-4 as relevant to the fine-tuning task is between twice and thrice as likely to be correlated with predicted positive feedback.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is quite accessible for a reader whose area of focus is not interpretability.

Weaknesses:
As a reviewer not particularly experienced with work on interpretability, the takeaways of this paper are somewhat unclear. For example, if we finetuned a new model on one of the datasets used in this paper and trained probes in a similar way from its activations, what would that tell us about the the difference between the base and RLHF versions of that model? Alternately, is the goal to discover information about a model where the base and RLHF-tuned versions are available but the data is not, and hence we do not know what factors might have influenced the preference annotations that guided the annotation. 

I did not fully understand how the activation deltas are calculated. While most of the paper is fairly readable to a reviewer with a different area of focus, this aspect could be improved.

Limitations:
The paper has discussed limitations but not broader impact of their method.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose an approach for measuring and interpreting the divergence between learned feedback patterns (LFPs, or simply the model's activation patterns) and the feedback reward distribution of the preference training dataset. To do so, they identify layers whose activations have moved the most during RLHF training and input these layers' activations into a sparse auto-encoder (SAE) that is trained to provide sparse representations of the LLM's activations. Then, they train probes to predict the feedback signal (e.g. reward, sentiment label) from the SAE's outputs. They use these probes both to measure the divergence of the LFPs from the actual feedback signals and to interpret which features are most important for the LFPs.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The authors ask an interesting question of whether we can measure and interpret the difference between a trained model's activation patterns and the preference distribution it has been trained on. The interpretability aspect of this question is interesting, since it can help us better understand what exactly the model has learned (or not learned) from its training dataset.
- The authors provide a good explanation of why sparse auto-encoders are being used for this task (rather than interpreting the raw model activations), as well as the limitations thereof.

Weaknesses:
- The effectiveness of this probing method seems to rely on many key assumptions being true, such as (i) sparse autoencoder outputs being more interpretable than the original model's outputs, (ii) sparse autoencoder output representations being faithful to the original model's representations, (iii) the probes being accurate, and (iv) GPT-4 being accurate/faithful when giving descriptions of each feature. There is very little experimental evidence provided for confirming that any of these assumptions are true, and these claims are difficult to test in the first place.
   - In fact, the authors mention that a likely reason for the low correlation between the probe's predictions and the VADER lexicon (for some models) is ""the complexity of the probe's task...a linear regression model is unlikely to recover such granular rewards accurately from just the activations"" (L265-266). Although they do find a high correlation for one model, the insufficiency of this probe implies that it is not effective for accurately measuring the divergence between the model's activation patterns and the feedback label distribution. If the correlation is low, we cannot tell whether that is the probe's failure, or if the model has not  acquired strong LFPs, or some combination of the two. Since this probing technique is a central contribution of the paper, I would expect stronger probes and more rigorous evaluation of the effectiveness of the probes.
   - How can one ensure that GPT-4's interpretations of the features are accurate or faithful?
- Table 5 purports to check whether the predicted LFP-related features were actually important and useful to the LLM, but the numbers before and after ablation are often very close together (or identical, in the case of GPT-Neo-125m). It would be helpful to report confidence intervals or standard errors to check whether these differences are significant. But as it currently stands, this table's results does not seem to strongly support the claim that the predicted LFP-related features are indeed relevant to/critical for LFPs.

- Lack of clarity in explaining methods:
    - Much of the writing about the methods is unclear, contradictory, or omits many details. For instance, the explanation of the logistic regression probe in L233-234 says ""we label the concatenated activations as positive or negative based on the averaged activation deltas for each token over the entire input sequence, and train a logistic regression model to classify the activations,"" which would suggest that this probe's inputs are the activations. But L493 (in the appendix) says ""...we give a positive or negative label to concatenated autoencoder outputs based on the sign of their activation delta. We then train a logistic regression model to predict the labels from the concatenated autoencoder outputs,"" which suggests that the inputs are actually the autoencoder outputs, not the original model's activations. Which is it?
   - In Section 3.4, how is GPT-4 prompted to provide the explanations?
   - Given how confusing and verbose the methodology is, I would encourage the authors to write out some of the procedures in equation form, rather than long paragraphs of text.

Limitations:
The limitations section was well-written and thorough, and covered many of the concerns I had myself. An additional limitation is that this method is computationally expensive and requires both training another model and running inference on a sufficiently powerful LLM (e.g. GPT-4) to interpret the features. In this paper, most of the results were for smaller models (under 1B params), and it is unclear whether this method would be scalable to larger models.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xUjBZR6b1T;"REVIEW 
Summary:
ReVideo presents a novel view of video editing by modifying content with input trajectory to create new content. It designs a three-stage strategy to wrestle out the problem of ignoring motion control when direct training. The main contribution of this work relies on the new task of editing motion via user-specified trajectory while keeping the original video movement. The editing results are superior and photorealistic.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The first video editing work on creating new motion and content.
2. Good writing; the paper is easy to follow, and the motivation and three-stage training strategy on decoupling content and motion control is reasonable. The proposed SAFM learned a dynamic fusion weight at different timesteps.
3. The editing results are photorealistic and adhere to the original motion or follow user-specified trajectory with no artifacts.

Weaknesses:
1. The author did not provide the method or explanation of how ReVideo edits the first frame, making the total editing pipeline not end-to-end for users.
2. Part of the original video motion, like mouth movement in the Zuckerberg->robot (head6) and tail movement in dog->lion, is not kept in the edited video.
3. I would like to know how the drag-based editing method handles non-rigid motion, such as the rotation of car tires from a side view. In examples like sea2 and sea2_2, where a shark and a dinosaur are added, the limbs of the animal seem unable to move, making the video look unrealistic. However, in soccer and some human-centric examples, the legs of dogs and people can move normally. Therefore, I would like the authors to add an example of a vehicle moving on the road from a side view, including the movement of the wheels, to address my concerns. This may be a limitation of the drag-based method.
3. There is no quantitative comparison of the ablation study; I understand that the image results in Fig 7 are clear, but only one video qualitative ablation is not reasonable. 
4. There are no qualitative video comparisons with other methods in the supp or project page, but only Fig 6, and the automatic metrics are worse than pika even though I understand the clip scores are not accurate, which can not reflect temporal consistency accurately. I suggest the author supply the comparison video between Revideo and other methods in the rebuttal phase.
5. The training cost of three stages: even though Revideo makes great progress in creating new motion, training cost like GPU costs, time costs, memory costs and so on, is still a problem since users prefer to edit a video in a zero-shot manner when using a pretrained video generation model and the compared methods like AnyV2V is training-free.

Limitations:
no significant limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a video editing method that enables precise localized adjustments to content and motion within specific areas of a video. It introduces a three-stage training strategy and a spatiotemporal adaptive fusion module to integrate edits across frames and locations effectively. This method allows for complex editing tasks such as changing content while maintaining motion, adding new motion to static content, and simultaneously modifying both elements.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper introduces a novel challenge of editing both content and motion in specific video areas and combines techniques from diffusion models and video editing to achieve nuanced control.
- The three-stage training strategy enhances the robustness and effectiveness of the edits, supported by experimental validation that demonstrates superior performance compared to existing methods.
- The paper is well-organized and clearly explains complex concepts, including the innovative spatiotemporal adaptive fusion module and detailed training strategy.

Weaknesses:
- The decoupling training could cause some artifacts. Although the paper demonstrates these artifacts could mostly be alleviated by deblocking training. I can still see some blocky/unnatural results in the result videos.
- The training is quite complicated and separated into three stages. I feel the training strategy could 'overfit' this particular video dataset.
- This method is more like a direct combination of video diffusion and ControlNet.
- More detailed implementation specifics, particularly regarding parameter settings and the architecture of the spatiotemporal adaptive fusion module, are needed.
- The method's computational demands and potential scalability issues are not adequately addressed. For example, what kind of GPU does one need to perform training and testing? 
- The paper focuses heavily on technical aspects with less consideration of user interaction.

Limitations:
Quality limited by SVD.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents ReVideo, a new approach for precise local video editing of both content and motion. It introduces a coarse-to-fine training strategy to progressively decouple content and motion control, and a spatiotemporal adaptive fusion module to integrate them effectively. Experiments show ReVideo can modify local video content, customize motion trajectories, or change both simultaneously, and extend to multi-region editing.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This appears to be the first attempt at exploring local editing of both content and motion in videos using diffusion models. Being able to modify content and motion trajectories in specific regions is a novel capability compared to prior work.
- The proposed three-stage coarse-to-fine training strategy to progressively decouple content and motion control is an interesting technical approach to deal with the core challenge.
- The spatiotemporal adaptive fusion module is another novel component to integrate the content and motion conditions across sampling - steps and spatial locations.
- Extending the approach to allow multi-area editing without requiring specific training demonstrates flexibility.
- Most of the visual and quantitative results show improvements over prior methods 

Overall, this paper addresses a timely and important topic with significant potential benefits for the community. Despite some weaknesses, the reviewer recommends acceptance, considering this is a relatively new area and the paper presents promising results. The score may be adjusted based on the quality of the rebuttal.

Weaknesses:
## Practicality of the Editing Workflow

The current editing interface requires users to specify both a target content image and a set of motion trajectories. While this allows for fine-grained control, it may not be the most intuitive or efficient workflow for common editing tasks. Consider the scenario of object removal - the user would need to carefully craft a content image with the object removed and ensure that the remaining motion trajectories are consistent. An alternative approach could be to directly specify the regions to remove and have the model infer the appropriate content and motion changes automatically. The paper would benefit from a more detailed discussion of the practical trade-offs and usability considerations of the proposed editing framework.

## Limited Motion Control
While the method allows for editing the motion of individual objects, it assumes that the overall scene motion (camera movement, background motion) remains fixed. This limits the applicability of the approach in scenarios where the goal is to modify the global motion patterns (e.g. stabilizing shaky footage, changing the camera viewpoint).

## Precise Placement and Key Contributions of this Paper

While the individual technical components (e.g. coarse-to-fine training, adaptive fusion) are well-motivated, it's worth considering whether similar strategies have been explored in related domains. For instance, progressive training to handle multi-factor variation has been used in GANs, and spatially-adaptive normalization is common in style transfer. Drawing more connections to such related work would clarify the novelty of the specific adaptations made here.

## Content-Motion Entanglement

- The key technical contribution of the paper is the decoupling of content and motion information through a coarse-to-fine training strategy. However, it's not clear if this decoupling is complete or if there are still some residual entanglements between the two factors. For instance, the edited content may still contain some motion information that could interfere with the specified motion trajectories, leading to artifacts or inconsistencies. A more thorough analysis of the content-motion separation and its impact on the editing quality would be informative.

- Is decoupling content and motion the only way to address the issue - could a joint representation learning approach work instead? Acknowledging alternate strategies would help justify the chosen approach.

- **Figure 4 is not very intuitive. It would benefit from additional justification, theoretical analysis, and insights into why such a simple composition from two videos is effective.** This is a key concern.

## Multi-area Editing 
- The extension to multi-area editing is a nice addition, but the paper could go further in characterizing the challenges involved. Are there issues with preserving global coherence across multiple edited regions? How does the method scale with the number of regions? Providing such details would give a more complete picture of the capability.

## Clarity and Reproducibility
- Implementation details: There are some missing specifics that could hamper reproducibility. For instance:

   - How exactly are the editing regions defined during training - what is the procedure for randomly sampling them?
   - What metrics are used for the ""threshold filtering"" of motion trajectories and how were the thresholds chosen?
   - Are there any data augmentation, regularization or optimization tricks used during training?

## Evaluation Metrics

The quantitative evaluation relies primarily on low-level metrics like PSNR and LPIPS, which may not fully capture the perceptual quality and coherence of the edited videos. Additional metrics could provide a more comprehensive assessment:

- Metrics that specifically measure the consistency of the edited regions with the target content and motion (e.g. using an object detector or tracker).
- Metrics that evaluate the temporal stability and smoothness of the edited videos (e.g. some metrics that are used in video inpainting tasks, Please refer to [this repo](https://github.com/MichiganCOG/video-inpainting-evaluation) for details).
- Human evaluations of the overall realism, coherence, and faithfulness to the editing inputs (e.g. through user studies).



## Robustness Evaluation and Ablation Studies

While the paper does include ablations for a few key components (e.g. SAFM, training stages), there are other design choices that are not fully explored. For instance:

   - How important is the choice of motion representation (trajectory vs. alternatives)? Testing with different motion inputs would reveal the sensitivity to this factor.
   - What is the impact of the trajectory sampling strategy and hyperparameters? Varying the number and selection of trajectories could provide insight into the robustness.
   - How does the performance vary with the size and shape of the editing regions? A systematic evaluation across different region properties would be informative.
   - Only the end-to-end video editing pipelines are compared, but not the individual technical components. For instance, how does SAFM compare to simpler fusion schemes used in prior work?
  - Input noise and perturbations (e.g. in the content image or motion trajectories)

## Dataset Complexity 

-  While the approach achieves good results on the chosen datasets, it's unclear how well it would generalize to more complex video content (e.g. with dynamic backgrounds, scene changes, occlusions etc.). Discussing the potential failure modes and current limitations would help scope the contribution appropriately.

- The examples shown in the paper are largely limited to simple object-level edits in relatively constrained scenarios (e.g. clean backgrounds, single objects). It's unclear how well the method would perform on more challenging videos with complex scenes, multiple objects, occlusions, camera motion, etc. Testing on a wider range of video complexity would help establish the generality of the approach.

## Editing Scenarios
The paper demonstrates a few key editing applications (e.g. object addition/removal, motion editing), but there are other important scenarios that are not explored, such as: performing semantic-level edits (e.g. changing the action or interaction between objects).
Showcasing the method's performance across a fuller range of editing tasks would demonstrate its versatility.

## Open Source
Will the code for training and inference be released?

Limitations:
Please refer to the weakness section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
ReVideo presents a novel view of video editing by modifying content with input trajectory to create new content. It designs a three-stage strategy to wrestle out the problem of ignoring motion control when direct training. The main contribution of this work relies on the new task of editing motion via user-specified trajectory while keeping the original video movement. The editing results are superior and photorealistic.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The first video editing work on creating new motion and content.
2. Good writing; the paper is easy to follow, and the motivation and three-stage training strategy on decoupling content and motion control is reasonable. The proposed SAFM learned a dynamic fusion weight at different timesteps.
3. The editing results are photorealistic and adhere to the original motion or follow user-specified trajectory with no artifacts.

Weaknesses:
1. The author did not provide the method or explanation of how ReVideo edits the first frame, making the total editing pipeline not end-to-end for users.
2. Part of the original video motion, like mouth movement in the Zuckerberg->robot (head6) and tail movement in dog->lion, is not kept in the edited video.
3. I would like to know how the drag-based editing method handles non-rigid motion, such as the rotation of car tires from a side view. In examples like sea2 and sea2_2, where a shark and a dinosaur are added, the limbs of the animal seem unable to move, making the video look unrealistic. However, in soccer and some human-centric examples, the legs of dogs and people can move normally. Therefore, I would like the authors to add an example of a vehicle moving on the road from a side view, including the movement of the wheels, to address my concerns. This may be a limitation of the drag-based method.
3. There is no quantitative comparison of the ablation study; I understand that the image results in Fig 7 are clear, but only one video qualitative ablation is not reasonable. 
4. There are no qualitative video comparisons with other methods in the supp or project page, but only Fig 6, and the automatic metrics are worse than pika even though I understand the clip scores are not accurate, which can not reflect temporal consistency accurately. I suggest the author supply the comparison video between Revideo and other methods in the rebuttal phase.
5. The training cost of three stages: even though Revideo makes great progress in creating new motion, training cost like GPU costs, time costs, memory costs and so on, is still a problem since users prefer to edit a video in a zero-shot manner when using a pretrained video generation model and the compared methods like AnyV2V is training-free.

Limitations:
no significant limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a video editing method that enables precise localized adjustments to content and motion within specific areas of a video. It introduces a three-stage training strategy and a spatiotemporal adaptive fusion module to integrate edits across frames and locations effectively. This method allows for complex editing tasks such as changing content while maintaining motion, adding new motion to static content, and simultaneously modifying both elements.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper introduces a novel challenge of editing both content and motion in specific video areas and combines techniques from diffusion models and video editing to achieve nuanced control.
- The three-stage training strategy enhances the robustness and effectiveness of the edits, supported by experimental validation that demonstrates superior performance compared to existing methods.
- The paper is well-organized and clearly explains complex concepts, including the innovative spatiotemporal adaptive fusion module and detailed training strategy.

Weaknesses:
- The decoupling training could cause some artifacts. Although the paper demonstrates these artifacts could mostly be alleviated by deblocking training. I can still see some blocky/unnatural results in the result videos.
- The training is quite complicated and separated into three stages. I feel the training strategy could 'overfit' this particular video dataset.
- This method is more like a direct combination of video diffusion and ControlNet.
- More detailed implementation specifics, particularly regarding parameter settings and the architecture of the spatiotemporal adaptive fusion module, are needed.
- The method's computational demands and potential scalability issues are not adequately addressed. For example, what kind of GPU does one need to perform training and testing? 
- The paper focuses heavily on technical aspects with less consideration of user interaction.

Limitations:
Quality limited by SVD.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents ReVideo, a new approach for precise local video editing of both content and motion. It introduces a coarse-to-fine training strategy to progressively decouple content and motion control, and a spatiotemporal adaptive fusion module to integrate them effectively. Experiments show ReVideo can modify local video content, customize motion trajectories, or change both simultaneously, and extend to multi-region editing.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This appears to be the first attempt at exploring local editing of both content and motion in videos using diffusion models. Being able to modify content and motion trajectories in specific regions is a novel capability compared to prior work.
- The proposed three-stage coarse-to-fine training strategy to progressively decouple content and motion control is an interesting technical approach to deal with the core challenge.
- The spatiotemporal adaptive fusion module is another novel component to integrate the content and motion conditions across sampling - steps and spatial locations.
- Extending the approach to allow multi-area editing without requiring specific training demonstrates flexibility.
- Most of the visual and quantitative results show improvements over prior methods 

Overall, this paper addresses a timely and important topic with significant potential benefits for the community. Despite some weaknesses, the reviewer recommends acceptance, considering this is a relatively new area and the paper presents promising results. The score may be adjusted based on the quality of the rebuttal.

Weaknesses:
## Practicality of the Editing Workflow

The current editing interface requires users to specify both a target content image and a set of motion trajectories. While this allows for fine-grained control, it may not be the most intuitive or efficient workflow for common editing tasks. Consider the scenario of object removal - the user would need to carefully craft a content image with the object removed and ensure that the remaining motion trajectories are consistent. An alternative approach could be to directly specify the regions to remove and have the model infer the appropriate content and motion changes automatically. The paper would benefit from a more detailed discussion of the practical trade-offs and usability considerations of the proposed editing framework.

## Limited Motion Control
While the method allows for editing the motion of individual objects, it assumes that the overall scene motion (camera movement, background motion) remains fixed. This limits the applicability of the approach in scenarios where the goal is to modify the global motion patterns (e.g. stabilizing shaky footage, changing the camera viewpoint).

## Precise Placement and Key Contributions of this Paper

While the individual technical components (e.g. coarse-to-fine training, adaptive fusion) are well-motivated, it's worth considering whether similar strategies have been explored in related domains. For instance, progressive training to handle multi-factor variation has been used in GANs, and spatially-adaptive normalization is common in style transfer. Drawing more connections to such related work would clarify the novelty of the specific adaptations made here.

## Content-Motion Entanglement

- The key technical contribution of the paper is the decoupling of content and motion information through a coarse-to-fine training strategy. However, it's not clear if this decoupling is complete or if there are still some residual entanglements between the two factors. For instance, the edited content may still contain some motion information that could interfere with the specified motion trajectories, leading to artifacts or inconsistencies. A more thorough analysis of the content-motion separation and its impact on the editing quality would be informative.

- Is decoupling content and motion the only way to address the issue - could a joint representation learning approach work instead? Acknowledging alternate strategies would help justify the chosen approach.

- **Figure 4 is not very intuitive. It would benefit from additional justification, theoretical analysis, and insights into why such a simple composition from two videos is effective.** This is a key concern.

## Multi-area Editing 
- The extension to multi-area editing is a nice addition, but the paper could go further in characterizing the challenges involved. Are there issues with preserving global coherence across multiple edited regions? How does the method scale with the number of regions? Providing such details would give a more complete picture of the capability.

## Clarity and Reproducibility
- Implementation details: There are some missing specifics that could hamper reproducibility. For instance:

   - How exactly are the editing regions defined during training - what is the procedure for randomly sampling them?
   - What metrics are used for the ""threshold filtering"" of motion trajectories and how were the thresholds chosen?
   - Are there any data augmentation, regularization or optimization tricks used during training?

## Evaluation Metrics

The quantitative evaluation relies primarily on low-level metrics like PSNR and LPIPS, which may not fully capture the perceptual quality and coherence of the edited videos. Additional metrics could provide a more comprehensive assessment:

- Metrics that specifically measure the consistency of the edited regions with the target content and motion (e.g. using an object detector or tracker).
- Metrics that evaluate the temporal stability and smoothness of the edited videos (e.g. some metrics that are used in video inpainting tasks, Please refer to [this repo](https://github.com/MichiganCOG/video-inpainting-evaluation) for details).
- Human evaluations of the overall realism, coherence, and faithfulness to the editing inputs (e.g. through user studies).



## Robustness Evaluation and Ablation Studies

While the paper does include ablations for a few key components (e.g. SAFM, training stages), there are other design choices that are not fully explored. For instance:

   - How important is the choice of motion representation (trajectory vs. alternatives)? Testing with different motion inputs would reveal the sensitivity to this factor.
   - What is the impact of the trajectory sampling strategy and hyperparameters? Varying the number and selection of trajectories could provide insight into the robustness.
   - How does the performance vary with the size and shape of the editing regions? A systematic evaluation across different region properties would be informative.
   - Only the end-to-end video editing pipelines are compared, but not the individual technical components. For instance, how does SAFM compare to simpler fusion schemes used in prior work?
  - Input noise and perturbations (e.g. in the content image or motion trajectories)

## Dataset Complexity 

-  While the approach achieves good results on the chosen datasets, it's unclear how well it would generalize to more complex video content (e.g. with dynamic backgrounds, scene changes, occlusions etc.). Discussing the potential failure modes and current limitations would help scope the contribution appropriately.

- The examples shown in the paper are largely limited to simple object-level edits in relatively constrained scenarios (e.g. clean backgrounds, single objects). It's unclear how well the method would perform on more challenging videos with complex scenes, multiple objects, occlusions, camera motion, etc. Testing on a wider range of video complexity would help establish the generality of the approach.

## Editing Scenarios
The paper demonstrates a few key editing applications (e.g. object addition/removal, motion editing), but there are other important scenarios that are not explored, such as: performing semantic-level edits (e.g. changing the action or interaction between objects).
Showcasing the method's performance across a fuller range of editing tasks would demonstrate its versatility.

## Open Source
Will the code for training and inference be released?

Limitations:
Please refer to the weakness section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xSziO6gQgG;"REVIEW 
Summary:
This paper studies the implicit bias of the gradient descent on the Next-Token Prediction (LTP) problem in linear models. They first formulate this NTP problem as minimizing the cross-entropy (CE) loss over distinct contexts, each tied with a sparse conditional probability over the token space. They then provide the necessary conditions for the CE loss to reach the entropy lower bound, i.e., the NTP-compatible condition and the NTP-separable condition. Then, they prove one sufficient condition for those two conditions is oevrparameterization, i.e., the dimension of the embedding space d is larger than the number of distinct contexts in the dataset. Assuming both compatible and separable conditions, they then prove the directional convergence of the minimizer of the CE loss within a certain range and the directional convergence of the GD iterate towards the direction of the solution of an NTP-SVM.

In general, I think this paper delves into a good and important problem: the optimization path and implicit bias of NTP mechanism. The authors provided a good formulation, and the proof is solid.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. They investigate an interesting and important problem: the optimization path and the implicit bias of NTP.

2. Their formulation of NTP into the CE minimization over distinct contexts is novel.

3. They provide rigorous theoretical results and the proofs are solid, to my knowledge.

Weaknesses:
1. The main issue of this paper is that, for the NTP-compatible and separable conditions to hold, one needs d > m. Does this overparametrization condition usually hold in practice or not? To my knowledge, in practice, the embedding dimension d is much smaller than the number of training data. Since m is not the number of training data and can be much smaller than that, it is not clear to me whether this assumption is possible in practice.

2. There are some paragraphs that are not very clearly written. For example, in lines 154-157, why does equation 4 constrain W^p w.r.t. this subspace? Why is the solution W* unique, assuming equation 4 has a solution? I think those can be expressed as lemmas to make them clearer. In line 148, the authors claim that (3a) holds if and only if the data satisfies the NTP-compatible condition. The 'if' direction is trivial, but the other direction needs a more rigorous proof.

Limitations:
See above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the implicit bias of optimization in next token prediction tasks by analyzing the structure of the decoding matrix at infinite time. The paper introduces two novel conditions under which the loss reaches its minimum theoretical value and demonstrates that if these conditions hold (which can be, for example, the case when the model is overparameterized), then after GD training, the decoding matrix will converge (in direction) to a matrix reminiscent of the maximum-margin matrix in ""standard"" classification.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work studies a timely topic (next token prediction) and approaches it from a learning theoretic perspective (implicit bias of optimization), which has proven to be very fruitful in ""standard"" classification. The assumption of sparse contexts is clever and should be of wider applicability. The results are novel and analogous to similar results that were proven for ""standard"" classification. Furthermore, the presentation is comprehensive, with many pointers to related work, which help contextualize this paper's contributions.

Weaknesses:
A weakness, which the authors do acknowledge in their work, that prevented me from giving a higher score is that there is no clear connection between the structure of the weights and generalization, as there exists in ""standard""/one-hot classification. As a result, it is unclear how much insight can be derived from the current result. I would appreciate the authors' thoughts on this.

Minor: The text is too dense in places, with the authors trying to include more details than what the space permits. I would suggest moving some of the discussion in Sections 6 and 7 to the Appendix to facilitate a smoother flow.

Limitations:
The authors thoroughly discuss the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective, the central challenge being to discern the ""implicit bias"" of the optimizer towards particular solutions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well written, and the notation is very clear.

- The paper provides a a very interesting starting point for studying the solutions found by gradient descent in NTP settings

Weaknesses:
While the paper provides a a very interesting starting point for studying the solutions found by gradient descent in NTP settings, it's not very clear whether margin maximization practically corresponds to any meaningful takeaway in language modeling.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study investigates the structural properties of solutions chosen by gradient-based optimizers for next-token prediction (NTP), framing NTP as cross-entropy minimization across various contexts with sparse conditional probability distributions over a finite vocabulary. It focuses on the optimization bias of gradient descent (GD), characterizing how GD selects parameters that equate the logits’ differences of supported tokens to their log-odds.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This study enables deriving the data-entropy lower bound in NTP for understanding the optimization and generalization properties of NTP models.

Weaknesses:
The study's focus on linear models  analyzing CE loss for NTP may limit its novelty and applicability, making its contributions to the field appear unclear compared to existing research.

Limitations:
The limitations of this study include its reliance on the simplicity of the analyzed model, unclear distinctions and advantages over existing research, and its omission of key aspects such as the properties of attention mechanisms.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the implicit bias of the gradient descent on the Next-Token Prediction (LTP) problem in linear models. They first formulate this NTP problem as minimizing the cross-entropy (CE) loss over distinct contexts, each tied with a sparse conditional probability over the token space. They then provide the necessary conditions for the CE loss to reach the entropy lower bound, i.e., the NTP-compatible condition and the NTP-separable condition. Then, they prove one sufficient condition for those two conditions is oevrparameterization, i.e., the dimension of the embedding space d is larger than the number of distinct contexts in the dataset. Assuming both compatible and separable conditions, they then prove the directional convergence of the minimizer of the CE loss within a certain range and the directional convergence of the GD iterate towards the direction of the solution of an NTP-SVM.

In general, I think this paper delves into a good and important problem: the optimization path and implicit bias of NTP mechanism. The authors provided a good formulation, and the proof is solid.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. They investigate an interesting and important problem: the optimization path and the implicit bias of NTP.

2. Their formulation of NTP into the CE minimization over distinct contexts is novel.

3. They provide rigorous theoretical results and the proofs are solid, to my knowledge.

Weaknesses:
1. The main issue of this paper is that, for the NTP-compatible and separable conditions to hold, one needs d > m. Does this overparametrization condition usually hold in practice or not? To my knowledge, in practice, the embedding dimension d is much smaller than the number of training data. Since m is not the number of training data and can be much smaller than that, it is not clear to me whether this assumption is possible in practice.

2. There are some paragraphs that are not very clearly written. For example, in lines 154-157, why does equation 4 constrain W^p w.r.t. this subspace? Why is the solution W* unique, assuming equation 4 has a solution? I think those can be expressed as lemmas to make them clearer. In line 148, the authors claim that (3a) holds if and only if the data satisfies the NTP-compatible condition. The 'if' direction is trivial, but the other direction needs a more rigorous proof.

Limitations:
See above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the implicit bias of optimization in next token prediction tasks by analyzing the structure of the decoding matrix at infinite time. The paper introduces two novel conditions under which the loss reaches its minimum theoretical value and demonstrates that if these conditions hold (which can be, for example, the case when the model is overparameterized), then after GD training, the decoding matrix will converge (in direction) to a matrix reminiscent of the maximum-margin matrix in ""standard"" classification.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work studies a timely topic (next token prediction) and approaches it from a learning theoretic perspective (implicit bias of optimization), which has proven to be very fruitful in ""standard"" classification. The assumption of sparse contexts is clever and should be of wider applicability. The results are novel and analogous to similar results that were proven for ""standard"" classification. Furthermore, the presentation is comprehensive, with many pointers to related work, which help contextualize this paper's contributions.

Weaknesses:
A weakness, which the authors do acknowledge in their work, that prevented me from giving a higher score is that there is no clear connection between the structure of the weights and generalization, as there exists in ""standard""/one-hot classification. As a result, it is unclear how much insight can be derived from the current result. I would appreciate the authors' thoughts on this.

Minor: The text is too dense in places, with the authors trying to include more details than what the space permits. I would suggest moving some of the discussion in Sections 6 and 7 to the Appendix to facilitate a smoother flow.

Limitations:
The authors thoroughly discuss the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective, the central challenge being to discern the ""implicit bias"" of the optimizer towards particular solutions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well written, and the notation is very clear.

- The paper provides a a very interesting starting point for studying the solutions found by gradient descent in NTP settings

Weaknesses:
While the paper provides a a very interesting starting point for studying the solutions found by gradient descent in NTP settings, it's not very clear whether margin maximization practically corresponds to any meaningful takeaway in language modeling.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study investigates the structural properties of solutions chosen by gradient-based optimizers for next-token prediction (NTP), framing NTP as cross-entropy minimization across various contexts with sparse conditional probability distributions over a finite vocabulary. It focuses on the optimization bias of gradient descent (GD), characterizing how GD selects parameters that equate the logits’ differences of supported tokens to their log-odds.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This study enables deriving the data-entropy lower bound in NTP for understanding the optimization and generalization properties of NTP models.

Weaknesses:
The study's focus on linear models  analyzing CE loss for NTP may limit its novelty and applicability, making its contributions to the field appear unclear compared to existing research.

Limitations:
The limitations of this study include its reliance on the simplicity of the analyzed model, unclear distinctions and advantages over existing research, and its omission of key aspects such as the properties of attention mechanisms.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
x7pjdDod6Z;"REVIEW 
Summary:
This paper introduces MeshFormer, a sparse-view reconstruction model designed to generate high-quality 3D textured meshes from sparse RGB images and their corresponding normal maps. By leveraging voxel representation, 3D inductive biases, SDF loss, and normal information, the model shows comparable inference performance to concurrent methods, while the entire training process can be completed using only 8 GPUs within a week (concurrent methods typically require around 100 GPUs). Experimental results demonstrate the effectiveness of the design.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors provided a detailed explanation of the motivations behind the model designs (including the introduction of voxel representation, the introduction of 3D full (or sparse) convolution, and so on) and demonstrated the reasonableness of these choices.

2. Compared to baseline methods, this model is simpler to train and demonstrates better qualitative and quantitative results. 

3. The ablation study demonstrates the effectiveness of normal input, SDF supervision, geometry enhancement, and other methods proposed in the paper.

Weaknesses:
1. Although the authors provide detailed textual descriptions in the method section, it would be better if more mathematical symbols and equations were used, which could explain the entire pipeline more clearly and unambiguously.

2. For reproducibility, the authors should provide more implementation details, including a more detailed model architecture, the values of hyperparameters (e.g., \lambda in the loss function), and other relevant information.

3. The authors don’t report the comparison of inference time and memory usage between the proposed model and the baseline models.

Limitations:
Yes, the authors addressed limitations, potential negative societal impact, and mitigation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a high-quality feed-forward 3D object reconstruction method from sparse view RGB images. It uses an explicit voxel structure for better geometric inductive bias, auxiliary inputs such as 2D diffusion generated normal images and SDF representation for better geometric details, and an end-to-end trainable pipeline that eliminates the need for multi-stage refinement. The method gives high quality reconstruction results, especially in terms of fine-grained and smooth geometry.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Although the network architecture and 3D representations are more complicated than previous methods, they are end-to-end differentiable and alleviate the training burden of multi-stage refinement.
2. The idea of using 2D diffusion generated normal images as input to the reconstruction pipeline is interesting and insightful.
3. It is more computationally efficient to train (Line 73).
4. The qualitative results are impressive, especially the mesh normals.

Weaknesses:
1. In original LRM the only supervision signal needed is RGB images. The proposed method, however, needs access to the full 3D shape for supervising the occupancy. It is fine for hand-made 3D assets but might poses some difficulty when trying to scale to real datasets.

Limitations:
1. It requires 2D diffusion models to generate auxiliary inputs, which can drastically slow down the reconstruction speed.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this work, the authors propose a sparse view reconstruction model that utilizes a set of images (with camera poses) and corresponding normal maps to produce a reconstructed textured mesh. The primary contribution lies in adopting voxel-based 3D representation and employing a network architecture that integrates both 3D convolution and attention layers. Moreover, direct geometry supervision (SDF loss) is applied during the training process, alongside rendering-based losses. Experimental results demonstrate that the generated 3D shapes achieve state-of-the-art performance when compared to existing works on the single-view to 3D task.

However, as highlighted in the weakness section, there are potential misclaims regarding the technical contributions. It is highly recommended to revise the manuscript to cite and discuss these related works. Despite this, I am currently inclined towards accepting the paper and would be happy to champion it if the aforementioned issues are addressed in the final version.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The writing is clear and easy to follow.
- The combination of SDF loss and rendering losses appears novel for training a feed-forward based network. Additionally, the ablation study in Table 3(b) clearly indicates that SDF supervision is crucial for achieving good geometry, as evidenced by the significant CD difference between (b) and (g).
- Although [33] has explored using normal maps for the reconstruction task, it seems new to employ normal maps as inputs and supervision for a feed-forward reconstruction network.
- Experimental results demonstrate state-of-the-art performance over existing baselines, as shown in Table 1 and Figure 3. Furthermore, it is illustrated that existing methods cannot achieve similar performance given the same computational resources (Table 2).
- The ablation study confirms that various components are essential for the final performance, including considering normal input and SDF supervision.

Weaknesses:
Possibly Misclaimed Technical Novelties:

However, the current manuscript may contain several misclaims regarding its technical novelties.

One claimed novelty is the adoption of a 3D voxel representation. However, the use of 3D voxel-like volumes in reconstruction is not a new idea and has been well-explored in various works, including:

A. Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process, CVPR 2023

B. SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation, CVPR 2023

C. Locally Attentional SDF Diffusion for Controllable 3D Shape Generation, SIGGRAPH 2023

D. One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion, CVPR 2024

E. Make-A-Shape: a Ten-Million-scale 3D Shape Model, ICML 2024

Additionally, the use of convolution + transformer layers to process grid input seems to be standard procedure in 2D generation tasks, as seen in:

Diffusion Models Beat GANs on Image Synthesis, NeurIPS 2021

Similar architectures have also been widely adopted in some of the aforementioned 3D reconstruction works, such as [A, C, D, E].

Regarding image conditioning, the cross-attention with image patch features is also well-explored in various works mentioned above, such as [C, D, E].

Limitations:
The main limitation is well described in Section 5.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes an improved framework for feed-forward reconstruction models. The authors advocate a number of improvements over the initial design of Large Reconstruction Model, including model architecture and training schemes. Experiments show that the method reconstructs better geometry and texture on Google Scanned Objects and OmniObject3D datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is focused on ablating different components for feed-forward sparse-view reconstruction, and in-depth analyses are provided for each design choice. Although there are no complicated new method proposed, such analysis bring value for understanding how and why each component works.
- The proposed method is evaluated on (preprocessed) real-world multi-view datasets, showing improvements over baselines on all metrics. Extensive ablative analyses are also provided to better understand the behaviors of the proposed method.

Weaknesses:
- Since this is more of an analysis paper, it would be good if the authors could also document the other components that were tried/ablated but did not see significant differences.
- Since training resources was discussed and compared, it would be nice if there could be an analysis on the mesh generation/reconstruction quality over training time.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this work, the authors propose MeshFormer, a sparse-view reconstruction model that explicitly leverages 3D native structure, input guidance, and training supervision. They leverage 3D sparse voxels as their representation and combine transformers with 3D (sparse) convolutions to inject 3D prior. Additionally, they propose to take the corresponding normal maps together with sparse-view RGBs as input and also generate them as output, which could be used for geometry enhancement. Extensive experiments show that MeshFormer can be trained efficiently and outperforms state-of-the-art methods in terms of generating high-quality textured meshes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- MeshFormer is able to generate high-quality textured meshes with fine-grained geometric details.

- The authors find that using normal images together with RGB images greatly helps in predicting geometric details. Additionally, the model outputs a normal map, which can be used for geometry enhancement.

- The proposed method explicitly leverages 3D native structure, input guidance, and training supervision, resulting in faster convergence speed and better geometric details.

Weaknesses:
- Pixel-based 2D methods (e.g., LGM) can preserve thin details, while 3D-based methods often smooth these details. How do you justify that? For example, in Figure 3 Column 4, the loose thread of the toy is captured by LGM, while MeshFormer ignores it.

- The proposed name ""VoxelFormer"" seems improper to me. It seems more like a 3D UNet with a deep bottleneck composed of multiple transformer layers.

- The projection-aware cross-attention layer projects 3D voxels onto the m views to interpolate m RGB and normal features. However, in the object case, one 3D voxel usually only corresponds to one view (due to occlusion). This cross-attention is projection-aware but not truly 3D-aware. Have you tried some occlusion-aware attention in your sparse model? Since you already have the coarse structure of the object, it could be used to filter out unneeded features.

- According to Table 3 (d), you mention ""we replace the cross-attention with simple average pooling and observe a significant performance drop."" Could you also try max-pooling? Additionally, do you concatenate the 3D feature voxel at every level of the network, as done in One-2-3-45++?

Limitations:
The authors already include limitations and broader impact in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces MeshFormer, a sparse-view reconstruction model designed to generate high-quality 3D textured meshes from sparse RGB images and their corresponding normal maps. By leveraging voxel representation, 3D inductive biases, SDF loss, and normal information, the model shows comparable inference performance to concurrent methods, while the entire training process can be completed using only 8 GPUs within a week (concurrent methods typically require around 100 GPUs). Experimental results demonstrate the effectiveness of the design.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors provided a detailed explanation of the motivations behind the model designs (including the introduction of voxel representation, the introduction of 3D full (or sparse) convolution, and so on) and demonstrated the reasonableness of these choices.

2. Compared to baseline methods, this model is simpler to train and demonstrates better qualitative and quantitative results. 

3. The ablation study demonstrates the effectiveness of normal input, SDF supervision, geometry enhancement, and other methods proposed in the paper.

Weaknesses:
1. Although the authors provide detailed textual descriptions in the method section, it would be better if more mathematical symbols and equations were used, which could explain the entire pipeline more clearly and unambiguously.

2. For reproducibility, the authors should provide more implementation details, including a more detailed model architecture, the values of hyperparameters (e.g., \lambda in the loss function), and other relevant information.

3. The authors don’t report the comparison of inference time and memory usage between the proposed model and the baseline models.

Limitations:
Yes, the authors addressed limitations, potential negative societal impact, and mitigation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a high-quality feed-forward 3D object reconstruction method from sparse view RGB images. It uses an explicit voxel structure for better geometric inductive bias, auxiliary inputs such as 2D diffusion generated normal images and SDF representation for better geometric details, and an end-to-end trainable pipeline that eliminates the need for multi-stage refinement. The method gives high quality reconstruction results, especially in terms of fine-grained and smooth geometry.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Although the network architecture and 3D representations are more complicated than previous methods, they are end-to-end differentiable and alleviate the training burden of multi-stage refinement.
2. The idea of using 2D diffusion generated normal images as input to the reconstruction pipeline is interesting and insightful.
3. It is more computationally efficient to train (Line 73).
4. The qualitative results are impressive, especially the mesh normals.

Weaknesses:
1. In original LRM the only supervision signal needed is RGB images. The proposed method, however, needs access to the full 3D shape for supervising the occupancy. It is fine for hand-made 3D assets but might poses some difficulty when trying to scale to real datasets.

Limitations:
1. It requires 2D diffusion models to generate auxiliary inputs, which can drastically slow down the reconstruction speed.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this work, the authors propose a sparse view reconstruction model that utilizes a set of images (with camera poses) and corresponding normal maps to produce a reconstructed textured mesh. The primary contribution lies in adopting voxel-based 3D representation and employing a network architecture that integrates both 3D convolution and attention layers. Moreover, direct geometry supervision (SDF loss) is applied during the training process, alongside rendering-based losses. Experimental results demonstrate that the generated 3D shapes achieve state-of-the-art performance when compared to existing works on the single-view to 3D task.

However, as highlighted in the weakness section, there are potential misclaims regarding the technical contributions. It is highly recommended to revise the manuscript to cite and discuss these related works. Despite this, I am currently inclined towards accepting the paper and would be happy to champion it if the aforementioned issues are addressed in the final version.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The writing is clear and easy to follow.
- The combination of SDF loss and rendering losses appears novel for training a feed-forward based network. Additionally, the ablation study in Table 3(b) clearly indicates that SDF supervision is crucial for achieving good geometry, as evidenced by the significant CD difference between (b) and (g).
- Although [33] has explored using normal maps for the reconstruction task, it seems new to employ normal maps as inputs and supervision for a feed-forward reconstruction network.
- Experimental results demonstrate state-of-the-art performance over existing baselines, as shown in Table 1 and Figure 3. Furthermore, it is illustrated that existing methods cannot achieve similar performance given the same computational resources (Table 2).
- The ablation study confirms that various components are essential for the final performance, including considering normal input and SDF supervision.

Weaknesses:
Possibly Misclaimed Technical Novelties:

However, the current manuscript may contain several misclaims regarding its technical novelties.

One claimed novelty is the adoption of a 3D voxel representation. However, the use of 3D voxel-like volumes in reconstruction is not a new idea and has been well-explored in various works, including:

A. Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process, CVPR 2023

B. SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation, CVPR 2023

C. Locally Attentional SDF Diffusion for Controllable 3D Shape Generation, SIGGRAPH 2023

D. One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion, CVPR 2024

E. Make-A-Shape: a Ten-Million-scale 3D Shape Model, ICML 2024

Additionally, the use of convolution + transformer layers to process grid input seems to be standard procedure in 2D generation tasks, as seen in:

Diffusion Models Beat GANs on Image Synthesis, NeurIPS 2021

Similar architectures have also been widely adopted in some of the aforementioned 3D reconstruction works, such as [A, C, D, E].

Regarding image conditioning, the cross-attention with image patch features is also well-explored in various works mentioned above, such as [C, D, E].

Limitations:
The main limitation is well described in Section 5.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes an improved framework for feed-forward reconstruction models. The authors advocate a number of improvements over the initial design of Large Reconstruction Model, including model architecture and training schemes. Experiments show that the method reconstructs better geometry and texture on Google Scanned Objects and OmniObject3D datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is focused on ablating different components for feed-forward sparse-view reconstruction, and in-depth analyses are provided for each design choice. Although there are no complicated new method proposed, such analysis bring value for understanding how and why each component works.
- The proposed method is evaluated on (preprocessed) real-world multi-view datasets, showing improvements over baselines on all metrics. Extensive ablative analyses are also provided to better understand the behaviors of the proposed method.

Weaknesses:
- Since this is more of an analysis paper, it would be good if the authors could also document the other components that were tried/ablated but did not see significant differences.
- Since training resources was discussed and compared, it would be nice if there could be an analysis on the mesh generation/reconstruction quality over training time.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this work, the authors propose MeshFormer, a sparse-view reconstruction model that explicitly leverages 3D native structure, input guidance, and training supervision. They leverage 3D sparse voxels as their representation and combine transformers with 3D (sparse) convolutions to inject 3D prior. Additionally, they propose to take the corresponding normal maps together with sparse-view RGBs as input and also generate them as output, which could be used for geometry enhancement. Extensive experiments show that MeshFormer can be trained efficiently and outperforms state-of-the-art methods in terms of generating high-quality textured meshes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- MeshFormer is able to generate high-quality textured meshes with fine-grained geometric details.

- The authors find that using normal images together with RGB images greatly helps in predicting geometric details. Additionally, the model outputs a normal map, which can be used for geometry enhancement.

- The proposed method explicitly leverages 3D native structure, input guidance, and training supervision, resulting in faster convergence speed and better geometric details.

Weaknesses:
- Pixel-based 2D methods (e.g., LGM) can preserve thin details, while 3D-based methods often smooth these details. How do you justify that? For example, in Figure 3 Column 4, the loose thread of the toy is captured by LGM, while MeshFormer ignores it.

- The proposed name ""VoxelFormer"" seems improper to me. It seems more like a 3D UNet with a deep bottleneck composed of multiple transformer layers.

- The projection-aware cross-attention layer projects 3D voxels onto the m views to interpolate m RGB and normal features. However, in the object case, one 3D voxel usually only corresponds to one view (due to occlusion). This cross-attention is projection-aware but not truly 3D-aware. Have you tried some occlusion-aware attention in your sparse model? Since you already have the coarse structure of the object, it could be used to filter out unneeded features.

- According to Table 3 (d), you mention ""we replace the cross-attention with simple average pooling and observe a significant performance drop."" Could you also try max-pooling? Additionally, do you concatenate the 3D feature voxel at every level of the network, as done in One-2-3-45++?

Limitations:
The authors already include limitations and broader impact in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xRdpCOdghl;"REVIEW 
Summary:
The paper suggests a new sampling method for the labeled set of semi-supervised learning. This sampling method, termed RDSS, selects a set of examples that is both representative of the data, and diverse. The paper shows that using such a sampling function improves both freematch and flexmatch, and compares it against other sampling methods, and methods from AL and SSAL.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea of the paper is good, and is well supported by theory. The experimental setup does convince me that the suggested method is better than random sampling when picking the labeled set of SSL. However, a better comparison to previous works is required, see the Weaknesses section.

Clarity: The paper is clearly written, the idea is well presented and intuitive, and the paper is easy to read and follow.

Weaknesses:
Some of the claims made by paper already appeared in previous art. Specifically, [1] showed that ""traditional"" AL methods do not pick bad labeled sets for SSL when compared to random sampling. [2] showed that when the labeled set is particularly small, instead of traditional AL techniques, one should focus on labeling examples that are more typical and diverse, showing that such a method can drastically improve both AL and sampling techniques for SSL. [3] presented sampling strategy, showing that picking examples that are representative and diverse examples for the labeled set of  SSL improves it by a big margin in low-budget scenarios.

The proposed manuscript does not reference or compare to any of these works. This affects both the novelty, significance and quality of the proposed method: the novelty is somewhat more limited, as many of the ideas overlap with existing works. The significance of this work is impacted, as while the problem at hand is important, it is unclear if the presented ideas pose significant advancement over the existing methods, and the quality is diminished, as a lot of comparisons are missing in the experimental setup.

Specifically, any low-budget strategy could be potentially applied to SSL as well, so those methods should be compared against as well. See for example [4], [5].

Additionally, the vice-versa argument should also hold -- if AL methods can be applied in this case, this method can be used as a method for picking labeled examples for active learning purposes and should be tested as such, as the literature in AL is much broader than the literature of picking the labeled set in SSL, which can provide a much wider context for the given work.

In addition, the framing of the paper is a bit unclear to me. I think the paper could benefit from explaining use cases in which one has the option to pick in advance the labeled set for SSL, which is not already covered by AL use cases.

-------

[1] Mittal, Sudhanshu, et al. Parting with illusions about deep active learning. (2019).

[2] Hacohen, Guy et al. Active learning on a budget: Opposite strategies suit high and low budgets. (2022).

[3] Yehuda, Ofer, et al. ""Active learning through a covering lens."" (2022).

[4] Mahmood, Rafid, et al. ""Low budget active learning via wasserstein distance: An integer programming approach."" (2021).

[5] Wen, Ziting, et al. ""NTKCPL: Active Learning on Top of Self-Supervised Model by Estimating True Coverage."" (2023).

Limitations:
not relevant

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a Representative and Diverse Sample Selection approach (RDSS) that utilizes a modified Frank-Wolfe algorithm to minimize a novel α-Maximum Mean Discrepancy (α-MMD) criterion, aiming to select a representative and diverse subset from unlabeled data for annotation. Experimental results demonstrate that RDSS consistently improves the performance of several popular semi-supervised learning frameworks and outperforms state-of-the-art sample selection methods used in Active Learning (AL) and Semi-Supervised Active Learning (SSAL), even under constrained annotation budgets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.This paper is in Well-written, logically organized, and smoothly expressed.
2. The presented results demonstrate the effectiveness of the proposed approach.

Weaknesses:
1. The author conducted tests on two baseline methods(FlexMatch [58] and Freematch [50]), but neither of them represents the current state-of-the-art.
2. Some details of the experiments are unclear, such as in Table 3.

Limitations:
as Weaknesses

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new sample selection method, RDSS, for the SSL task. RDSS considers both the representativeness and diversity of the selected sample and achieves state-of-the-art performance. This is achieved by the proposed α-MMD criterion and an efficient optimization algorithm GKHR.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. RDSS considers both representativeness and diversity of samples, which is a convincing strategy, and the experimental results also demonstrate the effectiveness of this motivation.
2. Sufficient theoretical analysis and experimental comparisons are conducted to demonstrate the effectiveness of the proposed method.

Weaknesses:
I would like to see images of the actual selected samples and visualizations of the feature distribution to demonstrate that RDSS indeed balances the representativeness and diversity.

Limitations:
Please refer to the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Choice of the labeled set in the semi supervised learning is critical for the final performance of the model. This problem can also be looked as AL with SSL, or single shot AL with SSL (in other words similar to experimental design). This works provides a way to select the seed set which is representative, as well as diverse. The problem is reduced to minimizing MMD and similarity score of the selected examples. The paper finally proposes a greedy algorithm, and compare the proposed method against various subset selection baselines, and AL.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I like the motivation of the problem and a neat theoretical derivation of the objective, and the provided theoretical analysis. Paper was also easy to follow and experiments are compelling.

Weaknesses:
- From a purely combinatorial point of view, I think that the final objective is supermodular in nature. Given the vast literature on submodular/supermodular functions, is it not possible to get an algorithm purely from that standpoint? If so, how different would it be from the proposed one? 

- Can one derive things such as leverage scores to detect the outlier-ness of a given point (or any other score)? If so, then couldn't one use something such as diversity - outlier score (or add a score that models likelihood) , with diversity such as Facility location function, and optimize the final objective using greedy? 

- In experiments I believe one of the strong baselines such as facility location function is missing. Facility Location has a rich history and have been used in several instances in Active Learning ([1, 2, 3, 4]). I believe authors can add a small discussion on FL and add that baseline. Furthermore, other diversity based approaches have also been considered in the past [5]

- Now a days a lot of focus is also for doing finetuning of existing CLIP models [3]. I'd appreciate one experiment on fine-tuning the CLIP models using the proposed method. 


References
- [1] Submodularity in machine learning and artificial intelligence
- [2] An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models
- [3] LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning
- [4] Deep Submodular Peripteral networks
- [5] GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning

Limitations:
Refer to the weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper suggests a new sampling method for the labeled set of semi-supervised learning. This sampling method, termed RDSS, selects a set of examples that is both representative of the data, and diverse. The paper shows that using such a sampling function improves both freematch and flexmatch, and compares it against other sampling methods, and methods from AL and SSAL.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea of the paper is good, and is well supported by theory. The experimental setup does convince me that the suggested method is better than random sampling when picking the labeled set of SSL. However, a better comparison to previous works is required, see the Weaknesses section.

Clarity: The paper is clearly written, the idea is well presented and intuitive, and the paper is easy to read and follow.

Weaknesses:
Some of the claims made by paper already appeared in previous art. Specifically, [1] showed that ""traditional"" AL methods do not pick bad labeled sets for SSL when compared to random sampling. [2] showed that when the labeled set is particularly small, instead of traditional AL techniques, one should focus on labeling examples that are more typical and diverse, showing that such a method can drastically improve both AL and sampling techniques for SSL. [3] presented sampling strategy, showing that picking examples that are representative and diverse examples for the labeled set of  SSL improves it by a big margin in low-budget scenarios.

The proposed manuscript does not reference or compare to any of these works. This affects both the novelty, significance and quality of the proposed method: the novelty is somewhat more limited, as many of the ideas overlap with existing works. The significance of this work is impacted, as while the problem at hand is important, it is unclear if the presented ideas pose significant advancement over the existing methods, and the quality is diminished, as a lot of comparisons are missing in the experimental setup.

Specifically, any low-budget strategy could be potentially applied to SSL as well, so those methods should be compared against as well. See for example [4], [5].

Additionally, the vice-versa argument should also hold -- if AL methods can be applied in this case, this method can be used as a method for picking labeled examples for active learning purposes and should be tested as such, as the literature in AL is much broader than the literature of picking the labeled set in SSL, which can provide a much wider context for the given work.

In addition, the framing of the paper is a bit unclear to me. I think the paper could benefit from explaining use cases in which one has the option to pick in advance the labeled set for SSL, which is not already covered by AL use cases.

-------

[1] Mittal, Sudhanshu, et al. Parting with illusions about deep active learning. (2019).

[2] Hacohen, Guy et al. Active learning on a budget: Opposite strategies suit high and low budgets. (2022).

[3] Yehuda, Ofer, et al. ""Active learning through a covering lens."" (2022).

[4] Mahmood, Rafid, et al. ""Low budget active learning via wasserstein distance: An integer programming approach."" (2021).

[5] Wen, Ziting, et al. ""NTKCPL: Active Learning on Top of Self-Supervised Model by Estimating True Coverage."" (2023).

Limitations:
not relevant

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a Representative and Diverse Sample Selection approach (RDSS) that utilizes a modified Frank-Wolfe algorithm to minimize a novel α-Maximum Mean Discrepancy (α-MMD) criterion, aiming to select a representative and diverse subset from unlabeled data for annotation. Experimental results demonstrate that RDSS consistently improves the performance of several popular semi-supervised learning frameworks and outperforms state-of-the-art sample selection methods used in Active Learning (AL) and Semi-Supervised Active Learning (SSAL), even under constrained annotation budgets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.This paper is in Well-written, logically organized, and smoothly expressed.
2. The presented results demonstrate the effectiveness of the proposed approach.

Weaknesses:
1. The author conducted tests on two baseline methods(FlexMatch [58] and Freematch [50]), but neither of them represents the current state-of-the-art.
2. Some details of the experiments are unclear, such as in Table 3.

Limitations:
as Weaknesses

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new sample selection method, RDSS, for the SSL task. RDSS considers both the representativeness and diversity of the selected sample and achieves state-of-the-art performance. This is achieved by the proposed α-MMD criterion and an efficient optimization algorithm GKHR.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. RDSS considers both representativeness and diversity of samples, which is a convincing strategy, and the experimental results also demonstrate the effectiveness of this motivation.
2. Sufficient theoretical analysis and experimental comparisons are conducted to demonstrate the effectiveness of the proposed method.

Weaknesses:
I would like to see images of the actual selected samples and visualizations of the feature distribution to demonstrate that RDSS indeed balances the representativeness and diversity.

Limitations:
Please refer to the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Choice of the labeled set in the semi supervised learning is critical for the final performance of the model. This problem can also be looked as AL with SSL, or single shot AL with SSL (in other words similar to experimental design). This works provides a way to select the seed set which is representative, as well as diverse. The problem is reduced to minimizing MMD and similarity score of the selected examples. The paper finally proposes a greedy algorithm, and compare the proposed method against various subset selection baselines, and AL.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I like the motivation of the problem and a neat theoretical derivation of the objective, and the provided theoretical analysis. Paper was also easy to follow and experiments are compelling.

Weaknesses:
- From a purely combinatorial point of view, I think that the final objective is supermodular in nature. Given the vast literature on submodular/supermodular functions, is it not possible to get an algorithm purely from that standpoint? If so, how different would it be from the proposed one? 

- Can one derive things such as leverage scores to detect the outlier-ness of a given point (or any other score)? If so, then couldn't one use something such as diversity - outlier score (or add a score that models likelihood) , with diversity such as Facility location function, and optimize the final objective using greedy? 

- In experiments I believe one of the strong baselines such as facility location function is missing. Facility Location has a rich history and have been used in several instances in Active Learning ([1, 2, 3, 4]). I believe authors can add a small discussion on FL and add that baseline. Furthermore, other diversity based approaches have also been considered in the past [5]

- Now a days a lot of focus is also for doing finetuning of existing CLIP models [3]. I'd appreciate one experiment on fine-tuning the CLIP models using the proposed method. 


References
- [1] Submodularity in machine learning and artificial intelligence
- [2] An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models
- [3] LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning
- [4] Deep Submodular Peripteral networks
- [5] GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning

Limitations:
Refer to the weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xRQxan3WkM;"REVIEW 
Summary:
This paper studies the implicit bias of the Adam optimizer for logistic regression on linearly separable data. The authors prove that Adam converges to the linear classifier with the maximum $\ell_\infty$-margin. This result contrasts with the classical results on (stochastic) gradient descent (with or without momentum), which converge to the maximum $\ell_2$-margin solution.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The authors theoretically study a popular yet not well-understood optimization method, Adam, in the context of a well-studied classical problem: logistic regression on linearly separable data. This offers a solid and insightful contribution to understanding Adam. In particular, distinguishing Adam from (S)GD with/without momentum on this classical problem is a very interesting result.
- The technical contributions are also of independent interest, as they prove the results for Adam without relying on the stability constant (which is closer to practice) and use mild assumptions.
- The paper is well-written and easy to follow. The proof sketch provides a clear and comprehensive overview of the proof of the main theorem.

Weaknesses:
There are no major concerns about this paper. Below are minor comments and some areas for improvement:
- The paper does not provide an intuition behind why Adam achieves the maximum $\ell_\infty$-margin solution, in contrast to GD which achieves the maximum $\ell_2$-margin solution. It would be great if the authors could offer insights on how the $\ell_\infty$-margin arises instead of the $\ell_2$-margin, for example, through a warm-up analysis with SignGD ($\beta_1=\beta_2=0$) or RMSProp ($\beta_1=0$). One way to provide an intuition is as follows: Gunasekar et al. (2018) proved that steepest descent converges to the max-margin solution, implying that SignGD (steepest descent w.r.t. $\ell_\infty$-norm) converges to the maximum $\ell_\infty$-margin solution. Since SignGD is known to be a good proxy for Adam, this may offer an insight into why Adam converges to the maximum $\ell_\infty$-margin solution.
- The authors claim that the bounds in Corollary 4.7 are derived under worst-case scenarios and argue that this is why, in practice, we often observe margins converging faster than the bounds in the corollary. However, this statement lacks supporting evidence. The paper should prove that the rate of convergence is tight. Otherwise, the observed faster convergence of margins in experiments might simply indicate that the bound is not tight enough.
- Some sentences, including those in the abstract, use the term ""convergence"" unclearly. For example, in the abstract, ""this convergence occurs within polynomial time"" does not indicate the objective (the normalized $\ell_\infty$-margin in this case) of convergence. This could be confused with other notions of convergence, such as convergence in direction (i.e., $\frac{w_t}{\lVert w_t \rVert} \to \frac{w^*}{\lVert w^* \rVert}$).
- (page 6, line 183) According to the paper, the normalized $\ell_2$-margin converges at a speed of $O(\log \log t / \log t)$ when using GD. However, this should be corrected to $O(1 / \log t)$. According to Soudry et al. (2018), the normalized weight vector converges to the maximum $\ell_2$-margin vector ""in direction"" with a convergence rate of $O(\log \log t / \log t)$, i.e., $\lVert \frac{w_t}{\lVert w_t \rVert} - \frac{w^*}{\lVert w^* \rVert}\rVert = O(\log \log t / \log t)$. However, the normalized $\ell_2$-margin converges at the speed of $O(1/\log t)$, i.e., $|\min \frac{\langle w_t, y_t \cdot x_t \rangle}{\lVert w_t \rVert} - \frac{\langle w^*, y_t \cdot x_t \rangle}{\lVert w^* \rVert} | = O(1/\log t)$.
- (page 1, line 25) Typo: reply on -> rely on

---
[Gunasekar et al. 2018] Characterizing Implicit Bias in Terms of Optimization Geometry, ICML 2018.

[Soudry et al. 2018] The Implicit Bias of Gradient Descent on Separable Data, JMLR 2018.

Limitations:
The paper discusses its limitations and future directions, including the extension of the results to homogeneous neural networks and the analysis of stochastic Adam instead of full-batch Adam. I think both directions are promising avenues for future research.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The main focus of this paper is on the implicit bias of Adam for a single layer linear model which performs binary classification on separable data. In particular, assuming a zero stability constant $\epsilon$, this paper reveals that Adam finds the solution that achieves maximum-$\ell_\infty$-margin and characterizes the convergence rate for different classes of learning rate. This implicit bias is different from the $\ell_2$-norm minimization solution obtained by previous work which does not assume $\epsilon = 0$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is clearly written and well-organized. It is easy and clear to follow the argument and motivation of this paper, e.g., the proof sketch makes it easy to follow the way how the theoretical conclusion is developed. In addition, to me, the introduction of the related works are comprehensive and clear. It also clearly summarizes the difference between this paper and related works.
- The settings and results of this paper are new compared to previous works, i.e., previous works showed an $\ell_2$-norm solution implicit bias of Adam on separable data while this paper reveals an $\ell_{\infty}$-norm implicit bias when the stability constant $\epsilon$ is zero.

Weaknesses:
Despite the novelty of the theoretical claims, I still have several concerns, which I will discuss in the following.

1. Removing the stability constant $\epsilon$ makes the approach of this paper fails to characterize the influence of it, which, though being small, still has non-negligible effect, e.g., [1] observed that Adam with an $\epsilon$ that is too small does not even converge in certain circumstances. Treating $\epsilon$ as 0 seems a bit rough to me. 

    In addition, [2] showed that Adam minimizes the interpolation norm of gradients that depends on magnitudes of various hyper parameters including the stability constant $\epsilon$ (although [2] did not specify the types of loss functions and model architectures). [1] claimed that Adam with nonzero $\epsilon$ converges to $\ell_2$-norm solution, which is also verified by extensive experiments. As a comparison, this paper showed that both Adam with $\epsilon=0$ and with a non-negligible $\epsilon$ do not converge to the aforementioned solutions (line 210). In this sense, it seems that the conclusion reached by this paper contradicts with those derived by [1, 2]. Therefore, in my view, it would be better to start with a non-zero $\epsilon$ and let the case with $\epsilon=0$ be a special case to better capture the effect of the $\epsilon$ on the implicit bias.

2. This paper only considers a simple setting: the model is only a one-layer linear model and there is no stochastic sampling noise which is typically necessary in practice. As a comparison, authors of [1] have already studied Adam on separable data for homogeneous models, which can cover the single layer model of the current work as a special case. Thus excluding the stochastic sampling noise in the current work is kind of unsatisfying to me since the model is already a simple one. In addition, I think that the authors of the current work should at least repeat the experiments conducted in [1] (such as those for homogeneous neural networks) to further support their theoretical claims, especially considering that the authors claimed in line 210 that their results are more accurate than those of [1].

**Reference**

[1] Wang et al. The implicit bias for adaptive optimization algorithms on homogeneous neural networks.

[2] Cattaneo et al. On the Implicit Bias of Adam.

Limitations:
I do not find a separate limitation section in the main part. In my view, removing the stability constant is a bit rough. This makes the approach presented in this paper fail to capture how the implicit bias of Adam changes for different values of stability constant.  

The societal impact is not applicable to this work as it focuses on theoretical parts of Adam.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper examines the implicit bias of the Adam optimizer in the context of linear logistic regression, demonstrating that it converges to the maximum $\ell_\infty$-margin solution under certain mild conditions. The authors note that omitting the stability constant in Adam updates results in a different implicit bias than gradient descent, with or without momentum, which converges to the maximum $\ell_2$-margin solution. They also explore various decreasing learning rates, showing that Adam's margin converges at a polynomial rate, which is faster than that of gradient descent. Additionally, they provide numerical experiments that support their findings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Understanding why Adam performs better than GD in several settings is an important problem and this work takes an important step towards this by showing that Adam has a different implicit bias than GD in the linear logistic regression setting.

- Overall, the paper is well-written and easy to follow. The proof sketch in Section 6 is explained well.

Weaknesses:
- The paper does not present results for a fixed learning rate and only considers a set of decreasing learning rates.

    - The discussion in lines 50-52 and after Corollary 4.7, comparing the rates of Adam and GD, should also comment on the convergence rates for GD with adaptive learning rates (e.g., normalized GD) which have been shown to converge faster (see [1] and related work) than GD.

    - (Minor) In Assumption 4.3, ‘non-increasing’ should be ‘decreasing’ or ‘diminishing’.

- The results in prior work on implicit bias of GD are global (hold for any initialization), whereas the results in this paper require an assumption on the initialization (Ass. 4.2). Based on the discussion following this assumption, it might be better to state an assumption on the data and then show that the condition on the initialization holds as a Lemma.

- The paper does not comment on how optimal the obtained rates in Corollary 4.7 are.

**References:**

[1] Wang et al., Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling, 2023.

Limitations:
There is no potential negative impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the author studies the implicit bias of Adam optimizer for a single layer neural network on separable data. The author's work suggests that, compared to the implicit bias of gradient descent which is the max $ \ell_2 $ margin solution, Adam solution converges to the maximum $ \ell_\infty $ margin solution. For this work, authors take both exponential and logistic loss and find that the convergence speed is on a polynomial order. 

In order to confirm the results, the authors perform experiments on synthetic datasets for binary classification tasks and confirm Adam’s convergence to the $ \ell_\infty $ margin comparatively.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work is novel (to the best of my knowledge) and interesting as the study of implicit bias of Adam could have further implications in characterizing the difference in optimization behavior of Adam vs SGD in practical scenarios. The assumptions of the work have been clearly presented and seem reasonable. With regard to the $ \epsilon $, while theoretical results are not provided, the authors include convincing experimental illustrations to convince me of the assumption. I also appreciate the well written proof sketch which helps convey the ideas

Weaknesses:
At the moment, I have some concerns with the paper which are more fit to be discussed as questions.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The main focus of this paper is on the implicit bias of Adam for a single layer linear model which performs binary classification on separable data. In particular, assuming a zero stability constant $\epsilon$, this paper reveals that Adam finds the solution that achieves maximum-$\ell_\infty$-margin and characterizes the convergence rate for different classes of learning rate. This implicit bias is different from the $\ell_2$-norm minimization solution obtained by previous work which does not assume $\epsilon = 0$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is clearly written and well-organized. It is easy and clear to follow the argument and motivation of this paper, e.g., the proof sketch makes it easy to follow the way how the theoretical conclusion is developed. In addition, to me, the introduction of the related works are comprehensive and clear. It also clearly summarizes the difference between this paper and related works.
- The settings and results of this paper are new compared to previous works, i.e., previous works showed an $\ell_2$-norm solution implicit bias of Adam on separable data while this paper reveals an $\ell_{\infty}$-norm implicit bias when the stability constant $\epsilon$ is zero.

Weaknesses:
Despite the novelty of the theoretical claims, I still have several concerns, which I will discuss in the following.

1. Removing the stability constant $\epsilon$ makes the approach of this paper fails to characterize the influence of it, which, though being small, still has non-negligible effect, e.g., [1] observed that Adam with an $\epsilon$ that is too small does not even converge in certain circumstances. Treating $\epsilon$ as 0 seems a bit rough to me. 

    In addition, [2] showed that Adam minimizes the interpolation norm of gradients that depends on magnitudes of various hyper parameters including the stability constant $\epsilon$ (although [2] did not specify the types of loss functions and model architectures). [1] claimed that Adam with nonzero $\epsilon$ converges to $\ell_2$-norm solution, which is also verified by extensive experiments. As a comparison, this paper showed that both Adam with $\epsilon=0$ and with a non-negligible $\epsilon$ do not converge to the aforementioned solutions (line 210). In this sense, it seems that the conclusion reached by this paper contradicts with those derived by [1, 2]. Therefore, in my view, it would be better to start with a non-zero $\epsilon$ and let the case with $\epsilon=0$ be a special case to better capture the effect of the $\epsilon$ on the implicit bias.

2. This paper only considers a simple setting: the model is only a one-layer linear model and there is no stochastic sampling noise which is typically necessary in practice. As a comparison, authors of [1] have already studied Adam on separable data for homogeneous models, which can cover the single layer model of the current work as a special case. Thus excluding the stochastic sampling noise in the current work is kind of unsatisfying to me since the model is already a simple one. In addition, I think that the authors of the current work should at least repeat the experiments conducted in [1] (such as those for homogeneous neural networks) to further support their theoretical claims, especially considering that the authors claimed in line 210 that their results are more accurate than those of [1].

**Reference**

[1] Wang et al. The implicit bias for adaptive optimization algorithms on homogeneous neural networks.

[2] Cattaneo et al. On the Implicit Bias of Adam.

Limitations:
I do not find a separate limitation section in the main part. In my view, removing the stability constant is a bit rough. This makes the approach presented in this paper fail to capture how the implicit bias of Adam changes for different values of stability constant.  

The societal impact is not applicable to this work as it focuses on theoretical parts of Adam.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper examines the implicit bias of the Adam optimizer in the context of linear logistic regression, demonstrating that it converges to the maximum $\ell_\infty$-margin solution under certain mild conditions. The authors note that omitting the stability constant in Adam updates results in a different implicit bias than gradient descent, with or without momentum, which converges to the maximum $\ell_2$-margin solution. They also explore various decreasing learning rates, showing that Adam's margin converges at a polynomial rate, which is faster than that of gradient descent. Additionally, they provide numerical experiments that support their findings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Understanding why Adam performs better than GD in several settings is an important problem and this work takes an important step towards this by showing that Adam has a different implicit bias than GD in the linear logistic regression setting.

- Overall, the paper is well-written and easy to follow. The proof sketch in Section 6 is explained well.

Weaknesses:
- The paper does not present results for a fixed learning rate and only considers a set of decreasing learning rates.

    - The discussion in lines 50-52 and after Corollary 4.7, comparing the rates of Adam and GD, should also comment on the convergence rates for GD with adaptive learning rates (e.g., normalized GD) which have been shown to converge faster (see [1] and related work) than GD.

    - (Minor) In Assumption 4.3, ‘non-increasing’ should be ‘decreasing’ or ‘diminishing’.

- The results in prior work on implicit bias of GD are global (hold for any initialization), whereas the results in this paper require an assumption on the initialization (Ass. 4.2). Based on the discussion following this assumption, it might be better to state an assumption on the data and then show that the condition on the initialization holds as a Lemma.

- The paper does not comment on how optimal the obtained rates in Corollary 4.7 are.

**References:**

[1] Wang et al., Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling, 2023.

Limitations:
There is no potential negative impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the author studies the implicit bias of Adam optimizer for a single layer neural network on separable data. The author's work suggests that, compared to the implicit bias of gradient descent which is the max $ \ell_2 $ margin solution, Adam solution converges to the maximum $ \ell_\infty $ margin solution. For this work, authors take both exponential and logistic loss and find that the convergence speed is on a polynomial order. 

In order to confirm the results, the authors perform experiments on synthetic datasets for binary classification tasks and confirm Adam’s convergence to the $ \ell_\infty $ margin comparatively.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work is novel (to the best of my knowledge) and interesting as the study of implicit bias of Adam could have further implications in characterizing the difference in optimization behavior of Adam vs SGD in practical scenarios. The assumptions of the work have been clearly presented and seem reasonable. With regard to the $ \epsilon $, while theoretical results are not provided, the authors include convincing experimental illustrations to convince me of the assumption. I also appreciate the well written proof sketch which helps convey the ideas

Weaknesses:
At the moment, I have some concerns with the paper which are more fit to be discussed as questions.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the implicit bias of the Adam optimizer for logistic regression on linearly separable data. The authors prove that Adam converges to the linear classifier with the maximum $\ell_\infty$-margin. This result contrasts with the classical results on (stochastic) gradient descent (with or without momentum), which converge to the maximum $\ell_2$-margin solution.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The authors theoretically study a popular yet not well-understood optimization method, Adam, in the context of a well-studied classical problem: logistic regression on linearly separable data. This offers a solid and insightful contribution to understanding Adam. In particular, distinguishing Adam from (S)GD with/without momentum on this classical problem is a very interesting result.
- The technical contributions are also of independent interest, as they prove the results for Adam without relying on the stability constant (which is closer to practice) and use mild assumptions.
- The paper is well-written and easy to follow. The proof sketch provides a clear and comprehensive overview of the proof of the main theorem.

Weaknesses:
There are no major concerns about this paper. Below are minor comments and some areas for improvement:
- The paper does not provide an intuition behind why Adam achieves the maximum $\ell_\infty$-margin solution, in contrast to GD which achieves the maximum $\ell_2$-margin solution. It would be great if the authors could offer insights on how the $\ell_\infty$-margin arises instead of the $\ell_2$-margin, for example, through a warm-up analysis with SignGD ($\beta_1=\beta_2=0$) or RMSProp ($\beta_1=0$). One way to provide an intuition is as follows: Gunasekar et al. (2018) proved that steepest descent converges to the max-margin solution, implying that SignGD (steepest descent w.r.t. $\ell_\infty$-norm) converges to the maximum $\ell_\infty$-margin solution. Since SignGD is known to be a good proxy for Adam, this may offer an insight into why Adam converges to the maximum $\ell_\infty$-margin solution.
- The authors claim that the bounds in Corollary 4.7 are derived under worst-case scenarios and argue that this is why, in practice, we often observe margins converging faster than the bounds in the corollary. However, this statement lacks supporting evidence. The paper should prove that the rate of convergence is tight. Otherwise, the observed faster convergence of margins in experiments might simply indicate that the bound is not tight enough.
- Some sentences, including those in the abstract, use the term ""convergence"" unclearly. For example, in the abstract, ""this convergence occurs within polynomial time"" does not indicate the objective (the normalized $\ell_\infty$-margin in this case) of convergence. This could be confused with other notions of convergence, such as convergence in direction (i.e., $\frac{w_t}{\lVert w_t \rVert} \to \frac{w^*}{\lVert w^* \rVert}$).
- (page 6, line 183) According to the paper, the normalized $\ell_2$-margin converges at a speed of $O(\log \log t / \log t)$ when using GD. However, this should be corrected to $O(1 / \log t)$. According to Soudry et al. (2018), the normalized weight vector converges to the maximum $\ell_2$-margin vector ""in direction"" with a convergence rate of $O(\log \log t / \log t)$, i.e., $\lVert \frac{w_t}{\lVert w_t \rVert} - \frac{w^*}{\lVert w^* \rVert}\rVert = O(\log \log t / \log t)$. However, the normalized $\ell_2$-margin converges at the speed of $O(1/\log t)$, i.e., $|\min \frac{\langle w_t, y_t \cdot x_t \rangle}{\lVert w_t \rVert} - \frac{\langle w^*, y_t \cdot x_t \rangle}{\lVert w^* \rVert} | = O(1/\log t)$.
- (page 1, line 25) Typo: reply on -> rely on

---
[Gunasekar et al. 2018] Characterizing Implicit Bias in Terms of Optimization Geometry, ICML 2018.

[Soudry et al. 2018] The Implicit Bias of Gradient Descent on Separable Data, JMLR 2018.

Limitations:
The paper discusses its limitations and future directions, including the extension of the results to homogeneous neural networks and the analysis of stochastic Adam instead of full-batch Adam. I think both directions are promising avenues for future research.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xQWJBeK5rh;"REVIEW 
Summary:
The paper introduces the SICSM framework, integrating Selective State Space Models (SSMs) with Generative Flow Networks (GFNs) to tackle challenges in dynamical systems characterized by irregularly sampled trajectories and partial observations. SICSM leverages the adaptive temporal modeling capabilities of SSMs to learn input-dependent transition functions, enhancing structural inference accuracy. It aggregates diverse temporal dependencies and channels them into a GFN to approximate the posterior distribution of the system’s structure. Extensive evaluations across multiple datasets demonstrate SICSM's good performance in accurately inferring complex interactions in partially observed systems.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The integration of Selective SSMs with GFNs is a novel approach that addresses significant challenges in structural inference for dynamical systems. The adaptive mechanisms for handling irregular sampling and partial observations are particularly innovative.
- The research is thorough and well-documented, with extensive evaluations across a variety of datasets. The methodological rigor and comprehensive experimental validation enhance the reliability of the findings.
- The paper is well-organized and clearly written, with detailed explanations of the methodologies and experimental setups. Figures and diagrams effectively illustrate the concepts and results.
- The proposed SICSM framework has broad applicability in scientific discovery and system diagnostics across multiple disciplines. Its ability to handle real-world complexities such as irregular sampling and partial observations makes it a valuable tool for researchers.

Weaknesses:
- The implementation of SICSM is computationally intensive, requiring significant resources and expertise. This complexity may limit its accessibility and widespread adoption.

Limitations:
The authors have adequately addressed the limitations of their work, including the challenges posed by irregular sampling and partial observations. They propose future research directions to explore dynamic systems with mutable structural elements, indicating a proactive approach to potential limitations. The discussion on incorporating prior knowledge and adapting to different hop distances further strengthens the framework’s applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to combine State Space Models and Generative Flow Networks to perform structural inference in an irregular time series context. The proposed method is evaluated on a series of different tasks where it performs well, and compared to a number of baselines. The method's robustness to short time series and missing observations is evaluated.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proposes an interesting architecture and solves problems that have the potential to be very relevant in real world contexts, such as biological time series. The empirical evaluation is fairly thorough about testing on many different tasks.

Weaknesses:
My main concerns for the paper are its low novelty and its low number of ablations, which make it hard to understand how specific pieces contribute to the performance of the method.

Generally I'm uncomfortable with the way many things are presented in the paper, it's not always clear what's a novel contribution and what's not. I encourage the authors to be clear and exercise an abundance of caution. 

/!\\ In my humble opinion this paper uncomfortably downplays its similarity to DAG-GFN [14] and JSP-GFN [15] in several places, and I'm not even an author of these papers. This is especially concerning considering that in many instances JSP-GFN is the closest performing baseline to the proposed method.

Limitations:
Adequate.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors consider the problem of structure learning of dynamical systems from irregularly sampled trajectories and partially observed systems. They propose Structural Inference with Conjoined State Space Models (SICSM), a method based on selective state space models (SSMs) and generative flow network (GFNs). The central idea of this work is to use a SSM for modelling the behaviour of dynamical systems while using a GFN to learn the interacting graph structure between the variables of the system. The authors evaluate their proposed approach on a comprehensive set of datasets for various tasks and compare against a numerous baselines.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors present a method that addresses a challenging problem in the domain structure learning of dynamical systems -- i.e. learning system structure from irregularly sampled trajectories and partially observed systems. The use of SSMs to approximate system dynamics while using GFNs to learn the graph structure of the system is unique and novel approach to this problem. The authors provide a comprehensive evaluation of their method over variety of systems for irregularly sampled trajectories and partially observed systems, demonstrating SICSM consistently outperforms counterpart approaches.

Weaknesses:
- The method has 3 key components: state space model, embedding residual blocks, and a GFN to approximate the graph structure of the system. It is not entirely clear how these individual components interact and the explicit need for the GFN (see questions below). 
- The authors consider a comprehensive set of datasets and baselines, but only one evaluation metrics (AUROC). For example, some other metrics to consider for this task are: structural hamming distance (SHD), F1-score, area under the precision-recall curve (AUPRC). Only considering one evaluation metrics makes it difficult to assess the robustness of the approach.
- Another method that seems relevant to this work which address an similar problems is CUTS (Cheng et al. 2023). It appears that majority of the baselines considered in this work are or not necessarily methods explicitly tailored to handle irregular time-series. Including a method like CUTS in this evaluation may be important to create a fairer comparison of SICSM. 

References:
Cheng, Yuxiao, et al. ""Cuts: Neural causal discovery from irregular time-series data."" International Conference on Learning Representations (2023).

Limitations:
The authors discuss limitations and broader impacts in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Processes of scientific interest which are representable as graphs, in biology, chemistry, material sciences, mechanics, are an important application for machine learning. Nodes often represent physical objects, some of which influence each other. Nodes exhibit a set of features which can be observed over time. Prior knowledge about the process stems from a mechanistic understanding and can often be represented as the presence or absence of edges between nodes. Node feature observations may be irregularly spaced through time; not all nodes may be observed with every observation. 
This paper develops a statistical model for this application with support for irregularly sampled and partial observations of node features, as well as prior knowledge incorporation. Prior knowledge is restricted to the indication of presence, but not absence, of edges. Partially observable nodes are assumed to be from a static node set throughout all observations (i.e., nodes are either always observable or always unobservable). Observations are not assume to contain a timestamp indication (as in mobile phone accelerometer readings, which may be irregularly sampled but whose timestamp is read at input).
The model's architecture is relatively sophisticated and is based on generative flow networks to represent and learn the structural aspects of the graph, and state space models to represent the evolution of node features over time. 
The paper presents experiments on 16 datasets stemming from 4 physical models, and compares to 7 other models, showing superiority in scenarios where observations are irregularly spaced or nodes partially observable.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper takes an established problem class (graph systems) with its known challenges (irregular sampling, partial observations), which is not original. However it goes to great lengths to make use of two strong methods, GFN and SSM, with a resulting combination that seems reasonable, strong and of useful application.
The paper is generally clear, notations are coherent and legible, several diagrams support the explanation. To improve the writing, a running example might help bridge the abstractions (node, edge, state...) to physical reality, illuminating and motivating the implementation. The same goes to comment on the connection between the model and the applied datasets (some of this is covered in Annex C, with the exception of C.5 which leaves the physical counterparts of modelled data undescribed).

Weaknesses:
Experimental validation is moderately convincing. Baseline implementations seem strong, with care taken to recover implementations of competing methods, as documented in Annex D. However, all datasets are synthetic. The only real dataset, PEMS, presented in Annex C.5, with results in Annex E.2. In addition, experimental validation seems unconcerned with performance outside the specific cases of partial observations or irregular sampling -- reducing the paper's claim to ""this model is better for these two scenarios only"".
There seem to be a duplication in the presentation of datasets (both in sec5.1, between the paragraphs starting l.279 and l.290, and again between Annex C.1 and C.2 vs C.4) -- this is confusing. Also, sec3.3 seems to be internally redundant with duplicated points (e.g. l.151 vs eq3, and l.148 vs l.157, which again is confusing. Numerous sentences have incorrect English syntax which obscures their meaning

Limitations:
* Checklist point 4 and 5: implementation link is claimed to be provided here and in Annex l.863, but I don't see it. It should be provided in the main paper since the supplementary can't be assumed to be reviewed.
* Limitations: a few more assumptions on the usage scenario should be spelt out, as mentioned in this review
* Checklist point 7: It is certainly possible to report error bars on plots through shading, provided they are not as tiny as you make them here. In addition, error bars could be reported, without lengthening the main paper, in the Annexes -- but they aren't, with the only exception of Table 3 on an experiment which is not reported in the main paper.
* Checklist point 6: experimental settings are not as detailed as that they would allow reproduction. Several details are missing for this, e.g. batch size, data splits.
* Checklist point 12: the claim, and requirement of the checklist that assets have their license mentioned is not complied with regarding either datasets or existing code for competing methods. Despite the claim, this point mostly is not complied with.
* Checklist point 13: does this imply the code is not intended to be released as an asset?

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to combine State Space Models and Generative Flow Networks to perform structural inference in an irregular time series context. The proposed method is evaluated on a series of different tasks where it performs well, and compared to a number of baselines. The method's robustness to short time series and missing observations is evaluated.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proposes an interesting architecture and solves problems that have the potential to be very relevant in real world contexts, such as biological time series. The empirical evaluation is fairly thorough about testing on many different tasks.

Weaknesses:
My main concerns for the paper are its low novelty and its low number of ablations, which make it hard to understand how specific pieces contribute to the performance of the method.

Generally I'm uncomfortable with the way many things are presented in the paper, it's not always clear what's a novel contribution and what's not. I encourage the authors to be clear and exercise an abundance of caution. 

/!\\ In my humble opinion this paper uncomfortably downplays its similarity to DAG-GFN [14] and JSP-GFN [15] in several places, and I'm not even an author of these papers. This is especially concerning considering that in many instances JSP-GFN is the closest performing baseline to the proposed method.

Limitations:
Adequate.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces the SICSM framework, integrating Selective State Space Models (SSMs) with Generative Flow Networks (GFNs) to tackle challenges in dynamical systems characterized by irregularly sampled trajectories and partial observations. SICSM leverages the adaptive temporal modeling capabilities of SSMs to learn input-dependent transition functions, enhancing structural inference accuracy. It aggregates diverse temporal dependencies and channels them into a GFN to approximate the posterior distribution of the system’s structure. Extensive evaluations across multiple datasets demonstrate SICSM's good performance in accurately inferring complex interactions in partially observed systems.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The integration of Selective SSMs with GFNs is a novel approach that addresses significant challenges in structural inference for dynamical systems. The adaptive mechanisms for handling irregular sampling and partial observations are particularly innovative.
- The research is thorough and well-documented, with extensive evaluations across a variety of datasets. The methodological rigor and comprehensive experimental validation enhance the reliability of the findings.
- The paper is well-organized and clearly written, with detailed explanations of the methodologies and experimental setups. Figures and diagrams effectively illustrate the concepts and results.
- The proposed SICSM framework has broad applicability in scientific discovery and system diagnostics across multiple disciplines. Its ability to handle real-world complexities such as irregular sampling and partial observations makes it a valuable tool for researchers.

Weaknesses:
- The implementation of SICSM is computationally intensive, requiring significant resources and expertise. This complexity may limit its accessibility and widespread adoption.

Limitations:
The authors have adequately addressed the limitations of their work, including the challenges posed by irregular sampling and partial observations. They propose future research directions to explore dynamic systems with mutable structural elements, indicating a proactive approach to potential limitations. The discussion on incorporating prior knowledge and adapting to different hop distances further strengthens the framework’s applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors consider the problem of structure learning of dynamical systems from irregularly sampled trajectories and partially observed systems. They propose Structural Inference with Conjoined State Space Models (SICSM), a method based on selective state space models (SSMs) and generative flow network (GFNs). The central idea of this work is to use a SSM for modelling the behaviour of dynamical systems while using a GFN to learn the interacting graph structure between the variables of the system. The authors evaluate their proposed approach on a comprehensive set of datasets for various tasks and compare against a numerous baselines.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors present a method that addresses a challenging problem in the domain structure learning of dynamical systems -- i.e. learning system structure from irregularly sampled trajectories and partially observed systems. The use of SSMs to approximate system dynamics while using GFNs to learn the graph structure of the system is unique and novel approach to this problem. The authors provide a comprehensive evaluation of their method over variety of systems for irregularly sampled trajectories and partially observed systems, demonstrating SICSM consistently outperforms counterpart approaches.

Weaknesses:
- The method has 3 key components: state space model, embedding residual blocks, and a GFN to approximate the graph structure of the system. It is not entirely clear how these individual components interact and the explicit need for the GFN (see questions below). 
- The authors consider a comprehensive set of datasets and baselines, but only one evaluation metrics (AUROC). For example, some other metrics to consider for this task are: structural hamming distance (SHD), F1-score, area under the precision-recall curve (AUPRC). Only considering one evaluation metrics makes it difficult to assess the robustness of the approach.
- Another method that seems relevant to this work which address an similar problems is CUTS (Cheng et al. 2023). It appears that majority of the baselines considered in this work are or not necessarily methods explicitly tailored to handle irregular time-series. Including a method like CUTS in this evaluation may be important to create a fairer comparison of SICSM. 

References:
Cheng, Yuxiao, et al. ""Cuts: Neural causal discovery from irregular time-series data."" International Conference on Learning Representations (2023).

Limitations:
The authors discuss limitations and broader impacts in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Processes of scientific interest which are representable as graphs, in biology, chemistry, material sciences, mechanics, are an important application for machine learning. Nodes often represent physical objects, some of which influence each other. Nodes exhibit a set of features which can be observed over time. Prior knowledge about the process stems from a mechanistic understanding and can often be represented as the presence or absence of edges between nodes. Node feature observations may be irregularly spaced through time; not all nodes may be observed with every observation. 
This paper develops a statistical model for this application with support for irregularly sampled and partial observations of node features, as well as prior knowledge incorporation. Prior knowledge is restricted to the indication of presence, but not absence, of edges. Partially observable nodes are assumed to be from a static node set throughout all observations (i.e., nodes are either always observable or always unobservable). Observations are not assume to contain a timestamp indication (as in mobile phone accelerometer readings, which may be irregularly sampled but whose timestamp is read at input).
The model's architecture is relatively sophisticated and is based on generative flow networks to represent and learn the structural aspects of the graph, and state space models to represent the evolution of node features over time. 
The paper presents experiments on 16 datasets stemming from 4 physical models, and compares to 7 other models, showing superiority in scenarios where observations are irregularly spaced or nodes partially observable.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper takes an established problem class (graph systems) with its known challenges (irregular sampling, partial observations), which is not original. However it goes to great lengths to make use of two strong methods, GFN and SSM, with a resulting combination that seems reasonable, strong and of useful application.
The paper is generally clear, notations are coherent and legible, several diagrams support the explanation. To improve the writing, a running example might help bridge the abstractions (node, edge, state...) to physical reality, illuminating and motivating the implementation. The same goes to comment on the connection between the model and the applied datasets (some of this is covered in Annex C, with the exception of C.5 which leaves the physical counterparts of modelled data undescribed).

Weaknesses:
Experimental validation is moderately convincing. Baseline implementations seem strong, with care taken to recover implementations of competing methods, as documented in Annex D. However, all datasets are synthetic. The only real dataset, PEMS, presented in Annex C.5, with results in Annex E.2. In addition, experimental validation seems unconcerned with performance outside the specific cases of partial observations or irregular sampling -- reducing the paper's claim to ""this model is better for these two scenarios only"".
There seem to be a duplication in the presentation of datasets (both in sec5.1, between the paragraphs starting l.279 and l.290, and again between Annex C.1 and C.2 vs C.4) -- this is confusing. Also, sec3.3 seems to be internally redundant with duplicated points (e.g. l.151 vs eq3, and l.148 vs l.157, which again is confusing. Numerous sentences have incorrect English syntax which obscures their meaning

Limitations:
* Checklist point 4 and 5: implementation link is claimed to be provided here and in Annex l.863, but I don't see it. It should be provided in the main paper since the supplementary can't be assumed to be reviewed.
* Limitations: a few more assumptions on the usage scenario should be spelt out, as mentioned in this review
* Checklist point 7: It is certainly possible to report error bars on plots through shading, provided they are not as tiny as you make them here. In addition, error bars could be reported, without lengthening the main paper, in the Annexes -- but they aren't, with the only exception of Table 3 on an experiment which is not reported in the main paper.
* Checklist point 6: experimental settings are not as detailed as that they would allow reproduction. Several details are missing for this, e.g. batch size, data splits.
* Checklist point 12: the claim, and requirement of the checklist that assets have their license mentioned is not complied with regarding either datasets or existing code for competing methods. Despite the claim, this point mostly is not complied with.
* Checklist point 13: does this imply the code is not intended to be released as an asset?

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xOCAURlVM9;"REVIEW 
Summary:
This paper presents a novel 3D object retrieval method. First, to facilitate this task, the authors build 3 datasets for training and evaluation, which may significantly benefit the community. Then the paper propose the Isomorphic Assembly Embedding (IAE) and the Structured Fuzzy Reconstruction (SFR) modules, which are designed to generate assembly embeddings with geometric-semantic consistency and overcome the distribution skew of unseen categories. Besides, HIConv is proposed to capture high-order correlations within and among objects. Extensive experiments show that the method achieves sota performance.

Soundness:
3: good

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
1. This paper builds 3 datasets for the task, which may facilitate future research. 
2. The paper proposes several novel modules to capture the part-level and inter-object features for object retrieval.
3. The task itself is important in shape understanding.

Weaknesses:
1. No visualization results.
2. The presentation is hard to understand. There are quite some complex equations, like Eq 2 and Eq 4. Please briefly explain what they mean and how they work.
3. In Fig. 1, it shows that intra-object features are extracted before inter-category features. But in Fig. 2, I only see Inter-object features? It's hard for me to match them up.
4. I still don't understand the input. So you need dense point cloud with ground truth 3D part segmentation as input, right? If the segmenation is not perfect, will the method collapse? if the point cloud undergoes SE(3)-transformation, will the method collapse? Can this method handle partial point cloud input, like the point cloud back-projected from depth map?

Limitations:
The authors didn't discuss the limitations

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The manuscript introduces a framework (HAFR) for addressing the challenge of open-set 3D object retrieval. The authors propose a bottom-up approach focusing on part assembly, leveraging both geometric and semantic information of object parts to enhance retrieval performance across categories, including those unseen during training.

The HAFR framework consists of two main modules: Isomorphic Assembly Embedding (IAE) and Structured Fuzzy Reconstruction (SFR). The IAE module utilizes Hypergraph Isomorphism Convolution (HIConv) and assembly auto-encoders to generate embeddings with geometric-semantic consistency. The SFR module tackles distribution skew in open-set retrieval by constructing a leveraged hypergraph based on local and global correlations and employs a memory bank for fuzzy-aware reconstruction.

The authors have created three datasets, OP-SHNP, OP-INTRA, and OP-COSEG, to benchmark their approach. Extensive experiments demonstrate the superiority of HAFR over current state-of-the-art methods in open-set 3D object retrieval tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a method for open-set 3D object retrieval that cleverly integrates part-level information using hypergraphs, which is a unique and promising direction in the field. The HAFR framework is well-thought-out, with clearly defined modules (IAE and SFR) that address different aspects of the retrieval task, from assembly isomorphism to distribution skew mitigation.
- The construction of three new datasets with part-level annotations provides a valuable resource for the research community and supports the validation of the proposed method.
- The methodology is clearly described, and the algorithms are well-structured, making it relatively easy for readers to follow the technical contributions.
- The paper is well-written and easy to follow.

Weaknesses:
- The paper does not address scenarios with varying numbers of parts per object. Expanding the framework to handle flexibility in the number of parts could improve its applicability.
- The manuscript could benefit from a discussion on the computational complexity and efficiency of the proposed methods, especially when scaling to larger datasets or higher-dimensional part features.
- Why not evaluate on the PartNet(https://partnet.cs.stanford.edu/)?
- Although the paper claims state-of-the-art performance, they do not achieve the best (SDML is the best on OP-COSEG for NDCG metric), what is the reason?
- Some implementation details, such as network architecture specifics and hyperparameter settings, could be better elaborated to ensure reproducibility.
- The paper mentions that data and code will be made available upon acceptance, which is good practice. - However, providing this information upfront or during the review process could enhance transparency and reproducibility. For the three datasets, the detailed construction is missing and encourages the authors to publicize the data, facilitating the community.
- The limitations and failure cases should be discussed comprehensively.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to utilize the part-assembly representation method to mitigate the distribution skew of unseen categories, enhancing the generalization performance for open-set 3D object retrieval. Compared to previous methods, this paper benefits from part-level representation learning rather than object-level representation, obtaining in a good generalization on unseen categories. To utilize the part-level representation, this paper introduces Isomorphic Assembly Embedding (IAE) and the Structured Fuzzy Reconstruction (SFR) modules. The former can generate the assembly embedding isomorphically for each object, and the latter is used for generating the fuzzy representation thus overcoming the distribution skew of unseen categories.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The problem is well-motivated and the solution seems working well. The results are good. The paper also contributes three 3D point cloud datasets with multiple part annotations for benchmarking. Extensive experiments on the three benchmarks demonstrate the superiority of the proposed method over current state-of-the-art 3D object retrieval methods.

Weaknesses:
1. The datasets OP-INTRA and OP-COSEG mentioned in the paper may have limitations in category diversity, number of parts, and dataset size, which may affect the generalization ability of the model.
2. The framework comprises many sub-architectures, such as the HIConv layer, multiple auto-encoders, fuzzy embeddings, and memory bank, it seems to be relatively complex. However, this paper does not explicitly discuss the computational efficiency of the model, including training and inference time, and computational cost.
3. Though the paper proposes a solution to the open set problem, the datasets are all virtual. Its generalization ability to unseen categories in real-world applications still needs further verification.
4. The ablation studies show the effect of the HIConv layer. However, only comparisons with MLP and GIN are performed, but no comparisons with other neural layers such as KAN, nor is the number of HIConv layers ablated.
5. The experiments are only conducted on the proposed datasets. The generalization ability of the model on a wider data distribution requires more verification. It would be better to add some experiments on previous public datasets or datasets without open-set settings to demonstrate generalization capabilities.

Limitations:
The paper should add discussions on limitations and possibly show some failure cases.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method for finding similar samples from a set of 3D objects given query objects in an open setting, where objects can belong to both already seen and new categories. This method is based on considering 3D objects as hypergraphs consisting of individual geometric and semantic parts of objects. The hypergraph is used to form Isomorphic Assembly Embedding. The second part of the proposed HAFR framework is the Structured Fuzzy Representation module that constructs a hypergraph based on local certainty and global uncertainty correlation to enable transfer from seen to unseen categories. The authors propose a new layer, HIConv, which improves the quality of the generated representation. The authors demonstrate the effectiveness of their approach on three datasets that they constructed for this task.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The idea that one can understand the whole object shape from its parts sounds interesting and reasonable.
- The description of Isomorphic Assembly Embedding and Structured Fuzzy Reconstruction is formal and rather clear.
- The authors conduct extensive ablation studies of their method.

Weaknesses:
- Based on the provided experiments, it is unclear if HAFR can generalize well to an unseen domain. Are the results in Table 2 provided for the same suite of model weights?
- The literature review does not include existing methods for open-set 3d object retrieval and recent methods for closed-set 3d object retrieval.
- When comparing with other methods, the authors use their own modification of existing multimodal methods. A comparison with modern methods for open-set 3d object retrieval, such as [1], is necessary to demonstrate the effectiveness of this particular method of object representation.
- The method's description lacks an explanation of how the resulting fuzzy embeddings are used to find similar objects. Additionally, the description contains undefined concepts like isomorphism loss and integration function. If these concepts are not introduced by the authors, please include references to articles where they are defined.

[1] Zhou, J., Wang, J., Ma, B., Liu, Y. S., Huang, T., & Wang, X. (2023). Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773.

Limitations:
The authors discuss limitations in the conclusion regarding the use of the assembly fuzzy representation for a varying number of object parts. In my opinion, another limitation is the need to segment the point cloud into parts to use this method.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a framework for open-set 3D object retrieval, called the Hypergraph-Based Assembly Fuzzy Representation (HAFR) framework. This model leverages an Isomorphic Assembly Embedding (IAE) to integrate geometric and semantic consistency. Furthermore, a Structured Fuzzy Reconstruction (SFR) is used to overcome the distribution skew of unseen categories. On three point cloud datasets constructed by the authors, this model outperforms the state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The motivation for this work is well-established.
- The idea of using hypergraph structures to achieve high-order correlations both within and between objects is novel.
- Sufficient quantitative and qualitative comparisons verify the effectiveness of the proposed model.

Weaknesses:
- In structured fuzzy reconstruction, the value of k in the k-nearest neighbors seems to determine the global uncertainty hyperedge. However, the paper lacks explanation or experiments to clarify the selection of k value.

- While HGM2R [1] employs a multimodal approach, the IAE component appears to be similar to the Multi-Modal 3D Object Embedding in HGM2R. What are the differences and unique contributions of IAE compared to the embedding technique used in HGM2R?

-In Table 2, although HGM2R also utilizes hypergraphs, it shows only slight improvements over previous methods in most metrics. For example, the mAP scores on three datasets are only about 0.1 higher. However, the method proposed in this paper demonstrates a significant improvement over HGM2R on the OP-COSEG dataset, with an increase of nearly 0.6. How can this result be explained?
[1] Hypergraph-Based Multi-Modal Representation for Open-Set 3D Object Retrieval. TPAMI 2023.

Limitations:
The authors discusseds the limitations in the conclusion section. But I did not find the societal impacts mentioned in the conclusion section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel 3D object retrieval method. First, to facilitate this task, the authors build 3 datasets for training and evaluation, which may significantly benefit the community. Then the paper propose the Isomorphic Assembly Embedding (IAE) and the Structured Fuzzy Reconstruction (SFR) modules, which are designed to generate assembly embeddings with geometric-semantic consistency and overcome the distribution skew of unseen categories. Besides, HIConv is proposed to capture high-order correlations within and among objects. Extensive experiments show that the method achieves sota performance.

Soundness:
3: good

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
1. This paper builds 3 datasets for the task, which may facilitate future research. 
2. The paper proposes several novel modules to capture the part-level and inter-object features for object retrieval.
3. The task itself is important in shape understanding.

Weaknesses:
1. No visualization results.
2. The presentation is hard to understand. There are quite some complex equations, like Eq 2 and Eq 4. Please briefly explain what they mean and how they work.
3. In Fig. 1, it shows that intra-object features are extracted before inter-category features. But in Fig. 2, I only see Inter-object features? It's hard for me to match them up.
4. I still don't understand the input. So you need dense point cloud with ground truth 3D part segmentation as input, right? If the segmenation is not perfect, will the method collapse? if the point cloud undergoes SE(3)-transformation, will the method collapse? Can this method handle partial point cloud input, like the point cloud back-projected from depth map?

Limitations:
The authors didn't discuss the limitations

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The manuscript introduces a framework (HAFR) for addressing the challenge of open-set 3D object retrieval. The authors propose a bottom-up approach focusing on part assembly, leveraging both geometric and semantic information of object parts to enhance retrieval performance across categories, including those unseen during training.

The HAFR framework consists of two main modules: Isomorphic Assembly Embedding (IAE) and Structured Fuzzy Reconstruction (SFR). The IAE module utilizes Hypergraph Isomorphism Convolution (HIConv) and assembly auto-encoders to generate embeddings with geometric-semantic consistency. The SFR module tackles distribution skew in open-set retrieval by constructing a leveraged hypergraph based on local and global correlations and employs a memory bank for fuzzy-aware reconstruction.

The authors have created three datasets, OP-SHNP, OP-INTRA, and OP-COSEG, to benchmark their approach. Extensive experiments demonstrate the superiority of HAFR over current state-of-the-art methods in open-set 3D object retrieval tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a method for open-set 3D object retrieval that cleverly integrates part-level information using hypergraphs, which is a unique and promising direction in the field. The HAFR framework is well-thought-out, with clearly defined modules (IAE and SFR) that address different aspects of the retrieval task, from assembly isomorphism to distribution skew mitigation.
- The construction of three new datasets with part-level annotations provides a valuable resource for the research community and supports the validation of the proposed method.
- The methodology is clearly described, and the algorithms are well-structured, making it relatively easy for readers to follow the technical contributions.
- The paper is well-written and easy to follow.

Weaknesses:
- The paper does not address scenarios with varying numbers of parts per object. Expanding the framework to handle flexibility in the number of parts could improve its applicability.
- The manuscript could benefit from a discussion on the computational complexity and efficiency of the proposed methods, especially when scaling to larger datasets or higher-dimensional part features.
- Why not evaluate on the PartNet(https://partnet.cs.stanford.edu/)?
- Although the paper claims state-of-the-art performance, they do not achieve the best (SDML is the best on OP-COSEG for NDCG metric), what is the reason?
- Some implementation details, such as network architecture specifics and hyperparameter settings, could be better elaborated to ensure reproducibility.
- The paper mentions that data and code will be made available upon acceptance, which is good practice. - However, providing this information upfront or during the review process could enhance transparency and reproducibility. For the three datasets, the detailed construction is missing and encourages the authors to publicize the data, facilitating the community.
- The limitations and failure cases should be discussed comprehensively.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to utilize the part-assembly representation method to mitigate the distribution skew of unseen categories, enhancing the generalization performance for open-set 3D object retrieval. Compared to previous methods, this paper benefits from part-level representation learning rather than object-level representation, obtaining in a good generalization on unseen categories. To utilize the part-level representation, this paper introduces Isomorphic Assembly Embedding (IAE) and the Structured Fuzzy Reconstruction (SFR) modules. The former can generate the assembly embedding isomorphically for each object, and the latter is used for generating the fuzzy representation thus overcoming the distribution skew of unseen categories.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The problem is well-motivated and the solution seems working well. The results are good. The paper also contributes three 3D point cloud datasets with multiple part annotations for benchmarking. Extensive experiments on the three benchmarks demonstrate the superiority of the proposed method over current state-of-the-art 3D object retrieval methods.

Weaknesses:
1. The datasets OP-INTRA and OP-COSEG mentioned in the paper may have limitations in category diversity, number of parts, and dataset size, which may affect the generalization ability of the model.
2. The framework comprises many sub-architectures, such as the HIConv layer, multiple auto-encoders, fuzzy embeddings, and memory bank, it seems to be relatively complex. However, this paper does not explicitly discuss the computational efficiency of the model, including training and inference time, and computational cost.
3. Though the paper proposes a solution to the open set problem, the datasets are all virtual. Its generalization ability to unseen categories in real-world applications still needs further verification.
4. The ablation studies show the effect of the HIConv layer. However, only comparisons with MLP and GIN are performed, but no comparisons with other neural layers such as KAN, nor is the number of HIConv layers ablated.
5. The experiments are only conducted on the proposed datasets. The generalization ability of the model on a wider data distribution requires more verification. It would be better to add some experiments on previous public datasets or datasets without open-set settings to demonstrate generalization capabilities.

Limitations:
The paper should add discussions on limitations and possibly show some failure cases.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method for finding similar samples from a set of 3D objects given query objects in an open setting, where objects can belong to both already seen and new categories. This method is based on considering 3D objects as hypergraphs consisting of individual geometric and semantic parts of objects. The hypergraph is used to form Isomorphic Assembly Embedding. The second part of the proposed HAFR framework is the Structured Fuzzy Representation module that constructs a hypergraph based on local certainty and global uncertainty correlation to enable transfer from seen to unseen categories. The authors propose a new layer, HIConv, which improves the quality of the generated representation. The authors demonstrate the effectiveness of their approach on three datasets that they constructed for this task.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The idea that one can understand the whole object shape from its parts sounds interesting and reasonable.
- The description of Isomorphic Assembly Embedding and Structured Fuzzy Reconstruction is formal and rather clear.
- The authors conduct extensive ablation studies of their method.

Weaknesses:
- Based on the provided experiments, it is unclear if HAFR can generalize well to an unseen domain. Are the results in Table 2 provided for the same suite of model weights?
- The literature review does not include existing methods for open-set 3d object retrieval and recent methods for closed-set 3d object retrieval.
- When comparing with other methods, the authors use their own modification of existing multimodal methods. A comparison with modern methods for open-set 3d object retrieval, such as [1], is necessary to demonstrate the effectiveness of this particular method of object representation.
- The method's description lacks an explanation of how the resulting fuzzy embeddings are used to find similar objects. Additionally, the description contains undefined concepts like isomorphism loss and integration function. If these concepts are not introduced by the authors, please include references to articles where they are defined.

[1] Zhou, J., Wang, J., Ma, B., Liu, Y. S., Huang, T., & Wang, X. (2023). Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773.

Limitations:
The authors discuss limitations in the conclusion regarding the use of the assembly fuzzy representation for a varying number of object parts. In my opinion, another limitation is the need to segment the point cloud into parts to use this method.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a framework for open-set 3D object retrieval, called the Hypergraph-Based Assembly Fuzzy Representation (HAFR) framework. This model leverages an Isomorphic Assembly Embedding (IAE) to integrate geometric and semantic consistency. Furthermore, a Structured Fuzzy Reconstruction (SFR) is used to overcome the distribution skew of unseen categories. On three point cloud datasets constructed by the authors, this model outperforms the state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The motivation for this work is well-established.
- The idea of using hypergraph structures to achieve high-order correlations both within and between objects is novel.
- Sufficient quantitative and qualitative comparisons verify the effectiveness of the proposed model.

Weaknesses:
- In structured fuzzy reconstruction, the value of k in the k-nearest neighbors seems to determine the global uncertainty hyperedge. However, the paper lacks explanation or experiments to clarify the selection of k value.

- While HGM2R [1] employs a multimodal approach, the IAE component appears to be similar to the Multi-Modal 3D Object Embedding in HGM2R. What are the differences and unique contributions of IAE compared to the embedding technique used in HGM2R?

-In Table 2, although HGM2R also utilizes hypergraphs, it shows only slight improvements over previous methods in most metrics. For example, the mAP scores on three datasets are only about 0.1 higher. However, the method proposed in this paper demonstrates a significant improvement over HGM2R on the OP-COSEG dataset, with an increase of nearly 0.6. How can this result be explained?
[1] Hypergraph-Based Multi-Modal Representation for Open-Set 3D Object Retrieval. TPAMI 2023.

Limitations:
The authors discusseds the limitations in the conclusion section. But I did not find the societal impacts mentioned in the conclusion section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xO9GHdmK76;"REVIEW 
Summary:
This work proposes a novel approach for enhancing neural network performance by scaling feature interaction spaces to infinite dimensions using kernel methods. Recent advancements have introduced feature interaction spaces, but these are often limited to finite dimensions, primarily through element-wise multiplications. To overcome these limitations, the authors propose InfiNet, a model architecture leveraging the Radial Basis Function (RBF) kernel to enable infinite-dimensional feature interactions. Finally, the authors provide several empirical results on standard vision tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This work provides an interesting generalization of feature-feature interactions via kernels. For the best of my knowledge, this is a novel idea that appears to perform well in practice. However, I am not overly familiar with the current state of the field of deep learning for computer vision. It further provides several larger-scale experiments and interesting ablations.

Weaknesses:
* there is no theoretical justification that increasing the dimension of the feature-feature interaction space will lead to better generalization. The paper does a good job analysing this question with ablations. However, this remains an open theoretical question.

* I understand that the motivation for this work comes from applications in computer vision. However, since a major focus in this paper is on comparing the proposed approach to self attention, it would be interesting to not only test this method on images, but also on language. 

* the method is reported to have lower FLOPs on average than competing methods. Why is that? Is that a major drawback of this method?

* performance improvement on ImageNet is only marginally. In many cases the proposed method even performs worse than competing methods.

* paragraph starting in line 148: this is on over-claim and has to be removed or rigorously proved. It is not clear how a higher order of $k$ implies better generalization or training. Unless shown in this paper or referenced from another paper, this has to be removed.

Minor: 

* line 28: more context for formulating self attention that way has to be provided. It is explained in more detail only at the end of section 3. 

* caption of figure 2: there is '?'. Moreover, a description of the presented images should be included. What is shown in Figure 2 on the right hand side? This is only explained in the main text,not the caption. This needs to be changed.

* figure 2, first image on the left: hard to read -- text overlaps with drawing.

Limitations:
* The paper provides empirical results only on vision tasks. However, a major selling point of this paper is generalizing approaches like self attention in terms of feature-feature interactions. Therefore, comparisons with transformers on language tasks should be performed. 

* No theoretical analysis is provided proving that the proposed method leads to better generalization.

* The method appears to have on average lower FLOPs than competing methods, while at the same time only marginally outperforming (or even performing worse than) competing methods on imageNet.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies placing a kernel function inside of a neural network architecture to facilitate interaction of features/dimensional expansion. They consider deep convolutional networks with parallel pathway features $x$ and $x'$ and a kernel function computed with both pathways' features as inputs $k(x,x')$. Standard kernel mathematics is used to explain feature expansion. The main novel results are empirical performance of these ""InfiNet"" architectures, which are shown to perform well in a number of computer vision tests.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea of unifying different orders of interaction embodied in various neural network architectures, including Transformers is appealing and probably important. The accuracy of the InfiNet experiments is impressive, with a moderate reduction in FLOPs. The paper is easy to read and well-organized, although suggestions are given for how it could be improved.

Weaknesses:
My main concerns with the paper are a lack of context for the approach as well as missing important explanations. I also think a good amount of the math that's included could be considered ""filler"" material that could go into the appendix, since it doesn't represent new results. (I am referring to sections 4.1 and 4.2, most of which can be found in most textbooks which cover kernel methods.)

* Notation which is commonly used in the paper $\oplus$, $\otimes$, * is not explained. You should _explicitly_ define it somewhere, at least in the appendix (and refer people there). In particular, people may be confused by * for elementwise/Hadamard multiplication, since in convnet literature this is often the convolution operator. You call this the ""Star Operation"" in line 124, but I think it is just elementwise multiplication.
* The authors seem to have missed the vast literature on the connections between random features, neural networks at init, and kernel methods. (CKNs are mentioned but without any discussion of the topics I mention here.) In particular, one way that you could approximate the InfiNet architecture would be to take the two feature streams and pass them each into the same wide, random network/layer and compute the dot product of features at the next level. That would only approximate the kernel function in the InfiNet architecture, and is likely less efficient, but it provides a way to perform dimensionality expansion with a more traditional layer. The authors should discuss these connections.
* Different order of interactions have been studied in random feature and kernel settings already. In random features, interaction order is connected to the sparsity of weights, see e.g. https://arxiv.org/abs/2103.03191 and https://arxiv.org/abs/1909.02603. In kernels, this were referred to as additive kernels https://arxiv.org/abs/1602.00287, also studied in multiple kernel learning https://arxiv.org/abs/0809.1493 (these are just some examples among a larger literature).
* The authors do not seem to want to release their code. They have said ""Yes"" on Question 5, stating that the code and data are open, but there is no link or indication in the text that the code is available or will be when the paper is published. That seems deceptive.

Limitations:
I would strongly prefer that the limitations be included in the main text during the discussion. With movement of some of the standard math, there would be space.

The authors only consider the squared exponential kernel with bandwidth parameter equal to 1. Other kernels might work better. In particular, the effective dimensionality of the RKHS (related to the kernel decay rates) would be higher with a ""less smooth"" kernel like the exponential/Laplace kernel.

The results are likely not reproducible unless the authors release their code.

The results are also limited only to supervised vision tasks, rather than other modalities or unsupervised settings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a new architecture for computer vision applications that models high-order interactions between features. The architecture is similar to an attention block, but introduces an RBF Kernel layer that captures interactions of order higher than two. The resulting method has strong empirical performance across image classification tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of the paper is very interesitng and novel.
- The empirical results show promising performance across involve multiple tasks against sophisticated methods

Weaknesses:
- The presentation of the method seems overly complex in some places. For example, providing a clearer explanation of each new layer (perhaps in pseudocode) would help. While the Infiniblock definition is clear, the reader needs to go back to the previous section to understand the input/output shaped of the RBF layer, which takes work, and can be made simpler. Making clearer the intuition behind high-order interactions would be helpful as well. Showing examples of what the model learns would be helpful to make things concrete.
- The empirical performance is reasonably similar to those of previous methods, hence the empirical improvement is not that large.

Limitations:
I do not see any ethical and societal implications of the work that need to be discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper shifts the focus from traditional neural network design, which emphasizes feature representation space scaling, to feature interaction space scaling. It introduces a new model architecture, InfiNet, that enables feature interaction within an infinite-dimensional space using the RBF kernel, leading to state-of-the-art results. The paper also discusses the limitations of current models in capturing low-order interactions and proposes the use of classic kernel methods to engage features in an infinite-dimensional space.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The idea of the paper is simple, novel and well exposed. 

- The paper introduces InfiNet, a model architecture that leverages infinite-dimensional feature interactions using RBF kernels, which enhances model performance of traditional models.

- InfiNet achieves new state-of-the-art performance in various tasks, demonstrating the effectiveness of infinite-dimensional interactions.

- The paper includes extensive experiments on datasets like ImageNet and MS COCO, showing the scalability and efficiency of InfiNet.

Weaknesses:
- the paper builds on the simple use of kernel methods. The novelty of the methods is minimal, in the end it is an RBF kernel.

- the performance improvement of Infinet over other models is mostly marginal and no errors have been displayed.

- the paper doesn't really have theoretical novelty

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a novel approach for enhancing neural network performance by scaling feature interaction spaces to infinite dimensions using kernel methods. Recent advancements have introduced feature interaction spaces, but these are often limited to finite dimensions, primarily through element-wise multiplications. To overcome these limitations, the authors propose InfiNet, a model architecture leveraging the Radial Basis Function (RBF) kernel to enable infinite-dimensional feature interactions. Finally, the authors provide several empirical results on standard vision tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This work provides an interesting generalization of feature-feature interactions via kernels. For the best of my knowledge, this is a novel idea that appears to perform well in practice. However, I am not overly familiar with the current state of the field of deep learning for computer vision. It further provides several larger-scale experiments and interesting ablations.

Weaknesses:
* there is no theoretical justification that increasing the dimension of the feature-feature interaction space will lead to better generalization. The paper does a good job analysing this question with ablations. However, this remains an open theoretical question.

* I understand that the motivation for this work comes from applications in computer vision. However, since a major focus in this paper is on comparing the proposed approach to self attention, it would be interesting to not only test this method on images, but also on language. 

* the method is reported to have lower FLOPs on average than competing methods. Why is that? Is that a major drawback of this method?

* performance improvement on ImageNet is only marginally. In many cases the proposed method even performs worse than competing methods.

* paragraph starting in line 148: this is on over-claim and has to be removed or rigorously proved. It is not clear how a higher order of $k$ implies better generalization or training. Unless shown in this paper or referenced from another paper, this has to be removed.

Minor: 

* line 28: more context for formulating self attention that way has to be provided. It is explained in more detail only at the end of section 3. 

* caption of figure 2: there is '?'. Moreover, a description of the presented images should be included. What is shown in Figure 2 on the right hand side? This is only explained in the main text,not the caption. This needs to be changed.

* figure 2, first image on the left: hard to read -- text overlaps with drawing.

Limitations:
* The paper provides empirical results only on vision tasks. However, a major selling point of this paper is generalizing approaches like self attention in terms of feature-feature interactions. Therefore, comparisons with transformers on language tasks should be performed. 

* No theoretical analysis is provided proving that the proposed method leads to better generalization.

* The method appears to have on average lower FLOPs than competing methods, while at the same time only marginally outperforming (or even performing worse than) competing methods on imageNet.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies placing a kernel function inside of a neural network architecture to facilitate interaction of features/dimensional expansion. They consider deep convolutional networks with parallel pathway features $x$ and $x'$ and a kernel function computed with both pathways' features as inputs $k(x,x')$. Standard kernel mathematics is used to explain feature expansion. The main novel results are empirical performance of these ""InfiNet"" architectures, which are shown to perform well in a number of computer vision tests.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea of unifying different orders of interaction embodied in various neural network architectures, including Transformers is appealing and probably important. The accuracy of the InfiNet experiments is impressive, with a moderate reduction in FLOPs. The paper is easy to read and well-organized, although suggestions are given for how it could be improved.

Weaknesses:
My main concerns with the paper are a lack of context for the approach as well as missing important explanations. I also think a good amount of the math that's included could be considered ""filler"" material that could go into the appendix, since it doesn't represent new results. (I am referring to sections 4.1 and 4.2, most of which can be found in most textbooks which cover kernel methods.)

* Notation which is commonly used in the paper $\oplus$, $\otimes$, * is not explained. You should _explicitly_ define it somewhere, at least in the appendix (and refer people there). In particular, people may be confused by * for elementwise/Hadamard multiplication, since in convnet literature this is often the convolution operator. You call this the ""Star Operation"" in line 124, but I think it is just elementwise multiplication.
* The authors seem to have missed the vast literature on the connections between random features, neural networks at init, and kernel methods. (CKNs are mentioned but without any discussion of the topics I mention here.) In particular, one way that you could approximate the InfiNet architecture would be to take the two feature streams and pass them each into the same wide, random network/layer and compute the dot product of features at the next level. That would only approximate the kernel function in the InfiNet architecture, and is likely less efficient, but it provides a way to perform dimensionality expansion with a more traditional layer. The authors should discuss these connections.
* Different order of interactions have been studied in random feature and kernel settings already. In random features, interaction order is connected to the sparsity of weights, see e.g. https://arxiv.org/abs/2103.03191 and https://arxiv.org/abs/1909.02603. In kernels, this were referred to as additive kernels https://arxiv.org/abs/1602.00287, also studied in multiple kernel learning https://arxiv.org/abs/0809.1493 (these are just some examples among a larger literature).
* The authors do not seem to want to release their code. They have said ""Yes"" on Question 5, stating that the code and data are open, but there is no link or indication in the text that the code is available or will be when the paper is published. That seems deceptive.

Limitations:
I would strongly prefer that the limitations be included in the main text during the discussion. With movement of some of the standard math, there would be space.

The authors only consider the squared exponential kernel with bandwidth parameter equal to 1. Other kernels might work better. In particular, the effective dimensionality of the RKHS (related to the kernel decay rates) would be higher with a ""less smooth"" kernel like the exponential/Laplace kernel.

The results are likely not reproducible unless the authors release their code.

The results are also limited only to supervised vision tasks, rather than other modalities or unsupervised settings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a new architecture for computer vision applications that models high-order interactions between features. The architecture is similar to an attention block, but introduces an RBF Kernel layer that captures interactions of order higher than two. The resulting method has strong empirical performance across image classification tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of the paper is very interesitng and novel.
- The empirical results show promising performance across involve multiple tasks against sophisticated methods

Weaknesses:
- The presentation of the method seems overly complex in some places. For example, providing a clearer explanation of each new layer (perhaps in pseudocode) would help. While the Infiniblock definition is clear, the reader needs to go back to the previous section to understand the input/output shaped of the RBF layer, which takes work, and can be made simpler. Making clearer the intuition behind high-order interactions would be helpful as well. Showing examples of what the model learns would be helpful to make things concrete.
- The empirical performance is reasonably similar to those of previous methods, hence the empirical improvement is not that large.

Limitations:
I do not see any ethical and societal implications of the work that need to be discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper shifts the focus from traditional neural network design, which emphasizes feature representation space scaling, to feature interaction space scaling. It introduces a new model architecture, InfiNet, that enables feature interaction within an infinite-dimensional space using the RBF kernel, leading to state-of-the-art results. The paper also discusses the limitations of current models in capturing low-order interactions and proposes the use of classic kernel methods to engage features in an infinite-dimensional space.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The idea of the paper is simple, novel and well exposed. 

- The paper introduces InfiNet, a model architecture that leverages infinite-dimensional feature interactions using RBF kernels, which enhances model performance of traditional models.

- InfiNet achieves new state-of-the-art performance in various tasks, demonstrating the effectiveness of infinite-dimensional interactions.

- The paper includes extensive experiments on datasets like ImageNet and MS COCO, showing the scalability and efficiency of InfiNet.

Weaknesses:
- the paper builds on the simple use of kernel methods. The novelty of the methods is minimal, in the end it is an RBF kernel.

- the performance improvement of Infinet over other models is mostly marginal and no errors have been displayed.

- the paper doesn't really have theoretical novelty

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xNncVKbwwS;"REVIEW 
Summary:
This paper introduces methods for constrained OCO
which automatically achieve the optimal rate without knowing
in advance whether the losses are convex, strongly convex,
exp-concave, or smooth, while using only 1 projection per round.
This is notable because the standard approach proceeds by combining
several expert algorithms with a meta algorithm; in constrained settings
these expert algorithms require implementing a potentially expensive
projection. This work avoids projecting each of the expert algorithm
iterates leveraging the constrained-to-unconstrained reduction of Cutkosky 2020.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper addresses a clear and real problem that has been left unaddressed
by the majority of literature on this topic. The approach is a pretty straight-forward
modification of existing reductions, but uses them in a new and unexpected way.

Weaknesses:
The main weakness is that the paper feels poorly factored. There is
a very large number of back references to previous equations, and the paper would be
very hard to read in print. To actually follow the math, it's almost necessary to
read the paper with a pdf viewer which can display pop-up previews when hovering over links.
I think this would be remedied by better factoring the results into lemmas and propositions.

As noted, the approach is a fairly straight-forward modification of the results from
Cutkosky \& Orabona (2018) and  Cutkosky (2020), and essentially boils down to
not dropping negative terms in the analysis, and then exposing matching terms
in the regret decomposition. I think this is fine overall; these considerations
are missing from the literature, and this is a fitting
place for them to enter the literature.

Limitations:
Yes, the paper points out in the conclusion that the bounded domain / gradient assumption is
a significant limitation that they hope to address in future work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of online convex optimization with unknown smoothness properties of the loss functions, which can be convex, strongly convex, or exp-concave. The authors propose an algorithm that achieves regret bounds of order $\sqrt{T}$, $\log T$, and $d \log T$ respectively, while requiring only a single projection step per round on the original domain $\mathcal{X}$. Such projections can indeed be computationally expensive. Additionally, the authors present regret bounds with improvment for small losses.

Most algorithms that achieve similar adaptive regret upper bounds rely on meta-algorithms that combine experts (running ONS or OGD with surrogate losses), inspired by the MetaGrad algorithm. Typically, these algorithms necessitate $\log(T)$ projection steps per round (one per expert), which can be computationally burdensome. Mhammedi et al. (2019) reduced this projection cost to $O(1)$ but at the expense of a $d \log T$ regret for strongly convex losses. To overcome this, the authors introduce new surrogate losses based on a black-box reduction technique by Cutkosky et al. (2018), which simplifies the constrained optimization problem on $\mathcal{X}$ to another domain, such as the Euclidean ball, where projections are easier.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-written and offers valuable insights into the use of surrogate losses to adapt to strong convexity or exp-concavity. It may serve as a comprehensive entry point into the extensive literature on universal OCO algorithms.
- Despite combining various existing techniques, the results are non-trivial and required solving technical challenges, especially for the strongly convex case. The authors introduce novel negative terms in the analysis to achieve their results.
- Experiments included in the appendix demonstrate that the computational improvements can be significant.

Weaknesses:
- The theoretical improvements may appear incremental, appealing to a niche audience interested. The improvement being only in the specific case of strongly convex $\log T$ regret with $O(1)$ projection steps. The primary high-level ideas in the algorithm and analysis are based on prior work.
- The paper still relies on a meta-aggregation procedure, which, although theoretically effective, is not particularly elegant and maintains a per-round complexity of order $O(\log T)$. Achieving $O(1)$ complexity per round seems however highly challenging.
- The convex rate is actually $O(\sqrt{T \log\log T})$, not $O(\sqrt{T})$ as stated in the results.

Limitations:
The authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies universal OCO algorithms with fewer projections. Previous work either use $O(\log T)$ projections per round, or have a sub-optimal dependence on $d$ for strongly-convex loss. This work designs a new surrogate loss to achieve tight regret for Lipschitz convex/exp-concave/strongly-convex losses simultaneously, with only 1 projection per round.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The technical contributions are solid: this paper makes a strict improvement over previous results.

The paper is very well-written, clearly introducing the challenges and the main ideas. Details of the analysis and algorithm are nicely explained.

Weaknesses:
The contribution seems somewhat incremental to me. The only improvement is a $d$ factor for strongly-convex loss. Such result is nice to know but I'm not sure how significant such it is. In addition, the technical novelty isn't significant either.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies universal OCO algorithms with fewer projections. Previous work either use $O(\log T)$ projections per round, or have a sub-optimal dependence on $d$ for strongly-convex loss. This work designs a new surrogate loss to achieve tight regret for Lipschitz convex/exp-concave/strongly-convex losses simultaneously, with only 1 projection per round.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The technical contributions are solid: this paper makes a strict improvement over previous results.

The paper is very well-written, clearly introducing the challenges and the main ideas. Details of the analysis and algorithm are nicely explained.

Weaknesses:
The contribution seems somewhat incremental to me. The only improvement is a $d$ factor for strongly-convex loss. Such result is nice to know but I'm not sure how significant such it is. In addition, the technical novelty isn't significant either.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces methods for constrained OCO
which automatically achieve the optimal rate without knowing
in advance whether the losses are convex, strongly convex,
exp-concave, or smooth, while using only 1 projection per round.
This is notable because the standard approach proceeds by combining
several expert algorithms with a meta algorithm; in constrained settings
these expert algorithms require implementing a potentially expensive
projection. This work avoids projecting each of the expert algorithm
iterates leveraging the constrained-to-unconstrained reduction of Cutkosky 2020.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper addresses a clear and real problem that has been left unaddressed
by the majority of literature on this topic. The approach is a pretty straight-forward
modification of existing reductions, but uses them in a new and unexpected way.

Weaknesses:
The main weakness is that the paper feels poorly factored. There is
a very large number of back references to previous equations, and the paper would be
very hard to read in print. To actually follow the math, it's almost necessary to
read the paper with a pdf viewer which can display pop-up previews when hovering over links.
I think this would be remedied by better factoring the results into lemmas and propositions.

As noted, the approach is a fairly straight-forward modification of the results from
Cutkosky \& Orabona (2018) and  Cutkosky (2020), and essentially boils down to
not dropping negative terms in the analysis, and then exposing matching terms
in the regret decomposition. I think this is fine overall; these considerations
are missing from the literature, and this is a fitting
place for them to enter the literature.

Limitations:
Yes, the paper points out in the conclusion that the bounded domain / gradient assumption is
a significant limitation that they hope to address in future work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of online convex optimization with unknown smoothness properties of the loss functions, which can be convex, strongly convex, or exp-concave. The authors propose an algorithm that achieves regret bounds of order $\sqrt{T}$, $\log T$, and $d \log T$ respectively, while requiring only a single projection step per round on the original domain $\mathcal{X}$. Such projections can indeed be computationally expensive. Additionally, the authors present regret bounds with improvment for small losses.

Most algorithms that achieve similar adaptive regret upper bounds rely on meta-algorithms that combine experts (running ONS or OGD with surrogate losses), inspired by the MetaGrad algorithm. Typically, these algorithms necessitate $\log(T)$ projection steps per round (one per expert), which can be computationally burdensome. Mhammedi et al. (2019) reduced this projection cost to $O(1)$ but at the expense of a $d \log T$ regret for strongly convex losses. To overcome this, the authors introduce new surrogate losses based on a black-box reduction technique by Cutkosky et al. (2018), which simplifies the constrained optimization problem on $\mathcal{X}$ to another domain, such as the Euclidean ball, where projections are easier.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-written and offers valuable insights into the use of surrogate losses to adapt to strong convexity or exp-concavity. It may serve as a comprehensive entry point into the extensive literature on universal OCO algorithms.
- Despite combining various existing techniques, the results are non-trivial and required solving technical challenges, especially for the strongly convex case. The authors introduce novel negative terms in the analysis to achieve their results.
- Experiments included in the appendix demonstrate that the computational improvements can be significant.

Weaknesses:
- The theoretical improvements may appear incremental, appealing to a niche audience interested. The improvement being only in the specific case of strongly convex $\log T$ regret with $O(1)$ projection steps. The primary high-level ideas in the algorithm and analysis are based on prior work.
- The paper still relies on a meta-aggregation procedure, which, although theoretically effective, is not particularly elegant and maintains a per-round complexity of order $O(\log T)$. Achieving $O(1)$ complexity per round seems however highly challenging.
- The convex rate is actually $O(\sqrt{T \log\log T})$, not $O(\sqrt{T})$ as stated in the results.

Limitations:
The authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xNZEjFe0mh;"REVIEW 
Summary:
This work introduces three algorithms for communication-efficient Federated Group Distributionally Robust Optimization. The effectiveness of the proposed algorithms are verified through both theoretical and experimental results.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1) This work studies an important problem of federated group distributionally robust optimization.
2) The theoretical results show the advantages of the proposed algorithms.

Weaknesses:
1) This work proposes three algorithms, including FGDRO-CVaR, FGDRO-KL, and FGDRO-KL-Adam. There lacks a comparison between these algorithms. For example, what are the connections and differences between these algorithms?
2) The analysis for FGDRO-CVaR assumes the loss function to be rho-weakly convex, which is missing from the main context.

Limitations:
See weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of reducing communication costs and sample complexity in Federated Group Distributionally Robust Optimization (FGDRO). The authors present the FGDRO-CVaR algorithm and the FGDRO-KL algorithm to address different constraints. Subsequently, they conduct extensive experiments across various real-world tasks, including NLP and CV tasks. The corresponding empirical results confirm the effectiveness of their proposed methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The exploration of reducing communication costs for federated group DRO is a rarely-studied topic within the FL community.

2. The theoretical convergence analysis for the proposed algorithms is somewhat solid.

3. The authors conduct comprehensive experiments to validate the effectiveness of the devised algorithms.

Weaknesses:
1. The contributions and novelties of this paper are unclear. It appears that the authors have directly combined existing federated adaptive algorithms with pre-existing federated group DRO methods in this paper.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents three methods for Federated Learning Group Distributionally Robust Optimization: (i) one tailored to reduce the CVaR which optimizes the top K-losses, (ii) another one tailored to tackle the KL divergence, and finally (iii) one that uses Adam locally. The paper is well written and the ideas are presented. To the best of my knowledge, the proofs are correct. My main concerns are regarding the relevance and importance of the subject, the lack of experiments, and the lack of empirical studies on communication efficiency.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
[S1] The paper is well-written, and the ideas are presented. 

[S2] The theoretical results are correct, to the best of my knowledge.

Weaknesses:
[W1] The relevance of the subject is not entirely addressed. See [Q1]

[W2] The experiment section is limited. In particular, the paper does not present any intuition on the problems they are solving. They do not consider the number of samples per server for example. I believe the authors should include a class imbalance problem [AN AGNOSTIC APPROACH TO FEDERATED LEARNING WITH CLASS IMBALANCE - Shen et al, ICLR 22].

[W3] Communication efficiency is not properly addressed by the authors. The authors show the number of communication rounds required, but they do not take into account how much is communicated. The authors claim that this method is more efficient in terms of communication, and they show it theoretically, but in the experiment section, there is no evidence of communication efficiency. I suggest the authors reveal the communication cost associated with each method, measured in the amount of data shared between servers. 

[W4] Privacy is an important subject of Federated Learning, but in this paper, there is no analysis of the privacy aspect. Can the authors elaborate on the privacy aspect of this work? 

[W5] Federated learning is a technique used to train on a set of machines. The idea is that the number of machines that participate is large. It appears to me that the largest number of servers is 17. This seems to me insufficient for a distributed learning problem.

Limitations:
See weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to improve the efficiency of existing federated group distributionally robust optimization (FGDRO) when considering two specific types of regularization, condition value at risk and KL divergence. To address the first type of problem, the authors propose FGDRO-CVaR that reduces the sample complexity and communication costs simultaneously. For KL conditions, the proposed FGDRO-KL reduces the sample complexity while retaining the same communication costs. Moreover, the authors integrate the notion of Adam into FGDRO-KL, yielding FGDRO-KL-Adam and achieving better convergence speed.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper is well-written, though some background information is missing.
2. The problem is well-motivated. The sample and communication efficiency is a pivotal problem in federated learning, though the benefits are not fully analyzed in the experiments.
3. The proposed method is grounded and improves over prior baselines.

Weaknesses:
1. The background can be more thoroughly explained. The authors are encouraged to provide additional context to address the following questions, which will greatly enhance the paper's completeness. Why is federated group distributionally robust optimization (FGDRO) an important problem or technique? What are the sources of the additional communication costs? Why is it necessary to consider two different types of regularization? Are these types of regularization relevant to different applications?

2. My major concerns lie in the experiments and their settings. 

    - **Data Splits.** While FGDRO's main advantage appears to be its ability to address non-IID optimization, the experimental setup concerning data splits lacks clarity. An analysis of the non-IID levels, such as those derived from different Dirichlet-distributed data splits with varying $\lambda$ values, is missing. Including more representative baselines, such as SCAFFOLD and FedProx, which are also designed for non-IID optimization, could further enhance the analyses.

    - **Performance.** The proposed method performs similarly to the baselines in most experiments. For example, in Tables 2 and 3, apart from the Adam variant, the proposed method is comparable to the baselines. This would be acceptable if the proposed method demonstrated improved efficiency; however, relevant analyses on this aspect are absent from the experiments.

    - **Communication or Sample Complexity Analysis.** An empirical analysis comparing complexity versus utility would be beneficial and highlight the advantages of the proposed method. For instance, the experiment in Figure 1 can be extended to a comparison among different baselines.

Limitations:
Yes, the authors have discussed the limitations in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work introduces three algorithms for communication-efficient Federated Group Distributionally Robust Optimization. The effectiveness of the proposed algorithms are verified through both theoretical and experimental results.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1) This work studies an important problem of federated group distributionally robust optimization.
2) The theoretical results show the advantages of the proposed algorithms.

Weaknesses:
1) This work proposes three algorithms, including FGDRO-CVaR, FGDRO-KL, and FGDRO-KL-Adam. There lacks a comparison between these algorithms. For example, what are the connections and differences between these algorithms?
2) The analysis for FGDRO-CVaR assumes the loss function to be rho-weakly convex, which is missing from the main context.

Limitations:
See weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of reducing communication costs and sample complexity in Federated Group Distributionally Robust Optimization (FGDRO). The authors present the FGDRO-CVaR algorithm and the FGDRO-KL algorithm to address different constraints. Subsequently, they conduct extensive experiments across various real-world tasks, including NLP and CV tasks. The corresponding empirical results confirm the effectiveness of their proposed methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The exploration of reducing communication costs for federated group DRO is a rarely-studied topic within the FL community.

2. The theoretical convergence analysis for the proposed algorithms is somewhat solid.

3. The authors conduct comprehensive experiments to validate the effectiveness of the devised algorithms.

Weaknesses:
1. The contributions and novelties of this paper are unclear. It appears that the authors have directly combined existing federated adaptive algorithms with pre-existing federated group DRO methods in this paper.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents three methods for Federated Learning Group Distributionally Robust Optimization: (i) one tailored to reduce the CVaR which optimizes the top K-losses, (ii) another one tailored to tackle the KL divergence, and finally (iii) one that uses Adam locally. The paper is well written and the ideas are presented. To the best of my knowledge, the proofs are correct. My main concerns are regarding the relevance and importance of the subject, the lack of experiments, and the lack of empirical studies on communication efficiency.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
[S1] The paper is well-written, and the ideas are presented. 

[S2] The theoretical results are correct, to the best of my knowledge.

Weaknesses:
[W1] The relevance of the subject is not entirely addressed. See [Q1]

[W2] The experiment section is limited. In particular, the paper does not present any intuition on the problems they are solving. They do not consider the number of samples per server for example. I believe the authors should include a class imbalance problem [AN AGNOSTIC APPROACH TO FEDERATED LEARNING WITH CLASS IMBALANCE - Shen et al, ICLR 22].

[W3] Communication efficiency is not properly addressed by the authors. The authors show the number of communication rounds required, but they do not take into account how much is communicated. The authors claim that this method is more efficient in terms of communication, and they show it theoretically, but in the experiment section, there is no evidence of communication efficiency. I suggest the authors reveal the communication cost associated with each method, measured in the amount of data shared between servers. 

[W4] Privacy is an important subject of Federated Learning, but in this paper, there is no analysis of the privacy aspect. Can the authors elaborate on the privacy aspect of this work? 

[W5] Federated learning is a technique used to train on a set of machines. The idea is that the number of machines that participate is large. It appears to me that the largest number of servers is 17. This seems to me insufficient for a distributed learning problem.

Limitations:
See weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to improve the efficiency of existing federated group distributionally robust optimization (FGDRO) when considering two specific types of regularization, condition value at risk and KL divergence. To address the first type of problem, the authors propose FGDRO-CVaR that reduces the sample complexity and communication costs simultaneously. For KL conditions, the proposed FGDRO-KL reduces the sample complexity while retaining the same communication costs. Moreover, the authors integrate the notion of Adam into FGDRO-KL, yielding FGDRO-KL-Adam and achieving better convergence speed.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper is well-written, though some background information is missing.
2. The problem is well-motivated. The sample and communication efficiency is a pivotal problem in federated learning, though the benefits are not fully analyzed in the experiments.
3. The proposed method is grounded and improves over prior baselines.

Weaknesses:
1. The background can be more thoroughly explained. The authors are encouraged to provide additional context to address the following questions, which will greatly enhance the paper's completeness. Why is federated group distributionally robust optimization (FGDRO) an important problem or technique? What are the sources of the additional communication costs? Why is it necessary to consider two different types of regularization? Are these types of regularization relevant to different applications?

2. My major concerns lie in the experiments and their settings. 

    - **Data Splits.** While FGDRO's main advantage appears to be its ability to address non-IID optimization, the experimental setup concerning data splits lacks clarity. An analysis of the non-IID levels, such as those derived from different Dirichlet-distributed data splits with varying $\lambda$ values, is missing. Including more representative baselines, such as SCAFFOLD and FedProx, which are also designed for non-IID optimization, could further enhance the analyses.

    - **Performance.** The proposed method performs similarly to the baselines in most experiments. For example, in Tables 2 and 3, apart from the Adam variant, the proposed method is comparable to the baselines. This would be acceptable if the proposed method demonstrated improved efficiency; however, relevant analyses on this aspect are absent from the experiments.

    - **Communication or Sample Complexity Analysis.** An empirical analysis comparing complexity versus utility would be beneficial and highlight the advantages of the proposed method. For instance, the experiment in Figure 1 can be extended to a comparison among different baselines.

Limitations:
Yes, the authors have discussed the limitations in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xM5m7J6Lbl;"REVIEW 
Summary:
This paper defines social markov decision processes (SMDPs) as an MDP generalization incorporating a population of individuals with distinct utility profiles aggregated by a social welfare function. It provides a novel quantitative definition of alignment in this context, then leverages this definition to characterize probably approximately aligned policies and safe policies, prove the conditions under which they exist, and relate them to the accuracy of the reward model.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper is well written, and the background is particularly clear.
2. The definitions and theoretical results are thorough and rigorous. This paper precisely relates the probability of aligned behavior to the world model accuracy, which I believe is valuable.
3. This paper acknowledges that realistic inaccuracy in the world model could cause intolerable uncertainty in the PAA policy, and shows a more practical approach (safeguarding a black-box policy).

Weaknesses:
Even the more practical approach of safeguarding a black-box policy may have severe limitations. I believe the paper would be strengthened by a discussion of the feasibility of this -- in particular, what is computational complexity of computing $\mathcal{A}_{safe}$ for a SMDP?

Typo: On line 277, I believe ""expansive"" should be ""expensive"".

Limitations:
This paper includes an excellent discussion of the limitations of these results, including the theoretical conditions under which PAA and safe policies will be unreliable. The paper also discusses further practical and philosophical limitations in Section 5.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper applies ideas from the Probably Approximately Correct framework to agent alignment. The paper defines a new idea of a policy which is Probably Approximately Aligned and explores the existence of such policies under certain assumptions of social welfare and models of the world. The authors show that probably approximately aligned (and approximately aligned) policies exist when there is a sufficiently accurate world model. However, to compute this policy is quite expensive. Thus, the authors also develop the idea of a safe policy which can be derived using a PAA policy and seems to be a policy that will probably not result in a catastrophically bad state.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper appears to be a very reasonable application of a well established form of analysis into a novel domain.

The main idea of providing bounds for the quality of an agent's policy is very important and will likely be the focus of much work in the near future. This is quite useful work and appears to me as the potential basis for work that can eventually have significant beneficial impact on the world.

The paper is generally well written and the motivation is clear. In places the math is a little dense but it seems to be as approachable as it can be for this sort of analysis. I do certainly appreciate that you've put a moderate amount of the work into the actual paper rather than stuffing all the important stuff into the appendix.

Weaknesses:
Not a weakness, but my disclaimer: I was not able to thoroughly review every detail of the math due to time constraints so my understanding of the paper is limited.

The primary (and minor) issue I see with the paper is that it is quite abstract and doesn't give a clear idea of how close this is to being useful. While obviously difficult to fit into a conference paper, an experimental section may give some intuition for details such as how accurate a world model really needs to be, how beneficial PAA/safe policies are, etc.

It seems that Sec 3.2 is constructive in a sense and provides a PAA policy. Some further commentary on the practicality of this policy (is it entirely impractical to use it for synthetic experiments, or simply impractical in any useful setting/world model?) would help to contextualize the paper.

Limitations:
Limitations are well stated.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper aims to define alignment quantitatively and ensure AI agents' actions are predictable and safe. The paper start by outlines the basics of utility and social choice theory, focusing on quantifying social satisfaction and the conditions under which it is measurable. Next, the paper defines probably approximately aligned (PAA) and approximately aligned (AA) policies and provides a modified sparse sampling algorithm to achieve these policies under certain conditions. The paper also presents the idea of ""safe policies"" and a method to ensure AI actions are verifiably safe for society.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Originality: This paper introduces a novel, quantitative definition of alignment in social decision-making contexts, drawing from utility and social choice theory.

- Quality: The paper primarily focuses on theoretical contributions rather than empirical experiments. It is well-structured.

- Clarity: The paper provides detailed mathematical derivations and proofs to support the existence of PAA and safe policies. It includes extensive references and context, including foundational works in utility theory, social choice, AI safety, and reinforcement learning, emphasizing the interdisciplinary nature of aligning AI with human values.

- Significance: This work has a significant impact. While primarily theoretical, the work aims to provide a foundation for developing AI systems that could be safely used in critical applications like social governance, policy-making, or resource allocation.

Weaknesses:
The safeguarding method is described in a general context, with limited discussion of its applicability to specific real-world problems. Consider adding examples of real-world applications where the safeguarding method could be particularly beneficial. For instance, discuss its application in autonomous vehicle systems, healthcare decision-making, or financial trading algorithms.

Limitations:
The authors discuss various limitations of their approach, including computational complexity for large state spaces and strong assumptions about the availability and accuracy of information. The paper also highlights challenges in building reliable world models and the philosophical questions surrounding the informational basis of utilities.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the potential for AI agents to safely make critical decisions, such as those in a government setting, by examining the concept of alignment. It introduces Probably Approximately Aligned (PAA) policies, which are policies that are nearly optimal in aligning with social welfare objectives. The authors draw from utility and social choice theories to provide a quantitative definition of alignment and propose methods to ensure AI actions are verifiably safe for society. They also discuss the practical challenges in implementing such policies and suggest future directions for research in this area. The focus is on developing a theoretical framework that could eventually be applied to AI governance and decision-making processes.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The authors draw from utility and social choice theories to provide a quantitative definition of alignment and propose methods to ensure AI actions are verifiably safe for society.

Weaknesses:
I think the problem is not well presented.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the potential for AI agents to safely make critical decisions, such as those in a government setting, by examining the concept of alignment. It introduces Probably Approximately Aligned (PAA) policies, which are policies that are nearly optimal in aligning with social welfare objectives. The authors draw from utility and social choice theories to provide a quantitative definition of alignment and propose methods to ensure AI actions are verifiably safe for society. They also discuss the practical challenges in implementing such policies and suggest future directions for research in this area. The focus is on developing a theoretical framework that could eventually be applied to AI governance and decision-making processes.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The authors draw from utility and social choice theories to provide a quantitative definition of alignment and propose methods to ensure AI actions are verifiably safe for society.

Weaknesses:
I think the problem is not well presented.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper defines social markov decision processes (SMDPs) as an MDP generalization incorporating a population of individuals with distinct utility profiles aggregated by a social welfare function. It provides a novel quantitative definition of alignment in this context, then leverages this definition to characterize probably approximately aligned policies and safe policies, prove the conditions under which they exist, and relate them to the accuracy of the reward model.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper is well written, and the background is particularly clear.
2. The definitions and theoretical results are thorough and rigorous. This paper precisely relates the probability of aligned behavior to the world model accuracy, which I believe is valuable.
3. This paper acknowledges that realistic inaccuracy in the world model could cause intolerable uncertainty in the PAA policy, and shows a more practical approach (safeguarding a black-box policy).

Weaknesses:
Even the more practical approach of safeguarding a black-box policy may have severe limitations. I believe the paper would be strengthened by a discussion of the feasibility of this -- in particular, what is computational complexity of computing $\mathcal{A}_{safe}$ for a SMDP?

Typo: On line 277, I believe ""expansive"" should be ""expensive"".

Limitations:
This paper includes an excellent discussion of the limitations of these results, including the theoretical conditions under which PAA and safe policies will be unreliable. The paper also discusses further practical and philosophical limitations in Section 5.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper applies ideas from the Probably Approximately Correct framework to agent alignment. The paper defines a new idea of a policy which is Probably Approximately Aligned and explores the existence of such policies under certain assumptions of social welfare and models of the world. The authors show that probably approximately aligned (and approximately aligned) policies exist when there is a sufficiently accurate world model. However, to compute this policy is quite expensive. Thus, the authors also develop the idea of a safe policy which can be derived using a PAA policy and seems to be a policy that will probably not result in a catastrophically bad state.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper appears to be a very reasonable application of a well established form of analysis into a novel domain.

The main idea of providing bounds for the quality of an agent's policy is very important and will likely be the focus of much work in the near future. This is quite useful work and appears to me as the potential basis for work that can eventually have significant beneficial impact on the world.

The paper is generally well written and the motivation is clear. In places the math is a little dense but it seems to be as approachable as it can be for this sort of analysis. I do certainly appreciate that you've put a moderate amount of the work into the actual paper rather than stuffing all the important stuff into the appendix.

Weaknesses:
Not a weakness, but my disclaimer: I was not able to thoroughly review every detail of the math due to time constraints so my understanding of the paper is limited.

The primary (and minor) issue I see with the paper is that it is quite abstract and doesn't give a clear idea of how close this is to being useful. While obviously difficult to fit into a conference paper, an experimental section may give some intuition for details such as how accurate a world model really needs to be, how beneficial PAA/safe policies are, etc.

It seems that Sec 3.2 is constructive in a sense and provides a PAA policy. Some further commentary on the practicality of this policy (is it entirely impractical to use it for synthetic experiments, or simply impractical in any useful setting/world model?) would help to contextualize the paper.

Limitations:
Limitations are well stated.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper aims to define alignment quantitatively and ensure AI agents' actions are predictable and safe. The paper start by outlines the basics of utility and social choice theory, focusing on quantifying social satisfaction and the conditions under which it is measurable. Next, the paper defines probably approximately aligned (PAA) and approximately aligned (AA) policies and provides a modified sparse sampling algorithm to achieve these policies under certain conditions. The paper also presents the idea of ""safe policies"" and a method to ensure AI actions are verifiably safe for society.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Originality: This paper introduces a novel, quantitative definition of alignment in social decision-making contexts, drawing from utility and social choice theory.

- Quality: The paper primarily focuses on theoretical contributions rather than empirical experiments. It is well-structured.

- Clarity: The paper provides detailed mathematical derivations and proofs to support the existence of PAA and safe policies. It includes extensive references and context, including foundational works in utility theory, social choice, AI safety, and reinforcement learning, emphasizing the interdisciplinary nature of aligning AI with human values.

- Significance: This work has a significant impact. While primarily theoretical, the work aims to provide a foundation for developing AI systems that could be safely used in critical applications like social governance, policy-making, or resource allocation.

Weaknesses:
The safeguarding method is described in a general context, with limited discussion of its applicability to specific real-world problems. Consider adding examples of real-world applications where the safeguarding method could be particularly beneficial. For instance, discuss its application in autonomous vehicle systems, healthcare decision-making, or financial trading algorithms.

Limitations:
The authors discuss various limitations of their approach, including computational complexity for large state spaces and strong assumptions about the availability and accuracy of information. The paper also highlights challenges in building reliable world models and the philosophical questions surrounding the informational basis of utilities.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xL7Ve14AHA;"REVIEW 
Summary:
This article proposes an optimization algorithm RAMDA for training structured neural networks, which combines a number of optimization techniques including dual averaging, momentum, and coordinate-wise preconditioners. Similar to the existing RMDA algorithm, RAMDA also has the capacity to identify the local manifold structure of the solution. The author(s) provide theoretical analyses to justify the convergence property of RAMDA, and develop an inexact subproblem solver as required by RAMDA.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed RAMDA algorithm extends the existing RMDA algorithm by adding a coordinate-wise preconditioner, and its theoretical analysis seems to be novel.

Weaknesses:
I think one major weakness of the current manuscript is the **correctness** of some theoretical results presented in the article.

1. Theorem 1 suggests that the regularizer function $\psi$ can be nonconvex. However, as the RAMDA algorithm heavily relies on the proximal operator of $\psi$, how do you define the proximal operator when $\psi$ is nonconvex? For example, equation (6) is used to define the new iterate $W^t$, but when $\psi$ is nonconvex, it is likely that the ""argmin"" is a set and is not uniquely defined.

2. Taking a closer look at the proof of Theorem 1, I feel that the author(s) may have a misunderstanding of an existing theorem. In Appendix B, equation (11) is obtained by citing Theorem 10.15 of [1]. However, Theorem 10.15 of [1] applies to functions of the form $F(x)=f(x)+\psi(x)$, where $f$ is smooth and nonconvex, but $\psi$ is convex. In other words, the non-convexity only applies to the smooth part, not the regularizer.

3. If the findings above are valid, then the author(s) may need a thorough examination of the technical proofs to see if there is any error.

4. If we assume $\psi$ is convex, then there should be an Nesterov-accelerated version of Algorithm 2 that converges in $O(\varepsilon_t^{-1/2})$ iterations, which is faster than the rate given in Theorem 1.

[1] Beck, A. (2017). First-order methods in optimization. Society for Industrial and Applied Mathematics.


===============================================================

Edit: during the rebuttal the author(s) seem to have addressed the concerns above.

Limitations:
The authors have discussed the limitations of the proposed algorithm in Section 5 and Appendix C.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper develops regularized adaptive momentum dual averaging (RAMDA) for structured neural networks. The method uses the preconditioning matrix to accelerate the convergence of a regularized momentum dual averaging (RMDA) method at the price of requiring the local solver (e.g. standard proximal gradient methods) to solve the subproblem. By the preconditioning matrix inspired by the AdaGrad stepsizes in Eq. (2), RAMDA outperforms RMDA and other existing gradient-based methods for solving structured neural networks in various learning applications.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Theoretical results suggest the convergence towards the solution to the subproblem when the proximal gradient methods are used as the local solvers, and almost surely convergence of RAMDA that derives from the manifold theory under the standard $L$-smoothness assumption on the objective functions $f$.   

2. Empirical results illustrate the superior performance of RAMDA over RMDA and other existing gradient-based methods for various neural network tasks. Clear criteria, e.g. for solving the subproblems, are clearly stated in the numerical experiments.

Weaknesses:
1. I think there is an error in Eq. (3) where it should be the square root $\sqrt{\cdot}$ in the diagonal operator for $P^t$. This is because $P^t$ uses $U^t$ that is computed from the element-wise multiplicative product of the gradient $G^t$. Is $P^t$ inspired by the AdaGrad stepsizes? If so, then adding the justifications on using $P^t$ in RAMDA is worthwhile to better distinct RAMDA from RMDA. 
2. In the experiments, can you comment on the impact of the different $\epsilon_t$ on the training performance of RAMDA? Because I believe that using $\epsilon_t$ a bit higher than $10^{-8}$ set in your experiments RAMDA might achieve far lower training time than other methods while keeping still comparable perplexity to solve Transformer-XL with WikiText-103 in Table 4, or Tacotron2 with LJSpeech in Table 5.

Limitations:
The limitations of current theoretical results are clearly and fairly discussed after Theorem 2.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
#### Summary
The paper introduces the Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. RAMDA addresses the challenge of solving the subproblem involved in the regularized adaptive methods, which typically lacks a closed-form solution. The paper presents an inexactness condition that retains convergence guarantees and proposes an efficient subproblem solver. The algorithm leverages manifold identification theory to ensure that the iterates of RAMDA attain the ideal structure induced by the regularizer at convergence. Extensive experiments demonstrate the effectiveness of RAMDA in various tasks, including computer vision, language modeling, and speech synthesis.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
#### Strengths
1. **Novel Algorithm**: RAMDA combines adaptive momentum dual averaging with efficient inexact subproblem solving, providing a practical and theoretically sound method for training structured neural networks.
2. **Theoretical Guarantees**: The paper provides strong theoretical support, including convergence guarantees and structure identification, ensuring the algorithm's robustness.
3. **Practical Efficiency**: The proposed inexact subproblem solver is efficient, making RAMDA feasible for large-scale applications.
4. **Empirical Validation**: Extensive experiments across multiple domains demonstrate the superior performance of RAMDA in terms of both prediction accuracy and structured sparsity.

Weaknesses:
#### Weaknesses
1. **Computational Complexity**: The computational complexity of the proposed subproblem solver, especially for high-dimensional data, needs more detailed discussion.
2. **Generality**: While the paper focuses on specific types of structured neural networks, extending the methodology to other models and regularizers would enhance its generality.
3. **Comparative Analysis**: More detailed comparisons with other state-of-the-art methods, beyond the provided benchmarks, would strengthen the empirical validation.
4. **Implementation Details**: Practical guidelines for implementing RAMDA, including parameter tuning and handling different data distributions, are somewhat lacking.

Limitations:
no

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This article proposes an optimization algorithm RAMDA for training structured neural networks, which combines a number of optimization techniques including dual averaging, momentum, and coordinate-wise preconditioners. Similar to the existing RMDA algorithm, RAMDA also has the capacity to identify the local manifold structure of the solution. The author(s) provide theoretical analyses to justify the convergence property of RAMDA, and develop an inexact subproblem solver as required by RAMDA.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed RAMDA algorithm extends the existing RMDA algorithm by adding a coordinate-wise preconditioner, and its theoretical analysis seems to be novel.

Weaknesses:
I think one major weakness of the current manuscript is the **correctness** of some theoretical results presented in the article.

1. Theorem 1 suggests that the regularizer function $\psi$ can be nonconvex. However, as the RAMDA algorithm heavily relies on the proximal operator of $\psi$, how do you define the proximal operator when $\psi$ is nonconvex? For example, equation (6) is used to define the new iterate $W^t$, but when $\psi$ is nonconvex, it is likely that the ""argmin"" is a set and is not uniquely defined.

2. Taking a closer look at the proof of Theorem 1, I feel that the author(s) may have a misunderstanding of an existing theorem. In Appendix B, equation (11) is obtained by citing Theorem 10.15 of [1]. However, Theorem 10.15 of [1] applies to functions of the form $F(x)=f(x)+\psi(x)$, where $f$ is smooth and nonconvex, but $\psi$ is convex. In other words, the non-convexity only applies to the smooth part, not the regularizer.

3. If the findings above are valid, then the author(s) may need a thorough examination of the technical proofs to see if there is any error.

4. If we assume $\psi$ is convex, then there should be an Nesterov-accelerated version of Algorithm 2 that converges in $O(\varepsilon_t^{-1/2})$ iterations, which is faster than the rate given in Theorem 1.

[1] Beck, A. (2017). First-order methods in optimization. Society for Industrial and Applied Mathematics.


===============================================================

Edit: during the rebuttal the author(s) seem to have addressed the concerns above.

Limitations:
The authors have discussed the limitations of the proposed algorithm in Section 5 and Appendix C.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper develops regularized adaptive momentum dual averaging (RAMDA) for structured neural networks. The method uses the preconditioning matrix to accelerate the convergence of a regularized momentum dual averaging (RMDA) method at the price of requiring the local solver (e.g. standard proximal gradient methods) to solve the subproblem. By the preconditioning matrix inspired by the AdaGrad stepsizes in Eq. (2), RAMDA outperforms RMDA and other existing gradient-based methods for solving structured neural networks in various learning applications.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Theoretical results suggest the convergence towards the solution to the subproblem when the proximal gradient methods are used as the local solvers, and almost surely convergence of RAMDA that derives from the manifold theory under the standard $L$-smoothness assumption on the objective functions $f$.   

2. Empirical results illustrate the superior performance of RAMDA over RMDA and other existing gradient-based methods for various neural network tasks. Clear criteria, e.g. for solving the subproblems, are clearly stated in the numerical experiments.

Weaknesses:
1. I think there is an error in Eq. (3) where it should be the square root $\sqrt{\cdot}$ in the diagonal operator for $P^t$. This is because $P^t$ uses $U^t$ that is computed from the element-wise multiplicative product of the gradient $G^t$. Is $P^t$ inspired by the AdaGrad stepsizes? If so, then adding the justifications on using $P^t$ in RAMDA is worthwhile to better distinct RAMDA from RMDA. 
2. In the experiments, can you comment on the impact of the different $\epsilon_t$ on the training performance of RAMDA? Because I believe that using $\epsilon_t$ a bit higher than $10^{-8}$ set in your experiments RAMDA might achieve far lower training time than other methods while keeping still comparable perplexity to solve Transformer-XL with WikiText-103 in Table 4, or Tacotron2 with LJSpeech in Table 5.

Limitations:
The limitations of current theoretical results are clearly and fairly discussed after Theorem 2.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
#### Summary
The paper introduces the Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. RAMDA addresses the challenge of solving the subproblem involved in the regularized adaptive methods, which typically lacks a closed-form solution. The paper presents an inexactness condition that retains convergence guarantees and proposes an efficient subproblem solver. The algorithm leverages manifold identification theory to ensure that the iterates of RAMDA attain the ideal structure induced by the regularizer at convergence. Extensive experiments demonstrate the effectiveness of RAMDA in various tasks, including computer vision, language modeling, and speech synthesis.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
#### Strengths
1. **Novel Algorithm**: RAMDA combines adaptive momentum dual averaging with efficient inexact subproblem solving, providing a practical and theoretically sound method for training structured neural networks.
2. **Theoretical Guarantees**: The paper provides strong theoretical support, including convergence guarantees and structure identification, ensuring the algorithm's robustness.
3. **Practical Efficiency**: The proposed inexact subproblem solver is efficient, making RAMDA feasible for large-scale applications.
4. **Empirical Validation**: Extensive experiments across multiple domains demonstrate the superior performance of RAMDA in terms of both prediction accuracy and structured sparsity.

Weaknesses:
#### Weaknesses
1. **Computational Complexity**: The computational complexity of the proposed subproblem solver, especially for high-dimensional data, needs more detailed discussion.
2. **Generality**: While the paper focuses on specific types of structured neural networks, extending the methodology to other models and regularizers would enhance its generality.
3. **Comparative Analysis**: More detailed comparisons with other state-of-the-art methods, beyond the provided benchmarks, would strengthen the empirical validation.
4. **Implementation Details**: Practical guidelines for implementing RAMDA, including parameter tuning and handling different data distributions, are somewhat lacking.

Limitations:
no

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xImeJtdUiw;"REVIEW 
Summary:
The paper introduces a novel multi-modal model, IsoFormer, designed to integrate DNA, RNA, and protein sequences for predicting RNA transcript isoform expression across different tissues. It utilizes pre-trained modality-specific encoders to generate embeddings that are then combined using a sophisticated aggregation method. The model demonstrates significant improvements in prediction accuracy compared to single-modality approaches.

Contriburion:

1. Developed the first general-purpose multi-modal model integrating DNA, RNA, and protein sequences.

2. Demonstrated successful application of transfer learning from modality-specific encoders.

3. Provided a new robust framework for advancing the prediction of RNA transcript isoform expression.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Innovative Integration of Modalities: The paper presents the first attempt to integrate three biological sequence modalities (DNA, RNA, and proteins) in a unified model, providing a comprehensive approach reflective of natural biological processes.

2. Effective Transfer Learning: IsoFormer effectively leverages pre-trained encoders to enhance its predictive power, benefiting from both intra-modal and inter-modal transfer learning.

3. Robust Evaluation: Experiments demonstrate the model's capability, outperforming existing methods in predicting transcript isoform expression, which is a challenging task due to its multi-modal nature.

Weaknesses:
1. Complexity and Computation: The model's complexity and the computational demands might limit its accessibility and use, particularly in environments with restricted resources.

2. More Comprehensive Evaluation for PLM's representation learning capability would make this paper better.

Limitations:
1. Data Requirements: The effectiveness of the model is contingent on the availability of comprehensive and high-quality multi-modal datasets.

2. Generalizability: While promising, the results are primarily validated on specific types of gene expression data, and its performance across broader biological applications remains to be fully assessed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework for the multi-modality pretrain model according to the Central dogma of biology. The method encode DNA, protein and RNA at the same time. The proposed method can transfer knowledge from the encoders pretraining and modalities.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is well-organized and easy to follow
The authors have proved that the multi-modality of single cell data can help model predictions.

Weaknesses:
Lack of experiments. 1. More ablation studies should be conducted about removing different modalities of the model in Table 2 ( e.g. we observe only RNA can achieve a high performance, what about protein+RNA? ).

 2. More dataset details should be included. The split of training/validation/test sets is not clear. If the authors do the experiments on the same dataset, they should split the dataset according to the tissues to validate the transferability of the proposed method.

Limitations:
The biological system is more complex. The proposed method only include the direct map from DNA to RNA. However, in real world, RNA can effect the expression of DNAs. More details should be discuss in the future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper models isoform relative abundance across tissues with a multimodal approach based on 3 pretrained encoders for DNA, RNA, and AA sequences.  DNA encoder uses a sequence centered on the gene’s TSS, RNA encoder uses the known isoform sequence from RNAseq and the protein encoder uses corresponding AA sequence.  They perform multiple ablations on the utility of having all 3 separate encoders and, given separate encoders, how to aggregate them into a single isoform specific embedding/prediction, and look at attention layers of RNA module to find biologically meaningful regions of attention.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Isoform level analysis using 3 separate pretrained encoders for DNA, RNA, and AA sequences is a good strategy.  The authors provide useful ablations on the utility of the multi modal approach and on modern strategies for combining those into a single embedding.  Looking for biolgoically meaningful interpretations of attention layers is useful.

Weaknesses:
I don’t think the authors can claim this is the first attempt to combine DNA, RNA, and AA modalities with techniques from NLP.   See the recent Evo work here https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2 .  While they evaluate their performance against Enformer, that’s a large part of their own model.  So the evaluations have an intramural feel to them.  It’d be interesting to see how their strategy compares to other multi modal models such as Evo, and more RNA centric work like Borzoi, which looks at a more fine grained look of variant effects on the DNA to RNA relationship.  Looking at average isoform abundance across individuals is all well and good, but GTEx also has individual genomes, and genomic variation across individuals will also of course affect splicing patterns and which isoforms come from what individuals.

Limitations:
The authors should be more explicit about the limitation of using reference genome and known isoform sequences and how this kind of sweeps splicing as a function of dna sequence under the rug.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces a novel multi-modal model, IsoFormer, designed to integrate DNA, RNA, and protein sequences for predicting RNA transcript isoform expression across different tissues. It utilizes pre-trained modality-specific encoders to generate embeddings that are then combined using a sophisticated aggregation method. The model demonstrates significant improvements in prediction accuracy compared to single-modality approaches.

Contriburion:

1. Developed the first general-purpose multi-modal model integrating DNA, RNA, and protein sequences.

2. Demonstrated successful application of transfer learning from modality-specific encoders.

3. Provided a new robust framework for advancing the prediction of RNA transcript isoform expression.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Innovative Integration of Modalities: The paper presents the first attempt to integrate three biological sequence modalities (DNA, RNA, and proteins) in a unified model, providing a comprehensive approach reflective of natural biological processes.

2. Effective Transfer Learning: IsoFormer effectively leverages pre-trained encoders to enhance its predictive power, benefiting from both intra-modal and inter-modal transfer learning.

3. Robust Evaluation: Experiments demonstrate the model's capability, outperforming existing methods in predicting transcript isoform expression, which is a challenging task due to its multi-modal nature.

Weaknesses:
1. Complexity and Computation: The model's complexity and the computational demands might limit its accessibility and use, particularly in environments with restricted resources.

2. More Comprehensive Evaluation for PLM's representation learning capability would make this paper better.

Limitations:
1. Data Requirements: The effectiveness of the model is contingent on the availability of comprehensive and high-quality multi-modal datasets.

2. Generalizability: While promising, the results are primarily validated on specific types of gene expression data, and its performance across broader biological applications remains to be fully assessed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework for the multi-modality pretrain model according to the Central dogma of biology. The method encode DNA, protein and RNA at the same time. The proposed method can transfer knowledge from the encoders pretraining and modalities.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is well-organized and easy to follow
The authors have proved that the multi-modality of single cell data can help model predictions.

Weaknesses:
Lack of experiments. 1. More ablation studies should be conducted about removing different modalities of the model in Table 2 ( e.g. we observe only RNA can achieve a high performance, what about protein+RNA? ).

 2. More dataset details should be included. The split of training/validation/test sets is not clear. If the authors do the experiments on the same dataset, they should split the dataset according to the tissues to validate the transferability of the proposed method.

Limitations:
The biological system is more complex. The proposed method only include the direct map from DNA to RNA. However, in real world, RNA can effect the expression of DNAs. More details should be discuss in the future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper models isoform relative abundance across tissues with a multimodal approach based on 3 pretrained encoders for DNA, RNA, and AA sequences.  DNA encoder uses a sequence centered on the gene’s TSS, RNA encoder uses the known isoform sequence from RNAseq and the protein encoder uses corresponding AA sequence.  They perform multiple ablations on the utility of having all 3 separate encoders and, given separate encoders, how to aggregate them into a single isoform specific embedding/prediction, and look at attention layers of RNA module to find biologically meaningful regions of attention.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Isoform level analysis using 3 separate pretrained encoders for DNA, RNA, and AA sequences is a good strategy.  The authors provide useful ablations on the utility of the multi modal approach and on modern strategies for combining those into a single embedding.  Looking for biolgoically meaningful interpretations of attention layers is useful.

Weaknesses:
I don’t think the authors can claim this is the first attempt to combine DNA, RNA, and AA modalities with techniques from NLP.   See the recent Evo work here https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2 .  While they evaluate their performance against Enformer, that’s a large part of their own model.  So the evaluations have an intramural feel to them.  It’d be interesting to see how their strategy compares to other multi modal models such as Evo, and more RNA centric work like Borzoi, which looks at a more fine grained look of variant effects on the DNA to RNA relationship.  Looking at average isoform abundance across individuals is all well and good, but GTEx also has individual genomes, and genomic variation across individuals will also of course affect splicing patterns and which isoforms come from what individuals.

Limitations:
The authors should be more explicit about the limitation of using reference genome and known isoform sequences and how this kind of sweeps splicing as a function of dna sequence under the rug.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xDrKZOZEOc;"REVIEW 
Summary:
The paper introduces Optimization Consistency Models (OptCM) as a novel method for solving combinatorial optimization (CO) problems efficiently. Traditional diffusion models, although powerful, are computationally intensive due to their iterative denoising processes. OptCM overcomes this limitation by learning direct mappings from noise levels to optimal solutions, enabling rapid single-step solution generation. The contributions of this paper are three-fold. First, OptCM reduces the computational overhead significantly by enabling fast, single-step solution generation while maintaining high solution quality. Second, This protocol ensures that samples from different generative trajectories converge consistently to the optimal solution. Thrid, Introduced at the test stage, this method enhances solution exploration and quality during inference.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
OptCM significantly reduces the computational overhead by enabling fast, single-step solution generation, compared to the multiple steps required by traditional diffusion models. This efficiency allows for rapid inference, making it practical for real-time and large-scale applications.

Despite the reduced computational steps, OptCM maintains high solution quality, often outperforming state-of-the-art methods that require more extensive processing. The optimization consistency training ensures that the generated solutions are close to the optimal solution.

The optimization consistency training protocol is a novel approach that minimizes the differences among samples from varying generative trajectories, ensuring robust and consistent solution generation. This method enhances the model's ability to generalize across different problem instances.

The introduction of a consistency-based gradient search during the test stage allows for further exploration and refinement of the solution space, improving the final solution quality. This approach bridges the gap between training and inference, making the model more adaptable to new instances.

Weaknesses:
Overall, the paper's strengths lie in its innovative approach to reducing computational complexity while maintaining high solution quality, its robust and versatile model design, and its impressive performance on benchmark tasks. However, there are some weaknesses in this paper. Some weaknesses listed below might be found to be too general and don't require the authors to address them now.

The advanced techniques used in OptCM, such as optimization consistency training and gradient-based search, may be challenging to implement and require a deep understanding of the underlying principles.

The model's performance is closely tied to the quality and diversity of the problem instances used during training. If the training set does not adequately represent the test instances, the model's effectiveness might be reduced.

While the paper demonstrates the superiority of OptCM over state-of-the-art neural solvers, it could provide a more detailed comparison with traditional, non-neural methods in terms of both performance and computational efficiency.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances CO DM-based neural solvers under the setting where labeled training graphs are available by considering Consistency Models and gradient search (which was adopted from T2T).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1- The paper is in general well-written and technically sound.

2- The use of CMs to accelerate the sampling procedure.

Weaknesses:
See Questions.

Limitations:
See Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents Optimization Consistency Models (OptCM) for solving combinatorial optimization (CO) problems efficiently. By leveraging the consistency model, OptCM maps varying noise levels to optimal solutions in a single step, significantly reducing computational overhead. This approach is validated through extensive experiments on the Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS), demonstrating significant superior efficiency compared with state-of-the-art diffusion-based models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper introduces a consistency model to improve the efficiency of diffusion-based combinatorial optimization solvers.
2. Extensive experiments on TSP and MIS show that OptCM can outperform existing methods.

Weaknesses:
1. This work is mainly incremental, based on previous works DIFUSCO [1] and T2T [2].
2. Larger-size TSPs, such as those with 100000 nodes, should be tested against state-of-the-art learning-based methods [3].
3. Despite significantly improving solving efficiency, the proposed method is limited in addressing constrained COPs (e.g., CVRP and more complex COPs) and requires optimal solutions as labels.

[1] DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization, NeurIPS, 2023.

[2] T2T: From Distribution Learning in Training to Gradient Search in Testing for Combinatorial Optimization, NeurIPS, 2023.

[3] GLOP: Learning Global Partition and Local Construction for Solving Large-scale Routing Problems in Real-time, AAAI, 2024.

Limitations:
The authors have discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduced a new algorithm for solving some classic combinatorial optimization problems. The method falls into the category of learn-based generative solvers. More specifically, it is a direct extension of the DIFUSCO [1] and T2t [2] solver, which are diffusion-based generative solvers. The improvement is mostly done through improving on the sampling step of the two aforementioned works with consistency models (CM) [3], a recent notable regime that enables drastic reduction of the number of function evaluations (NFE, or sampling steps) of vanilla diffusion models. The novelty lies in extending CM into discrete regime and combining it with consistency-based gradient search, which is necessary for combinatorial optimization problem. Empirical evaluations show the effectiveness of this new solver, where it achieves competitive objective value in a much shorter time, compared to various baselines. 


[1] Z. Sun and Y. Yang, “DIFUSCO: Graph-based diffusion solvers for combinatorial optimization, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. 

[2] Y. Li, J. Guo, R. Wang, and J. Yan, “T2t: From distribution learning in training to gradient search in testing for combinatorial optimization,” in Advances in Neural Information Processing Systems, 2023.

[3]  Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, “Consistency models,” arXiv preprint arXiv:2303.01469, 2023

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Overall I think the method is novel. The extension of consistency training framework into diffusion-based CO solvers is not trivial, and this work is trying to solve a well-motivated problem. The empirical evaluations are quite convincing compared to diffusion-based CO solvers (of course, one expects so as consistency models inference time is much faster than diffusion generative models). 

2. The paper is overall well-written, and the authors seem to have done quite thorough literature reviews to gather sufficient baselines to compare to the performance of their work to.

Weaknesses:
1. It is necessary to elaborate on why the authors wrote ""... note F1 is exactly the (implicit) the objective of the diffusion and consistency models..."" at line 259. In other words we need to see why (should be a lemma with proof) we have the quantity $F_1$ in equation (6) is the equivalence of the loss in eq (4).

2. It is unclear how the overall training paradigm takes place in practice, including the gradient search part. The authors should include an algorithm box on the training of their OptCM framework. Algorithm 1 on Multistep Consistency Sampling is almost the same as in the original Consistency Models paper, so I suggest the authors move them to the Appendix. If I'm not mistaken, the training of consistency models is quite tricky, for example, it is crucial to design a suitable training time discretization schedule for CM to work well. Could the authors elaborate on this for their problem? 

 I'm willing to re-evaluate my score if the authors can answer these two points.

Limitations:
Consistency training introduced additional training overhead; I think the authors did not discuss this in Appendix D.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Optimization Consistency Models (OptCM) as a novel method for solving combinatorial optimization (CO) problems efficiently. Traditional diffusion models, although powerful, are computationally intensive due to their iterative denoising processes. OptCM overcomes this limitation by learning direct mappings from noise levels to optimal solutions, enabling rapid single-step solution generation. The contributions of this paper are three-fold. First, OptCM reduces the computational overhead significantly by enabling fast, single-step solution generation while maintaining high solution quality. Second, This protocol ensures that samples from different generative trajectories converge consistently to the optimal solution. Thrid, Introduced at the test stage, this method enhances solution exploration and quality during inference.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
OptCM significantly reduces the computational overhead by enabling fast, single-step solution generation, compared to the multiple steps required by traditional diffusion models. This efficiency allows for rapid inference, making it practical for real-time and large-scale applications.

Despite the reduced computational steps, OptCM maintains high solution quality, often outperforming state-of-the-art methods that require more extensive processing. The optimization consistency training ensures that the generated solutions are close to the optimal solution.

The optimization consistency training protocol is a novel approach that minimizes the differences among samples from varying generative trajectories, ensuring robust and consistent solution generation. This method enhances the model's ability to generalize across different problem instances.

The introduction of a consistency-based gradient search during the test stage allows for further exploration and refinement of the solution space, improving the final solution quality. This approach bridges the gap between training and inference, making the model more adaptable to new instances.

Weaknesses:
Overall, the paper's strengths lie in its innovative approach to reducing computational complexity while maintaining high solution quality, its robust and versatile model design, and its impressive performance on benchmark tasks. However, there are some weaknesses in this paper. Some weaknesses listed below might be found to be too general and don't require the authors to address them now.

The advanced techniques used in OptCM, such as optimization consistency training and gradient-based search, may be challenging to implement and require a deep understanding of the underlying principles.

The model's performance is closely tied to the quality and diversity of the problem instances used during training. If the training set does not adequately represent the test instances, the model's effectiveness might be reduced.

While the paper demonstrates the superiority of OptCM over state-of-the-art neural solvers, it could provide a more detailed comparison with traditional, non-neural methods in terms of both performance and computational efficiency.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances CO DM-based neural solvers under the setting where labeled training graphs are available by considering Consistency Models and gradient search (which was adopted from T2T).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1- The paper is in general well-written and technically sound.

2- The use of CMs to accelerate the sampling procedure.

Weaknesses:
See Questions.

Limitations:
See Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents Optimization Consistency Models (OptCM) for solving combinatorial optimization (CO) problems efficiently. By leveraging the consistency model, OptCM maps varying noise levels to optimal solutions in a single step, significantly reducing computational overhead. This approach is validated through extensive experiments on the Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS), demonstrating significant superior efficiency compared with state-of-the-art diffusion-based models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper introduces a consistency model to improve the efficiency of diffusion-based combinatorial optimization solvers.
2. Extensive experiments on TSP and MIS show that OptCM can outperform existing methods.

Weaknesses:
1. This work is mainly incremental, based on previous works DIFUSCO [1] and T2T [2].
2. Larger-size TSPs, such as those with 100000 nodes, should be tested against state-of-the-art learning-based methods [3].
3. Despite significantly improving solving efficiency, the proposed method is limited in addressing constrained COPs (e.g., CVRP and more complex COPs) and requires optimal solutions as labels.

[1] DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization, NeurIPS, 2023.

[2] T2T: From Distribution Learning in Training to Gradient Search in Testing for Combinatorial Optimization, NeurIPS, 2023.

[3] GLOP: Learning Global Partition and Local Construction for Solving Large-scale Routing Problems in Real-time, AAAI, 2024.

Limitations:
The authors have discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduced a new algorithm for solving some classic combinatorial optimization problems. The method falls into the category of learn-based generative solvers. More specifically, it is a direct extension of the DIFUSCO [1] and T2t [2] solver, which are diffusion-based generative solvers. The improvement is mostly done through improving on the sampling step of the two aforementioned works with consistency models (CM) [3], a recent notable regime that enables drastic reduction of the number of function evaluations (NFE, or sampling steps) of vanilla diffusion models. The novelty lies in extending CM into discrete regime and combining it with consistency-based gradient search, which is necessary for combinatorial optimization problem. Empirical evaluations show the effectiveness of this new solver, where it achieves competitive objective value in a much shorter time, compared to various baselines. 


[1] Z. Sun and Y. Yang, “DIFUSCO: Graph-based diffusion solvers for combinatorial optimization, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. 

[2] Y. Li, J. Guo, R. Wang, and J. Yan, “T2t: From distribution learning in training to gradient search in testing for combinatorial optimization,” in Advances in Neural Information Processing Systems, 2023.

[3]  Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, “Consistency models,” arXiv preprint arXiv:2303.01469, 2023

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Overall I think the method is novel. The extension of consistency training framework into diffusion-based CO solvers is not trivial, and this work is trying to solve a well-motivated problem. The empirical evaluations are quite convincing compared to diffusion-based CO solvers (of course, one expects so as consistency models inference time is much faster than diffusion generative models). 

2. The paper is overall well-written, and the authors seem to have done quite thorough literature reviews to gather sufficient baselines to compare to the performance of their work to.

Weaknesses:
1. It is necessary to elaborate on why the authors wrote ""... note F1 is exactly the (implicit) the objective of the diffusion and consistency models..."" at line 259. In other words we need to see why (should be a lemma with proof) we have the quantity $F_1$ in equation (6) is the equivalence of the loss in eq (4).

2. It is unclear how the overall training paradigm takes place in practice, including the gradient search part. The authors should include an algorithm box on the training of their OptCM framework. Algorithm 1 on Multistep Consistency Sampling is almost the same as in the original Consistency Models paper, so I suggest the authors move them to the Appendix. If I'm not mistaken, the training of consistency models is quite tricky, for example, it is crucial to design a suitable training time discretization schedule for CM to work well. Could the authors elaborate on this for their problem? 

 I'm willing to re-evaluate my score if the authors can answer these two points.

Limitations:
Consistency training introduced additional training overhead; I think the authors did not discuss this in Appendix D.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xCIbVuXwPM;"REVIEW 
Summary:
This paper studies surrogate loss design and the trade-off between surrogate consistency and loss dimension. The contributions are three-fold: (1) the characterization of the hallucination region, where the decoded prediction from the surrogate loss minimizer gives a class with no target probability mass, indicating a completely ""irrational"" prediction; (2) the construction of the (""weakly but reasonably"" inconsistent) calibrated surrogate and link under the low-noise setup; (3) the decomposition of property eliciation into multiple elicitation problems with low dimensions.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
+ **Very well-motivated problem**: Consistent/calibrated surrogate losses, the main topic of this paper, are sometimes too restrictive because the (in)consistency analysis hinges on ""far-fetched"" distributions---as argued by the authors, and as can be seen in some counterexamples to show the inconsistency of the well-known Crammer-Singer hinge loss. If we can remove such ""pathological"" situations from the entire distribution space, we would have a nicer characterization of loss functions. This is the central motivation of this paper.
+ **Interesting instances to show calibrated surrogates under the low-noise condition**: This paper mainly investigates two situations to convince the benefits of the relaxed notion of calibration: the unit cube and permutahedron. Both make sense with reasonably practical scenarios and provide the first attempt to show the benefits of incorporating the knowledge of the noise level in loss function design.

Weaknesses:
+ **The result with the unit cube may need $d \\geq n$**: At first sight, the statement of Corollary 7 (the calibrated link design for the unit cube) does not have any restriction on $\\alpha$ (say, any dependency on the embedding $d$), unlike Corollary 8 (the calibrated link design for the permutahedron). When I look at its proof, I suspect we need the condition $d \\geq n$ for this because the proof leverages that ""$P\_\\alpha^y$ is a strict subset of the orthant that contains $v\_y$"" at l.500. Otherwise, it is strange because we can choose an arbitrarily small embedding dimension $d$.
+ **The trade-off could be seen only for the unit cube case**: The main contribution of this paper is to showcase the trade-off between the consistency and the embedding dimension, as suggested by the title. However, we may not see a clear trade-off in the unit-cube case. This is also related to the above point: Once we have $d \\geq n$, there may not be a clear trade-off for $\\alpha$ and $d$. Given this, we are not very sure how universally we can observe the trade-off across different elicitation problems.

Nevertheless, I don't think these points are significant enough to undermine the contributions of this paper. I expect the authors to address them, which leads to the presentation of the contributions in a more fair/precise manner.

Limitations:
This work has discussed the limitations. Specifically, there is computational hardness to break down an elicitation problem into multiple low-dimensional problems in Section 5.

This work does not have potential negative societal impacts because the contributions of this paper are relevant only to general machine learning theory.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method called polytope embedding, which embeds multiclass predictions onto real numbers. The paper studies the properties of this embedding, like hallucination and calibration. Further, with low-noise assumptions, the authors showed more calibration results for their embedding in some cases like embedding into the unit cube.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The topic of this paper on how to design a consistent surrogate loss seems interesting. The results proved in the paper also relate to topics that people care about like hallucination and calibration.

Weaknesses:
Even though each result seems interesting, the structure of this paper is not very clear to me. I am open to different arguments but I think it is hard for readers to understand what the authors are trying to show with this paper.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper examines the trade-off between consistency and dimensionality in multi-class classification. It has been known that the lower bound on the dimension for consistent surrogate losses under any distribution is $n - 1$, wheren $n$ is the dimension of the input space. The authors propose the notion of partial consistency, which permits the establishment of surrogate losses at much lower dimensions than the input dimension. This method allows for the consistency of lower-dimensional surrogate losses under a low-noise condition and the construction of dimensionally reduced consistent surrogate losses across multiple problem instances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is the first study to explore the control of the trade-off between consistency and dimension in multi-class classification.

2. The paper is well-written and clear, even though it is theoretically dense.

Weaknesses:
1. The paper demonstrates the existence of distributions under which consistency holds for low-dimensional surrogates but does not offer much guidance on how to identify such distributions for a given surrogate loss.

2. The paper restricts its study and analysis to asymptotic guarantees.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the problem of constructing consistent multiclass surrogate losses for the 0-1 loss while reducing the dimension of the scoring function is studied. The concept of partial consistency, which can be dated back to the study of multiclass SVM, is used as a crucial part of this work. It is first revealed that any losses with scoring function’s dimension less than #class number-1 will lead to severe misclassification. Then low noise assumption is used to enable the trade off between (partial) consistency and the scoring function’s dimension number. An attempt of recovering full consistency with several scoring functions of lower dimensions is also made.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. A clear trade off between the strictness of low-noise assumption and scoring function’s dimension number is quantified.

2. The analysis of hallucinations is also enlightening, which provides an interesting insight of why some well-suited models (with some losses) can make wrong predictions even with no real-world evidence.

3. The clear presentation and layout of the results make them easy for readers to understand.

Weaknesses:
While the dimension of predictor is reduced and thus the computational cost of training can be promisingly reduced, the computational cost of the inference stage may increase compared with that of the traditional #class number-dimensional scoring functions. Is there any remedy for this problem?

Limitations:
Please see the weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method called polytope embedding, which embeds multiclass predictions onto real numbers. The paper studies the properties of this embedding, like hallucination and calibration. Further, with low-noise assumptions, the authors showed more calibration results for their embedding in some cases like embedding into the unit cube.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The topic of this paper on how to design a consistent surrogate loss seems interesting. The results proved in the paper also relate to topics that people care about like hallucination and calibration.

Weaknesses:
Even though each result seems interesting, the structure of this paper is not very clear to me. I am open to different arguments but I think it is hard for readers to understand what the authors are trying to show with this paper.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies surrogate loss design and the trade-off between surrogate consistency and loss dimension. The contributions are three-fold: (1) the characterization of the hallucination region, where the decoded prediction from the surrogate loss minimizer gives a class with no target probability mass, indicating a completely ""irrational"" prediction; (2) the construction of the (""weakly but reasonably"" inconsistent) calibrated surrogate and link under the low-noise setup; (3) the decomposition of property eliciation into multiple elicitation problems with low dimensions.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
+ **Very well-motivated problem**: Consistent/calibrated surrogate losses, the main topic of this paper, are sometimes too restrictive because the (in)consistency analysis hinges on ""far-fetched"" distributions---as argued by the authors, and as can be seen in some counterexamples to show the inconsistency of the well-known Crammer-Singer hinge loss. If we can remove such ""pathological"" situations from the entire distribution space, we would have a nicer characterization of loss functions. This is the central motivation of this paper.
+ **Interesting instances to show calibrated surrogates under the low-noise condition**: This paper mainly investigates two situations to convince the benefits of the relaxed notion of calibration: the unit cube and permutahedron. Both make sense with reasonably practical scenarios and provide the first attempt to show the benefits of incorporating the knowledge of the noise level in loss function design.

Weaknesses:
+ **The result with the unit cube may need $d \\geq n$**: At first sight, the statement of Corollary 7 (the calibrated link design for the unit cube) does not have any restriction on $\\alpha$ (say, any dependency on the embedding $d$), unlike Corollary 8 (the calibrated link design for the permutahedron). When I look at its proof, I suspect we need the condition $d \\geq n$ for this because the proof leverages that ""$P\_\\alpha^y$ is a strict subset of the orthant that contains $v\_y$"" at l.500. Otherwise, it is strange because we can choose an arbitrarily small embedding dimension $d$.
+ **The trade-off could be seen only for the unit cube case**: The main contribution of this paper is to showcase the trade-off between the consistency and the embedding dimension, as suggested by the title. However, we may not see a clear trade-off in the unit-cube case. This is also related to the above point: Once we have $d \\geq n$, there may not be a clear trade-off for $\\alpha$ and $d$. Given this, we are not very sure how universally we can observe the trade-off across different elicitation problems.

Nevertheless, I don't think these points are significant enough to undermine the contributions of this paper. I expect the authors to address them, which leads to the presentation of the contributions in a more fair/precise manner.

Limitations:
This work has discussed the limitations. Specifically, there is computational hardness to break down an elicitation problem into multiple low-dimensional problems in Section 5.

This work does not have potential negative societal impacts because the contributions of this paper are relevant only to general machine learning theory.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper examines the trade-off between consistency and dimensionality in multi-class classification. It has been known that the lower bound on the dimension for consistent surrogate losses under any distribution is $n - 1$, wheren $n$ is the dimension of the input space. The authors propose the notion of partial consistency, which permits the establishment of surrogate losses at much lower dimensions than the input dimension. This method allows for the consistency of lower-dimensional surrogate losses under a low-noise condition and the construction of dimensionally reduced consistent surrogate losses across multiple problem instances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is the first study to explore the control of the trade-off between consistency and dimension in multi-class classification.

2. The paper is well-written and clear, even though it is theoretically dense.

Weaknesses:
1. The paper demonstrates the existence of distributions under which consistency holds for low-dimensional surrogates but does not offer much guidance on how to identify such distributions for a given surrogate loss.

2. The paper restricts its study and analysis to asymptotic guarantees.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the problem of constructing consistent multiclass surrogate losses for the 0-1 loss while reducing the dimension of the scoring function is studied. The concept of partial consistency, which can be dated back to the study of multiclass SVM, is used as a crucial part of this work. It is first revealed that any losses with scoring function’s dimension less than #class number-1 will lead to severe misclassification. Then low noise assumption is used to enable the trade off between (partial) consistency and the scoring function’s dimension number. An attempt of recovering full consistency with several scoring functions of lower dimensions is also made.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. A clear trade off between the strictness of low-noise assumption and scoring function’s dimension number is quantified.

2. The analysis of hallucinations is also enlightening, which provides an interesting insight of why some well-suited models (with some losses) can make wrong predictions even with no real-world evidence.

3. The clear presentation and layout of the results make them easy for readers to understand.

Weaknesses:
While the dimension of predictor is reduced and thus the computational cost of training can be promisingly reduced, the computational cost of the inference stage may increase compared with that of the traditional #class number-dimensional scoring functions. Is there any remedy for this problem?

Limitations:
Please see the weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
x7usmidzxj;"REVIEW 
Summary:
In this paper, the authors analyze the convergence of Adam under milder noise conditions (affline variance) and milder smoothness conditions (both $L$-smoothness and $(L_0,L_q)$-smoothness) and propose a $O(\text{polylog}(T)/\sqrt T)$ convergence rate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper analyses the convergence of Adam under milder smoothness conditions compared to the previous work. The result is relatively solid and convincing. The writing structure is also relatively clear.

Weaknesses:
1. As the author claimed in their paper, they did not provide numerical experiments in this paper. While this paper is a theoretical paper focusing on the convergence analysis of Adam, some simple numerical experiments aligning with the results will make it more convincing.
2. This paper exhibits a slight lack of novelty. Since after checking out the proof details, I found that the crucial techniques were almost proposed by the previous related works.  However, this weakness is trivial, especially for a theoretical paper, and as I claimed in the Strength part, the result of this paper is solid.

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper provides two probabilistic convergence rates for Adam with generalized affine variance noise under smoothness and generalized smooth condition, respectively, which achieves comparable results to many prior results.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Please see the above Summary.

Weaknesses:
1. I suggest that authors should provide detailed formulas for some notations including $\textbf{g}_t, g(\textbf{x}), \nabla f(\textbf{x})$. Is $\nabla f(\textbf{x})$ the gradient with the form of expectation.

2. In line 118, the reference [10] is cited repeatedly.

3. Section 5 is used to discuss the most related works and make comparisons with the main results in this paper. However, authors only discuss the most related works without any comparison with their main results. 

4. As mentioned in 1., if $\nabla f(\textbf{x})$ is the gradient with the form of expectation, the two results (Theorems 3.1 and 4.1) in this paper are not fully high probability since $\frac{1}{T} \sum_{t=1}^T ||\nabla f(x_t)||^2$ is equivalent to $\frac{1}{T} \sum_{t=1}^T ||E_{z_i}[\nabla f(x_t,z_i)]||^2$ ($z_i$ is the training data defined by me) which is smaller than $\frac{1}{T} \sum_{t=1}^T E_{z_i}[||\nabla f(x_t,z_i)||^2]$. In other words, the results with the form $\frac{1}{T} \sum_{t=1}^T E_{z_i}[||\nabla f(x_t,z_i)||^2]$ can directly derive the corresponding results with the form $\frac{1}{T} \sum_{t=1}^T ||E_{z_i}[\nabla f(x_t,z_i)]||^2$, to say nothing of the one with additional high probability. Therefore, from my perspective, high probability is not an advantage for this paper, but rather weakens $\frac{1}{T} \sum_{t=1}^T ||E_{z_i}[\nabla f(x_t,z_i)]||^2$.

Limitations:
The contribution of this paper is a bit weak. Although authors weaken some assumptions, such as noise and smoothness, the form of their results is also weakened. Therefore, I don’t ensure the weakened assumptions are their advantages.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the high-probability convergence of Adam in the non-convex setting under relaxed assumptions. The authors consider a general noise condition that governs affine, sub-Gaussian, and bounded noise conditions. They also consider a generalized smoothness condition motivated by language model experiments. Under these assumptions, they obtain a convergence rate of $\text{poly}(\log T/\delta)/T$, where $T$ is the number of iterations and $\delta$ is the confidence level.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Their result look novel and significant. They have shown the high-probability convergence of Adam under relaxed conditions than all previous papers.
2. The proofs look correct. 
3. The paper is well-written and results are clearly presented.

Weaknesses:
1. One major concern is, by choosing $\beta_2=1-1/T$, does the author essentially reduce Adam to SGD with momentum, as this makes $v_t$ almost a constant? Btw, I think for [18] and [23] in Table 1, $\beta_1$ should be $1-1/\sqrt{T}$. Please also check other rows more carefully.

I will increase the score if this concern is addressed.

Limitations:
See weaknesses and questions above

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides two probabilistic convergence rates for Adam with generalized affine variance noise under smoothness and generalized smooth condition, respectively, which achieves comparable results to many prior results.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Please see the above Summary.

Weaknesses:
1. I suggest that authors should provide detailed formulas for some notations including $\textbf{g}_t, g(\textbf{x}), \nabla f(\textbf{x})$. Is $\nabla f(\textbf{x})$ the gradient with the form of expectation.

2. In line 118, the reference [10] is cited repeatedly.

3. Section 5 is used to discuss the most related works and make comparisons with the main results in this paper. However, authors only discuss the most related works without any comparison with their main results. 

4. As mentioned in 1., if $\nabla f(\textbf{x})$ is the gradient with the form of expectation, the two results (Theorems 3.1 and 4.1) in this paper are not fully high probability since $\frac{1}{T} \sum_{t=1}^T ||\nabla f(x_t)||^2$ is equivalent to $\frac{1}{T} \sum_{t=1}^T ||E_{z_i}[\nabla f(x_t,z_i)]||^2$ ($z_i$ is the training data defined by me) which is smaller than $\frac{1}{T} \sum_{t=1}^T E_{z_i}[||\nabla f(x_t,z_i)||^2]$. In other words, the results with the form $\frac{1}{T} \sum_{t=1}^T E_{z_i}[||\nabla f(x_t,z_i)||^2]$ can directly derive the corresponding results with the form $\frac{1}{T} \sum_{t=1}^T ||E_{z_i}[\nabla f(x_t,z_i)]||^2$, to say nothing of the one with additional high probability. Therefore, from my perspective, high probability is not an advantage for this paper, but rather weakens $\frac{1}{T} \sum_{t=1}^T ||E_{z_i}[\nabla f(x_t,z_i)]||^2$.

Limitations:
The contribution of this paper is a bit weak. Although authors weaken some assumptions, such as noise and smoothness, the form of their results is also weakened. Therefore, I don’t ensure the weakened assumptions are their advantages.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the authors analyze the convergence of Adam under milder noise conditions (affline variance) and milder smoothness conditions (both $L$-smoothness and $(L_0,L_q)$-smoothness) and propose a $O(\text{polylog}(T)/\sqrt T)$ convergence rate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper analyses the convergence of Adam under milder smoothness conditions compared to the previous work. The result is relatively solid and convincing. The writing structure is also relatively clear.

Weaknesses:
1. As the author claimed in their paper, they did not provide numerical experiments in this paper. While this paper is a theoretical paper focusing on the convergence analysis of Adam, some simple numerical experiments aligning with the results will make it more convincing.
2. This paper exhibits a slight lack of novelty. Since after checking out the proof details, I found that the crucial techniques were almost proposed by the previous related works.  However, this weakness is trivial, especially for a theoretical paper, and as I claimed in the Strength part, the result of this paper is solid.

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the high-probability convergence of Adam in the non-convex setting under relaxed assumptions. The authors consider a general noise condition that governs affine, sub-Gaussian, and bounded noise conditions. They also consider a generalized smoothness condition motivated by language model experiments. Under these assumptions, they obtain a convergence rate of $\text{poly}(\log T/\delta)/T$, where $T$ is the number of iterations and $\delta$ is the confidence level.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Their result look novel and significant. They have shown the high-probability convergence of Adam under relaxed conditions than all previous papers.
2. The proofs look correct. 
3. The paper is well-written and results are clearly presented.

Weaknesses:
1. One major concern is, by choosing $\beta_2=1-1/T$, does the author essentially reduce Adam to SGD with momentum, as this makes $v_t$ almost a constant? Btw, I think for [18] and [23] in Table 1, $\beta_1$ should be $1-1/\sqrt{T}$. Please also check other rows more carefully.

I will increase the score if this concern is addressed.

Limitations:
See weaknesses and questions above

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wm9JZq7RCe;"REVIEW 
Summary:
This paper presents a study on tokenization by investigating the behavior of transformers on simple data generating processes . It shows that, in the absence of any tokenization, transformers trained on $k$th-order Markov processes predict characters according to a unigram model, which is quite problematic given how poor unigram models are at modeling Markovian data. Paradoxically, they observe that, even the simplest unigram model learnt by transformers *with the appropriate tokenization* is able to model the probability of sequences sampled from a $k$th-order Markov process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written, with empirical observation intermingled with theory, which I quite liked. The theory is also accompanied by a lot if intuition, insight and interpretation, which really helps drive the point home.

Weaknesses:
- In section 3.2, the authors chose to focus on developing guarantees for a newly developed tokenizer, which, to my knowledge, is seldom used. It would've been maybe of greater use to the community to also, or instead, establish these guarantees for the more commonly-used tokenizers, such as BPE.

- I appreciate that this is mostly a theoretical study of tokenizers, and while the observations put forward are valuable, I found myself wondering what practical takeaways this paper presents to improve current tokenizers. That is something I would love the authors to comment on.

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors show that tokenization is a fundamental property of transformer-based models, in the sense that without it, it is hard (if not impossible) to achieve low cross-entropy loss on next-word prediction. They show that tokenization helps breaking the unigram barrier (i.e., the best loss a unigram model can achieve) and give a theoretical characterization of the information tokenizers provide in terms of statistics on token distribution.

In particular:

Section 2.1 shows how models without tokenization cannot achieve the optimal cross-entropy loss, while when equipped with a tokenizer they break the unigram barrier.

Section 3 studies tokenizers that assign all possible substrings (up to length r) as tokens in the dictionary and shows their theoretical optimality in learning processes ruled by k-Markov chains. A consequence is that unigram models can also do that, in the limit.

Of course, this comes at the expense of the model's efficiency and potential attacks that one can run on an exponential number of tokens (i.e., the surface attack grows very large).

Finally, the authors show that tokenizers can trade off the vocabulary size while maintaining low cross-entropy (i.e., they can behave like an optimal model).

Finally, they extend the theoretical framework to LZW tokenizers.

Experiments are conducted on tokenized vs. non-tokenized models on {k=1}-Markov models and then on some real datasets to show that tokenizers trade-off complexity and efficiency in learning an optimal representation of the characters (and their frequency) in the training distribution.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The article studies an important problem, and I think there is value in the paper.
To the best of my knowledge, comparing BPE to non-tokenized models is new, and the figures give some interesting insights (e.g., Figure 2).
Your paper contains much theoretical work, contributing to its quality.
The results in the limit for unigram and BPE/LZW models are noticeable (Section 3 and Eq. 2).

In general, the results seem solid and are also interesting for linguists and NLP researchers. BPE and other tokenization methods find a trade-off between unigram models, as per Eq. 2, and the complexity of the resulting vocabulary (and model).

Weaknesses:
One of the main weaknesses of this work is how it is presented. 
Maybe it's me, but I found it quite hard to read. See questions.

Another concern is how theoretical results apply to real-world datasets. See questions, but Fig. 5 seems to mitigate the impact of your theoretical results.
In fact, for the vocabulary that grows larger, all the models have a similar value of cross entropy (for around ~50K tokens).

The article seems rushed, as there are many typos (I just listed some).
- Line 150 “the a”
- Line 173, “it make since” --> “sense”
- Line 175, eq. and many others --> Eq. (it’s not wrong per-se, but you capitalize Fig, Example, etc.)
- The Notation paragraph shouldn’t go with related works but should be in the next section.
- Notation in 2 is a bit sloppy (this is a personal suggestion): you can use D() and E() for the decoder/encoder (and enclose them with \mathcal).

Limitations:
Please see previous sections.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper offers theoretical insights into the importance of tokenization in language models. Tokenization is ostensibly the artifact that makes training LMs not an end-to-end procedure. This design choice introduces biases, as it is not optimized for exactly the same criterion as the full model. Yet training without a tokenization step almost always leads to worse language models. This paper attempts to provide reasons based in probability theory for this phenomenon. The authors first explore a toy setting, in which transformer models are tasked with predicting distributions from kth order Markov processes. They offer a theoretical explanation for why the error of models is capped at that of a unigram model and how tokenization alleviates this issue. They then show that tokenization schemes with certain properties can achieve the optimal cross-entropy loss. The work offers some basic experimental results confirming their insights.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Tokenization is a core part NLP pipelines yet it still needs to be better understood from a theoretical perspective. The questions that this paper tries to answer are very relevant for both model interpretability and further development
* The theory is presented in an understandable manner and results for specific popular tokenization schemes are provided.

Weaknesses:
* The theory presented in this work is for a specific type of data-generating distribution (kth order Markov) and we can’t directly extrapolate these results to linguistic distributions, which do not necessarily follow such a distribution. There is minimal discussion about the relationship between kth order Markov and linguistic distributions, which leaves the reader questioning how relevant these results actually are.
* Ultimately, the results are limited; they essentially show an expected result (the existence of an optimal unigram language model as the dictionary size grows to infinity). While some intuition can be gained from these results, the theoretical implications are limited.
* There is minimal discussion of the empirical results and what conclusions can be drawn from them. Given how much of the theory is not directly applicable to real language modeling settings, it feels like such a discussion should be very important

Limitations:
Limitations are not discussed in depth. The authors should address their limited experimental setting and the applicability of the results to linguistic distributions (which are not evidently k-order Markovian)

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the learning dynamics of unigram language models on top of tokenised vs non-tokenised data, comparing these models’ expected cross-entropy to the distribution’s entropy. The paper performs this analysis while considering different data generating distributions (mainly focusing on relatively simple markov chains), and different tokenisation methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper tackles an interesting and timely topic: how tokenisation enables language modeling. 

This paper provides an interesting theoretical analysis of the effect of tokenisation on unigram language modeling.

This paper also provides a couple of empirical analyses of how unigram models perform on real data.

The paper is relatively easy to follow, even though some of the mathematical results could be spelled out a bit more clearly to make it easier for a reader to follow them.

Weaknesses:
This paper’s framing, in my opinion, significantly over-claims its results:
* The title “Toward a Theory of Tokenization in LLMs” is very broad for the current contributions. A more appropriate title, in my opinion, would be “Analysing tokenisation’s effect on unigram distributions”, or something analogous to it. There is no “theory of tokenisation” being proposed here, but a theoretical analysis of how tokenisation affects a simple model’s cross-entropy.
* The abstract and introduction also significantly overclaim results, with statements such as “we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization” while focusing on unigram cross-entropies. Transformers may serve as motivation to this work (as they initially learn unigram statistics), but are not in fact analysed here.

I think the paper would also be significantly more straightforward to read if the framing was fixed and it was clear from the start that the paper's analyses would focus on unigram models.

Limitations:
I think some important limitations are not sufficiently discussed in this paper. The most important of which is that the analysis focuses on unigram statistics, and transformers can clearly learn more than that. Expanding the limitations pointed out by Remark 3.3 in a dedicated limitations section could also be useful.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a study on tokenization by investigating the behavior of transformers on simple data generating processes . It shows that, in the absence of any tokenization, transformers trained on $k$th-order Markov processes predict characters according to a unigram model, which is quite problematic given how poor unigram models are at modeling Markovian data. Paradoxically, they observe that, even the simplest unigram model learnt by transformers *with the appropriate tokenization* is able to model the probability of sequences sampled from a $k$th-order Markov process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written, with empirical observation intermingled with theory, which I quite liked. The theory is also accompanied by a lot if intuition, insight and interpretation, which really helps drive the point home.

Weaknesses:
- In section 3.2, the authors chose to focus on developing guarantees for a newly developed tokenizer, which, to my knowledge, is seldom used. It would've been maybe of greater use to the community to also, or instead, establish these guarantees for the more commonly-used tokenizers, such as BPE.

- I appreciate that this is mostly a theoretical study of tokenizers, and while the observations put forward are valuable, I found myself wondering what practical takeaways this paper presents to improve current tokenizers. That is something I would love the authors to comment on.

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors show that tokenization is a fundamental property of transformer-based models, in the sense that without it, it is hard (if not impossible) to achieve low cross-entropy loss on next-word prediction. They show that tokenization helps breaking the unigram barrier (i.e., the best loss a unigram model can achieve) and give a theoretical characterization of the information tokenizers provide in terms of statistics on token distribution.

In particular:

Section 2.1 shows how models without tokenization cannot achieve the optimal cross-entropy loss, while when equipped with a tokenizer they break the unigram barrier.

Section 3 studies tokenizers that assign all possible substrings (up to length r) as tokens in the dictionary and shows their theoretical optimality in learning processes ruled by k-Markov chains. A consequence is that unigram models can also do that, in the limit.

Of course, this comes at the expense of the model's efficiency and potential attacks that one can run on an exponential number of tokens (i.e., the surface attack grows very large).

Finally, the authors show that tokenizers can trade off the vocabulary size while maintaining low cross-entropy (i.e., they can behave like an optimal model).

Finally, they extend the theoretical framework to LZW tokenizers.

Experiments are conducted on tokenized vs. non-tokenized models on {k=1}-Markov models and then on some real datasets to show that tokenizers trade-off complexity and efficiency in learning an optimal representation of the characters (and their frequency) in the training distribution.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The article studies an important problem, and I think there is value in the paper.
To the best of my knowledge, comparing BPE to non-tokenized models is new, and the figures give some interesting insights (e.g., Figure 2).
Your paper contains much theoretical work, contributing to its quality.
The results in the limit for unigram and BPE/LZW models are noticeable (Section 3 and Eq. 2).

In general, the results seem solid and are also interesting for linguists and NLP researchers. BPE and other tokenization methods find a trade-off between unigram models, as per Eq. 2, and the complexity of the resulting vocabulary (and model).

Weaknesses:
One of the main weaknesses of this work is how it is presented. 
Maybe it's me, but I found it quite hard to read. See questions.

Another concern is how theoretical results apply to real-world datasets. See questions, but Fig. 5 seems to mitigate the impact of your theoretical results.
In fact, for the vocabulary that grows larger, all the models have a similar value of cross entropy (for around ~50K tokens).

The article seems rushed, as there are many typos (I just listed some).
- Line 150 “the a”
- Line 173, “it make since” --> “sense”
- Line 175, eq. and many others --> Eq. (it’s not wrong per-se, but you capitalize Fig, Example, etc.)
- The Notation paragraph shouldn’t go with related works but should be in the next section.
- Notation in 2 is a bit sloppy (this is a personal suggestion): you can use D() and E() for the decoder/encoder (and enclose them with \mathcal).

Limitations:
Please see previous sections.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper offers theoretical insights into the importance of tokenization in language models. Tokenization is ostensibly the artifact that makes training LMs not an end-to-end procedure. This design choice introduces biases, as it is not optimized for exactly the same criterion as the full model. Yet training without a tokenization step almost always leads to worse language models. This paper attempts to provide reasons based in probability theory for this phenomenon. The authors first explore a toy setting, in which transformer models are tasked with predicting distributions from kth order Markov processes. They offer a theoretical explanation for why the error of models is capped at that of a unigram model and how tokenization alleviates this issue. They then show that tokenization schemes with certain properties can achieve the optimal cross-entropy loss. The work offers some basic experimental results confirming their insights.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Tokenization is a core part NLP pipelines yet it still needs to be better understood from a theoretical perspective. The questions that this paper tries to answer are very relevant for both model interpretability and further development
* The theory is presented in an understandable manner and results for specific popular tokenization schemes are provided.

Weaknesses:
* The theory presented in this work is for a specific type of data-generating distribution (kth order Markov) and we can’t directly extrapolate these results to linguistic distributions, which do not necessarily follow such a distribution. There is minimal discussion about the relationship between kth order Markov and linguistic distributions, which leaves the reader questioning how relevant these results actually are.
* Ultimately, the results are limited; they essentially show an expected result (the existence of an optimal unigram language model as the dictionary size grows to infinity). While some intuition can be gained from these results, the theoretical implications are limited.
* There is minimal discussion of the empirical results and what conclusions can be drawn from them. Given how much of the theory is not directly applicable to real language modeling settings, it feels like such a discussion should be very important

Limitations:
Limitations are not discussed in depth. The authors should address their limited experimental setting and the applicability of the results to linguistic distributions (which are not evidently k-order Markovian)

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the learning dynamics of unigram language models on top of tokenised vs non-tokenised data, comparing these models’ expected cross-entropy to the distribution’s entropy. The paper performs this analysis while considering different data generating distributions (mainly focusing on relatively simple markov chains), and different tokenisation methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper tackles an interesting and timely topic: how tokenisation enables language modeling. 

This paper provides an interesting theoretical analysis of the effect of tokenisation on unigram language modeling.

This paper also provides a couple of empirical analyses of how unigram models perform on real data.

The paper is relatively easy to follow, even though some of the mathematical results could be spelled out a bit more clearly to make it easier for a reader to follow them.

Weaknesses:
This paper’s framing, in my opinion, significantly over-claims its results:
* The title “Toward a Theory of Tokenization in LLMs” is very broad for the current contributions. A more appropriate title, in my opinion, would be “Analysing tokenisation’s effect on unigram distributions”, or something analogous to it. There is no “theory of tokenisation” being proposed here, but a theoretical analysis of how tokenisation affects a simple model’s cross-entropy.
* The abstract and introduction also significantly overclaim results, with statements such as “we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization” while focusing on unigram cross-entropies. Transformers may serve as motivation to this work (as they initially learn unigram statistics), but are not in fact analysed here.

I think the paper would also be significantly more straightforward to read if the framing was fixed and it was clear from the start that the paper's analyses would focus on unigram models.

Limitations:
I think some important limitations are not sufficiently discussed in this paper. The most important of which is that the analysis focuses on unigram statistics, and transformers can clearly learn more than that. Expanding the limitations pointed out by Remark 3.3 in a dedicated limitations section could also be useful.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
x4Kk4FxLs3;"REVIEW 
Summary:
The paper introduces PARD, a graph generation model that combines autoregressive and diffusion models. Traditional autoregressive models are effective but sensitive to order, while diffusion models are permutation-invariant but need many denoising steps and extra features. PARD overcomes these issues by generating graphs block-by-block using a partial order of nodes and edges. It employs a shared diffusion model with an equivariant network and a higher-order graph transformer, supporting parallel training like GPT. PARD achieves state-of-the-art performance on various datasets, including large ones like MOSES, without extra features.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
1. This work presents a successful showcase of the combination of autoregressive modeling with diffusion model on graph.
2. The proposed partial order ensures the permutation-invariance in the autoregressive generation process.
3. Impressive experimental results show the effectiveness and efficiency.

Weaknesses:
1. It is unclear how the diffusion model is employed in PARD. Sec 3.1 and the second part of Eq. 6 are not quite relevant to each other. Can you elaborate on that?
2. Please provide proof or reference for some statements. e.g.: 2-FWL expressivity for the proposed higher-order transformer
3. Please provide the results of other baselines on QM9 if possible.

Limitations:
There is no discussion about the limitation in the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a graph generation method that combines AutoRegressive (AR) models and diffusion models. By utilizing a unique partial order, it addresses the issue of non-exchangeable probabilities in AR models and the efficiency problem in diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed block-wise AR diffusion model in this paper offers a new idea for graph generation, particularly by introducing the use of weight-degree to differentiate blocks.
2. The limitations of equivariant networks demonstrated in this paper also hold value for further exploration and resolution within the community.
3. The overall structure and writing of the paper are relatively clear.

Weaknesses:
1. There is a part in the paper that I believe needs to be clarified more clearly to ensure logical coherence. Why does diffusion based on equivariant network solve the flaw in equivariant modeling? I think besides the analogy of tempering iron (or higher/lower energy), more mathematical proofs are needed.

2. Ablation of PPGN is necessary to demonstrate its effectiveness.

3. Following the experimental settings of GDSS, NSPDK is also an important metric for QM9 and ZINC250K.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes a new graph generative model based on an autoregressive procedure. It proposes an approach to deciding a partial order of graph nodes according to their degrees in a node-removal procedure. Based on the partial order, the work devises a new graph generative model.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
The graph algorithm of deciding a partial order of graph nodes would be interesting if such an algorithm does not exist in the literature of graph theory.

Weaknesses:
The work lacks justification. As the field has moved to generative methods with discrete-diffusion models, which are already permutation-invariant, it is less clear about the advantage of designing a complex autoregressive model to satisfy the permutation-invariant property. 

The advantage of the model is not obvious even considering only autoregressive models. Note that Chen et al. [9] have an approach of ""optimizing"" node orders for the generative model and show that the likelihood calculation is more accurate with their approach than a pre-determined order. How does the work justify its advantage over such an approach?

The analysis in 3.3 does not seem to be reasonable. The **probability calculations** are indeed the same for nodes in the same orbit, but they may get different connections in the sampling procedure and then break the symmetry. The analysis in 3.3 is well known, and it is not a concern for generative models. In some diffusion-based generative models, the starting graph is a graph with no edges, then all nodes are in the same orbit, but it is not an issue at all because the edge sampling process will break the symmetry. 

Without clear justification, I don't know where performance improvements are from (maybe architecture improvement?). I feel that the work should have a thorough investigation of the model.

Limitations:
The proposed model seems to have a long running time because it needs to run a diffusion model at the generation of each block.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to integrate autoregression models with diffusion models seamlessly to harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without order sensitivity. It also proposes architectural improvement to make the model and algorithm efficient and scalable. The presentation is smooth and the experimental results on both molecular and general graph generation demonstrate its effectiveness.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
It proposes a novel graph decomposition method considering not individual node and its degree but subsets of nodes with structual similarity. In this way, it removes node order sensitivity in the graph but only needs to maintain the order of the blocks. Within each block, the diffusion model focuses on a much smaller graph and thus has the efficiency to generate a denoised graph.

Weaknesses:
It would be better if the authors can provide some insights about the hyperparameter the maximum of hops $K_h$.

Limitations:
The authors have adequately mentioned several limitations of their work which sound quite reasonable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces PARD, a graph generation model that combines autoregressive and diffusion models. Traditional autoregressive models are effective but sensitive to order, while diffusion models are permutation-invariant but need many denoising steps and extra features. PARD overcomes these issues by generating graphs block-by-block using a partial order of nodes and edges. It employs a shared diffusion model with an equivariant network and a higher-order graph transformer, supporting parallel training like GPT. PARD achieves state-of-the-art performance on various datasets, including large ones like MOSES, without extra features.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
1. This work presents a successful showcase of the combination of autoregressive modeling with diffusion model on graph.
2. The proposed partial order ensures the permutation-invariance in the autoregressive generation process.
3. Impressive experimental results show the effectiveness and efficiency.

Weaknesses:
1. It is unclear how the diffusion model is employed in PARD. Sec 3.1 and the second part of Eq. 6 are not quite relevant to each other. Can you elaborate on that?
2. Please provide proof or reference for some statements. e.g.: 2-FWL expressivity for the proposed higher-order transformer
3. Please provide the results of other baselines on QM9 if possible.

Limitations:
There is no discussion about the limitation in the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a graph generation method that combines AutoRegressive (AR) models and diffusion models. By utilizing a unique partial order, it addresses the issue of non-exchangeable probabilities in AR models and the efficiency problem in diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed block-wise AR diffusion model in this paper offers a new idea for graph generation, particularly by introducing the use of weight-degree to differentiate blocks.
2. The limitations of equivariant networks demonstrated in this paper also hold value for further exploration and resolution within the community.
3. The overall structure and writing of the paper are relatively clear.

Weaknesses:
1. There is a part in the paper that I believe needs to be clarified more clearly to ensure logical coherence. Why does diffusion based on equivariant network solve the flaw in equivariant modeling? I think besides the analogy of tempering iron (or higher/lower energy), more mathematical proofs are needed.

2. Ablation of PPGN is necessary to demonstrate its effectiveness.

3. Following the experimental settings of GDSS, NSPDK is also an important metric for QM9 and ZINC250K.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes a new graph generative model based on an autoregressive procedure. It proposes an approach to deciding a partial order of graph nodes according to their degrees in a node-removal procedure. Based on the partial order, the work devises a new graph generative model.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
The graph algorithm of deciding a partial order of graph nodes would be interesting if such an algorithm does not exist in the literature of graph theory.

Weaknesses:
The work lacks justification. As the field has moved to generative methods with discrete-diffusion models, which are already permutation-invariant, it is less clear about the advantage of designing a complex autoregressive model to satisfy the permutation-invariant property. 

The advantage of the model is not obvious even considering only autoregressive models. Note that Chen et al. [9] have an approach of ""optimizing"" node orders for the generative model and show that the likelihood calculation is more accurate with their approach than a pre-determined order. How does the work justify its advantage over such an approach?

The analysis in 3.3 does not seem to be reasonable. The **probability calculations** are indeed the same for nodes in the same orbit, but they may get different connections in the sampling procedure and then break the symmetry. The analysis in 3.3 is well known, and it is not a concern for generative models. In some diffusion-based generative models, the starting graph is a graph with no edges, then all nodes are in the same orbit, but it is not an issue at all because the edge sampling process will break the symmetry. 

Without clear justification, I don't know where performance improvements are from (maybe architecture improvement?). I feel that the work should have a thorough investigation of the model.

Limitations:
The proposed model seems to have a long running time because it needs to run a diffusion model at the generation of each block.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to integrate autoregression models with diffusion models seamlessly to harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without order sensitivity. It also proposes architectural improvement to make the model and algorithm efficient and scalable. The presentation is smooth and the experimental results on both molecular and general graph generation demonstrate its effectiveness.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
It proposes a novel graph decomposition method considering not individual node and its degree but subsets of nodes with structual similarity. In this way, it removes node order sensitivity in the graph but only needs to maintain the order of the blocks. Within each block, the diffusion model focuses on a much smaller graph and thus has the efficiency to generate a denoised graph.

Weaknesses:
It would be better if the authors can provide some insights about the hyperparameter the maximum of hops $K_h$.

Limitations:
The authors have adequately mentioned several limitations of their work which sound quite reasonable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
x4HMnqs6IE;"REVIEW 
Summary:
This paper presents a method called $\text{ID}^3$ for the task of synthetic face recognition. The authors highlight that the accuracy of face recognition using generated data still lags behind that of training directly on real face data. They propose optimizing the generation process from the perspectives of diversity and consistency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Clear explanation of formulas and algorithm flow.
- Achieved SOTA results compared to methods from the past two years.

Weaknesses:
- There has been extensive research on ID preserving, and recent models based on LDM (e.g., Face0, PhotoMaker, FaceStudio, InstantID) can also be used for synthetic face recognition. The paper lacks analysis and comparative experiments on these models.
- The Face Attribute Conditioning Signal includes age and pose (pose angle range: [-90°, 90°]). However, the visual results in the paper do not reflect these attributes. The variation in pose is minimal, and there is no demonstration of different levels of age (which you mentioned as [0-100]).
- The paper devotes too much space to mathematical derivations and lacks intuitive visual results. For example, using different attributes and ID information to guide the model could be visualized by showing how the various layers of the Unet perceive this information.

Limitations:
Using ID attributes to assist in the generation results is already common in diffusion-based tasks. This method is essentially a conditional guided generation, and its technical contribution is limited.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on synthetic face recognition and proposes to concentrate on three aspects: inter-class diversity, intra-class diversity, and intra-class identity preservation. Based on those, an ID-preserving loss is employed to generate diverse but identity-preserving facial images. This paper also demonstrates the proposed loss is equal to lower bound of an adjusted conditional log-likelihood over ID-preserving data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work is well-written and well-organized. It brings some insights for SFR.
2. The idea of 3 aspects is good, and quite general for SFR
3. The proposed method shows advances when using the FFHQ dataset

Weaknesses:
Here are several concerns regarding this work:
1. The idea of Attribute Conditioning Signal is not fit for synthetic face recognition tasks, because factors contributing to solid FR training cannot be determined by simply adjusting face attributes. One reason is that the attribute network (e.g., pose, age) is not generalized enough, as the pre-trained models are obtained from relatively small-scale datasets compared to FR datasets. Additionally, the authors have not addressed which attributes are effective for FR, leaving this important question unanswered.

2. The performance trained on FFHQ dataset appears good; however FFHQ dataset has explicitly banned its use for face recognition applications. Furthermore, FFHQ is relatively small(210k images) which doesn’t contain enough diversity, that’s the reason facial attributes can be of improvement in this experiment. For more details on FFHQ please refer to: https://github.com/NVlabs/ffhq-dataset

3. When it comes to the relatively large dataset CASIA-WebFace, the improvement over DCFace is marginal. One problem is that DCFace is trained with CASIA-WebFace only, not the FFHQ+CASIA mentioned by the author.

4. Experiments are not sufficient. For example, DCFace provides experiment results on 3 data volumes: 500k, 1M and 1.2M. These are not included in this paper.

5. There are some typos, for example, Y_i should be given in line 194

Limitations:
This paper shows an attempt to generate diverse facial images of each identity. However, what makes solid FR training is not studied. Furthermore, domain GAP exists when adopting the trained diffusion model for generation, the input embedding might differ from the embedding of the synthesized image. Consequently, the focus is on changing facial attributes but preserving identity is interesting but the overall improvement is marginal, and FFHQ has license issues related to the application of Face Recognition. I think it will be more convincing if the results of CASIA-WebFace under 1M and 1.2 M settings are presented.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes ID3, an identity-preserving-yet-diversified diffusion model for generating synthetic face data for face recognition. ID3 leverages identity embeddings and facial attributes to control inter-class and intra-class diversity of generated faces while preserving intra-class identity consistency, demonstrating state-of-the-art performance on multiple synthetic face recognition benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
See questions section in detail.

Weaknesses:
See questions section in detail.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper ""ID3: Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition"" introduces a novel synthetic face recognition (SFR) approach using diffusion models. It focuses on maintaining identity consistency while providing high diversity in generated face images. The proposed ID3 model leverages identity-preserving losses and a structured sampling algorithm that respects identity characteristics. This effectively addresses the common pitfalls of existing SFR approaches that lead to poor generalization on real-world data.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
*   **Originality**: The paper presents an innovative use of diffusion models tailored to synthetic face recognition, emphasizing identity preservation.
*   **Quality**: Demonstrated improvement over state-of-the-art models through extensive benchmarking.
*   **Clarity**: Exceptionally clear presentation and thorough explanation of the methodology and results.
*   **Significance**: This paper addresses significant challenges in synthetic data generation and offers substantial benefits for training more robust and generalizable face recognition systems.

Weaknesses:
*   **Generalization**: Additional tests on further diversified real-world datasets could strengthen the generalization claims.
*   **Complexity**: It would be beneficial to have details on the computational demands and scalability of the model when deployed in practical, real-world scenarios.

Limitations:
The paper discusses potential limitations, including the need for extensive computational resources and the model's performance dependency on the quality of input identity embeddings. It also mentions the ongoing challenge of bridging the gap between synthetic and real-world face recognition performance.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a method called $\text{ID}^3$ for the task of synthetic face recognition. The authors highlight that the accuracy of face recognition using generated data still lags behind that of training directly on real face data. They propose optimizing the generation process from the perspectives of diversity and consistency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Clear explanation of formulas and algorithm flow.
- Achieved SOTA results compared to methods from the past two years.

Weaknesses:
- There has been extensive research on ID preserving, and recent models based on LDM (e.g., Face0, PhotoMaker, FaceStudio, InstantID) can also be used for synthetic face recognition. The paper lacks analysis and comparative experiments on these models.
- The Face Attribute Conditioning Signal includes age and pose (pose angle range: [-90°, 90°]). However, the visual results in the paper do not reflect these attributes. The variation in pose is minimal, and there is no demonstration of different levels of age (which you mentioned as [0-100]).
- The paper devotes too much space to mathematical derivations and lacks intuitive visual results. For example, using different attributes and ID information to guide the model could be visualized by showing how the various layers of the Unet perceive this information.

Limitations:
Using ID attributes to assist in the generation results is already common in diffusion-based tasks. This method is essentially a conditional guided generation, and its technical contribution is limited.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on synthetic face recognition and proposes to concentrate on three aspects: inter-class diversity, intra-class diversity, and intra-class identity preservation. Based on those, an ID-preserving loss is employed to generate diverse but identity-preserving facial images. This paper also demonstrates the proposed loss is equal to lower bound of an adjusted conditional log-likelihood over ID-preserving data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work is well-written and well-organized. It brings some insights for SFR.
2. The idea of 3 aspects is good, and quite general for SFR
3. The proposed method shows advances when using the FFHQ dataset

Weaknesses:
Here are several concerns regarding this work:
1. The idea of Attribute Conditioning Signal is not fit for synthetic face recognition tasks, because factors contributing to solid FR training cannot be determined by simply adjusting face attributes. One reason is that the attribute network (e.g., pose, age) is not generalized enough, as the pre-trained models are obtained from relatively small-scale datasets compared to FR datasets. Additionally, the authors have not addressed which attributes are effective for FR, leaving this important question unanswered.

2. The performance trained on FFHQ dataset appears good; however FFHQ dataset has explicitly banned its use for face recognition applications. Furthermore, FFHQ is relatively small(210k images) which doesn’t contain enough diversity, that’s the reason facial attributes can be of improvement in this experiment. For more details on FFHQ please refer to: https://github.com/NVlabs/ffhq-dataset

3. When it comes to the relatively large dataset CASIA-WebFace, the improvement over DCFace is marginal. One problem is that DCFace is trained with CASIA-WebFace only, not the FFHQ+CASIA mentioned by the author.

4. Experiments are not sufficient. For example, DCFace provides experiment results on 3 data volumes: 500k, 1M and 1.2M. These are not included in this paper.

5. There are some typos, for example, Y_i should be given in line 194

Limitations:
This paper shows an attempt to generate diverse facial images of each identity. However, what makes solid FR training is not studied. Furthermore, domain GAP exists when adopting the trained diffusion model for generation, the input embedding might differ from the embedding of the synthesized image. Consequently, the focus is on changing facial attributes but preserving identity is interesting but the overall improvement is marginal, and FFHQ has license issues related to the application of Face Recognition. I think it will be more convincing if the results of CASIA-WebFace under 1M and 1.2 M settings are presented.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes ID3, an identity-preserving-yet-diversified diffusion model for generating synthetic face data for face recognition. ID3 leverages identity embeddings and facial attributes to control inter-class and intra-class diversity of generated faces while preserving intra-class identity consistency, demonstrating state-of-the-art performance on multiple synthetic face recognition benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
See questions section in detail.

Weaknesses:
See questions section in detail.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper ""ID3: Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition"" introduces a novel synthetic face recognition (SFR) approach using diffusion models. It focuses on maintaining identity consistency while providing high diversity in generated face images. The proposed ID3 model leverages identity-preserving losses and a structured sampling algorithm that respects identity characteristics. This effectively addresses the common pitfalls of existing SFR approaches that lead to poor generalization on real-world data.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
*   **Originality**: The paper presents an innovative use of diffusion models tailored to synthetic face recognition, emphasizing identity preservation.
*   **Quality**: Demonstrated improvement over state-of-the-art models through extensive benchmarking.
*   **Clarity**: Exceptionally clear presentation and thorough explanation of the methodology and results.
*   **Significance**: This paper addresses significant challenges in synthetic data generation and offers substantial benefits for training more robust and generalizable face recognition systems.

Weaknesses:
*   **Generalization**: Additional tests on further diversified real-world datasets could strengthen the generalization claims.
*   **Complexity**: It would be beneficial to have details on the computational demands and scalability of the model when deployed in practical, real-world scenarios.

Limitations:
The paper discusses potential limitations, including the need for extensive computational resources and the model's performance dependency on the quality of input identity embeddings. It also mentions the ongoing challenge of bridging the gap between synthetic and real-world face recognition performance.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
x4EoTQW7ka;"REVIEW 
Summary:
The paper introduces DropBP, an innovative approach to accelerate the fine-tuning of Large Language Models (LLMs) by selectively dropping layers during backward propagation. This method is presented as a means to reduce computational costs and activation memory, significant challenges in the efficient fine-tuning of LLMs. The authors have provided a clear implementation of DropBP as a PyTorch extension and demonstrated its effectiveness through experiments on various LLMs and datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The concept of dropping backward propagation layers to reduce computational overhead is differential from previous work and addresses an important issue in training large models.

- The paper includes extensive experiments that validate the effectiveness of DropBP in reducing training time and memory usage while maintaining accuracy.

- The development of a PyTorch extension for DropBP facilitates easy integration with existing training codes, enhancing the practical applicability of the method.

Weaknesses:
- The motivation is not well illustrated. I agree with that dropping sublayers could lead to training efficiency as the model turns to a shallower counterpart. However, I mean, pervious work like LayerDrop and others omit the layer computation in the forward pass. Then the computation could be removed in the subsequent backward computation with essential engineering efforts. Thus it lacks a clear distinction in terms of technical innovation compared to these previous works.

- While the paper proposes omitting sublayer computation in the backward pass, it's unclear why the forward pass computation remains unchanged. Justifying this choice or exploring alternatives would strengthen the contribution.

- The faster convergence observed in Figure 5 with DropBP compared to the vanilla model is counterintuitive. The observation here quite confuses me since the backward pass optimizes a partial computation graph, concerns regarding overfitting arise. The paper would benefit from a discussion on potential regularization techniques employed to address this, and a comparison with related work (e.g., [1]) that utilizes sublayer dropping for regularization in training a deep Transformer model. 


  
  [1] Li et al., 2021 (AAAI) Learning Light-Weight Translation Models from Deep Transformer

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel method to reduce the computational and memory costs associated with fine-tuning large language models (LLMs). The authors introduce DropBP, a technique that randomly drops layers during backward propagation, effectively reducing the computational operations (FLOPs) and activation memory needed. This method assigns drop rates based on the sensitivity of each layer to ensure stable training. The approach is applicable to both full fine-tuning and parameter-efficient fine-tuning (PEFT) methods. The paper reports significant improvements in training time, convergence speed, and maximum sequence length when fine-tuning LLaMA2 models with DropBP.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- DropBP introduces a novel method for reducing the computational and memory costs associated with fine-tuning LLMs. This is an important contribution to the field, given the increasing size and complexity of these models.

- The paper provides empirical evidence that DropBP significantly reduces training time (by 44%), accelerates convergence (1.5× faster), and increases the maximum sequence length (up to 6.2×) on a single NVIDIA A100 GPU. These results demonstrate the effectiveness of the approach. The authors conduct thorough experiments on multiple datasets and models, providing a robust evaluation of DropBP's performance across different scenarios.

Weaknesses:
- The paper mentions that the sensitivity calculation is done only once and has negligible overhead. However, more details on this process and its potential impact on training time would provide a clearer understanding of any trade-offs involved.

- The paper could benefit from a more detailed theoretical analysis of why DropBP works as effectively as it does. This would strengthen the paper by providing a deeper understanding of the underlying principles.

Limitations:
Limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed to drop layers during backward prop (BP) based on layer sensitivity. The method aims to reduce the cost for gradient computation and storage for intermediate activation in full BP.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Reducing the cost of full BP in PEFT has been an important challenge. 
2. The method is simple and is easy to integrate to either full fine-tuning or PEFT. 
3. Experiments demonstrate that DropBP can speed up the training while retaining the accuracy. The resulting memory reduction makes longer sequence modeling accessible.

Weaknesses:
1. The idea of optimizing NNs with sparse gradient is not new. This paper needs to add more discussion and comparison with related works in sparse learning e.g., [1-3]
2. Table 1 only shows results on two datasets and limited benchmark. It is unclear if the method works well for generation tasks and domain-specific transfer learning.
3. It is unclear which algorithm is used to solve the constraint minimization problem, i.e., to determine the layer-specific rates based on sensitivity, and its extra computational cost.
4. (Minor) In fine-tuning, DropBP drops a set of layers. However, the sensitivity of a set of layers may not be accurately represented by the direct summation of the sensitivities of individual layers in the set.

[1] Sun, Xu, et al. ""meprop: Sparsified back propagation for accelerated deep learning with reduced overfitting.""
[2] Sung, Yi-Lin, Varun Nair, and Colin A. Raffel. ""Training neural networks with fixed sparse masks."" 
[3] Brock, Andrew, et al. ""Freezeout: Accelerate training by progressively freezing layers.""

Limitations:
The authors have addressed the limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces DropBP, an innovative approach to accelerate the fine-tuning of Large Language Models (LLMs) by selectively dropping layers during backward propagation. This method is presented as a means to reduce computational costs and activation memory, significant challenges in the efficient fine-tuning of LLMs. The authors have provided a clear implementation of DropBP as a PyTorch extension and demonstrated its effectiveness through experiments on various LLMs and datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The concept of dropping backward propagation layers to reduce computational overhead is differential from previous work and addresses an important issue in training large models.

- The paper includes extensive experiments that validate the effectiveness of DropBP in reducing training time and memory usage while maintaining accuracy.

- The development of a PyTorch extension for DropBP facilitates easy integration with existing training codes, enhancing the practical applicability of the method.

Weaknesses:
- The motivation is not well illustrated. I agree with that dropping sublayers could lead to training efficiency as the model turns to a shallower counterpart. However, I mean, pervious work like LayerDrop and others omit the layer computation in the forward pass. Then the computation could be removed in the subsequent backward computation with essential engineering efforts. Thus it lacks a clear distinction in terms of technical innovation compared to these previous works.

- While the paper proposes omitting sublayer computation in the backward pass, it's unclear why the forward pass computation remains unchanged. Justifying this choice or exploring alternatives would strengthen the contribution.

- The faster convergence observed in Figure 5 with DropBP compared to the vanilla model is counterintuitive. The observation here quite confuses me since the backward pass optimizes a partial computation graph, concerns regarding overfitting arise. The paper would benefit from a discussion on potential regularization techniques employed to address this, and a comparison with related work (e.g., [1]) that utilizes sublayer dropping for regularization in training a deep Transformer model. 


  
  [1] Li et al., 2021 (AAAI) Learning Light-Weight Translation Models from Deep Transformer

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel method to reduce the computational and memory costs associated with fine-tuning large language models (LLMs). The authors introduce DropBP, a technique that randomly drops layers during backward propagation, effectively reducing the computational operations (FLOPs) and activation memory needed. This method assigns drop rates based on the sensitivity of each layer to ensure stable training. The approach is applicable to both full fine-tuning and parameter-efficient fine-tuning (PEFT) methods. The paper reports significant improvements in training time, convergence speed, and maximum sequence length when fine-tuning LLaMA2 models with DropBP.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- DropBP introduces a novel method for reducing the computational and memory costs associated with fine-tuning LLMs. This is an important contribution to the field, given the increasing size and complexity of these models.

- The paper provides empirical evidence that DropBP significantly reduces training time (by 44%), accelerates convergence (1.5× faster), and increases the maximum sequence length (up to 6.2×) on a single NVIDIA A100 GPU. These results demonstrate the effectiveness of the approach. The authors conduct thorough experiments on multiple datasets and models, providing a robust evaluation of DropBP's performance across different scenarios.

Weaknesses:
- The paper mentions that the sensitivity calculation is done only once and has negligible overhead. However, more details on this process and its potential impact on training time would provide a clearer understanding of any trade-offs involved.

- The paper could benefit from a more detailed theoretical analysis of why DropBP works as effectively as it does. This would strengthen the paper by providing a deeper understanding of the underlying principles.

Limitations:
Limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed to drop layers during backward prop (BP) based on layer sensitivity. The method aims to reduce the cost for gradient computation and storage for intermediate activation in full BP.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Reducing the cost of full BP in PEFT has been an important challenge. 
2. The method is simple and is easy to integrate to either full fine-tuning or PEFT. 
3. Experiments demonstrate that DropBP can speed up the training while retaining the accuracy. The resulting memory reduction makes longer sequence modeling accessible.

Weaknesses:
1. The idea of optimizing NNs with sparse gradient is not new. This paper needs to add more discussion and comparison with related works in sparse learning e.g., [1-3]
2. Table 1 only shows results on two datasets and limited benchmark. It is unclear if the method works well for generation tasks and domain-specific transfer learning.
3. It is unclear which algorithm is used to solve the constraint minimization problem, i.e., to determine the layer-specific rates based on sensitivity, and its extra computational cost.
4. (Minor) In fine-tuning, DropBP drops a set of layers. However, the sensitivity of a set of layers may not be accurately represented by the direct summation of the sensitivities of individual layers in the set.

[1] Sun, Xu, et al. ""meprop: Sparsified back propagation for accelerated deep learning with reduced overfitting.""
[2] Sung, Yi-Lin, Varun Nair, and Colin A. Raffel. ""Training neural networks with fixed sparse masks."" 
[3] Brock, Andrew, et al. ""Freezeout: Accelerate training by progressively freezing layers.""

Limitations:
The authors have addressed the limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wlqfOvlTQz;"REVIEW 
Summary:
This paper introduces reinforcement learning (RL) problems where agents observe one-step lookahead information (either rewards or transitions) before choosing actions in episodic tabular MDPs. Two relevant lines of work exist: the control literature, which studies a similar lookahead concept in the continuous state-space scenario, and the RL planning community, which commonly obtains lookahead information from learned transition models. However, this paper assumes the reward/transition information to be available before selecting an action. The core contributions are:

1) Formalising the look-ahead setting for the reward and transition in an episodic MDP setting.
2) Derivation of the Bellman equations in the original space by setting up an equivalence with an equivalent new MDP.
3) Development of two algorithms for reward (MVP-RL) and transition lookahead ( MVP-TL).
4) First sub-linear regret bound win the lookahead setting.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is the first to provide regret bound on the lookahead learning setting. This encompass a somewhat broad spectrum of problems that were independently studied such as the Canadian traveler problem and the prophet inequalities. 

They paper is well written and easy to follow for non-expert in learning theory. It presents the core ideas in an understandable way in the main paper and use the appendix for technical proofs.

Weaknesses:
The paper could be strengthened by adding experimental results studying the difference in behaviour and performance between standard RL algorithm, MVP and the proposed solution MVP-RL. More specifically, I would be interested in understanding the difference in behaviour when changing the tails of the reward/transition distributions.

Limitations:
The limitations outlined in the paper provide a fair representation of how the theoretical results could be extended in various directions, such as multi-step and stochastic action sets.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors proposed new forms of Bellman equations for environments where the agent knows the reward or transition outcomes one step ahead (without knowing the full model).

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
While previous papers (e.g., Boutilier et al. 2018) discussed utilizing lookahead information (and proved convergence), the authors claim they are the first to present regret results.

Weaknesses:
While the theoretical contribution is clear, the authors must also provide practical validation.

Limitations:
Some aspects of the approach might be seen as incremental advancements  rather than groundbreaking theoretical analysis .

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This manuscript proposes the RL method with lookahead information. The authors discuss two scenarios: reward lookahead and transition lookahead. Under such scenarios, the proposed method estimates the reward distribution and transition distribution, respectively. Then the monotonic value propagation skill is applied to calculate the value function. The authors show that the proposed method has strong theoretical properties and the reward regret is strongly bounded under two circumstances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The manuscript is well organized, and the structure is clear. The authors shows very promising bound for both reward lookahead and transition lookahead scenarios.

Weaknesses:
This is a theoretical paper, however, the authors miss to deliver some numerical or empirical studies. It is suggested to add some empirical experiments, at least with simulated data. 

Algorithm 1&2 shows the procedure for training, I am confused about the inference process. How to select the action give certain state in inference? The authors are suggested to give some explanations in the Algorithm 1&2.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the setting where the agent can see the possible next rewards and next states without assuming a prior knowledge of the environment dynamics. The predicted next rewards and next states are estimated by empirical distribution. The paper considers extending Monotonic Value Propagation to such a setting and proves that the proposed algorithms can achieve tight regret bounds.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A tight regret bound is proved for the proposed algorithm, establishing theoretical justification for lookahead information and advantages of planning in RL in general.
- The paper does not assume known environment dynamics as in most previous works, which makes the algorithm applicable to standard RL settings. The lack of known environment dynamics may bring various challenges to planning, such as agents not relying on the lookahead information when the estimated environment dynamics are still far from the true one in the early stages. The paper shows that the lookahead information can still be very beneficial despite such challenges.
- The paper is well-written, and the proof is easy to follow.

Weaknesses:
Even though a tight regret bound has been proved, empirical experiments with examples showing how the agent uses the lookahead information will strengthen the paper.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies an RL problem with a special setting, called one-step lookahead, where the agent can observe the reward or the state at the next step before the current action is taken. The paper focuses on the problem with an unknown environment (transition function). The authors proposed an efficient algorithms leveraging the empirical distribution of the lookahead information and claimed that the algorithms achieve tight regret against a strong baseline.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper studies an interesting RL problem where one-step lookahead information is available to the agent while the environment is unknown. 

2. The paper clearly presents the problem, the solution, and a comparison between the proposed algorithm and the baseline in terms of regret bound.

3. The paper offers explanation of the terms in the regret bounds and justified its explanation.

Weaknesses:
1. One concern is the application of such a lookahead setting. The agents during training and running needs to know what will be realized in order to make actions at the current state. Not sure what real-world scenarios this setting can be applicable to.


2. RL with lookahead information has been investigated before from a theoretical point of view. See [R1, p64]. [R2] [R3]. [R1] discusses the lookahead in the approximation of the bellman function. [R2-R3] considers controlled lookahead where the agents decide the step of lookahead as a strategy. It is not straightforward to see in this paper how the lookahead studied in this paper different from those references. 

[R1] Bertsekas, Dimitri. Reinforcement learning and optimal control. Vol. 1. Athena Scientific, 2019.
[R2] Biedenkapp, André, et al. ""TempoRL: Learning when to act."" International Conference on Machine Learning. PMLR, 2021.
[R3] Huang, Yunhan, Veeraruna Kavitha, and Quanyan Zhu. ""Continuous-time markov decision processes with controlled observations."" 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, 2019.


3.  It is not clear the source of the baseline mentioning in the paper. For example, ""compared to a stronger baseline that also has access to lookahead information"". The paper should includes the reference whenever the baseline is compared with the proposed solution.

Limitations:
Discussed in the Weakness section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces reinforcement learning (RL) problems where agents observe one-step lookahead information (either rewards or transitions) before choosing actions in episodic tabular MDPs. Two relevant lines of work exist: the control literature, which studies a similar lookahead concept in the continuous state-space scenario, and the RL planning community, which commonly obtains lookahead information from learned transition models. However, this paper assumes the reward/transition information to be available before selecting an action. The core contributions are:

1) Formalising the look-ahead setting for the reward and transition in an episodic MDP setting.
2) Derivation of the Bellman equations in the original space by setting up an equivalence with an equivalent new MDP.
3) Development of two algorithms for reward (MVP-RL) and transition lookahead ( MVP-TL).
4) First sub-linear regret bound win the lookahead setting.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is the first to provide regret bound on the lookahead learning setting. This encompass a somewhat broad spectrum of problems that were independently studied such as the Canadian traveler problem and the prophet inequalities. 

They paper is well written and easy to follow for non-expert in learning theory. It presents the core ideas in an understandable way in the main paper and use the appendix for technical proofs.

Weaknesses:
The paper could be strengthened by adding experimental results studying the difference in behaviour and performance between standard RL algorithm, MVP and the proposed solution MVP-RL. More specifically, I would be interested in understanding the difference in behaviour when changing the tails of the reward/transition distributions.

Limitations:
The limitations outlined in the paper provide a fair representation of how the theoretical results could be extended in various directions, such as multi-step and stochastic action sets.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors proposed new forms of Bellman equations for environments where the agent knows the reward or transition outcomes one step ahead (without knowing the full model).

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
While previous papers (e.g., Boutilier et al. 2018) discussed utilizing lookahead information (and proved convergence), the authors claim they are the first to present regret results.

Weaknesses:
While the theoretical contribution is clear, the authors must also provide practical validation.

Limitations:
Some aspects of the approach might be seen as incremental advancements  rather than groundbreaking theoretical analysis .

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This manuscript proposes the RL method with lookahead information. The authors discuss two scenarios: reward lookahead and transition lookahead. Under such scenarios, the proposed method estimates the reward distribution and transition distribution, respectively. Then the monotonic value propagation skill is applied to calculate the value function. The authors show that the proposed method has strong theoretical properties and the reward regret is strongly bounded under two circumstances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The manuscript is well organized, and the structure is clear. The authors shows very promising bound for both reward lookahead and transition lookahead scenarios.

Weaknesses:
This is a theoretical paper, however, the authors miss to deliver some numerical or empirical studies. It is suggested to add some empirical experiments, at least with simulated data. 

Algorithm 1&2 shows the procedure for training, I am confused about the inference process. How to select the action give certain state in inference? The authors are suggested to give some explanations in the Algorithm 1&2.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the setting where the agent can see the possible next rewards and next states without assuming a prior knowledge of the environment dynamics. The predicted next rewards and next states are estimated by empirical distribution. The paper considers extending Monotonic Value Propagation to such a setting and proves that the proposed algorithms can achieve tight regret bounds.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A tight regret bound is proved for the proposed algorithm, establishing theoretical justification for lookahead information and advantages of planning in RL in general.
- The paper does not assume known environment dynamics as in most previous works, which makes the algorithm applicable to standard RL settings. The lack of known environment dynamics may bring various challenges to planning, such as agents not relying on the lookahead information when the estimated environment dynamics are still far from the true one in the early stages. The paper shows that the lookahead information can still be very beneficial despite such challenges.
- The paper is well-written, and the proof is easy to follow.

Weaknesses:
Even though a tight regret bound has been proved, empirical experiments with examples showing how the agent uses the lookahead information will strengthen the paper.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies an RL problem with a special setting, called one-step lookahead, where the agent can observe the reward or the state at the next step before the current action is taken. The paper focuses on the problem with an unknown environment (transition function). The authors proposed an efficient algorithms leveraging the empirical distribution of the lookahead information and claimed that the algorithms achieve tight regret against a strong baseline.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper studies an interesting RL problem where one-step lookahead information is available to the agent while the environment is unknown. 

2. The paper clearly presents the problem, the solution, and a comparison between the proposed algorithm and the baseline in terms of regret bound.

3. The paper offers explanation of the terms in the regret bounds and justified its explanation.

Weaknesses:
1. One concern is the application of such a lookahead setting. The agents during training and running needs to know what will be realized in order to make actions at the current state. Not sure what real-world scenarios this setting can be applicable to.


2. RL with lookahead information has been investigated before from a theoretical point of view. See [R1, p64]. [R2] [R3]. [R1] discusses the lookahead in the approximation of the bellman function. [R2-R3] considers controlled lookahead where the agents decide the step of lookahead as a strategy. It is not straightforward to see in this paper how the lookahead studied in this paper different from those references. 

[R1] Bertsekas, Dimitri. Reinforcement learning and optimal control. Vol. 1. Athena Scientific, 2019.
[R2] Biedenkapp, André, et al. ""TempoRL: Learning when to act."" International Conference on Machine Learning. PMLR, 2021.
[R3] Huang, Yunhan, Veeraruna Kavitha, and Quanyan Zhu. ""Continuous-time markov decision processes with controlled observations."" 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, 2019.


3.  It is not clear the source of the baseline mentioning in the paper. For example, ""compared to a stronger baseline that also has access to lookahead information"". The paper should includes the reference whenever the baseline is compared with the proposed solution.

Limitations:
Discussed in the Weakness section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
veMnGKXvTx;"REVIEW 
Summary:
A Homology Consistency (HC) constraint for efficient transfer on VLMs is proposed in this paper, which explicitly constrains the correspondence of image and text latent manifolds through structural equivalence based on persistent homology in downstream tuning.
The proposed method tracks the persistence of the homology classes of topological features across multiple scales and guide the directions of persistence tracks in image and text manifolds to coincide each other. Additionally, a deviating perturbation is applied to generalize the persistence coincidence to unseen data. Experiments on recognition and generalization tasks show the superior performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper is well-written with a straightforward motivation.
2. A theoretically well-founded homology consistency (HC) constraint based on persistent homology is proposed for efficient transfer on VLMs.
3. Experiments on recognition tasks show the superior performance.

Weaknesses:
The hyper-parameters η, λ, ω should be determined at 16 shots and then migrated to other few-shot settings. If the number of samples is less than 16, how should the aforementioned hyper-parameters be set, and will there be a significant difference in performance?

Limitations:
The experiments only conducted on recognition tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper identifies a key issue with existing methods for tuning pre-trained vision-language models to downstream tasks with limited data: they adjust the alignment between image and text based solely on observed samples, which may not generalize well beyond the training data. To address this issue, the paper proposes a novel constraint from the perspective of topological data analysis.

This constraint employs persistent homology to ensure the structural equivalence of image and text latent manifolds during tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.  The paper offers a new way of looking at model tuning through the lens of topological analysis, with a focus on understanding the structure of data spaces for better semantic alignment in vision-language tasks. I appreciate this perspective on the issue.

2.  The proposed method exhibits a thoughtful theoretical underpinning, using persistent homology to enhance the generalizability of image-text alignment adjusting.&#x20;

3.  The paper is well-written and the reason for leveraging topological data analysis to enhance semantic alignment during the tuning process is reasonable and easy to follow up.

Weaknesses:
The paper does not adequately discuss how it relates to existing image and text alignment techniques, including those based on distance metrics, mutual information, adversarial training, and attention mechanisms. This lack of comparative analysis creates a gap in fully appreciating the distinctive contributions and potential advantages.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a Homology Consistency (HC) constraint for efficient transfer learning on vision-language models (VLMs), ensuring task-specific image-text alignment while preserving general knowledge by using structural equivalence based on persistent homology. This approach mimics the topology of latent manifolds and tracks the persistence of topological features.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well motivated, and the motivation of using homology consistency is interesting. 
2. This paper has a good theoretic support.

Weaknesses:
1. The performance of the proposed method is worse than the baseline method in low-shot (1-shot and 2-shot) tasks. 
2. The improvement in Table 2 is marginal. Is the comparison fair with the same random seed? How many runs did you conduct? Could the authors also report the standard deviation of the score? 
3. Moreover, is 16-shot common in this benchmark? 16 shot seems a lot in few-shot learning. 
4. Can you also elaborate more why with only DP, the performance drops in Table 3? 
5. In addition, could you elaborate more why choosing 0-th homology classes? What are the potential effects of using other homology classes?

Limitations:
My concern is the performance improvement is marginal and limited to more shots setting (16-shot).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes Homology Consistency (HC) constraint for transfer learning on VLMs, and it explicitly constrains the correspondence of image and text latent manifolds by structural equivalence based on persistent homology in downstream tuning.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is well-founded and clearly explains the proposed homology consistency (HC) constraint.

2. Extensive experiments are performed on 11 benchmark datasets.

Weaknesses:
1. The paper lacks discussions on the computational cost of the proposed techniques.

2. The proposed method for constraining the structural equivalence of image and text latent manifolds seems generalizable to other learning tasks for vision-language models. However, the proposed method is only evaluated for few-shot learning of vision language models.

3. Although the model outperforms other methods in most cases, the improvements are relatively marginal.

4. The paper only applies the method to a limited number of adapter models (TaskRes and Tip-Adapter-F).

Limitations:
The limitations are not discussed sufficiently (see weaknesses).

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
A Homology Consistency (HC) constraint for efficient transfer on VLMs is proposed in this paper, which explicitly constrains the correspondence of image and text latent manifolds through structural equivalence based on persistent homology in downstream tuning.
The proposed method tracks the persistence of the homology classes of topological features across multiple scales and guide the directions of persistence tracks in image and text manifolds to coincide each other. Additionally, a deviating perturbation is applied to generalize the persistence coincidence to unseen data. Experiments on recognition and generalization tasks show the superior performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper is well-written with a straightforward motivation.
2. A theoretically well-founded homology consistency (HC) constraint based on persistent homology is proposed for efficient transfer on VLMs.
3. Experiments on recognition tasks show the superior performance.

Weaknesses:
The hyper-parameters η, λ, ω should be determined at 16 shots and then migrated to other few-shot settings. If the number of samples is less than 16, how should the aforementioned hyper-parameters be set, and will there be a significant difference in performance?

Limitations:
The experiments only conducted on recognition tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper identifies a key issue with existing methods for tuning pre-trained vision-language models to downstream tasks with limited data: they adjust the alignment between image and text based solely on observed samples, which may not generalize well beyond the training data. To address this issue, the paper proposes a novel constraint from the perspective of topological data analysis.

This constraint employs persistent homology to ensure the structural equivalence of image and text latent manifolds during tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.  The paper offers a new way of looking at model tuning through the lens of topological analysis, with a focus on understanding the structure of data spaces for better semantic alignment in vision-language tasks. I appreciate this perspective on the issue.

2.  The proposed method exhibits a thoughtful theoretical underpinning, using persistent homology to enhance the generalizability of image-text alignment adjusting.&#x20;

3.  The paper is well-written and the reason for leveraging topological data analysis to enhance semantic alignment during the tuning process is reasonable and easy to follow up.

Weaknesses:
The paper does not adequately discuss how it relates to existing image and text alignment techniques, including those based on distance metrics, mutual information, adversarial training, and attention mechanisms. This lack of comparative analysis creates a gap in fully appreciating the distinctive contributions and potential advantages.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a Homology Consistency (HC) constraint for efficient transfer learning on vision-language models (VLMs), ensuring task-specific image-text alignment while preserving general knowledge by using structural equivalence based on persistent homology. This approach mimics the topology of latent manifolds and tracks the persistence of topological features.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well motivated, and the motivation of using homology consistency is interesting. 
2. This paper has a good theoretic support.

Weaknesses:
1. The performance of the proposed method is worse than the baseline method in low-shot (1-shot and 2-shot) tasks. 
2. The improvement in Table 2 is marginal. Is the comparison fair with the same random seed? How many runs did you conduct? Could the authors also report the standard deviation of the score? 
3. Moreover, is 16-shot common in this benchmark? 16 shot seems a lot in few-shot learning. 
4. Can you also elaborate more why with only DP, the performance drops in Table 3? 
5. In addition, could you elaborate more why choosing 0-th homology classes? What are the potential effects of using other homology classes?

Limitations:
My concern is the performance improvement is marginal and limited to more shots setting (16-shot).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes Homology Consistency (HC) constraint for transfer learning on VLMs, and it explicitly constrains the correspondence of image and text latent manifolds by structural equivalence based on persistent homology in downstream tuning.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is well-founded and clearly explains the proposed homology consistency (HC) constraint.

2. Extensive experiments are performed on 11 benchmark datasets.

Weaknesses:
1. The paper lacks discussions on the computational cost of the proposed techniques.

2. The proposed method for constraining the structural equivalence of image and text latent manifolds seems generalizable to other learning tasks for vision-language models. However, the proposed method is only evaluated for few-shot learning of vision language models.

3. Although the model outperforms other methods in most cases, the improvements are relatively marginal.

4. The paper only applies the method to a limited number of adapter models (TaskRes and Tip-Adapter-F).

Limitations:
The limitations are not discussed sufficiently (see weaknesses).

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
x2zY4hZcmg;"REVIEW 
Summary:
Naive model predictive shielding may overly restrict exploration thereby preventing an RL agent from learning a policy with good performance. In order to prevent this, the authors propose a method to optimise a backup policy that is provably safe using an online planner. An approximate model such as double integrator or differential drive is used for planning. Improvements are demonstrated on five benchmarks that involve static or dynamic obstacle avoidance as compared to provably safe and approximately safe RL methods. A provable guarantee is provided for recovery regret.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
Presentation is clear with backing proofs and demonstrable results

Problem that is being solved is clearly delineated and addressed using sound techniques

Experimental comparisons are performed rigorously with attention to detail

Weaknesses:
Literature review and comparisons are partial to the RL literature. There is a long-standing literature in control [A, B, C] to use an approximate model to plan using predictive control. A whole host of methods to learn a safety-filter/shielding on the fly has been explored with robust optimization-based offline and online control techniques. Most of these methods would implicitly solve the problem this paper is trying to address. However, it is interesting that the paper uses the Q function in the online optimization. This aspect is novel and unique to this paper.

It is unclear how much computation and time it takes to run MCTS online at each time in order to do dynamic shielding at runtime.

Dynamics model such as double integrator and differential drive are too simple. It would be interesting to see how well these would work with more complicated and/or higher-dimensional dynamics.

[A] Breeden, Joseph, and Dimitra Panagou. ""Predictive control barrier functions for online safety critical control."" 2022 IEEE 61st Conference on Decision and Control (CDC). IEEE, 2022.

[B] Wabersich, Kim P., and Melanie N. Zeilinger. ""Predictive control barrier functions: Enhanced safety mechanisms for learning-based control."" IEEE Transactions on Automatic Control 68.5 (2022): 2638-2651.

[C] Wabersich, Kim P., et al. ""Data-driven safety filters: Hamilton-jacobi reachability, control barrier functions, and predictive methods for uncertain systems."" IEEE Control Systems Magazine 43.5 (2023): 137-177.

Limitations:
The authors have discussed limitations and there is potential for the approach to scale even though the experiments in the paper are on simple examples.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new method for safety shielding. More precisely, the authors extend Model Predictive Shielding (MPS), where an agent reverts to a safe backup policy if, for the next predicted state, this policy would not be able to guarantee safety anymore. MPS is often overly conservative, particularly in cases where the backup policy is very different from the final policy (for example, it may only consider breaking, while the final policy may be able to steer around an object). To improve this, whenever an action is potentially unsafe, the agent first uses a short-horizon planner to see if there exists some safe action that may be better than the one of the backup policy (i.e., one for which the backup policy could still recover in the future, but for which our learned agent predicts a higher reward). The authors formalize this framework and show recovery regret for this framework diminishes exponentially with the horizon. Next, they show that an implementation of this framework outperforms prior methods, both in terms of performance and the number of required shield invocations.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The topic of the paper, safety shielding, is relevant and significant. Safe RL (and particularly, safety shielding) is a promising line of research but is often overly conservative in practice: the methods proposed in this paper take a step toward reducing this problem while still giving formal guarantees about safety. The topic is relevant for the NeurIPS community (particularly those interested in RL), both as a method that could immediately be used or to extend the method to more complex settings (i.e., with a stochastic/unknown model).

The paper is well-written and easy to read: the intuition behind the method is clear, and the analysis of the results is easy to follow. The framework is well formalized (using visualizations where helpful), and the given pseudo-code helps with reproducibility. The experiments are extensive and convincingly show the advantages of the proposed method.

Weaknesses:
Apart from some minor remarks that I add below, this paper has one main weakness: it does not clearly indicate the computational complexity of its method nor the scalability. The results do not show computation times, and (as far as I could tell) no mention is made of either the average planning time or some time limit for this planning phase. From some ball-parking, the additional time required for this method may be significant (solving up to millions of short-horizon planning problems), so a quantification of this computational cost should be provided.

Some more minor remarks:
* The paper only mentions how the framework is implemented (i.e., what RL & planning method it uses) in the appendix: it would be nice to (briefly) mention this in the results section as well;
* In Table 2, the results of CPO and TD3 are not bold, even though some are equal to those of the best frameworks: this should be fixed;
* One limitation of the proposed framework is that it assumes the environment is deterministic: it would be nice to mention this in the limitations section.

Limitations:
Limitations are adequately addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce Dynamic Model Predictive Shielding (DMPS) an extension of Model Predictive Sheilding (MPS) that adress some of its key limitations, such as overconservatism when deploying the backup policy which consequently hinders exploration of the neural 'task' agent and slows down conergence. The key innovation of DMPS is that it incoropoates a local planner for dynamic recovery that leverages both a pre-computed backup policy and the neural 'task' policies Q values for planning for short and long horizon returns while maintaining safety of the system. DMPS is a provably safe reinforcement learning (PSRL) method, meaning that it guarantees safety of the system (regardless of the underlying neural 'task' policy) by only exploring within the set of safe and recoverable states defined by the backup policy.  This realised by planning for a fixed n step trajectory and checking whether the system can be recovered by the backup policy given the action proposed by the agent. The authors demonstrate that DMPS outperforms MPS and other baselines in terms of performance and safety in various benchmarks. It also emphasizes the importance of aligning reinforcement learning agents with real-world safety requirements, while discussing some of the limitations of their approach.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper has several strengths: I find that the paper is very well written and easy to follow, with sufficient details in necessary places and abstractions in other places where the details may not immediately matter, as such, it is a very nice read. The theoretical analysis of the recovery regret is convincing and interesting. Furthermore, the overall framework is attractive from the point of view that it is provably safe, something I personally find is crucial for deploying RL in the real world, rather than safe at convergence or in expectation like a lot safe RL methods. I find that the dynamic planning module is an innovative solution to the intuitive issue faced by most shielding methods (Figure 2) and I feel that this work constitutes a step in the right direction for improving shielding methods and making them more practical. The experimental evaluation I feel is strong and thorough as in most cases DMPS clearly outperforms MPS and REVEL, although I think it is missing something (see weaknesses).

Weaknesses:
The key weakness of the PSRL framework is the reliance on a perfect (or sufficiently accurate) dynamics model of the environment, the safety performance of the backup policy and the computation of the safe invariant set. In contrast to the first shielding approaches for RL [1], which operate primarly on discrete state and actions spaces, DMPS does not need to compute the shield before learning can begining which significantly reduces the engineering overhead before training. This of course comes at a cost, in practice the shields in [1] are substatially more lightweight during ""inference"", (although in theory there could be exponential blow up) in part due to only operating on discrete or discretized state/action spaces but also because a lot of the necessary computation is done before hand. This is a key limitation of DMPS as it relys on planning at each timestep which might be costly and infeasible for on-board computation or edge devices. Fruthermore, it seems that there is still a significant amount of prior knowledge required for DMPS to work effectively, first we have to have a ""perfect"" dynamics model (for provable guarantees) secondly I presume we need to handcraft a backup policy and then compute its invariant safe set so as to plan for recoverability. The first limitation is mentioned in the paper but not really discussed in much detail, the second limitation is find is crucial and I don't think is really mentioned in the paper. In particular it is a non-trivial challenge to come up with a backup policy that has a maximal safe invariant set, perhaps for the environments the authors consider it is easy (just decelerate) but for more dynamics environments and in general this is not the case and I feel like more discussion about both these limitations (i.e. the limitations of the PSRL setting) is needed. 

While I find the experimental evaluation compelling I feel it is slightly misleading and it is missing something. In Table 2 CPO and TD3 score the same or higher in a few of the static benchmarks but there scores are not in bold, is there a reason for this that I am missing? I also feel like a comparison to PPO-Lag or DDPG-Lag would really help make the results that bit more convincing.

All that being said, in principle I advocate for acceptance of this paper.

[1] Alshiekh, Mohammed, et al. ""Safe reinforcement learning via shielding."" Proceedings of the AAAI conference on artificial intelligence. Vol. 32. No. 1. 2018.

Limitations:
See weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The approach called dynamic model-predictive shielding for safe reinforcement learning is proposed as an improvement over its static counterpart. The main idea is to optimize for expected return on action with respect to the reinforcement-learning task when choosing a shielding backup action, and to incorporate planning horizon prediction into learning for the policy to learn to avoid unsafe actions. This dynamic version is evaluated on several static and dynamic obstacle-avoidance benchmarks and compared to static model-predictive shielding and three more planning-based approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The core idea of the approach is interesting and potentially valuable: to achieve synergy between safety and optimal performance in model-predictive shielding via incorporating planning into policy learning and taking expected performance into account during backup planning. Similar attempts have been done previously. In comparison, this work proposes a novel notion of ""recovery regret"" as a heuristic to guide mutual integration of planning and reinforcement learning. 

The strength of the paper is in extensive evaluation and comparison to multiple approaches. The notion of recovery regret can also be of independent interest for model-predictive shielding research. Dynamic shielding outperforms other approaches in the evaluation in terms of the number of shielding invocations, which indicates synergy between planning and learning over time.

Weaknesses:
Potential weaknesses of the approach are in scalability of the planner and tightness of the probabilistic bounds on safety.

Minor:
- ""more optimal recovery approach"" --> an optimal/a better

Limitations:
The authors explicitly discuss limitations which are fairly common for problem domain.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper seeks to address provably safe RL problems where safety must be ensured even during training. It proposed DMPS, which enhances prior Model Predictive Shielding approach, to dynamically select safe actions when danger is imminent. DMPS employs local planner to plan for recovery actions and the planner objective consists of both short-term and long-term rewards. Feedback from the planner can then be used to incrementally train the neural policy to guide it towards safe policy set.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Quality
* Overall, the approach described in the paper is sound and it combines many established components (e.g. backup policy, local planner, estimate future reward using model unrolling and Q-estimate) to facilitate safe RL.  
* The paper provides theoretical bound on the recovery regret as the sampling limit in the local planner approaches inifinity.
2. Clarity
* The paper is written in a clear and lucid manner. The figures, algorithm and equations are structured in a way that is easily understandable to the readers.

Weaknesses:
1. Originality
*  The main difference between DMPS and MPS is the use of local planner when backup policy is triggered. The technical approach used in DMPS is not particularly new as there are already some similar approaches of estimating a safety Q-value and perform planning based on the Q-value [1, 2].
2. Significance
* The only difference between DMPS and the prior MPS seems to be the local planner and (as discussed in point 1) this local planner is not particularly novel. Having said that, I do agree that the proposed DMPS does show improvement over MPS in some experiment scenarios.
* While the paper mentions a small planning horizon is sufficient for the local planner to plan safe yet rewarding actions, I feel that this may not be true in most cases. To steer the agent back to safety (and yet rewarding), a long sequence of actions may be required. If the planning horizon is set too small, then DMPS falls back to backup policy and the performance would be the same as MPS. In this case, I guess the only solution is to increase in planning horizon and in turn increase the computational overhead of DMPS exponentially.
* The local planner requires perfect information of the transition and the transition must be deterministic. This may restrict its applicability, especially given that there're prior work on model-based RL where transition can be stochastic and learned instead.  

References  
[1] Clavera, I., Fu, Y. and Abbeel, P., Model-Augmented Actor-Critic: Backpropagating through Paths. In International Conference on Learning Representations.  
[2] Thomas, G., Luo, Y. and Ma, T., 2021. Safe reinforcement learning by imagining the near future. Advances in Neural Information Processing Systems, 34, pp.13859-13869.  
[3] Nagabandi, A., Kahn, G., Fearing, R.S. and Levine, S., 2018, May. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE international conference on robotics and automation (ICRA) (pp. 7559-7566). IEEE.  
[4] Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems. 2018.  
[5] Janner, M., Fu, J., Zhang, M. and Levine, S., 2019. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32.

Limitations:
The paper discussed the known environment model as its limitation. I agree that this is a limitation which warrants further investigation. As studied in [5], it is very challenging for a model to accurately predict future trajectories with long horizon. Since DMPS relies on having an accurate environment model for safety adherence, this may limit its applicability to practical scenarios where environment model is not given and needs to be learned.  

Another related point is that it is unclear what value to set for the local planner horizon. Different tasks may require corrective action sequence of different lengths. Setting the horizon too short may revert DMPS performance back to MPS and setting the horizon too long may increase the computational overhead beyond acceptable threshold.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Naive model predictive shielding may overly restrict exploration thereby preventing an RL agent from learning a policy with good performance. In order to prevent this, the authors propose a method to optimise a backup policy that is provably safe using an online planner. An approximate model such as double integrator or differential drive is used for planning. Improvements are demonstrated on five benchmarks that involve static or dynamic obstacle avoidance as compared to provably safe and approximately safe RL methods. A provable guarantee is provided for recovery regret.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
Presentation is clear with backing proofs and demonstrable results

Problem that is being solved is clearly delineated and addressed using sound techniques

Experimental comparisons are performed rigorously with attention to detail

Weaknesses:
Literature review and comparisons are partial to the RL literature. There is a long-standing literature in control [A, B, C] to use an approximate model to plan using predictive control. A whole host of methods to learn a safety-filter/shielding on the fly has been explored with robust optimization-based offline and online control techniques. Most of these methods would implicitly solve the problem this paper is trying to address. However, it is interesting that the paper uses the Q function in the online optimization. This aspect is novel and unique to this paper.

It is unclear how much computation and time it takes to run MCTS online at each time in order to do dynamic shielding at runtime.

Dynamics model such as double integrator and differential drive are too simple. It would be interesting to see how well these would work with more complicated and/or higher-dimensional dynamics.

[A] Breeden, Joseph, and Dimitra Panagou. ""Predictive control barrier functions for online safety critical control."" 2022 IEEE 61st Conference on Decision and Control (CDC). IEEE, 2022.

[B] Wabersich, Kim P., and Melanie N. Zeilinger. ""Predictive control barrier functions: Enhanced safety mechanisms for learning-based control."" IEEE Transactions on Automatic Control 68.5 (2022): 2638-2651.

[C] Wabersich, Kim P., et al. ""Data-driven safety filters: Hamilton-jacobi reachability, control barrier functions, and predictive methods for uncertain systems."" IEEE Control Systems Magazine 43.5 (2023): 137-177.

Limitations:
The authors have discussed limitations and there is potential for the approach to scale even though the experiments in the paper are on simple examples.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new method for safety shielding. More precisely, the authors extend Model Predictive Shielding (MPS), where an agent reverts to a safe backup policy if, for the next predicted state, this policy would not be able to guarantee safety anymore. MPS is often overly conservative, particularly in cases where the backup policy is very different from the final policy (for example, it may only consider breaking, while the final policy may be able to steer around an object). To improve this, whenever an action is potentially unsafe, the agent first uses a short-horizon planner to see if there exists some safe action that may be better than the one of the backup policy (i.e., one for which the backup policy could still recover in the future, but for which our learned agent predicts a higher reward). The authors formalize this framework and show recovery regret for this framework diminishes exponentially with the horizon. Next, they show that an implementation of this framework outperforms prior methods, both in terms of performance and the number of required shield invocations.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The topic of the paper, safety shielding, is relevant and significant. Safe RL (and particularly, safety shielding) is a promising line of research but is often overly conservative in practice: the methods proposed in this paper take a step toward reducing this problem while still giving formal guarantees about safety. The topic is relevant for the NeurIPS community (particularly those interested in RL), both as a method that could immediately be used or to extend the method to more complex settings (i.e., with a stochastic/unknown model).

The paper is well-written and easy to read: the intuition behind the method is clear, and the analysis of the results is easy to follow. The framework is well formalized (using visualizations where helpful), and the given pseudo-code helps with reproducibility. The experiments are extensive and convincingly show the advantages of the proposed method.

Weaknesses:
Apart from some minor remarks that I add below, this paper has one main weakness: it does not clearly indicate the computational complexity of its method nor the scalability. The results do not show computation times, and (as far as I could tell) no mention is made of either the average planning time or some time limit for this planning phase. From some ball-parking, the additional time required for this method may be significant (solving up to millions of short-horizon planning problems), so a quantification of this computational cost should be provided.

Some more minor remarks:
* The paper only mentions how the framework is implemented (i.e., what RL & planning method it uses) in the appendix: it would be nice to (briefly) mention this in the results section as well;
* In Table 2, the results of CPO and TD3 are not bold, even though some are equal to those of the best frameworks: this should be fixed;
* One limitation of the proposed framework is that it assumes the environment is deterministic: it would be nice to mention this in the limitations section.

Limitations:
Limitations are adequately addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce Dynamic Model Predictive Shielding (DMPS) an extension of Model Predictive Sheilding (MPS) that adress some of its key limitations, such as overconservatism when deploying the backup policy which consequently hinders exploration of the neural 'task' agent and slows down conergence. The key innovation of DMPS is that it incoropoates a local planner for dynamic recovery that leverages both a pre-computed backup policy and the neural 'task' policies Q values for planning for short and long horizon returns while maintaining safety of the system. DMPS is a provably safe reinforcement learning (PSRL) method, meaning that it guarantees safety of the system (regardless of the underlying neural 'task' policy) by only exploring within the set of safe and recoverable states defined by the backup policy.  This realised by planning for a fixed n step trajectory and checking whether the system can be recovered by the backup policy given the action proposed by the agent. The authors demonstrate that DMPS outperforms MPS and other baselines in terms of performance and safety in various benchmarks. It also emphasizes the importance of aligning reinforcement learning agents with real-world safety requirements, while discussing some of the limitations of their approach.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper has several strengths: I find that the paper is very well written and easy to follow, with sufficient details in necessary places and abstractions in other places where the details may not immediately matter, as such, it is a very nice read. The theoretical analysis of the recovery regret is convincing and interesting. Furthermore, the overall framework is attractive from the point of view that it is provably safe, something I personally find is crucial for deploying RL in the real world, rather than safe at convergence or in expectation like a lot safe RL methods. I find that the dynamic planning module is an innovative solution to the intuitive issue faced by most shielding methods (Figure 2) and I feel that this work constitutes a step in the right direction for improving shielding methods and making them more practical. The experimental evaluation I feel is strong and thorough as in most cases DMPS clearly outperforms MPS and REVEL, although I think it is missing something (see weaknesses).

Weaknesses:
The key weakness of the PSRL framework is the reliance on a perfect (or sufficiently accurate) dynamics model of the environment, the safety performance of the backup policy and the computation of the safe invariant set. In contrast to the first shielding approaches for RL [1], which operate primarly on discrete state and actions spaces, DMPS does not need to compute the shield before learning can begining which significantly reduces the engineering overhead before training. This of course comes at a cost, in practice the shields in [1] are substatially more lightweight during ""inference"", (although in theory there could be exponential blow up) in part due to only operating on discrete or discretized state/action spaces but also because a lot of the necessary computation is done before hand. This is a key limitation of DMPS as it relys on planning at each timestep which might be costly and infeasible for on-board computation or edge devices. Fruthermore, it seems that there is still a significant amount of prior knowledge required for DMPS to work effectively, first we have to have a ""perfect"" dynamics model (for provable guarantees) secondly I presume we need to handcraft a backup policy and then compute its invariant safe set so as to plan for recoverability. The first limitation is mentioned in the paper but not really discussed in much detail, the second limitation is find is crucial and I don't think is really mentioned in the paper. In particular it is a non-trivial challenge to come up with a backup policy that has a maximal safe invariant set, perhaps for the environments the authors consider it is easy (just decelerate) but for more dynamics environments and in general this is not the case and I feel like more discussion about both these limitations (i.e. the limitations of the PSRL setting) is needed. 

While I find the experimental evaluation compelling I feel it is slightly misleading and it is missing something. In Table 2 CPO and TD3 score the same or higher in a few of the static benchmarks but there scores are not in bold, is there a reason for this that I am missing? I also feel like a comparison to PPO-Lag or DDPG-Lag would really help make the results that bit more convincing.

All that being said, in principle I advocate for acceptance of this paper.

[1] Alshiekh, Mohammed, et al. ""Safe reinforcement learning via shielding."" Proceedings of the AAAI conference on artificial intelligence. Vol. 32. No. 1. 2018.

Limitations:
See weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The approach called dynamic model-predictive shielding for safe reinforcement learning is proposed as an improvement over its static counterpart. The main idea is to optimize for expected return on action with respect to the reinforcement-learning task when choosing a shielding backup action, and to incorporate planning horizon prediction into learning for the policy to learn to avoid unsafe actions. This dynamic version is evaluated on several static and dynamic obstacle-avoidance benchmarks and compared to static model-predictive shielding and three more planning-based approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The core idea of the approach is interesting and potentially valuable: to achieve synergy between safety and optimal performance in model-predictive shielding via incorporating planning into policy learning and taking expected performance into account during backup planning. Similar attempts have been done previously. In comparison, this work proposes a novel notion of ""recovery regret"" as a heuristic to guide mutual integration of planning and reinforcement learning. 

The strength of the paper is in extensive evaluation and comparison to multiple approaches. The notion of recovery regret can also be of independent interest for model-predictive shielding research. Dynamic shielding outperforms other approaches in the evaluation in terms of the number of shielding invocations, which indicates synergy between planning and learning over time.

Weaknesses:
Potential weaknesses of the approach are in scalability of the planner and tightness of the probabilistic bounds on safety.

Minor:
- ""more optimal recovery approach"" --> an optimal/a better

Limitations:
The authors explicitly discuss limitations which are fairly common for problem domain.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper seeks to address provably safe RL problems where safety must be ensured even during training. It proposed DMPS, which enhances prior Model Predictive Shielding approach, to dynamically select safe actions when danger is imminent. DMPS employs local planner to plan for recovery actions and the planner objective consists of both short-term and long-term rewards. Feedback from the planner can then be used to incrementally train the neural policy to guide it towards safe policy set.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Quality
* Overall, the approach described in the paper is sound and it combines many established components (e.g. backup policy, local planner, estimate future reward using model unrolling and Q-estimate) to facilitate safe RL.  
* The paper provides theoretical bound on the recovery regret as the sampling limit in the local planner approaches inifinity.
2. Clarity
* The paper is written in a clear and lucid manner. The figures, algorithm and equations are structured in a way that is easily understandable to the readers.

Weaknesses:
1. Originality
*  The main difference between DMPS and MPS is the use of local planner when backup policy is triggered. The technical approach used in DMPS is not particularly new as there are already some similar approaches of estimating a safety Q-value and perform planning based on the Q-value [1, 2].
2. Significance
* The only difference between DMPS and the prior MPS seems to be the local planner and (as discussed in point 1) this local planner is not particularly novel. Having said that, I do agree that the proposed DMPS does show improvement over MPS in some experiment scenarios.
* While the paper mentions a small planning horizon is sufficient for the local planner to plan safe yet rewarding actions, I feel that this may not be true in most cases. To steer the agent back to safety (and yet rewarding), a long sequence of actions may be required. If the planning horizon is set too small, then DMPS falls back to backup policy and the performance would be the same as MPS. In this case, I guess the only solution is to increase in planning horizon and in turn increase the computational overhead of DMPS exponentially.
* The local planner requires perfect information of the transition and the transition must be deterministic. This may restrict its applicability, especially given that there're prior work on model-based RL where transition can be stochastic and learned instead.  

References  
[1] Clavera, I., Fu, Y. and Abbeel, P., Model-Augmented Actor-Critic: Backpropagating through Paths. In International Conference on Learning Representations.  
[2] Thomas, G., Luo, Y. and Ma, T., 2021. Safe reinforcement learning by imagining the near future. Advances in Neural Information Processing Systems, 34, pp.13859-13869.  
[3] Nagabandi, A., Kahn, G., Fearing, R.S. and Levine, S., 2018, May. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE international conference on robotics and automation (ICRA) (pp. 7559-7566). IEEE.  
[4] Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems. 2018.  
[5] Janner, M., Fu, J., Zhang, M. and Levine, S., 2019. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32.

Limitations:
The paper discussed the known environment model as its limitation. I agree that this is a limitation which warrants further investigation. As studied in [5], it is very challenging for a model to accurately predict future trajectories with long horizon. Since DMPS relies on having an accurate environment model for safety adherence, this may limit its applicability to practical scenarios where environment model is not given and needs to be learned.  

Another related point is that it is unclear what value to set for the local planner horizon. Different tasks may require corrective action sequence of different lengths. Setting the horizon too short may revert DMPS performance back to MPS and setting the horizon too long may increase the computational overhead beyond acceptable threshold.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
x2780VcMOI;"REVIEW 
Summary:
Whereas prior work (Hewitt and Manning 2018) probed syntactic distance and depth, this work proposed to push that forward by also probing headedness and dependency type.  Specifically, this doesn't separately probe those three, but aims for a single vector space where euclidean distance defines syntactic distance but the difference vector maps to a UD dependency label (optimized with a contrastive learning objective).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
It is a pretty well-written paper, and the framing of the angular probe idea seems well explained and to have some elegance to it (in aiming for a single underlying representation); parts of the implementation seem well-considered to get that single representation.

Weaknesses:
- If viewed merely as a combination of probing structure and labeling, it is very similar to a work like Muller-Eberstein et al. 2022.   The advantage of this paper -  having more of a shared representation -- is appealing, but I wish the consequences of that shared space were better explored.
- Analysis was somewhat lacking: for a probing paper, there were relatively work showing what this tells us about the syntactic behavior of models.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Previous work introduced linear probes to explore how syntactic relationships are encoded in LLM embeddings.  This work aims to take it a step further and examine how types of syntactic relationships are encoded in the LLMs.  They introduce a polar probe that when optimized can predict the type of syntactic relations via the angle between them.  In a multi-faceted evaluation, the model outperforms baselines (which are essentially ablations of the model) in terms of stronger cosine similarity between the same relations, and in terms of tree accuracy (UAS/LAS).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- An interesting paper with a clear contribution, building on existing probing work while asking a couple new research questions

- The results appear convincing

- The potential to explore syntax through the lens of LLMs, especially when LLMs can be easily trained on unlabeled text, or especially when LLMs are increasingly multilingual, points to some exciting future directions.

- The evaluation also includes some linguistically interesting example cases.  Essentially exactly what I would have asked for (in addition to the larger corpora studies)

Weaknesses:
- I find the distinction between probing and parsing to be not entirely clear.  At the point where the evaluation is in terms of UAS/LAS, could this not be compared directly to parsers on the same data (especially since building on top of LLM embeddings would be the most performant solution)?  And where would the discrepancies be, and what would that mean?  Do LLMs not encode those relationships?

- In general the paper seems to suffer from a lack of convincing baselines.  The baselines presented -- the structural or angular probe, are steps along the path to the polar probe.

- Cosine similarity between identical syntactic categories is surprisingly low (to me).  The ranking of categories in terms of the strength of that correlation is also surprising, ith things like 'case' being quite strong.  In general there are many ""odd"" patterns that I don't have an intuitive explanation for why they occur, and aren't discussed in detail in this work.

- There is no dedicated related work.  I do think the parsing literature, and especially the parsing-on-top-of-LLMs literature is relevant.

Limitations:
Yes, the limitations are described.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes polar probes, a kind of structural probe that learns a distance and rotation function that can more accurately classify syntactic structure from language model representations than previous approaches. In particular, the question of whether direction can represent the type of syntactic relationship is answered. The authors find that their polar probe outperforms previous methods quite significantly and show

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is very well presented and a pleasure to read. The empirical findings are strong and clearly support the hypothesis that the direction, as well as the angle of the representations of an LM projected on a plane represent the syntactic relationships encoded by the model. The authors show that this interpretation is able to much more strongly reconstruct ground truth syntactic parses from hidden state representations than structured probes. The controlled dataset provides a clean comparison in a challenging setting and is a useful resource for future work. A major finding of this work is that it vastly raises the upper bar for how well we should consider syntax to be encoded by language models.

Weaknesses:
Weaknesses like the focus on dependency parses and drawbacks of current tokenizers are addressed in limitations, but are still weaknesses nonetheless.

Please include  UUAS, LAS and Balanced Accuracy for the evaluation on the controlled dataset separately for comparison.

As thorough as this paper is, I think it could go deeper on the model analysis. It's nice that the layer-wise analysis is consistent with previous work, but this would be mostly expected. For example, could the authors show that models of different sizes capture more/less syntactic complexity? Is there a critical point where syntax becomes well represented and gains are diminishing after more scaling? Do larger models capture more of the ""tail"" of rare syntactic constructions? This could be carried out on the GPT2 or Pythia family of models.

Nits:

- please make the text in the legend/axis labels for figure 3 bigger

- Typo L36: ""proposed settle""

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes polar probes, a kind of structural probe that learns a distance and rotation function that can more accurately classify syntactic structure from language model representations than previous approaches. In particular, the question of whether direction can represent the type of syntactic relationship is answered. The authors find that their polar probe outperforms previous methods quite significantly and show

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is very well presented and a pleasure to read. The empirical findings are strong and clearly support the hypothesis that the direction, as well as the angle of the representations of an LM projected on a plane represent the syntactic relationships encoded by the model. The authors show that this interpretation is able to much more strongly reconstruct ground truth syntactic parses from hidden state representations than structured probes. The controlled dataset provides a clean comparison in a challenging setting and is a useful resource for future work. A major finding of this work is that it vastly raises the upper bar for how well we should consider syntax to be encoded by language models.

Weaknesses:
Weaknesses like the focus on dependency parses and drawbacks of current tokenizers are addressed in limitations, but are still weaknesses nonetheless.

Please include  UUAS, LAS and Balanced Accuracy for the evaluation on the controlled dataset separately for comparison.

As thorough as this paper is, I think it could go deeper on the model analysis. It's nice that the layer-wise analysis is consistent with previous work, but this would be mostly expected. For example, could the authors show that models of different sizes capture more/less syntactic complexity? Is there a critical point where syntax becomes well represented and gains are diminishing after more scaling? Do larger models capture more of the ""tail"" of rare syntactic constructions? This could be carried out on the GPT2 or Pythia family of models.

Nits:

- please make the text in the legend/axis labels for figure 3 bigger

- Typo L36: ""proposed settle""

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Whereas prior work (Hewitt and Manning 2018) probed syntactic distance and depth, this work proposed to push that forward by also probing headedness and dependency type.  Specifically, this doesn't separately probe those three, but aims for a single vector space where euclidean distance defines syntactic distance but the difference vector maps to a UD dependency label (optimized with a contrastive learning objective).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
It is a pretty well-written paper, and the framing of the angular probe idea seems well explained and to have some elegance to it (in aiming for a single underlying representation); parts of the implementation seem well-considered to get that single representation.

Weaknesses:
- If viewed merely as a combination of probing structure and labeling, it is very similar to a work like Muller-Eberstein et al. 2022.   The advantage of this paper -  having more of a shared representation -- is appealing, but I wish the consequences of that shared space were better explored.
- Analysis was somewhat lacking: for a probing paper, there were relatively work showing what this tells us about the syntactic behavior of models.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Previous work introduced linear probes to explore how syntactic relationships are encoded in LLM embeddings.  This work aims to take it a step further and examine how types of syntactic relationships are encoded in the LLMs.  They introduce a polar probe that when optimized can predict the type of syntactic relations via the angle between them.  In a multi-faceted evaluation, the model outperforms baselines (which are essentially ablations of the model) in terms of stronger cosine similarity between the same relations, and in terms of tree accuracy (UAS/LAS).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- An interesting paper with a clear contribution, building on existing probing work while asking a couple new research questions

- The results appear convincing

- The potential to explore syntax through the lens of LLMs, especially when LLMs can be easily trained on unlabeled text, or especially when LLMs are increasingly multilingual, points to some exciting future directions.

- The evaluation also includes some linguistically interesting example cases.  Essentially exactly what I would have asked for (in addition to the larger corpora studies)

Weaknesses:
- I find the distinction between probing and parsing to be not entirely clear.  At the point where the evaluation is in terms of UAS/LAS, could this not be compared directly to parsers on the same data (especially since building on top of LLM embeddings would be the most performant solution)?  And where would the discrepancies be, and what would that mean?  Do LLMs not encode those relationships?

- In general the paper seems to suffer from a lack of convincing baselines.  The baselines presented -- the structural or angular probe, are steps along the path to the polar probe.

- Cosine similarity between identical syntactic categories is surprisingly low (to me).  The ranking of categories in terms of the strength of that correlation is also surprising, ith things like 'case' being quite strong.  In general there are many ""odd"" patterns that I don't have an intuitive explanation for why they occur, and aren't discussed in detail in this work.

- There is no dedicated related work.  I do think the parsing literature, and especially the parsing-on-top-of-LLMs literature is relevant.

Limitations:
Yes, the limitations are described.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wzof7Y66xs;"REVIEW 
Summary:
The paper introduces a hierarchical selective classification technique that incorporates hierarchical risk and coverage. The authors additionally proposed an algorithm that guarantees target accuracy. Experimental results demonstrate the method's effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Hierarchical selective classification is a new area and therefore the current method is one of the first techniques to deal with such problem. Its application to critical settings can be substantial.

Weaknesses:
•	The need of a prior tree among classes can limit its usage for complex scenarios. The construction of such tree can be a non-trivial step for the applicability of the approach. 

•	The main contribution looks an extension of previous methods for the hierarchical case.

Limitations:
•	The requirement for a hierarchical class tree prior to using the method is a limitation.

•	The current experimental section is limited in terms of baseline approaches and datasets, making it challenging to gain a comprehensive understanding from the existing experiments.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose hierarchical selective classification, a method that selects the hierarchical granularity of its prediction based on uncertainty.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is well-written, and the proposed method is quite intuitive.
* I like the idea that if uncertain, it makes sense to predict at a higher level of granularity.
* The theoretical results & statements are sound.
* Extensive experimental results showing the superiority of the proposed method to DARTS and a non-hierarchical baseline.
* Applicability to pre-trained models

Weaknesses:
* My biggest uncertainty is the similarity of this work to conformal prediction. To me, it seems that this method is very similar to conformal prediction, where the set of possible prediction sets is restricted via this pre-defined hierarchy. While, as far as I know, it has not been explored, it decreases the perceived novelty. 
* A weakness of the setting rather than the method is that it assumes the knowledge of the underlying hierarchy. As such, the applicability is somewhat limited. The paper would benefit from a way to unsupervisedly learn this hierarchy, e.g. based on classes whose predicted probabilities are positively correlated.
* As also touched upon in the concluding remarks, the method is post-hoc rather than being optimized during training, thus, likely not performing up to the highest possible level.
* Minor: Line 158-159 is a worded badly, similar to ""... thus, we do A. Unlike others that do A, we do A+B"".

Limitations:
Limitations are adequately discussed apart from the necessity of assuming knowledge of the underlying class hierarchy.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework for selective classification called hierarchical selective classification. In a setting where a hierarchy in the classification task is present, the authors devise a selection strategy that considers confidence at different levels of the classification hierarchy. Extensive experimental analysis is performed over 1115 ImageNet classifiers.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The main strengths of the paper are:

1. the idea of applying selective classification in a hierarchical setting is novel;
2. the theoretical analysis relies on conformal prediction, which guarantees the soundness of the results;
3. the proposed framework can impact high-risk settings, as shown in the healthcare example.

Weaknesses:
Overall, I think the paper is solid. My main concern is that the empirical evaluation could be improved, especially regarding motivations and attention to detail. 
A few examples:
* I do not fully understand why the authors focus so much on showing how different training regimes affect HSC performance. I guess this improves the overall predictive performance of the (hierarchical) classifier, which is expected to impact the HSC task positively.
* As the authors correctly claim, the training regimes were not optimized for hierarchical selective classification. Despite the clear computation-wise motivation, I argue that including regimes optimized for HSC would make the empirical evaluation sounder.
* a few lines are off: for instance, I would argue that line 279, i.e.,
>CLIP achieves an exceptional improvement, surpassing 40%
>
 does not match what is shown in Figure 4 (which shows an improvement below 40%).

Limitations:
The paper briefly discusses limitations. I think this section could be expanded, e.g. considering Q1.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes an extension of selective classification following a class hierarchy to reduce the specificity of model prediction when there is a high uncertainty. In particular, if the prediction confidence of a class is smaller than a predefined threshold, the proposed algorithm would proceed towards a higher class level in the hierarchical structure (the parent node), until the confidence of the considering node exceeds that threshold. The paper also formulises hierarchical risk and coverage, so that the area under curve can be used as a metric to benchmark different selective classification methods. An extensive number of pretrained classifiers on ImageNet dataset are then used to evaluate the proposed method and show promising results. The paper also include a PAC-like theoretical result, so that when finding the optimal threshold, one can select appropriate hyper-parameters to achieve their desired outcome with certain confidence level.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper goes into details to provide an adequate background about selective classification, the definition of heirarchical risk and coverage as well as its area under curve as a metric to quantify the performance of hierarchical-based selective classification. It also links to previous studies in the same subfield. In general, the paper is well written and easy to follow.

The paper also includes a theoretical result on the guarantee of the learning algorithm when one wants to find the optimal thresholding value for their hierarchical selective classification. This simple theoretical results does strengthen the paper.

The paper also include an extensive number of experiments and ablation studies to provide insights into the newly-proposed method.

Weaknesses:
The paper relies on the setting with the following assumptions:
- It is an inference rule. This means that the algorithm is used at test time only. If this could be even integrated into training is a plus.
- It needs a validation set to find the optimal hyper-parameter $\theta$, or the threshold (partly mentioned in the conclusion). It is understandable because there is no training involve here, so there is a need for that. However, in some cases, there may not be additional data available.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a hierarchical selective classification technique that incorporates hierarchical risk and coverage. The authors additionally proposed an algorithm that guarantees target accuracy. Experimental results demonstrate the method's effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Hierarchical selective classification is a new area and therefore the current method is one of the first techniques to deal with such problem. Its application to critical settings can be substantial.

Weaknesses:
•	The need of a prior tree among classes can limit its usage for complex scenarios. The construction of such tree can be a non-trivial step for the applicability of the approach. 

•	The main contribution looks an extension of previous methods for the hierarchical case.

Limitations:
•	The requirement for a hierarchical class tree prior to using the method is a limitation.

•	The current experimental section is limited in terms of baseline approaches and datasets, making it challenging to gain a comprehensive understanding from the existing experiments.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose hierarchical selective classification, a method that selects the hierarchical granularity of its prediction based on uncertainty.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is well-written, and the proposed method is quite intuitive.
* I like the idea that if uncertain, it makes sense to predict at a higher level of granularity.
* The theoretical results & statements are sound.
* Extensive experimental results showing the superiority of the proposed method to DARTS and a non-hierarchical baseline.
* Applicability to pre-trained models

Weaknesses:
* My biggest uncertainty is the similarity of this work to conformal prediction. To me, it seems that this method is very similar to conformal prediction, where the set of possible prediction sets is restricted via this pre-defined hierarchy. While, as far as I know, it has not been explored, it decreases the perceived novelty. 
* A weakness of the setting rather than the method is that it assumes the knowledge of the underlying hierarchy. As such, the applicability is somewhat limited. The paper would benefit from a way to unsupervisedly learn this hierarchy, e.g. based on classes whose predicted probabilities are positively correlated.
* As also touched upon in the concluding remarks, the method is post-hoc rather than being optimized during training, thus, likely not performing up to the highest possible level.
* Minor: Line 158-159 is a worded badly, similar to ""... thus, we do A. Unlike others that do A, we do A+B"".

Limitations:
Limitations are adequately discussed apart from the necessity of assuming knowledge of the underlying class hierarchy.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework for selective classification called hierarchical selective classification. In a setting where a hierarchy in the classification task is present, the authors devise a selection strategy that considers confidence at different levels of the classification hierarchy. Extensive experimental analysis is performed over 1115 ImageNet classifiers.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The main strengths of the paper are:

1. the idea of applying selective classification in a hierarchical setting is novel;
2. the theoretical analysis relies on conformal prediction, which guarantees the soundness of the results;
3. the proposed framework can impact high-risk settings, as shown in the healthcare example.

Weaknesses:
Overall, I think the paper is solid. My main concern is that the empirical evaluation could be improved, especially regarding motivations and attention to detail. 
A few examples:
* I do not fully understand why the authors focus so much on showing how different training regimes affect HSC performance. I guess this improves the overall predictive performance of the (hierarchical) classifier, which is expected to impact the HSC task positively.
* As the authors correctly claim, the training regimes were not optimized for hierarchical selective classification. Despite the clear computation-wise motivation, I argue that including regimes optimized for HSC would make the empirical evaluation sounder.
* a few lines are off: for instance, I would argue that line 279, i.e.,
>CLIP achieves an exceptional improvement, surpassing 40%
>
 does not match what is shown in Figure 4 (which shows an improvement below 40%).

Limitations:
The paper briefly discusses limitations. I think this section could be expanded, e.g. considering Q1.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes an extension of selective classification following a class hierarchy to reduce the specificity of model prediction when there is a high uncertainty. In particular, if the prediction confidence of a class is smaller than a predefined threshold, the proposed algorithm would proceed towards a higher class level in the hierarchical structure (the parent node), until the confidence of the considering node exceeds that threshold. The paper also formulises hierarchical risk and coverage, so that the area under curve can be used as a metric to benchmark different selective classification methods. An extensive number of pretrained classifiers on ImageNet dataset are then used to evaluate the proposed method and show promising results. The paper also include a PAC-like theoretical result, so that when finding the optimal threshold, one can select appropriate hyper-parameters to achieve their desired outcome with certain confidence level.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper goes into details to provide an adequate background about selective classification, the definition of heirarchical risk and coverage as well as its area under curve as a metric to quantify the performance of hierarchical-based selective classification. It also links to previous studies in the same subfield. In general, the paper is well written and easy to follow.

The paper also includes a theoretical result on the guarantee of the learning algorithm when one wants to find the optimal thresholding value for their hierarchical selective classification. This simple theoretical results does strengthen the paper.

The paper also include an extensive number of experiments and ablation studies to provide insights into the newly-proposed method.

Weaknesses:
The paper relies on the setting with the following assumptions:
- It is an inference rule. This means that the algorithm is used at test time only. If this could be even integrated into training is a plus.
- It needs a validation set to find the optimal hyper-parameter $\theta$, or the threshold (partly mentioned in the conclusion). It is understandable because there is no training involve here, so there is a need for that. However, in some cases, there may not be additional data available.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wz2KvvEk44;"REVIEW 
Summary:
Visual-based Reinforcement Learning (RL) often fails to generalize across unseen environments. This work proposes SMG (Separated Models for Generalization) to improve the generalization in VRL by introducing two models to separately extract task-relevant and task-irrelevant representations through image reconstruction. Specifically, SMG proposes two additional consistency losses on relevant features, improving generalization. Extensive experiments, including video-hard DMC, color-hard DMC, and manipulation tasks, show SMG excels in diverse settings and tasks, demonstrating robust performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Separating foreground and background for reconstruction makes sense for improving the generalization in VRL.

- Extensive experiments in various experimental settings demonstrate the effectiveness of SMG.

- The learned mask looks very effective (Fig. 3 and Fig. 7).

Weaknesses:
- Distinguishing between controllable and uncontrollable parts for learning a mask model has been widely discussed in the community, like TIA [1], Denoised MDP [2], ISO-Dream [3] and so on. Although I appreciate authors' efforts to discuss its difference against TIA (appendix E.2), I think the novelty of learning mask models to distinguish noise from the environment is limited. Nevertheless, I believe that this paper has made contributions in applying mask models to the field of visual RL generalization.

- I'm curious about the performance of the proposed method in some more challenging settings, like RL-Vigen [4].

- As there are many losses, it is better to add a detailed pseudo code about how to calculate all these losses, which can make the paper more readable.

- This proposed SGM is considered to be seamlessly combined with any existing off-policy RL algorithms. As the experiments mainly consider SAC as the RL backbone, I'm curious about its performance with other methods, like DrQ or SVEA.

- The related work part only discusses observation generalization in RL and some other types of generalization also should be discussed, like dynamic generalization [5,6] and task generalization [7,8].

Overall, I lean toward boardline of this work. I will participate in subsequent discussions and would like to adjust my scores, especially for the response to my concerns about experiments.

[1] Learning Task Informed Abstractions

[2] Denoised MDPs: Learning World Models Better Than the World Itself

[3] Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models

[4] RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization

[5] Context-aware dynamics model for generalization in model-based reinforcement learning

[6] Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability

[7] Zero-shot task generalization with multi-task deep reinforcement learning

[8] Task Aware Dreamer for Task Generalization in Reinforcement Learning

Limitations:
Yes, this work has discussed its limitations in Sec. 6.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel method that utilizes two model branches to extract task-relevant and task-irrelevant representations separately from visual observations, aiming to enhance the zero-shot generalization ability of RL agents. The approach introduces four additional loss terms and two consistency losses to guide the agent's focus towards task-relevant areas across different scenarios. The proposed method can be seamlessly integrated into existing standard off-policy RL algorithms as a plug-and-play module. Experimental results demonstrate the effectiveness of the proposed model on two environments, surpassing previous benchmarks such as SAC and DrQ.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is clearly written and easy to follow.
2. Based on the separated models architecture, this paper proposes multiple effective loss functions to focus on task-relevant features in visual-based RL generalization.
3. The authors provide detailed validations on the DMC environment and robotic manipulation tasks. They demonstrate the advantages of the proposed loss terms across multiple tasks in DMC (Table 3) and showcase the state-of-the-art performance of SMG (Table 1, 2).

Weaknesses:
1. While the paper compares the performance with model-free RL methods, it would be beneficial to also include a comparison with model-based RL methods. Previous works such as DreamerPro [1], Iso-Dream [2], and Denoised-MDP [3] have addressed visual distractions to enhance the generalization ability of RL agents.

[1] Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations.

[2] Iso-Dream: Isolating Noncontrollable Visual Dynamics in World Models.

[3] Denoised mdps: Learning world models better than the world itself.

2. The paper lacks sufficient discussion and analysis of its limitations. 
3. The serial numbers in some figures appear to be somewhat disorganized.

Limitations:
This paper discusses the limitations, but that is not enough.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel approach called SMG (Separated Models for Generalization) to improve generalization in visual-based reinforcement learning (RL). The approach works by using separate foreground and background encoders/decoders and employing a mask to isolate task-relevant regions. In addition, it also applies four additional losses(mask ratio, background, Q-value and empowerment losses) to to enhance the model’s ability to distinguish between two types of representations. To make the learned models generalize to different visual styles, it introduces attribution augmentation and consistency losses. The authors position this as a plug-and-play method that can enhance existing RL algorithms' generalization capabilities.

Experiments show SMG outperforms baseline methods, particularly in video-background settings where it maintains performance even with significant visual changes. Ablation studies validate the importance of each component.

The main contributions are:

- SMG: A separated model architecture with two branches to extract task-relevant and task-irrelevant representations from visual observations.

- Two consistency losses to guide the agent's focus on task-relevant areas across different scenarios.

- Strong performance on DMControl benchmark tasks, especially in video-background settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper has several strengths:

- SMG achieves state-of-the-art performance on the DMControl Generalization Benchmark, particularly excelling in the challenging video-background settings. This demonstrates the practical effectiveness of the approach.

- SMG is a plug-and-play method that can enhance existing RL algorithms' generalization capabilities. It is designed to be easily integrated with existing off-policy RL algorithms, enhancing its practical value and potential for wide adoption.

- This paper includes detailed ablation studies that validate the importance of each component in the SMG architecture, providing insights into the method's workings.

- This paper is well-written. And it also provides clear visualizations of the reconstruction process, helping readers understand how SMG extracts and utilizes task-relevant features.

Weaknesses:
This paper has several weaknesses:

- My major concern is the overclaim made by this paper. While it claims to address the generalization gap in visual-based reinforcement learning, the method proposed primarily tackles scenarios where only the backgrounds differ. However, visual generalization challenges are more diverse and include variations such as different lighting conditions and textures, which are common in real-world robotics applications. These scenarios appear to be overlooked in this paper.

- SMG introduces a lot of loss terms and associated hyperparameters, which could complicate tuning in practical applications.
    - Specifically, the mask ratio $\rho$ appears to be crucial for performance, as it is the sole factor preventing the model from classifying everything as foreground. Given that $\rho$ represents the ratio between the foreground and the entire image, it likely necessitates per-task tuning, which could prove to be challenging and not scalable.

- The foreground consistency loss, as discussed in Section 3.3, heavily depends on the predicted mask to construct the augmented observation. During the initial stages of training, this process relies on potentially inaccurate mask predictions and attributions. Although the authors describe this as a bootstrapping process, further analysis regarding its stability and potential failure modes would be beneficial.

- The paper could be strengthened by considering a broader range of baselines. For example:
    - Recent studies [1] suggest that visual encoders pre-trained on large-scale image datasets can improve the visual robustness of a policy. This paper does not make any comparisons with visual pre-training methods.
    - Large vision foundation models like SAM [2] could potentially be utilized to provide supervision for generating foreground masks. Would this approach be more effective than training a mask predictor from scratch?


- The additional computation overhead introduced by the extra modules is concerning.
    - The architecture, which involves separate models, essentially doubles the number of parameters compared to baseline methods. Although the authors argue that the performance improvements are due to the novel architecture rather than the increased number of parameters, this could still be problematic for practical applications with limited computational resources.
    - Training time: The reported wall time for SMG is significantly longer than that of the baseline methods (22 hours versus 8-13 hours for 500,000 steps).

[1] Hansen, Nicklas, et al. ""On pre-training for visuo-motor control: Revisiting a learning-from-scratch baseline."" arXiv preprint arXiv:2212.05749 (2022).

[2] Kirillov, Alexander, et al. ""Segment anything."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

Limitations:
There is no separate section for limitations in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a novel objective to improve robustness of the visual encoder in RL to background noise and to color perturbations. First, the authors split the visual encoder into two models: background encoder/decoder and foreground encoder/decoder. The proposed training objective contains multiple components: 
- overall reconstruction loss that combines outputs of the background and the foreground decoders modulated by a mask;
- mask ratio loss that prevents the foreground mask from taking up too much of the image;
- background reconstruction loss that uses the learned mask to generate a new data sample;
- q-value loss that makes the foreground representation capture value information;
- empowerment loss that makes the foreground representations capture the agent actions;
- foreground and q-value consistency losses that make sure that changing the background (using the learned mask) doesn't change the foreground features and q-values

The method is tested on DMC generalization benchmark and on robotic manipulation tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method performs really well with various distractors;
- The idea of re-using the learned masks for augmentations is interesting and, as far as I can tell, novel;

Weaknesses:
- The writing is a bit sloppy, with many typos and confusing sentences;
- The resulting objective is too complex and has too many terms;
- No comparison to TIA, although the presented method is quite similar. Was that because you only compare to model-free methods?

Typos (some of them, I didn't write down all of them, please run a spell checker on the text):
line 13: achieving free from overfitting : not clear what this means
line 38: further strengths -> further strengthens
line 100: focused in -> focused on
line 536: Comparision -> comparison

Limitations:
The authors have adequately described limitations and potential negative societal impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Visual-based Reinforcement Learning (RL) often fails to generalize across unseen environments. This work proposes SMG (Separated Models for Generalization) to improve the generalization in VRL by introducing two models to separately extract task-relevant and task-irrelevant representations through image reconstruction. Specifically, SMG proposes two additional consistency losses on relevant features, improving generalization. Extensive experiments, including video-hard DMC, color-hard DMC, and manipulation tasks, show SMG excels in diverse settings and tasks, demonstrating robust performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Separating foreground and background for reconstruction makes sense for improving the generalization in VRL.

- Extensive experiments in various experimental settings demonstrate the effectiveness of SMG.

- The learned mask looks very effective (Fig. 3 and Fig. 7).

Weaknesses:
- Distinguishing between controllable and uncontrollable parts for learning a mask model has been widely discussed in the community, like TIA [1], Denoised MDP [2], ISO-Dream [3] and so on. Although I appreciate authors' efforts to discuss its difference against TIA (appendix E.2), I think the novelty of learning mask models to distinguish noise from the environment is limited. Nevertheless, I believe that this paper has made contributions in applying mask models to the field of visual RL generalization.

- I'm curious about the performance of the proposed method in some more challenging settings, like RL-Vigen [4].

- As there are many losses, it is better to add a detailed pseudo code about how to calculate all these losses, which can make the paper more readable.

- This proposed SGM is considered to be seamlessly combined with any existing off-policy RL algorithms. As the experiments mainly consider SAC as the RL backbone, I'm curious about its performance with other methods, like DrQ or SVEA.

- The related work part only discusses observation generalization in RL and some other types of generalization also should be discussed, like dynamic generalization [5,6] and task generalization [7,8].

Overall, I lean toward boardline of this work. I will participate in subsequent discussions and would like to adjust my scores, especially for the response to my concerns about experiments.

[1] Learning Task Informed Abstractions

[2] Denoised MDPs: Learning World Models Better Than the World Itself

[3] Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models

[4] RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization

[5] Context-aware dynamics model for generalization in model-based reinforcement learning

[6] Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability

[7] Zero-shot task generalization with multi-task deep reinforcement learning

[8] Task Aware Dreamer for Task Generalization in Reinforcement Learning

Limitations:
Yes, this work has discussed its limitations in Sec. 6.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel method that utilizes two model branches to extract task-relevant and task-irrelevant representations separately from visual observations, aiming to enhance the zero-shot generalization ability of RL agents. The approach introduces four additional loss terms and two consistency losses to guide the agent's focus towards task-relevant areas across different scenarios. The proposed method can be seamlessly integrated into existing standard off-policy RL algorithms as a plug-and-play module. Experimental results demonstrate the effectiveness of the proposed model on two environments, surpassing previous benchmarks such as SAC and DrQ.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is clearly written and easy to follow.
2. Based on the separated models architecture, this paper proposes multiple effective loss functions to focus on task-relevant features in visual-based RL generalization.
3. The authors provide detailed validations on the DMC environment and robotic manipulation tasks. They demonstrate the advantages of the proposed loss terms across multiple tasks in DMC (Table 3) and showcase the state-of-the-art performance of SMG (Table 1, 2).

Weaknesses:
1. While the paper compares the performance with model-free RL methods, it would be beneficial to also include a comparison with model-based RL methods. Previous works such as DreamerPro [1], Iso-Dream [2], and Denoised-MDP [3] have addressed visual distractions to enhance the generalization ability of RL agents.

[1] Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations.

[2] Iso-Dream: Isolating Noncontrollable Visual Dynamics in World Models.

[3] Denoised mdps: Learning world models better than the world itself.

2. The paper lacks sufficient discussion and analysis of its limitations. 
3. The serial numbers in some figures appear to be somewhat disorganized.

Limitations:
This paper discusses the limitations, but that is not enough.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel approach called SMG (Separated Models for Generalization) to improve generalization in visual-based reinforcement learning (RL). The approach works by using separate foreground and background encoders/decoders and employing a mask to isolate task-relevant regions. In addition, it also applies four additional losses(mask ratio, background, Q-value and empowerment losses) to to enhance the model’s ability to distinguish between two types of representations. To make the learned models generalize to different visual styles, it introduces attribution augmentation and consistency losses. The authors position this as a plug-and-play method that can enhance existing RL algorithms' generalization capabilities.

Experiments show SMG outperforms baseline methods, particularly in video-background settings where it maintains performance even with significant visual changes. Ablation studies validate the importance of each component.

The main contributions are:

- SMG: A separated model architecture with two branches to extract task-relevant and task-irrelevant representations from visual observations.

- Two consistency losses to guide the agent's focus on task-relevant areas across different scenarios.

- Strong performance on DMControl benchmark tasks, especially in video-background settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper has several strengths:

- SMG achieves state-of-the-art performance on the DMControl Generalization Benchmark, particularly excelling in the challenging video-background settings. This demonstrates the practical effectiveness of the approach.

- SMG is a plug-and-play method that can enhance existing RL algorithms' generalization capabilities. It is designed to be easily integrated with existing off-policy RL algorithms, enhancing its practical value and potential for wide adoption.

- This paper includes detailed ablation studies that validate the importance of each component in the SMG architecture, providing insights into the method's workings.

- This paper is well-written. And it also provides clear visualizations of the reconstruction process, helping readers understand how SMG extracts and utilizes task-relevant features.

Weaknesses:
This paper has several weaknesses:

- My major concern is the overclaim made by this paper. While it claims to address the generalization gap in visual-based reinforcement learning, the method proposed primarily tackles scenarios where only the backgrounds differ. However, visual generalization challenges are more diverse and include variations such as different lighting conditions and textures, which are common in real-world robotics applications. These scenarios appear to be overlooked in this paper.

- SMG introduces a lot of loss terms and associated hyperparameters, which could complicate tuning in practical applications.
    - Specifically, the mask ratio $\rho$ appears to be crucial for performance, as it is the sole factor preventing the model from classifying everything as foreground. Given that $\rho$ represents the ratio between the foreground and the entire image, it likely necessitates per-task tuning, which could prove to be challenging and not scalable.

- The foreground consistency loss, as discussed in Section 3.3, heavily depends on the predicted mask to construct the augmented observation. During the initial stages of training, this process relies on potentially inaccurate mask predictions and attributions. Although the authors describe this as a bootstrapping process, further analysis regarding its stability and potential failure modes would be beneficial.

- The paper could be strengthened by considering a broader range of baselines. For example:
    - Recent studies [1] suggest that visual encoders pre-trained on large-scale image datasets can improve the visual robustness of a policy. This paper does not make any comparisons with visual pre-training methods.
    - Large vision foundation models like SAM [2] could potentially be utilized to provide supervision for generating foreground masks. Would this approach be more effective than training a mask predictor from scratch?


- The additional computation overhead introduced by the extra modules is concerning.
    - The architecture, which involves separate models, essentially doubles the number of parameters compared to baseline methods. Although the authors argue that the performance improvements are due to the novel architecture rather than the increased number of parameters, this could still be problematic for practical applications with limited computational resources.
    - Training time: The reported wall time for SMG is significantly longer than that of the baseline methods (22 hours versus 8-13 hours for 500,000 steps).

[1] Hansen, Nicklas, et al. ""On pre-training for visuo-motor control: Revisiting a learning-from-scratch baseline."" arXiv preprint arXiv:2212.05749 (2022).

[2] Kirillov, Alexander, et al. ""Segment anything."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

Limitations:
There is no separate section for limitations in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a novel objective to improve robustness of the visual encoder in RL to background noise and to color perturbations. First, the authors split the visual encoder into two models: background encoder/decoder and foreground encoder/decoder. The proposed training objective contains multiple components: 
- overall reconstruction loss that combines outputs of the background and the foreground decoders modulated by a mask;
- mask ratio loss that prevents the foreground mask from taking up too much of the image;
- background reconstruction loss that uses the learned mask to generate a new data sample;
- q-value loss that makes the foreground representation capture value information;
- empowerment loss that makes the foreground representations capture the agent actions;
- foreground and q-value consistency losses that make sure that changing the background (using the learned mask) doesn't change the foreground features and q-values

The method is tested on DMC generalization benchmark and on robotic manipulation tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method performs really well with various distractors;
- The idea of re-using the learned masks for augmentations is interesting and, as far as I can tell, novel;

Weaknesses:
- The writing is a bit sloppy, with many typos and confusing sentences;
- The resulting objective is too complex and has too many terms;
- No comparison to TIA, although the presented method is quite similar. Was that because you only compare to model-free methods?

Typos (some of them, I didn't write down all of them, please run a spell checker on the text):
line 13: achieving free from overfitting : not clear what this means
line 38: further strengths -> further strengthens
line 100: focused in -> focused on
line 536: Comparision -> comparison

Limitations:
The authors have adequately described limitations and potential negative societal impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ww62xltEfB;"REVIEW 
Summary:
This paper investigates and proposes a novel bi-Lipschitz neural network architecture. This architecture provides a simple, direct and tight control of the Lipschitz and inverse Lipschitz constants through the use of two parameters, the ideal minimum, equipped with theoretical guarantees. To devise their architecture the authors exploit convex neural networks and the Legendre-Fenchel duality. The authors also propose a variant of their bi-Lipschitz architecture that is more scalable by exploiting partially input convex neural networks. Finally, the authors propose a set of experiments to showcase the utility of our model in concrete machine learning applications, namely, uncertainty estimation and monotone problem settings and show that it can improve previous methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written. After writing a clearly structured related work (with an extensive background and related work proposed in Appendix A), the authors propose their new design and explicitly explain how the forward pass of their network is computed as well as the expressivity and how the backpropagation can be done. 
- The authors acknowledge that the computational cost of their approach can pose serious limitation and propose to overcome this problem with partially input convex neural networks.

Weaknesses:
- It would be interesting of the authors could provide experiments with both their architectures with respect to computational cost, and highlight time of training etc.

Limitations:
Lack of experiments wrt to computational cost and maybe an experiment with a more larger dataset than the current ones use in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel neural network architecture called BLNN (Bi-Lipschitz Neural Network) that allows direct control and parameterization of the overall bi-Lipschitzness of the network. The main contributions include: i) a framework that allows tight control of Lipschitz and inverse Lipschitz constants of networks via using convex neural networks and the Legendre-Fenchel transformation, ii) comprehensive theoretical analysis, iii) empirical evaluation showing the nice performance of BLNN on tasks like function fitting, out-of-distribution detection, and monotone regression.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality:**

The paper presents a novel approach to constructing bi-Lipschitz neural networks that is distinctly different from existing methods. The use of convex neural networks and Legendre-Fenchel transformation to directly parameterize overall bi-Lipschitzness is quite novel. The extension (e.g. partially bi-Lipschitz networks, etc) is also new.


**Quality:**

The quality of the paper is good. The authors provide detailed proofs and analyses for their key claims, including the bi-Lipschitz properties of their construction and the expressive power of the resulting networks. The experiments cover various scenarios, from simple function fitting to uncertainty estimation and monotone regression. The results are quite competitive.


**Clarity:**

The paper is generally well-structured and clearly written. However, given the technical nature and the length of the paper, understanding the paper fully is still a tough task.

**Significance:**

The paper's contributions are significant in its solid theoretical developments. The significance is further underscored by the improved performance on tasks like out-of-distribution detection and monotone function learning. In conclusion, this paper presents a novel approach to an important problem in deep learning theory and practice.

Weaknesses:
1. Computational Complexity: A detailed analysis of time and space complexity compared to traditional networks can be helpful.

2. Scalability and Practical Implications: There's insufficient exploration of how the method scales to very large networks or complex datasets (e.g. TinyImageNet).

3. Hyperparameter Sensitivity: More discussions on this issue will be beneficial.

4. The paper could be more explicit about scenarios where the theoretical guarantees might not hold, and could explore potential extensions to other network architectures beyond feedforward networks.

Limitations:
It seems that an improved discussion on potential negative societal impacts or broader ethical considerations is still missing.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to control the bi-Lipschitzness of a neural-network by parameterizing the output by the Legendre-Fenchel-Dual. This involves parameterizing a strongly convex function and computing the minimum of that function in the forward pass. Several benchmarks are studied in simple regression tasks and uncertainty quantification.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
-The framework is interesting because it parameterizes bi-Lipschitz networks in a way that is not layer-wise and instead takes advantage of the Legendre-Fenchel transform LFT / convex conjugate of parameterized strongly-convex functions (ICNN), which only modifies the output of the output.

-Computing the LFT of a given function can be costly, however the paper offers a non-asymptotic bound for the Lipschitz constant and tractable gradient.

-The experimental results show a considerable improvement in tightness and regularity over other Lipschitz controlled networks like spectral normalization, AOL and Sandwich layers on small regression tasks. In particular BiLipNet behaves a lot better when the Lipschitz constant is overestimated in existing parameterizations.

Weaknesses:
-Computing the LFT seems to be quite expensive, hence why the experiments are only on simple 2d problems and fashion-MNIST. For this reason I'm doubtful that it will be used for any large-scale network training pipelines where tight Lipschitz control and estimation is challenging.

-The provable approximation class is limited to alpha-strongly monotone functions and is the derivative of some function almost everywhere. Lipschitz layers like AOL, SLL and Sandwich layer are all solutions to the LipSDP framework which only requires the activations themselves to be alpha-strongly monotone for alpha >= 0 (Fazlyab et al., 2019).

Limitations:
Limitation are adequately addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates and proposes a novel bi-Lipschitz neural network architecture. This architecture provides a simple, direct and tight control of the Lipschitz and inverse Lipschitz constants through the use of two parameters, the ideal minimum, equipped with theoretical guarantees. To devise their architecture the authors exploit convex neural networks and the Legendre-Fenchel duality. The authors also propose a variant of their bi-Lipschitz architecture that is more scalable by exploiting partially input convex neural networks. Finally, the authors propose a set of experiments to showcase the utility of our model in concrete machine learning applications, namely, uncertainty estimation and monotone problem settings and show that it can improve previous methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written. After writing a clearly structured related work (with an extensive background and related work proposed in Appendix A), the authors propose their new design and explicitly explain how the forward pass of their network is computed as well as the expressivity and how the backpropagation can be done. 
- The authors acknowledge that the computational cost of their approach can pose serious limitation and propose to overcome this problem with partially input convex neural networks.

Weaknesses:
- It would be interesting of the authors could provide experiments with both their architectures with respect to computational cost, and highlight time of training etc.

Limitations:
Lack of experiments wrt to computational cost and maybe an experiment with a more larger dataset than the current ones use in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel neural network architecture called BLNN (Bi-Lipschitz Neural Network) that allows direct control and parameterization of the overall bi-Lipschitzness of the network. The main contributions include: i) a framework that allows tight control of Lipschitz and inverse Lipschitz constants of networks via using convex neural networks and the Legendre-Fenchel transformation, ii) comprehensive theoretical analysis, iii) empirical evaluation showing the nice performance of BLNN on tasks like function fitting, out-of-distribution detection, and monotone regression.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality:**

The paper presents a novel approach to constructing bi-Lipschitz neural networks that is distinctly different from existing methods. The use of convex neural networks and Legendre-Fenchel transformation to directly parameterize overall bi-Lipschitzness is quite novel. The extension (e.g. partially bi-Lipschitz networks, etc) is also new.


**Quality:**

The quality of the paper is good. The authors provide detailed proofs and analyses for their key claims, including the bi-Lipschitz properties of their construction and the expressive power of the resulting networks. The experiments cover various scenarios, from simple function fitting to uncertainty estimation and monotone regression. The results are quite competitive.


**Clarity:**

The paper is generally well-structured and clearly written. However, given the technical nature and the length of the paper, understanding the paper fully is still a tough task.

**Significance:**

The paper's contributions are significant in its solid theoretical developments. The significance is further underscored by the improved performance on tasks like out-of-distribution detection and monotone function learning. In conclusion, this paper presents a novel approach to an important problem in deep learning theory and practice.

Weaknesses:
1. Computational Complexity: A detailed analysis of time and space complexity compared to traditional networks can be helpful.

2. Scalability and Practical Implications: There's insufficient exploration of how the method scales to very large networks or complex datasets (e.g. TinyImageNet).

3. Hyperparameter Sensitivity: More discussions on this issue will be beneficial.

4. The paper could be more explicit about scenarios where the theoretical guarantees might not hold, and could explore potential extensions to other network architectures beyond feedforward networks.

Limitations:
It seems that an improved discussion on potential negative societal impacts or broader ethical considerations is still missing.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to control the bi-Lipschitzness of a neural-network by parameterizing the output by the Legendre-Fenchel-Dual. This involves parameterizing a strongly convex function and computing the minimum of that function in the forward pass. Several benchmarks are studied in simple regression tasks and uncertainty quantification.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
-The framework is interesting because it parameterizes bi-Lipschitz networks in a way that is not layer-wise and instead takes advantage of the Legendre-Fenchel transform LFT / convex conjugate of parameterized strongly-convex functions (ICNN), which only modifies the output of the output.

-Computing the LFT of a given function can be costly, however the paper offers a non-asymptotic bound for the Lipschitz constant and tractable gradient.

-The experimental results show a considerable improvement in tightness and regularity over other Lipschitz controlled networks like spectral normalization, AOL and Sandwich layers on small regression tasks. In particular BiLipNet behaves a lot better when the Lipschitz constant is overestimated in existing parameterizations.

Weaknesses:
-Computing the LFT seems to be quite expensive, hence why the experiments are only on simple 2d problems and fashion-MNIST. For this reason I'm doubtful that it will be used for any large-scale network training pipelines where tight Lipschitz control and estimation is challenging.

-The provable approximation class is limited to alpha-strongly monotone functions and is the derivative of some function almost everywhere. Lipschitz layers like AOL, SLL and Sandwich layer are all solutions to the LipSDP framework which only requires the activations themselves to be alpha-strongly monotone for alpha >= 0 (Fazlyab et al., 2019).

Limitations:
Limitation are adequately addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wvQHQgnpGN;"REVIEW 
Summary:
This paper studies the two-player zero-sum stochastic Markov games (2p0s-MGs) with large scale or continuous state spaces. These problems have a large cardinality and function approximation methods are needed. The paper consider a spectral dynamic embedding method and proposed SDEPO. This methods utilized the transition dynamics in the construction of the state-space value function. SDEPO is able to converge with order $1/\epsilon$, which matches the optimal rate in single agent RL.
Theorems are provided for the last iterate convergence of the SDEPO algorithm. The effectiveness of the algorithm has been verified in games against baseline methods.

Generally the paper is well structured, but section 3-4 should be better explained while section 5 and 6 focused on the ""practical algorithms"" is stretched quite far from the analytical results in the previous sections.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The function approximation approach for markov games is a necessity for those problems with large/infinite state cardinality. It is indeed true that the dynamics of the problem was not utilized in previous methods. This algorithm seems to be the first work addressing this.

This work adapted the spectral dynamic embedding for stochastic nonlinear control problems and proposed SDEPO, the motivation is clear and well stated.

Weaknesses:
The only evaluation for the proposed algorithm is a rate of success in playing games with baseline algorithms. To the reviewer, this seems to be very limited. For a submission with strong theoretical focus, current result fails to validate the convergence properties of the proposed algorithm.

The sections for main results are not very well-written and is a bit difficult to read, more explanation would be appreciated. Although this could be due to the page limit.

Assumption 5 is in fact quite strong, a brief discussion on the impact and reasoning should be provided.

Section 5 and 6 seems a bit rushed and is intended to bring out the neural networks, the prior sections discussed the setting with tabular actions, where in these sections the action space is seen as continuous and more algorithms have been added, with no analytical results. I suggest the authors focusing on the existing setting with better presentation, explanation and more experiments.

Another problem this paper did not address is what are the current existing algorithms involving dynamics and function approximation in the single agent setting. The single agent RL with function approximation literature should be somewhat addressed in general.

Limitations:
The authors did not fully address the limitations of this paper. The authors mentioned that the algorithm is non-independent and relies on some assumptions. There are some other limitations of this work, which has been raised in the previous sections.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new algorithm named Spectral Dynamic Embedding Policy Optimization (SDEPO) to solve the zero-sum Markov games with continous state and finite actions. The convergence analysis indicates that the proposed method achieves the best-known sample complexity as the case of finite-state space; this paper is the first theoretical result in handling the continuous state space with known dynamic and infinite horizon.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is the first result for solving the NE of infinite-horizon two-player zero-sum Markov games  with continuous state space when the dynamic is known. Moreover, this paper resents sufficient introduction on the technical backgrounds and preliminaries. All assumptions are clearly listed. Lastly, the theoretical results are verified using emprical experiments.

Weaknesses:
1. The Assumption 1 is not reasonable. It says, whatever the state $s$ and the action $(a,b)$ are, the agent can move to any state $s'$ with a positive probability. Please correct me if I am wrong.

2. I am confused about what is new in the Spectral Dynamic Embedding method. It seems that both Bochner and Mercer theorem are well-known. This paper simply applies them to represent the transition probability and the reward using some kernels. Then everything is the same as traditional method in RL. 

3. A mild comment on Assumption 3: Since the optimal policy might be deterministic, it means that $\pi(a|s)$ is likely to be zero for some $a$. During the training, the policy $\pi_k$ will tend to the optimal policy; the mass at non-optimal action will also approach to $0$. It means if  $\underline{c}$ is larger than $\epsilon$, then $\pi_k$ will never converge to the optimal action in the sense of $L_\infty$ norm. From my understanding, the author needs to set $\underline{c}$ to be $\epsilon$ and it won't affect the complexity.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce an innovative approach to solving 2p0s-MGs with continuous state spaces, providing both theoretical guarantees and practical improvements over existing methods. The SDEPO algorithm and its variants offer efficient and scalable solutions for complex Markov games, potentially applicable to various domains in reinforcement learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes a new Spectral Dynamic Embedding Policy Optimization algorithm that effectively addresses two-player zero-sum Markov games with continuous state space and finite action space.
2. To handle the finite action spaces, a practical variant of SDEPO is proposed to manage continuous action spaces, with empirical results showcasing its superior performance.
3. The complexity result of SDEPO matches the best-known results for policy gradient algorithms in the single-agent setting, proving its efficiency.

Weaknesses:
1. The spectral embedding methods can be computationally intensive in practice due to the complexity of handling spectral dynamic embeddings.
2. Why were these specific feature generation methods chosen? Is the proposed method sensitive to feature generation methods?
3. The experiments are somewhat limited, expanding the empirical section to include more complex and diverse scenarios would significantly strengthen the paper.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the two-player zero-sum stochastic Markov games (2p0s-MGs) with large scale or continuous state spaces. These problems have a large cardinality and function approximation methods are needed. The paper consider a spectral dynamic embedding method and proposed SDEPO. This methods utilized the transition dynamics in the construction of the state-space value function. SDEPO is able to converge with order $1/\epsilon$, which matches the optimal rate in single agent RL.
Theorems are provided for the last iterate convergence of the SDEPO algorithm. The effectiveness of the algorithm has been verified in games against baseline methods.

Generally the paper is well structured, but section 3-4 should be better explained while section 5 and 6 focused on the ""practical algorithms"" is stretched quite far from the analytical results in the previous sections.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The function approximation approach for markov games is a necessity for those problems with large/infinite state cardinality. It is indeed true that the dynamics of the problem was not utilized in previous methods. This algorithm seems to be the first work addressing this.

This work adapted the spectral dynamic embedding for stochastic nonlinear control problems and proposed SDEPO, the motivation is clear and well stated.

Weaknesses:
The only evaluation for the proposed algorithm is a rate of success in playing games with baseline algorithms. To the reviewer, this seems to be very limited. For a submission with strong theoretical focus, current result fails to validate the convergence properties of the proposed algorithm.

The sections for main results are not very well-written and is a bit difficult to read, more explanation would be appreciated. Although this could be due to the page limit.

Assumption 5 is in fact quite strong, a brief discussion on the impact and reasoning should be provided.

Section 5 and 6 seems a bit rushed and is intended to bring out the neural networks, the prior sections discussed the setting with tabular actions, where in these sections the action space is seen as continuous and more algorithms have been added, with no analytical results. I suggest the authors focusing on the existing setting with better presentation, explanation and more experiments.

Another problem this paper did not address is what are the current existing algorithms involving dynamics and function approximation in the single agent setting. The single agent RL with function approximation literature should be somewhat addressed in general.

Limitations:
The authors did not fully address the limitations of this paper. The authors mentioned that the algorithm is non-independent and relies on some assumptions. There are some other limitations of this work, which has been raised in the previous sections.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new algorithm named Spectral Dynamic Embedding Policy Optimization (SDEPO) to solve the zero-sum Markov games with continous state and finite actions. The convergence analysis indicates that the proposed method achieves the best-known sample complexity as the case of finite-state space; this paper is the first theoretical result in handling the continuous state space with known dynamic and infinite horizon.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is the first result for solving the NE of infinite-horizon two-player zero-sum Markov games  with continuous state space when the dynamic is known. Moreover, this paper resents sufficient introduction on the technical backgrounds and preliminaries. All assumptions are clearly listed. Lastly, the theoretical results are verified using emprical experiments.

Weaknesses:
1. The Assumption 1 is not reasonable. It says, whatever the state $s$ and the action $(a,b)$ are, the agent can move to any state $s'$ with a positive probability. Please correct me if I am wrong.

2. I am confused about what is new in the Spectral Dynamic Embedding method. It seems that both Bochner and Mercer theorem are well-known. This paper simply applies them to represent the transition probability and the reward using some kernels. Then everything is the same as traditional method in RL. 

3. A mild comment on Assumption 3: Since the optimal policy might be deterministic, it means that $\pi(a|s)$ is likely to be zero for some $a$. During the training, the policy $\pi_k$ will tend to the optimal policy; the mass at non-optimal action will also approach to $0$. It means if  $\underline{c}$ is larger than $\epsilon$, then $\pi_k$ will never converge to the optimal action in the sense of $L_\infty$ norm. From my understanding, the author needs to set $\underline{c}$ to be $\epsilon$ and it won't affect the complexity.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce an innovative approach to solving 2p0s-MGs with continuous state spaces, providing both theoretical guarantees and practical improvements over existing methods. The SDEPO algorithm and its variants offer efficient and scalable solutions for complex Markov games, potentially applicable to various domains in reinforcement learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes a new Spectral Dynamic Embedding Policy Optimization algorithm that effectively addresses two-player zero-sum Markov games with continuous state space and finite action space.
2. To handle the finite action spaces, a practical variant of SDEPO is proposed to manage continuous action spaces, with empirical results showcasing its superior performance.
3. The complexity result of SDEPO matches the best-known results for policy gradient algorithms in the single-agent setting, proving its efficiency.

Weaknesses:
1. The spectral embedding methods can be computationally intensive in practice due to the complexity of handling spectral dynamic embeddings.
2. Why were these specific feature generation methods chosen? Is the proposed method sensitive to feature generation methods?
3. The experiments are somewhat limited, expanding the empirical section to include more complex and diverse scenarios would significantly strengthen the paper.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wlcm21C4nk;"REVIEW 
Summary:
Recent research indicates that rate-coding is crucial for information representation in deep Spiking Neural Networks (SNNs) trained via Backpropagation Through Time (BPTT). Building on this insight, a new training strategy called rate-based backpropagation has been developed to leverage rate-based representations, reducing the complexity of BPTT. This approach focuses on averaged dynamics to simplify the computational graph, thereby lowering memory and computational requirements. Theoretical and empirical analyses demonstrate that this method closely approximates BPTT's gradient optimization, maintaining comparable performance while surpassing other efficient training techniques. This advancement is poised to enable more scalable and resource-efficient SNN training, particularly in environments with limited resources.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is very well written and documented.
2.	The contributions have been discussed comprehensively.
3.	The experiments have been conducted on multiple benchmarks.

Weaknesses:
Some important details (such as the top-level algorithm of the proposed rate-based backpropagation method and details of the experimental setup) are reported in the appendix, while, due to their importance, they should be moved to the main manuscript.

Limitations:
The limitations and societal impacts have been discussed in Appendix D.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel rate-based backpropagation method for spiking neural network (SNNS) training, which effectively separates the time-dependent backpropagation (BPTT) process and thus reduces computational and memory costs. The method employs a rate-encoded approximation to capture the basic information and is validated by empirical experiments on various datasets, demonstrating that it is superior in terms of training efficiency and accuracy when compared to the traditional BPTT.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Empirical results on multiple datasets (CIFAR-10, CIFAR-100, ImageNet, CIFAR10-DVS) support the theoretical claims and ensure accuracy while reducing memory and time costs.
2. The paper is well-written, clearly explaining the proposed method, theoretical underpinnings, and experimental validation.

Weaknesses:
1.	In lines 53-55, this paper mentions that the proposed method reduces training time, but there is no relevant experimental proof in the experiments section.

Limitations:
The authors fully explain the limitations and potential social implications of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work falls into the category of efficient SNN training methods. This paper proposes a reduced computational graph to reduce the memory and computational demands of SNNs training. This work has the potential to train SNNs on resource-limited devices. The paper evaluates the methods on CIFAR-10, CIFAR-100, ImageNet, and other datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper addresses the issue of high time and memory costs in training spiking neural networks. 

This paper provides solid theoretical insights into the error bound and its relation to SNN BPTT training. 

The results of this work are comparable to the performance of the BPTT counterpart.

Weaknesses:
Not a clear comparison of the differences with existing e-prop methods in terms of methodology. 

No generalization results on hyperparameters (e.g., $\lambda$) are presented in this work. I raise this question because most work on SNNs uses large values of $\lambda$, but this work used 0.2 as $\lambda$.

Limitations:
See weakness and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a rate-based SNN training method, which can effectively reduce memory and time cost during training. They proved the efficiency of the rate-based back-propagation training and demonstrate that the rate-based training outperforms other back-propagation methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The rate-based method achieves better performance and uses less computing resource compared with BPTT, which is impressive.

This paper is well-written and well-organized.

Weaknesses:
The novelty is weak. There are two previous works that share similar idea with this paper, since they all use rate-based backpropagation [1,2]. The author needs to briefly explain the differences between these papers.

The rate-based backpropagation is not suitable for sequential tasks.

[1] Li, Yuhang, et al. ""Differentiable spike: Rethinking gradient-descent for training spiking neural networks."" Advances in Neural Information Processing Systems (2021).
[2] Bu, Tong, et al. ""Rate gradient approximation attack threats deep spiking neural networks."" Computer Vision and Pattern Recognition (2023).

Limitations:
See in weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Recent research indicates that rate-coding is crucial for information representation in deep Spiking Neural Networks (SNNs) trained via Backpropagation Through Time (BPTT). Building on this insight, a new training strategy called rate-based backpropagation has been developed to leverage rate-based representations, reducing the complexity of BPTT. This approach focuses on averaged dynamics to simplify the computational graph, thereby lowering memory and computational requirements. Theoretical and empirical analyses demonstrate that this method closely approximates BPTT's gradient optimization, maintaining comparable performance while surpassing other efficient training techniques. This advancement is poised to enable more scalable and resource-efficient SNN training, particularly in environments with limited resources.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is very well written and documented.
2.	The contributions have been discussed comprehensively.
3.	The experiments have been conducted on multiple benchmarks.

Weaknesses:
Some important details (such as the top-level algorithm of the proposed rate-based backpropagation method and details of the experimental setup) are reported in the appendix, while, due to their importance, they should be moved to the main manuscript.

Limitations:
The limitations and societal impacts have been discussed in Appendix D.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel rate-based backpropagation method for spiking neural network (SNNS) training, which effectively separates the time-dependent backpropagation (BPTT) process and thus reduces computational and memory costs. The method employs a rate-encoded approximation to capture the basic information and is validated by empirical experiments on various datasets, demonstrating that it is superior in terms of training efficiency and accuracy when compared to the traditional BPTT.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Empirical results on multiple datasets (CIFAR-10, CIFAR-100, ImageNet, CIFAR10-DVS) support the theoretical claims and ensure accuracy while reducing memory and time costs.
2. The paper is well-written, clearly explaining the proposed method, theoretical underpinnings, and experimental validation.

Weaknesses:
1.	In lines 53-55, this paper mentions that the proposed method reduces training time, but there is no relevant experimental proof in the experiments section.

Limitations:
The authors fully explain the limitations and potential social implications of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work falls into the category of efficient SNN training methods. This paper proposes a reduced computational graph to reduce the memory and computational demands of SNNs training. This work has the potential to train SNNs on resource-limited devices. The paper evaluates the methods on CIFAR-10, CIFAR-100, ImageNet, and other datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper addresses the issue of high time and memory costs in training spiking neural networks. 

This paper provides solid theoretical insights into the error bound and its relation to SNN BPTT training. 

The results of this work are comparable to the performance of the BPTT counterpart.

Weaknesses:
Not a clear comparison of the differences with existing e-prop methods in terms of methodology. 

No generalization results on hyperparameters (e.g., $\lambda$) are presented in this work. I raise this question because most work on SNNs uses large values of $\lambda$, but this work used 0.2 as $\lambda$.

Limitations:
See weakness and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a rate-based SNN training method, which can effectively reduce memory and time cost during training. They proved the efficiency of the rate-based back-propagation training and demonstrate that the rate-based training outperforms other back-propagation methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The rate-based method achieves better performance and uses less computing resource compared with BPTT, which is impressive.

This paper is well-written and well-organized.

Weaknesses:
The novelty is weak. There are two previous works that share similar idea with this paper, since they all use rate-based backpropagation [1,2]. The author needs to briefly explain the differences between these papers.

The rate-based backpropagation is not suitable for sequential tasks.

[1] Li, Yuhang, et al. ""Differentiable spike: Rethinking gradient-descent for training spiking neural networks."" Advances in Neural Information Processing Systems (2021).
[2] Bu, Tong, et al. ""Rate gradient approximation attack threats deep spiking neural networks."" Computer Vision and Pattern Recognition (2023).

Limitations:
See in weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
wsHMb4J2o9;"REVIEW 
Summary:
The paper introduces the BFA - a novel quantity to predict and control feature learning in DNNs, as well as the feature speed formula which allows expressing the magnitude of feature updates after one GD step. The paper recovers key properties of known HP scalings, and also extends these results by introducing a new HP scaling for large depth ReLU MLPs.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The BFA and BFK are interesting objects to study and the geometrical picture that arises (mentioned in the introduction) gives a nice intuition.

2. The main results (Thms 2.1 and 3.2) are clearly stated and the proofs are straightforward. 

3. The contributions are clearly stated and the relation to previous work distinguishes these contributions. 

4. Earlier results are recovered here with a transparent derivation, but Ref [1] also provided quite an intuitive derivation, as you mentioned. 





[1] https://arxiv.org/abs/2310.17813

Weaknesses:
Despite the strengths mentioned above, I did not give a higher score for the following reasons: 

1. Novelty for HP scaling: 
As far as I can see, the main takeaway regarding HP scaling is the extension of known results, such as muP, to the limit of large width-then-depth. While this is indeed new, this is a somewhat limited contribution. 

2. Applicability of results: 
While some of the results are rather general (like Thm 2.1), some other parts of the results seem to apply only under rather limited conditions, e.g. only a single input. 

3. Experimental findings: 
I found issues with some of the experimental findings: I did not find a mention of what is assumed about the data: is it synthetic, random, from some known benchmark etc. Also, by inspecting Fig 2b I was not convinced that the output sensitivity is bounded away from zero. 

4. I feel that the paper could be made less technical and more readable by delegating some of the proofs to the Appendix and using the space for some visualizations. 




typos: 
- Fig1 caption 1st line: witdh -> width

Limitations:
The authors adequately addressed the limitations of their results.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel perspective on infinite width and depth feature learning networks. It introduces the backward-to-feature kernel (BFK) as a central quantity determining the evolution of the intermediate layer features. The paper shows that the movement of the hidden layer features can be exactly related to an angle $\theta_\ell$ between the backward pass and the feature velocity, and uses insights on the scaling of the cosine of this angle with width to recover essentially all known infinite width and depth feature learning limits, as well as a novel large depth MLP limit.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper studies an extremely important topic in deep learning theory. Given the technically challenging nature of the study of large width and depth limits, the paper is superbly well-written and accessible. Prior papers by Yang et al and Bordelon et al have done important work in developing the study of large width and depth limits, but their derivations are either very dense or otherwise rely on non-rigorous methods to derive the scalings. This paper manages to both rigorously motivate feature learning at infinite width and depth while simultaneously making the paper short and accessible. This is a major strength and no easy feat. I commend the authors on it. 

Beyond this, there are several results of strong technical merit that will be of value for researchers studying infinite width and depth limits. The infinite depth MLP and scale invariant learning rate discussions are particularly interesting. The authors do a good job placing their work in context by presenting tables comparing their parameterization to others. 

Ultimately, I believe that this paper is not only technically novel and sound, but is also a service to the community. I strongly recommend it for acceptance.

Weaknesses:
There are no technical weaknesses that I have found, and I have gone through the derivations in detail. My only comment is expository:
In equation 1, the definition of the forward pass $T_{\ell}(f_{\ell-1}, w_\ell)$ as well as its discussion in terms of selection derivatives is quite technical and may confuse readers from outside sub-communities in machine learning. I recommend stating more clearly that this includes a simple forward pass such as $W_{\ell} \cdot f_{\ell-1}$ and perhaps adding a footnote to make this first paragraph a bit more readable.

Limitations:
Large width and depth limits may serve to determine the scaling laws for the next frontier of language and vision models, which may have major societal impact. However, the theoretical nature of the paper limits any major negative impacts.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a technical strategy for deriving neural net parameterizations that relies on controlling the angle between the activation gradient and the feature update. The authors derive various theoretical results about this quantity, including a formula for computing it, and some analyses in the context of MLPs and ResNets. The authors claim to use this principle to derive new parameterizations, but crucially they never test them in a real learning problem.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- the authors propose an interesting notion and derive interesting analyses surrounding it
- the parts of the math I checked seem rigorous and sound
- the authors do a good job of connecting their work to related work
- the ideas are quite creative

Weaknesses:
I need to preface this review by saying that this feedback is intended to be constructive and to help you improve the paper. My current impression is that the paper is not ready for publication. I strongly encourage you to keep working in this direction, and I hope this feedback will be useful for that.

With that said, the main issues I see with the paper are:

### **No real experimental evaluation**

My understanding is that the main practical outcome of your work and theoretical analysis is a new parameterization for training neural networks. I feel that it is really important for you to test this parameterization to check that it is useful, or at least to see what its properties are in an actual training situation. It's so easy to come by free cloud compute (e.g. Google Colab) that I can't really see a reason for not doing this.

I don't feel that the experiments in Figures 1 and 2 are enough to convince me of the utility of your framework. Also I'm not sure how to reproduce these experiments. For example, what dataset did you use? What is the loss function?

As a side note, I'm also a bit doubtful that you can even train MLPs effectively beyond depth 20 or so. I read the Jelassi et al paper (https://arxiv.org/abs/2305.07810) and noticed they don't test their parameterization either. I may be wrong here, but I don't think you can hope for some engineer or experimentalist to pick up the paper and implement things for you. I think you have to be proactive here.

### **Doesn't go that far beyond existing ideas**
A lot of the paper focuses on dealing with analyzing or re-deriving existing parameterizations---e.g. muP or the 1/sqrt(L) depth scaling in ResNets. But this is not so interesting because it has already been done and there are already ways to analyze these things. What does your analysis offer that prior analyses do not? I also want to point out that concurrent works to this paper go beyond 1/sqrt(L) depth scaling rules. For example arxiv.org/abs/2405.15712 and arxiv.org/abs/2405.14813. And these papers actually experimentally test these deviations. Clearly these are concurrent works, but I just mention it to demonstrate that there is more out there.

### **Paper only seems to focus on batch size one**

In my opinion, doing things only at batch size one is a bit toy, and it would be better to directly analyze larger batch sizes.

Limitations:
Overall, I think it's hard to assess the limitations without having more thorough experimental evaluation. I would encourage you to work out how to streamline the mathematical exposition, and then to start testing these ideas.

I realize this feedback might be construed as being fairly negative, but I hope that it can help to improve the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the feature learning speed of the layers of Neural Networks (NNs). Specifically, it proposes to measure it through the quantity *Backward-Feature Angle* (BFA), denoted by $\theta_l$ for a layer $l$. This quantity is directly related to the layer-wise decomposition of the Neural Tangent Kernel (NTK). In practice, the BFA is measured experimentally and several properties (feature learning, signal propagation, etc.) can be related to the BFA.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
# Originality

This paper tackles an important problem: the relation between the optimal hyperparameters of a neural network and its architecture.

# Clarity

This paper is easy to read and the statements are clear.

Weaknesses:
# Originality

The BFA is closely related to the layer-wise decomposition of the NTK, which is already widely used in the NN optimization literature [1, 2, 3, 4]. Overall, the BFA does not contain any information that is not already available with previous objects.

# Significance

The benefits and the properties of the BFA are still unclear.

For instance, Section 5 proposes a new scaling of the hyperparameters, that is not clearly related to the BFA. Besides, the experimental validation of this new scaling is not provided.

# Quality

The contribution of this paper is unclear. The usefulness of the BFA, either theoretical or experimental, is still unclear, and the proposed hyperparameter scaling is not tested experimentally.

# EDIT: References

[1] Gradient descent provably optimizes over-parameterized neural networks (2018), Du et al.

[2] Gradient descent finds global minima of deep neural networks (2019), Du et al.

[3] A convergence theory for deep learning via over-parameterization (2019), Allen-Zhu et al.

[4] Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks (2020), Zou et al.

Limitations:
Lack of experimental validation.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a technical strategy for deriving neural net parameterizations that relies on controlling the angle between the activation gradient and the feature update. The authors derive various theoretical results about this quantity, including a formula for computing it, and some analyses in the context of MLPs and ResNets. The authors claim to use this principle to derive new parameterizations, but crucially they never test them in a real learning problem.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- the authors propose an interesting notion and derive interesting analyses surrounding it
- the parts of the math I checked seem rigorous and sound
- the authors do a good job of connecting their work to related work
- the ideas are quite creative

Weaknesses:
I need to preface this review by saying that this feedback is intended to be constructive and to help you improve the paper. My current impression is that the paper is not ready for publication. I strongly encourage you to keep working in this direction, and I hope this feedback will be useful for that.

With that said, the main issues I see with the paper are:

### **No real experimental evaluation**

My understanding is that the main practical outcome of your work and theoretical analysis is a new parameterization for training neural networks. I feel that it is really important for you to test this parameterization to check that it is useful, or at least to see what its properties are in an actual training situation. It's so easy to come by free cloud compute (e.g. Google Colab) that I can't really see a reason for not doing this.

I don't feel that the experiments in Figures 1 and 2 are enough to convince me of the utility of your framework. Also I'm not sure how to reproduce these experiments. For example, what dataset did you use? What is the loss function?

As a side note, I'm also a bit doubtful that you can even train MLPs effectively beyond depth 20 or so. I read the Jelassi et al paper (https://arxiv.org/abs/2305.07810) and noticed they don't test their parameterization either. I may be wrong here, but I don't think you can hope for some engineer or experimentalist to pick up the paper and implement things for you. I think you have to be proactive here.

### **Doesn't go that far beyond existing ideas**
A lot of the paper focuses on dealing with analyzing or re-deriving existing parameterizations---e.g. muP or the 1/sqrt(L) depth scaling in ResNets. But this is not so interesting because it has already been done and there are already ways to analyze these things. What does your analysis offer that prior analyses do not? I also want to point out that concurrent works to this paper go beyond 1/sqrt(L) depth scaling rules. For example arxiv.org/abs/2405.15712 and arxiv.org/abs/2405.14813. And these papers actually experimentally test these deviations. Clearly these are concurrent works, but I just mention it to demonstrate that there is more out there.

### **Paper only seems to focus on batch size one**

In my opinion, doing things only at batch size one is a bit toy, and it would be better to directly analyze larger batch sizes.

Limitations:
Overall, I think it's hard to assess the limitations without having more thorough experimental evaluation. I would encourage you to work out how to streamline the mathematical exposition, and then to start testing these ideas.

I realize this feedback might be construed as being fairly negative, but I hope that it can help to improve the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces the BFA - a novel quantity to predict and control feature learning in DNNs, as well as the feature speed formula which allows expressing the magnitude of feature updates after one GD step. The paper recovers key properties of known HP scalings, and also extends these results by introducing a new HP scaling for large depth ReLU MLPs.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The BFA and BFK are interesting objects to study and the geometrical picture that arises (mentioned in the introduction) gives a nice intuition.

2. The main results (Thms 2.1 and 3.2) are clearly stated and the proofs are straightforward. 

3. The contributions are clearly stated and the relation to previous work distinguishes these contributions. 

4. Earlier results are recovered here with a transparent derivation, but Ref [1] also provided quite an intuitive derivation, as you mentioned. 





[1] https://arxiv.org/abs/2310.17813

Weaknesses:
Despite the strengths mentioned above, I did not give a higher score for the following reasons: 

1. Novelty for HP scaling: 
As far as I can see, the main takeaway regarding HP scaling is the extension of known results, such as muP, to the limit of large width-then-depth. While this is indeed new, this is a somewhat limited contribution. 

2. Applicability of results: 
While some of the results are rather general (like Thm 2.1), some other parts of the results seem to apply only under rather limited conditions, e.g. only a single input. 

3. Experimental findings: 
I found issues with some of the experimental findings: I did not find a mention of what is assumed about the data: is it synthetic, random, from some known benchmark etc. Also, by inspecting Fig 2b I was not convinced that the output sensitivity is bounded away from zero. 

4. I feel that the paper could be made less technical and more readable by delegating some of the proofs to the Appendix and using the space for some visualizations. 




typos: 
- Fig1 caption 1st line: witdh -> width

Limitations:
The authors adequately addressed the limitations of their results.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel perspective on infinite width and depth feature learning networks. It introduces the backward-to-feature kernel (BFK) as a central quantity determining the evolution of the intermediate layer features. The paper shows that the movement of the hidden layer features can be exactly related to an angle $\theta_\ell$ between the backward pass and the feature velocity, and uses insights on the scaling of the cosine of this angle with width to recover essentially all known infinite width and depth feature learning limits, as well as a novel large depth MLP limit.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper studies an extremely important topic in deep learning theory. Given the technically challenging nature of the study of large width and depth limits, the paper is superbly well-written and accessible. Prior papers by Yang et al and Bordelon et al have done important work in developing the study of large width and depth limits, but their derivations are either very dense or otherwise rely on non-rigorous methods to derive the scalings. This paper manages to both rigorously motivate feature learning at infinite width and depth while simultaneously making the paper short and accessible. This is a major strength and no easy feat. I commend the authors on it. 

Beyond this, there are several results of strong technical merit that will be of value for researchers studying infinite width and depth limits. The infinite depth MLP and scale invariant learning rate discussions are particularly interesting. The authors do a good job placing their work in context by presenting tables comparing their parameterization to others. 

Ultimately, I believe that this paper is not only technically novel and sound, but is also a service to the community. I strongly recommend it for acceptance.

Weaknesses:
There are no technical weaknesses that I have found, and I have gone through the derivations in detail. My only comment is expository:
In equation 1, the definition of the forward pass $T_{\ell}(f_{\ell-1}, w_\ell)$ as well as its discussion in terms of selection derivatives is quite technical and may confuse readers from outside sub-communities in machine learning. I recommend stating more clearly that this includes a simple forward pass such as $W_{\ell} \cdot f_{\ell-1}$ and perhaps adding a footnote to make this first paragraph a bit more readable.

Limitations:
Large width and depth limits may serve to determine the scaling laws for the next frontier of language and vision models, which may have major societal impact. However, the theoretical nature of the paper limits any major negative impacts.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the feature learning speed of the layers of Neural Networks (NNs). Specifically, it proposes to measure it through the quantity *Backward-Feature Angle* (BFA), denoted by $\theta_l$ for a layer $l$. This quantity is directly related to the layer-wise decomposition of the Neural Tangent Kernel (NTK). In practice, the BFA is measured experimentally and several properties (feature learning, signal propagation, etc.) can be related to the BFA.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
# Originality

This paper tackles an important problem: the relation between the optimal hyperparameters of a neural network and its architecture.

# Clarity

This paper is easy to read and the statements are clear.

Weaknesses:
# Originality

The BFA is closely related to the layer-wise decomposition of the NTK, which is already widely used in the NN optimization literature [1, 2, 3, 4]. Overall, the BFA does not contain any information that is not already available with previous objects.

# Significance

The benefits and the properties of the BFA are still unclear.

For instance, Section 5 proposes a new scaling of the hyperparameters, that is not clearly related to the BFA. Besides, the experimental validation of this new scaling is not provided.

# Quality

The contribution of this paper is unclear. The usefulness of the BFA, either theoretical or experimental, is still unclear, and the proposed hyperparameter scaling is not tested experimentally.

# EDIT: References

[1] Gradient descent provably optimizes over-parameterized neural networks (2018), Du et al.

[2] Gradient descent finds global minima of deep neural networks (2019), Du et al.

[3] A convergence theory for deep learning via over-parameterization (2019), Allen-Zhu et al.

[4] Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks (2020), Zou et al.

Limitations:
Lack of experimental validation.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wsGzvhnoaX;"REVIEW 
Summary:
This paper considers using quantum methods for stochastic optimization, using zeroth order queries. It looks like the main idea is that using quantum methods, one can summarize over finite difference calculations quickly and efficiently, to arrive at approximate subgradients efficiently; this would usually be very inefficient for classical methods. Overall, they are able to show speedup, from $O(\epsilon^{-4})$ to $O(\epsilon^{-3})$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The problem is well contained and the premise is believable. 

The classical optimization bits looks reasonable, and the results make sense. 

I skimmed through the appendix, and the classical optimization parts are reasonable.

Weaknesses:
Section 3 is a bit hard to follow. The specific speedup offered by the quantum method is not entirely clear, though it is likely coming from Theorem B1. Perhaps a deeper discussion of this, and why this quantum speedup exists (e.g. is it a consequence of Deusch Josza? Can you provide a more complete argument for where the speedup appears? )

Limitations:
no societal limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates quantum algorithms for finding the $(\delta,\epsilon)$-Goldstein stationary point of a potentially nonconvex and nonsmooth objective function $f$. Utilizing quantum variance reduction techniques as outlined in [42], the authors have developed a zeroth-order quantum estimator for the gradient of the smoothed surrogate of $f$. The stationary point of this smoothed surrogate is also the Goldstein stationary point of $f$ when using an appropriate smoothing parameter $\delta$. Leveraging this zeroth-order quantum estimator, the authors propose two algorithms, QGFM and QGFM+, to find the Goldstein stationary point, achieving a quantum speedup on the order of $\epsilon^{-2/3}$. Additionally, the QGFM+ framework adjusts the variance level during each variance reduction step, providing further acceleration to the Q-SPIDER algorithm described in [42] for smooth nonconvex optimization.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper initiates the study of quantum algorithms for finding Goldstein stationary points, a significant problem in continuous optimization. Additionally, the authors present an explicit construction of the quantum sampling oracle using the quantum zeroth-order oracle, including a detailed discussion on the number of qubits required.

Weaknesses:
Despite the detailed implementation and calculations, the overall technical approach remains relatively straightforward. The zeroth-order quantum estimator combines the classical stochastic gradient estimator for the smoothed surrogate with the quantum variance reduction algorithm in [42]. The quantum algorithms for finding the Goldstein stationary point are obtained by replacing the classical estimators with quantum estimators. Moreover, the narrative is somewhat incomplete due to the absence of lower bound results.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies quantum algorithm for non-smooth non-convex stochastic optimization with zeroth-order oracle. It introduces an effective quantum estimator that reduces the variance compared to classical zeroth-order estimators. Upon substituting this estimator into known zeroth-order non-smooth optimizers, namely GFM and GDM+, the resulting quantum optimizer achieves improved rate $\tilde O(d^{3/2}\delta^{-1}\epsilon^{-3})$ and $\tilde O(d^{3/2}\delta^{-1}\epsilon^{-7/3})$ respectively for finding a $(\delta,\epsilon)$-Goldstein stationary point. Notably, quantum speedup improves upon the classical lower bound $\delta^{-1}\epsilon^{-3}$ by a factor of $\epsilon^{2/3}$. Moreover, a modified algorithm achieves $O(\sqrt{d}\epsilon^{-7/3})$ for smooth optimization, improving upon the best known rate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes a new zeroth-order quantum estimator. This leads to new quantum algorithms that solves zeroth-order non-smooth non-convex optimization problem, which is not well studied in the literature. Moreover, the proposed algorithms show quantum speedup compared to their classical (non-quantum) counterparts. Notably, it improves over the classical lower bound of $\Omega(\delta^{-1}\epsilon^{-3})$ by a factor of $\epsilon^{2/3}$. Overall, these results represent a significant contribution to the understanding of optimization with quantum oracles. Given my expertise lies primarily in optimization and not in quantum computation, I am only able to assess the optimization-related aspects of this work.

Weaknesses:
Although the dependence on $\delta,\epsilon$ is improved, the dimension dependence is suboptimal. In particular, since GFM and GFM+ are known to have suboptimal dimension dependence $d^{3/2}$, so do QGFM and QGFM+. On the other hand, as observed by Kornowsky and Shamir [1], optimizing the random smoothing $f_\delta$ with a non-smooth optimizer, such as online-to-non-convex (o2nc) [2], eliminates this $\sqrt{d}$ factor and achieves $O(d)$ in dimension. Hence, my intuition suggests that upon substituting the quantum estimator into o2nc and following a similar approach to Kornowsky and Shamir, the authors might be able to recover $O(d)$ (or even better) dimension dependence. 

[1] Kornowski, G. and Shamir, O., “An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization”, 2023. doi:10.48550/arXiv.2307.04504.

[2] Cutkosky, A., Mehta, H., and Orabona, F., “Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex Conversion”, 2023. doi:10.48550/arXiv.2302.03775.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces new quantum algorithms for non-smooth non-convex optimization problems. The authors propose a quantum gradient estimator for smoothed objectives and develop the Quantum Gradient-Free Method (QGFM) and its enhanced version, QGFM+, which achieve better query complexities than their classical counterparts. These complexities demonstrate a marked quantum speedup over classical counterparts, indicating the potential of quantum computing in optimizing complex functions more efficiently. The paper also discusses the construction of quantum oracles and the application of variance reduction techniques, paving the way for future research in quantum optimization.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper proposed new zeroth order quantum optimization algorithms achieving better computational complexities compared to classical methods for non-smooth and non-convex optimization.
- Technically, they construct efficient quantum gradient estimators and quantum superpositions over required distributions as a key subroutine.
- They also proposed a quantum algorithm for non-convex smooth problems with an adaptive variance level, accelerating prior quantum algorithms to get more speedups.

Weaknesses:
- The assumptions of having a quantum stochastic function value oracle may be strong. Could the authors explain more about why it is reasonable and important to have such a function oracle?
- The technical core for quantum speedups seems to be the quantum mean value estimation procedure, which is already used in many other optimization problems and scenarios. Could the authors explain more about the technical novelty of their work?

Limitations:
The quantum complexity lower bound on this problem is not proved in this paper, which is mentioned in the conclusion part.  Also, as noted in remark 3.7, implementing quantum sample oracle may require the uses of QRAM, which is currently limited by the physical realizations.
This is a theoretical work, so there is no potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers using quantum methods for stochastic optimization, using zeroth order queries. It looks like the main idea is that using quantum methods, one can summarize over finite difference calculations quickly and efficiently, to arrive at approximate subgradients efficiently; this would usually be very inefficient for classical methods. Overall, they are able to show speedup, from $O(\epsilon^{-4})$ to $O(\epsilon^{-3})$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The problem is well contained and the premise is believable. 

The classical optimization bits looks reasonable, and the results make sense. 

I skimmed through the appendix, and the classical optimization parts are reasonable.

Weaknesses:
Section 3 is a bit hard to follow. The specific speedup offered by the quantum method is not entirely clear, though it is likely coming from Theorem B1. Perhaps a deeper discussion of this, and why this quantum speedup exists (e.g. is it a consequence of Deusch Josza? Can you provide a more complete argument for where the speedup appears? )

Limitations:
no societal limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates quantum algorithms for finding the $(\delta,\epsilon)$-Goldstein stationary point of a potentially nonconvex and nonsmooth objective function $f$. Utilizing quantum variance reduction techniques as outlined in [42], the authors have developed a zeroth-order quantum estimator for the gradient of the smoothed surrogate of $f$. The stationary point of this smoothed surrogate is also the Goldstein stationary point of $f$ when using an appropriate smoothing parameter $\delta$. Leveraging this zeroth-order quantum estimator, the authors propose two algorithms, QGFM and QGFM+, to find the Goldstein stationary point, achieving a quantum speedup on the order of $\epsilon^{-2/3}$. Additionally, the QGFM+ framework adjusts the variance level during each variance reduction step, providing further acceleration to the Q-SPIDER algorithm described in [42] for smooth nonconvex optimization.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper initiates the study of quantum algorithms for finding Goldstein stationary points, a significant problem in continuous optimization. Additionally, the authors present an explicit construction of the quantum sampling oracle using the quantum zeroth-order oracle, including a detailed discussion on the number of qubits required.

Weaknesses:
Despite the detailed implementation and calculations, the overall technical approach remains relatively straightforward. The zeroth-order quantum estimator combines the classical stochastic gradient estimator for the smoothed surrogate with the quantum variance reduction algorithm in [42]. The quantum algorithms for finding the Goldstein stationary point are obtained by replacing the classical estimators with quantum estimators. Moreover, the narrative is somewhat incomplete due to the absence of lower bound results.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies quantum algorithm for non-smooth non-convex stochastic optimization with zeroth-order oracle. It introduces an effective quantum estimator that reduces the variance compared to classical zeroth-order estimators. Upon substituting this estimator into known zeroth-order non-smooth optimizers, namely GFM and GDM+, the resulting quantum optimizer achieves improved rate $\tilde O(d^{3/2}\delta^{-1}\epsilon^{-3})$ and $\tilde O(d^{3/2}\delta^{-1}\epsilon^{-7/3})$ respectively for finding a $(\delta,\epsilon)$-Goldstein stationary point. Notably, quantum speedup improves upon the classical lower bound $\delta^{-1}\epsilon^{-3}$ by a factor of $\epsilon^{2/3}$. Moreover, a modified algorithm achieves $O(\sqrt{d}\epsilon^{-7/3})$ for smooth optimization, improving upon the best known rate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes a new zeroth-order quantum estimator. This leads to new quantum algorithms that solves zeroth-order non-smooth non-convex optimization problem, which is not well studied in the literature. Moreover, the proposed algorithms show quantum speedup compared to their classical (non-quantum) counterparts. Notably, it improves over the classical lower bound of $\Omega(\delta^{-1}\epsilon^{-3})$ by a factor of $\epsilon^{2/3}$. Overall, these results represent a significant contribution to the understanding of optimization with quantum oracles. Given my expertise lies primarily in optimization and not in quantum computation, I am only able to assess the optimization-related aspects of this work.

Weaknesses:
Although the dependence on $\delta,\epsilon$ is improved, the dimension dependence is suboptimal. In particular, since GFM and GFM+ are known to have suboptimal dimension dependence $d^{3/2}$, so do QGFM and QGFM+. On the other hand, as observed by Kornowsky and Shamir [1], optimizing the random smoothing $f_\delta$ with a non-smooth optimizer, such as online-to-non-convex (o2nc) [2], eliminates this $\sqrt{d}$ factor and achieves $O(d)$ in dimension. Hence, my intuition suggests that upon substituting the quantum estimator into o2nc and following a similar approach to Kornowsky and Shamir, the authors might be able to recover $O(d)$ (or even better) dimension dependence. 

[1] Kornowski, G. and Shamir, O., “An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization”, 2023. doi:10.48550/arXiv.2307.04504.

[2] Cutkosky, A., Mehta, H., and Orabona, F., “Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex Conversion”, 2023. doi:10.48550/arXiv.2302.03775.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces new quantum algorithms for non-smooth non-convex optimization problems. The authors propose a quantum gradient estimator for smoothed objectives and develop the Quantum Gradient-Free Method (QGFM) and its enhanced version, QGFM+, which achieve better query complexities than their classical counterparts. These complexities demonstrate a marked quantum speedup over classical counterparts, indicating the potential of quantum computing in optimizing complex functions more efficiently. The paper also discusses the construction of quantum oracles and the application of variance reduction techniques, paving the way for future research in quantum optimization.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper proposed new zeroth order quantum optimization algorithms achieving better computational complexities compared to classical methods for non-smooth and non-convex optimization.
- Technically, they construct efficient quantum gradient estimators and quantum superpositions over required distributions as a key subroutine.
- They also proposed a quantum algorithm for non-convex smooth problems with an adaptive variance level, accelerating prior quantum algorithms to get more speedups.

Weaknesses:
- The assumptions of having a quantum stochastic function value oracle may be strong. Could the authors explain more about why it is reasonable and important to have such a function oracle?
- The technical core for quantum speedups seems to be the quantum mean value estimation procedure, which is already used in many other optimization problems and scenarios. Could the authors explain more about the technical novelty of their work?

Limitations:
The quantum complexity lower bound on this problem is not proved in this paper, which is mentioned in the conclusion part.  Also, as noted in remark 3.7, implementing quantum sample oracle may require the uses of QRAM, which is currently limited by the physical realizations.
This is a theoretical work, so there is no potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wqs2RMq4CW;"REVIEW 
Summary:
This paper studies corruption-robust linear bandit optimization and characterizes the regret bound in terms of both weak and strong corruption measures. Under the stochastic setting, this paper proposes a phased elimination algorithm, and the regret bounds match the lower bound. Under the adversarial setting, the paper proposes two individual algorithms for the two corruption measures respectively. In addition, this paper studies gap-dependent misspecification setting through reduction, and discusses a use case for linear MDPs.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The regret bounds in terms of both corruption measures are provided, where the regret bound depending on $C_\infty$ is first introduced in this paper.
- The theoretical results are supported with detailed proof.
- This paper is generally well-written.

Weaknesses:
- The algorithms are efficient regarding regret bound, but the computational complexity is not discussed.
- A conclusion section could be added.

Limitations:
Some limitations are discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors characterize the problem of learning the presence of reward corruption in the linear bandit setting. They provide matching upper and lower bounds in the corrupted stochastic setting, and initiate the study on the corrupted adversarial setting, for which they obtain optimal scaling in the corruption level.

Not only that, the authors prove a general reduction that efficiently handles gap-dependent misspecification with corruption-robust algorithms. They were able to show that linear MDPs with gap-dependent misspecification are efficiently learnable. While this reduction is general, interestingly they denied the possibility to obtain the tightest rate for gap-dependent misspecification. This observation leads them to develop a specialized algorithm which, in the linear bandit setting, obtains the optimal rate. According to their argument, this resolves the open problem of Liu et al. (2023a).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Interesting results
    - Deterministic algorithm cannot avoid suboptimal regret (Proposition 1)
    - Matching upper and lower bound on the stochastic setting, by just changing deterministic sampling to stochastic.
    - Solving an open problem of instance-dependent misspecified setting.

- Clearly state the limitations of previous works and their technical novelties.
    - Easy to understand their contributions.

Weaknesses:
- (Minor) The algorithms are not seriously different from the previous works as they mentioned, but this is just a minor point - every theoretical improvement is important. 

- Not clear what they tried to say on page 9
    - Why Theorem 6.2 shows that $\rho \leq \frac{1}{d}$ is not optimal?
    - Impossibility result (from line 304): so basically what authors are trying to say is, that 'their' reduction is not applicable for a tighter result, right? It is not about any reduction from corruption to misspecification. 

- No future works.

Limitations:
The authors adequately addressed the limitations. It would be great if authors provide possible future works. 
There is no potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studied the corrupted linear bandits. The authors propose four different metrics to evaluate the total corruption in Eq. (1). Many settings are considered in this paper. For stochastic LB, the proposed algorithm achieves a regret bound of $d\sqrt{T}+\sqrt{d} C_{\infty}$. For adversarial LB, the proposed algorithm achieves a regret bound in the order of $d\sqrt{T}+\sqrt{d} C_{\infty}$ or $d^{3}\sqrt{T}+d^{5/2} C$. The authors also consider the gap-dependent misspecification, where the misspecification level of an arm $a$ can be evaluated by $\rho$ times the gap of arm $a$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
See summary.

Weaknesses:
**Weaknesses and Questions:**
1. At lines 107-109, the authors claim that the strong adversary is equivalent to the CM viewpoint. This doesn't seem right. For regret, the strong adversary is harder than the CM viewpoint. Thus, it is unfair and wrong to compare He et al. (2022) in the same way.
2. At line 131, adversarial linear bandits are discussed. However, no problem definition of this problem is introduced before line 131.
3. This paper studies the fixed action set, while the previous works He et al. (2022) and Foster et al. (2020) allow the action set to be chosen by an adaptive adversary, which is much harder than this paper. Table 1 is not fair. He et al. (2022) is for the adaptive adversarial viewpoint, which is totally different from the stochastic LB. For a fixed action set, the optimal regret without $C$ should be $\sqrt{d T \log k}$, where $k$ is the number of arms.
4. Assumption 1 is not very reasonable.

Limitations:
N.A.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies corruption-robust linear bandit optimization and characterizes the regret bound in terms of both weak and strong corruption measures. Under the stochastic setting, this paper proposes a phased elimination algorithm, and the regret bounds match the lower bound. Under the adversarial setting, the paper proposes two individual algorithms for the two corruption measures respectively. In addition, this paper studies gap-dependent misspecification setting through reduction, and discusses a use case for linear MDPs.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The regret bounds in terms of both corruption measures are provided, where the regret bound depending on $C_\infty$ is first introduced in this paper.
- The theoretical results are supported with detailed proof.
- This paper is generally well-written.

Weaknesses:
- The algorithms are efficient regarding regret bound, but the computational complexity is not discussed.
- A conclusion section could be added.

Limitations:
Some limitations are discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors characterize the problem of learning the presence of reward corruption in the linear bandit setting. They provide matching upper and lower bounds in the corrupted stochastic setting, and initiate the study on the corrupted adversarial setting, for which they obtain optimal scaling in the corruption level.

Not only that, the authors prove a general reduction that efficiently handles gap-dependent misspecification with corruption-robust algorithms. They were able to show that linear MDPs with gap-dependent misspecification are efficiently learnable. While this reduction is general, interestingly they denied the possibility to obtain the tightest rate for gap-dependent misspecification. This observation leads them to develop a specialized algorithm which, in the linear bandit setting, obtains the optimal rate. According to their argument, this resolves the open problem of Liu et al. (2023a).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Interesting results
    - Deterministic algorithm cannot avoid suboptimal regret (Proposition 1)
    - Matching upper and lower bound on the stochastic setting, by just changing deterministic sampling to stochastic.
    - Solving an open problem of instance-dependent misspecified setting.

- Clearly state the limitations of previous works and their technical novelties.
    - Easy to understand their contributions.

Weaknesses:
- (Minor) The algorithms are not seriously different from the previous works as they mentioned, but this is just a minor point - every theoretical improvement is important. 

- Not clear what they tried to say on page 9
    - Why Theorem 6.2 shows that $\rho \leq \frac{1}{d}$ is not optimal?
    - Impossibility result (from line 304): so basically what authors are trying to say is, that 'their' reduction is not applicable for a tighter result, right? It is not about any reduction from corruption to misspecification. 

- No future works.

Limitations:
The authors adequately addressed the limitations. It would be great if authors provide possible future works. 
There is no potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studied the corrupted linear bandits. The authors propose four different metrics to evaluate the total corruption in Eq. (1). Many settings are considered in this paper. For stochastic LB, the proposed algorithm achieves a regret bound of $d\sqrt{T}+\sqrt{d} C_{\infty}$. For adversarial LB, the proposed algorithm achieves a regret bound in the order of $d\sqrt{T}+\sqrt{d} C_{\infty}$ or $d^{3}\sqrt{T}+d^{5/2} C$. The authors also consider the gap-dependent misspecification, where the misspecification level of an arm $a$ can be evaluated by $\rho$ times the gap of arm $a$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
See summary.

Weaknesses:
**Weaknesses and Questions:**
1. At lines 107-109, the authors claim that the strong adversary is equivalent to the CM viewpoint. This doesn't seem right. For regret, the strong adversary is harder than the CM viewpoint. Thus, it is unfair and wrong to compare He et al. (2022) in the same way.
2. At line 131, adversarial linear bandits are discussed. However, no problem definition of this problem is introduced before line 131.
3. This paper studies the fixed action set, while the previous works He et al. (2022) and Foster et al. (2020) allow the action set to be chosen by an adaptive adversary, which is much harder than this paper. Table 1 is not fair. He et al. (2022) is for the adaptive adversarial viewpoint, which is totally different from the stochastic LB. For a fixed action set, the optimal regret without $C$ should be $\sqrt{d T \log k}$, where $k$ is the number of arms.
4. Assumption 1 is not very reasonable.

Limitations:
N.A.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wqLC4G1GN3;"REVIEW 
Summary:
The paper addresses the limitations of existing diffusion-based inverse problem solvers, which typically frame signal recovery as a probabilistic sampling task. The authors propose a novel approach that redefines the generative process as a discrete optimal control task. Inspired by the iterative Linear Quadratic Regulator  algorithm, this new framework named diffusion optimal control, can handle various differentiable forward measurement operators, including super-resolution, inpainting, and deblurring.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper introduces a novel framework based on optimal control theory to solve diffusion-based inverse problems, moving away from the traditional probabilistic sampling approaches. This is a significant theoretical advancement.
2. The framework addresses critical drawbacks of current methods, such as the intractability of the conditional likelihood function and dependence on score network approximations. This leads to more robust and potentially more accurate solutions.

Weaknesses:
1. The method involves complex mathematical formulations and optimal control theory, which may pose challenges for implementation and understanding by practitioners who are not familiar with these concepts. The need to compute Jacobian and Hessian matrices, as well as the regularized inverses, may lead to significant computational demands, particularly in high-dimensional settings.

2. Lacking of enough experiments, such as MRI reconstruction or other medical images. Including more diverse datasets and additional baseline methods would provide a more comprehensive evaluation.

Limitations:
The method requires the forward measurement operator to be differentiable. In cases where this is not possible or practical, the applicability of the proposed framework may be limited.

The performance evaluations rely on specific pretrained models. The method’s robustness and performance with other pretrained models, or models trained on different data distributions, would need further investigation.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes diffusion optimal control that solves inverse problems via posterior sampling by combining the power of a pre-trained unconditional diffusion model and the iterative Linear Quadratic Regulator algorithm to produce optimal controls that steer the reverse diffusion process to correctly recover the original signal. 
The framework is general and able to handle any differentiable forward measurement operator and establishes a new baseline in image reconstruction.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The idea of augmenting the reverse diffusion sampling process with a perturbation control is quite novel and general for arbitrary cost functions, although the paper focuses specifically on the cost for posterior sampling.
* The writing is generally good (see more comments regarding writing in questions). It is concise and to the point with good intuitions provided.
* Efforts (e.g. low-rank approximations) have been made to bring down the computational cost in iLQR for the high-dimensional image setting.
* The empirical performance of the proposed method is strong and establishes new state-of-the-art results.

Weaknesses:
* The runtime of the proposed method seems high and is not much discussed. iLQR is a global-in-time iterative method that could potentially require a large number of iterations to converge (and all nice things discussed in Section 4. rely on this convergence). On top of that, for each iteration, there needs to be $\Omega(T)$ matrix solves which can be quite slow given the dimension of the images (even with techniques like low-rank approximation).  It would be interesting to see ablation studies on the effect of num_iter in Algorithm 1. It would be also more convincing to report the runtime of each method in Table 1.
* There is no analysis of the approximation error of iLQR (the first and second-order Taylor approximations) in the studied setting. Specifically, it seems to me that a lot of heavy lifting is done when the control $u_t$ is designed to be only a perturbation of the reverse diffusion step. For instance, does this imply that the value function is smoother (hence the Taylor approximation is more accurate) when parameterized by $u_t$?

Limitations:
There's not much discussion about the limitations in the paper. There is no potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new approach to conditional generation tasks through score-based diffusion models, with a focus on inverse problems.   As an alternative to using the likelihood $p(y | x_t)$ to guide the time-reversed SDE towards the posterior distribution, the authors reformulate this as an optimal control problem.    Starting from the ODE-based time-reversed flow for the unconditioned prior, the authors derive a controller based on the iLQR algorithm to guide the particles towards high posterior probability regions.    The authors provide theory to demonstrate that the optimal guidance coincides precisely with the desired conditional scores.  They demonstrate the method on a number of benchmarks including image inpainting and other inverse problems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well written and very clear.   The method appears novel and addresses a legitimate challenge in conditional diffusion models.  As the authors acknowledge: optimal control formulation of diffusions exist, but not (to my knowledge) in the context of guiding conditional diffusion models.     The theoretical results provide a sound justification of the validity of the approach.   The numerical results demonstrate that it is competitive in terms of accuracy compared to baseline, established methodology.

Weaknesses:
The main weakness is the sheer computational cost of the algorithm,  the need to compute very expensive hessians drastically limits the practical use of this method.    The authors suggest a number of low rank approximations to mitigate this, but it is unclear how much is lost by introducing them.      One point of question is the interplay between the number of diffusion steps $m$ and $T$.   As $m\rightarrow \infty$, for $T$ fixed and large we expect that the baseline conditional diffusion model will improve significantly in accuracy.  Generally, I feel that the configuration of the baseline has not been explored (or if it has, it has not been reported carefully).   Similarly, the authors claim that they have done equivalent budget analysis in the experiments -- I could not find the details of this: is it the case that the computational cost is the equivalent?  Have the author really explored the hyper-parameter space for these methods.

Limitations:
The main limitation is the large computational cost of this methodology.   This has been identified and acknowledged.  The authors have claimed to do an equivalent budget analysis, but this maybe needed a bit more details (wall-clock time, etc).   Potential societal impacts are addressed in the Appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper uses the optimal control theory to solve the diffusion posterior sampling problem by iterative Linear Quadratic Regulator (iLQR) algorithm. The method could be utilized to solve both linear and nonlinear inverse problems.  Experiments on MNIST and FFHQ demonstrate the outperformance of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-written, with a good summary of previous methods and their shortcomings. 

2. The proposed method that interprets the reverse diffusion process as an uncontrolled non-linear dynamical system is novel. Theoretical support is provided to verify the algorithm.

Weaknesses:
1. The method is well-backed but might be computationally exhausting.

2. The experiments are limited. Quantitative results on different datasets and nonlinear inverse problems are lacking.

Limitations:
Yes. They mentioned limitations and impacts in the appendix, which looks good to me.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper tackles inverse problem via the perspective of optimal control. By treating the diffusion process (ODE) as a non-linear dynamic system and the extra guidance term as control signal, the authors manage to optimize the diffusion trajectory via the iterative Linear Quadratic Regulator (iLQR) algorithm. Several techniques are used to make the iLQR algorithm more efficient. This paper show good results on FFHQ dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea is interesting and reasonable. Using optimal control to solve the inverse problem enables us to optimize the whole sampling trajectory and avoid the error for estimating $x_0$ via Tweedie's formula. And the results on FFHQ dataset looks good.

Weaknesses:
1. High computation cost: Despite the advantages mentioned above, one obvious drawback of this method is the potential high computation cost. This includes: 

      a. Computing and storing the Jacobian matrices, which can be of very high dimension, can be very costly. Although the authors 
      further propose some techniques to reducing the cost, these methods might also bring extra approximation error as well as more hyper-parameters to tune; 

     b. Optimizing the the whole trajectory requires evaluating the whole trajectory for many times and do iterative updates. This requires more computation. Thus, though in Table 1, the authors denoted their methods as $T=50$ and $T=20$, considering the iterative update nature over the whole trajectory, this might not be directly comparable (and might actually need more computation) to other methods, which are denoted as $T=1000$. And the authors might have to greatly reduce the timesteps to make the whole algorithm affordable, this might also bring extra approximation error.


2. Lack of more complex dataset: Though the authors achieve good performance on FFHQ dataset, considering the human face data is relatively easy (aligned, not very multimodal), it is still not very clear to me how the proposed method can work on more complex dataset, for example, on ImageNet. From my own experience, the ImageNet data can be much harder than the FFHQ human face data in the inverse problem. And considering the approximation error introduced in iLQR algorithm, computing the Jacobian matrices as well as using less timesteps, it might raise concerning regarding whether the proposed algorithm can work well on more complex dataset.

3. Minor suggestion: I think it might be better for the authors to add more introduction for the optimal control part in the main paper. Or at least give more clear introduction for the notation used in 2.3. Currently, I find it not very clear to people without much background in optimal control.

Limitations:
The authors has adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper uses tools from optimal control to introduce a novel approach for solving inverse problems with diffusion models. The authors propose reframing the generative process of diffusion models as a discrete optimal control problem allowing to leverage the iterative Linear Quadratic Regulator (iLQR) algorithm. Tackling limitations of existing probabilistic sampling methods, the resulting method demonstrates promising performance for inverse problems on FFHQ, such as super-resolution, inpainting, and deblurring.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
While many connections between optimal control and diffusion models have been established, the proposed algorithm leverages variants of iLQR to provide a fresh perspective on training-free posterior sampling with diffusion models. The paper provides additional theoretical guarantees as well as multiple modifications (randomized low-rank approximations, matrix-free evaluations, and adaptive optimizers) to reduce computational costs. Finally, several ablations are presented for the proposed method.

Weaknesses:
1. The claims of the paper are not sufficiently supported by experiments and/or theory (the first statements in the following are just examples---similar statements can be found throughout the paper):
	* ""dependence on the approximation quality of the underlying terms in the diffusion process"": only a result for a single image is provided (Fig. 6). The current paper also does not seem to provide theoretical results for such robustness as claimed in ""reconstruction performance is theoretically and empirically robust to the accuracy of the approximated prior score"".
	* ""its sensitivity to the temporal discretization scheme"": for the baselines, again only a result for a single image is provided (Fig. 3). Moreover, the number of steps is typically reduced to accelerate the algorithm. Accordingly, methods should be compared in terms of performance vs. runtime/flops and not the number of diffusion steps. It seems that the proposed method is significantly more expensive than competing methods (in particular, since `num_iters>=50` full simulations are used).
	* ""its inherent inaccuracy due to the intractability of the conditional score function"": The conditional score function remains intractable, one just obtains an approximation via iLQR, since the obtained $x_0$'s obtained from the iLQR iterations only *approximately* converge to the posterior distribution *in the limit*. Statements like ""Moreover, our model always estimates $x_0$ exactly, rather than forming an approximation $\hat{x}_0 \approx x_0$"" sound misleading. Using iLQRs, we simulate ""nominal"" trajectories and thus iteratively obtain an approximate candidate for $x_0$ which will be used for the refinement of the control. In a similar (however, useless) fashion one could also use, e.g., DPS to obtain an estimate of $x_0$ and then run a probability flow ODE simulation where the scores are conditioned on $x_0$ (instead of $x_t$) to have a ""method [that] produces controls that coincide precisely with the desired conditional scores"". However, the advantage of DPS lies in the fact that only a single simulation is needed.
	* ""on several inverse problem tasks across several datasets"": Apart from a single figure on MNIST (without metrics and for only a single baseline and task), results are only provided for FFHQ.

2. Moreover, several of the mentioned limitations have been already tackled by alternative approaches to posterior sampling with diffusion models, e.g., variational approaches (https://arxiv.org/abs/2305.04391) or resampling strategies (https://arxiv.org/abs/2307.08123).
3. Finally, the appendix could provide further details on
	* hyperparameter choices and optimization for the baselines.
	* precise assumptions for the theorems.

Limitations:
See ""weaknesses"" above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper uses the optimal control theory to solve the diffusion posterior sampling problem by iterative Linear Quadratic Regulator (iLQR) algorithm. The method could be utilized to solve both linear and nonlinear inverse problems.  Experiments on MNIST and FFHQ demonstrate the outperformance of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-written, with a good summary of previous methods and their shortcomings. 

2. The proposed method that interprets the reverse diffusion process as an uncontrolled non-linear dynamical system is novel. Theoretical support is provided to verify the algorithm.

Weaknesses:
1. The method is well-backed but might be computationally exhausting.

2. The experiments are limited. Quantitative results on different datasets and nonlinear inverse problems are lacking.

Limitations:
Yes. They mentioned limitations and impacts in the appendix, which looks good to me.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper tackles inverse problem via the perspective of optimal control. By treating the diffusion process (ODE) as a non-linear dynamic system and the extra guidance term as control signal, the authors manage to optimize the diffusion trajectory via the iterative Linear Quadratic Regulator (iLQR) algorithm. Several techniques are used to make the iLQR algorithm more efficient. This paper show good results on FFHQ dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea is interesting and reasonable. Using optimal control to solve the inverse problem enables us to optimize the whole sampling trajectory and avoid the error for estimating $x_0$ via Tweedie's formula. And the results on FFHQ dataset looks good.

Weaknesses:
1. High computation cost: Despite the advantages mentioned above, one obvious drawback of this method is the potential high computation cost. This includes: 

      a. Computing and storing the Jacobian matrices, which can be of very high dimension, can be very costly. Although the authors 
      further propose some techniques to reducing the cost, these methods might also bring extra approximation error as well as more hyper-parameters to tune; 

     b. Optimizing the the whole trajectory requires evaluating the whole trajectory for many times and do iterative updates. This requires more computation. Thus, though in Table 1, the authors denoted their methods as $T=50$ and $T=20$, considering the iterative update nature over the whole trajectory, this might not be directly comparable (and might actually need more computation) to other methods, which are denoted as $T=1000$. And the authors might have to greatly reduce the timesteps to make the whole algorithm affordable, this might also bring extra approximation error.


2. Lack of more complex dataset: Though the authors achieve good performance on FFHQ dataset, considering the human face data is relatively easy (aligned, not very multimodal), it is still not very clear to me how the proposed method can work on more complex dataset, for example, on ImageNet. From my own experience, the ImageNet data can be much harder than the FFHQ human face data in the inverse problem. And considering the approximation error introduced in iLQR algorithm, computing the Jacobian matrices as well as using less timesteps, it might raise concerning regarding whether the proposed algorithm can work well on more complex dataset.

3. Minor suggestion: I think it might be better for the authors to add more introduction for the optimal control part in the main paper. Or at least give more clear introduction for the notation used in 2.3. Currently, I find it not very clear to people without much background in optimal control.

Limitations:
The authors has adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the limitations of existing diffusion-based inverse problem solvers, which typically frame signal recovery as a probabilistic sampling task. The authors propose a novel approach that redefines the generative process as a discrete optimal control task. Inspired by the iterative Linear Quadratic Regulator  algorithm, this new framework named diffusion optimal control, can handle various differentiable forward measurement operators, including super-resolution, inpainting, and deblurring.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper introduces a novel framework based on optimal control theory to solve diffusion-based inverse problems, moving away from the traditional probabilistic sampling approaches. This is a significant theoretical advancement.
2. The framework addresses critical drawbacks of current methods, such as the intractability of the conditional likelihood function and dependence on score network approximations. This leads to more robust and potentially more accurate solutions.

Weaknesses:
1. The method involves complex mathematical formulations and optimal control theory, which may pose challenges for implementation and understanding by practitioners who are not familiar with these concepts. The need to compute Jacobian and Hessian matrices, as well as the regularized inverses, may lead to significant computational demands, particularly in high-dimensional settings.

2. Lacking of enough experiments, such as MRI reconstruction or other medical images. Including more diverse datasets and additional baseline methods would provide a more comprehensive evaluation.

Limitations:
The method requires the forward measurement operator to be differentiable. In cases where this is not possible or practical, the applicability of the proposed framework may be limited.

The performance evaluations rely on specific pretrained models. The method’s robustness and performance with other pretrained models, or models trained on different data distributions, would need further investigation.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes diffusion optimal control that solves inverse problems via posterior sampling by combining the power of a pre-trained unconditional diffusion model and the iterative Linear Quadratic Regulator algorithm to produce optimal controls that steer the reverse diffusion process to correctly recover the original signal. 
The framework is general and able to handle any differentiable forward measurement operator and establishes a new baseline in image reconstruction.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The idea of augmenting the reverse diffusion sampling process with a perturbation control is quite novel and general for arbitrary cost functions, although the paper focuses specifically on the cost for posterior sampling.
* The writing is generally good (see more comments regarding writing in questions). It is concise and to the point with good intuitions provided.
* Efforts (e.g. low-rank approximations) have been made to bring down the computational cost in iLQR for the high-dimensional image setting.
* The empirical performance of the proposed method is strong and establishes new state-of-the-art results.

Weaknesses:
* The runtime of the proposed method seems high and is not much discussed. iLQR is a global-in-time iterative method that could potentially require a large number of iterations to converge (and all nice things discussed in Section 4. rely on this convergence). On top of that, for each iteration, there needs to be $\Omega(T)$ matrix solves which can be quite slow given the dimension of the images (even with techniques like low-rank approximation).  It would be interesting to see ablation studies on the effect of num_iter in Algorithm 1. It would be also more convincing to report the runtime of each method in Table 1.
* There is no analysis of the approximation error of iLQR (the first and second-order Taylor approximations) in the studied setting. Specifically, it seems to me that a lot of heavy lifting is done when the control $u_t$ is designed to be only a perturbation of the reverse diffusion step. For instance, does this imply that the value function is smoother (hence the Taylor approximation is more accurate) when parameterized by $u_t$?

Limitations:
There's not much discussion about the limitations in the paper. There is no potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new approach to conditional generation tasks through score-based diffusion models, with a focus on inverse problems.   As an alternative to using the likelihood $p(y | x_t)$ to guide the time-reversed SDE towards the posterior distribution, the authors reformulate this as an optimal control problem.    Starting from the ODE-based time-reversed flow for the unconditioned prior, the authors derive a controller based on the iLQR algorithm to guide the particles towards high posterior probability regions.    The authors provide theory to demonstrate that the optimal guidance coincides precisely with the desired conditional scores.  They demonstrate the method on a number of benchmarks including image inpainting and other inverse problems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well written and very clear.   The method appears novel and addresses a legitimate challenge in conditional diffusion models.  As the authors acknowledge: optimal control formulation of diffusions exist, but not (to my knowledge) in the context of guiding conditional diffusion models.     The theoretical results provide a sound justification of the validity of the approach.   The numerical results demonstrate that it is competitive in terms of accuracy compared to baseline, established methodology.

Weaknesses:
The main weakness is the sheer computational cost of the algorithm,  the need to compute very expensive hessians drastically limits the practical use of this method.    The authors suggest a number of low rank approximations to mitigate this, but it is unclear how much is lost by introducing them.      One point of question is the interplay between the number of diffusion steps $m$ and $T$.   As $m\rightarrow \infty$, for $T$ fixed and large we expect that the baseline conditional diffusion model will improve significantly in accuracy.  Generally, I feel that the configuration of the baseline has not been explored (or if it has, it has not been reported carefully).   Similarly, the authors claim that they have done equivalent budget analysis in the experiments -- I could not find the details of this: is it the case that the computational cost is the equivalent?  Have the author really explored the hyper-parameter space for these methods.

Limitations:
The main limitation is the large computational cost of this methodology.   This has been identified and acknowledged.  The authors have claimed to do an equivalent budget analysis, but this maybe needed a bit more details (wall-clock time, etc).   Potential societal impacts are addressed in the Appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper uses tools from optimal control to introduce a novel approach for solving inverse problems with diffusion models. The authors propose reframing the generative process of diffusion models as a discrete optimal control problem allowing to leverage the iterative Linear Quadratic Regulator (iLQR) algorithm. Tackling limitations of existing probabilistic sampling methods, the resulting method demonstrates promising performance for inverse problems on FFHQ, such as super-resolution, inpainting, and deblurring.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
While many connections between optimal control and diffusion models have been established, the proposed algorithm leverages variants of iLQR to provide a fresh perspective on training-free posterior sampling with diffusion models. The paper provides additional theoretical guarantees as well as multiple modifications (randomized low-rank approximations, matrix-free evaluations, and adaptive optimizers) to reduce computational costs. Finally, several ablations are presented for the proposed method.

Weaknesses:
1. The claims of the paper are not sufficiently supported by experiments and/or theory (the first statements in the following are just examples---similar statements can be found throughout the paper):
	* ""dependence on the approximation quality of the underlying terms in the diffusion process"": only a result for a single image is provided (Fig. 6). The current paper also does not seem to provide theoretical results for such robustness as claimed in ""reconstruction performance is theoretically and empirically robust to the accuracy of the approximated prior score"".
	* ""its sensitivity to the temporal discretization scheme"": for the baselines, again only a result for a single image is provided (Fig. 3). Moreover, the number of steps is typically reduced to accelerate the algorithm. Accordingly, methods should be compared in terms of performance vs. runtime/flops and not the number of diffusion steps. It seems that the proposed method is significantly more expensive than competing methods (in particular, since `num_iters>=50` full simulations are used).
	* ""its inherent inaccuracy due to the intractability of the conditional score function"": The conditional score function remains intractable, one just obtains an approximation via iLQR, since the obtained $x_0$'s obtained from the iLQR iterations only *approximately* converge to the posterior distribution *in the limit*. Statements like ""Moreover, our model always estimates $x_0$ exactly, rather than forming an approximation $\hat{x}_0 \approx x_0$"" sound misleading. Using iLQRs, we simulate ""nominal"" trajectories and thus iteratively obtain an approximate candidate for $x_0$ which will be used for the refinement of the control. In a similar (however, useless) fashion one could also use, e.g., DPS to obtain an estimate of $x_0$ and then run a probability flow ODE simulation where the scores are conditioned on $x_0$ (instead of $x_t$) to have a ""method [that] produces controls that coincide precisely with the desired conditional scores"". However, the advantage of DPS lies in the fact that only a single simulation is needed.
	* ""on several inverse problem tasks across several datasets"": Apart from a single figure on MNIST (without metrics and for only a single baseline and task), results are only provided for FFHQ.

2. Moreover, several of the mentioned limitations have been already tackled by alternative approaches to posterior sampling with diffusion models, e.g., variational approaches (https://arxiv.org/abs/2305.04391) or resampling strategies (https://arxiv.org/abs/2307.08123).
3. Finally, the appendix could provide further details on
	* hyperparameter choices and optimization for the baselines.
	* precise assumptions for the theorems.

Limitations:
See ""weaknesses"" above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wpGJ2AX6SZ;"REVIEW 
Summary:
This paper introduces a new framework into algorithmic predictions. The paper asks and answers the question ""how can we incorporate human input into the prediction algorithm, which may not even be captured in the training data""? The authors develop a method that first runs the predictor, and then runs a second predictor using the human input. The authors show that even a simple instantiation of their method can outperform existing predictors. They use the X-ray classification task as experimental datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is written very clearly, and offers a novel method to incorporate human input into algorithmic prediction. Both theoretical derivations and experiment results are sound. The contributions of this paper is significant, and I believe this paper deserves to be accepted in its current form.

Weaknesses:
The paper would be even more satisfying if the method is presented as a framework rather than a specific instantiation. In addition, it would be great if the authors can discuss potential ways to improve on the method they propose, and what these methods mean in the broader context of incorporating human feedback into algorithmic predictions. Nevertheless, these small weaknesses does not diminish the significance and novelty of this paper.

Limitations:
The authors have addressed the limitations in the conclusion section

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework to incorporate human expert knowledge in algorithmic predictions. Under this framework, the authors introduce a meta-algorithm that uses a training dataset including human expert predictions together with a multi calibrated partition of the data; a partition of the dataset into bins, where each bin contains data that are indistinguishable to the predictive model. Using the data of each bin the meta-algorithm trains a regression algorithm to predict the true label from the human expert prediction. In this way, the authors aim to leverage the human expertise, that may be more accurate than the predictive algorithm on specific instances, to achieve complimentary—to achieve higher predictive accuracy through human AI collaboration than the performance of a human expert or AI in isolation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper suggests an elegant method to improve algorithmic predictions in light of human expertise, that could have significant applications such as the medical domain, where the additional information of human experts may lead them to more accurate predictions on certain instances compared ot predictive models.    

The paper is very well and clearly written, nicely motivated and follows a clear structure. There is a thorough and comprehensive discussion on related work as well as a comprehensive and clearly presented experimental evaluation.

Weaknesses:
Since the theoretical results of section 6 complement the ones of section 4, it would be perhaps more natural to follow them, rather than placing them after the experimental evaluation, which appears a bit odd.

Limitations:
The authors adequately discuss the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper first presents some theory for the modelling of how to identify when human judgements may offer a better diagnosis - through access to additional information - than machine predictions, despite the latter typically being more accurate. This is followed by exploring how to integrate the human input with the algorithmic (model) input. Subsequently, the authors present some focussed experimental results using chest x-ray interpretation that support their proposition.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: carefully drawn comparison with the literature, situates and differentiates the contribution.

Quality + Clarity (addressed together):

Clear abstract and intro with well-defined contributions. Content offers a reasonable balance between technical and intuitive. Recognition of the value of the human contribution and seeking to integrate it in decision making.

The later mathematical results (section 4) have effective accompanying interpretations (see complementary point in weaknesses).

Effective, selective presentation of results: choosing one and going into detail, while two other cases in the appendices support the same observation, rather than trying to squeeze them all into the paper body. Same applies to results in section 5.2.

Significance: provides a sound framework for a particular, amenable class of collaboration problems that allows for the proper incorporation of human prediction where machine prediction could fall short.

Weaknesses:
Clarity: Indistinguishability and multicalibration are critical elements to the contribution; it would be helpful if the interpretation of their definitions (3.1, 3.2) went into a bit more detail for accessibility.

This reader is not succeeding in following the argument about robustness (section 6).

Limitations:
Section 7 provides some properly reflective critique on scope and applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a framework for joint human-AI prediction, where human experts can augment AI predictions in particular ex ante identifiable subsets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper makes a lot of interesting contributions. First, its scope is broad and important: it tackles the question of how and whether human judgment can improve the predictions of any learning algorithm. That is and will remain to be a very important question in our time. It contributes a very interesting framework, rooted in algorithmic indistinguishability and multicalibration, to find subsets in which no algorithm in a user-specified class has predictive power (because they are algorithmically indistinguishable) but human experts do (because they might have more access to the instances, such as doctors examining patients). It demonstrates that using this framework, we can find subsets of instances where human experts can outperform algorithms, and thus the combination of the two can outperform either alone. It applies this to an important medical problem and in another domain of making predictions from photos of people. It even extends the framework to apply to a setting with noncompliance. The community stands to learn a lot from this paper.

Weaknesses:
As the authors mention, the framework is dependent on minimizing mean squared error only.

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a new framework into algorithmic predictions. The paper asks and answers the question ""how can we incorporate human input into the prediction algorithm, which may not even be captured in the training data""? The authors develop a method that first runs the predictor, and then runs a second predictor using the human input. The authors show that even a simple instantiation of their method can outperform existing predictors. They use the X-ray classification task as experimental datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is written very clearly, and offers a novel method to incorporate human input into algorithmic prediction. Both theoretical derivations and experiment results are sound. The contributions of this paper is significant, and I believe this paper deserves to be accepted in its current form.

Weaknesses:
The paper would be even more satisfying if the method is presented as a framework rather than a specific instantiation. In addition, it would be great if the authors can discuss potential ways to improve on the method they propose, and what these methods mean in the broader context of incorporating human feedback into algorithmic predictions. Nevertheless, these small weaknesses does not diminish the significance and novelty of this paper.

Limitations:
The authors have addressed the limitations in the conclusion section

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework to incorporate human expert knowledge in algorithmic predictions. Under this framework, the authors introduce a meta-algorithm that uses a training dataset including human expert predictions together with a multi calibrated partition of the data; a partition of the dataset into bins, where each bin contains data that are indistinguishable to the predictive model. Using the data of each bin the meta-algorithm trains a regression algorithm to predict the true label from the human expert prediction. In this way, the authors aim to leverage the human expertise, that may be more accurate than the predictive algorithm on specific instances, to achieve complimentary—to achieve higher predictive accuracy through human AI collaboration than the performance of a human expert or AI in isolation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper suggests an elegant method to improve algorithmic predictions in light of human expertise, that could have significant applications such as the medical domain, where the additional information of human experts may lead them to more accurate predictions on certain instances compared ot predictive models.    

The paper is very well and clearly written, nicely motivated and follows a clear structure. There is a thorough and comprehensive discussion on related work as well as a comprehensive and clearly presented experimental evaluation.

Weaknesses:
Since the theoretical results of section 6 complement the ones of section 4, it would be perhaps more natural to follow them, rather than placing them after the experimental evaluation, which appears a bit odd.

Limitations:
The authors adequately discuss the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper first presents some theory for the modelling of how to identify when human judgements may offer a better diagnosis - through access to additional information - than machine predictions, despite the latter typically being more accurate. This is followed by exploring how to integrate the human input with the algorithmic (model) input. Subsequently, the authors present some focussed experimental results using chest x-ray interpretation that support their proposition.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: carefully drawn comparison with the literature, situates and differentiates the contribution.

Quality + Clarity (addressed together):

Clear abstract and intro with well-defined contributions. Content offers a reasonable balance between technical and intuitive. Recognition of the value of the human contribution and seeking to integrate it in decision making.

The later mathematical results (section 4) have effective accompanying interpretations (see complementary point in weaknesses).

Effective, selective presentation of results: choosing one and going into detail, while two other cases in the appendices support the same observation, rather than trying to squeeze them all into the paper body. Same applies to results in section 5.2.

Significance: provides a sound framework for a particular, amenable class of collaboration problems that allows for the proper incorporation of human prediction where machine prediction could fall short.

Weaknesses:
Clarity: Indistinguishability and multicalibration are critical elements to the contribution; it would be helpful if the interpretation of their definitions (3.1, 3.2) went into a bit more detail for accessibility.

This reader is not succeeding in following the argument about robustness (section 6).

Limitations:
Section 7 provides some properly reflective critique on scope and applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a framework for joint human-AI prediction, where human experts can augment AI predictions in particular ex ante identifiable subsets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper makes a lot of interesting contributions. First, its scope is broad and important: it tackles the question of how and whether human judgment can improve the predictions of any learning algorithm. That is and will remain to be a very important question in our time. It contributes a very interesting framework, rooted in algorithmic indistinguishability and multicalibration, to find subsets in which no algorithm in a user-specified class has predictive power (because they are algorithmically indistinguishable) but human experts do (because they might have more access to the instances, such as doctors examining patients). It demonstrates that using this framework, we can find subsets of instances where human experts can outperform algorithms, and thus the combination of the two can outperform either alone. It applies this to an important medical problem and in another domain of making predictions from photos of people. It even extends the framework to apply to a setting with noncompliance. The community stands to learn a lot from this paper.

Weaknesses:
As the authors mention, the framework is dependent on minimizing mean squared error only.

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
woRFmNJiLp;"REVIEW 
Summary:
This paper proposed a new method for LLM alignment during pre-training. The proposed method is call ""native alignment"". This method include three steps: pretrain date duplication, alignment rewriting, and model training. They trained small size alignment expert model for alignment rewriting and use the model to rewrite large-scale pre-training data. The rewriting process suppose to solve format issue, value/fairness issue, unsafe content in pre-training data. They experimented with Arabic data and LLMs. Their experiments shows that the proposed method can help LLMs be more safe and helpful.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper proposed a new idea to align LLMs during pre-training. It seems an interesting topic. 
2. The paper writing is clear and well-organized.

Weaknesses:
1. Lack of comparison to existing post-alignment methods. The proposed method is a ""native alignment"" during pre-training. I wonder if this method can outperform the post-alignment methods. While the author acknowledged this limitation, I still feel it is important for strengthening their claim. 
2. Need more analyses to better understand their method's potential trade-off. For example, I wonder if rewriting pre-training data undermines the LLM's capacity to understand and learn Arabic dialects. The rewriting process may convert Arabic dialects into MSA. I also wonder if the rewriting data inherited hallucinations from LLM and deteriorated the trained model. 
3. The paper needs more clarification on experiment details. For example, they exploit Arabic data to investigate their method, however, the evaluation dataset, BeaverTails dataset, is an English dataset. I wonder how they evaluate and if they translate the samples.

Limitations:
I think that the paper needs more diverse analyses to understand the potential trade-off of their method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduce a method called ""native alignment"", which is a set of procedures to create data and train an LLM to rewrite raw text into ""useful"" texts for pretraining. They apply this technique specifically for Arabic LLMs and conduct experiments to show that this pre-processing of pre-training data helps produce better Arabic LLM down the line. As bonus, they release open-source Arabic LLMs for the communities

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper ideas are presented clearly and easy-to-understand

Weaknesses:
* As a proclaimed novelty, the paper draws itself between pre-alignment and post-alignment, indicating that previous work only focus on post-alignment but not pre-alignment. However, I afraid the paper misunderstands the concept of post-alignment (RLHF) and fails make an accurate comparison. Post alignment (RLHF) is finetuning technique to train the models to reward good-vs-bad response according to human values, and train the policy models to lean on the good behavior and stay-away from the bad behaviors gradually, often with the existence of a reference model (DPO and RLHF).

Meanwhile, the ""native alignment"" presented in the paper is a data-cleaning procedure, and it does not having any resemblance or contrast with ""post-alignment"". Furthermore, using LLMs or training LLMs to rewrite raw text to produce cleaner data is not new or novel, there are many techniques out there that do so, and there are abundant open-source data on huggingface which were produced in similar ways.
This confusion between data cleaning and alignment makes the paper less credible and the lack of novelty it the methodology itself, as a data cleaning method, is also troublesome.

Obviously as a result, the paper did not provide any necessary and required experimental comparisons with other data cleaning methods.

* Though I do appreciate the paper's effort for Arabic community, the scope of only Arabic LLM is small and generally inconclusive, that such method is not shown to generalize to other languages, domains. Perhaps, thus, the work is really not suitable for NeurIPS but more suitable for CL-type venues

* It is unclear from the writing whether the authors pretrained Llama-3 with Arabic from scratch or further finetune from Llama-3 checkpoint. In either case, there should be explanation and further ablation studies.

Limitations:
The authors discussed limitations

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a data augmentation pipeline which modifies the pre training data for large language models in key aspects such as formatting, values, content moderation and knowledge preservation. The resulting pipeline, termed native alignment, is applied Arabic LLMs due to the relatively small pretraining corpus available and the difference between Arabic and western culture. Experiments are conducted to test the performance on a few metrics including trustworthiness, knowledge, and Arabic localisation.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This is a well written paper targeting the important topic of llm alignment. It also addresses the relatively under explored sub question of how to improve alignment at pretraining. The resulting pipeline presents a reasonable idea, and the evaluations are clear and I find them comprehensive too. The author(s) should also be commended for their transparency regarding the limitations of the paper.

Weaknesses:
Although this might have become the norm of recent LLM papers, I still think it is important to include a discussion of the metrics used to measure things like 'trustworthiness' and 'knowledge', as these are qualitative metrics, whereas in the paper, it seems like the authors just quoted some existing evaluation pipeline.

Limitations:
As the authors are already open about, comparisons with other post alignment methods are not included. The authors attribute this to an absence of existing alignment evaluation benchmark, but I don't fully understand this - what is stopping the authors from using the same alignment benchmarks as ones they have already used to compare with other pretrained models?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on alignment of LLMs to human preferences and suggests to shift the alignment step from instruction-tuning (post-alignment) to the earlier stage of continued pre-training (native alignment). For that end it proposes an approach to creating aligned pre-training data, consisting of three steps: (1) seed data cleanup and rewriting with humans'/LLM help, (2) training a supervised cleanup model on that seed set and (3) processing the final pre-training dataset with that cleanup model. Presented experiments show that alignment data results in higher final quality compared to unprocessed pre-training data and that the performance gain does not reach a plateau at 12B tokens, suggesting that the amount of alignment data should be limited by the budget allocated to train an LLM. Experiments are performed on Llama-3-8B and Llama-3-70B and the Arabic language.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A high-impact and efficient approach to pre-aligned model training is introduced

- Two pre-aligned LLMs for Arabic are released openly based on the experiments in this paper

- Related work is excellent, the paper is written very clearly and is easy to comprehend

Weaknesses:
1. No direct comparison between native alignment and post-alignment is reported

2. Minor text discrepancies are present:
- rows 16-18: partial sentence ""while.."" is not finished
- row 47: missing verb: ""LLaMA3-Tamed-8B could beneficial"" --> ""LLaMA3-Tamed-8B could be beneficial""
- row 326: typo: ""instruction tinning"" --> ""instruction tuning""
- row 150: ""pre-training"" should be called ""continued pre-training"" in this case

3. The created seed data and cleanup models are not released

Limitations:
Ok

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a new method for LLM alignment during pre-training. The proposed method is call ""native alignment"". This method include three steps: pretrain date duplication, alignment rewriting, and model training. They trained small size alignment expert model for alignment rewriting and use the model to rewrite large-scale pre-training data. The rewriting process suppose to solve format issue, value/fairness issue, unsafe content in pre-training data. They experimented with Arabic data and LLMs. Their experiments shows that the proposed method can help LLMs be more safe and helpful.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper proposed a new idea to align LLMs during pre-training. It seems an interesting topic. 
2. The paper writing is clear and well-organized.

Weaknesses:
1. Lack of comparison to existing post-alignment methods. The proposed method is a ""native alignment"" during pre-training. I wonder if this method can outperform the post-alignment methods. While the author acknowledged this limitation, I still feel it is important for strengthening their claim. 
2. Need more analyses to better understand their method's potential trade-off. For example, I wonder if rewriting pre-training data undermines the LLM's capacity to understand and learn Arabic dialects. The rewriting process may convert Arabic dialects into MSA. I also wonder if the rewriting data inherited hallucinations from LLM and deteriorated the trained model. 
3. The paper needs more clarification on experiment details. For example, they exploit Arabic data to investigate their method, however, the evaluation dataset, BeaverTails dataset, is an English dataset. I wonder how they evaluate and if they translate the samples.

Limitations:
I think that the paper needs more diverse analyses to understand the potential trade-off of their method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduce a method called ""native alignment"", which is a set of procedures to create data and train an LLM to rewrite raw text into ""useful"" texts for pretraining. They apply this technique specifically for Arabic LLMs and conduct experiments to show that this pre-processing of pre-training data helps produce better Arabic LLM down the line. As bonus, they release open-source Arabic LLMs for the communities

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper ideas are presented clearly and easy-to-understand

Weaknesses:
* As a proclaimed novelty, the paper draws itself between pre-alignment and post-alignment, indicating that previous work only focus on post-alignment but not pre-alignment. However, I afraid the paper misunderstands the concept of post-alignment (RLHF) and fails make an accurate comparison. Post alignment (RLHF) is finetuning technique to train the models to reward good-vs-bad response according to human values, and train the policy models to lean on the good behavior and stay-away from the bad behaviors gradually, often with the existence of a reference model (DPO and RLHF).

Meanwhile, the ""native alignment"" presented in the paper is a data-cleaning procedure, and it does not having any resemblance or contrast with ""post-alignment"". Furthermore, using LLMs or training LLMs to rewrite raw text to produce cleaner data is not new or novel, there are many techniques out there that do so, and there are abundant open-source data on huggingface which were produced in similar ways.
This confusion between data cleaning and alignment makes the paper less credible and the lack of novelty it the methodology itself, as a data cleaning method, is also troublesome.

Obviously as a result, the paper did not provide any necessary and required experimental comparisons with other data cleaning methods.

* Though I do appreciate the paper's effort for Arabic community, the scope of only Arabic LLM is small and generally inconclusive, that such method is not shown to generalize to other languages, domains. Perhaps, thus, the work is really not suitable for NeurIPS but more suitable for CL-type venues

* It is unclear from the writing whether the authors pretrained Llama-3 with Arabic from scratch or further finetune from Llama-3 checkpoint. In either case, there should be explanation and further ablation studies.

Limitations:
The authors discussed limitations

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a data augmentation pipeline which modifies the pre training data for large language models in key aspects such as formatting, values, content moderation and knowledge preservation. The resulting pipeline, termed native alignment, is applied Arabic LLMs due to the relatively small pretraining corpus available and the difference between Arabic and western culture. Experiments are conducted to test the performance on a few metrics including trustworthiness, knowledge, and Arabic localisation.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This is a well written paper targeting the important topic of llm alignment. It also addresses the relatively under explored sub question of how to improve alignment at pretraining. The resulting pipeline presents a reasonable idea, and the evaluations are clear and I find them comprehensive too. The author(s) should also be commended for their transparency regarding the limitations of the paper.

Weaknesses:
Although this might have become the norm of recent LLM papers, I still think it is important to include a discussion of the metrics used to measure things like 'trustworthiness' and 'knowledge', as these are qualitative metrics, whereas in the paper, it seems like the authors just quoted some existing evaluation pipeline.

Limitations:
As the authors are already open about, comparisons with other post alignment methods are not included. The authors attribute this to an absence of existing alignment evaluation benchmark, but I don't fully understand this - what is stopping the authors from using the same alignment benchmarks as ones they have already used to compare with other pretrained models?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on alignment of LLMs to human preferences and suggests to shift the alignment step from instruction-tuning (post-alignment) to the earlier stage of continued pre-training (native alignment). For that end it proposes an approach to creating aligned pre-training data, consisting of three steps: (1) seed data cleanup and rewriting with humans'/LLM help, (2) training a supervised cleanup model on that seed set and (3) processing the final pre-training dataset with that cleanup model. Presented experiments show that alignment data results in higher final quality compared to unprocessed pre-training data and that the performance gain does not reach a plateau at 12B tokens, suggesting that the amount of alignment data should be limited by the budget allocated to train an LLM. Experiments are performed on Llama-3-8B and Llama-3-70B and the Arabic language.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A high-impact and efficient approach to pre-aligned model training is introduced

- Two pre-aligned LLMs for Arabic are released openly based on the experiments in this paper

- Related work is excellent, the paper is written very clearly and is easy to comprehend

Weaknesses:
1. No direct comparison between native alignment and post-alignment is reported

2. Minor text discrepancies are present:
- rows 16-18: partial sentence ""while.."" is not finished
- row 47: missing verb: ""LLaMA3-Tamed-8B could beneficial"" --> ""LLaMA3-Tamed-8B could be beneficial""
- row 326: typo: ""instruction tinning"" --> ""instruction tuning""
- row 150: ""pre-training"" should be called ""continued pre-training"" in this case

3. The created seed data and cleanup models are not released

Limitations:
Ok

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
woENr7FJaI;"REVIEW 
Summary:
This paper presents the Automated Multi-level Preference (AMP) framework for improving MLLMs by addressing hallucination issues. The framework introduces a multi-level preference system for RLHF, aiming to enhance the learning process by providing more granular feedback.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The introduction of multi-level preferences rather than binary ones narrows the gap between adjacent levels, enabling MLLMs to discern subtle differences and integrate cross-level comparisons.
- The automated pipeline for generating high-quality multi-level preference datasets without human annotators is a significant contribution, potentially reducing bias and noise while saving time and resources.
- Extensive experiments across multiple benchmarks demonstrate the effectiveness of the proposed method.

Weaknesses:
- The contribution of the paper heavily relies on the preference fine-tuning algorithm, showing limited innovation beyond this aspect.
- The method does not demonstrate significant improvements on the LLaVA-Bench benchmark.
- The method's performance on the adversarial tasks of the POPE benchmark is moderate, suggesting a need to reconsider the impact of MDPO on model robustness and how to balance performance and robustness.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors develop an automated dataset generation pipeline capable of producing multi-level preference datasets without the need for human annotators. This paper introduces a novel multi-round dialogues hallucination benchmark, MRHal-Bench. Additionally, the authors design the Multi-level Direct Preference Optimization (MDPO) algorithm, which employs a specifically crafted learning objective to facilitate multi-level preference learning. Extensive experiments conducted on both the hallucination benchmark and a general benchmark demonstrate the effectiveness of this method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. To make the labeling of multi-level preference datasets cost-effective and efficient, this paper proposes an automated dataset generation pipeline capable of producing high-quality preference datasets.

2. To narrow the gap between two preference samples in DPO and make the model more easily distinguish the differences between preference data, this paper proposes a multi-level DPO algorithm that use multi-level preference data to provide a broader range of comparisons with hallucination examples.

Weaknesses:
1. It is recommended to provide more quantitative information on the preference dataset generated by the automated dataset generation pipeline. For instance, the authors could use a subset of the dataset to demonstrate the similarity results compared to human annotators.
2. In this paper, the authors conduct experiments on three hallucination benchmarks and only one general benchmark. To verify the more general applicability of the method, additional experiments are needed on general benchmarks such as TextVQA, GQA, and IconQA.
3. In Table 1, the authors compare several MLLMs and RLHF-based MLLMs across MMHal-Bench, MRHal-Bench and LLaVA-Bench. However, the baseline model should be more up-to-date. Could you compare it with more current models such as LLaVA-v1.6, DeepSeek-VL, or MiniCPM-V?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work aims to mitigate hallucinations in Multimodal Large Language Models through preference optimization. Motivated by two limitations of binary preferences widely used in existing work, authors proposed a multi-level preference framework. The framework consists of 1) an automated dataset generation pipeline that converts each image-text pair into an image with multiple text descriptions from superior to inferior quality 2) a Multi-level Direct Preference Optimization algorithm that enumerates over all preference pairs with the standard DPO objective. Additionally, authors introduce a new hallucination benchmark, MRHal-Bench. The proposed framework has been evaluated on three benchmarks: MMHal-Bench, LLaVA-Bench, and MRHal-Bench against 5 base models and 5 preference fine-tuned model. The proposed framework achieves best state-of-the-art on MMHal-Bench and MRHal-Bench, although only improved over the second best FGAIF by a small margin. Authors also include comprehensive ablation studies on the effects of multi-level preference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The application of multi-level preference alignment to the problem of mitigating hallucination in multimodal LLMs is novel.
* Conduct a comprehensive comparison with existing preference fine-tuned multimodal LLMs and baselines on three benchmarks. Improve over existing methods by a small margin.
* Provide an extensive ablation study of the multi-level preference term.

Additionally, automating of the multi-level preference data generation could be a potential strength as well, but currently lacks evaluation to justify its quality (see weakness).

Weaknesses:
I would like to see authors address the following weaknesses: 

* **Lack intrinsic evaluation of the automated multi-level preference dataset**. The quality is only implicitly justified by the improvement on the three final benchmarks (L258-L264), which makes it unclear what are the artifacts introduced in the automated data generation. Although human or GPT-4 annotation can be inconsistent sometimes, it is still good to collect some annotations to directly assess how the generated preferences align with the degree of hallucination. Similarly, the current auto-check mechanism is ad-hoc and introduces another component, i.e., CLIP, which could introduce additional errors into the system. It would be good to conduct some evaluation on the auto-check mechanism as well. 
* **Missing comparison with rank-based preference alignment approaches**: Despite being a novel application, non-binary preference alignment has been studied both theoretically and empirically in context other than hallucination in MLLMs, for example Zhu et al. 2023 [1], Brown et al. [2], Myers et al. [3],  Song et al. [4]. It would be great if this work could engage with prior literature on non-binary preference alignment, for example, discussing how does the proposed objective compare with ranking-based approach in prior work?
* **Missing results of FGAIF on MRHal-Bench** In Table 1, FGAIF has a performance that is considerably close to the proposed methods (-0.14, +0.05) on MMHal-Bench and outperform the proposed method on LLaVA-Bench, yet it's missing results MRHal-Bench. These missing numbers could affect the comparison between the two methods.

References:
* [1] Zhu et al. Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons.
* [2] Brown et al. Safe imitation learning via fast bayesian reward inference from preferences.
* [3] Myers et al. Learning Multimodal Rewards from Rankings.
* [4] Song et al. Preference Ranking Optimization for Human Alignment.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents the Automated Multi-level Preference (AMP) framework for improving MLLMs by addressing hallucination issues. The framework introduces a multi-level preference system for RLHF, aiming to enhance the learning process by providing more granular feedback.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The introduction of multi-level preferences rather than binary ones narrows the gap between adjacent levels, enabling MLLMs to discern subtle differences and integrate cross-level comparisons.
- The automated pipeline for generating high-quality multi-level preference datasets without human annotators is a significant contribution, potentially reducing bias and noise while saving time and resources.
- Extensive experiments across multiple benchmarks demonstrate the effectiveness of the proposed method.

Weaknesses:
- The contribution of the paper heavily relies on the preference fine-tuning algorithm, showing limited innovation beyond this aspect.
- The method does not demonstrate significant improvements on the LLaVA-Bench benchmark.
- The method's performance on the adversarial tasks of the POPE benchmark is moderate, suggesting a need to reconsider the impact of MDPO on model robustness and how to balance performance and robustness.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors develop an automated dataset generation pipeline capable of producing multi-level preference datasets without the need for human annotators. This paper introduces a novel multi-round dialogues hallucination benchmark, MRHal-Bench. Additionally, the authors design the Multi-level Direct Preference Optimization (MDPO) algorithm, which employs a specifically crafted learning objective to facilitate multi-level preference learning. Extensive experiments conducted on both the hallucination benchmark and a general benchmark demonstrate the effectiveness of this method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. To make the labeling of multi-level preference datasets cost-effective and efficient, this paper proposes an automated dataset generation pipeline capable of producing high-quality preference datasets.

2. To narrow the gap between two preference samples in DPO and make the model more easily distinguish the differences between preference data, this paper proposes a multi-level DPO algorithm that use multi-level preference data to provide a broader range of comparisons with hallucination examples.

Weaknesses:
1. It is recommended to provide more quantitative information on the preference dataset generated by the automated dataset generation pipeline. For instance, the authors could use a subset of the dataset to demonstrate the similarity results compared to human annotators.
2. In this paper, the authors conduct experiments on three hallucination benchmarks and only one general benchmark. To verify the more general applicability of the method, additional experiments are needed on general benchmarks such as TextVQA, GQA, and IconQA.
3. In Table 1, the authors compare several MLLMs and RLHF-based MLLMs across MMHal-Bench, MRHal-Bench and LLaVA-Bench. However, the baseline model should be more up-to-date. Could you compare it with more current models such as LLaVA-v1.6, DeepSeek-VL, or MiniCPM-V?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work aims to mitigate hallucinations in Multimodal Large Language Models through preference optimization. Motivated by two limitations of binary preferences widely used in existing work, authors proposed a multi-level preference framework. The framework consists of 1) an automated dataset generation pipeline that converts each image-text pair into an image with multiple text descriptions from superior to inferior quality 2) a Multi-level Direct Preference Optimization algorithm that enumerates over all preference pairs with the standard DPO objective. Additionally, authors introduce a new hallucination benchmark, MRHal-Bench. The proposed framework has been evaluated on three benchmarks: MMHal-Bench, LLaVA-Bench, and MRHal-Bench against 5 base models and 5 preference fine-tuned model. The proposed framework achieves best state-of-the-art on MMHal-Bench and MRHal-Bench, although only improved over the second best FGAIF by a small margin. Authors also include comprehensive ablation studies on the effects of multi-level preference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The application of multi-level preference alignment to the problem of mitigating hallucination in multimodal LLMs is novel.
* Conduct a comprehensive comparison with existing preference fine-tuned multimodal LLMs and baselines on three benchmarks. Improve over existing methods by a small margin.
* Provide an extensive ablation study of the multi-level preference term.

Additionally, automating of the multi-level preference data generation could be a potential strength as well, but currently lacks evaluation to justify its quality (see weakness).

Weaknesses:
I would like to see authors address the following weaknesses: 

* **Lack intrinsic evaluation of the automated multi-level preference dataset**. The quality is only implicitly justified by the improvement on the three final benchmarks (L258-L264), which makes it unclear what are the artifacts introduced in the automated data generation. Although human or GPT-4 annotation can be inconsistent sometimes, it is still good to collect some annotations to directly assess how the generated preferences align with the degree of hallucination. Similarly, the current auto-check mechanism is ad-hoc and introduces another component, i.e., CLIP, which could introduce additional errors into the system. It would be good to conduct some evaluation on the auto-check mechanism as well. 
* **Missing comparison with rank-based preference alignment approaches**: Despite being a novel application, non-binary preference alignment has been studied both theoretically and empirically in context other than hallucination in MLLMs, for example Zhu et al. 2023 [1], Brown et al. [2], Myers et al. [3],  Song et al. [4]. It would be great if this work could engage with prior literature on non-binary preference alignment, for example, discussing how does the proposed objective compare with ranking-based approach in prior work?
* **Missing results of FGAIF on MRHal-Bench** In Table 1, FGAIF has a performance that is considerably close to the proposed methods (-0.14, +0.05) on MMHal-Bench and outperform the proposed method on LLaVA-Bench, yet it's missing results MRHal-Bench. These missing numbers could affect the comparison between the two methods.

References:
* [1] Zhu et al. Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons.
* [2] Brown et al. Safe imitation learning via fast bayesian reward inference from preferences.
* [3] Myers et al. Learning Multimodal Rewards from Rankings.
* [4] Song et al. Preference Ranking Optimization for Human Alignment.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wnPlJNiqfA;"REVIEW 
Summary:
This paper proposes a novel algorithm, KFNN (K-free Nearest Neighbor), which is specifically designed to enhance label integration for crowdsourcing. KFNN integrates two key components named label distribution enhancement and K-free optimization, which significantly contribute to improving the effectiveness and robustness of the label integration process. The idea of automatically determining the optimal neighborhood size for each instance is particularly innovative and well-executed. The experimental results further validate the effectiveness and robustness of the proposed algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The KFNN proposed in this paper is interesting and innovative. The authors reveal the limitations of fixed neighborhood sizes in existing label integration algorithms and propose an algorithm that automatically determines the optimal neighborhood size based on instance attributes and noisy labels. This algorithm significantly improves the robustness of label integration.
2.	The paper provides a solid theoretical foundation for the proposed KFNN algorithm, followed by comprehensive experimental validation. The theoretical analysis is robust and convincingly demonstrates the expected performance improvements. The experiments are well-designed and cover a wide range of datasets, both simulated and real-world, to ensure the generalizability of the results. The experimental results, including comparisons with baseline algorithms, further validate the effectiveness and robustness of the proposed algorithm.
3.	The paper is well-written and clearly presents the proposed methodology and findings. The structure of the paper is logical, making it easy to follow the complex concepts introduced. The use of figures and tables to illustrate key points is effective and aids in comprehension.

Weaknesses:
1. While the paper provides strong theoretical and experimental results, there is limited discussion on the computational efficiency and scalability of the proposed KFNN algorithm. I suggest moving the algorithmic flow and time complexity analysis from Appendix A to the main text.
2. There are some repetitive sentences and structures in this paper that should be further condensed. For example, Sections 5.1 and 5.2 should be merged and the repetitive statements in them should be deleted.
3. The experiments are already comprehensive, but analysis and discussion of the optimal neighborhood size determined by KFNN could still be added, which would help to understand how the neighborhood size should be set. Moreover, according to the results presented in Tables 1-4, KFNN is generally highly effective. However, on few datasets, KFNN does not perform as well as MV. These anomalies are valuable for identifying deficiencies in KFNN and should be further investigated and discussed.

Limitations:
The authors openly discuss the limitations of their work, particularly the empirical parameters in the Kalman filter and the roughness of the distribution transformation process. Please refer to the Weaknesses for other limitations I have found.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel label integration algorithm, KFNN (K-Free Nearest Neighbor), designed to enhance the performance of crowdsourcing platforms by intelligently determining the optimal neighborhood size for each instance based on its attributes and noisy labels. The authors propose a two-component solution involving label distribution enhancement and K-free optimization, which leverages the Mahalanobis distance and a Kalman filter to mitigate noise from neighbor instances. The paper's claims are well-aligned with the theoretical and experimental results, demonstrating the effectiveness and robustness of KFNN against existing state-of-the-art algorithms in various crowdsourcing scenarios.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.	Novel contribution to an important problem
The innovative approach of highlighting the limitations caused by fixed neighborhood sizes in existing label integration algorithms, and using attributes and noisy labels to determine the neighborhood size for each instance automatically, is a significant contribution to crowdsourcing. 

2.	Complete and rigorous theoretical proof
The theoretical underpinnings are sound, with clear assumptions and proofs provided for the proposed methods. The use of the Mahalanobis distance and the Kalman filter is well-justified.

3.	Good writing quality and clarity
This paper is well-written and enjoyable to read. The challenges are clearly stated and the contributions are easy to capture. 

4.	Reproducibility
The paper's open data and code policy is highly appreciated, promoting research transparency. Enhancing reproducibility with clear versioning and setup instructions would be a valuable addition, showcasing a strong commitment to open scientific practices.

Weaknesses:
1.	Simulation experiment results
The symbol • indicates that the algorithm in the row significantly outperforms the algorithm in the corresponding column. How is ""significantly outperforms"" defined for Macro-F1 score and integration accuracy?

2.	Ablation experiment results
Since this study focuses on automatically adjusting neighborhood sizes, how does the performance of this method compare with baselines that use fixed neighborhood sizes?

Limitations:
see weaknesses

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel label integration approach KFNN by adaptively determining the optimal neighborhood size. KFNN utilizes a Mahalanobis distance distribution to model the relationship between each instance and all classes. The authors also provide adequate theoretical analysis to illustrate the effectiveness of the proposed method. Experiments demonstrate that the proposed method can achieve the state-of-the-art performance on simulation and real-world dataset. The paper is well-written and easy to follow. This idea is very intuitive and effective for crowdsourcing task. The paper proves the effectiveness of introducing Mahalanobis distance distribution for crowdsourcing from the perspective of methodology, theory and experiments.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to follow. The logic of the whole paper is clear.
2. The paper’s idea is very intuitive and effective for crowdsourcing task. The authors introduce the Mahalanobis distance distribution to model the relationship between each instance and all classes. Experiments verify that the proposed method can achieve the best performance compared with SOTAs.
3. The authors provide adequate evidences to verify the effectiveness of the proposed method from the perspective of methodology, theory and experiments on simulation and real-world datasets.

Weaknesses:
1. In section 2, the authors introduce two categories of label integration algorithms. And the proposed KFNN belongs to the algorithms which leverage neighbor instance. I suggest adding some discussion about the pros and cons of these two categories of approaches.
2. In methodology part and theoretical analysis part, the authors discuss the superiority of Mahalanobis distance compared with Euclidean distance. Can the authors verify the difference between Mahalanobis distance and Euclidean distance on this task from an experimental perspective?
3. In Table 3 and Table 4, why some results are missing? Appropriate explanation facilitates reading of the paper.

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new algorithm for label integration called KFNN. Existing methods related to KNN produce more noisy labels; however, they fix the neighborhood size, regardless of the fact that instances close to the center of classes should have more neighbors than instances close to the boundary of classes. To tackle this problem, KFNN estimates a Mahalanobis distance distribution between each instance and all classes to enhance the multiple noisy label distribution and utilizes a Kalman filter to mitigate the impact of noise. Finally, KFNN can automatically determine the optimal neighborhood size through max-margin learning.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1. The paper studies an important problem.   
S2. A new solution is proposed to tackle the problem.   
S3. Experiments are conducted on several datasets.

Weaknesses:
W1. The motivations need more enhancements.   
W2. Some technical details require more explanations.   
W3. The application scope of the proposed method in crowdsourcing is limited.    
W4. The performance improvement of the proposed method is unsatisfactory.    
W5. Experiments are conducted in a simulation environment, which can be much simpler than a real-world crowdsourcing platform.

Limitations:
Please refer to the weaknesses and questions.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel algorithm, KFNN (K-free Nearest Neighbor), which is specifically designed to enhance label integration for crowdsourcing. KFNN integrates two key components named label distribution enhancement and K-free optimization, which significantly contribute to improving the effectiveness and robustness of the label integration process. The idea of automatically determining the optimal neighborhood size for each instance is particularly innovative and well-executed. The experimental results further validate the effectiveness and robustness of the proposed algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The KFNN proposed in this paper is interesting and innovative. The authors reveal the limitations of fixed neighborhood sizes in existing label integration algorithms and propose an algorithm that automatically determines the optimal neighborhood size based on instance attributes and noisy labels. This algorithm significantly improves the robustness of label integration.
2.	The paper provides a solid theoretical foundation for the proposed KFNN algorithm, followed by comprehensive experimental validation. The theoretical analysis is robust and convincingly demonstrates the expected performance improvements. The experiments are well-designed and cover a wide range of datasets, both simulated and real-world, to ensure the generalizability of the results. The experimental results, including comparisons with baseline algorithms, further validate the effectiveness and robustness of the proposed algorithm.
3.	The paper is well-written and clearly presents the proposed methodology and findings. The structure of the paper is logical, making it easy to follow the complex concepts introduced. The use of figures and tables to illustrate key points is effective and aids in comprehension.

Weaknesses:
1. While the paper provides strong theoretical and experimental results, there is limited discussion on the computational efficiency and scalability of the proposed KFNN algorithm. I suggest moving the algorithmic flow and time complexity analysis from Appendix A to the main text.
2. There are some repetitive sentences and structures in this paper that should be further condensed. For example, Sections 5.1 and 5.2 should be merged and the repetitive statements in them should be deleted.
3. The experiments are already comprehensive, but analysis and discussion of the optimal neighborhood size determined by KFNN could still be added, which would help to understand how the neighborhood size should be set. Moreover, according to the results presented in Tables 1-4, KFNN is generally highly effective. However, on few datasets, KFNN does not perform as well as MV. These anomalies are valuable for identifying deficiencies in KFNN and should be further investigated and discussed.

Limitations:
The authors openly discuss the limitations of their work, particularly the empirical parameters in the Kalman filter and the roughness of the distribution transformation process. Please refer to the Weaknesses for other limitations I have found.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel label integration algorithm, KFNN (K-Free Nearest Neighbor), designed to enhance the performance of crowdsourcing platforms by intelligently determining the optimal neighborhood size for each instance based on its attributes and noisy labels. The authors propose a two-component solution involving label distribution enhancement and K-free optimization, which leverages the Mahalanobis distance and a Kalman filter to mitigate noise from neighbor instances. The paper's claims are well-aligned with the theoretical and experimental results, demonstrating the effectiveness and robustness of KFNN against existing state-of-the-art algorithms in various crowdsourcing scenarios.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.	Novel contribution to an important problem
The innovative approach of highlighting the limitations caused by fixed neighborhood sizes in existing label integration algorithms, and using attributes and noisy labels to determine the neighborhood size for each instance automatically, is a significant contribution to crowdsourcing. 

2.	Complete and rigorous theoretical proof
The theoretical underpinnings are sound, with clear assumptions and proofs provided for the proposed methods. The use of the Mahalanobis distance and the Kalman filter is well-justified.

3.	Good writing quality and clarity
This paper is well-written and enjoyable to read. The challenges are clearly stated and the contributions are easy to capture. 

4.	Reproducibility
The paper's open data and code policy is highly appreciated, promoting research transparency. Enhancing reproducibility with clear versioning and setup instructions would be a valuable addition, showcasing a strong commitment to open scientific practices.

Weaknesses:
1.	Simulation experiment results
The symbol • indicates that the algorithm in the row significantly outperforms the algorithm in the corresponding column. How is ""significantly outperforms"" defined for Macro-F1 score and integration accuracy?

2.	Ablation experiment results
Since this study focuses on automatically adjusting neighborhood sizes, how does the performance of this method compare with baselines that use fixed neighborhood sizes?

Limitations:
see weaknesses

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel label integration approach KFNN by adaptively determining the optimal neighborhood size. KFNN utilizes a Mahalanobis distance distribution to model the relationship between each instance and all classes. The authors also provide adequate theoretical analysis to illustrate the effectiveness of the proposed method. Experiments demonstrate that the proposed method can achieve the state-of-the-art performance on simulation and real-world dataset. The paper is well-written and easy to follow. This idea is very intuitive and effective for crowdsourcing task. The paper proves the effectiveness of introducing Mahalanobis distance distribution for crowdsourcing from the perspective of methodology, theory and experiments.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to follow. The logic of the whole paper is clear.
2. The paper’s idea is very intuitive and effective for crowdsourcing task. The authors introduce the Mahalanobis distance distribution to model the relationship between each instance and all classes. Experiments verify that the proposed method can achieve the best performance compared with SOTAs.
3. The authors provide adequate evidences to verify the effectiveness of the proposed method from the perspective of methodology, theory and experiments on simulation and real-world datasets.

Weaknesses:
1. In section 2, the authors introduce two categories of label integration algorithms. And the proposed KFNN belongs to the algorithms which leverage neighbor instance. I suggest adding some discussion about the pros and cons of these two categories of approaches.
2. In methodology part and theoretical analysis part, the authors discuss the superiority of Mahalanobis distance compared with Euclidean distance. Can the authors verify the difference between Mahalanobis distance and Euclidean distance on this task from an experimental perspective?
3. In Table 3 and Table 4, why some results are missing? Appropriate explanation facilitates reading of the paper.

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new algorithm for label integration called KFNN. Existing methods related to KNN produce more noisy labels; however, they fix the neighborhood size, regardless of the fact that instances close to the center of classes should have more neighbors than instances close to the boundary of classes. To tackle this problem, KFNN estimates a Mahalanobis distance distribution between each instance and all classes to enhance the multiple noisy label distribution and utilizes a Kalman filter to mitigate the impact of noise. Finally, KFNN can automatically determine the optimal neighborhood size through max-margin learning.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1. The paper studies an important problem.   
S2. A new solution is proposed to tackle the problem.   
S3. Experiments are conducted on several datasets.

Weaknesses:
W1. The motivations need more enhancements.   
W2. Some technical details require more explanations.   
W3. The application scope of the proposed method in crowdsourcing is limited.    
W4. The performance improvement of the proposed method is unsatisfactory.    
W5. Experiments are conducted in a simulation environment, which can be much simpler than a real-world crowdsourcing platform.

Limitations:
Please refer to the weaknesses and questions.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
OuQYWNuNxm;"REVIEW 
Summary:
The authors provide a formalization on how to introduce search heuristics for channel-simulation (sometimes called ""relative entropy coding"" in the ML literature). The encoder and decoder agree a priori on a binning scheme that divides the support of the prior/public distribution, which is used to control the search during encoding.

Disclaimer: while I'm familiar with the basic literature (e.g. REC, PFRL, ORC), I haven't kept up with the literature since 2022.

I'm more than willing to change my scores if the authors point out I missed something or didn't fully understand the contributions.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is nicely written. This is notably a complicated topic to understand and I feel the authors made quite a didactic effort.
- The excess code-length is bounded for the distributions found in practice for NTC.

Weaknesses:
- Line 21: I feel this statement is misleading. In the first scenario (lines 14 to 20), the actual value taken on by the random variable is what is being transmitted. Meanwhile, in channel-simulation, all we care about is that the received quantity be distributed according to $Q_{Z | X}$. These are fundamentally different problems. The comparison would make some sense if, in the first scenario, the decoder is necessarily stochastic, as decoding can be seen as sampling from some posterior distribution over the data conditioned on the latent.

Limitations:
- Line 267: The results are computationally intensive, and thus impractical, even for MNIST. Requiring the value of mutual information to be known by encoder/decoder seems a bit much (the authors mention this in the limitations section).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
**Global disclaimer:** I am very unfamiliar with the topic of the paper. I did my best to try and read the literature and understand as much as I could but my input may be very limited.

**Summary:**
The paper focus on relative entropy coding (REC) algorithms and propose to circumvent a major pitfall which is the prohibitive encoding time. Indeed the commonly used PFR algorithm requires to sample points until finding a good enough alignment with the target distribution. Efficient algorithms only exist in dimension 1. The authors propose to refine the algorithm by partitioning the search space into bins that will better drive the sampling from the coding to the target distribution.
The author then theoretically prove that their approach by is founded (Theorem 3.1), and derive precise codelength bounds for the space partition algorithm. They also discuss a generic partitionning strategy.
Finally, a benchmark highlight the benefits of the space partition approach for compression against the standard PFR algorithm on toy example and real datasets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The paper is very didactic, well written and is quite nice to read. The proposed approached is well structured and detailed (sections 3.1). The theory exposed in the paper is sound and the proof seem correct to me.
The benchmark is well made and honest.
Finally the appendix is very helpful, I particularly appreciated appendix B that shed some light on the partition strategy.

Weaknesses:
The idea is finally quite elementary: dividing the space once for all into bins. The proofs are ""elementary"" (still technical) refinements from the classical PFR theory (starting point is often [Li and El Gamal, 2018] and [ Flamich, 2024]).

Limitations:
The approach is not fully generic and despite very good success on some examples, it could fail on other as clearly explained in Appendix B.

Typos: the punctuation missing in the maths of the appendix

**Proposed score:**
My rating is based on the fact that the paper is sound, the maths are correct but the scope of the idea is quite simple. I do believe that this work is well presented, interesting and worth publication. On the other hand I am not familiar with this field. As such, I propose a score of 5/10 (a weak accept) but I am very open to discussion with the authors and the other reviewers.

Rating:
8: accept, good paper

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a space partioning technique to speed up  relative entropy coding.
In entropy coding the sender first transforms X into a representation Z that they encode.
This particular step can done  by Poisson functional representation (PFR).
Unfortunately, PFR’s random runtime can be a significant drawback.

To circumvent this issue,  the author propose a space partitioning scheme to reduce runtime in practical scenarios.
This can be seen (sic) as a search heuristic to a standard REC algorithm

The author then proceed to give a theoretical analysis of the method and support their improvement with numerical results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper adresses an important practical problem with many important application.
The algorithm and theoretical results are stated clearly.
The context and related work is clear to the reviewer (who is not an expert in the field).
The numerics show clearly the advantage/improvements  yielded by the technique.

Weaknesses:
Although the proposed technique is ""simple and efficient"", it lacks discussion whether the binning technique impacts the final result (besides codelength).

Limitations:
The authors recognize some clear limitations of their approach in the conclusion section.
In particular, the approach proposed by the authors is too limited and does not extend to all possible practical cases.
The reviewer agrees with those limitations, although still believes they should not be an obstacle for publication. as this work is a ""first step"" in this direction.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors provide a formalization on how to introduce search heuristics for channel-simulation (sometimes called ""relative entropy coding"" in the ML literature). The encoder and decoder agree a priori on a binning scheme that divides the support of the prior/public distribution, which is used to control the search during encoding.

Disclaimer: while I'm familiar with the basic literature (e.g. REC, PFRL, ORC), I haven't kept up with the literature since 2022.

I'm more than willing to change my scores if the authors point out I missed something or didn't fully understand the contributions.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is nicely written. This is notably a complicated topic to understand and I feel the authors made quite a didactic effort.
- The excess code-length is bounded for the distributions found in practice for NTC.

Weaknesses:
- Line 21: I feel this statement is misleading. In the first scenario (lines 14 to 20), the actual value taken on by the random variable is what is being transmitted. Meanwhile, in channel-simulation, all we care about is that the received quantity be distributed according to $Q_{Z | X}$. These are fundamentally different problems. The comparison would make some sense if, in the first scenario, the decoder is necessarily stochastic, as decoding can be seen as sampling from some posterior distribution over the data conditioned on the latent.

Limitations:
- Line 267: The results are computationally intensive, and thus impractical, even for MNIST. Requiring the value of mutual information to be known by encoder/decoder seems a bit much (the authors mention this in the limitations section).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
**Global disclaimer:** I am very unfamiliar with the topic of the paper. I did my best to try and read the literature and understand as much as I could but my input may be very limited.

**Summary:**
The paper focus on relative entropy coding (REC) algorithms and propose to circumvent a major pitfall which is the prohibitive encoding time. Indeed the commonly used PFR algorithm requires to sample points until finding a good enough alignment with the target distribution. Efficient algorithms only exist in dimension 1. The authors propose to refine the algorithm by partitioning the search space into bins that will better drive the sampling from the coding to the target distribution.
The author then theoretically prove that their approach by is founded (Theorem 3.1), and derive precise codelength bounds for the space partition algorithm. They also discuss a generic partitionning strategy.
Finally, a benchmark highlight the benefits of the space partition approach for compression against the standard PFR algorithm on toy example and real datasets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The paper is very didactic, well written and is quite nice to read. The proposed approached is well structured and detailed (sections 3.1). The theory exposed in the paper is sound and the proof seem correct to me.
The benchmark is well made and honest.
Finally the appendix is very helpful, I particularly appreciated appendix B that shed some light on the partition strategy.

Weaknesses:
The idea is finally quite elementary: dividing the space once for all into bins. The proofs are ""elementary"" (still technical) refinements from the classical PFR theory (starting point is often [Li and El Gamal, 2018] and [ Flamich, 2024]).

Limitations:
The approach is not fully generic and despite very good success on some examples, it could fail on other as clearly explained in Appendix B.

Typos: the punctuation missing in the maths of the appendix

**Proposed score:**
My rating is based on the fact that the paper is sound, the maths are correct but the scope of the idea is quite simple. I do believe that this work is well presented, interesting and worth publication. On the other hand I am not familiar with this field. As such, I propose a score of 5/10 (a weak accept) but I am very open to discussion with the authors and the other reviewers.

Rating:
8: accept, good paper

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a space partioning technique to speed up  relative entropy coding.
In entropy coding the sender first transforms X into a representation Z that they encode.
This particular step can done  by Poisson functional representation (PFR).
Unfortunately, PFR’s random runtime can be a significant drawback.

To circumvent this issue,  the author propose a space partitioning scheme to reduce runtime in practical scenarios.
This can be seen (sic) as a search heuristic to a standard REC algorithm

The author then proceed to give a theoretical analysis of the method and support their improvement with numerical results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper adresses an important practical problem with many important application.
The algorithm and theoretical results are stated clearly.
The context and related work is clear to the reviewer (who is not an expert in the field).
The numerics show clearly the advantage/improvements  yielded by the technique.

Weaknesses:
Although the proposed technique is ""simple and efficient"", it lacks discussion whether the binning technique impacts the final result (besides codelength).

Limitations:
The authors recognize some clear limitations of their approach in the conclusion section.
In particular, the approach proposed by the authors is too limited and does not extend to all possible practical cases.
The reviewer agrees with those limitations, although still believes they should not be an obstacle for publication. as this work is a ""first step"" in this direction.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wkwGedn19x;"REVIEW 
Summary:
This paper introduces CRATE-α, an enhanced variant of the CRATE (Coding RATE Transformer) architecture, designed to scale efficiently while maintaining mathematical interpretability. The authors address the open question of CRATE's scalability by proposing strategic modifications to the sparse coding block and a refined training recipe. Extensive experiments demonstrate CRATE-α's effectiveness, showcasing improved performance on ImageNet classification tasks compared to the original CRATE model. Notably, the CRATE-α-B model achieved an 83.2% accuracy rate, a significant improvement over the previous best CRATE-B model.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents a novel architecture, CRATE-α, that builds upon the existing CRATE model with minimal yet strategic modifications, enhancing scalability without compromising interpretability.

The authors provide a wealth of empirical evidence supporting the effectiveness of CRATE-α, including comparative results on ImageNet classification tasks and a detailed analysis of training behaviors across different model scales.

A key strength is the paper's focus on maintaining the interpretability of the model, which is often a trade-off in scaling deep learning models. The authors demonstrate that CRATE-α models retain high-quality unsupervised object segmentation capabilities.

The paper includes a thorough exploration of scaling behaviors, from Base to Large to Huge model sizes, using both supervised learning on ImageNet and vision-language pre-training with contrastive learning on DataComp1B.

Weaknesses:
Could the proposed architecture work well on other tasks like NLP?


While the paper provides a detailed analysis of the model's performance on ImageNet, there might be a need for more discussion on how these results generalize to other datasets and real-world applications.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores how to train white-box Transformers at scale for visual tasks. The authors propose a new model architecture called CRATE-$\alpha$, which extends the sparse coding block of the original CRATE model. A series of CRATE-$\alpha$ models were trained with varying model sizes, data sizes, and patch sizes using optimized training recipes. The main experiments focus on supervised classification and contrastive CLIP learning, with additional demonstrations of unsupervised semantic segmentation capability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Originality:** The paper continues the white-box design philosophy of the original CRATE model while integrating advanced techniques such as overparameterized sparse coding, decoupled dictionary, and residual connections. Although some of these techniques have been previously validated, successfully combining them with a white-box Transformer is a noteworthy achievement. The integration not only works effectively but also yields commendable results.

**Quality:** The paper is technically sound overall, employing rigorous notation and formula definitions to elucidate the design principles. The proposed models demonstrate significant improvements compared to the previous generation of CRATE models. Additionally, the authors are careful and honest in evaluating the weaknesses and limitations of their work.

Weaknesses:
**Clarity:**
- The paper is heavily symbolized, relying extensively on intricate mathematical formulations rather than clear diagrams and straightforward language. Although this maintains academic rigor and professionalism, it severely hampers understanding of the paper's details and the broader dissemination of the model. Incorporating corresponding illustrations to explain the three modifications and comparing them with the standard Transformer structure would be beneficial.
- The organization of Section 4 is not concise, making it easy for readers to lose track.
  - The distinction between the paragraphs ""Dataset and Evaluation"" and ""Training & Fine-tuning"" is not well-defined, especially with the scattered descriptions of the data used.
  - The frequent interleaving of experimental setup descriptions with the presentation of experimental results disrupts the flow and coherence of the narrative.

**Significance:** 
- Although CRATE-$\alpha$ shows significant improvements over the original CRATE model, it still lags behind the state-of-the-art. For example, in the right side of Figure 1, CRATE-$\alpha$ typically requires nearly double the training FLOPs to achieve the same accuracy as ViT. 
- If the scalability and interpretability of a white-box Transformer architecture does not offer substantial insights and improvements, practitioners might prefer models with stronger performance but lower interpretability.

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the scalability problem of white-box transformer CRATE and proposes CRATE-$\alpha$ to enhance the scaling ability of CRATE. To be specific, the authors propose three strategic but minimal modifications for the CRATE model architecture: Overparameterized sparse coding block, Decoupled dictionary, and Residual connection. Extensive experiments across different datasets and settings demonstrate the effectiveness of the proposed approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. It is quite meaningful to study white-box transformers and try to increase their scalability which promises its application in potential usage.

2. Comprehensive evaluation. The proposed method is validated on multiple datasets and tasks which demonstrate the scalability of CRATE-$\alpha$.

3. The presentation is clear. Overall, the paper is well-organized and the method is easy to follow.

Weaknesses:
1. Performance gaps with vanilla ViT. As shown in Figure 1, CRATE-$\alpha$ still lags behind vanilla ViT across different scales remarkably which may limit its application in real scenarios. Besides, it is suggested to compare with vanilla ViT in computational costs, number of parameters, and inference speed as well.

2. According to the model configuration, the number of parameters of CRATE-$\alpha$ is almost four times as CRATE and it is strange to consider those as the same scale models. Moreover, how do the proposed new modules contribute to the performance gain of CRATE-$\alpha$? Is it simply because of larger models?

3. Although the authors made lots of efforts in scaling CRATE to CRATE-$\alpha$, they only spent limited space in the paper to discuss the interpretability of the proposed method. This short paragraph may not be enough to justify why the authors are motivated to study the white-box transformers.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to train CRATE at a large scale for vision tasks. The contribution includes an architecture modification to the sparse coding block and a light training recipe. The new model, called CRATE-alpha, shows large improvements compared with the previous CRATE model. The experiments also show promising results on unsupervised object segmentation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a careful study to enhance the performance of CRATE. The paper introduces key modifications to the existing CRATE, including the sparse coding block, decoupled dictionary, and residual connection. 
- The paper investigates its scaling behavior and shows promising improvements of the newly introduced CRATE-alpha.
- The paper presents in-depth experiments, such as the scaling analysis on ImageNet. The paper also shows improvements for semantic interpretability. 
- The figures and model architecture are well-illustrated.

Weaknesses:
Overall I find the paper is well-presented and solid. Below are my minor concerns for this paper:
- The paper is highly centered on improving CRATE. Most of the findings might not be transferable to other models. This may limit its impact to the general audience in NuerIPS community.
- It would be interesting to further understand its potential downstream applications (not only vision but also language data)

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces CRATE-α, an enhanced variant of the CRATE (Coding RATE Transformer) architecture, designed to scale efficiently while maintaining mathematical interpretability. The authors address the open question of CRATE's scalability by proposing strategic modifications to the sparse coding block and a refined training recipe. Extensive experiments demonstrate CRATE-α's effectiveness, showcasing improved performance on ImageNet classification tasks compared to the original CRATE model. Notably, the CRATE-α-B model achieved an 83.2% accuracy rate, a significant improvement over the previous best CRATE-B model.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents a novel architecture, CRATE-α, that builds upon the existing CRATE model with minimal yet strategic modifications, enhancing scalability without compromising interpretability.

The authors provide a wealth of empirical evidence supporting the effectiveness of CRATE-α, including comparative results on ImageNet classification tasks and a detailed analysis of training behaviors across different model scales.

A key strength is the paper's focus on maintaining the interpretability of the model, which is often a trade-off in scaling deep learning models. The authors demonstrate that CRATE-α models retain high-quality unsupervised object segmentation capabilities.

The paper includes a thorough exploration of scaling behaviors, from Base to Large to Huge model sizes, using both supervised learning on ImageNet and vision-language pre-training with contrastive learning on DataComp1B.

Weaknesses:
Could the proposed architecture work well on other tasks like NLP?


While the paper provides a detailed analysis of the model's performance on ImageNet, there might be a need for more discussion on how these results generalize to other datasets and real-world applications.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores how to train white-box Transformers at scale for visual tasks. The authors propose a new model architecture called CRATE-$\alpha$, which extends the sparse coding block of the original CRATE model. A series of CRATE-$\alpha$ models were trained with varying model sizes, data sizes, and patch sizes using optimized training recipes. The main experiments focus on supervised classification and contrastive CLIP learning, with additional demonstrations of unsupervised semantic segmentation capability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Originality:** The paper continues the white-box design philosophy of the original CRATE model while integrating advanced techniques such as overparameterized sparse coding, decoupled dictionary, and residual connections. Although some of these techniques have been previously validated, successfully combining them with a white-box Transformer is a noteworthy achievement. The integration not only works effectively but also yields commendable results.

**Quality:** The paper is technically sound overall, employing rigorous notation and formula definitions to elucidate the design principles. The proposed models demonstrate significant improvements compared to the previous generation of CRATE models. Additionally, the authors are careful and honest in evaluating the weaknesses and limitations of their work.

Weaknesses:
**Clarity:**
- The paper is heavily symbolized, relying extensively on intricate mathematical formulations rather than clear diagrams and straightforward language. Although this maintains academic rigor and professionalism, it severely hampers understanding of the paper's details and the broader dissemination of the model. Incorporating corresponding illustrations to explain the three modifications and comparing them with the standard Transformer structure would be beneficial.
- The organization of Section 4 is not concise, making it easy for readers to lose track.
  - The distinction between the paragraphs ""Dataset and Evaluation"" and ""Training & Fine-tuning"" is not well-defined, especially with the scattered descriptions of the data used.
  - The frequent interleaving of experimental setup descriptions with the presentation of experimental results disrupts the flow and coherence of the narrative.

**Significance:** 
- Although CRATE-$\alpha$ shows significant improvements over the original CRATE model, it still lags behind the state-of-the-art. For example, in the right side of Figure 1, CRATE-$\alpha$ typically requires nearly double the training FLOPs to achieve the same accuracy as ViT. 
- If the scalability and interpretability of a white-box Transformer architecture does not offer substantial insights and improvements, practitioners might prefer models with stronger performance but lower interpretability.

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the scalability problem of white-box transformer CRATE and proposes CRATE-$\alpha$ to enhance the scaling ability of CRATE. To be specific, the authors propose three strategic but minimal modifications for the CRATE model architecture: Overparameterized sparse coding block, Decoupled dictionary, and Residual connection. Extensive experiments across different datasets and settings demonstrate the effectiveness of the proposed approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. It is quite meaningful to study white-box transformers and try to increase their scalability which promises its application in potential usage.

2. Comprehensive evaluation. The proposed method is validated on multiple datasets and tasks which demonstrate the scalability of CRATE-$\alpha$.

3. The presentation is clear. Overall, the paper is well-organized and the method is easy to follow.

Weaknesses:
1. Performance gaps with vanilla ViT. As shown in Figure 1, CRATE-$\alpha$ still lags behind vanilla ViT across different scales remarkably which may limit its application in real scenarios. Besides, it is suggested to compare with vanilla ViT in computational costs, number of parameters, and inference speed as well.

2. According to the model configuration, the number of parameters of CRATE-$\alpha$ is almost four times as CRATE and it is strange to consider those as the same scale models. Moreover, how do the proposed new modules contribute to the performance gain of CRATE-$\alpha$? Is it simply because of larger models?

3. Although the authors made lots of efforts in scaling CRATE to CRATE-$\alpha$, they only spent limited space in the paper to discuss the interpretability of the proposed method. This short paragraph may not be enough to justify why the authors are motivated to study the white-box transformers.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to train CRATE at a large scale for vision tasks. The contribution includes an architecture modification to the sparse coding block and a light training recipe. The new model, called CRATE-alpha, shows large improvements compared with the previous CRATE model. The experiments also show promising results on unsupervised object segmentation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a careful study to enhance the performance of CRATE. The paper introduces key modifications to the existing CRATE, including the sparse coding block, decoupled dictionary, and residual connection. 
- The paper investigates its scaling behavior and shows promising improvements of the newly introduced CRATE-alpha.
- The paper presents in-depth experiments, such as the scaling analysis on ImageNet. The paper also shows improvements for semantic interpretability. 
- The figures and model architecture are well-illustrated.

Weaknesses:
Overall I find the paper is well-presented and solid. Below are my minor concerns for this paper:
- The paper is highly centered on improving CRATE. Most of the findings might not be transferable to other models. This may limit its impact to the general audience in NuerIPS community.
- It would be interesting to further understand its potential downstream applications (not only vision but also language data)

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wiMaws0FWB;"REVIEW 
Summary:
In this paper, authors study the implicit bias of the mirror descent algorithm, from the perspective of the optimization trajectory of the continuous flow version. They propose the conceptions of horizon shape and horizon function $\phi_\infty$ to help characterize the properties of mirror flow at infinity. Since $\phi_\infty$ defines a norm, they prove that the mirror flow will eventually converge in direction to a max $\phi_\infty$-margin solution under the linear exponential-tail classification setting. Their findings contribute to a deeper understanding of the inherent characteristics of mirror descent across a wide range of potential functions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The result of this work is solid, containing the general class of potential functions, and the authors derive a calculation rule of $\phi_\infty$ for a general class of potential functions.
2. The paper is informative and well-structured, particularly in section 3. By using the example of gradient descent within the framework established by the preceding lemmas, which is a special case of mirror descent, the authors clearly outline the reasons why mirror flow will eventually converge in direction without complicated formulas.

Weaknesses:
1. Since mirror descent is not so popular in the practice of machine learning problems,  there could be more discussion about the implications of their results. For example, Figure 1 is really interesting as it reveals that the mirror descent shares the same structure of implicit bias with the steepest descent [1], what is the essence of such similarities?
2. The setting of an infinitely small learning rate, i.e., optimization flow,  might be a little strong under a simple linear classification problem compared to the previous works. I suggest the authors state the technical challenges of the discrete optimization process of mirror descent.
3. I might be wrong, but it seems not strict to apply the Bolzano–Weierstrass theorem to an uncountably infinite set at page 5, line 160 and 61.

[1] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832–1841. PMLR, 2018.

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper examines the implicit bias of mirror flow (the continuous-time counterpart of mirror descent) in the context of binary classification for linearly separable data. Given that the problem has infinitely many solutions, obtained at infinity, the authors aim to identify which solution is achieved through mirror flow. Assuming an exponential tail on the loss function, the authors demonstrate that mirror flow converges directionally to a maximum margin classifier, where the margin is characterized by a horizon function of the mirror potential. This result extends many existing works, and numerical experiments are provided to verify the findings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and well-organized. Despite the technical nature of the analysis and results, the paper is relatively easy to follow.
2. The paper is also well-motivated. Although mirror descent is not commonly used as an algorithm for training neural networks, analyzing its convergence is valuable for understanding the implicit bias of gradient descent in various neural network architectures.
3. I did not verify the details of the proof. However, the paper provides several motivating examples, including the quadratic potential corresponding to gradient flow, which makes the results quite convincing.
4. The main results extend several prior works.

Weaknesses:
Although the authors have stated that the convergence rate is left for future study, it would be beneficial to provide at least empirical evidence of the convergence rate. The authors mentioned in line 294 that the convergence rate varies across different potentials.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This manuscript examines the implicit bias of mirror descent on a classification problem when the dataset is linearly separable. Assuming a coercive gradient, it demonstrates that the implicit bias is characterized by the shape of the level set of the mirror potential near infinity. Their analysis successfully recovers existing results for p-norm potentials and identifies the implicit bias of the potentials emerging in the analysis of linear neural networks. Additionally, it leaves the characterization of the implicit bias when the gradient is not coercive as an interesting open problem.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
I think the paper is very well-written and has a solid contribution.

It addresses an important problem, aiming to understand the implicit bias of neural networks. Prior work has shown that the dynamics of linear networks can be characterized by mirror descent, highlighting the relevance of this study.

Weaknesses:
NA

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers the asymptotic behaviour of the mirror descent (MD) algorithm for a linear classification task. It is shown that the classifier (hyperplane orthogonal to $\beta$) will be a max-margin classifer, where the margin is determined by some unknown horizon function $\phi_\infty$. This works extend prior work which consider $\ell_p$ and homogeneous potential functions for MD, and shows this result for very general $\phi$.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper makes an interesting statement about behaviour of mirror descent on classification tasks, will minimal assumptions. In doing so, it takes a big step and extends previous work to the cover general potential functions.
The paper is well written and the figures help with understanding the concepts of convergence.

---
*While I could understand the paper, this is not my area of research. I do not find myself fit to evaluate the paper on soundness, relevance to the sub-field and importance of contributions.*

Weaknesses:
- The paper does not characterize $\phi_\infty$ in terms of the bregman potential $\phi$ (and other relevant entities). 
The main result expresses that there exists some function, that is minimized by $\bar \beta_\infty$, the direction of the classifier as $t\rightarrow \infty$. 
I think this limits the relevance and strength of the result. For instance, this does not help with interpretability compared to the case where we can prove the optimization algorithm converging to a max-margin classifier wrt the $\ell_2$ norm.

- I am not sure about relevance and use-cases of the mirror descent algorithm with very general potentials. As far as I know, typically, a small set of norm-based or entropic (neg-ent, tsallis, etc) are used within applications of ML. So while the theorem makes an interesting statement for an optimization standpoint, I'm not sure how relevant it is for the ML community. The theorem is also not entirely relevant to the pure optimization community since it's for the specific case of linear classification with finite data.
---
*While I could understand the paper, this is not my area of research. I do not find myself fit to evaluate the paper on soundness, relevance to the sub-field and importance of contributions.*

Limitations:
The limitations are discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, authors study the implicit bias of the mirror descent algorithm, from the perspective of the optimization trajectory of the continuous flow version. They propose the conceptions of horizon shape and horizon function $\phi_\infty$ to help characterize the properties of mirror flow at infinity. Since $\phi_\infty$ defines a norm, they prove that the mirror flow will eventually converge in direction to a max $\phi_\infty$-margin solution under the linear exponential-tail classification setting. Their findings contribute to a deeper understanding of the inherent characteristics of mirror descent across a wide range of potential functions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The result of this work is solid, containing the general class of potential functions, and the authors derive a calculation rule of $\phi_\infty$ for a general class of potential functions.
2. The paper is informative and well-structured, particularly in section 3. By using the example of gradient descent within the framework established by the preceding lemmas, which is a special case of mirror descent, the authors clearly outline the reasons why mirror flow will eventually converge in direction without complicated formulas.

Weaknesses:
1. Since mirror descent is not so popular in the practice of machine learning problems,  there could be more discussion about the implications of their results. For example, Figure 1 is really interesting as it reveals that the mirror descent shares the same structure of implicit bias with the steepest descent [1], what is the essence of such similarities?
2. The setting of an infinitely small learning rate, i.e., optimization flow,  might be a little strong under a simple linear classification problem compared to the previous works. I suggest the authors state the technical challenges of the discrete optimization process of mirror descent.
3. I might be wrong, but it seems not strict to apply the Bolzano–Weierstrass theorem to an uncountably infinite set at page 5, line 160 and 61.

[1] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832–1841. PMLR, 2018.

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper examines the implicit bias of mirror flow (the continuous-time counterpart of mirror descent) in the context of binary classification for linearly separable data. Given that the problem has infinitely many solutions, obtained at infinity, the authors aim to identify which solution is achieved through mirror flow. Assuming an exponential tail on the loss function, the authors demonstrate that mirror flow converges directionally to a maximum margin classifier, where the margin is characterized by a horizon function of the mirror potential. This result extends many existing works, and numerical experiments are provided to verify the findings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and well-organized. Despite the technical nature of the analysis and results, the paper is relatively easy to follow.
2. The paper is also well-motivated. Although mirror descent is not commonly used as an algorithm for training neural networks, analyzing its convergence is valuable for understanding the implicit bias of gradient descent in various neural network architectures.
3. I did not verify the details of the proof. However, the paper provides several motivating examples, including the quadratic potential corresponding to gradient flow, which makes the results quite convincing.
4. The main results extend several prior works.

Weaknesses:
Although the authors have stated that the convergence rate is left for future study, it would be beneficial to provide at least empirical evidence of the convergence rate. The authors mentioned in line 294 that the convergence rate varies across different potentials.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This manuscript examines the implicit bias of mirror descent on a classification problem when the dataset is linearly separable. Assuming a coercive gradient, it demonstrates that the implicit bias is characterized by the shape of the level set of the mirror potential near infinity. Their analysis successfully recovers existing results for p-norm potentials and identifies the implicit bias of the potentials emerging in the analysis of linear neural networks. Additionally, it leaves the characterization of the implicit bias when the gradient is not coercive as an interesting open problem.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
I think the paper is very well-written and has a solid contribution.

It addresses an important problem, aiming to understand the implicit bias of neural networks. Prior work has shown that the dynamics of linear networks can be characterized by mirror descent, highlighting the relevance of this study.

Weaknesses:
NA

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers the asymptotic behaviour of the mirror descent (MD) algorithm for a linear classification task. It is shown that the classifier (hyperplane orthogonal to $\beta$) will be a max-margin classifer, where the margin is determined by some unknown horizon function $\phi_\infty$. This works extend prior work which consider $\ell_p$ and homogeneous potential functions for MD, and shows this result for very general $\phi$.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper makes an interesting statement about behaviour of mirror descent on classification tasks, will minimal assumptions. In doing so, it takes a big step and extends previous work to the cover general potential functions.
The paper is well written and the figures help with understanding the concepts of convergence.

---
*While I could understand the paper, this is not my area of research. I do not find myself fit to evaluate the paper on soundness, relevance to the sub-field and importance of contributions.*

Weaknesses:
- The paper does not characterize $\phi_\infty$ in terms of the bregman potential $\phi$ (and other relevant entities). 
The main result expresses that there exists some function, that is minimized by $\bar \beta_\infty$, the direction of the classifier as $t\rightarrow \infty$. 
I think this limits the relevance and strength of the result. For instance, this does not help with interpretability compared to the case where we can prove the optimization algorithm converging to a max-margin classifier wrt the $\ell_2$ norm.

- I am not sure about relevance and use-cases of the mirror descent algorithm with very general potentials. As far as I know, typically, a small set of norm-based or entropic (neg-ent, tsallis, etc) are used within applications of ML. So while the theorem makes an interesting statement for an optimization standpoint, I'm not sure how relevant it is for the ML community. The theorem is also not entirely relevant to the pure optimization community since it's for the specific case of linear classification with finite data.
---
*While I could understand the paper, this is not my area of research. I do not find myself fit to evaluate the paper on soundness, relevance to the sub-field and importance of contributions.*

Limitations:
The limitations are discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wiK6bwuxjE;"REVIEW 
Summary:
This paper proposes a monocular 3D detection framework inspired by Masked Autoencoders (MAE), designed to address the challenge of object occlusions in 3D object detection. It utilizes a unique depth-aware masking module that simulates occlusions by adaptively masking non-occluded object features based on depth information, coupled with a lightweight completion network that reconstructs these masked features to learn occlusion-tolerant representations. It generates training pairs of non-occluded and occluded object representations directly, enhancing its capability to handle occlusions effectively. The framework is optimized for low computational overhead during inference, as it does not require object masking at this stage.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method outperforms the conventional methods across various datasets such as KITTI and Nuscenes. It demonstrates the effectiveness of the proposed method. Moreover, the proposed method achieves real-time inference time.
2. An extensive ablation study is proven to demonstrate the impact of the proposed module. 
3. The idea is simple yet effective.

Weaknesses:
1. The performance improvement is marginal, especially on the cross-validation in Table 6. 
2. Missing evaluation on the Waymo dataset

Limitations:
See the weakness and question parts.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces a novel framework for improving monocular 3D object detection, particularly in handling object occlusions. The proposed MonoMAE leverages depth-aware masking to simulate occlusions in the feature space and employs a lightweight completion network to reconstruct occluded object regions, thereby learning occlusion-tolerant representations. Experiments show that this learning stratgy helps to improve the performance of monocular 3D object detection.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper is well-structured, with a clear problem statement, methodology, experiments, and ablation studies that substantiate the contributions and effectiveness of MonoMAE.
2. This paper addresses a significant challenge in monocular 3D object detection, object occlusion, with a novel approach using depth-aware masked autoencoders.

Weaknesses:
1.  The reliance on depth-aware masking to simulate occlusions may not perfectly replicate natural occlusion patterns, potentially affecting the model's reconstruction accuracy. The gap between synthetically masked and naturally occluded object queries could limit the model's robustness in real-world scenarios.
2. While this paper claims generalizability, the lack of extensive cross-dataset validation leaves the true scope of its generalization capability somewhat unproven.

Limitations:
1. The paper could provide a more detailed analysis of the computational efficiency, including speed and resource usage, to fully assess the practicality of MonoMAE for real-time applications.
2. This paper only present results of vehicle detection. The performance of detecting objects with small sizes is unknown.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper applies Masked Autoencoder to 3D object detection. It distinguishes object queries into occluded and non-occluded categories, and during training, it applies depth-aware masking to the non-occluded queries and learns by completing them. At test time, the completion is applied to the occluded queries.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It achieved state-of-the-art performance on the KITTI 3D dataset.
- The idea of interpreting occluded queries as masked queries to solve the problem is interesting.
- The training and test times are illustrated clearly in figures.

Weaknesses:
- As stated in the limitations section, occlusion at the image level and masking at the feature level of object queries are not the same. Further analysis is needed to understand the actual implications of masking in object queries.
- If masking serves the role of occlusion at the image level, there should be no reason for the mask ratio to vary with depth, yet depth-aware masking is highly beneficial. An analysis is needed to understand why depth-aware masking works well compared to random masking.
- In my opinion, the performance of the Non-Occluded Query Grouping classification is crucial for the framework to function properly. Although classification accuracy is provided in the supplementary material, it would be helpful to include various metrics such as precision, recall, and F1-score. If the results of the Non-Occluded Query Grouping classification are biased, it might be interesting to apply completion not only to the occluded queries but also to the non-occluded queries at test time.

Limitations:
Limitations are included in the main text.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces MonoMAE, a novel monocular 3D object detection framework designed to improve detection performance in the presence of object occlusions. MonoMAE leverages the concept of Masked Autoencoders, treating object occlusions as natural masking and training the network to complete occluded regions. This innovative approach addresses the pervasive issue of object occlusions in monocular 3D detection, leading to superior detection performance. Extensive experiments on datasets like KITTI 3D and nuScenes show that MonoMAE outperforms state-of-the-art methods in both qualitative and quantitative measures.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The introduction of depth-aware masking to simulate occlusions and the use of a lightweight query completion network are innovative and address a significant challenge in monocular 3D detection.
2. MonoMAE improves detection performance without the need for additional training data or annotations, making it a practical solution for real-world applications like autonomous driving and robotics.
3. The framework demonstrates superior performance on benchmark datasets (KITTI 3D and nuScenes), outperforming existing state-of-the-art methods in both occluded and non-occluded scenarios.
4. MonoMAE shows strong generalization capabilities to new domains, which is critical for deploying models in diverse environments.

Weaknesses:
1. In many datasets and methods, objects are not merely labeled as ""occluded"" or ""non-occluded."" Instead, they may be assigned occlusion levels or degrees that quantify the extent to which an object is occluded. These levels provide more granularity and can influence how models are trained and evaluated. It would be beneficial to specify how occlusion levels are defined and used. Clarifying whether discrete or continuous levels are employed and how these influence the labeling, training, and evaluation processes will provide a clearer understanding of the methodology and its robustness in handling occlusions.
2. The paper does not provide explicit details about the accuracy of the occlusion classification network or how this accuracy influences the overall 3D object detection network. This information appears to be missing.
3. The paper does not explicitly report the performance or accuracy of the query completion network. Including a report on the performance of this network, such as quantitative results or visualization of the reconstructed queries, would be valuable. It would demonstrate whether the query completion network is learning meaningful features and contributing effectively to the overall 3D object detection performance.

Limitations:
The authors discussed some failure cases in their paper, as well as the gap between the generated occlusion pattern and natural occlusion patterns.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a monocular 3D detection framework inspired by Masked Autoencoders (MAE), designed to address the challenge of object occlusions in 3D object detection. It utilizes a unique depth-aware masking module that simulates occlusions by adaptively masking non-occluded object features based on depth information, coupled with a lightweight completion network that reconstructs these masked features to learn occlusion-tolerant representations. It generates training pairs of non-occluded and occluded object representations directly, enhancing its capability to handle occlusions effectively. The framework is optimized for low computational overhead during inference, as it does not require object masking at this stage.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method outperforms the conventional methods across various datasets such as KITTI and Nuscenes. It demonstrates the effectiveness of the proposed method. Moreover, the proposed method achieves real-time inference time.
2. An extensive ablation study is proven to demonstrate the impact of the proposed module. 
3. The idea is simple yet effective.

Weaknesses:
1. The performance improvement is marginal, especially on the cross-validation in Table 6. 
2. Missing evaluation on the Waymo dataset

Limitations:
See the weakness and question parts.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces a novel framework for improving monocular 3D object detection, particularly in handling object occlusions. The proposed MonoMAE leverages depth-aware masking to simulate occlusions in the feature space and employs a lightweight completion network to reconstruct occluded object regions, thereby learning occlusion-tolerant representations. Experiments show that this learning stratgy helps to improve the performance of monocular 3D object detection.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper is well-structured, with a clear problem statement, methodology, experiments, and ablation studies that substantiate the contributions and effectiveness of MonoMAE.
2. This paper addresses a significant challenge in monocular 3D object detection, object occlusion, with a novel approach using depth-aware masked autoencoders.

Weaknesses:
1.  The reliance on depth-aware masking to simulate occlusions may not perfectly replicate natural occlusion patterns, potentially affecting the model's reconstruction accuracy. The gap between synthetically masked and naturally occluded object queries could limit the model's robustness in real-world scenarios.
2. While this paper claims generalizability, the lack of extensive cross-dataset validation leaves the true scope of its generalization capability somewhat unproven.

Limitations:
1. The paper could provide a more detailed analysis of the computational efficiency, including speed and resource usage, to fully assess the practicality of MonoMAE for real-time applications.
2. This paper only present results of vehicle detection. The performance of detecting objects with small sizes is unknown.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper applies Masked Autoencoder to 3D object detection. It distinguishes object queries into occluded and non-occluded categories, and during training, it applies depth-aware masking to the non-occluded queries and learns by completing them. At test time, the completion is applied to the occluded queries.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It achieved state-of-the-art performance on the KITTI 3D dataset.
- The idea of interpreting occluded queries as masked queries to solve the problem is interesting.
- The training and test times are illustrated clearly in figures.

Weaknesses:
- As stated in the limitations section, occlusion at the image level and masking at the feature level of object queries are not the same. Further analysis is needed to understand the actual implications of masking in object queries.
- If masking serves the role of occlusion at the image level, there should be no reason for the mask ratio to vary with depth, yet depth-aware masking is highly beneficial. An analysis is needed to understand why depth-aware masking works well compared to random masking.
- In my opinion, the performance of the Non-Occluded Query Grouping classification is crucial for the framework to function properly. Although classification accuracy is provided in the supplementary material, it would be helpful to include various metrics such as precision, recall, and F1-score. If the results of the Non-Occluded Query Grouping classification are biased, it might be interesting to apply completion not only to the occluded queries but also to the non-occluded queries at test time.

Limitations:
Limitations are included in the main text.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces MonoMAE, a novel monocular 3D object detection framework designed to improve detection performance in the presence of object occlusions. MonoMAE leverages the concept of Masked Autoencoders, treating object occlusions as natural masking and training the network to complete occluded regions. This innovative approach addresses the pervasive issue of object occlusions in monocular 3D detection, leading to superior detection performance. Extensive experiments on datasets like KITTI 3D and nuScenes show that MonoMAE outperforms state-of-the-art methods in both qualitative and quantitative measures.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The introduction of depth-aware masking to simulate occlusions and the use of a lightweight query completion network are innovative and address a significant challenge in monocular 3D detection.
2. MonoMAE improves detection performance without the need for additional training data or annotations, making it a practical solution for real-world applications like autonomous driving and robotics.
3. The framework demonstrates superior performance on benchmark datasets (KITTI 3D and nuScenes), outperforming existing state-of-the-art methods in both occluded and non-occluded scenarios.
4. MonoMAE shows strong generalization capabilities to new domains, which is critical for deploying models in diverse environments.

Weaknesses:
1. In many datasets and methods, objects are not merely labeled as ""occluded"" or ""non-occluded."" Instead, they may be assigned occlusion levels or degrees that quantify the extent to which an object is occluded. These levels provide more granularity and can influence how models are trained and evaluated. It would be beneficial to specify how occlusion levels are defined and used. Clarifying whether discrete or continuous levels are employed and how these influence the labeling, training, and evaluation processes will provide a clearer understanding of the methodology and its robustness in handling occlusions.
2. The paper does not provide explicit details about the accuracy of the occlusion classification network or how this accuracy influences the overall 3D object detection network. This information appears to be missing.
3. The paper does not explicitly report the performance or accuracy of the query completion network. Including a report on the performance of this network, such as quantitative results or visualization of the reconstructed queries, would be valuable. It would demonstrate whether the query completion network is learning meaningful features and contributing effectively to the overall 3D object detection performance.

Limitations:
The authors discussed some failure cases in their paper, as well as the gap between the generated occlusion pattern and natural occlusion patterns.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wiEHZSV15I;"REVIEW 
Summary:
This paper proposes  a Selective Structured Components-based Neural Network for Long-term Time Series Forecasting

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper demonstrates originality by addressing a crucial limitation in existing SOTA methods, maintains high quality through thorough experimentation and clear presentation, offers significant advancements to the field of time series forecasting, and ensures clarity that aids in understanding and reproduction of the work.

2. The motivation of this paper is intuitive and compelling. Given the large model sizes of current SOTA methods like PatchTST, the idea of using a smaller model to achieve comparable or better performance is highly attractive. 

3. The experiments are thorough, and the proposed method achieves state-of-the-art performance. The effectiveness of each sub-module is demonstrated through detailed ablation studies. 

4. The code is open source and reproducible, with a straightforward and clear usage process.

Weaknesses:
1. In the experiment section, it is noted that most papers use the ETT dataset for ablation studies, likely due to its smaller size, which allows for quicker results. However, you chose the ECL and Traffic datasets instead of ETT, which is a more comprehensive and reliable approach. While this choice is commendable, there is no explanation provided for not using the ETT dataset. 

2.It would be more informative to report the model size directly in Table 1. Including the model size would provide a clearer comparison with other SOTA methods and highlight the efficiency of your proposed model. 

3.Baselines: Some MTSF models based on LLM have been widely applied [1]. If the authors can demonstrate that SSCNN has advantages in both performance and efficiency, this paper will be more convincing.

4.Some extremely lightweight models have also been proven to have satisfactory performance [2] . Compared to these methods, what are the main advantages of SSCNN? 

[1]One Fits All:Power General Time Series Analysis by Pretrained LM

[2]FITS: Modeling Time Series with 10k Parameters

Limitations:
No problem

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper identifies data decomposition as a core bottleneck in time series forecasting and proposes a novel model named SSCNN, a decomposition-based model innovatively enhanced with a selection mechanism. SSCNN is specifically designed to adeptly capture complex regularities in data while maintaining a minimal parameter scale. This paper also provides an in-depth comparison between decomposition and patching, examining both capability and parsimony.  Comprehensive experiments show the superior performance of SSCNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strong Points:

1. The insight of this paper is attractive and compelling. One of the most crucial characteristics of time series is that they can be viewed as composed of components with different natures, e.g., season, trend, and residual. However, this characteristic has been rarely utilized in related works, or it has been implemented in trivial ways. This paper identifies data decomposition as a core bottleneck in time series forecasting and proves its effectiveness. By decomposing complex data into more learnable components, SSCNN achieves state-of-the-art performance with a minimal number of parameters.

2. The writing of this paper is very clear. I can easily follow the author's logic and understand their points.

3. The experimental results are extensive, including overall performance results, ablation studies of each component, hyperparameter experiments, etc., which validate the effectiveness of SSCNN.

4. The code is reproducible and well documented. I have successfully replicated the authors' results.

5. The authors also provide an in-depth comparative analysis and experimental results between patching and decomposition, which help readers understand the advantages of SSCNN’s insights.

This paper emphasizes the importance of decomposition in long-term time series forecasting, addressing the analytical gap in feature decomposition for the first time and providing insights into its rationale for capability and parsimony compared to patching.

Weaknesses:
I have some minor questions and suggestions. If the author addresses the following points, I will increase my score.

Weak Points:

Experimental Setting: Most works using the Time-Series-Library repository predict up to 720 steps, yet your results do not include this prediction horizon. It would be beneficial to explain why 720-step predictions were not included.

Figures: I suggest the authors add more explanatory information to Figure 1 to help readers grasp the main architecture of SSCNN from the figure and its caption alone. Moreover, some font styles (italic) in Figure 1 seem different from the character styles in the main text. I recommend unifying the styles.

Minor Issues: The operator $\lfloor \cdot \rfloor$ is used in the paper but not explained. In Figure 3(a), if I understand correctly, “HDformer” should be replaced by “SSCNN.”

Figures: The text size of the legends in the figures is too small, making them difficult to read. Adjusting the text size to be consistent with the main text would enhance the readability of the figures and improve the overall presentation quality of the paper.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses long-term time series forecasting and critiques the reliance on complex models with extensive parameters. It proposes a decomposition method specifically designed for time series dynamics, achieving better forecasting performance across various datasets. Remarkably, the new model uses over 99% fewer parameters than other methods, highlighting the efficiency of domain-specific approaches. This research calls for a move away from complexity in LTSF, showcasing the effectiveness of focused decomposition techniques rather than relying on large-scale models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is praiseworthy for its intuitive approach. It tackles a significant problem by proposing a method that matches or surpasses current state-of-the-art models like PatchTST while using a smaller model footprint. The experimental results strongly validate this approach.

2.	The model consistently performs well under various experimental conditions, including different input window sizes and hyperparameter settings. Statistical tests demonstrate the reliability of the results across multiple initializations, strengthening the study's credibility.

3.	The authors provide a thorough comparison between decomposition and patching in terms of effectiveness and simplicity, demonstrating the superior benefits of decomposition over patching.

Weaknesses:
1.	The clarity of the methodology could be improved with further elaboration.
2.  The evaluation could be strengthened by including comparisons with LLM-based models, such as:

 [1] Jin, Ming, et al. ""Time-LLM: Time Series Forecasting by Reprogramming Large Language Models."" The Twelfth International Conference on Learning Representations.

[2] Bian, Yuxuan, et al. ""Multi-patch prediction: Adapting llms for time series representation learning."" arXiv preprint arXiv:2402.04852 (2024).

Limitations:
Whether this model can be applied to other types of time series data, e.g. trajectory.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This study unveils a groundbreaking approach to time series forecasting, notable for its minimal parameter count. It stands as the first model to consistently outperform state-of-the-art (SOTA) techniques while remaining compact. Unlike prevalent methods such as PatchTST and iTransformer, which are powerful but cumbersome, and emerging methods like TimeMixer and SCNN, which are lightweight yet inadequate for complex tasks, this model achieves superior performance without the associated heft.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The model consistently delivers superior accuracy compared to state-of-the-art (SOTA) methods while maintaining a minimal model size. This accomplishment distinguishes it from other methods.

2. The framework unifies the ability to capture various patterns in time series data, offering a streamlined and enhanced alternative to existing models built with MLPs or Transformers.

3. The authors conduct extensive experiments, showcasing the model's strong performance compared to selected SOTA models, which are sufficiently representative of the latest advancements in the field.

Weaknesses:
1. There is a gap between the introduction and Section 3 regarding the decomposition of the time series into four components. The authors do not explain why these four components are sufficient. For longer sequences, is there a need for more components? Are there references that support this approach? This discussion should be included at the beginning of Section 3.

2. Manually disabling the spatial component for certain datasets appears suboptimal. It would be more effective if the algorithm could automatically determine whether including the spatial component is beneficial for each dataset.

3. The paper's formatting needs improvements. It seems the authors may have additional content to include. Although the figures in the methodology section are clear and informative, resizing and rearranging them could provide more space for adding valuable content to the main text.

Limitations:
The authors have raised the limitation of the model concerning computational efficiency, along with potential solutions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Title: Parsimony or Capability? Decomposition delivers both in long term time series forecasting.

Long term time series forecasting has been an important research problem which applies to different problem domains. This paper proposes a decomposition method which shows significant performance on the benchmarks with less parameters. This method been evaluated extensively on the various datasets and been competitive to existing models. With such approach models can be enhanced to adapt domain characteristics more effectively in various time series applications.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. SSCNN reduces the parameter count substantially compared to traditional models, holding onto less than 1% of the parameters while still performing well across different datasets.
2. The model captures complex data patterns effectively using fewer parameters, utilizing a structured component-based approach with a selection mechanism to improve prediction accuracy.
3. SSCNN excels in time series forecasting, managing diverse data types and confirming its effectiveness through thorough experimentation.
4. SSCNN improves plain feature decomposition by incorporating a selection mechanism. This allows the model to identify fine-grained dependencies at each time step, which is essential for enhancing the accuracy of the decomposed structured components and, consequently, the overall prediction accuracy.
5. Extensive analysis has been performed to validate the method on existing benchmarks and compared with state-of-the-art methods.
6. Supplementary materials are satisfactory and provide explanation about the dataset and the implementation.

Weaknesses:
1. Figures lack captions.
2. Include some limitations of the model as well.
3. second contribution and third one looks quite similar.

Limitations:
Authors have adequately addressed the limitations related to computational efficiency and capability of model.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper approaches the problem of long term time series forecasting (LTSF) using a compositional technique to reduce the model size without compromising the quality of solution. The proposed technique is a transformer based architecture with a lower number of parameters, and delivers similar performance as state of the art models for LTSF.

The limitation of existing approaches, such as data patching, is that they fail to take into account the spatio-temporal dependencies, and end up with a blow up in the number of latent variables. This results only in a very small improvement even if the model size is increased substantially. The proposed technique in the paper is based on a inference step and an extrapolation step without any information loss.

The paper evaluates the proposed approach, called SSCNN, with seven datasets, which has a combination of regular and volatile patterns. The baseline and state of the art approaches compared against include iTransformer, TimeMixer, and PatchTST. SSCNN consistently achieves the best scores, with respect to MSE and MAE. The paper also conducts ablation studies to show that each new component in the architecture is vital to the performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The work studies an important and hard problem in time series forecasting which is the problem of efficient and accurate long term forecasting. Compositional techniques have been successful in other areas of AI including reinforcement learning, planning, and finite state controller synthesis. So, it makes sense to apply similar ideas in the space of long term time series forecasting.

Weaknesses:
While the high level message is presented well, I found the details of the proposed method and experiments are hard to follow. A running example with the explanation of the new layers will be useful.

The main contribution with respect to results is somewhat hard to grasp and align with the theoretical claims of the paper. Overall, I think there is room for improvement in the presentation of experimental results. I found some missing details in the experimental section that include:

1. Why is SSCNN missing Figure 3(a)?
2. What is the value of T_{out} in Figure 2?
3. What is the forward window size in Figure 3?

In figure 2, it would be useful to move some of the methods to the appendix, and keep only the critical ones  in the main body of the paper. Same is true for Figure 3. It is hard to go back and forth between figures 1, and figures2&3.

Minor:

1. I would suggest providing some more details about the experimental results in point 3 of the contributions (lines 80-82)
2. Figure 3 is hard to read in print.
3. Having only one legend for all the subplots (Figure 2(a)-(d) and Figure 3(a)-(d)) will better than repeating the legends in all subplots.

Limitations:
Yes, the paper has outlined the limitation of computational efficiency and provided some insight into how it can be improved in future work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes  a Selective Structured Components-based Neural Network for Long-term Time Series Forecasting

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper demonstrates originality by addressing a crucial limitation in existing SOTA methods, maintains high quality through thorough experimentation and clear presentation, offers significant advancements to the field of time series forecasting, and ensures clarity that aids in understanding and reproduction of the work.

2. The motivation of this paper is intuitive and compelling. Given the large model sizes of current SOTA methods like PatchTST, the idea of using a smaller model to achieve comparable or better performance is highly attractive. 

3. The experiments are thorough, and the proposed method achieves state-of-the-art performance. The effectiveness of each sub-module is demonstrated through detailed ablation studies. 

4. The code is open source and reproducible, with a straightforward and clear usage process.

Weaknesses:
1. In the experiment section, it is noted that most papers use the ETT dataset for ablation studies, likely due to its smaller size, which allows for quicker results. However, you chose the ECL and Traffic datasets instead of ETT, which is a more comprehensive and reliable approach. While this choice is commendable, there is no explanation provided for not using the ETT dataset. 

2.It would be more informative to report the model size directly in Table 1. Including the model size would provide a clearer comparison with other SOTA methods and highlight the efficiency of your proposed model. 

3.Baselines: Some MTSF models based on LLM have been widely applied [1]. If the authors can demonstrate that SSCNN has advantages in both performance and efficiency, this paper will be more convincing.

4.Some extremely lightweight models have also been proven to have satisfactory performance [2] . Compared to these methods, what are the main advantages of SSCNN? 

[1]One Fits All:Power General Time Series Analysis by Pretrained LM

[2]FITS: Modeling Time Series with 10k Parameters

Limitations:
No problem

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper identifies data decomposition as a core bottleneck in time series forecasting and proposes a novel model named SSCNN, a decomposition-based model innovatively enhanced with a selection mechanism. SSCNN is specifically designed to adeptly capture complex regularities in data while maintaining a minimal parameter scale. This paper also provides an in-depth comparison between decomposition and patching, examining both capability and parsimony.  Comprehensive experiments show the superior performance of SSCNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strong Points:

1. The insight of this paper is attractive and compelling. One of the most crucial characteristics of time series is that they can be viewed as composed of components with different natures, e.g., season, trend, and residual. However, this characteristic has been rarely utilized in related works, or it has been implemented in trivial ways. This paper identifies data decomposition as a core bottleneck in time series forecasting and proves its effectiveness. By decomposing complex data into more learnable components, SSCNN achieves state-of-the-art performance with a minimal number of parameters.

2. The writing of this paper is very clear. I can easily follow the author's logic and understand their points.

3. The experimental results are extensive, including overall performance results, ablation studies of each component, hyperparameter experiments, etc., which validate the effectiveness of SSCNN.

4. The code is reproducible and well documented. I have successfully replicated the authors' results.

5. The authors also provide an in-depth comparative analysis and experimental results between patching and decomposition, which help readers understand the advantages of SSCNN’s insights.

This paper emphasizes the importance of decomposition in long-term time series forecasting, addressing the analytical gap in feature decomposition for the first time and providing insights into its rationale for capability and parsimony compared to patching.

Weaknesses:
I have some minor questions and suggestions. If the author addresses the following points, I will increase my score.

Weak Points:

Experimental Setting: Most works using the Time-Series-Library repository predict up to 720 steps, yet your results do not include this prediction horizon. It would be beneficial to explain why 720-step predictions were not included.

Figures: I suggest the authors add more explanatory information to Figure 1 to help readers grasp the main architecture of SSCNN from the figure and its caption alone. Moreover, some font styles (italic) in Figure 1 seem different from the character styles in the main text. I recommend unifying the styles.

Minor Issues: The operator $\lfloor \cdot \rfloor$ is used in the paper but not explained. In Figure 3(a), if I understand correctly, “HDformer” should be replaced by “SSCNN.”

Figures: The text size of the legends in the figures is too small, making them difficult to read. Adjusting the text size to be consistent with the main text would enhance the readability of the figures and improve the overall presentation quality of the paper.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses long-term time series forecasting and critiques the reliance on complex models with extensive parameters. It proposes a decomposition method specifically designed for time series dynamics, achieving better forecasting performance across various datasets. Remarkably, the new model uses over 99% fewer parameters than other methods, highlighting the efficiency of domain-specific approaches. This research calls for a move away from complexity in LTSF, showcasing the effectiveness of focused decomposition techniques rather than relying on large-scale models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is praiseworthy for its intuitive approach. It tackles a significant problem by proposing a method that matches or surpasses current state-of-the-art models like PatchTST while using a smaller model footprint. The experimental results strongly validate this approach.

2.	The model consistently performs well under various experimental conditions, including different input window sizes and hyperparameter settings. Statistical tests demonstrate the reliability of the results across multiple initializations, strengthening the study's credibility.

3.	The authors provide a thorough comparison between decomposition and patching in terms of effectiveness and simplicity, demonstrating the superior benefits of decomposition over patching.

Weaknesses:
1.	The clarity of the methodology could be improved with further elaboration.
2.  The evaluation could be strengthened by including comparisons with LLM-based models, such as:

 [1] Jin, Ming, et al. ""Time-LLM: Time Series Forecasting by Reprogramming Large Language Models."" The Twelfth International Conference on Learning Representations.

[2] Bian, Yuxuan, et al. ""Multi-patch prediction: Adapting llms for time series representation learning."" arXiv preprint arXiv:2402.04852 (2024).

Limitations:
Whether this model can be applied to other types of time series data, e.g. trajectory.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This study unveils a groundbreaking approach to time series forecasting, notable for its minimal parameter count. It stands as the first model to consistently outperform state-of-the-art (SOTA) techniques while remaining compact. Unlike prevalent methods such as PatchTST and iTransformer, which are powerful but cumbersome, and emerging methods like TimeMixer and SCNN, which are lightweight yet inadequate for complex tasks, this model achieves superior performance without the associated heft.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The model consistently delivers superior accuracy compared to state-of-the-art (SOTA) methods while maintaining a minimal model size. This accomplishment distinguishes it from other methods.

2. The framework unifies the ability to capture various patterns in time series data, offering a streamlined and enhanced alternative to existing models built with MLPs or Transformers.

3. The authors conduct extensive experiments, showcasing the model's strong performance compared to selected SOTA models, which are sufficiently representative of the latest advancements in the field.

Weaknesses:
1. There is a gap between the introduction and Section 3 regarding the decomposition of the time series into four components. The authors do not explain why these four components are sufficient. For longer sequences, is there a need for more components? Are there references that support this approach? This discussion should be included at the beginning of Section 3.

2. Manually disabling the spatial component for certain datasets appears suboptimal. It would be more effective if the algorithm could automatically determine whether including the spatial component is beneficial for each dataset.

3. The paper's formatting needs improvements. It seems the authors may have additional content to include. Although the figures in the methodology section are clear and informative, resizing and rearranging them could provide more space for adding valuable content to the main text.

Limitations:
The authors have raised the limitation of the model concerning computational efficiency, along with potential solutions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Title: Parsimony or Capability? Decomposition delivers both in long term time series forecasting.

Long term time series forecasting has been an important research problem which applies to different problem domains. This paper proposes a decomposition method which shows significant performance on the benchmarks with less parameters. This method been evaluated extensively on the various datasets and been competitive to existing models. With such approach models can be enhanced to adapt domain characteristics more effectively in various time series applications.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. SSCNN reduces the parameter count substantially compared to traditional models, holding onto less than 1% of the parameters while still performing well across different datasets.
2. The model captures complex data patterns effectively using fewer parameters, utilizing a structured component-based approach with a selection mechanism to improve prediction accuracy.
3. SSCNN excels in time series forecasting, managing diverse data types and confirming its effectiveness through thorough experimentation.
4. SSCNN improves plain feature decomposition by incorporating a selection mechanism. This allows the model to identify fine-grained dependencies at each time step, which is essential for enhancing the accuracy of the decomposed structured components and, consequently, the overall prediction accuracy.
5. Extensive analysis has been performed to validate the method on existing benchmarks and compared with state-of-the-art methods.
6. Supplementary materials are satisfactory and provide explanation about the dataset and the implementation.

Weaknesses:
1. Figures lack captions.
2. Include some limitations of the model as well.
3. second contribution and third one looks quite similar.

Limitations:
Authors have adequately addressed the limitations related to computational efficiency and capability of model.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper approaches the problem of long term time series forecasting (LTSF) using a compositional technique to reduce the model size without compromising the quality of solution. The proposed technique is a transformer based architecture with a lower number of parameters, and delivers similar performance as state of the art models for LTSF.

The limitation of existing approaches, such as data patching, is that they fail to take into account the spatio-temporal dependencies, and end up with a blow up in the number of latent variables. This results only in a very small improvement even if the model size is increased substantially. The proposed technique in the paper is based on a inference step and an extrapolation step without any information loss.

The paper evaluates the proposed approach, called SSCNN, with seven datasets, which has a combination of regular and volatile patterns. The baseline and state of the art approaches compared against include iTransformer, TimeMixer, and PatchTST. SSCNN consistently achieves the best scores, with respect to MSE and MAE. The paper also conducts ablation studies to show that each new component in the architecture is vital to the performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The work studies an important and hard problem in time series forecasting which is the problem of efficient and accurate long term forecasting. Compositional techniques have been successful in other areas of AI including reinforcement learning, planning, and finite state controller synthesis. So, it makes sense to apply similar ideas in the space of long term time series forecasting.

Weaknesses:
While the high level message is presented well, I found the details of the proposed method and experiments are hard to follow. A running example with the explanation of the new layers will be useful.

The main contribution with respect to results is somewhat hard to grasp and align with the theoretical claims of the paper. Overall, I think there is room for improvement in the presentation of experimental results. I found some missing details in the experimental section that include:

1. Why is SSCNN missing Figure 3(a)?
2. What is the value of T_{out} in Figure 2?
3. What is the forward window size in Figure 3?

In figure 2, it would be useful to move some of the methods to the appendix, and keep only the critical ones  in the main body of the paper. Same is true for Figure 3. It is hard to go back and forth between figures 1, and figures2&3.

Minor:

1. I would suggest providing some more details about the experimental results in point 3 of the contributions (lines 80-82)
2. Figure 3 is hard to read in print.
3. Having only one legend for all the subplots (Figure 2(a)-(d) and Figure 3(a)-(d)) will better than repeating the legends in all subplots.

Limitations:
Yes, the paper has outlined the limitation of computational efficiency and provided some insight into how it can be improved in future work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wgpmDyJgsg;"REVIEW 
Summary:
The paper introduces a novel optimization-based method for sparse-view 3D reconstruction from unposed images. The method uses off-the-shelf pose estimator to get pose initialization, then it uses rendering loss and generative priors to optimize the pose and 3D reconstruction. In detail, the generative priors involve a multi-view SDS loss on generated novel views using Zero123. The method demonstrates satisfying results on the evaluation data, and the ablation study shows the effectiveness of each proposed technique.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Good performance. The reconstruction quality and pose estimation accuracy are satisfying.
- The paper is well-written and is easy to follow.
- The idea of rejecting images with large pose error is interesting.
- The technical part of the paper is solid.

Weaknesses:
- Missing baseline. For the reconstruction methods, the only baseline is LEAP, which is a feedforward method. In contrast, the proposed method is an optimization-based method, which introduces pose-processing to estimated poses. I would suggest adding baseline of SPARF [1] and using the same pose initialization. Moreover, why not comparing with UpFusion?
- Unknown inference speed. Will the joint optimization of pose and shape be slow? Could you provide a analysis of inference time?
- Related work. One related work is iFusion [2], which uses generative priors for pose estimation and is very relevant to the philosophy of the proposed method. Another related work is FORGE [3], which introduces pose optimization for sparse view reconstruction. Moreover, the authors should discuss the prior sparse-view reconstruction from unposed images works with more details, the authors should provide more comparison and contrast with prior work. The current discussion is too short (Line 90-92).
- Ablation study. The ablation study is performed with the Ray Diffusion pose initialization. How will it look like using Dust3r initialization? This is important as the ablation should be performed with the best base model.


[1] Truong, Prune, et al. ""Sparf: Neural radiance fields from sparse and noisy poses."" CVPR 2023.
[2] Wu, Chin-Hsuan et al. “iFusion: Inverting Diffusion for Pose-Free Reconstruction from Sparse Views.” ArXiv 2023.
[3] Jiang, Hanwen et al. “Few-View Object Reconstruction with Unknown Categories and Camera Poses.” 3DV 2024.

Limitations:
Please see weaknesses and questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a framework for joint 3D reconstruction and pose refinement. Specifically, given estimated camera poses from off-the-shelf models, the proposed method first leverages diffusion priors and rendering loss for 3D reconstruction. The 3D reconstruction is further used to refine the current pose parameters. The 3D reconstruction and pose refinement are conducted in an alternative way. An outlier identification and correction strategy is also introduced to make full use of the given image while mitigating the adverse effect of noisy camera estimations at the same time. Experimental comparison with several pose estimation baselines shows that the proposed method can refine inaccurate pose estimation effectively.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper tackles a practical problem in real-world scenarios, where ground truth camera poses are not always available.
2. The proposed method is shown to be effective when applying to different pose estimation baselines.
3. The proposed outlier removal and correction is effective from the ablation study results in Table 4.

Weaknesses:
1. The proposed method is compared with SPARF only in the setting of using pose from different pose estimation baselines. However, it would be more convincing to also present the results using the same setting of SPARF, which adds noise into the GT camera pose. This will be a direct comparison with SPARF’s original results reported in their paper.
2. The proposed method is compared with LEAP for 3D reconstruction results. However, the comparison is a bit unfair since LEAP does not require any initial camera poses. 
3. The description of how to effectively detect the outliers (line 212 - line 214) is not very clear. Similarly, the procedure of how to correct the outlier poses (line 223 - line 225) is not very clear either. How the MSE and LPIPS are computed and compared since there is no correspondence?

Limitations:
Limitations are addressed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for the joint reconstruction of camera poses and 3D objects given sparse input views. The core idea is to use a pose-conditioned diffusion model (Zero-123) as a prior, impose the SDS loss, and jointly optimize the poses and objects, similar to the approach in ID-pose. To improve the robustness and quality of the optimization, the authors made several modifications: (1) Using a 6 DoF pose-conditioned diffusion model instead of a 3 DoF model. (2) Adding strategies for outlier detection and correction. (Although somewhat empirical, it proves effective.)

This approach requires initial camera poses (from methods such as RelPose++, RayDiffusion, etc.) and is not capable of reconstructing poses from scratch (e.g., purely random camera poses). Experimental results demonstrate that, compared to SPARF and ID-pose, the proposed method achieves better pose estimation quality. Additionally, it provides better object reconstruction in terms of novel view synthesis quality compared to LEAP.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1) The approach is technically sound, and I believe the reported results are reproducible.

(2) The reconstructed results look good and represent the state-of-the-art in object-level pose-free reconstruction.

(3) The paper is well-written, making it easy to read and understand.

Weaknesses:
(1) This optimization-based method requires more time compared to a feed-forward model, taking about 5-10 minutes. Additionally, the writing discussing this aspect is somewhat unclear: the paper states, “with increased inference time depending on the number of outliers.” Could this statement be more specific? How much does the time increase with the number of outliers? The correction of outliers may be time-consuming as it requires dense searches of initial camera poses.

(2) (Minor) The method focuses only on object-level reconstruction, which makes the scope seem narrow.

(3) The authors do not sufficiently discuss experiments in a more “standard” sparse-view setting, such as using 3 or 4 views. The reported experiments use at least 6 views, which is not a particularly small number.

Limitations:
As discussed in the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method named MV-DreamGaussian for tackling the problem of 3D reconstruction from sparse multi-view inputs. In particular, the paper extends the DreamGaussian work to use multi-view images as the inputs and proposes a scheme to optimize the inaccurate camera poses of the multi-view images.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well written and I can follow smoothly.
- The authors proposed a finetuned version of Zero-1-to-3 with 6 DoF camera parametrization which shows an advantage over 3 DoF camera parameterization in the original paper.
- The proposed pose refinement scheme is novel and very effective according to the authors' experiments compared with SPARF as well as the ablation study which shows that adding the proposed pose refinement improves the pose accuracy and reconstruction quality significantly. The design of the outlier removal based on photometric error ranking and discrete search is empirical but works quite well.

Weaknesses:
- This paper presents very limited novelty in the reconstruction part with a trivial extension to DreamGaussian to use multi-view images, which is already implemented in a public repository [stable-dreamfusion](https://github.com/ashawkey/stable-dreamfusion).
- The major weakness of the paper is the lack of fair comparisons in terms of the 3D reconstruction. The authors only compared with LEAP for the 3D reconstruction. However, LEAP is a work that **does not require any pose inputs**, whereas the proposed work needs relatively good pose initialization (e.g., Dust3r) and conduct refinement on it. In addition, the underlying 3D representation is different, too: LEAP uses NeRF while the proposed work uses 3D Gaussian. I'm confused as to why the authors did not compare with SPARF for the reconstruction quality too since SPARF shares the same input setup as the proposed work. Besides, the very recent work DMV3D would also be a good method to compare with.

Limitations:
The authors have discussed the limitations of the paper and I generally agree with them.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel optimization-based method for sparse-view 3D reconstruction from unposed images. The method uses off-the-shelf pose estimator to get pose initialization, then it uses rendering loss and generative priors to optimize the pose and 3D reconstruction. In detail, the generative priors involve a multi-view SDS loss on generated novel views using Zero123. The method demonstrates satisfying results on the evaluation data, and the ablation study shows the effectiveness of each proposed technique.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Good performance. The reconstruction quality and pose estimation accuracy are satisfying.
- The paper is well-written and is easy to follow.
- The idea of rejecting images with large pose error is interesting.
- The technical part of the paper is solid.

Weaknesses:
- Missing baseline. For the reconstruction methods, the only baseline is LEAP, which is a feedforward method. In contrast, the proposed method is an optimization-based method, which introduces pose-processing to estimated poses. I would suggest adding baseline of SPARF [1] and using the same pose initialization. Moreover, why not comparing with UpFusion?
- Unknown inference speed. Will the joint optimization of pose and shape be slow? Could you provide a analysis of inference time?
- Related work. One related work is iFusion [2], which uses generative priors for pose estimation and is very relevant to the philosophy of the proposed method. Another related work is FORGE [3], which introduces pose optimization for sparse view reconstruction. Moreover, the authors should discuss the prior sparse-view reconstruction from unposed images works with more details, the authors should provide more comparison and contrast with prior work. The current discussion is too short (Line 90-92).
- Ablation study. The ablation study is performed with the Ray Diffusion pose initialization. How will it look like using Dust3r initialization? This is important as the ablation should be performed with the best base model.


[1] Truong, Prune, et al. ""Sparf: Neural radiance fields from sparse and noisy poses."" CVPR 2023.
[2] Wu, Chin-Hsuan et al. “iFusion: Inverting Diffusion for Pose-Free Reconstruction from Sparse Views.” ArXiv 2023.
[3] Jiang, Hanwen et al. “Few-View Object Reconstruction with Unknown Categories and Camera Poses.” 3DV 2024.

Limitations:
Please see weaknesses and questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a framework for joint 3D reconstruction and pose refinement. Specifically, given estimated camera poses from off-the-shelf models, the proposed method first leverages diffusion priors and rendering loss for 3D reconstruction. The 3D reconstruction is further used to refine the current pose parameters. The 3D reconstruction and pose refinement are conducted in an alternative way. An outlier identification and correction strategy is also introduced to make full use of the given image while mitigating the adverse effect of noisy camera estimations at the same time. Experimental comparison with several pose estimation baselines shows that the proposed method can refine inaccurate pose estimation effectively.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper tackles a practical problem in real-world scenarios, where ground truth camera poses are not always available.
2. The proposed method is shown to be effective when applying to different pose estimation baselines.
3. The proposed outlier removal and correction is effective from the ablation study results in Table 4.

Weaknesses:
1. The proposed method is compared with SPARF only in the setting of using pose from different pose estimation baselines. However, it would be more convincing to also present the results using the same setting of SPARF, which adds noise into the GT camera pose. This will be a direct comparison with SPARF’s original results reported in their paper.
2. The proposed method is compared with LEAP for 3D reconstruction results. However, the comparison is a bit unfair since LEAP does not require any initial camera poses. 
3. The description of how to effectively detect the outliers (line 212 - line 214) is not very clear. Similarly, the procedure of how to correct the outlier poses (line 223 - line 225) is not very clear either. How the MSE and LPIPS are computed and compared since there is no correspondence?

Limitations:
Limitations are addressed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for the joint reconstruction of camera poses and 3D objects given sparse input views. The core idea is to use a pose-conditioned diffusion model (Zero-123) as a prior, impose the SDS loss, and jointly optimize the poses and objects, similar to the approach in ID-pose. To improve the robustness and quality of the optimization, the authors made several modifications: (1) Using a 6 DoF pose-conditioned diffusion model instead of a 3 DoF model. (2) Adding strategies for outlier detection and correction. (Although somewhat empirical, it proves effective.)

This approach requires initial camera poses (from methods such as RelPose++, RayDiffusion, etc.) and is not capable of reconstructing poses from scratch (e.g., purely random camera poses). Experimental results demonstrate that, compared to SPARF and ID-pose, the proposed method achieves better pose estimation quality. Additionally, it provides better object reconstruction in terms of novel view synthesis quality compared to LEAP.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1) The approach is technically sound, and I believe the reported results are reproducible.

(2) The reconstructed results look good and represent the state-of-the-art in object-level pose-free reconstruction.

(3) The paper is well-written, making it easy to read and understand.

Weaknesses:
(1) This optimization-based method requires more time compared to a feed-forward model, taking about 5-10 minutes. Additionally, the writing discussing this aspect is somewhat unclear: the paper states, “with increased inference time depending on the number of outliers.” Could this statement be more specific? How much does the time increase with the number of outliers? The correction of outliers may be time-consuming as it requires dense searches of initial camera poses.

(2) (Minor) The method focuses only on object-level reconstruction, which makes the scope seem narrow.

(3) The authors do not sufficiently discuss experiments in a more “standard” sparse-view setting, such as using 3 or 4 views. The reported experiments use at least 6 views, which is not a particularly small number.

Limitations:
As discussed in the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method named MV-DreamGaussian for tackling the problem of 3D reconstruction from sparse multi-view inputs. In particular, the paper extends the DreamGaussian work to use multi-view images as the inputs and proposes a scheme to optimize the inaccurate camera poses of the multi-view images.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well written and I can follow smoothly.
- The authors proposed a finetuned version of Zero-1-to-3 with 6 DoF camera parametrization which shows an advantage over 3 DoF camera parameterization in the original paper.
- The proposed pose refinement scheme is novel and very effective according to the authors' experiments compared with SPARF as well as the ablation study which shows that adding the proposed pose refinement improves the pose accuracy and reconstruction quality significantly. The design of the outlier removal based on photometric error ranking and discrete search is empirical but works quite well.

Weaknesses:
- This paper presents very limited novelty in the reconstruction part with a trivial extension to DreamGaussian to use multi-view images, which is already implemented in a public repository [stable-dreamfusion](https://github.com/ashawkey/stable-dreamfusion).
- The major weakness of the paper is the lack of fair comparisons in terms of the 3D reconstruction. The authors only compared with LEAP for the 3D reconstruction. However, LEAP is a work that **does not require any pose inputs**, whereas the proposed work needs relatively good pose initialization (e.g., Dust3r) and conduct refinement on it. In addition, the underlying 3D representation is different, too: LEAP uses NeRF while the proposed work uses 3D Gaussian. I'm confused as to why the authors did not compare with SPARF for the reconstruction quality too since SPARF shares the same input setup as the proposed work. Besides, the very recent work DMV3D would also be a good method to compare with.

Limitations:
The authors have discussed the limitations of the paper and I generally agree with them.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wSpIdUXZYX;"REVIEW 
Summary:
This paper presents a new operator learning method for solving multiphysics PDEs. The attention scheme is designed on channel space to capture multiple physical variables, which is called co-domain. Moreover, positional encoding and normalization layers are considered. Such a strategy enables self-supervised pretraining of PDE systems. The experiments have shown the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed idea is interesting. It enables the generalization to coupled physical systems, which is of interest to the scientific machine learning (SciML) community. Also, self-supervised pretraining is one emerging tool in SciML and will gain a lot of attention in the future. 

- This paper provides the experiments on a Navier-Stokes equation and its coupled version with the elastic wave equation.

- This paper is well-organized and well-written. The details are easy to follow.

Weaknesses:
- This paper only considers one coupled system, i.e., NS and NS+EW. It may not validate the general applicability of the proposed method. The motivation of using this case should be enhanced. Also, considering some other PDE systems might strengthen the paper, such as the Rayleigh-Benard convection system. It is also a coupled system with NS + temperature.  

- The motivation for the combination of positional encoding, self-attention, and normalization layers seems to be better clarified. Although those parts are modular (claimed in Line 68), the connections between each other are also important. 

- In Appendix B.1, it would be good to include more details of self-supervised pretraining, such as masked ratio.  
The evaluation metrics might not be sufficient. This paper only considers L2 errors. There are many papers considering relative l2 error [1]. For the turbulence data, researchers also care about the infinity norm a lot. It would be better to add more evaluation metrics in this paper.

**References:** 

[1] Hao, Zhongkai, et al. ""Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training."" arXiv preprint arXiv:2403.03542 (2024).

[2] Ren, Pu, et al. ""Superbench: A super-resolution benchmark dataset for scientific machine learning."" arXiv preprint arXiv:2306.14070 (2023).

Limitations:
Please see my concerns in **Weaknesses** and **Questions**.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduced an innovative attention-based neural operator and evaluated it against various baselines. They employed masked pretraining and finetuning techniques, comparing the model's performance to multiple benchmarks. Their study included interesting problems such as fluid-structure interactions. The authors showed that their approach is effective for few-shot learning in their experimental evaluations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper presents a new neural operator architecture based on attention mechanisms. This architecture demonstrates superior performance compared to tested baseline models on the NS and NS+EW benchmarks, highlighting its potential advancements in solving PDE-related problems.
- To the best of my knowledge, the authors are the first ones to use masked training for PDE learning effectively
- The mathematical formulation of the proposed model is well-articulated in the paper. This clarity helps readers understand the underlying principles of the model's operation.
- The study addresses a compelling and very relevant multiphysics problem involving fluid-structure interactions.
- The authors demonstrated through empirical evidence that their approach is effective for few-shot finetuning in various scenarios.

Weaknesses:
- The study in Table 1 demonstrates the model’s performance on two specific PDEs: Navier-Stokes for fluid flow and a coupled Navier-Stokes with elastodynamics equations for fluid-solid interactions. While these cases provide some insight into the model's capabilities, they are not sufficient to generalize the model's applicability to a broader range of multiphysics problems.
- For the NS dataset with Reynolds number Re=400, the model trained from scratch with only 25 samples matches the performance of the pretrained model. In the case of NS+EW benchmark, when the Reynolds number increases to 4000, even with just 5 samples, both the finetuned and scratch-trained models exhibit similar testing errors. This suggests that pretraining may not provide significant advantages in many cases.
- The use of the L2 loss metric to evaluate model performance is problematic because it aggregates outputs of different physical meanings, such as pressure p, velocity u, and displacement d, into a single loss value. This can obscure individual variable contributions and lead to misleading conclusions about model accuracy.
- The absence of prediction visualizations diminishes the interpretability of the L2 loss values. Visualizing predictions could provide more intuitive insights into model performance and clarify discrepancies in the loss metric.
- The study in the Table 1 does not include a Fourier Neural Operator. Including such a benchmark is crucial to fairly evaluate CoDA-NO’s performance against an FNO model of similar size.
- The FNO model used for comparison in Table 2 has 1.9 billion parameters, vastly outnumbering the CoDA-NO's 11 million parameters. This overparameterization likely affects the model's performance due to the relatively small training set sizes and makes the comparison with CoDA-NO’s performance misleading. Smaller FNO models could provide a more realistic performance benchmark. The claim that CoDA-NO’s better performance compared to a much larger FNO model demonstrates parameter efficiency is misleading. Parameter efficiency should be evaluated with models of comparable sizes, and overparameterized models may not reflect typical scenarios.
- CoDA-NO has significantly higher inference times compared to other baseline models. 

These points collectively highlight the need for more comprehensive experiments, appropriate metrics, realistic model comparisons, and practical considerations like inference time to fully evaluate the model's capabilities.

Limitations:
The authors explained the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Codomain Attention Neural Operator, which tokenizes function along the channel dimension. It allows to learn representations of different PDE systems within a single model. The authors shows that finetuning a pretrained CoDA-NO on different physics yields good accuracy.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- I like the problem setting and the idea of the algorithm.
- The experimental task is quite interesting.
- The results are convincing, and the fact that the model can generalize to higher Reynold numbers seen during training is promising.
- I liked the used of GNO for handling non-uniform grids.
- The code seems solid.

Weaknesses:
- To me, the main weakness of the paper is that the presentation lacks of clarity. I don't see the point of doing 3 pages of mathematics in function space if, in practice, everything is done in discrete space. I think this blurs the message of the paper and it is difficult for the reader to understand what is the relevant information for understanding the actual CoDA-NO algorithm. In my opinion, these mathematics are not essential to the algorithm and could be put in appendix. I can always express a neural network architecture in function space, but since in practice we are working on discretized space, it is never done in experimental deep learning papers. Moreover, no discussion on how to go from infinite-dimensional space to discretized space is given by the authors.
This space could be used to have the actual detailed architecture. I may have missed the point on the usefulness of these sections and am willing to understand the point of view of the authors regarding this. 
- I don't fully understand the CoDA-NO algorithm and I think a Figure showing the whole architecture would have clarified this.

Limitations:
Overall, I think my main obstacle to provide a better score is the fact that the paper is not very clear due to the introduction of a lot of mathematics not needed to understand the algorithm in practice. These mathematics do not bring any theoretical insight of the algorithm. They are just expressing the architecture in function space. I think this space could have been used to be clearer to explain what is the architecture or the philosophy of the work. Usually, in papers where I see such mathematics, a study of the sample complexity is provided (I know it is almost impossible to do for neural networks).

I am willing to discuss these points with the authors and to modify my score accordingly.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose CoDA-NO, a neural operator architecture that captures interactions across different physical variables of coupled PDE systems. The method involves a generalization of the transformer architecture, including self-attention, positional encodings, and normalization, to function spaces. On two novel datasets for fluid-structure interaction and fluid dynamics, the authors show that their method achieves state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper investigates an interesting problem of how to appropriately capture interactions across different physical variables, that allows for generalization to new codomains.
- As far as I am aware, the generalization of the Transformer architecture to function spaces is novel.
- The experimental results, especially the generalization capabilities (from fluid dynamics to fluid-solid interactions) are impressive.
- Ablation studies on the proposed architectural changes are thorough.

Weaknesses:
Overall, the experiments seem quite compelling. However, it could be illuminating to provide a graphical visualization of the data from Table 1, regarding efficiency of fine-tuning and robustness to out-of-distribution inputs: see questions.

Limitations:
The authors address the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a new operator learning method for solving multiphysics PDEs. The attention scheme is designed on channel space to capture multiple physical variables, which is called co-domain. Moreover, positional encoding and normalization layers are considered. Such a strategy enables self-supervised pretraining of PDE systems. The experiments have shown the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed idea is interesting. It enables the generalization to coupled physical systems, which is of interest to the scientific machine learning (SciML) community. Also, self-supervised pretraining is one emerging tool in SciML and will gain a lot of attention in the future. 

- This paper provides the experiments on a Navier-Stokes equation and its coupled version with the elastic wave equation.

- This paper is well-organized and well-written. The details are easy to follow.

Weaknesses:
- This paper only considers one coupled system, i.e., NS and NS+EW. It may not validate the general applicability of the proposed method. The motivation of using this case should be enhanced. Also, considering some other PDE systems might strengthen the paper, such as the Rayleigh-Benard convection system. It is also a coupled system with NS + temperature.  

- The motivation for the combination of positional encoding, self-attention, and normalization layers seems to be better clarified. Although those parts are modular (claimed in Line 68), the connections between each other are also important. 

- In Appendix B.1, it would be good to include more details of self-supervised pretraining, such as masked ratio.  
The evaluation metrics might not be sufficient. This paper only considers L2 errors. There are many papers considering relative l2 error [1]. For the turbulence data, researchers also care about the infinity norm a lot. It would be better to add more evaluation metrics in this paper.

**References:** 

[1] Hao, Zhongkai, et al. ""Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training."" arXiv preprint arXiv:2403.03542 (2024).

[2] Ren, Pu, et al. ""Superbench: A super-resolution benchmark dataset for scientific machine learning."" arXiv preprint arXiv:2306.14070 (2023).

Limitations:
Please see my concerns in **Weaknesses** and **Questions**.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduced an innovative attention-based neural operator and evaluated it against various baselines. They employed masked pretraining and finetuning techniques, comparing the model's performance to multiple benchmarks. Their study included interesting problems such as fluid-structure interactions. The authors showed that their approach is effective for few-shot learning in their experimental evaluations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper presents a new neural operator architecture based on attention mechanisms. This architecture demonstrates superior performance compared to tested baseline models on the NS and NS+EW benchmarks, highlighting its potential advancements in solving PDE-related problems.
- To the best of my knowledge, the authors are the first ones to use masked training for PDE learning effectively
- The mathematical formulation of the proposed model is well-articulated in the paper. This clarity helps readers understand the underlying principles of the model's operation.
- The study addresses a compelling and very relevant multiphysics problem involving fluid-structure interactions.
- The authors demonstrated through empirical evidence that their approach is effective for few-shot finetuning in various scenarios.

Weaknesses:
- The study in Table 1 demonstrates the model’s performance on two specific PDEs: Navier-Stokes for fluid flow and a coupled Navier-Stokes with elastodynamics equations for fluid-solid interactions. While these cases provide some insight into the model's capabilities, they are not sufficient to generalize the model's applicability to a broader range of multiphysics problems.
- For the NS dataset with Reynolds number Re=400, the model trained from scratch with only 25 samples matches the performance of the pretrained model. In the case of NS+EW benchmark, when the Reynolds number increases to 4000, even with just 5 samples, both the finetuned and scratch-trained models exhibit similar testing errors. This suggests that pretraining may not provide significant advantages in many cases.
- The use of the L2 loss metric to evaluate model performance is problematic because it aggregates outputs of different physical meanings, such as pressure p, velocity u, and displacement d, into a single loss value. This can obscure individual variable contributions and lead to misleading conclusions about model accuracy.
- The absence of prediction visualizations diminishes the interpretability of the L2 loss values. Visualizing predictions could provide more intuitive insights into model performance and clarify discrepancies in the loss metric.
- The study in the Table 1 does not include a Fourier Neural Operator. Including such a benchmark is crucial to fairly evaluate CoDA-NO’s performance against an FNO model of similar size.
- The FNO model used for comparison in Table 2 has 1.9 billion parameters, vastly outnumbering the CoDA-NO's 11 million parameters. This overparameterization likely affects the model's performance due to the relatively small training set sizes and makes the comparison with CoDA-NO’s performance misleading. Smaller FNO models could provide a more realistic performance benchmark. The claim that CoDA-NO’s better performance compared to a much larger FNO model demonstrates parameter efficiency is misleading. Parameter efficiency should be evaluated with models of comparable sizes, and overparameterized models may not reflect typical scenarios.
- CoDA-NO has significantly higher inference times compared to other baseline models. 

These points collectively highlight the need for more comprehensive experiments, appropriate metrics, realistic model comparisons, and practical considerations like inference time to fully evaluate the model's capabilities.

Limitations:
The authors explained the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Codomain Attention Neural Operator, which tokenizes function along the channel dimension. It allows to learn representations of different PDE systems within a single model. The authors shows that finetuning a pretrained CoDA-NO on different physics yields good accuracy.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- I like the problem setting and the idea of the algorithm.
- The experimental task is quite interesting.
- The results are convincing, and the fact that the model can generalize to higher Reynold numbers seen during training is promising.
- I liked the used of GNO for handling non-uniform grids.
- The code seems solid.

Weaknesses:
- To me, the main weakness of the paper is that the presentation lacks of clarity. I don't see the point of doing 3 pages of mathematics in function space if, in practice, everything is done in discrete space. I think this blurs the message of the paper and it is difficult for the reader to understand what is the relevant information for understanding the actual CoDA-NO algorithm. In my opinion, these mathematics are not essential to the algorithm and could be put in appendix. I can always express a neural network architecture in function space, but since in practice we are working on discretized space, it is never done in experimental deep learning papers. Moreover, no discussion on how to go from infinite-dimensional space to discretized space is given by the authors.
This space could be used to have the actual detailed architecture. I may have missed the point on the usefulness of these sections and am willing to understand the point of view of the authors regarding this. 
- I don't fully understand the CoDA-NO algorithm and I think a Figure showing the whole architecture would have clarified this.

Limitations:
Overall, I think my main obstacle to provide a better score is the fact that the paper is not very clear due to the introduction of a lot of mathematics not needed to understand the algorithm in practice. These mathematics do not bring any theoretical insight of the algorithm. They are just expressing the architecture in function space. I think this space could have been used to be clearer to explain what is the architecture or the philosophy of the work. Usually, in papers where I see such mathematics, a study of the sample complexity is provided (I know it is almost impossible to do for neural networks).

I am willing to discuss these points with the authors and to modify my score accordingly.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose CoDA-NO, a neural operator architecture that captures interactions across different physical variables of coupled PDE systems. The method involves a generalization of the transformer architecture, including self-attention, positional encodings, and normalization, to function spaces. On two novel datasets for fluid-structure interaction and fluid dynamics, the authors show that their method achieves state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper investigates an interesting problem of how to appropriately capture interactions across different physical variables, that allows for generalization to new codomains.
- As far as I am aware, the generalization of the Transformer architecture to function spaces is novel.
- The experimental results, especially the generalization capabilities (from fluid dynamics to fluid-solid interactions) are impressive.
- Ablation studies on the proposed architectural changes are thorough.

Weaknesses:
Overall, the experiments seem quite compelling. However, it could be illuminating to provide a graphical visualization of the data from Table 1, regarding efficiency of fine-tuning and robustness to out-of-distribution inputs: see questions.

Limitations:
The authors address the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
u7JRmrGutT;"REVIEW 
Summary:
This paper studies the approximation of non-uniform Graph Edit Distance (GED) using graph neural networks. It considers four types of edit operations with different costs: edge deletion, edge addition, node deletion, and node addition. The paper proposes explicitly accounting for the costs assigned to each type of edit operation to adapt to both uniform and non-uniform cost settings. Moreover, it introduces node-pair embeddings for connected and disconnected node pairs within graphs as edge and non-edge embeddings. It computes the node and node-pair alignments through three types of distance functions to approximate the considered operations. Experiments on real-world datasets under a variety of edit cost settings show that the proposed method outperforms selected baselines on chosen metrics.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* This paper studies an important task with many real-world applications.
* The authors propose generating learnable embeddings for disconnected node pairs, which is new to me.
* Extensive experiments have been conducted.

Weaknesses:
* The clarity of the paper should be improved. For example:
  * What are the differences between the pad indicator $q$ and  $η$? Are they inconsistent notation?
  * The paper is not self-contained. The related work should be in the main paper rather than in the appendix. Additionally, some related work, such as [1], which clearly highly influences this paper, is missing.
* The technical contribution of this paper is somewhat limited, as it directly uses MPNNs for representing nodes and the Gumbel-Sinkhorn network to establish alignment.
* The size of the graphs used in the experiments is relatively small (with at most 20 nodes) compared with the graphs used in other neural approaches.
* Restricting the model parameters for the baselines does not seem fair to me, since the proposed model heavily relies on the iterative non-parameterized Gumbel-Sinkhorn refinement.
* The size of the graphs used in the experiments is relatively small, so it is unclear if the proposed model can scale to graphs with, say, one or two hundred nodes. Moreover, since obtaining training data for GED is nontrivial, it is also unclear how the proposed model performs when the supervision signal is not the optimal GED value. I suggest that the authors conduct experiments on commonly used GED datasets with larger graphs, such as the IMDB dataset provided in [2], or use large synthetic graphs following [3].


Minors:

* There are some typos (Line 278 Additiional -> Additional). Some sentences are missing periods, for example, in Lines 273, 551.
* The caption of tables should be above the tables.

Reference

[1] Piao C, Xu T, Sun X, et al. Computing Graph Edit Distance via Neural Graph Matching[J]. Proceedings of the VLDB Endowment, 2023, 16(8): 1817-1829.

[2] Yunsheng Bai, Haoyang Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. 2019. SimGNN: A Neural Network Approach to Fast Graph Similarity Computation. Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (2019).

[3] Roy, Indradyumna et al. “Maximum Common Subgraph Guided Graph Retrieval: Late and Early Interaction Networks.  NeurIPS 2022.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes GRAPHEDX, an innovative neural model that deftly handles Graph Edit Distance (GED) calculations with customizable edit costs. By representing graphs as rich sets of node and edge embeddings and using a Gumbel-Sinkhorn permutation generator, GRAPHEDX captures the subtle nuances of graph structure that influence edit distances. Experiments across diverse datasets demonstrate GRAPHEDX's superiority, with the model consistently outperforming state-of-the-art baselines by significant margins, especially in scenarios with unequal edit costs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The handling of GED metric with unequal cost is a timely and under-explored topic. In real world, many use cases involving GED need to consider such unequal cost scenario.
2. Rigorous evaluation on both the accuracy and efficiency (runnign time) is performed, showing that the proposed method achieves high accuracy while maintaining efficiency.
3. The current model may not fully address richly attributed graphs with complex node and edge features.
To show the transfer learning ability, I encourage the authors to try training on one dataset and testing on another. This would reduce concern on overfitting to one particular dataset
The paper is well-written.

Weaknesses:
1. The current model may not fully address richly attributed graphs with complex node and edge features.
2. To show the transfer learning ability, I encourage the authors to try training on one dataset and testing on another. This would reduce concern on overfitting to one particular dataset

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes GRAPHEDX, a neural model that learns to estimate the graph edit distace among a pair of graphs, not only in the case of equal and symmetric costs for edit operations, but in the case of unequal costs specified for the four edit operations. The core of the proposal represents each graph as a set of node & edge embeddings, designs neural set divergence surrogates based on those embeddings, and replaces the QAP term corresponding to each operation with its surrogate. The method learns alignments to compute surrogates via a Gumbel-Sinkhorn permutation generator while also ensuring consistency between the node and edge alignments and rendering them sensitive to the presence and absence of edges between node-pairs.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
S1. Addresses a gap in the field that previous works have left open.
S2. Outpeforms previous effort in both equal-cost and unequal-cost settings.

Weaknesses:
W1. The cost of training and estimating is not shown.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes GRAPHEDX, a neural model that learns to estimate the graph edit distace among a pair of graphs, not only in the case of equal and symmetric costs for edit operations, but in the case of unequal costs specified for the four edit operations. The core of the proposal represents each graph as a set of node & edge embeddings, designs neural set divergence surrogates based on those embeddings, and replaces the QAP term corresponding to each operation with its surrogate. The method learns alignments to compute surrogates via a Gumbel-Sinkhorn permutation generator while also ensuring consistency between the node and edge alignments and rendering them sensitive to the presence and absence of edges between node-pairs.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
S1. Addresses a gap in the field that previous works have left open.
S2. Outpeforms previous effort in both equal-cost and unequal-cost settings.

Weaknesses:
W1. The cost of training and estimating is not shown.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the approximation of non-uniform Graph Edit Distance (GED) using graph neural networks. It considers four types of edit operations with different costs: edge deletion, edge addition, node deletion, and node addition. The paper proposes explicitly accounting for the costs assigned to each type of edit operation to adapt to both uniform and non-uniform cost settings. Moreover, it introduces node-pair embeddings for connected and disconnected node pairs within graphs as edge and non-edge embeddings. It computes the node and node-pair alignments through three types of distance functions to approximate the considered operations. Experiments on real-world datasets under a variety of edit cost settings show that the proposed method outperforms selected baselines on chosen metrics.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* This paper studies an important task with many real-world applications.
* The authors propose generating learnable embeddings for disconnected node pairs, which is new to me.
* Extensive experiments have been conducted.

Weaknesses:
* The clarity of the paper should be improved. For example:
  * What are the differences between the pad indicator $q$ and  $η$? Are they inconsistent notation?
  * The paper is not self-contained. The related work should be in the main paper rather than in the appendix. Additionally, some related work, such as [1], which clearly highly influences this paper, is missing.
* The technical contribution of this paper is somewhat limited, as it directly uses MPNNs for representing nodes and the Gumbel-Sinkhorn network to establish alignment.
* The size of the graphs used in the experiments is relatively small (with at most 20 nodes) compared with the graphs used in other neural approaches.
* Restricting the model parameters for the baselines does not seem fair to me, since the proposed model heavily relies on the iterative non-parameterized Gumbel-Sinkhorn refinement.
* The size of the graphs used in the experiments is relatively small, so it is unclear if the proposed model can scale to graphs with, say, one or two hundred nodes. Moreover, since obtaining training data for GED is nontrivial, it is also unclear how the proposed model performs when the supervision signal is not the optimal GED value. I suggest that the authors conduct experiments on commonly used GED datasets with larger graphs, such as the IMDB dataset provided in [2], or use large synthetic graphs following [3].


Minors:

* There are some typos (Line 278 Additiional -> Additional). Some sentences are missing periods, for example, in Lines 273, 551.
* The caption of tables should be above the tables.

Reference

[1] Piao C, Xu T, Sun X, et al. Computing Graph Edit Distance via Neural Graph Matching[J]. Proceedings of the VLDB Endowment, 2023, 16(8): 1817-1829.

[2] Yunsheng Bai, Haoyang Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. 2019. SimGNN: A Neural Network Approach to Fast Graph Similarity Computation. Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (2019).

[3] Roy, Indradyumna et al. “Maximum Common Subgraph Guided Graph Retrieval: Late and Early Interaction Networks.  NeurIPS 2022.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes GRAPHEDX, an innovative neural model that deftly handles Graph Edit Distance (GED) calculations with customizable edit costs. By representing graphs as rich sets of node and edge embeddings and using a Gumbel-Sinkhorn permutation generator, GRAPHEDX captures the subtle nuances of graph structure that influence edit distances. Experiments across diverse datasets demonstrate GRAPHEDX's superiority, with the model consistently outperforming state-of-the-art baselines by significant margins, especially in scenarios with unequal edit costs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The handling of GED metric with unequal cost is a timely and under-explored topic. In real world, many use cases involving GED need to consider such unequal cost scenario.
2. Rigorous evaluation on both the accuracy and efficiency (runnign time) is performed, showing that the proposed method achieves high accuracy while maintaining efficiency.
3. The current model may not fully address richly attributed graphs with complex node and edge features.
To show the transfer learning ability, I encourage the authors to try training on one dataset and testing on another. This would reduce concern on overfitting to one particular dataset
The paper is well-written.

Weaknesses:
1. The current model may not fully address richly attributed graphs with complex node and edge features.
2. To show the transfer learning ability, I encourage the authors to try training on one dataset and testing on another. This would reduce concern on overfitting to one particular dataset

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
weemASPtzg;"REVIEW 
Summary:
This paper studies identifiability under unknown muilti-node  interventions (soft/hard), with general models (parametrtic/nonparametric) and **linear** mixing functions. This work provides both detailed proof which justifies the main theoretical statement, and a step-by-step algorithm which guides how to achieve identifiability in practice.
Overall, I find this work serves as an important step for interventional CRL towards more realistic settings.

&nbsp;

### References

[1] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning with interventions. arXiv:2301.08230, 2023.

[2] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning: Linear and general transformations. arXiv:2402.00849, 2024.

[3] Julius von Kügelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Kekic ́, Elias Bareinboim, David M Blei, and Bernhard Schölkopf. Nonparametric identifiability of causal rep- resentations from unknown interventions. In Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2023.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is extremely well written and clearly structured: it communicates clearly motivations, formulation, technical details, and theoretical implications. The experimental results adequately validate the theory in case of a linear causal model.

Weaknesses:
1. The proposed UMNI-CRL algorithm is claimed to work with *general* non-parametric causal models; however, the simulation experiment only showed results on *linear* structural equation model.  It would be great if the authors could report further experimental results on non-parametric causal models, to align with the theoretical claims. If there is a valid reason why it cannot be done, I am also very happy to hear.

2. Following the previous point, since this approach requires density estimation, it might not be scalable on nonparametric models. But to be fair, this seems to be a common limitation in many interventional CRL works [1, 2, 3].

3. Linearity assumption on the mixing function is restrictive, but the authors have acknowledged it and discussed possible future directions to overcome this limitation (sec. 6).

Limitations:
The authors discussed the remaining open problems and limitations in Section 6.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances Causal Representation Learning (CRL) by addressing the challenge of using unknown multi-node (UMN) interventions to identify latent causal variables and their structures.  The authors develop a score-based CRL algorithm that leverages UMN interventions to guarantee identifiability of latent variables and their causal graphs under both hard and soft interventions, achieving perfect identifiability with hard interventions and identifiability up to ancestors with soft interventions. Their method outperforms existing single-node approaches by ensuring robust recovery of causal structures in more complex, multi-intervention environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* Extending the causal representation learning to unknown multi-node interventions

* Proofs are provided 

* Pseudocode is provided

* Computational complexity is discussed

* Limitations are clearly stated

Weaknesses:
* The paper primarily focuses on causal models with linear transformations. This limits its applicability in many real scenarios

* The applicability of the assumptions in real scenarios was not discussed

* The method was not applied on real world-data

Limitations:
The paper acknowledges certain limitation. One notable limitation is the assumption of linear transformations in the causal models considered. This restricts the applicability to scenarios where causal relationships are adequately approximated by linear relationships. Additionally, while the paper addresses the challenge of UMN interventions, it acknowledges the complexity involved in identifying intervention targets in such settings, which can affect the ability to fully leverage the statistical diversity inherent in UMN interventions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies interventional causal representation learning, where one has access to interventional data, to identify latent causal factors and latent DAG in the unknown multi-node interventions regime. The authors consider a setting where the mixing function is linear and the latent causal model is nonparametric. Under the assumption of sufficient interventional diversity, the authors use score function arguments to show that the underlying causal factors of variation (and DAG) can be recovered (1) up to permutation and scaling from stochastic hard interventions and (2) up to ancestors from soft interventions. The authors propose a score-based framework (UMNI-CRL) and evaluate it on synthetic data generated from Erdős–Rényi random graph model.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- This work provides significant results in the unknown multi-node intervention setting, which is much more realistic than the common single-node intervention regime. As opposed to other works, this work studies CRL from a more general class of multi-node interventions (stochastic hard and soft).
- The paper is well-written, the concepts are explained well, and the theoretical identifiability results add a lot of value to the current CRL literature.
- The use of score functions and score differences in the observation space to estimate the unmixing function, especially for the UMN setting, is a novel and interesting approach for CRL.
- This work is the first to establish latent DAG recovery in the UMN setting under any type of multi-node intervention for arbitrary nonparametric latent causal models.

Weaknesses:
Although the theoretical contribution of this work is strong, the empirical evaluation is quite weak compared to other works in CRL. There are only experiments for n=4 causal variables. There is also no baseline comparison of the proposed framework with other methods in the UMN setting (e.g., [1]). Also, some discussions are a bit abridged and could use more elaboration in the paper (see below for details).

[1] Bing et al. “Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions” CLeaR 2024.

Limitations:
Limitations are discussed in Section 6.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper extends previous results on using score function for causal representation learning to the settings with unknown multi-node interventions. This new setting poses significant new challenges as opposed to the single node intervention case. The author first present theoretical identifiability result on hard interventions with latent additive noise model and on soft interventions. They then propose an algorithm called (UMNI)-CRL and test it on synthetic linear Gaussian dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written, easy to follow and with good motivations.

Weaknesses:
1. The transformation from latent to observed is noiseless, which could be a limitation. 
2. Line 199 says that: “This regularity condition ensures that the effect of a multi-node intervention is not the same on different nodes”. But how realistic or neccessary is this condition? It seems like it is very possible that an intervention can cause two downstream nodes to have the same effect although these two nodes is not influenced the same by all type of interventions. 
3. The experiments are only on synthetic dataset but I don’t think that is a big issue. 
4. Some potential missing citations
    
    [1] Kumar, Abhinav, and Gaurav Sinha. ""Disentangling mixtures of unknown causal interventions."" *Uncertainty in Artificial Intelligence*. PMLR, 2021.
    
    [2] Jiang, Yibo, and Bryon Aragam. ""Learning nonparametric latent causal graphs with unknown interventions."" *Advances in Neural Information Processing Systems* 36 (2024).

Limitations:
1. Theorem 1 only works for additive noise model.
2. The transformation from latent to observed noiseless.
3. Experiments are only on synthetic dataset. 
4. I am unsure if the algorithm is practical because it needs to estimate the score function.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces new identifiability results for CRL in environments with unknown multi-node interventions. It shows that, with sufficiently diverse interventional environments, one can achieve identifiability up to ancestors using soft interventions and perfect identifiability using hard interventions. The paper also provides an algorithm with identifiability guarantees.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper tackles the complex and underexplored multi-node intervention setting. The established identifiability can be crucial for extending current CRL theories into more practical contexts.
- The introduced algorithm that leverages score functions with different interventional environments is also interesting and insightful.
- The paper is well-motivated and articulated with high clarity.

Weaknesses:
- The proposed algorithm, while theoretically sound, seems computationally demanding. In fact, even a 4-node low-dimensional case requires a large number of environments and samples. The paper could benefit from a deeper discussion on the scalability of the algorithm.
- The current evaluation of the algorithm is limited to synthetic simulations. Expanding it to more realistic datasets would substantively improve its practical significance.

Limitations:
The paper acknowledges its main limitations in the reliance on linear transformations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies identifiability under unknown muilti-node  interventions (soft/hard), with general models (parametrtic/nonparametric) and **linear** mixing functions. This work provides both detailed proof which justifies the main theoretical statement, and a step-by-step algorithm which guides how to achieve identifiability in practice.
Overall, I find this work serves as an important step for interventional CRL towards more realistic settings.

&nbsp;

### References

[1] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning with interventions. arXiv:2301.08230, 2023.

[2] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning: Linear and general transformations. arXiv:2402.00849, 2024.

[3] Julius von Kügelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Kekic ́, Elias Bareinboim, David M Blei, and Bernhard Schölkopf. Nonparametric identifiability of causal rep- resentations from unknown interventions. In Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2023.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is extremely well written and clearly structured: it communicates clearly motivations, formulation, technical details, and theoretical implications. The experimental results adequately validate the theory in case of a linear causal model.

Weaknesses:
1. The proposed UMNI-CRL algorithm is claimed to work with *general* non-parametric causal models; however, the simulation experiment only showed results on *linear* structural equation model.  It would be great if the authors could report further experimental results on non-parametric causal models, to align with the theoretical claims. If there is a valid reason why it cannot be done, I am also very happy to hear.

2. Following the previous point, since this approach requires density estimation, it might not be scalable on nonparametric models. But to be fair, this seems to be a common limitation in many interventional CRL works [1, 2, 3].

3. Linearity assumption on the mixing function is restrictive, but the authors have acknowledged it and discussed possible future directions to overcome this limitation (sec. 6).

Limitations:
The authors discussed the remaining open problems and limitations in Section 6.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances Causal Representation Learning (CRL) by addressing the challenge of using unknown multi-node (UMN) interventions to identify latent causal variables and their structures.  The authors develop a score-based CRL algorithm that leverages UMN interventions to guarantee identifiability of latent variables and their causal graphs under both hard and soft interventions, achieving perfect identifiability with hard interventions and identifiability up to ancestors with soft interventions. Their method outperforms existing single-node approaches by ensuring robust recovery of causal structures in more complex, multi-intervention environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* Extending the causal representation learning to unknown multi-node interventions

* Proofs are provided 

* Pseudocode is provided

* Computational complexity is discussed

* Limitations are clearly stated

Weaknesses:
* The paper primarily focuses on causal models with linear transformations. This limits its applicability in many real scenarios

* The applicability of the assumptions in real scenarios was not discussed

* The method was not applied on real world-data

Limitations:
The paper acknowledges certain limitation. One notable limitation is the assumption of linear transformations in the causal models considered. This restricts the applicability to scenarios where causal relationships are adequately approximated by linear relationships. Additionally, while the paper addresses the challenge of UMN interventions, it acknowledges the complexity involved in identifying intervention targets in such settings, which can affect the ability to fully leverage the statistical diversity inherent in UMN interventions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies interventional causal representation learning, where one has access to interventional data, to identify latent causal factors and latent DAG in the unknown multi-node interventions regime. The authors consider a setting where the mixing function is linear and the latent causal model is nonparametric. Under the assumption of sufficient interventional diversity, the authors use score function arguments to show that the underlying causal factors of variation (and DAG) can be recovered (1) up to permutation and scaling from stochastic hard interventions and (2) up to ancestors from soft interventions. The authors propose a score-based framework (UMNI-CRL) and evaluate it on synthetic data generated from Erdős–Rényi random graph model.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- This work provides significant results in the unknown multi-node intervention setting, which is much more realistic than the common single-node intervention regime. As opposed to other works, this work studies CRL from a more general class of multi-node interventions (stochastic hard and soft).
- The paper is well-written, the concepts are explained well, and the theoretical identifiability results add a lot of value to the current CRL literature.
- The use of score functions and score differences in the observation space to estimate the unmixing function, especially for the UMN setting, is a novel and interesting approach for CRL.
- This work is the first to establish latent DAG recovery in the UMN setting under any type of multi-node intervention for arbitrary nonparametric latent causal models.

Weaknesses:
Although the theoretical contribution of this work is strong, the empirical evaluation is quite weak compared to other works in CRL. There are only experiments for n=4 causal variables. There is also no baseline comparison of the proposed framework with other methods in the UMN setting (e.g., [1]). Also, some discussions are a bit abridged and could use more elaboration in the paper (see below for details).

[1] Bing et al. “Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions” CLeaR 2024.

Limitations:
Limitations are discussed in Section 6.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper extends previous results on using score function for causal representation learning to the settings with unknown multi-node interventions. This new setting poses significant new challenges as opposed to the single node intervention case. The author first present theoretical identifiability result on hard interventions with latent additive noise model and on soft interventions. They then propose an algorithm called (UMNI)-CRL and test it on synthetic linear Gaussian dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written, easy to follow and with good motivations.

Weaknesses:
1. The transformation from latent to observed is noiseless, which could be a limitation. 
2. Line 199 says that: “This regularity condition ensures that the effect of a multi-node intervention is not the same on different nodes”. But how realistic or neccessary is this condition? It seems like it is very possible that an intervention can cause two downstream nodes to have the same effect although these two nodes is not influenced the same by all type of interventions. 
3. The experiments are only on synthetic dataset but I don’t think that is a big issue. 
4. Some potential missing citations
    
    [1] Kumar, Abhinav, and Gaurav Sinha. ""Disentangling mixtures of unknown causal interventions."" *Uncertainty in Artificial Intelligence*. PMLR, 2021.
    
    [2] Jiang, Yibo, and Bryon Aragam. ""Learning nonparametric latent causal graphs with unknown interventions."" *Advances in Neural Information Processing Systems* 36 (2024).

Limitations:
1. Theorem 1 only works for additive noise model.
2. The transformation from latent to observed noiseless.
3. Experiments are only on synthetic dataset. 
4. I am unsure if the algorithm is practical because it needs to estimate the score function.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces new identifiability results for CRL in environments with unknown multi-node interventions. It shows that, with sufficiently diverse interventional environments, one can achieve identifiability up to ancestors using soft interventions and perfect identifiability using hard interventions. The paper also provides an algorithm with identifiability guarantees.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper tackles the complex and underexplored multi-node intervention setting. The established identifiability can be crucial for extending current CRL theories into more practical contexts.
- The introduced algorithm that leverages score functions with different interventional environments is also interesting and insightful.
- The paper is well-motivated and articulated with high clarity.

Weaknesses:
- The proposed algorithm, while theoretically sound, seems computationally demanding. In fact, even a 4-node low-dimensional case requires a large number of environments and samples. The paper could benefit from a deeper discussion on the scalability of the algorithm.
- The current evaluation of the algorithm is limited to synthetic simulations. Expanding it to more realistic datasets would substantively improve its practical significance.

Limitations:
The paper acknowledges its main limitations in the reliance on linear transformations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wduRaBDRBS;"REVIEW 
Summary:
- This paper carries out an analysis of token merging[1] in the context of long-form video understanding and proposes learnable video token merging (VTM) to select semantics/saliency guided tokens for merging. 
- In token merging, at each layer, the tokens are divided into two sets source S and target T through uniform sampling. Tokens in S are matched to T based on similarity and merged (usually by average pooling). This paper compares this naive VTM with two other variants where selection of T is guided by informed heuristics: (1) region VTM where tokens at the center of each frame are more likely to be retained, (2) motion VTM where tokens with high motion are more likely to be retained. Through this analysis, authors argue that the strategy to select T plays an important role in the final performance.
- Motivated by this, authors propose a learnable VTM where it first predicts a saliency score for each input token. The target set T is sampled according to the probability distribution defined by saliency score. Since this partition operation is not differentiable, authors propose a novel training architecture where a parallel auxiliary network is trained alongside. The saliency scores are used to bias the attention score of the aux network, thereby supervising the saliency prediction to focus on important tokens. Aux network can be discarded at test time.
- Authors carry out a fair evaluation of the learnable VTM on LVU, Breakfast and COIN datasets by comparing against several baselines including ViS4mer, S5, D-sprv. Learnable VTM performs better than baselines in almost all evaluation tasks with low GPU memory usage and high throughput.

[1] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token Merging: Your ViT but faster. In ICLR, 2022.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- In learnable VTM, the idea of learnable and auxiliary path is interesting. There is no way to directly supervise the token saliency prediction of the main path because partition operation is non-differentiable. Hence the attention in auxiliary path is influenced by saliency scores of the main path, which encourages the saliency prediction to focus on important tokens.
- The evaluation is fair and consistent. The authors use the same backbone as the prior works to encode input video frames, thereby ensuring a fair evaluation.
- The results of learnable VTM on LVU dataset and Breakfast is noticeably better than baselines with less GPU memory usage. However, on COIN dataset, it doesn't perform better than S5 baseline.

Weaknesses:
### Major weaknesses
- One of the cited contribution is the exploration of region based and motion based VTM (Section 3.3) but it seems trivial. The effectiveness of token selection is already shown in learnable VTM. In light of that, there is an unreasonable focus section 3.3 which is unnecessary.
- Section 3.4 explains little about the details of learnable VTM, how it is trained, how the gradients flow in the presence of non-differentiable partition function, etc.
- There are some stretched claims based on qualitative and quantitative results. For example,
  - In Line 174, authors claim that center VTM performs better than naive VTM. However, according to Table 1, the results are mixed at best.
  - In Fig 5, authors also claim that the visualization of merged tokens show saliency based merging. However, the figure doesn't support the claim. There are many merged tokens on important salient features and some background tokens are not merged.

### Minor issues
- Line 19: it should be ""into the domain of video computer vision"" as all cited papers are video learning papers.
- Is there a difference between notation of C and D? It looks like both are used interchangably to denote token dimension.
- Table 2: How it throughput measured? fps?

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores various video token merging strategies in the context of long-form video classification and finally propose a learnable Video Token Merging algorithm that dynamically merges video tokens based on visual salient areas. The contributions are summarized as follow:
1.  Explore various video token merging methods including the naïve VTM, the region concentrated VTM, and the motion-based VTM.
2.  Propose the learnable video token merging algorithm, which estimates the saliency scores of each token and adaptively merge visual tokens based on those scores.
3. The proposed algorithm achieves the best or competitive results on various datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper explores various video token merging methods including the naïve VTM, the region concentrated VTM, and the motion-based VTM.
2. Compare with baseline and rule-based video token merging. The proposed learnable video token merging strategy has large improvement.
3. The two-paths design to deal with non-differentiable problem in partitioning process is interesting.

Weaknesses:
1. This paper proposes a leanable video token merging strategy. The similiar high-level idea can be found by CTS[1] in image domain. The novelty is insufficient。
2. This paper focuses on video token merging. However, I do not observe any specific design tailored for the video domain in terms of the methodology. let alone long video.



[1] Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper approaches the task of long-video understanding from token reduction perspective. Specifically, Transformer-based approaches suffers from memory bottleneck and quadratic computation complexity with increasing number of tokens, which is even more pressing with long-videos as input. The paper builds on a recently developed token merging mechanism, and proposes a learned saliency measure to modulate what tokens gets merged instead of using a random or hand-crafted saliency measure. The central hypothesis of the work is that typically techniques that use similarity as merging criteria may inadvertently lose out on salient tokens. The paper reports experiments on three conventional long-video benchmarks (LVU, Breakfast and COIN), and shows effectiveness of their approach compared to prior related works both in terms of performance and memory requirement. The paper also ablates the effectiveness of their proposed saliency measure (learned VTM) over hand-crafted measures including motion-based (using optical flow), center-biased and random schemes.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-written with most of the information presented for ease of understanding 
- The memory requirement is lower than S4, with competitive performance which highlights the importance of token selection in the case of long-videos

Weaknesses:
- Comparison to related token saliency approaches
    - The paper proposes a scheme to identify salient tokens by using a learned projection matrix $U_s \in \mathcal{R}^{D \times 1}$ with $\texttt{tanh}$ activation function
    - However, learnable token saliency methods have also been used in prior works, such as EVEREST [1], which uses a pair-wise learned saliency at feature-level (equation 2) using $\texttt{Conv3d}$. The resulting approach was shown to be effective in the Masked Autoencoding setup
    - Having a motion-based merging scheme is a good baseline, but some variants of learnable token saliency could also be tried to gain better understanding how token saliency gets influenced by different approaches   

- Role of $L_1, L_2, L_3$
    - The paper proposes to take tokens from $L_i$ consecutive frames for the $i^{th}$ VTM block
    - It seems that choosing the values of $L_i$ is quite crucial given its impact on performance and memory requirement (Table 6) that forms the central claim of the paper
    - However, the paper highlighted the contribution of token saliency more compared to the choice of $L_i$ hyperparameters
    - Did the authors experiment with a rather simplistic setup using a single VTM block and/or with all $L_i$ being 60? It would help the readers to gain better understanding of what works in long-videos 


- How saliency changes with tokens from different number of frames?
    - It seems that the saliency is being computed at each VTM block. It would be interesting to see how the saliency changes across the three VTM blocks
    - On that note, what VTM block’s saliency is being visualized in Figure 5?



### Minor
- Line 145-146: “$i$-th transformer block takes the tokens corresponding to $L_i$ frames without overlapping”
    - Confusing when $i$ is referred to as the frame number and the block number of transformer at the same time
- Line 145-147: $j$ is not defined


### Typos
- Line 24-25: “so the tokenizing the”
- Line 36: “selectio”
- Line 114-115: “applications are mostly remained”
- Line 117: “depedencies”
- Line 161: “in the videos, .”
- Line 165: “regarding less of the”
- Line 177: “sailent”, “the the”
- Line 306: “sailencies”


### References
[1] “EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal Tokens”. Sunil Hwang and Jaehong Yoon and Youngwan Lee and Sung Ju Hwang. ICML 2024.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper builds on Token Merging (ToMe) to improve its performance. In particular, the authors explore different ways to partition tokens so that the merging operation can lead to better performance while maintaining speed. They explore region-concentrated merging, motion-vector based merging and a learnable selector, and find that the learnable version works best. To make the network trainable, they employ a creative auxiliary path mechanism  to make everything differentiable. They find that their learnable VTM obtains good results compared to baselines on long form datasets (LVU, Coin, Breakfast), and that it outperforms the other methods they introduce.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem this paper addresses is an important one. Videos (especially long ones) have many redundant tokens and reducing their number while maintaining performance is a crucial problem to solve in the field.

The model itself is well designed and uses a creative auxiliary path to handle a non-differentiable partitioning process. Given the premise of the paper, the model is well-designed and seems to address the issue they propose. 

I also appreciate the exploration of different methods, and a comparison on which worked better. This kind of analysis is often missing from papers and I am grateful for the authors for including it.

Weaknesses:
I don’t really agree with the premise of the paper (and am open to a rebuttal to explain if I’m wrong here). Token Merging already explored merging for video in detail. The reason Token Merging is based on similarity is that by combining tokens that are extremely similar, the weighted average of the those tokens should produce an extremely similar result in the attention operation. This was also detailed more in the Token Merging follow-up TomeSD. If you use different criteria such as saliency (which is not really well-defined), this is no longer guaranteed, and from equation (10) it seems like the authors do not use the proportional attention scheme from ToMe (Eq 1 in the original paper). Table 8 doesn’t show the learnable method using this; it seems to just be about the pooling part rather than the attention operation.

I also don’t understand the intuition behind the saliency: shouldn’t we be aiming to combine together tokens that are NOT relevant, so that the transformer can focus more on the relevant tokens, rather than averaging (and thus losing) information from the more salient / important tokens? I’d really appreciate some clarification here. From Figure 3, it doesn’t look like learnable VTM is focusing on visually important tokens: it’s picking ones from the ceiling and wall in addition to the people.

My main issue is with the evaluation. The evaluation seems not quite fair, especially when measuring memory usage and throughput. Shouldn’t it be compared to baseline merging algorithms, like the naive ToMe? My impression is that the memory usage and throughput from VTM will be exactly the same as ToMe because it uses a similar partitioning scheme and constant factor reduction, which is why it may not be included in the results, but this seems important to include for context. Furthermore, the improvement on metrics is quite small, given that the speed is the same as other merging methods. Is this expected?

Also, The paper is motivated by “long-term” video, but evaluates on 64 frames, which isn’t really long and in my view, doesn’t merit only evaluating on LVU, Breakfast and COIN. Kinetics-400 has 300 frames per video, and is a more standard benchmark for evaluating video backbones - in fact, the original ToMe paper includes experiments on those datasets, which would make for a more fair comparison. Furthermore, nothing about the method itself is specific to these longer videos. I think evaluating on more standard datasets is crucial to measuring the actual strength of the method, especially compared to baselines like Token Merging. In particular, the long-form datasets are very compressible. 

The paper is not well-written and the grammar needs a lot of revision, making it hard to focus on the content of the paper itself. In addition, a lot of space is spent on methods that are not really used in the final results (center, region, motion vector) and on citing equations from preliminary works (token merging, attention). Given that a claimed contribution is an exploration of these different methods, I would also have expected more detailed ablations and experiments to understand exactly why some of the methods perform better than others.

Limitations:
The biggest limitation of this compared to baseline Token Merging is that it requires re-training. ToMe could be applied to a pre-trained network out-of-the-box. Learnable VTM cannot do this, making it impossible to apply to a pre-trained video network. The other proposed methods (region, motion vector) can do this though, and this would be a good thing to note somewhere in the paper.

In my view, the limitation of methods like VTM is that it always reduces a constant number of tokens per video, even though some videos are inherently more compressible than others. For example, Breakfast videos are extremely compressible compared to the average LVU video. However, this is beyond the scope of the paper, and using constant reduction is certainly more convenient, but this would be a good limitation to acknowledge and perhaps address in the future.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
- This paper carries out an analysis of token merging[1] in the context of long-form video understanding and proposes learnable video token merging (VTM) to select semantics/saliency guided tokens for merging. 
- In token merging, at each layer, the tokens are divided into two sets source S and target T through uniform sampling. Tokens in S are matched to T based on similarity and merged (usually by average pooling). This paper compares this naive VTM with two other variants where selection of T is guided by informed heuristics: (1) region VTM where tokens at the center of each frame are more likely to be retained, (2) motion VTM where tokens with high motion are more likely to be retained. Through this analysis, authors argue that the strategy to select T plays an important role in the final performance.
- Motivated by this, authors propose a learnable VTM where it first predicts a saliency score for each input token. The target set T is sampled according to the probability distribution defined by saliency score. Since this partition operation is not differentiable, authors propose a novel training architecture where a parallel auxiliary network is trained alongside. The saliency scores are used to bias the attention score of the aux network, thereby supervising the saliency prediction to focus on important tokens. Aux network can be discarded at test time.
- Authors carry out a fair evaluation of the learnable VTM on LVU, Breakfast and COIN datasets by comparing against several baselines including ViS4mer, S5, D-sprv. Learnable VTM performs better than baselines in almost all evaluation tasks with low GPU memory usage and high throughput.

[1] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token Merging: Your ViT but faster. In ICLR, 2022.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- In learnable VTM, the idea of learnable and auxiliary path is interesting. There is no way to directly supervise the token saliency prediction of the main path because partition operation is non-differentiable. Hence the attention in auxiliary path is influenced by saliency scores of the main path, which encourages the saliency prediction to focus on important tokens.
- The evaluation is fair and consistent. The authors use the same backbone as the prior works to encode input video frames, thereby ensuring a fair evaluation.
- The results of learnable VTM on LVU dataset and Breakfast is noticeably better than baselines with less GPU memory usage. However, on COIN dataset, it doesn't perform better than S5 baseline.

Weaknesses:
### Major weaknesses
- One of the cited contribution is the exploration of region based and motion based VTM (Section 3.3) but it seems trivial. The effectiveness of token selection is already shown in learnable VTM. In light of that, there is an unreasonable focus section 3.3 which is unnecessary.
- Section 3.4 explains little about the details of learnable VTM, how it is trained, how the gradients flow in the presence of non-differentiable partition function, etc.
- There are some stretched claims based on qualitative and quantitative results. For example,
  - In Line 174, authors claim that center VTM performs better than naive VTM. However, according to Table 1, the results are mixed at best.
  - In Fig 5, authors also claim that the visualization of merged tokens show saliency based merging. However, the figure doesn't support the claim. There are many merged tokens on important salient features and some background tokens are not merged.

### Minor issues
- Line 19: it should be ""into the domain of video computer vision"" as all cited papers are video learning papers.
- Is there a difference between notation of C and D? It looks like both are used interchangably to denote token dimension.
- Table 2: How it throughput measured? fps?

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores various video token merging strategies in the context of long-form video classification and finally propose a learnable Video Token Merging algorithm that dynamically merges video tokens based on visual salient areas. The contributions are summarized as follow:
1.  Explore various video token merging methods including the naïve VTM, the region concentrated VTM, and the motion-based VTM.
2.  Propose the learnable video token merging algorithm, which estimates the saliency scores of each token and adaptively merge visual tokens based on those scores.
3. The proposed algorithm achieves the best or competitive results on various datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper explores various video token merging methods including the naïve VTM, the region concentrated VTM, and the motion-based VTM.
2. Compare with baseline and rule-based video token merging. The proposed learnable video token merging strategy has large improvement.
3. The two-paths design to deal with non-differentiable problem in partitioning process is interesting.

Weaknesses:
1. This paper proposes a leanable video token merging strategy. The similiar high-level idea can be found by CTS[1] in image domain. The novelty is insufficient。
2. This paper focuses on video token merging. However, I do not observe any specific design tailored for the video domain in terms of the methodology. let alone long video.



[1] Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper approaches the task of long-video understanding from token reduction perspective. Specifically, Transformer-based approaches suffers from memory bottleneck and quadratic computation complexity with increasing number of tokens, which is even more pressing with long-videos as input. The paper builds on a recently developed token merging mechanism, and proposes a learned saliency measure to modulate what tokens gets merged instead of using a random or hand-crafted saliency measure. The central hypothesis of the work is that typically techniques that use similarity as merging criteria may inadvertently lose out on salient tokens. The paper reports experiments on three conventional long-video benchmarks (LVU, Breakfast and COIN), and shows effectiveness of their approach compared to prior related works both in terms of performance and memory requirement. The paper also ablates the effectiveness of their proposed saliency measure (learned VTM) over hand-crafted measures including motion-based (using optical flow), center-biased and random schemes.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-written with most of the information presented for ease of understanding 
- The memory requirement is lower than S4, with competitive performance which highlights the importance of token selection in the case of long-videos

Weaknesses:
- Comparison to related token saliency approaches
    - The paper proposes a scheme to identify salient tokens by using a learned projection matrix $U_s \in \mathcal{R}^{D \times 1}$ with $\texttt{tanh}$ activation function
    - However, learnable token saliency methods have also been used in prior works, such as EVEREST [1], which uses a pair-wise learned saliency at feature-level (equation 2) using $\texttt{Conv3d}$. The resulting approach was shown to be effective in the Masked Autoencoding setup
    - Having a motion-based merging scheme is a good baseline, but some variants of learnable token saliency could also be tried to gain better understanding how token saliency gets influenced by different approaches   

- Role of $L_1, L_2, L_3$
    - The paper proposes to take tokens from $L_i$ consecutive frames for the $i^{th}$ VTM block
    - It seems that choosing the values of $L_i$ is quite crucial given its impact on performance and memory requirement (Table 6) that forms the central claim of the paper
    - However, the paper highlighted the contribution of token saliency more compared to the choice of $L_i$ hyperparameters
    - Did the authors experiment with a rather simplistic setup using a single VTM block and/or with all $L_i$ being 60? It would help the readers to gain better understanding of what works in long-videos 


- How saliency changes with tokens from different number of frames?
    - It seems that the saliency is being computed at each VTM block. It would be interesting to see how the saliency changes across the three VTM blocks
    - On that note, what VTM block’s saliency is being visualized in Figure 5?



### Minor
- Line 145-146: “$i$-th transformer block takes the tokens corresponding to $L_i$ frames without overlapping”
    - Confusing when $i$ is referred to as the frame number and the block number of transformer at the same time
- Line 145-147: $j$ is not defined


### Typos
- Line 24-25: “so the tokenizing the”
- Line 36: “selectio”
- Line 114-115: “applications are mostly remained”
- Line 117: “depedencies”
- Line 161: “in the videos, .”
- Line 165: “regarding less of the”
- Line 177: “sailent”, “the the”
- Line 306: “sailencies”


### References
[1] “EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal Tokens”. Sunil Hwang and Jaehong Yoon and Youngwan Lee and Sung Ju Hwang. ICML 2024.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper builds on Token Merging (ToMe) to improve its performance. In particular, the authors explore different ways to partition tokens so that the merging operation can lead to better performance while maintaining speed. They explore region-concentrated merging, motion-vector based merging and a learnable selector, and find that the learnable version works best. To make the network trainable, they employ a creative auxiliary path mechanism  to make everything differentiable. They find that their learnable VTM obtains good results compared to baselines on long form datasets (LVU, Coin, Breakfast), and that it outperforms the other methods they introduce.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem this paper addresses is an important one. Videos (especially long ones) have many redundant tokens and reducing their number while maintaining performance is a crucial problem to solve in the field.

The model itself is well designed and uses a creative auxiliary path to handle a non-differentiable partitioning process. Given the premise of the paper, the model is well-designed and seems to address the issue they propose. 

I also appreciate the exploration of different methods, and a comparison on which worked better. This kind of analysis is often missing from papers and I am grateful for the authors for including it.

Weaknesses:
I don’t really agree with the premise of the paper (and am open to a rebuttal to explain if I’m wrong here). Token Merging already explored merging for video in detail. The reason Token Merging is based on similarity is that by combining tokens that are extremely similar, the weighted average of the those tokens should produce an extremely similar result in the attention operation. This was also detailed more in the Token Merging follow-up TomeSD. If you use different criteria such as saliency (which is not really well-defined), this is no longer guaranteed, and from equation (10) it seems like the authors do not use the proportional attention scheme from ToMe (Eq 1 in the original paper). Table 8 doesn’t show the learnable method using this; it seems to just be about the pooling part rather than the attention operation.

I also don’t understand the intuition behind the saliency: shouldn’t we be aiming to combine together tokens that are NOT relevant, so that the transformer can focus more on the relevant tokens, rather than averaging (and thus losing) information from the more salient / important tokens? I’d really appreciate some clarification here. From Figure 3, it doesn’t look like learnable VTM is focusing on visually important tokens: it’s picking ones from the ceiling and wall in addition to the people.

My main issue is with the evaluation. The evaluation seems not quite fair, especially when measuring memory usage and throughput. Shouldn’t it be compared to baseline merging algorithms, like the naive ToMe? My impression is that the memory usage and throughput from VTM will be exactly the same as ToMe because it uses a similar partitioning scheme and constant factor reduction, which is why it may not be included in the results, but this seems important to include for context. Furthermore, the improvement on metrics is quite small, given that the speed is the same as other merging methods. Is this expected?

Also, The paper is motivated by “long-term” video, but evaluates on 64 frames, which isn’t really long and in my view, doesn’t merit only evaluating on LVU, Breakfast and COIN. Kinetics-400 has 300 frames per video, and is a more standard benchmark for evaluating video backbones - in fact, the original ToMe paper includes experiments on those datasets, which would make for a more fair comparison. Furthermore, nothing about the method itself is specific to these longer videos. I think evaluating on more standard datasets is crucial to measuring the actual strength of the method, especially compared to baselines like Token Merging. In particular, the long-form datasets are very compressible. 

The paper is not well-written and the grammar needs a lot of revision, making it hard to focus on the content of the paper itself. In addition, a lot of space is spent on methods that are not really used in the final results (center, region, motion vector) and on citing equations from preliminary works (token merging, attention). Given that a claimed contribution is an exploration of these different methods, I would also have expected more detailed ablations and experiments to understand exactly why some of the methods perform better than others.

Limitations:
The biggest limitation of this compared to baseline Token Merging is that it requires re-training. ToMe could be applied to a pre-trained network out-of-the-box. Learnable VTM cannot do this, making it impossible to apply to a pre-trained video network. The other proposed methods (region, motion vector) can do this though, and this would be a good thing to note somewhere in the paper.

In my view, the limitation of methods like VTM is that it always reduces a constant number of tokens per video, even though some videos are inherently more compressible than others. For example, Breakfast videos are extremely compressible compared to the average LVU video. However, this is beyond the scope of the paper, and using constant reduction is certainly more convenient, but this would be a good limitation to acknowledge and perhaps address in the future.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wdGvRud1LS;"REVIEW 
Summary:
The paper presents a novel approach called Functional Maximal Correlation Algorithm with Trace cost (FMCA-T) for estimating cortico-muscular dependence by leveraging orthonormal decomposition of density ratios. This method is designed to model the relationship between EEG (electroencephalography) and EMG (electromyography) signals, addressing the challenges of interpretability, scalability, and local temporal dependence in cortico-muscular connectivity. The key contributions include introducing a matrix trace cost optimization for improved stability and efficiency, demonstrating robustness against nonstationary noise and delays, and effectively capturing movement and subject information from EEG features for enhanced classification accuracy. The proposed method outperforms existing baselines, particularly in cross-subject scenarios, and provides insights into channel-level and temporal dependencies, reinforcing its potential applications in brain-computer interface development and neuromuscular disorder diagnostics.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Innovative Method: Introduces the Functional Maximal Correlation Algorithm with Trace cost (FMCA-T), providing a novel approach for estimating cortico-muscular dependence.
2. Improved Stability and Efficiency: Utilizes matrix trace cost optimization, which is more stable and computationally efficient compared to traditional log-determinant cost methods.
3. Enhanced Classification Accuracy: Effectively captures movement and subject information from EEG features, significantly improving classification accuracy, especially in cross-subject scenarios	
4. Validation on Multiple Datasets: Validated using both simulated and real EEG-EMG datasets, confirming the method’s effectiveness and robustness.
5. Open Data and Reproducibility: Offers open access to datasets and detailed implementation code, facilitating reproducibility and further research in the field.

Weaknesses:
The provided baselines are relatively few; future work could expand on this.

Limitations:
1.Limited Dataset Size: The cross-subject classification performance drops, potentially due to the limited dataset size of only 25 participants. Larger and more diverse datasets may be needed to validate the method’s robustness comprehensively.
2. Generalization to Other Modalities: While the method shows promise for EEG and EMG signals, its generalizability to other types of biosignals or broader neural data modalities has not been extensively disscuss.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new method to model the relationship between cortical and muscular oscillations using EEG and EMG recordings. Traditional methods like Cortico-Muscular Coherence (CMC) have limitations, so the authors propose using statistical dependence estimators to learn eigenvalues, eigenfunctions, and projection spaces. This approach improves interpretability, scalability, and local temporal dependence. Experimental results show that the method accurately classifies movements and subjects, highlighting specific EEG channel activations during movements, and demonstrates robustness against noise and delays, suggesting its potential for diagnosing neuromuscular disorders and developing brain-computer interfaces.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper combines statistical dependence estimators with neural network optimization techniques. This fusion of methodologies enhances the ability to capture high-level and contextual connectivity between cortical and muscular oscillations.

2. The paper provides a detailed description of the proposed methodology, including the mathematical foundations, algorithmic implementation, and practical considerations. The inclusion of eigenvalues, eigenfunctions, and projection spaces adds depth to the analysis.

3. The authors conduct comprehensive experiments to validate their method. The results demonstrate the method's robustness against nonstationary noise and random delays, confirming its reliability and practical applicability.

Weaknesses:
1. Mathematical and Algorithmic Complexity: The proposed method involves complex mathematical formulations and advanced statistical techniques that may be challenging for a broader audience to grasp. Simplifying some of the mathematical derivations or providing more intuitive explanations and visualizations could make the paper more accessible.

2. Interpretation of Results: While the method highlights specific EEG channel activations during movements, the physiological and neuroscientific significance of these results could be further elaborated. Providing more detailed discussions on how these findings align with or differ from existing neuroscience research would enhance the interpretability and relevance of the results.

3. Scalability: The scalability of the proposed method to larger datasets or longer signal durations is not thoroughly addressed. Discussing the computational complexity and providing benchmarks on how the method performs with varying data sizes would be valuable.

Limitations:
Plz go and check weaknesses and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors apply novel but already existing (https://www.sciencedirect.com/science/article/pii/S0047259X2300074X, https://arxiv.org/pdf/2212.04631) machinery based approach on the orthonormal decomposition of density to decipher the relationship between cortical brain activity and the electromyographic signals during basic hand movements. The work is based on the publicly available dataset and the code is available. The unknown decomposition is modeled by a pair of neural networks concurrently processing EEG and EMG data in order to arrive at the internal representation for each of the modality. The internal representations are then aligned to minimize the rank of the joint covariance matrix. To guide the learning process, the authors propose a somewhat novel loss function equal to the negative trace of the canonical correlation matrix calculated using latent representations. The authors test their approach on the downstream tasks of classifying movement types in both within and across subject designs. They also apply the obtained representations to distinguish between participants based on their EEG data. The authors provide some interpretation to the obtained solution in the form of channel and temporal maps indicating the electrodes and time moments that contribute to the decoding most.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The authors applied novel but existing methodology of orthonormal density decomposition to the EEG+EMG dataset for the first time 
2. The authors introduced a novel loss function and showed that it provides better performance in the downstream task of EEG-based classification of movement types
3. The authors used multi subject dataset 
4. The authors attempted to provide interpretation of the obtained decision rule
5. The authors present detailed results of their experiments in the appendix

Weaknesses:
1. Several inaccuracies and lack of details in the mathematical expressions:
1.1 line 100, last expression and additional p(z) is needed in the integral
1.2 equation 3  - do the eigenvalues need to be normalized? Does the sum exclude the first normalized eigenvalue?
2. I would argue against the suggested novelty of the proposed loss function as it seems like the loss function that is closely associated with the Canonical correlation analysis (CCA) (equation 4). Generally speaking, the proposed approach boils down to the CCA in the latent variable space with latents computed by means of a CNN.
3. The authors claim that they “..design a specialized network topology to generate features for individual channels and time intervals, ensuring that the internal layers of this network quantify channel-level and temporal-level features, similar to [22–24]” - however unlike for instance the EEGnet, the authors use non-linearities in the temporal network (prior to the spatial) which in my view prevents the straightforward interpretation of the obtained representations at least using simple correlational measures. 
See also Q.1 and 2. 
4. The authors did not validate their approach to interpreting the decision rule and obtaining spatial and temporal maps with simulated data. This needs to be done and the simulated data should contain not only the neuronal sources coupled to the simulated EMG but also the sources unrelated to the signal of interest (EMG). The authors then need to demonstrate that their methods infers the proper spatial patterns corresponding to the task-relevant simulated neuronal sources. Ideally,  the obtained maps should be ranked based on their importance for the overall decoding accuracy. If this is not possible within the review cycle, the authors should significantly reduce the proportion of the manuscript dedicated to the physiological plausibility of their solution and instead describe limitations related to potentially non-physiological origin of the extracted features.  
5. It is disappointing that when interpreting the decision rule the authors did not provide information regarding the EEG frequency domain their network got tuned to during the training.

Limitations:
The authors partly addressed limitations but several items, see Weaknesses section, are left out.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel approach to analyzing cortico-muscular connectivity using statistical dependence measures based on density ratio decomposition. The authors apply a method called Functional Maximal Correlation Algorithm with Trace cost (FMCA-T) to paired EEG and EMG recordings. The key idea is to decompose the density ratio between EEG and EMG signals into eigenvalues and eigenfunctions, which can capture important contextual information that affects the EEG-EMG dependency such as type of movement or subject without having them labeled. They also use the learned eigenfunctions as feature projectors and train a classifier on top for movement type classification tasks.

The authors test their approach on simulated data (SinWav) and a real EEG-EMG dataset with 25 subjects performing 11 different upper limb movements. They compare FMCA-T against several baseline methods for dependence estimation and classification. They find that the learned eigenfunctions capture factors such as movement type and subject identity. Further, FMCA-T outperforms the baselines, for example by 10% for cross-subject classification of arm-reaching, hand-grasping and  wrist-twisting.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* very sophisticated method with clear motivation
* original idea cleanly mathematically derived (as far as I can tell)
* produces good results

Weaknesses:
* classification baselines, could be stronger, e.g. by also using [EEG Conformer](https://pubmed.ncbi.nlm.nih.gov/37015413/)  and [Deep4](https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.23730)
* text is very dense at times, definitely found some part hard to read, but not sure how much it can be made easier, possibly you could explain some concepts used in 2.2 in more detail in the supplementary

Limitations:
Authors could discuss a bit more under what conditions the assumption of conditional independence may be problematic and when it is fine for EEG/EMG. In terms of what one may expect to see in analyses as performed here under different conditions.
Of course, evaluation on further datasets would also be helpful for this.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a novel approach called Functional Maximal Correlation Algorithm with Trace cost (FMCA-T) for estimating cortico-muscular dependence by leveraging orthonormal decomposition of density ratios. This method is designed to model the relationship between EEG (electroencephalography) and EMG (electromyography) signals, addressing the challenges of interpretability, scalability, and local temporal dependence in cortico-muscular connectivity. The key contributions include introducing a matrix trace cost optimization for improved stability and efficiency, demonstrating robustness against nonstationary noise and delays, and effectively capturing movement and subject information from EEG features for enhanced classification accuracy. The proposed method outperforms existing baselines, particularly in cross-subject scenarios, and provides insights into channel-level and temporal dependencies, reinforcing its potential applications in brain-computer interface development and neuromuscular disorder diagnostics.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Innovative Method: Introduces the Functional Maximal Correlation Algorithm with Trace cost (FMCA-T), providing a novel approach for estimating cortico-muscular dependence.
2. Improved Stability and Efficiency: Utilizes matrix trace cost optimization, which is more stable and computationally efficient compared to traditional log-determinant cost methods.
3. Enhanced Classification Accuracy: Effectively captures movement and subject information from EEG features, significantly improving classification accuracy, especially in cross-subject scenarios	
4. Validation on Multiple Datasets: Validated using both simulated and real EEG-EMG datasets, confirming the method’s effectiveness and robustness.
5. Open Data and Reproducibility: Offers open access to datasets and detailed implementation code, facilitating reproducibility and further research in the field.

Weaknesses:
The provided baselines are relatively few; future work could expand on this.

Limitations:
1.Limited Dataset Size: The cross-subject classification performance drops, potentially due to the limited dataset size of only 25 participants. Larger and more diverse datasets may be needed to validate the method’s robustness comprehensively.
2. Generalization to Other Modalities: While the method shows promise for EEG and EMG signals, its generalizability to other types of biosignals or broader neural data modalities has not been extensively disscuss.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new method to model the relationship between cortical and muscular oscillations using EEG and EMG recordings. Traditional methods like Cortico-Muscular Coherence (CMC) have limitations, so the authors propose using statistical dependence estimators to learn eigenvalues, eigenfunctions, and projection spaces. This approach improves interpretability, scalability, and local temporal dependence. Experimental results show that the method accurately classifies movements and subjects, highlighting specific EEG channel activations during movements, and demonstrates robustness against noise and delays, suggesting its potential for diagnosing neuromuscular disorders and developing brain-computer interfaces.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper combines statistical dependence estimators with neural network optimization techniques. This fusion of methodologies enhances the ability to capture high-level and contextual connectivity between cortical and muscular oscillations.

2. The paper provides a detailed description of the proposed methodology, including the mathematical foundations, algorithmic implementation, and practical considerations. The inclusion of eigenvalues, eigenfunctions, and projection spaces adds depth to the analysis.

3. The authors conduct comprehensive experiments to validate their method. The results demonstrate the method's robustness against nonstationary noise and random delays, confirming its reliability and practical applicability.

Weaknesses:
1. Mathematical and Algorithmic Complexity: The proposed method involves complex mathematical formulations and advanced statistical techniques that may be challenging for a broader audience to grasp. Simplifying some of the mathematical derivations or providing more intuitive explanations and visualizations could make the paper more accessible.

2. Interpretation of Results: While the method highlights specific EEG channel activations during movements, the physiological and neuroscientific significance of these results could be further elaborated. Providing more detailed discussions on how these findings align with or differ from existing neuroscience research would enhance the interpretability and relevance of the results.

3. Scalability: The scalability of the proposed method to larger datasets or longer signal durations is not thoroughly addressed. Discussing the computational complexity and providing benchmarks on how the method performs with varying data sizes would be valuable.

Limitations:
Plz go and check weaknesses and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors apply novel but already existing (https://www.sciencedirect.com/science/article/pii/S0047259X2300074X, https://arxiv.org/pdf/2212.04631) machinery based approach on the orthonormal decomposition of density to decipher the relationship between cortical brain activity and the electromyographic signals during basic hand movements. The work is based on the publicly available dataset and the code is available. The unknown decomposition is modeled by a pair of neural networks concurrently processing EEG and EMG data in order to arrive at the internal representation for each of the modality. The internal representations are then aligned to minimize the rank of the joint covariance matrix. To guide the learning process, the authors propose a somewhat novel loss function equal to the negative trace of the canonical correlation matrix calculated using latent representations. The authors test their approach on the downstream tasks of classifying movement types in both within and across subject designs. They also apply the obtained representations to distinguish between participants based on their EEG data. The authors provide some interpretation to the obtained solution in the form of channel and temporal maps indicating the electrodes and time moments that contribute to the decoding most.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The authors applied novel but existing methodology of orthonormal density decomposition to the EEG+EMG dataset for the first time 
2. The authors introduced a novel loss function and showed that it provides better performance in the downstream task of EEG-based classification of movement types
3. The authors used multi subject dataset 
4. The authors attempted to provide interpretation of the obtained decision rule
5. The authors present detailed results of their experiments in the appendix

Weaknesses:
1. Several inaccuracies and lack of details in the mathematical expressions:
1.1 line 100, last expression and additional p(z) is needed in the integral
1.2 equation 3  - do the eigenvalues need to be normalized? Does the sum exclude the first normalized eigenvalue?
2. I would argue against the suggested novelty of the proposed loss function as it seems like the loss function that is closely associated with the Canonical correlation analysis (CCA) (equation 4). Generally speaking, the proposed approach boils down to the CCA in the latent variable space with latents computed by means of a CNN.
3. The authors claim that they “..design a specialized network topology to generate features for individual channels and time intervals, ensuring that the internal layers of this network quantify channel-level and temporal-level features, similar to [22–24]” - however unlike for instance the EEGnet, the authors use non-linearities in the temporal network (prior to the spatial) which in my view prevents the straightforward interpretation of the obtained representations at least using simple correlational measures. 
See also Q.1 and 2. 
4. The authors did not validate their approach to interpreting the decision rule and obtaining spatial and temporal maps with simulated data. This needs to be done and the simulated data should contain not only the neuronal sources coupled to the simulated EMG but also the sources unrelated to the signal of interest (EMG). The authors then need to demonstrate that their methods infers the proper spatial patterns corresponding to the task-relevant simulated neuronal sources. Ideally,  the obtained maps should be ranked based on their importance for the overall decoding accuracy. If this is not possible within the review cycle, the authors should significantly reduce the proportion of the manuscript dedicated to the physiological plausibility of their solution and instead describe limitations related to potentially non-physiological origin of the extracted features.  
5. It is disappointing that when interpreting the decision rule the authors did not provide information regarding the EEG frequency domain their network got tuned to during the training.

Limitations:
The authors partly addressed limitations but several items, see Weaknesses section, are left out.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel approach to analyzing cortico-muscular connectivity using statistical dependence measures based on density ratio decomposition. The authors apply a method called Functional Maximal Correlation Algorithm with Trace cost (FMCA-T) to paired EEG and EMG recordings. The key idea is to decompose the density ratio between EEG and EMG signals into eigenvalues and eigenfunctions, which can capture important contextual information that affects the EEG-EMG dependency such as type of movement or subject without having them labeled. They also use the learned eigenfunctions as feature projectors and train a classifier on top for movement type classification tasks.

The authors test their approach on simulated data (SinWav) and a real EEG-EMG dataset with 25 subjects performing 11 different upper limb movements. They compare FMCA-T against several baseline methods for dependence estimation and classification. They find that the learned eigenfunctions capture factors such as movement type and subject identity. Further, FMCA-T outperforms the baselines, for example by 10% for cross-subject classification of arm-reaching, hand-grasping and  wrist-twisting.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* very sophisticated method with clear motivation
* original idea cleanly mathematically derived (as far as I can tell)
* produces good results

Weaknesses:
* classification baselines, could be stronger, e.g. by also using [EEG Conformer](https://pubmed.ncbi.nlm.nih.gov/37015413/)  and [Deep4](https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.23730)
* text is very dense at times, definitely found some part hard to read, but not sure how much it can be made easier, possibly you could explain some concepts used in 2.2 in more detail in the supplementary

Limitations:
Authors could discuss a bit more under what conditions the assumption of conditional independence may be problematic and when it is fine for EEG/EMG. In terms of what one may expect to see in analyses as performed here under different conditions.
Of course, evaluation on further datasets would also be helpful for this.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wcxHbAY8B3;"REVIEW 
Summary:
This paper proposes GaussianMarker, a novel method for embedding invisible watermarks into 3D Gaussian Splatting (3DGS) models to protect their copyright. The key idea is to use uncertainty estimation to add imperceptible perturbations to 3D Gaussian parameters with high uncertainty. The method enables extraction of copyright messages from both the 3D Gaussian parameters and rendered 2D images, and demonstrates robustness to various 3D and 2D distortions. Experiments on multiple datasets show the effectiveness of the approach in terms of message decoding accuracy and visual quality preservation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Timely contribution addressing copyright protection for 3D Gaussian Splatting models, an increasingly important 3D asset format
* Clever use of uncertainty estimation to guide watermark embedding in a way that preserves visual quality
* Demonstrates robustness to various 3D and 2D distortions/attacks

Weaknesses:
* The decoder is trained per scene, rather than being a generalizable decoder. This makes the watermarking process essentially impractical for real-world use. It's not feasible for people to store a separate watermark encoder and decoder for each scene for the vast number of Gaussians distributed across the internet. Reflecting on the logic of image watermarking, a single watermark encoder and decoder can encode and decode information for any cover image, so the sender and receiver only need to jointly possess one watermark decoder. This is a more reasonable setup.
* Experiments focus mostly on relatively simple scenes - more complex, dynamic scenes could be challenging
* The robustness to more sophisticated attacks (e.g. adversarial perturbations) is not explored
* Discussion of potential negative impacts of the technology could be expanded

Limitations:
The authors have included a brief limitations section that acknowledges potential vulnerabilities to malicious attacks beyond their technical solution. However, this discussion could be expanded to consider more specific limitations of the approach, such as potential challenges with very complex scenes or highly dynamic content. The societal impact is briefly mentioned, focusing on the positive aspects of copyright protection. A more thorough examination of potential negative impacts or misuse scenarios would strengthen the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
3D Gaussian Splatting(3DGS) has gradually become the mainstream method for acquiring 3D assets, which has led to a demand for copyright protection of 3DGS. In this paper, a watermarking method based on uncertainty called GaussianMarker is proposed. Firstly, 3DGS is partitioned based on uncertainty, and the watermark is only added to the model parameters with high uncertainty. Subsequently, the corresponding parameters are perturbed using both 2D and 3D watermark encoders, enabling the extraction of watermark information from rendered 2D images as well as directly from 3D model parameters. Experimental results demonstrate the robustness of the proposed GaussianMarker method against 2D and 3D distortions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper proposes a method that utilizes uncertainty to partition 3D Gaussian. By embedding watermarks specifically in the parameters with high uncertainty, the method aims to mitigate the impact on the quality of the model.

2. The paper considers the extraction of watermarks in both 2D and 3D scenarios, taking into account the robustness of watermark extraction in these two contexts.

Weaknesses:
1. The paper mentions that the calculation of uncertainty is related to the model parameters, and in 3D Gaussian, each point has multiple parameters such as $\mu, R, S, c, and \alpha$. It would be helpful if the authors could clarify which specific parameters are used in the proposed method. Additionally, the paper provides a formula for calculating model uncertainty, but it is unclear how the uncertainty of each Gaussian is computed and used for partitioning. The authors should provide further explanation or clarification on this matter.
2. The description of the densify function $g(\cdot)$ in the paper states that it randomly samples a new position from a distribution. According to my understanding, the original Gaussian $G_i$ should have been replaced. However, Figure 2 shows that the original Gaussian  $G_i$ still exists, which is confusing to me.
3. During the watermark embedding process, it is unclear whether the 2D and 3D watermarks are embedded into the same model parameters. It would be helpful if the authors could clarify which specific model parameters of the 3D Gaussian are used for embedding the watermarks.
4. In the section on ""Distilling watermarking knowledge,"" the authors mention that ""the pre-trained feature from 2D space can be distilled to the 3D space."" It is important for the authors to provide an explanation of how this is achieved.

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a new method for embedding digital watermarks in 3D Gaussian Splatting (3DGS) models to protect the copyright of 3D assets. Traditional watermarking techniques for mesh, point cloud, and implicit radiance fields are not suitable for 3DGS, as they can cause distortions in rendered images. The authors propose an uncertainty-based approach that constrains perturbations to the model parameters, ensuring that watermarks remain invisible while preserving visual quality. The method allows for reliable extraction of copyright messages from both 3D Gaussians and 2D rendered images, even under various distortions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method ensures that the embedded watermarks do not cause significant distortions in the rendered 3D scenes or 2D images, maintaining the visual quality of the assets.
2. The approach is designed to be robust against various forms of 3D and 2D distortions, such as noise, translation, rotation, cropping, JPEG compression, scaling, and blurring. This enhances the reliability of copyright protection.
3. The method allows for the extraction of copyright messages from both 3D Gaussian parameters and 2D rendered images, providing multiple layers of security and verification.
4. Extensive experiments demonstrate that the method achieves state-of-the-art performance in both message decoding accuracy and view synthesis quality.

Weaknesses:
The malicious scenarios considered are limited to traditional distortions. \
More sophisticated scenarios should also be explored. \
For instance, a malicious actor could fine-tune the downloaded 3DGS or use an auto-encoder to remove embedded information ([1],[2],[3]). \
In such cases, how would the proposed method perform?

Additionally, a more complex scenario to consider is when a malicious actor renders Bob's 3DGS and uses it as training data to create their own 3DGS. \
How would the proposed method address these advanced threats?

[1] Fernandez et al., The Stable Signature: Rooting Watermarks in Latent Diffusion Models \
[2] Kim et al., WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models \
[3] Zhao et al., Invisible Image Watermarks Are Provably Removable Using Generative AI

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an uncertainty-based method to achieve watermarking for 3D Gaussian Splatting. Specifically, the Hessian matrix is used to estimate the parameter uncertainty. Then, the 3D Gaussians with high uncertainty are densified. The densified 3D Gaussians are trained to embed watermarking using a pre-trained 2D message decoder. After that, a 3D message decoder is trained using PointNet. Experimental results show that the proposed method achieves the best performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to follow. 

2. The experimental results show that the proposed method achieves new SOTA results. 

3. The proposed method can decode watermarking both in 2D rendered images and 3D assets. 

4. An uncertainty-based method is proposed to select trainable 3D Gaussians, which is reasonable.

Weaknesses:
1. One concern about this paper is its novelty. The major contribution of this paper is the introduction of uncertainty into 3D Gaussians watermarking. As the definition of uncertainty using Fisher Information comes from [42], simply using uncertainty for 3D Gaussians watermarking is quite simple and straightforward. Regarding the message decoders, they are all standard operations. HiDDeN [11] is used for the 2D message decoder, and PointNet [43] is used for the 3D message decoder. Therefore, the major contribution of the proposed method should be further justified.

2. The proposed method utilizes the 3D Gaussians with high uncertainty to embed watermarking. What if an attacker also uses this feature? The attacker could first identify the 3D Gaussians (after training/fine-tuning) with high uncertainty and then only attack these 3D Gaussians using techniques such as Noise, Translation, Rotation, or Cropout. Additionally, the attacker might delete some of the identified 3D Gaussians to compromise the 3DGS assets.

3. The influence of the parameter uncertainty threshold should be included in the experiments to assess the sensitivity of the uncertainty threshold on the proposed method. 

4. The results with different bit lengths are missing.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes GaussianMarker, a novel method for embedding invisible watermarks into 3D Gaussian Splatting (3DGS) models to protect their copyright. The key idea is to use uncertainty estimation to add imperceptible perturbations to 3D Gaussian parameters with high uncertainty. The method enables extraction of copyright messages from both the 3D Gaussian parameters and rendered 2D images, and demonstrates robustness to various 3D and 2D distortions. Experiments on multiple datasets show the effectiveness of the approach in terms of message decoding accuracy and visual quality preservation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Timely contribution addressing copyright protection for 3D Gaussian Splatting models, an increasingly important 3D asset format
* Clever use of uncertainty estimation to guide watermark embedding in a way that preserves visual quality
* Demonstrates robustness to various 3D and 2D distortions/attacks

Weaknesses:
* The decoder is trained per scene, rather than being a generalizable decoder. This makes the watermarking process essentially impractical for real-world use. It's not feasible for people to store a separate watermark encoder and decoder for each scene for the vast number of Gaussians distributed across the internet. Reflecting on the logic of image watermarking, a single watermark encoder and decoder can encode and decode information for any cover image, so the sender and receiver only need to jointly possess one watermark decoder. This is a more reasonable setup.
* Experiments focus mostly on relatively simple scenes - more complex, dynamic scenes could be challenging
* The robustness to more sophisticated attacks (e.g. adversarial perturbations) is not explored
* Discussion of potential negative impacts of the technology could be expanded

Limitations:
The authors have included a brief limitations section that acknowledges potential vulnerabilities to malicious attacks beyond their technical solution. However, this discussion could be expanded to consider more specific limitations of the approach, such as potential challenges with very complex scenes or highly dynamic content. The societal impact is briefly mentioned, focusing on the positive aspects of copyright protection. A more thorough examination of potential negative impacts or misuse scenarios would strengthen the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
3D Gaussian Splatting(3DGS) has gradually become the mainstream method for acquiring 3D assets, which has led to a demand for copyright protection of 3DGS. In this paper, a watermarking method based on uncertainty called GaussianMarker is proposed. Firstly, 3DGS is partitioned based on uncertainty, and the watermark is only added to the model parameters with high uncertainty. Subsequently, the corresponding parameters are perturbed using both 2D and 3D watermark encoders, enabling the extraction of watermark information from rendered 2D images as well as directly from 3D model parameters. Experimental results demonstrate the robustness of the proposed GaussianMarker method against 2D and 3D distortions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper proposes a method that utilizes uncertainty to partition 3D Gaussian. By embedding watermarks specifically in the parameters with high uncertainty, the method aims to mitigate the impact on the quality of the model.

2. The paper considers the extraction of watermarks in both 2D and 3D scenarios, taking into account the robustness of watermark extraction in these two contexts.

Weaknesses:
1. The paper mentions that the calculation of uncertainty is related to the model parameters, and in 3D Gaussian, each point has multiple parameters such as $\mu, R, S, c, and \alpha$. It would be helpful if the authors could clarify which specific parameters are used in the proposed method. Additionally, the paper provides a formula for calculating model uncertainty, but it is unclear how the uncertainty of each Gaussian is computed and used for partitioning. The authors should provide further explanation or clarification on this matter.
2. The description of the densify function $g(\cdot)$ in the paper states that it randomly samples a new position from a distribution. According to my understanding, the original Gaussian $G_i$ should have been replaced. However, Figure 2 shows that the original Gaussian  $G_i$ still exists, which is confusing to me.
3. During the watermark embedding process, it is unclear whether the 2D and 3D watermarks are embedded into the same model parameters. It would be helpful if the authors could clarify which specific model parameters of the 3D Gaussian are used for embedding the watermarks.
4. In the section on ""Distilling watermarking knowledge,"" the authors mention that ""the pre-trained feature from 2D space can be distilled to the 3D space."" It is important for the authors to provide an explanation of how this is achieved.

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a new method for embedding digital watermarks in 3D Gaussian Splatting (3DGS) models to protect the copyright of 3D assets. Traditional watermarking techniques for mesh, point cloud, and implicit radiance fields are not suitable for 3DGS, as they can cause distortions in rendered images. The authors propose an uncertainty-based approach that constrains perturbations to the model parameters, ensuring that watermarks remain invisible while preserving visual quality. The method allows for reliable extraction of copyright messages from both 3D Gaussians and 2D rendered images, even under various distortions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method ensures that the embedded watermarks do not cause significant distortions in the rendered 3D scenes or 2D images, maintaining the visual quality of the assets.
2. The approach is designed to be robust against various forms of 3D and 2D distortions, such as noise, translation, rotation, cropping, JPEG compression, scaling, and blurring. This enhances the reliability of copyright protection.
3. The method allows for the extraction of copyright messages from both 3D Gaussian parameters and 2D rendered images, providing multiple layers of security and verification.
4. Extensive experiments demonstrate that the method achieves state-of-the-art performance in both message decoding accuracy and view synthesis quality.

Weaknesses:
The malicious scenarios considered are limited to traditional distortions. \
More sophisticated scenarios should also be explored. \
For instance, a malicious actor could fine-tune the downloaded 3DGS or use an auto-encoder to remove embedded information ([1],[2],[3]). \
In such cases, how would the proposed method perform?

Additionally, a more complex scenario to consider is when a malicious actor renders Bob's 3DGS and uses it as training data to create their own 3DGS. \
How would the proposed method address these advanced threats?

[1] Fernandez et al., The Stable Signature: Rooting Watermarks in Latent Diffusion Models \
[2] Kim et al., WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models \
[3] Zhao et al., Invisible Image Watermarks Are Provably Removable Using Generative AI

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an uncertainty-based method to achieve watermarking for 3D Gaussian Splatting. Specifically, the Hessian matrix is used to estimate the parameter uncertainty. Then, the 3D Gaussians with high uncertainty are densified. The densified 3D Gaussians are trained to embed watermarking using a pre-trained 2D message decoder. After that, a 3D message decoder is trained using PointNet. Experimental results show that the proposed method achieves the best performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to follow. 

2. The experimental results show that the proposed method achieves new SOTA results. 

3. The proposed method can decode watermarking both in 2D rendered images and 3D assets. 

4. An uncertainty-based method is proposed to select trainable 3D Gaussians, which is reasonable.

Weaknesses:
1. One concern about this paper is its novelty. The major contribution of this paper is the introduction of uncertainty into 3D Gaussians watermarking. As the definition of uncertainty using Fisher Information comes from [42], simply using uncertainty for 3D Gaussians watermarking is quite simple and straightforward. Regarding the message decoders, they are all standard operations. HiDDeN [11] is used for the 2D message decoder, and PointNet [43] is used for the 3D message decoder. Therefore, the major contribution of the proposed method should be further justified.

2. The proposed method utilizes the 3D Gaussians with high uncertainty to embed watermarking. What if an attacker also uses this feature? The attacker could first identify the 3D Gaussians (after training/fine-tuning) with high uncertainty and then only attack these 3D Gaussians using techniques such as Noise, Translation, Rotation, or Cropout. Additionally, the attacker might delete some of the identified 3D Gaussians to compromise the 3DGS assets.

3. The influence of the parameter uncertainty threshold should be included in the experiments to assess the sensitivity of the uncertainty threshold on the proposed method. 

4. The results with different bit lengths are missing.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wcX04Wn34u;"REVIEW 
Summary:
In this paper, the authors propose a method to help alleviate the domain gaps among different datasets with different LiDAR sensors, which can enable zero-shot detection on a new dataset. The proposed method including Scene Modeling for foreground and background reconstruction and LiDAR Modeling with statistical and ray-drop modeling. Another contribution is that the authors also accelerate the ray casting algorithm using GPU. The authors conducted single-domain and Multi-domain unification experiments on Waymo, nuScenes, and KITTI datasets, which achieves SOTA performance compared to previous works. The authors also provide ablation studies on foreground diversity and LiDAR noise injection. In addition, the authors show the run time performance after the GPU acceleration.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The foreground and background reconstruction and LiDAR Modeling and the statistical and ray-drop modeling in LiDAR Modeling make the paper differ from previous works.
Quality: The code is provided. The performance is evaluated on multiple datasets, and achieves SOTA performance compared to previous works, and ablation studies are good. The GPU acceleration is also good.
Clarity: The images in the paper are clear and easy to understand.
Significance: The paper demonstrates the potential of zero-shot detection on a new dataset by 3D reconstruction from multiple different dataset and LiDAR settings and LiDAR simulation.

Weaknesses:
In the title of the paper, the use of terms such as ""Language,"" ""Translator,"" and ""LiT"" appears to be capitalizing on the popularity of the trending terms ""LLM"", ""ViT"", and ""DiT"", potentially misleading readers.
SECOND and PV-RCNN are relatively old detection models, it's better to have experiments on more recent models such as CenterPoint, and other SOTA models to further demonstrate the effect of domain unification on SOTA models and even achieve new SOTA results. This would significantly enhance the paper's persuasiveness and impact.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
To address the significant gap between different LiDAR datasets (related to sensors, environments, etc.), this paper proposes a solution that differs from the existing model-based adaptation approach. By employing a scene-reconstruction-data-simulation approach, it achieves consistent representation of different LiDAR datasets. This data-driven method partially resolves issues such as domain shift in autonomous-driving-related 3D  point cloud learning.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Innovatively analogizing the domain gap between different LiDAR data to that between languages, this paper proposes a data-driven cross-sensor training method from a ""translation"" perspective.

- The proposed method shows good performance across different datasets, especially in terms of the AP3D metric.

- The paper is well-written with clear logic and comprehensive experiments.

Weaknesses:
- Does ""foreground"" only refer to vehicles? Do pedestrians, bicycles, and similar entities fall into this category?

- Similarly, in background reconstruction, is consideration limited to rigid bodies like the ground? In autonomous driving scenarios, is there no need to consider non-rigid objects such as vegetation?

- In the current version, it seems that scene variations are not significant. Does this mean it's difficult to address zero-shot scenarios? For instance, if the source data are all from residential areas, is it challenging to accurately simulate point clouds from downtown areas?

Limitations:
The analysis and discussion regarding scene reconstruction need improvement, as suggested by the previous  comments.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposed a unifying LiDAR Translator named LiT to achieve LiDAR domain adaptation. Differing from current model-driven approaches, LiT adopts a novel data-driven approach, embedding disparate LiDAR attributes into a common representation. LiT
proposes a generalizable scene modeling and LiDAR statistical modeling. Besides, an efficient ray-casting engine is proposed to accelerate the above models. LiT also achieves efficient SoTA performance on several LiDAR datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1. LiT adopts a novel data-driven approach instead of the classical model-driven approach, embedding disparate LiDAR attributes into a common representation. This research direction provides much value for real-world applications in autonomous driving industries. 

S2. An effective ray-casting engine is proposed to accelerate LiT on GPUs.

S3. Experiments on widely used datasets demonstrate the SOTA performance of LiT.

Weaknesses:
W1. This work looks like a data normalization operation, only modifying different datasets into a unified representation.

W2. The authors argue that model-driven approaches will introduce considerable costs associated with customizing model structure and training data for new, specific domains. However, this work has an extra LiDAR statistical modeling, this operation also causes additional costs.

W3. Table 7 shows that LiT may not avoid the problem of model-driven approaches, that is, requiring different configurations for distinct datasets.

Limitations:
Although the proposed data-driven approach seemed to be a promising research direction, LiT lacks sufficient comparisons with the model-driven approaches. Besides, LiT seemed to show ""ununified"" for the training process of different adaptation tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel framework designed to unify LiDAR data into a single target “language” and unified domain detection capabilities across diverse LiDAR datasets, marking a step toward domain unification for LiDAR-based autonomous driving systems. Experiments on dataset KITTI, Waymo, and nuScenes demonstrate the superiority of the proposed method in the task of single-source and multi-sources domain adaptation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The paper is novel in introducing LiDAR Translator (LiT) to joint training across multiple datasets. LiT enables efficient state-of-the-art zero-shot and unified domain detection capabilities across diverse LiDAR datasets. 
2.	The paper is well-written and easy to follow, especially the part explaining the background. 
3.	It presents good experimental results and intuitive visualizations, convincingly demonstrating its effectiveness.

Weaknesses:
1.	The motivation of this paper is not clear. If it is possible to accurately model the target domain data, why is there a need to translate the source domain data into the target domain data?
2.	As the core component of this work, the translator requires more direct experimental validation, such as measuring the distributional differences between the translated data and the target data, rather than solely relying on verification through downstream domain adaptation tasks.
3.	It  lacks of comparative experiments with the latest state-of-the-art methods.

Limitations:
The authors discussed potential limitations about the data, annotation and the category of object.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a method to help alleviate the domain gaps among different datasets with different LiDAR sensors, which can enable zero-shot detection on a new dataset. The proposed method including Scene Modeling for foreground and background reconstruction and LiDAR Modeling with statistical and ray-drop modeling. Another contribution is that the authors also accelerate the ray casting algorithm using GPU. The authors conducted single-domain and Multi-domain unification experiments on Waymo, nuScenes, and KITTI datasets, which achieves SOTA performance compared to previous works. The authors also provide ablation studies on foreground diversity and LiDAR noise injection. In addition, the authors show the run time performance after the GPU acceleration.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The foreground and background reconstruction and LiDAR Modeling and the statistical and ray-drop modeling in LiDAR Modeling make the paper differ from previous works.
Quality: The code is provided. The performance is evaluated on multiple datasets, and achieves SOTA performance compared to previous works, and ablation studies are good. The GPU acceleration is also good.
Clarity: The images in the paper are clear and easy to understand.
Significance: The paper demonstrates the potential of zero-shot detection on a new dataset by 3D reconstruction from multiple different dataset and LiDAR settings and LiDAR simulation.

Weaknesses:
In the title of the paper, the use of terms such as ""Language,"" ""Translator,"" and ""LiT"" appears to be capitalizing on the popularity of the trending terms ""LLM"", ""ViT"", and ""DiT"", potentially misleading readers.
SECOND and PV-RCNN are relatively old detection models, it's better to have experiments on more recent models such as CenterPoint, and other SOTA models to further demonstrate the effect of domain unification on SOTA models and even achieve new SOTA results. This would significantly enhance the paper's persuasiveness and impact.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
To address the significant gap between different LiDAR datasets (related to sensors, environments, etc.), this paper proposes a solution that differs from the existing model-based adaptation approach. By employing a scene-reconstruction-data-simulation approach, it achieves consistent representation of different LiDAR datasets. This data-driven method partially resolves issues such as domain shift in autonomous-driving-related 3D  point cloud learning.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Innovatively analogizing the domain gap between different LiDAR data to that between languages, this paper proposes a data-driven cross-sensor training method from a ""translation"" perspective.

- The proposed method shows good performance across different datasets, especially in terms of the AP3D metric.

- The paper is well-written with clear logic and comprehensive experiments.

Weaknesses:
- Does ""foreground"" only refer to vehicles? Do pedestrians, bicycles, and similar entities fall into this category?

- Similarly, in background reconstruction, is consideration limited to rigid bodies like the ground? In autonomous driving scenarios, is there no need to consider non-rigid objects such as vegetation?

- In the current version, it seems that scene variations are not significant. Does this mean it's difficult to address zero-shot scenarios? For instance, if the source data are all from residential areas, is it challenging to accurately simulate point clouds from downtown areas?

Limitations:
The analysis and discussion regarding scene reconstruction need improvement, as suggested by the previous  comments.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposed a unifying LiDAR Translator named LiT to achieve LiDAR domain adaptation. Differing from current model-driven approaches, LiT adopts a novel data-driven approach, embedding disparate LiDAR attributes into a common representation. LiT
proposes a generalizable scene modeling and LiDAR statistical modeling. Besides, an efficient ray-casting engine is proposed to accelerate the above models. LiT also achieves efficient SoTA performance on several LiDAR datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1. LiT adopts a novel data-driven approach instead of the classical model-driven approach, embedding disparate LiDAR attributes into a common representation. This research direction provides much value for real-world applications in autonomous driving industries. 

S2. An effective ray-casting engine is proposed to accelerate LiT on GPUs.

S3. Experiments on widely used datasets demonstrate the SOTA performance of LiT.

Weaknesses:
W1. This work looks like a data normalization operation, only modifying different datasets into a unified representation.

W2. The authors argue that model-driven approaches will introduce considerable costs associated with customizing model structure and training data for new, specific domains. However, this work has an extra LiDAR statistical modeling, this operation also causes additional costs.

W3. Table 7 shows that LiT may not avoid the problem of model-driven approaches, that is, requiring different configurations for distinct datasets.

Limitations:
Although the proposed data-driven approach seemed to be a promising research direction, LiT lacks sufficient comparisons with the model-driven approaches. Besides, LiT seemed to show ""ununified"" for the training process of different adaptation tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel framework designed to unify LiDAR data into a single target “language” and unified domain detection capabilities across diverse LiDAR datasets, marking a step toward domain unification for LiDAR-based autonomous driving systems. Experiments on dataset KITTI, Waymo, and nuScenes demonstrate the superiority of the proposed method in the task of single-source and multi-sources domain adaptation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The paper is novel in introducing LiDAR Translator (LiT) to joint training across multiple datasets. LiT enables efficient state-of-the-art zero-shot and unified domain detection capabilities across diverse LiDAR datasets. 
2.	The paper is well-written and easy to follow, especially the part explaining the background. 
3.	It presents good experimental results and intuitive visualizations, convincingly demonstrating its effectiveness.

Weaknesses:
1.	The motivation of this paper is not clear. If it is possible to accurately model the target domain data, why is there a need to translate the source domain data into the target domain data?
2.	As the core component of this work, the translator requires more direct experimental validation, such as measuring the distributional differences between the translated data and the target data, rather than solely relying on verification through downstream domain adaptation tasks.
3.	It  lacks of comparative experiments with the latest state-of-the-art methods.

Limitations:
The authors discussed potential limitations about the data, annotation and the category of object.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wblxm5zdkE;"REVIEW 
Summary:
The paper studies online sample selection with individual and interaction 
constraints simultaneously. Specifically, the goal is to control (variants) of 
the false selection rate (FDR) and the expected similarity (ES) under the 
empirical bayes framework. Under distributional conditions, the proposed method 
controls the target quantities asymptotically. The method is evaluated on synthetic 
and real data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-presented and easy to follow.
2. The problem under consideration is of interest and relevant.

Weaknesses:
Please see my comments in the questions section.

Limitations:
The authors have partically addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a method for online sample selection. The authors introduce the concepts of _individual_ and _interactive constraints_, and demonstrate theoretically and empirically that their method satisfies both.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
The problem seems important and the formulation and approach novel. The authors provide both theoretical guarantees and empirical evidence, in both synthetic and real-world applications, of the effectiveness of their approach. The mathematical formulation seems sound, and the assumptions and theoretical results are clearly stated.

Weaknesses:
I am not familiar with the FDR control literature, and had to read parts of the paper (specifically sections 2.2, 2.3 and 4) multiple times to get a gist for the logic of the method and its empirical perfomance. This is reflected in my confidence score. If the paper is accepted, I highly recommend the authors revise the paper to make it easier to follow. A flowchart to illustrate the steps of the algorithm and/or to illustrate the differences between the Oracle and Data-driven selection procedures may be helpful; a toy example could also help. I also suggest including a longer description and/or table of the benchmark methods against which the empirical performance of II-COS was compared.

Limitations:
The authors discuss limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors quantify the uncertainty of response predictions using predictive inference; and systematically addressing individual and interactive constraints in a sequential manner.  An online selection rule is developed to ensure the above two types of constraints are under control at pre-specified levels simultaneously.  Simulated and real-data examples  are used to evaluate and illustrate their approach in terms of both online individual and interactive criteria control.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This is a nicely, clearly written paper that develops an online selection rule that is simple yet effective.  The simplicity of the online selection rule will enhance the potential for  this rule to be used in real life.   The authors’ claims are well supported both via theory, simulations and application to real data. The paper along with the appendix provides detailed information that allow for replicability.  Very nice!  I particularly appreciate the comparison with the approaches based on conformal p-values.

Weaknesses:
See questions below.

Limitations:
Comments are needed on the practicality of \hat\mu being a bijection.  How critical is this assumption to the method and the theoretical analysis.  If critical then this is a limitation.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a framework to perform online sample selection such that the unseen outcomes are in specific target range while also optimizing for constraints like diversity that are dependent on the input covariates. The additional constraint involving input covariates can help ensure properties like the diversity of candidates when selecting individuals for interviews while also guaranteeing that most of the interviewed individuals accept the offer. The paper proposes a data-driven procedure to select the subset of candidates in an online fashion by implementing the proposed algorithm. Under reasonably weak assumptions, the paper provides theoretical guarantees on satisfying both the above constraints in online sample selection. The experiments confirm that this framework ensures low false selection rates (i.e. unseen outcomes are in a specific target range) while optimizing for the additional covariate-dependent constraints like diversity on synthetic and real data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper proposes an intuitive way to incorporate covariate-dependent constraints like the diversity of candidates when performing online sample selection to optimize for metrics like false selection rates. This paper solves an important problem in online sample selection and demonstrates that the proposed method improves the covariate-dependent objective while maintaining comparable performance on false selection rates.

Weaknesses:
It would be interesting to understand the gaps between an ideal diversity profile and the profile obtained by the proposed method in Fig 3. Analysing the gap w.r.t changing g(X_i, X_j) function choice could be helpful. Would it be helpful to increase the weight of the g(X_i, X_j) term to reduce this gap and understand its implications on the satisfaction of individual constraints?

It is evident that the SAST baseline outperforms the proposed method in terms of FSR sometimes, which is understandable given there; 's a tradeoff with the interactive constraints (Table 2b, Fig 1).  It would be helpful to learn if we can reduce the gap between SAST and the proposed method by balancing the tradeoff (perhaps using a tunable hyperparameter that balances the two constraints?).

Limitations:
Yes, limitations and broader impacts are discussed in the last section of paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies online sample selection with individual and interaction 
constraints simultaneously. Specifically, the goal is to control (variants) of 
the false selection rate (FDR) and the expected similarity (ES) under the 
empirical bayes framework. Under distributional conditions, the proposed method 
controls the target quantities asymptotically. The method is evaluated on synthetic 
and real data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-presented and easy to follow.
2. The problem under consideration is of interest and relevant.

Weaknesses:
Please see my comments in the questions section.

Limitations:
The authors have partically addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a method for online sample selection. The authors introduce the concepts of _individual_ and _interactive constraints_, and demonstrate theoretically and empirically that their method satisfies both.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
The problem seems important and the formulation and approach novel. The authors provide both theoretical guarantees and empirical evidence, in both synthetic and real-world applications, of the effectiveness of their approach. The mathematical formulation seems sound, and the assumptions and theoretical results are clearly stated.

Weaknesses:
I am not familiar with the FDR control literature, and had to read parts of the paper (specifically sections 2.2, 2.3 and 4) multiple times to get a gist for the logic of the method and its empirical perfomance. This is reflected in my confidence score. If the paper is accepted, I highly recommend the authors revise the paper to make it easier to follow. A flowchart to illustrate the steps of the algorithm and/or to illustrate the differences between the Oracle and Data-driven selection procedures may be helpful; a toy example could also help. I also suggest including a longer description and/or table of the benchmark methods against which the empirical performance of II-COS was compared.

Limitations:
The authors discuss limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors quantify the uncertainty of response predictions using predictive inference; and systematically addressing individual and interactive constraints in a sequential manner.  An online selection rule is developed to ensure the above two types of constraints are under control at pre-specified levels simultaneously.  Simulated and real-data examples  are used to evaluate and illustrate their approach in terms of both online individual and interactive criteria control.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This is a nicely, clearly written paper that develops an online selection rule that is simple yet effective.  The simplicity of the online selection rule will enhance the potential for  this rule to be used in real life.   The authors’ claims are well supported both via theory, simulations and application to real data. The paper along with the appendix provides detailed information that allow for replicability.  Very nice!  I particularly appreciate the comparison with the approaches based on conformal p-values.

Weaknesses:
See questions below.

Limitations:
Comments are needed on the practicality of \hat\mu being a bijection.  How critical is this assumption to the method and the theoretical analysis.  If critical then this is a limitation.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a framework to perform online sample selection such that the unseen outcomes are in specific target range while also optimizing for constraints like diversity that are dependent on the input covariates. The additional constraint involving input covariates can help ensure properties like the diversity of candidates when selecting individuals for interviews while also guaranteeing that most of the interviewed individuals accept the offer. The paper proposes a data-driven procedure to select the subset of candidates in an online fashion by implementing the proposed algorithm. Under reasonably weak assumptions, the paper provides theoretical guarantees on satisfying both the above constraints in online sample selection. The experiments confirm that this framework ensures low false selection rates (i.e. unseen outcomes are in a specific target range) while optimizing for the additional covariate-dependent constraints like diversity on synthetic and real data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper proposes an intuitive way to incorporate covariate-dependent constraints like the diversity of candidates when performing online sample selection to optimize for metrics like false selection rates. This paper solves an important problem in online sample selection and demonstrates that the proposed method improves the covariate-dependent objective while maintaining comparable performance on false selection rates.

Weaknesses:
It would be interesting to understand the gaps between an ideal diversity profile and the profile obtained by the proposed method in Fig 3. Analysing the gap w.r.t changing g(X_i, X_j) function choice could be helpful. Would it be helpful to increase the weight of the g(X_i, X_j) term to reduce this gap and understand its implications on the satisfaction of individual constraints?

It is evident that the SAST baseline outperforms the proposed method in terms of FSR sometimes, which is understandable given there; 's a tradeoff with the interactive constraints (Table 2b, Fig 1).  It would be helpful to learn if we can reduce the gap between SAST and the proposed method by balancing the tradeoff (perhaps using a tunable hyperparameter that balances the two constraints?).

Limitations:
Yes, limitations and broader impacts are discussed in the last section of paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wbE0QCBWji;"REVIEW 
Summary:
This paper tackles the field of adversarial image generation by proposing an unrestricted attack method that can be applied to both targeted and untargeted attacks. The innovative approach considers a probabilistic perspective, treating the victim classifier and geometric constraints as distinct distributions. By drawing adversarial examples from the overlap region, the authors ensure that the semantics of the original image are preserved. The efficacy of this proposed approach is convincingly demonstrated through extensive experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. I find the probabilistic approach proposed in this paper to be particularly innovative and refreshing. The motivation behind this perspective is clearly articulated, providing a solid foundation for the authors' methodology.
2. I am impressed by the encouraging experimental results presented in this paper. The inclusion of a human annotation experiment is particularly noteworthy, as it adds an important layer of validation to the authors' claims. Moreover, the study's success in handling both transfer attacks and adversarial defense scenarios further underscores the model's robustness and effectiveness.

Weaknesses:
While the experimental results of the proposed method show promise, I do believe there is room for improvement. Specifically, I think it would be beneficial for the authors to provide more detailed information regarding the human experiment methodology, such as how the five reviewers for the MNIST experiment were selected. Furthermore, I would suggest that the authors consider conducting a follow-up experiment where human annotators are asked to identify perturbed images in the absence of a reference image for the ImageNet experiment, which is a more realistic scenario in an attack setting.

In addition, I find it intriguing that both NCF and cAdv demonstrated higher success rates in generating adversarial examples compared to the proposed method, as shown in Table 2. This highlights some shortcoming in the proposed approach. While it is expected that NCF would generate images that can be identified as perturbed, I am more surprised that cAdv was able to create perturbed images that are highly similar to the original ones.

Lastly, I think it would be beneficial for the authors to explore targeted attacks on ImageNet, given the success of this approach in previous papers such as ""Towards Transferable Targeted Attack"". This could provide valuable insights into the robustness and effectiveness of the proposed method.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new type of adversarial attack, which generates adversarial examples by solving a box-constrained non-convex optimization problem. Different from the traditional norm-bounded attacks, this paper focuses on unrestricted adversarial attacks by replacing the geometrical distance measure with a semantically-aware distance measure. Specifically, The authors propose using a Langevin Monte Carlo (LMC) technique to sample adversarial examples from a probabilistic distribution. To preserve semantics, the authors use a learned energy function to guide the generation of adversarial samples. Following this, rejection sampling and refinement techniques are employed to select and improve the quality of the generated samples. Experiments show that this attack can fool classifiers while preserving the semantic information compared to baseline methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper introduces an interesting perspective on generating adversarial examples, which is significantly different from the traditional norm-bounded adversarial attacks. 
2. This paper is theoretically sound and the proposed solution is very intuitive.
3. It is suprising that the proposed attack can achieve a 100% success rate on an adversarially trained model. Adversarial training is often regarded as a SOTA defense method. Therefore, in my view, this work can motivate researchers in this area to design better defense methods. 
4. The proposed method can either outperform baseline methods by a notable margin or significantly improve the quality of the generated adversarial examples in terms of preserving semantic meanings.

Weaknesses:
1. Selecting 20 images from each class in the MNIST test set seems to be too little. I understand that it might be infeasible for human annotators to annotate all adversarial images for the entire MNIST, so I would encourage authors to report the success rate except for human annotations using the entire MNIST. I believe this will make the results more convincing.
2. This paper is missing ablation studies for rejection sampling and sample refinement techniques. Is it necessary to include these techniques? How would it affect the attack success rate if one of them is removed?
3. This paper proposes a new attack method but lacks a discussion on how to defend against it. Although it is not compulsory, I am more willing to see how to defend this attack. Can you provide some intuitions on it?
4. Standard deviations are not reported in this paper. Repeated experiments are encouraged.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a probabilistic framework for generating adversarial examples, focusing on maintaining the semantic integrity of the original images while implementing substantial pixel-level modifications. Unlike conventional adversarial techniques that rely heavily on minimal geometric perturbations, this approach integrates a semantic understanding into the adversarial example generation process, leveraging energy-based models and diffusion models. The core innovation lies in embedding the semantic interpretation as a probabilistic distribution, which guides the adversarial example generation. This allows for effective deception of classifiers, including those equipped with adversarial defenses, while preserving the semantic content to an extent that remains imperceptible to human observers. Empirical evaluations demonstrate that the proposed method outperforms existing techniques in terms of both effectiveness against defenses and undetectability by humans, establishing a new paradigm for constructing robust and stealthy adversarial attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is clear and well-written, effectively highlighting its contributions with accessible explanations of complex ideas.

2. This paper presents a new probabilistic framework for generating adversarial examples that goes beyond traditional norm-bounded methods by integrating semantic distributions. The approach is theoretically robust, with the theoretical analysis providing a solid foundation that supports the model's effectiveness and introduces innovative concepts to the field of adversarial machine learning.

3. The proposed method significantly outperforms baseline methods, particularly in preserving their semantic integrity.

Weaknesses:
1. The assumption in Equation 4 lacks a detailed derivation, leaving it unclear whether $x_{\text{ori}}$ and $y_{\text{tar}}$ need to be independent. Providing a clear derivation and clarifying this assumption would enhance the theoretical rigor of the paper.

2. The training process for the diffusion models is sensitive and requires careful parameter tuning. The paper does not provide enough detail on this sensitivity or potential solutions to mitigate training instability, which impacts the robustness and reproducibility of the method.

3. The paper does not report standard deviations in the performance results. Repeating the experiments is recommended to ensure the reliability and consistency of the findings.

Limitations:
Paper limitations can be found in the above comments.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper tackles the field of adversarial image generation by proposing an unrestricted attack method that can be applied to both targeted and untargeted attacks. The innovative approach considers a probabilistic perspective, treating the victim classifier and geometric constraints as distinct distributions. By drawing adversarial examples from the overlap region, the authors ensure that the semantics of the original image are preserved. The efficacy of this proposed approach is convincingly demonstrated through extensive experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. I find the probabilistic approach proposed in this paper to be particularly innovative and refreshing. The motivation behind this perspective is clearly articulated, providing a solid foundation for the authors' methodology.
2. I am impressed by the encouraging experimental results presented in this paper. The inclusion of a human annotation experiment is particularly noteworthy, as it adds an important layer of validation to the authors' claims. Moreover, the study's success in handling both transfer attacks and adversarial defense scenarios further underscores the model's robustness and effectiveness.

Weaknesses:
While the experimental results of the proposed method show promise, I do believe there is room for improvement. Specifically, I think it would be beneficial for the authors to provide more detailed information regarding the human experiment methodology, such as how the five reviewers for the MNIST experiment were selected. Furthermore, I would suggest that the authors consider conducting a follow-up experiment where human annotators are asked to identify perturbed images in the absence of a reference image for the ImageNet experiment, which is a more realistic scenario in an attack setting.

In addition, I find it intriguing that both NCF and cAdv demonstrated higher success rates in generating adversarial examples compared to the proposed method, as shown in Table 2. This highlights some shortcoming in the proposed approach. While it is expected that NCF would generate images that can be identified as perturbed, I am more surprised that cAdv was able to create perturbed images that are highly similar to the original ones.

Lastly, I think it would be beneficial for the authors to explore targeted attacks on ImageNet, given the success of this approach in previous papers such as ""Towards Transferable Targeted Attack"". This could provide valuable insights into the robustness and effectiveness of the proposed method.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new type of adversarial attack, which generates adversarial examples by solving a box-constrained non-convex optimization problem. Different from the traditional norm-bounded attacks, this paper focuses on unrestricted adversarial attacks by replacing the geometrical distance measure with a semantically-aware distance measure. Specifically, The authors propose using a Langevin Monte Carlo (LMC) technique to sample adversarial examples from a probabilistic distribution. To preserve semantics, the authors use a learned energy function to guide the generation of adversarial samples. Following this, rejection sampling and refinement techniques are employed to select and improve the quality of the generated samples. Experiments show that this attack can fool classifiers while preserving the semantic information compared to baseline methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper introduces an interesting perspective on generating adversarial examples, which is significantly different from the traditional norm-bounded adversarial attacks. 
2. This paper is theoretically sound and the proposed solution is very intuitive.
3. It is suprising that the proposed attack can achieve a 100% success rate on an adversarially trained model. Adversarial training is often regarded as a SOTA defense method. Therefore, in my view, this work can motivate researchers in this area to design better defense methods. 
4. The proposed method can either outperform baseline methods by a notable margin or significantly improve the quality of the generated adversarial examples in terms of preserving semantic meanings.

Weaknesses:
1. Selecting 20 images from each class in the MNIST test set seems to be too little. I understand that it might be infeasible for human annotators to annotate all adversarial images for the entire MNIST, so I would encourage authors to report the success rate except for human annotations using the entire MNIST. I believe this will make the results more convincing.
2. This paper is missing ablation studies for rejection sampling and sample refinement techniques. Is it necessary to include these techniques? How would it affect the attack success rate if one of them is removed?
3. This paper proposes a new attack method but lacks a discussion on how to defend against it. Although it is not compulsory, I am more willing to see how to defend this attack. Can you provide some intuitions on it?
4. Standard deviations are not reported in this paper. Repeated experiments are encouraged.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a probabilistic framework for generating adversarial examples, focusing on maintaining the semantic integrity of the original images while implementing substantial pixel-level modifications. Unlike conventional adversarial techniques that rely heavily on minimal geometric perturbations, this approach integrates a semantic understanding into the adversarial example generation process, leveraging energy-based models and diffusion models. The core innovation lies in embedding the semantic interpretation as a probabilistic distribution, which guides the adversarial example generation. This allows for effective deception of classifiers, including those equipped with adversarial defenses, while preserving the semantic content to an extent that remains imperceptible to human observers. Empirical evaluations demonstrate that the proposed method outperforms existing techniques in terms of both effectiveness against defenses and undetectability by humans, establishing a new paradigm for constructing robust and stealthy adversarial attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is clear and well-written, effectively highlighting its contributions with accessible explanations of complex ideas.

2. This paper presents a new probabilistic framework for generating adversarial examples that goes beyond traditional norm-bounded methods by integrating semantic distributions. The approach is theoretically robust, with the theoretical analysis providing a solid foundation that supports the model's effectiveness and introduces innovative concepts to the field of adversarial machine learning.

3. The proposed method significantly outperforms baseline methods, particularly in preserving their semantic integrity.

Weaknesses:
1. The assumption in Equation 4 lacks a detailed derivation, leaving it unclear whether $x_{\text{ori}}$ and $y_{\text{tar}}$ need to be independent. Providing a clear derivation and clarifying this assumption would enhance the theoretical rigor of the paper.

2. The training process for the diffusion models is sensitive and requires careful parameter tuning. The paper does not provide enough detail on this sensitivity or potential solutions to mitigate training instability, which impacts the robustness and reproducibility of the method.

3. The paper does not report standard deviations in the performance results. Repeating the experiments is recommended to ensure the reliability and consistency of the findings.

Limitations:
Paper limitations can be found in the above comments.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wZigMVFURk;"REVIEW 
Summary:
The preprint proposes to replace the collocation based PINN loss by a sum of local continuous integrals over regions around the collocation points. These continuous integrals are then again discretized using Monte Carlo integration with a single quadrature pint. The authors furthermore propose to adapt the region size during training using gradient statistics.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors report good empirical performance on a number of benchmark problems.

Weaknesses:
The introduction of continuous integrals over regions around the collocation points that subsequentially are discretized by Monte Carlo integration again seems tautological. After all, the loss function in PINNs is already a Monte Carlo discretization of a continuous integral (over the whole computational domain). Furthermore, the analysis that the authors present for the modified loss in equation (5) should not be carried out with the continuous integral over the regions $\Omega_r$ but with its Monte Carlo approximation. Otherwise, the comparison to the discretized PINN loss is unfair.

Limitations:
See questions

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper extends the optimization process of PINNs from isolated points to their continuous neighborhood regions, which can theoretically decrease the generalization error, especially for hidden high-order constraints of PDEs. A practical training algorithm, Region Optimized PINN (RoPINN), is seamlessly derived from this new paradigm, which is implemented by a straightforward but effective Monte Carlo sampling method. By calibrating the sampling process into trust regions, RoPINN finely balances sampling efficiency and generalization error. Experimentally, RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of extending the optimization process of PINNs from isolated points to their continuous neighborhood regions is novel.
2. Theoretical results on generalization error, convergence rate and estimation error of sampling are provided. 
3. A practical training algorithm, Region Optimized PINN (RoPINN), is seamlessly derived from the region optimization paradigm and associated theoretical results,
4. RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation.

Weaknesses:
1. It is better to include the main proof idea of theoretical results in the main text.
2. Although generalization error bound is provided, an intuitive explanation of the reason behind the success of region optimization is desirable. For example, when sampling one point in each region, why is the total loss decreased compared with point optimization?

Limitations:
The case of sampling more than one points in each region is not discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors developed a region optimized PINN to improve the prediction accuracy compared to the scatter-point based PINN.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors proposed the region optimization paradigm and conducted a theoretical analysis.

Weaknesses:
The practical application scope is limited.

Limitations:
Some initial guess methods can be developed to efficiently determine the preferable region size and sample number.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel optimization method for training physics-informed neural networks (PINNs): Region optimization, which extends a regular pointwise optimization of PINNs to neighborhood regions, named RoPINNs. The paper provides theoretical analysis explaining the decrease of generalization error with RoPINNs and high-order PDE constraints satisfaction. Then the paper presents a practical algorithm to enable the RoPINNs paradigm by introducing Monte-Carlo approximation of region integral and a region calibration scheme. Lastly, the paper assesses the performance on several well-known benchmark problems and showed the improved performance over the considered baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-motivated and tackles the important problem in training PINNs (leveraging more information than just a point-to-point type mapping).

- The paper presents theoretical analysis on benefits of RoPINNs, decreased generalization errors and satisfaction of higher-order PDE constraints.

- The experimental results show that the proposed algorithm is effective in solving some challenging benchmark problems (known as failure modes) and is capable of producing more accurate solution approximates.

Weaknesses:
- Although shown to be very effective in several benchmark problems, the paper does not seem to provide general guidelines on how to set some important hyper-parameters such as initial region size and the number of sampling points. (While acknowledging that the authors indicate this as one of the limitations,) it would be great to see some experts’ guidelines.

- If the authors could provide some analysis with regards to computational wall time, that would provide more complete pictures on how the proposed method performs. For example, it would be information to see a figure depicting a scatter plot showing computational wall time versus final rMSE type information, where a point in the plot corresponds to a different hyper-parameter setting (that is, the number of sample points). 

- [minor] there is a typo in the second paragraph of Section 4.2: line 289 Figure 2 => Figure 3.

Limitations:
- some more discussions on practical guidelines would be needed for users who want to utilize this method in different applications 

- some additional experiments (regarding computational wall time) would be needed to provide a complete picture of the proposed method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The preprint proposes to replace the collocation based PINN loss by a sum of local continuous integrals over regions around the collocation points. These continuous integrals are then again discretized using Monte Carlo integration with a single quadrature pint. The authors furthermore propose to adapt the region size during training using gradient statistics.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors report good empirical performance on a number of benchmark problems.

Weaknesses:
The introduction of continuous integrals over regions around the collocation points that subsequentially are discretized by Monte Carlo integration again seems tautological. After all, the loss function in PINNs is already a Monte Carlo discretization of a continuous integral (over the whole computational domain). Furthermore, the analysis that the authors present for the modified loss in equation (5) should not be carried out with the continuous integral over the regions $\Omega_r$ but with its Monte Carlo approximation. Otherwise, the comparison to the discretized PINN loss is unfair.

Limitations:
See questions

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper extends the optimization process of PINNs from isolated points to their continuous neighborhood regions, which can theoretically decrease the generalization error, especially for hidden high-order constraints of PDEs. A practical training algorithm, Region Optimized PINN (RoPINN), is seamlessly derived from this new paradigm, which is implemented by a straightforward but effective Monte Carlo sampling method. By calibrating the sampling process into trust regions, RoPINN finely balances sampling efficiency and generalization error. Experimentally, RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of extending the optimization process of PINNs from isolated points to their continuous neighborhood regions is novel.
2. Theoretical results on generalization error, convergence rate and estimation error of sampling are provided. 
3. A practical training algorithm, Region Optimized PINN (RoPINN), is seamlessly derived from the region optimization paradigm and associated theoretical results,
4. RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation.

Weaknesses:
1. It is better to include the main proof idea of theoretical results in the main text.
2. Although generalization error bound is provided, an intuitive explanation of the reason behind the success of region optimization is desirable. For example, when sampling one point in each region, why is the total loss decreased compared with point optimization?

Limitations:
The case of sampling more than one points in each region is not discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors developed a region optimized PINN to improve the prediction accuracy compared to the scatter-point based PINN.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors proposed the region optimization paradigm and conducted a theoretical analysis.

Weaknesses:
The practical application scope is limited.

Limitations:
Some initial guess methods can be developed to efficiently determine the preferable region size and sample number.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel optimization method for training physics-informed neural networks (PINNs): Region optimization, which extends a regular pointwise optimization of PINNs to neighborhood regions, named RoPINNs. The paper provides theoretical analysis explaining the decrease of generalization error with RoPINNs and high-order PDE constraints satisfaction. Then the paper presents a practical algorithm to enable the RoPINNs paradigm by introducing Monte-Carlo approximation of region integral and a region calibration scheme. Lastly, the paper assesses the performance on several well-known benchmark problems and showed the improved performance over the considered baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-motivated and tackles the important problem in training PINNs (leveraging more information than just a point-to-point type mapping).

- The paper presents theoretical analysis on benefits of RoPINNs, decreased generalization errors and satisfaction of higher-order PDE constraints.

- The experimental results show that the proposed algorithm is effective in solving some challenging benchmark problems (known as failure modes) and is capable of producing more accurate solution approximates.

Weaknesses:
- Although shown to be very effective in several benchmark problems, the paper does not seem to provide general guidelines on how to set some important hyper-parameters such as initial region size and the number of sampling points. (While acknowledging that the authors indicate this as one of the limitations,) it would be great to see some experts’ guidelines.

- If the authors could provide some analysis with regards to computational wall time, that would provide more complete pictures on how the proposed method performs. For example, it would be information to see a figure depicting a scatter plot showing computational wall time versus final rMSE type information, where a point in the plot corresponds to a different hyper-parameter setting (that is, the number of sample points). 

- [minor] there is a typo in the second paragraph of Section 4.2: line 289 Figure 2 => Figure 3.

Limitations:
- some more discussions on practical guidelines would be needed for users who want to utilize this method in different applications 

- some additional experiments (regarding computational wall time) would be needed to provide a complete picture of the proposed method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wZgw4CrxwK;"REVIEW 
Summary:
The authors formulate a theoretical setup for a LLM text generation service to incentivize the service to output high quality text the consumer. The authors formulate this setup as having the service having a set of models that has quality (as rated by a evaluator on the end of the consumer) that increases with the cost of running the LLM. The goal is to derive a framework for paying the LLM service based on that quality of the text generated that incentivizes the LLM service to always use its best model. The authors go about this by formulating the definition of a contract in this setting, and defining various metric (max payment, avg. payment, etc.) that consumer aims optimize. They show that the set of contracts that will incentivize the LLM service to output text with the best model can be derived from the set of optimal hypothesis tests that distinguish which model is being used from the evaluator outputs. They derive how the optimal contract can be formulated from these hypothesis tests, when only bounds are known on the costs of running each model for the LLM service.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The theoretical setup and monotone assumption of model performance, cost, etc. is quite reasonable and tackles and interesting and relevant problem with LLM queries. The results are simple and intuitive, and connect nicely with previous work on contract theory and hypothesis testing.

Weaknesses:
The main issue is the theoretical setup does require an assumption that the bounds on cost are known, which seems somewhat impractical. It might be useful to explicitly comment on how the different metrics degrade with increased looseness of the cost bounds (linearly, it seems like), since one can always pick extremely conservative cost bounds.

Minor issue:

- In Definition 3, it would be helpful to illustrate why $B_R^*$ and $B^*_\rho$ are defined as they are, correctly. Further, the definition with $b \geq c_n$ is a bit strange, since the definition of minmax hypothesis test does not involve worst case over cost vectors, so it doesn't seem correct to use $b$ to derive the minimax contract (and instead it should remain a function of $c_n - c_1$) --- maybe dropping that $b \geq c_n$ case would be more accurate, since it is used in the definition of cost-robust later.

Limitations:
I think the limitations are sufficiently addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issue of moral hazard in pay-per-token pricing for large language model (LLM) services. Firms may use cheaper, lower-quality models to cut costs, compromising text quality. Moreover, the firms's costs may be unknown to the clients. To counter this, the authors propose a pay-for-performance framework using cost-robust contracts that incentivize high-quality text generation under the uncertainty about the firms's costs. These contracts are designed based on and have a one-to-one correspondence to optimal composite hypothesis tests. Approximation guarantees are provided with respect to the optimal contracts. Empirical evaluations show that these contracts provide robust incentives with small cost increases.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The results of characterizing the forms of optimal cost-robust contracts using hypothesis testing, as well as the approximation guarantees, are interesting and have valuable contributions.

Weaknesses:
1. The model's complexity is unnecessary. The problem could actually be studied in the most basic contract setting. 
2. The authors do not discuss the computational complexity of finding the optimal cost-robust contract. 
3. The authors do not discuss the cases where the action with the highest cost may not be the best action to incentivize.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
* The paper concerns the problem of incentivizing LLMs to use the most costly model, which is assumed to be the model with the best performance. Without proper incentive, the LLM company has the incentive to charge customers for the highest payment, but deliver the service using a lower-cost model, because the performance of the model is usually not verified. Therefore, the paper proposes to use contract design to solve this problem. In particular, an automatic detector first gives an integer score for the performance of an LLM. Then, the task is to design a payment for the company for each of the integer scores. The goal is to minimize the total payment conditioned on incentivizing the best-performed model. 
* The main contribution is the discussion of the cost-robust contract, meaning how to design the optimal contract while the costs of LLMs are unknown. Empirical evaluations have shown how to use the theory in practical settings given LLM performance data.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
* (I’m not an expert in contract design.) I appreciate the theoretical contributions of the paper. To me, section 4 has several interesting insights into connecting cost-robust contracts with hypothesis tests. As claimed, this is the first paper considering cost-robust contract design. However, the real contribution should be better evaluated by experts.
* In general, incentive issues of LLM uses have been very critical and challenging. I also like the connection between contract design and the production of LLMs.

Weaknesses:
Although I believe contract design can speak with the production of LLMs, I’m not fully convinced that the proposed model is a good idea to solve the considered problem. 

* In practice, each company pricing its own AIs, so who should be the principal? In other words, the paper assumes there is a trust-worthy third party who can run the quality-detector and commits to a contract with the LLM companies. I’m not sure this is feasible in practice. I hope the authors can explain more carefully the application scenarios of their theory.
* Furthermore, there is no evidence in the paper (and, I guess, on the Internet) that can prove LLM companies are cheating about their service quality. I also don’t think this is very likely because LLM companies have other incentives to provide high-quality services, e.g. their reputation. So, how do we know we are not solving a problem that does not exist?
* Even though we go with the assumption that there is a contract that the company (agent) agrees on, I doubt cost-robustness is the first-order concern. The cost data is usually publicly obtainable from energy reports, as the authors did in their experiments. Even though this data is not public, the energy cost is usually easy to estimate. Therefore, I don’t think incentivizing LLM production is a suitable application for cost-robust contracts.

Limitations:
Limitations are reasonably stated.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors formulate a theoretical setup for a LLM text generation service to incentivize the service to output high quality text the consumer. The authors formulate this setup as having the service having a set of models that has quality (as rated by a evaluator on the end of the consumer) that increases with the cost of running the LLM. The goal is to derive a framework for paying the LLM service based on that quality of the text generated that incentivizes the LLM service to always use its best model. The authors go about this by formulating the definition of a contract in this setting, and defining various metric (max payment, avg. payment, etc.) that consumer aims optimize. They show that the set of contracts that will incentivize the LLM service to output text with the best model can be derived from the set of optimal hypothesis tests that distinguish which model is being used from the evaluator outputs. They derive how the optimal contract can be formulated from these hypothesis tests, when only bounds are known on the costs of running each model for the LLM service.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The theoretical setup and monotone assumption of model performance, cost, etc. is quite reasonable and tackles and interesting and relevant problem with LLM queries. The results are simple and intuitive, and connect nicely with previous work on contract theory and hypothesis testing.

Weaknesses:
The main issue is the theoretical setup does require an assumption that the bounds on cost are known, which seems somewhat impractical. It might be useful to explicitly comment on how the different metrics degrade with increased looseness of the cost bounds (linearly, it seems like), since one can always pick extremely conservative cost bounds.

Minor issue:

- In Definition 3, it would be helpful to illustrate why $B_R^*$ and $B^*_\rho$ are defined as they are, correctly. Further, the definition with $b \geq c_n$ is a bit strange, since the definition of minmax hypothesis test does not involve worst case over cost vectors, so it doesn't seem correct to use $b$ to derive the minimax contract (and instead it should remain a function of $c_n - c_1$) --- maybe dropping that $b \geq c_n$ case would be more accurate, since it is used in the definition of cost-robust later.

Limitations:
I think the limitations are sufficiently addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issue of moral hazard in pay-per-token pricing for large language model (LLM) services. Firms may use cheaper, lower-quality models to cut costs, compromising text quality. Moreover, the firms's costs may be unknown to the clients. To counter this, the authors propose a pay-for-performance framework using cost-robust contracts that incentivize high-quality text generation under the uncertainty about the firms's costs. These contracts are designed based on and have a one-to-one correspondence to optimal composite hypothesis tests. Approximation guarantees are provided with respect to the optimal contracts. Empirical evaluations show that these contracts provide robust incentives with small cost increases.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The results of characterizing the forms of optimal cost-robust contracts using hypothesis testing, as well as the approximation guarantees, are interesting and have valuable contributions.

Weaknesses:
1. The model's complexity is unnecessary. The problem could actually be studied in the most basic contract setting. 
2. The authors do not discuss the computational complexity of finding the optimal cost-robust contract. 
3. The authors do not discuss the cases where the action with the highest cost may not be the best action to incentivize.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
* The paper concerns the problem of incentivizing LLMs to use the most costly model, which is assumed to be the model with the best performance. Without proper incentive, the LLM company has the incentive to charge customers for the highest payment, but deliver the service using a lower-cost model, because the performance of the model is usually not verified. Therefore, the paper proposes to use contract design to solve this problem. In particular, an automatic detector first gives an integer score for the performance of an LLM. Then, the task is to design a payment for the company for each of the integer scores. The goal is to minimize the total payment conditioned on incentivizing the best-performed model. 
* The main contribution is the discussion of the cost-robust contract, meaning how to design the optimal contract while the costs of LLMs are unknown. Empirical evaluations have shown how to use the theory in practical settings given LLM performance data.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
* (I’m not an expert in contract design.) I appreciate the theoretical contributions of the paper. To me, section 4 has several interesting insights into connecting cost-robust contracts with hypothesis tests. As claimed, this is the first paper considering cost-robust contract design. However, the real contribution should be better evaluated by experts.
* In general, incentive issues of LLM uses have been very critical and challenging. I also like the connection between contract design and the production of LLMs.

Weaknesses:
Although I believe contract design can speak with the production of LLMs, I’m not fully convinced that the proposed model is a good idea to solve the considered problem. 

* In practice, each company pricing its own AIs, so who should be the principal? In other words, the paper assumes there is a trust-worthy third party who can run the quality-detector and commits to a contract with the LLM companies. I’m not sure this is feasible in practice. I hope the authors can explain more carefully the application scenarios of their theory.
* Furthermore, there is no evidence in the paper (and, I guess, on the Internet) that can prove LLM companies are cheating about their service quality. I also don’t think this is very likely because LLM companies have other incentives to provide high-quality services, e.g. their reputation. So, how do we know we are not solving a problem that does not exist?
* Even though we go with the assumption that there is a contract that the company (agent) agrees on, I doubt cost-robustness is the first-order concern. The cost data is usually publicly obtainable from energy reports, as the authors did in their experiments. Even though this data is not public, the energy cost is usually easy to estimate. Therefore, I don’t think incentivizing LLM production is a suitable application for cost-robust contracts.

Limitations:
Limitations are reasonably stated.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wWyumwEYV8;"REVIEW 
Summary:
The authors aim to investigate spurious correlations learned by CLIP models. For this, they curate a novel dataset where animals are organized into common and uncommon backgrounds, e.g. a polar bear is more likely encountered in snow than on grass. The authors then perform experiments where they benchmark various CLIP and ImageNet models on the curated dataset. They observe that CLIP models suffer from spurious correlations which stem from changing the background.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
I think the issue of spurious correlations is important and one needs to understand how and whether VLMs learn spurious features. The paper presents many experiments and shows that scale or backbone capacity do not improve the effective robustness on CounterAnimal which is interesting.

Weaknesses:
The paper has many issues, both in terms of writing and the methodology which need to be fixed.

### Major:
**The authors missed important previous works**: The paper “Recognition in Terra Incognita” is very related to this work and also proposes a “dataset designed to measure recognition generalization to novel environments” based on camera traps. The dataset is sorted according to difficult environments for different animals, which makes it very similar to CounterAnimal. I think the authors need to cite and discuss this paper. Currently, I do not understand the benefit of having a new dataset in addition to the already present one. The waterbirds dataset is also highly similar and should be discussed (https://arxiv.org/pdf/1911.08731). The authors cite that paper, but do not discuss it in the Related Work section, nor put it into context with CounterAnimal. The backgrounds challenge (https://github.com/MadryLab/backgrounds_challenge) is also highly related and should be discussed. In general, the related work section is very weak, given how extensively spurious correlations and worst-group-accuracy have been studied. Another important work to be discussed would be ""Finding and Fixing Spurious Patterns with Explanations"" (https://arxiv.org/abs/2106.02112).

**The naming of the common vs counter groups is misleading**: 
Line 165: “Photos with the highest CLIP accuracy are assigned to the common group, and those with the lowest CLIP accuracy are assigned to the counter group.” I have a major understanding issue here. As far as I understood the paper before this line, the goal was to put images with common backgrounds into the common group and images with uncommon backgrounds into the counter group. This is also depicted in Fig. 1 or Table 1. The caption in Fig.1 says that “Most ice bears appear in a snow background (i.e., common), while it also is reasonable to find some ice bears in a grassy environment (i.e., counter)”. But here, the authors write that accuracy has actually been used to separate images into these groups? But then the frequency of the co-occurrence of certain backgrounds and classes has not been taken into account, or rather, it is a conjecture that those backgrounds where the CLIP model has higher accuracy on are more “common”? 

**The terms ""effective robustness"" and ""robustness"" are used interchangeably which is wrong and confusing**:
I think the paper conflates the terms “robustness” and “effective robustness” which is confusing. When looking at effective robustness plots, such as in Fig. 2, we are interested in the residual difference between the measured value and the value predicted by the linear fit. As I can see, all plotted markers (CLIP and ImageNet) lie on their respective linear fits, and none of the interventions, such as CLIP-DC or CLIP-DFN offer any effective robustness benefits. It is though true that the **absolute** robustness numbers are overall higher for the CLIP-DFN models, for larger models or models trained on more data. I am however confused by the authors discussion of this observation. On the one hand, they write that larger CLIP models are more robust but increasing the dataset size does not yield improvements. First, I am confused whether they mean “effective robustness” or “robustness” here. Second, I do not see the effect the authors are describing: Both more data and larger backbones have higher absolute robustness but the same effective robustness as the other models. The statement “CLIP models trained on high-quality data are more robust” is also confusing, because it is not clear whether “robustness” or “effective robustness” is meant. 

**Due to methodology issues, results on CLIP models cannot be compared to results on ImageNet models (or other advanced LVLMs):**
Line 60: “d) Spurious discovering: preserving classes and associated data based on the decrease in zero-shot performance (i.e., evaluating based on pre-trained CLIP models without fine-tuning) when shifting the backgrounds.” This step is really unclear. Do the authors curate the dataset based on the zero-shot accuracy of a CLIP model? From the introduction and the abstract, it sounds like the authors want to benchmark the robustness of CLIP vs ImageNet models on this custom dataset. But then, it is strange that CLIP models also seem to be used during the curation process. After reading the more detailed description in line 156, I think the statements made in line 85 are misleading. The authors write “ImageNet models are more robust to spurious correlations captured by CounterAnimal” and “Compared with CLIP models (colored in blue), surprisingly, we find that ImageNet models exhibit a stronger robustness to the spurious correlations in CounterAnimal.” Given that CounterAnimal has been curated based on the performance drop of a CLIP model, I find it very unsurprising that CLIP models perform worse on it compared to ImageNet models. I think that if CounterAnimal had been curated based on an ImageNet-trained ResNet50, the trend would have been reversed. I think all statements comparing CLIP and ImageNet trained models on CounterAnimal need to be relaxed and I think that this comparison is quite meaningless because of the described selection bias. I think that the whole Section 3.3. is misleading for this reason and statements such as the following cannot be made given the methodology issues: “Surprisingly, we find that ImageNet models are more robust to spurious features in the CounterAnimal dataset. This finding may contradict the common belief [Radford et al., 2021, Shi et al., 2023] that the CLIP models tend to be more robust to spurious correlations than single-modal supervised learning.” Similarly, the conjecture paragraph from line 265 onwards is wrong and cannot be made. 

For the same reason, the comparison to advanced LVLMs in line 273 onwards cannot be made.

Figure 1: Are these examples cherry-picked or are they representative of the data points present in CounterAnimal? I am asking this, because of the Winoground dataset [A]. This dataset tests the compositionality of VLMs by forcing a model to match two captions to two respective images. Winoground has later been criticized because the two images in the choice process are not equally hard [B]. For example, the model needs to match “the glass is on the grass” and “the grass is in the glass” to the corresponding images. However, there is much more grass in the image matching to the first caption, and the model likely picks that image for both captions just because there is more grass and it makes the decision in a bag-of-words-manner. To summarize, Winoground did not control for object size, orientation and other confounders. In Fig.1, it appears that the main objects (the polar bears) are equal in size, so size could be excluded as a possible confounder? Did the authors consider this possibility, i.e. that the drop in performance could be explained by other differences in the images from the respective domains?
[A] https://arxiv.org/abs/2204.03162
[B] https://arxiv.org/abs/2211.00768

### General:
Line 25: please cite CLIP

Line 64: “The resulting dataset covers a total of 45 animal classes, ends up with 7,174 common photos and 5,926 counter photos, aligning with the standard size as an evaluation dataset [Recht et al., 2019, Hendrycks et al., 2021].” -> I do not understand this statement. Different test sets have different numbers of test images. ImageNet Val has 50k images for example. In what sense are the presented numbers standard?


Line 94: “Overall, larger CLIP backbones (i.e., larger markers) can improve the effective robustness, implying that scaling up backbones may enhance robustness against spurious features.” -> I do not see this in Fig. 2. The larger markers appear to be on the fitted line, same as the smaller markers. Effective robustness measures the difference with respect to the linear fit, and there is none for the larger CLIP backbones. Please clarify this point.


Line 146: “feature noise involves severe feature corruptions” -> Please be more specific here. What do you mean with feature noise? Do features refer to animals features such as missing ears or such? Or to the images themselves?

Line 147: “clarity issues arise when animal objects are not in major positions” -> unclear formulation: what is a major position? Do the authors mean that the animals are too small or not in the center of the image?

Line 153: “Note that the class space of backgrounds as above is not entirely orthogonal with each other due to the inherent ambiguity of the real-world situations. Nevertheless, we try our best to discern the assigned background labels within each animal class.” -> This is unclear. How many images would be ambiguous? I could imagine that many images would have two backgrounds, such as e.g. grass and sky or snow and water. For example, the last image in Fig. 1 on the left has both snow and water. It is not clear to me that only picking the snow background and ignoring the water is correct here. Further, at least for CLIP, the caption can contain several background keywords.

Further, I imagine animals occur in all kinds of environments, but there are only two backgrounds for each animal. Were the other images also discarded?

Line 214: “Therefore, we conclude that our CounterAnimal dataset possesses some realistic shifts that are generally contained in large-scale pre-training data, regardless of backbones.” This conclusion cannot be drawn from this experiment since the backbone has not been varied here.

### Section 4:
The proposed experiment is very similar to the well-known ShiftMNIST [D] or ColoredMNIST [E] datasets, which test the influence of spurious correlations. The findings here are not novel and should be brought into perspective with previous work. I do not understand how Fig. 11 relates to the text. What is “supervised”, “obj”, “objbkg”?
[D] https://arxiv.org/pdf/1811.00401
[E] https://arxiv.org/pdf/1907.02893

### Typos, grammar:
The quality of the text is poor on some occasions which makes reading and understanding the paper difficult. The manuscript would benefit from a round of proof-reading. Some statements and formulations should be made more precise.
Line 32: “The performance boosts over ImageNet models seem to suggest that CLIP resolves distribution shifts and thus spark a rich discussion about its rationale.” Strange formulation. How can “distribution shifts be resolved”? Please rephrase for clarity.

Line 112: “More specifically, [Yang et al., 2023] report that CLIP models may misaligned frequently co-occured objects with the corresponding texts.”

Line 115: “[Tong et al., 2024] find that CLIP misaligned samples will further cause the hallucination of LVLMs.” I do not understand this statement, grammar errors.

Line 132: “Meanwhile, many existing datasets, e.g., DomainBed and Wilds, do not have overlapped label space with ImageNet, making the comparison between ImageNet and CLIP models hard.” There is a version of DomainBed [C] where the dataset has been filtered to only include classes compatible with ImageNet, such that an evaluation of ImageNet models is possible out-of-the-box.
[C] https://openreview.net/pdf?id=LiC2vmzbpMO

Line 171: “Recalling that, when CLIP models resort to the shortcut of data, the model performance will heavily correlate with the backgrounds presented in the common group yet is compromised when coming to the counter group.” Grammar errors, I do not understand this sentence. What is “the shortcut of data”?

Line 208: “It suggests that the CounterAnimal dataset captures some general spurious shifts that at least commonly present in the pre-train dataset of LAION400M.” grammar

Line 213: “Here, the spurious features degenerate the zero-shot robustness of CLIP models trained on both LAION2B and by OpenAI.” Typo? “degenerate”?

Line 243: “In Figure 7, we consider two pre-train datasets, namely, LAION2B and the close-soured data from OpenAI” typo

Line 297: “Nevertheless, in the following theorem, we justify that CLIP remains learning to use spurious features, aligned with our experimental observations on the CounterAnimal dataset.” grammar

Strange space break between line 310 and 311.

# Summary of the review:
We could fix the naming convention from ""common"" and ""counter"" to something like ""hard"" and ""easy"" since accuracy has been used rather than frequency of certain backgrounds to classify backgrounds into certain groups. Based on my arguments below, I believe we cannot compare CLIP models to ImageNet models on the proposed dataset in any sensible way due to the introduced selection bias. I believe the very title of the paper is misleading since the posed question cannot be answered based on the methodology issues. But if we remove the claims about comparing ImageNet models and CLIP models, then, the main point of the paper is that there exist backgrounds which are harder for CLIP models, given certain classes, and other backgrounds which are easier. I don't think that this observation is particularly interesting on its own. The authors did not relate the hardness of the backgrounds to their frequency in the pretraining dataset or anything else. The observation that backgrounds matter is also not novel but quite well-known and the authors do not offer a solution. Further, the writing is quite poor and confusing on many occasions; I provided many examples of incorrect and confusing sentences below.

Limitations:
The limitations discuss the comparison in performance of the CLIP vs ImageNet models which I believe cannot be made due to the methodological issues in this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents CounterAnimal, an evaluation dataset featuring two subsets: animals with common backgrounds and those with unusual backgrounds. The images were sourced from iNaturalist. Data with high CLIP accuracy are categorized as ""Common"",  while those with low CLIP accuracy are labeled as ""Counter"".  Results shows that CLIP models experience a greater accuracy drop compared to ImageNet models when tested on this dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper analyzes multiple factors affecting CLIP accuracy, including model size and training data quality.
- The paper combines both experimental results and theoretical analysis. The analysis in Section 5 is interesting and novel.
- The paper is well-written and easy to follow.

Weaknesses:
- The proposed dataset is not sufficiently robust to analyze the influence of spurious bias, as this is not the only difference between the common and counter datasets.
  - To analyze the accuracy drop caused by spurious features such as background, the background should be the only difference between common and counter image pairs. Prior work [4,5] has proposed such datasets focusing on background.
  - In the proposed dataset, other factors may influence the model accuracy gap besides background. For instance, as shown in Figure 1, the more varied gestures of ice bears on the right compared to the left could be a contributing factor to the accuracy drop.

- Current experiments cannot conclusively show that ImageNet models generalize better than CLIP.

   - As the common and counter groups are selected according to the CLIP accuracy (see line 165 in the paper),  they indicate easy and hard samples for CLIP.    Since ImageNet models have different training characteristics, it is natural that hard cases for these models may differ from those for CLIP, resulting in a smaller performance drop for ImageNet models. This result cannot support that ImageNet models are more robust than CLIP models.
   -  The accuracy drop from common to counter group can be greatly influenced by the model used to divide the common and counter dataset. Using the combined proposed common and counter dataset, a new Common' and Counter' dataset can be created based on the accuracy of ImageNet models. What is the impact of this dataset division on the accuracy drop for different models?


- Prior studies[1,2,3,4,5,6] have proposed datasets specifically to analyze the influence of background, which are not discussed in this work.  These datasets can be used for CLIP evaluation as they do not overlap with the CLIP training set. Additionally, creating datasets based on model accuracy in this work is similar to the approach in [6].

[1] Noise or Signal: The Role of Image Backgrounds in Object Recognition.

[2] Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models, NeurIPS 2019.

[3] Dataset Interfaces: Diagnosing Model Failures Using Controllable Counterfactual Generation.

[4] ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing, CVPR 2023.

[5] LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images, NeurIPS 2023.

[6] ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object, CVPR2024.

Limitations:
This paper has discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors create an evaluation dataset comprising two groups, one with animals in usual backgrounds (common group) and another with unusual backgrounds (counter group). They then evaluate a suite of models of different backbones, model sizes, and datasets. They find that CLIP models do poorly than ImageNet-trained models, and generally high quality data or bigger model size improves counter group accuracy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The CounterAnimal dataset is a nice contribution that can be of value to the community.
2. The authors have evaluated a number of models on the dataset and that too could be of value to the community.

Weaknesses:
Please see questions for more information.

Limitations:
The authors have addressed some limitations. For the rest, please see my questions block.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work asks one interesting question: ""Do CLIP models always generalize better than ImageNet models?"" Driven by this question, this work proposes a new benchmark dataset named CounterAnimal. This dataset consists of a) the common group: comprising animals in common backgrounds, and b) the counter group: including animals in plausible yet unusual backgrounds. The main idea is that the performance drops from the common to counter groups quantify the reliance on spurious background features for animal predictions. The main observation is that CLIP models exhibit notable performance drops when tested on the counter group. In comparison, ImageNet models can be more robust than CLIP models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It is always good to see a new and novel dataset proposed for evaluating CLIP and ImageNet-trained models. The proposed dataset CounterAnimal is complementary to existing datasets that cannot reflect the robustness of CLIP models to spurious correlations. 

- The dataset construction is well-presented. The statistics, curation, background labeling, and spurious discovery are well introduced in Section 2

- The analysis around spurious correlation is good. This work tries to give insights from several aspects, such as pre-trained datasets, scaling up, and learning paradigms. The observations are sound to me.

Weaknesses:
-  I found the analysis of why CLIPs rely on spurious features interesting. However, I think the claim is somewhat ""obvious"": there exists a relatively strong correlation between the object captions and the parts of image backgrounds, CLIP will learn to align the backgrounds, i.e., spurious features. If the training dataset contains many examples of spurious correlations, then models will tend to be biased.

- I am curious about why ImageNet models may not be so influenced by the spurious bias in CounterAnimal. Is this because the ImageNet training set does not have too many spurious correlation examples? Or ImageNet has a spurious bias but such bias is different from the one in CounterAnimal? Please provide a discussion or share some insights on this question. 

- This paper adopts absolute performance drop in Section 3.3. Such a metric may not be so robust. For example, model A drops from 40 to 39, and model B drops from 90 to 89. They drop the same but the I would say model B is better. Please comment on this, and discuss the metric of absolute performance drop.

Limitations:
The dataset is proposed, so a discussion on the potential bias/ privacy is needed. I appreciate this work highlights the future improvement of expanding semantic scope, data source, and ImageNet testbed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors aim to investigate spurious correlations learned by CLIP models. For this, they curate a novel dataset where animals are organized into common and uncommon backgrounds, e.g. a polar bear is more likely encountered in snow than on grass. The authors then perform experiments where they benchmark various CLIP and ImageNet models on the curated dataset. They observe that CLIP models suffer from spurious correlations which stem from changing the background.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
I think the issue of spurious correlations is important and one needs to understand how and whether VLMs learn spurious features. The paper presents many experiments and shows that scale or backbone capacity do not improve the effective robustness on CounterAnimal which is interesting.

Weaknesses:
The paper has many issues, both in terms of writing and the methodology which need to be fixed.

### Major:
**The authors missed important previous works**: The paper “Recognition in Terra Incognita” is very related to this work and also proposes a “dataset designed to measure recognition generalization to novel environments” based on camera traps. The dataset is sorted according to difficult environments for different animals, which makes it very similar to CounterAnimal. I think the authors need to cite and discuss this paper. Currently, I do not understand the benefit of having a new dataset in addition to the already present one. The waterbirds dataset is also highly similar and should be discussed (https://arxiv.org/pdf/1911.08731). The authors cite that paper, but do not discuss it in the Related Work section, nor put it into context with CounterAnimal. The backgrounds challenge (https://github.com/MadryLab/backgrounds_challenge) is also highly related and should be discussed. In general, the related work section is very weak, given how extensively spurious correlations and worst-group-accuracy have been studied. Another important work to be discussed would be ""Finding and Fixing Spurious Patterns with Explanations"" (https://arxiv.org/abs/2106.02112).

**The naming of the common vs counter groups is misleading**: 
Line 165: “Photos with the highest CLIP accuracy are assigned to the common group, and those with the lowest CLIP accuracy are assigned to the counter group.” I have a major understanding issue here. As far as I understood the paper before this line, the goal was to put images with common backgrounds into the common group and images with uncommon backgrounds into the counter group. This is also depicted in Fig. 1 or Table 1. The caption in Fig.1 says that “Most ice bears appear in a snow background (i.e., common), while it also is reasonable to find some ice bears in a grassy environment (i.e., counter)”. But here, the authors write that accuracy has actually been used to separate images into these groups? But then the frequency of the co-occurrence of certain backgrounds and classes has not been taken into account, or rather, it is a conjecture that those backgrounds where the CLIP model has higher accuracy on are more “common”? 

**The terms ""effective robustness"" and ""robustness"" are used interchangeably which is wrong and confusing**:
I think the paper conflates the terms “robustness” and “effective robustness” which is confusing. When looking at effective robustness plots, such as in Fig. 2, we are interested in the residual difference between the measured value and the value predicted by the linear fit. As I can see, all plotted markers (CLIP and ImageNet) lie on their respective linear fits, and none of the interventions, such as CLIP-DC or CLIP-DFN offer any effective robustness benefits. It is though true that the **absolute** robustness numbers are overall higher for the CLIP-DFN models, for larger models or models trained on more data. I am however confused by the authors discussion of this observation. On the one hand, they write that larger CLIP models are more robust but increasing the dataset size does not yield improvements. First, I am confused whether they mean “effective robustness” or “robustness” here. Second, I do not see the effect the authors are describing: Both more data and larger backbones have higher absolute robustness but the same effective robustness as the other models. The statement “CLIP models trained on high-quality data are more robust” is also confusing, because it is not clear whether “robustness” or “effective robustness” is meant. 

**Due to methodology issues, results on CLIP models cannot be compared to results on ImageNet models (or other advanced LVLMs):**
Line 60: “d) Spurious discovering: preserving classes and associated data based on the decrease in zero-shot performance (i.e., evaluating based on pre-trained CLIP models without fine-tuning) when shifting the backgrounds.” This step is really unclear. Do the authors curate the dataset based on the zero-shot accuracy of a CLIP model? From the introduction and the abstract, it sounds like the authors want to benchmark the robustness of CLIP vs ImageNet models on this custom dataset. But then, it is strange that CLIP models also seem to be used during the curation process. After reading the more detailed description in line 156, I think the statements made in line 85 are misleading. The authors write “ImageNet models are more robust to spurious correlations captured by CounterAnimal” and “Compared with CLIP models (colored in blue), surprisingly, we find that ImageNet models exhibit a stronger robustness to the spurious correlations in CounterAnimal.” Given that CounterAnimal has been curated based on the performance drop of a CLIP model, I find it very unsurprising that CLIP models perform worse on it compared to ImageNet models. I think that if CounterAnimal had been curated based on an ImageNet-trained ResNet50, the trend would have been reversed. I think all statements comparing CLIP and ImageNet trained models on CounterAnimal need to be relaxed and I think that this comparison is quite meaningless because of the described selection bias. I think that the whole Section 3.3. is misleading for this reason and statements such as the following cannot be made given the methodology issues: “Surprisingly, we find that ImageNet models are more robust to spurious features in the CounterAnimal dataset. This finding may contradict the common belief [Radford et al., 2021, Shi et al., 2023] that the CLIP models tend to be more robust to spurious correlations than single-modal supervised learning.” Similarly, the conjecture paragraph from line 265 onwards is wrong and cannot be made. 

For the same reason, the comparison to advanced LVLMs in line 273 onwards cannot be made.

Figure 1: Are these examples cherry-picked or are they representative of the data points present in CounterAnimal? I am asking this, because of the Winoground dataset [A]. This dataset tests the compositionality of VLMs by forcing a model to match two captions to two respective images. Winoground has later been criticized because the two images in the choice process are not equally hard [B]. For example, the model needs to match “the glass is on the grass” and “the grass is in the glass” to the corresponding images. However, there is much more grass in the image matching to the first caption, and the model likely picks that image for both captions just because there is more grass and it makes the decision in a bag-of-words-manner. To summarize, Winoground did not control for object size, orientation and other confounders. In Fig.1, it appears that the main objects (the polar bears) are equal in size, so size could be excluded as a possible confounder? Did the authors consider this possibility, i.e. that the drop in performance could be explained by other differences in the images from the respective domains?
[A] https://arxiv.org/abs/2204.03162
[B] https://arxiv.org/abs/2211.00768

### General:
Line 25: please cite CLIP

Line 64: “The resulting dataset covers a total of 45 animal classes, ends up with 7,174 common photos and 5,926 counter photos, aligning with the standard size as an evaluation dataset [Recht et al., 2019, Hendrycks et al., 2021].” -> I do not understand this statement. Different test sets have different numbers of test images. ImageNet Val has 50k images for example. In what sense are the presented numbers standard?


Line 94: “Overall, larger CLIP backbones (i.e., larger markers) can improve the effective robustness, implying that scaling up backbones may enhance robustness against spurious features.” -> I do not see this in Fig. 2. The larger markers appear to be on the fitted line, same as the smaller markers. Effective robustness measures the difference with respect to the linear fit, and there is none for the larger CLIP backbones. Please clarify this point.


Line 146: “feature noise involves severe feature corruptions” -> Please be more specific here. What do you mean with feature noise? Do features refer to animals features such as missing ears or such? Or to the images themselves?

Line 147: “clarity issues arise when animal objects are not in major positions” -> unclear formulation: what is a major position? Do the authors mean that the animals are too small or not in the center of the image?

Line 153: “Note that the class space of backgrounds as above is not entirely orthogonal with each other due to the inherent ambiguity of the real-world situations. Nevertheless, we try our best to discern the assigned background labels within each animal class.” -> This is unclear. How many images would be ambiguous? I could imagine that many images would have two backgrounds, such as e.g. grass and sky or snow and water. For example, the last image in Fig. 1 on the left has both snow and water. It is not clear to me that only picking the snow background and ignoring the water is correct here. Further, at least for CLIP, the caption can contain several background keywords.

Further, I imagine animals occur in all kinds of environments, but there are only two backgrounds for each animal. Were the other images also discarded?

Line 214: “Therefore, we conclude that our CounterAnimal dataset possesses some realistic shifts that are generally contained in large-scale pre-training data, regardless of backbones.” This conclusion cannot be drawn from this experiment since the backbone has not been varied here.

### Section 4:
The proposed experiment is very similar to the well-known ShiftMNIST [D] or ColoredMNIST [E] datasets, which test the influence of spurious correlations. The findings here are not novel and should be brought into perspective with previous work. I do not understand how Fig. 11 relates to the text. What is “supervised”, “obj”, “objbkg”?
[D] https://arxiv.org/pdf/1811.00401
[E] https://arxiv.org/pdf/1907.02893

### Typos, grammar:
The quality of the text is poor on some occasions which makes reading and understanding the paper difficult. The manuscript would benefit from a round of proof-reading. Some statements and formulations should be made more precise.
Line 32: “The performance boosts over ImageNet models seem to suggest that CLIP resolves distribution shifts and thus spark a rich discussion about its rationale.” Strange formulation. How can “distribution shifts be resolved”? Please rephrase for clarity.

Line 112: “More specifically, [Yang et al., 2023] report that CLIP models may misaligned frequently co-occured objects with the corresponding texts.”

Line 115: “[Tong et al., 2024] find that CLIP misaligned samples will further cause the hallucination of LVLMs.” I do not understand this statement, grammar errors.

Line 132: “Meanwhile, many existing datasets, e.g., DomainBed and Wilds, do not have overlapped label space with ImageNet, making the comparison between ImageNet and CLIP models hard.” There is a version of DomainBed [C] where the dataset has been filtered to only include classes compatible with ImageNet, such that an evaluation of ImageNet models is possible out-of-the-box.
[C] https://openreview.net/pdf?id=LiC2vmzbpMO

Line 171: “Recalling that, when CLIP models resort to the shortcut of data, the model performance will heavily correlate with the backgrounds presented in the common group yet is compromised when coming to the counter group.” Grammar errors, I do not understand this sentence. What is “the shortcut of data”?

Line 208: “It suggests that the CounterAnimal dataset captures some general spurious shifts that at least commonly present in the pre-train dataset of LAION400M.” grammar

Line 213: “Here, the spurious features degenerate the zero-shot robustness of CLIP models trained on both LAION2B and by OpenAI.” Typo? “degenerate”?

Line 243: “In Figure 7, we consider two pre-train datasets, namely, LAION2B and the close-soured data from OpenAI” typo

Line 297: “Nevertheless, in the following theorem, we justify that CLIP remains learning to use spurious features, aligned with our experimental observations on the CounterAnimal dataset.” grammar

Strange space break between line 310 and 311.

# Summary of the review:
We could fix the naming convention from ""common"" and ""counter"" to something like ""hard"" and ""easy"" since accuracy has been used rather than frequency of certain backgrounds to classify backgrounds into certain groups. Based on my arguments below, I believe we cannot compare CLIP models to ImageNet models on the proposed dataset in any sensible way due to the introduced selection bias. I believe the very title of the paper is misleading since the posed question cannot be answered based on the methodology issues. But if we remove the claims about comparing ImageNet models and CLIP models, then, the main point of the paper is that there exist backgrounds which are harder for CLIP models, given certain classes, and other backgrounds which are easier. I don't think that this observation is particularly interesting on its own. The authors did not relate the hardness of the backgrounds to their frequency in the pretraining dataset or anything else. The observation that backgrounds matter is also not novel but quite well-known and the authors do not offer a solution. Further, the writing is quite poor and confusing on many occasions; I provided many examples of incorrect and confusing sentences below.

Limitations:
The limitations discuss the comparison in performance of the CLIP vs ImageNet models which I believe cannot be made due to the methodological issues in this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents CounterAnimal, an evaluation dataset featuring two subsets: animals with common backgrounds and those with unusual backgrounds. The images were sourced from iNaturalist. Data with high CLIP accuracy are categorized as ""Common"",  while those with low CLIP accuracy are labeled as ""Counter"".  Results shows that CLIP models experience a greater accuracy drop compared to ImageNet models when tested on this dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper analyzes multiple factors affecting CLIP accuracy, including model size and training data quality.
- The paper combines both experimental results and theoretical analysis. The analysis in Section 5 is interesting and novel.
- The paper is well-written and easy to follow.

Weaknesses:
- The proposed dataset is not sufficiently robust to analyze the influence of spurious bias, as this is not the only difference between the common and counter datasets.
  - To analyze the accuracy drop caused by spurious features such as background, the background should be the only difference between common and counter image pairs. Prior work [4,5] has proposed such datasets focusing on background.
  - In the proposed dataset, other factors may influence the model accuracy gap besides background. For instance, as shown in Figure 1, the more varied gestures of ice bears on the right compared to the left could be a contributing factor to the accuracy drop.

- Current experiments cannot conclusively show that ImageNet models generalize better than CLIP.

   - As the common and counter groups are selected according to the CLIP accuracy (see line 165 in the paper),  they indicate easy and hard samples for CLIP.    Since ImageNet models have different training characteristics, it is natural that hard cases for these models may differ from those for CLIP, resulting in a smaller performance drop for ImageNet models. This result cannot support that ImageNet models are more robust than CLIP models.
   -  The accuracy drop from common to counter group can be greatly influenced by the model used to divide the common and counter dataset. Using the combined proposed common and counter dataset, a new Common' and Counter' dataset can be created based on the accuracy of ImageNet models. What is the impact of this dataset division on the accuracy drop for different models?


- Prior studies[1,2,3,4,5,6] have proposed datasets specifically to analyze the influence of background, which are not discussed in this work.  These datasets can be used for CLIP evaluation as they do not overlap with the CLIP training set. Additionally, creating datasets based on model accuracy in this work is similar to the approach in [6].

[1] Noise or Signal: The Role of Image Backgrounds in Object Recognition.

[2] Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models, NeurIPS 2019.

[3] Dataset Interfaces: Diagnosing Model Failures Using Controllable Counterfactual Generation.

[4] ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing, CVPR 2023.

[5] LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images, NeurIPS 2023.

[6] ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object, CVPR2024.

Limitations:
This paper has discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors create an evaluation dataset comprising two groups, one with animals in usual backgrounds (common group) and another with unusual backgrounds (counter group). They then evaluate a suite of models of different backbones, model sizes, and datasets. They find that CLIP models do poorly than ImageNet-trained models, and generally high quality data or bigger model size improves counter group accuracy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The CounterAnimal dataset is a nice contribution that can be of value to the community.
2. The authors have evaluated a number of models on the dataset and that too could be of value to the community.

Weaknesses:
Please see questions for more information.

Limitations:
The authors have addressed some limitations. For the rest, please see my questions block.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work asks one interesting question: ""Do CLIP models always generalize better than ImageNet models?"" Driven by this question, this work proposes a new benchmark dataset named CounterAnimal. This dataset consists of a) the common group: comprising animals in common backgrounds, and b) the counter group: including animals in plausible yet unusual backgrounds. The main idea is that the performance drops from the common to counter groups quantify the reliance on spurious background features for animal predictions. The main observation is that CLIP models exhibit notable performance drops when tested on the counter group. In comparison, ImageNet models can be more robust than CLIP models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It is always good to see a new and novel dataset proposed for evaluating CLIP and ImageNet-trained models. The proposed dataset CounterAnimal is complementary to existing datasets that cannot reflect the robustness of CLIP models to spurious correlations. 

- The dataset construction is well-presented. The statistics, curation, background labeling, and spurious discovery are well introduced in Section 2

- The analysis around spurious correlation is good. This work tries to give insights from several aspects, such as pre-trained datasets, scaling up, and learning paradigms. The observations are sound to me.

Weaknesses:
-  I found the analysis of why CLIPs rely on spurious features interesting. However, I think the claim is somewhat ""obvious"": there exists a relatively strong correlation between the object captions and the parts of image backgrounds, CLIP will learn to align the backgrounds, i.e., spurious features. If the training dataset contains many examples of spurious correlations, then models will tend to be biased.

- I am curious about why ImageNet models may not be so influenced by the spurious bias in CounterAnimal. Is this because the ImageNet training set does not have too many spurious correlation examples? Or ImageNet has a spurious bias but such bias is different from the one in CounterAnimal? Please provide a discussion or share some insights on this question. 

- This paper adopts absolute performance drop in Section 3.3. Such a metric may not be so robust. For example, model A drops from 40 to 39, and model B drops from 90 to 89. They drop the same but the I would say model B is better. Please comment on this, and discuss the metric of absolute performance drop.

Limitations:
The dataset is proposed, so a discussion on the potential bias/ privacy is needed. I appreciate this work highlights the future improvement of expanding semantic scope, data source, and ImageNet testbed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
wQiJNyPENt;"REVIEW 
Summary:
The paper introduces a new approach to batch Bayesian optimization that explicitly trades off between exploration and exploitation via energy and entropy terms. The method is able to efficiently generate large batches for evaluation that outperform comparable methods for Bayesian optimization.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The method is novel and well-motivated. 

I found the analysis in Appendix B to be especially strong, in which the proposed method BEEBO is compared to other methods for batch Bayesian optimization. This analysis shows the originality of the method and gives strong context for it.

The paper is clearly written and easy to read. The appendix was especially useful and had many valuable parts.

The analysis of heteroskedastic noise was great to see and shows a useful and understudied setting where the method is especially valuable.

Weaknesses:
I think the method is interesting and will be of value to the field. However, I do not find that the experimental evaluation of the method provides full support for the claims of the paper.

**Issue 1: Evaluation limited to large-batch setting**

The major issue is that the paper claims the method is for general batch Bayesian optimization problems, without any qualifiers that I can see. The experiments all use q=100, which is a large batch size. Smaller batch sizes are often of interest too, e.g. batch sizes of 5, 10, and 50 in the GIBBON paper. The setting q=100 used here is also used in the TURBO paper (Eriksson et al. 2019) where it is described as a ""large batch size."" 

Given the experiment results in the paper, I don't know if this method will perform well for small- or medium-sized batches. Thus, either the experiments need to be expanded to include experiments with batch sizes such as 5 and 10, or the framing of the paper needs to be adjusted to emphasize that the method is specifically for large-batch Bayesian optimization, not general batch BO problems.

This issue also relates to the choice of baselines. The experiments only explore large-batch settings, where GIBBON (as the paper notes) is known to perform poorly. If the paper wants to claim that it performs better than GIBBON in general, then it needs to make that comparison on batch sizes of 5 and 10. If the paper wants to claim superiority only on large-batch settings, then that's fine to only use q=100, but then it needs to compare to state-of-the-art for large batch. Thompson sampling is a popular method for large-batch settings which is included as a baseline, but to my knowledge the state-of-the-art for large-batch BO is TURBO (Eriksson et al. 2019). In fact the q=100 and the robot pushing and rover trajectory problems are all exactly as in the TURBO paper, so its inclusion as a baseline is pretty obvious and, I think, necessary.

**Issue 2: Lack of statistical significance**

The results of the experiments do not appear to be statistically significant. The main results given in the main text are tables, and these tables do not have confidence intervals. The only place where uncertainty in the results is show are the figures in the appendix, and there the confidence intervals appear to overlap in most cases. This is due to the use of only 5 replicates. I appreciate that these experiments are costly to run since they are using 1000 iterations, nevertheless the lack of statistical significance in most of the results provides weak support for the claim that BEEBO is actually better, vs. what we're seeing just being noise in the experiments. The paper needs some form of statistical analysis to convince the reader that what we're seeing is not just noise in the experiments. The best way to do this would be to include confidence intervals in tables 2 and 3, and then increase the number of replicates as necessary to achieve statistical significance in the differences in means. I do not feel it appropriate to highlight the method as being ""best value"" when it is possible that the confidence interval for that value contains the values of the other methods.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces a batched acquisition function that balances exploration and exploitation by using a weighted sum of mutual information and expected value, with the weights defining the trade-off. The discussion links the proposed algorithm to UCB and asserts that it naturally addresses heteroskedastic noise.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed acquisition function and its optimization and approximation methods are straightforward and practical. 
2. The paper provides extensive empirical results to illustrate the proposed algorithm's efficiency.

Weaknesses:
1. The introduced parameter controlling the trade-off lacks interpretation as in previous methods.
2. The completeness of the related work discussion is concerning. This is potential because the summarization lacks high-level extraction of the design of the algorithm, and the focus of the paper is, to some extent, scattered。

Limitations:
Discussed above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new acquisition function BEBBO for batched BO. BEBBO tries to build (negative) free-energies-like acquisition function, enabling gradient-based optimization, tight exploration-exploitation control, and risk-averse BO under heteroskedastic noise. It tries to improve existing parallel acquisition functions in the following ways:
1.	uses a hyper-parameter T to directly balance exploration and exploitation by separating these two parts clearly;
2.	keeps the behavior predictable by scaling E and I with batch size;
3.	enables the optimization of gradient descent by holding the availability of closed-form expressions for GP.
This paper demonstrates several experimental comparisons and shows its effectiveness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	This paper shows an enlightening acquisition function method inspired by statistical physics. 

2.	The experimental results show the effectiveness of BEBBO on problems without noise or with heteroskedastic noise.

Weaknesses:
1.	The idea is straightforward: simply combine two common components—entropy reduction and the weighted sum of values. In my opinion, the novelty is not very strong. 

2.	The article doesn't discuss the situation when BEEBO is used with other surrogate models. It seems that if BEEBO is not used with GP, it loses the advantages of closed-form expressions and gradient descent optimization. 

3.	In control problems shown in Figure A2, the performances of meanBEBBO and maxBEBBO are not outstanding. Especially, these two variants are surpassed by KB in Robot pushing problem.

4.	Although BEEBO performs well on many synthetic test problems, its versatility and effectiveness require more experimental validation in specific applications.

5.	In the experiments in main text, the authors only showed the comparison with q-UCB. It would be better to show comparisons with other batched baselines and provide a thorough analysis. The comparison with q-UCB shows the advantage on the balance between exploration and exploitation. But other advantages emphasized in the paper, such as the benefits of gradient descent optimization and the tight control of the explore-exploit trade-off, are not fully demonstrated. I suggest that the authors can re-organize the paper to move the comparison with other baselines to the main paper. 

6.	The theoretical analysis is not deep. No regret bound is analyzed.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Proposing a new acquisition function inspired by statistical physics, which allows explicit control of exploration-exploitation trade-offs in a batch BO setting.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Drawing inspiration from statistical physics is a promising direction, as it naturally aligns with Bayesian approaches.

Weaknesses:
**Major Points**
1. **Lack of Unique Selling Point:** 
   The method does not appear to solve any unique cases that other methods cannot. While the related work section outlines many similar approaches, this work only compares itself to q-UCB. Without a theoretical study, such as convergence rate analysis, there is insufficient motivation to adopt another heuristic approach like this. To demonstrate efficacy, a comprehensive empirical study with extensive comparisons to existing works is necessary, given the unclear advantages.

2. **Review of Claimed Selling Points:**
   - **Not Based on MC Integration Like BoTorch:** 
     While this is true, it is unclear if it is beneficial. MC approaches are approximations but have convergence guarantees (refer to the BoTorch paper's appendix). This work lacks such guarantees.
   - **Tight Control of Exploration-Exploitation Trade-off:** 
     The proposed method is not the only solution. UCB theory can bound the feasible region by [max LCB(x), UCB(x)] with 1 - $\delta$ probability (e.g., see [1]). This region can be controlled by $\beta$ hyperparameters, corresponding to $\delta$. Constrained batch BO within this region would yield similar results with theoretical guarantees.
   - **Heteroskedastic BO:** 
     There are no comparisons with existing methods. UCB variants can address this problem. Vanilla UCB theory does not differentiate between epistemic and aleatoric uncertainty. Therefore, UCB with heteroscedastic GP can serve the same purpose. For example, training a heteroscedastic GP with the observed dataset and replacing the inner GP on noise variance with normal isotropic likelihood variance (pure epistemic uncertainty) will yield similar results as risk-averse batch BO, without needing the change in modelling the acquisition functions regardless of hetero-/homo-scedastic cases like this work.

3. **Setting k in Practice:** 
   Setting  $k$ is not an easy task for users. In UCB, $\beta$ presents a similar challenge, but there are theoretical guidelines and algorithm to compute this (e.g., [2]).

4. **Constrained Uncertainty Sampling:** 
   This work can be understood as a variant of constrained uncertainty sampling. As [3] explains, variance reduction can be written similarly to the entropy proposed in this paper (see section 2.2). It also shows that the variance-only approach is inferior to UCB both theoretically and empirically. The batch setting may lead to model misspecification, particularly when hallucination (aka fantasization) is applied. The concerns and approach are notably similar to ([49] in your citation number), making a comparison with their method unavoidable.

5. **Data Augmentation Procedure:** 
   The explanation is unclear. Is it fantasization (aka hallucination) or simply observed points? How does this differ from Eq.(3) in [3]?

- [1] Fengxue Zhang, Jialin Song, James C Bowden, Alexan- der Ladd, Yisong Yue, Thomas Desautels, and Yuxin Chen. Learning regions of interest for bayesian optimiza- tion with adaptive level-set estimation. In International Conference on Machine Learning, pages 41579–41595. PMLR, 2023.
- [2] Kihyuk Hong, Yuhang Li, and Ambuj Tewari. An optimization-based algorithm for non-stationary kernel bandits without prior knowledge. In International Conference on Artificial Intelligence and Statistics, pages 3048–3085. PMLR, 2023.
- [3] Srinivas, N., Krause, A., Kakade, S., & Seeger, M. (2010). Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. In Proceedings of the 27th International Conference on Machine Learning (pp. 1015-1022).

Limitations:
Limitations are discussed in the discussion section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new approach to batch Bayesian optimization that explicitly trades off between exploration and exploitation via energy and entropy terms. The method is able to efficiently generate large batches for evaluation that outperform comparable methods for Bayesian optimization.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The method is novel and well-motivated. 

I found the analysis in Appendix B to be especially strong, in which the proposed method BEEBO is compared to other methods for batch Bayesian optimization. This analysis shows the originality of the method and gives strong context for it.

The paper is clearly written and easy to read. The appendix was especially useful and had many valuable parts.

The analysis of heteroskedastic noise was great to see and shows a useful and understudied setting where the method is especially valuable.

Weaknesses:
I think the method is interesting and will be of value to the field. However, I do not find that the experimental evaluation of the method provides full support for the claims of the paper.

**Issue 1: Evaluation limited to large-batch setting**

The major issue is that the paper claims the method is for general batch Bayesian optimization problems, without any qualifiers that I can see. The experiments all use q=100, which is a large batch size. Smaller batch sizes are often of interest too, e.g. batch sizes of 5, 10, and 50 in the GIBBON paper. The setting q=100 used here is also used in the TURBO paper (Eriksson et al. 2019) where it is described as a ""large batch size."" 

Given the experiment results in the paper, I don't know if this method will perform well for small- or medium-sized batches. Thus, either the experiments need to be expanded to include experiments with batch sizes such as 5 and 10, or the framing of the paper needs to be adjusted to emphasize that the method is specifically for large-batch Bayesian optimization, not general batch BO problems.

This issue also relates to the choice of baselines. The experiments only explore large-batch settings, where GIBBON (as the paper notes) is known to perform poorly. If the paper wants to claim that it performs better than GIBBON in general, then it needs to make that comparison on batch sizes of 5 and 10. If the paper wants to claim superiority only on large-batch settings, then that's fine to only use q=100, but then it needs to compare to state-of-the-art for large batch. Thompson sampling is a popular method for large-batch settings which is included as a baseline, but to my knowledge the state-of-the-art for large-batch BO is TURBO (Eriksson et al. 2019). In fact the q=100 and the robot pushing and rover trajectory problems are all exactly as in the TURBO paper, so its inclusion as a baseline is pretty obvious and, I think, necessary.

**Issue 2: Lack of statistical significance**

The results of the experiments do not appear to be statistically significant. The main results given in the main text are tables, and these tables do not have confidence intervals. The only place where uncertainty in the results is show are the figures in the appendix, and there the confidence intervals appear to overlap in most cases. This is due to the use of only 5 replicates. I appreciate that these experiments are costly to run since they are using 1000 iterations, nevertheless the lack of statistical significance in most of the results provides weak support for the claim that BEEBO is actually better, vs. what we're seeing just being noise in the experiments. The paper needs some form of statistical analysis to convince the reader that what we're seeing is not just noise in the experiments. The best way to do this would be to include confidence intervals in tables 2 and 3, and then increase the number of replicates as necessary to achieve statistical significance in the differences in means. I do not feel it appropriate to highlight the method as being ""best value"" when it is possible that the confidence interval for that value contains the values of the other methods.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces a batched acquisition function that balances exploration and exploitation by using a weighted sum of mutual information and expected value, with the weights defining the trade-off. The discussion links the proposed algorithm to UCB and asserts that it naturally addresses heteroskedastic noise.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed acquisition function and its optimization and approximation methods are straightforward and practical. 
2. The paper provides extensive empirical results to illustrate the proposed algorithm's efficiency.

Weaknesses:
1. The introduced parameter controlling the trade-off lacks interpretation as in previous methods.
2. The completeness of the related work discussion is concerning. This is potential because the summarization lacks high-level extraction of the design of the algorithm, and the focus of the paper is, to some extent, scattered。

Limitations:
Discussed above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new acquisition function BEBBO for batched BO. BEBBO tries to build (negative) free-energies-like acquisition function, enabling gradient-based optimization, tight exploration-exploitation control, and risk-averse BO under heteroskedastic noise. It tries to improve existing parallel acquisition functions in the following ways:
1.	uses a hyper-parameter T to directly balance exploration and exploitation by separating these two parts clearly;
2.	keeps the behavior predictable by scaling E and I with batch size;
3.	enables the optimization of gradient descent by holding the availability of closed-form expressions for GP.
This paper demonstrates several experimental comparisons and shows its effectiveness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	This paper shows an enlightening acquisition function method inspired by statistical physics. 

2.	The experimental results show the effectiveness of BEBBO on problems without noise or with heteroskedastic noise.

Weaknesses:
1.	The idea is straightforward: simply combine two common components—entropy reduction and the weighted sum of values. In my opinion, the novelty is not very strong. 

2.	The article doesn't discuss the situation when BEEBO is used with other surrogate models. It seems that if BEEBO is not used with GP, it loses the advantages of closed-form expressions and gradient descent optimization. 

3.	In control problems shown in Figure A2, the performances of meanBEBBO and maxBEBBO are not outstanding. Especially, these two variants are surpassed by KB in Robot pushing problem.

4.	Although BEEBO performs well on many synthetic test problems, its versatility and effectiveness require more experimental validation in specific applications.

5.	In the experiments in main text, the authors only showed the comparison with q-UCB. It would be better to show comparisons with other batched baselines and provide a thorough analysis. The comparison with q-UCB shows the advantage on the balance between exploration and exploitation. But other advantages emphasized in the paper, such as the benefits of gradient descent optimization and the tight control of the explore-exploit trade-off, are not fully demonstrated. I suggest that the authors can re-organize the paper to move the comparison with other baselines to the main paper. 

6.	The theoretical analysis is not deep. No regret bound is analyzed.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Proposing a new acquisition function inspired by statistical physics, which allows explicit control of exploration-exploitation trade-offs in a batch BO setting.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Drawing inspiration from statistical physics is a promising direction, as it naturally aligns with Bayesian approaches.

Weaknesses:
**Major Points**
1. **Lack of Unique Selling Point:** 
   The method does not appear to solve any unique cases that other methods cannot. While the related work section outlines many similar approaches, this work only compares itself to q-UCB. Without a theoretical study, such as convergence rate analysis, there is insufficient motivation to adopt another heuristic approach like this. To demonstrate efficacy, a comprehensive empirical study with extensive comparisons to existing works is necessary, given the unclear advantages.

2. **Review of Claimed Selling Points:**
   - **Not Based on MC Integration Like BoTorch:** 
     While this is true, it is unclear if it is beneficial. MC approaches are approximations but have convergence guarantees (refer to the BoTorch paper's appendix). This work lacks such guarantees.
   - **Tight Control of Exploration-Exploitation Trade-off:** 
     The proposed method is not the only solution. UCB theory can bound the feasible region by [max LCB(x), UCB(x)] with 1 - $\delta$ probability (e.g., see [1]). This region can be controlled by $\beta$ hyperparameters, corresponding to $\delta$. Constrained batch BO within this region would yield similar results with theoretical guarantees.
   - **Heteroskedastic BO:** 
     There are no comparisons with existing methods. UCB variants can address this problem. Vanilla UCB theory does not differentiate between epistemic and aleatoric uncertainty. Therefore, UCB with heteroscedastic GP can serve the same purpose. For example, training a heteroscedastic GP with the observed dataset and replacing the inner GP on noise variance with normal isotropic likelihood variance (pure epistemic uncertainty) will yield similar results as risk-averse batch BO, without needing the change in modelling the acquisition functions regardless of hetero-/homo-scedastic cases like this work.

3. **Setting k in Practice:** 
   Setting  $k$ is not an easy task for users. In UCB, $\beta$ presents a similar challenge, but there are theoretical guidelines and algorithm to compute this (e.g., [2]).

4. **Constrained Uncertainty Sampling:** 
   This work can be understood as a variant of constrained uncertainty sampling. As [3] explains, variance reduction can be written similarly to the entropy proposed in this paper (see section 2.2). It also shows that the variance-only approach is inferior to UCB both theoretically and empirically. The batch setting may lead to model misspecification, particularly when hallucination (aka fantasization) is applied. The concerns and approach are notably similar to ([49] in your citation number), making a comparison with their method unavoidable.

5. **Data Augmentation Procedure:** 
   The explanation is unclear. Is it fantasization (aka hallucination) or simply observed points? How does this differ from Eq.(3) in [3]?

- [1] Fengxue Zhang, Jialin Song, James C Bowden, Alexan- der Ladd, Yisong Yue, Thomas Desautels, and Yuxin Chen. Learning regions of interest for bayesian optimiza- tion with adaptive level-set estimation. In International Conference on Machine Learning, pages 41579–41595. PMLR, 2023.
- [2] Kihyuk Hong, Yuhang Li, and Ambuj Tewari. An optimization-based algorithm for non-stationary kernel bandits without prior knowledge. In International Conference on Artificial Intelligence and Statistics, pages 3048–3085. PMLR, 2023.
- [3] Srinivas, N., Krause, A., Kakade, S., & Seeger, M. (2010). Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. In Proceedings of the 27th International Conference on Machine Learning (pp. 1015-1022).

Limitations:
Limitations are discussed in the discussion section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
vtRotUd539;"REVIEW 
Summary:
Given the complexity of the process of neural network training, any understanding of robust phenomena that can be identified in the training process has potential value that can guide the design of models and algorithms. Neural Collapse (and its deep counterpart) is one such phenomenon that has been identified and reproduced across multiple model classes and datasets. This work shows that Neural Collapse also occurs for a recursive kernel-based model known as Deep RMF, when trained using an algorithm that is based on projection onto a matrix constructed from an outer products of gradients computed locally at each layer.
Additionally, the authors present experimental results that document neural collapse in these models when trained on standard datasets. They also show that in standard neural networks, the projection of features onto the gradient outer product leads to neural collapse, rather than the effect of the nonlinearity.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written, and presents both theoretical results and some empirical results that complement them, since they apply to datasets that violate the assumptions under which the results hold. They prove that deep Neural Collapse can indeed occur in models beyond standard neural networks trained with gradient descent.
The experimental results (specifically in Appendix D) demonstrate that the projection onto the gradient outer product matrix (AGOP) leads to neural collapse in standard models, motivating the further study of this object.

Weaknesses:
Given that the main results apply both to a non-standard kernel method and a non-standard training algorithm, it is unclear what the implications of the results are for more well-known models and algorithms. If the authors believe that these results have implications of this form, they should be presented more clearly. Algorithms that are not based on backpropagation are interesting both as possible means of explaining learning in biological systems where backpropagation is unrealistic, and in distributed settings where backpropagation may incur a prohibitive communication overhead. However, the motivation of the algorithm used appears to be that it is a simple model that demonstrates certain phenomena that arise in the training of deep networks. 

The authors assume that the gram matrix of the data is full-rank. This requires assuming that the number of datapoints is smaller than or equal to the input dimension (which subsumes assumption 4.1). Standard datasets violate this assumption.

Limitations:
Limitations and societal impacts have been addressed

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies deep neural collapse (DNC) in deep neural networks (DNN) through the prism of the neural feature ansatz (NFA) and deep recursive feature machines (RFM). It is comprised of several results:
- empirical evidence that DNC occurs in deep RFMs,
- a theoretical analysis of DNC in a high-dimensional RFM setting,
- a theoretical analysis of DNC in a kernel learning setting,
- empirical evidence that the mechanisms which lead to DNC in RFMs and traditional DNNs are the same.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
This paper shows that deep neural collapse occurs in a similar way in deep networks and deep recursive feature machines. It thus provides a simplified setting in which to investigate deep neural collapse, which is an important research direction to further our understanding of deep learning. Specifically, it shows that neural collapse can be obtained just by iterating linear regression problems, without backpropagating through a deep network.

Weaknesses:
My main issue with the paper is its writing, which makes it quite difficult to read.
- The notations could be improved in several places throughout the paper (see minor points below).
- I could not follow most of section 4.2, despite being rather familiar with kernel methods and their behavior in high dimensions. 
On a high level, I don't understand how a linear kernel could be the best setting for neural collapse. The text contradicts itself, as it simultaneously state that ""if [$\lambda_k = 0$] [...], collapse will occur in just one layer] , but also that ""this theory offers an explanation for why non-linear activation is needed"". A linear layer can collapse within-class variability but also typically collapses class means together, and thus cannot induce neural collapse (see paragraph below). 
On a technical level, $k_\Phi$ and $\lambda_\Phi$ are referred to before being defined, and I do not understand the roles played by $k$/$\lambda_k$ vs $k_\Phi$/$\lambda_\Phi$. Assumption 4.2 is also referred to before being stated.
- Section 4.3 is also slightly difficult to read. 
I took me several tries to guess that $k_M(x,x') = \tilde k_M(x,x')\mathrm{Id}$, which should appear in the paper. The terms ""input-level"" and ""output-dimension level"" kernels should be introduced for non-specialists in multi-task kernel learning.
I also do not understand the point of introducing $M$ if it is dropped afterwards. Theorem 4.4 could simply be stated as ""the optimal feature map for ridge regression is the one which already predicts the label: $\Phi(x) = y$"". This result is not very surprising, and is not very integrated in the paper. I suppose that it is some kernel-level equivalent of the unstructured feature model, and suggests that weight decay might be instrumental in bringing about neural collapse? The normalization of $k$ should be restated in the definition of Problem 3 (otherwise the optimal loss is obtained when $k \to 0$).
- The message of section 5 could be presented more clearly. What I understood was that it argues that RFMs and DNNs achieve neural collapse through the same means. I suggest making this point before introducing RFMs (in particular, stating the NFA correlations). I also did not understand why this mechanism is referred to as ""denoising"". 

My second issue is that I was not convinced by the claim that it is the right singular vectors and singular values which lead to neural collapse. By the same logic as lines 309-315, the right singular vectors do not change the DNC1 metric (with a ""full"" SVD where $U$ and $V$ are invertible). Similarly, if I were to divide operations in the network as $V^T\sigma$ and $US$ as opposed to $\sigma U$ and $SV^T$, I should see that it is now $US$ which is responsible for neural collapse (again with a full SVD). This conclusion also depends on the chosen metric for evaluating collapse. Why do the authors consider the ratios of traces of between- and within-class covariances, rather than the trace of their ratio (the Fisher linear discriminant)? It seems that it would reverse one of the conclusions of the analysis, since the trace of the Fisher discriminant ratio $\mathrm{tr}(\Sigma_W^{-1} \Sigma_B)$ is invariant to invertible linear transformations, and decreases under non-invertible linear transformations, so can only be improved through the non-linearity. If the conclusion of which part of the network is responsible for DNC depends strongly on the chosen metric, can we really ascribe meaning to the question? It seems to me that it is really the sequence of weights and non-linearity which _together_ induce DNC, and trying to separate their effects is not really possible.

Finally, Proposition A.1 was first published by Cho and Saul in _Kernel Methods for Deep Learning_, NIPS 2009. Besides, the expression of the kernel in eq. (5) can be simplified with algebra and trigonometry (compare with their eq. (6)).

Minor notes and suggestions:
- I suggest using a so-called ""diverging"" colormap (such as ""bwr"") in Figure 1 to clearly separate positive from negative correlations, and use the same range for both datasets.
- I suggest replacing ""Gram matrix"" with ""(uncentered) covariance"" to refer to $W^TW$, as weight matrices $W$ are generally decomposed in rows which correspond to individual neurons.
- The notation $||X||$ to refer to the vector in $\mathbb R^N$ of column norms of a matrix $X \in \mathbb R^{d\times N}$ is never introduced (and clashes with the usual convention that this is a matrix norm).
- Why is the last layer denoted $W_{L+1}$ instead of $m_{L+1}$?
- The choice of layer-indexing is confusing and seems inconsistent throughout the paper. Contrarily to what is stated in section 3.1, isn't $X_l$ the features after $l-1$ network layers? I suggest to denote the input as $X_0$ instead of $X_1$ to simplify the notations. Also, it seems that $M_l^{1/2} X_l$ should be referred to as $\tilde X_{l+1}$ rather than $\tilde X_l$ given the chosen conventions.
- Typo: missing a norm in the definition of $\bar H_l$ line 128.
-In section 4.2, I suggest defining activations before the kernels, e.g., $\tilde X_{l+1} = \kappa^{-1/2} M_l^{1/2} X_l$ and $X_{l+1} = \Phi_{\rm lin}(\tilde X_{l+1})$. I also suggest choosing a different notation for $k_{\rm lin}$ and $\Phi_{\rm lin}$ which are confusing as they imply linearity, and to avoid the awkward ""non-linear feature map $\Phi_{\rm lin}$"".
- Typo line 260: the output space of $k_M$ should be $\mathcal R^{C\times C}$.
- I suppose that $\lambda = \mu$ in section 4.3.
- In the caption of Figure 2, I suppose that ""fully-connected"" should be removed in the case of ResNet.

Limitations:
See weaknesses.

In its current state, I think that the paper is slightly below the acceptance bar and would require minor, if not major, changes before it can be fully appreciated by the NeurIPS community. I would be happy to raise my score if the authors address the points raised above.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The submission introduces a mechanism for Deep Neural Collapse (DNC) using the average gradient outer product (AGOP). The authors also propose the Deep Recursive Feature Machines (Deep RFM) model, which employs AGOP in its architecture to empirically and theoretically demonstrate DNC. The main contribution is that AGOP-based explanation is a data-based approach while prior work focused on data-agnostic explanations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* Using a data-based approach based on AGOP to explain DNC is novel to the best of my knowledge
* The paper offers both theoretical analysis and empirical evidence supporting the role of AGOP in inducing DNC
* The experiments are performed on different architectures and datasets

Weaknesses:
*  I found the paper challenging to read
* I am unsure about the practical implications of this work

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study two effects associated with neural collapse: the within class variability going to zero and the orthogonality/tight-frame of the class means. They study the deep recursive feature machine model, and show that neural collapse forms in that setting as well, due to the projection of the data onto the average-gradient outer product (AGOP). They show both empirical and theoretical results on this phenomenon, leveraging high-dimensional gaussian equivalence of nonlinear random feature models. Further, they show that the right singular vectors of the weight matrices are responsible for most the within-class variability collapse, projecting onto the subspace spanned by the gradients.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The writing of the paper is for the most part quite readable. The literature review is thorough and puts the results of this paper in a good context. The empirics are extensive and compelling. Moreover, the theoretical ideas leveraging the equivalence of nonlinear random features to a linear kernel with additional identity term make for a compelling argument about the mechanism for neural collapse in RFMs. Given the good mix of theory and experiment, I recommend this paper for acceptance.

Weaknesses:
Section 4 is doing many things at the same time. It may be better to split it into an empirical evidence section, and then do a section on the theoretical results. In particular, it would be good to give an idea of where the theoretical ideas are going at the start of 4.2 before setting out to prove the deep neural collapse results. This would substantially improve the readability of this section. 

This goes double for section 4.3. The opening paragraph of that section is unreadable:

*""Next, we show that the formation of the neural collapse is not only implicitly given by the specific
optimization procedure of the Deep RFM, but is also implicitly regularized for in the parametrized
kernel ridge regression, a model class that includes RFM (i.e., a single layer of Deep RFM)""*

I don't really understand what this is saying, or even what you're trying to accomplish in the entire subsection. I tried many times to read it. The whole subsection should be rewritten. There are many sentences there that make no sense to me. Here is another one:

*""Since we do not explicitly regularize M, we will drop the dependence on it, treat k as
 a free optimization variable and compute the optimal value of the following relaxed problem:""*

This is certainly not something you can do generally. For example if I had a matrix parameterized as $A = M_1 M_2$ and optimized just the $M_i$ with no explicit regularization, there are many cases where this isn't the same as optimizing $A$. Maybe you mean to say something else but once again I can't understand what you're trying to say. The prior subsection was compelling enough that I am discounting this rather poor form of writing. Please rewrite this section. 

More generally, there are many sentences throughout the paper that are not well-worded and seem to run on. Improving the writing would benefit the quality, precision, and reach of this otherwise strong paper. If in your rebuttal you can provide examples of improved presentation, I may raise my score higher.

Limitations:
This work elucidates an important phenomenon in deep learning theory. Developing a principled understanding of feature learning is likely to have implications for the interpretability and reliability of AI systems.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Given the complexity of the process of neural network training, any understanding of robust phenomena that can be identified in the training process has potential value that can guide the design of models and algorithms. Neural Collapse (and its deep counterpart) is one such phenomenon that has been identified and reproduced across multiple model classes and datasets. This work shows that Neural Collapse also occurs for a recursive kernel-based model known as Deep RMF, when trained using an algorithm that is based on projection onto a matrix constructed from an outer products of gradients computed locally at each layer.
Additionally, the authors present experimental results that document neural collapse in these models when trained on standard datasets. They also show that in standard neural networks, the projection of features onto the gradient outer product leads to neural collapse, rather than the effect of the nonlinearity.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written, and presents both theoretical results and some empirical results that complement them, since they apply to datasets that violate the assumptions under which the results hold. They prove that deep Neural Collapse can indeed occur in models beyond standard neural networks trained with gradient descent.
The experimental results (specifically in Appendix D) demonstrate that the projection onto the gradient outer product matrix (AGOP) leads to neural collapse in standard models, motivating the further study of this object.

Weaknesses:
Given that the main results apply both to a non-standard kernel method and a non-standard training algorithm, it is unclear what the implications of the results are for more well-known models and algorithms. If the authors believe that these results have implications of this form, they should be presented more clearly. Algorithms that are not based on backpropagation are interesting both as possible means of explaining learning in biological systems where backpropagation is unrealistic, and in distributed settings where backpropagation may incur a prohibitive communication overhead. However, the motivation of the algorithm used appears to be that it is a simple model that demonstrates certain phenomena that arise in the training of deep networks. 

The authors assume that the gram matrix of the data is full-rank. This requires assuming that the number of datapoints is smaller than or equal to the input dimension (which subsumes assumption 4.1). Standard datasets violate this assumption.

Limitations:
Limitations and societal impacts have been addressed

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies deep neural collapse (DNC) in deep neural networks (DNN) through the prism of the neural feature ansatz (NFA) and deep recursive feature machines (RFM). It is comprised of several results:
- empirical evidence that DNC occurs in deep RFMs,
- a theoretical analysis of DNC in a high-dimensional RFM setting,
- a theoretical analysis of DNC in a kernel learning setting,
- empirical evidence that the mechanisms which lead to DNC in RFMs and traditional DNNs are the same.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
This paper shows that deep neural collapse occurs in a similar way in deep networks and deep recursive feature machines. It thus provides a simplified setting in which to investigate deep neural collapse, which is an important research direction to further our understanding of deep learning. Specifically, it shows that neural collapse can be obtained just by iterating linear regression problems, without backpropagating through a deep network.

Weaknesses:
My main issue with the paper is its writing, which makes it quite difficult to read.
- The notations could be improved in several places throughout the paper (see minor points below).
- I could not follow most of section 4.2, despite being rather familiar with kernel methods and their behavior in high dimensions. 
On a high level, I don't understand how a linear kernel could be the best setting for neural collapse. The text contradicts itself, as it simultaneously state that ""if [$\lambda_k = 0$] [...], collapse will occur in just one layer] , but also that ""this theory offers an explanation for why non-linear activation is needed"". A linear layer can collapse within-class variability but also typically collapses class means together, and thus cannot induce neural collapse (see paragraph below). 
On a technical level, $k_\Phi$ and $\lambda_\Phi$ are referred to before being defined, and I do not understand the roles played by $k$/$\lambda_k$ vs $k_\Phi$/$\lambda_\Phi$. Assumption 4.2 is also referred to before being stated.
- Section 4.3 is also slightly difficult to read. 
I took me several tries to guess that $k_M(x,x') = \tilde k_M(x,x')\mathrm{Id}$, which should appear in the paper. The terms ""input-level"" and ""output-dimension level"" kernels should be introduced for non-specialists in multi-task kernel learning.
I also do not understand the point of introducing $M$ if it is dropped afterwards. Theorem 4.4 could simply be stated as ""the optimal feature map for ridge regression is the one which already predicts the label: $\Phi(x) = y$"". This result is not very surprising, and is not very integrated in the paper. I suppose that it is some kernel-level equivalent of the unstructured feature model, and suggests that weight decay might be instrumental in bringing about neural collapse? The normalization of $k$ should be restated in the definition of Problem 3 (otherwise the optimal loss is obtained when $k \to 0$).
- The message of section 5 could be presented more clearly. What I understood was that it argues that RFMs and DNNs achieve neural collapse through the same means. I suggest making this point before introducing RFMs (in particular, stating the NFA correlations). I also did not understand why this mechanism is referred to as ""denoising"". 

My second issue is that I was not convinced by the claim that it is the right singular vectors and singular values which lead to neural collapse. By the same logic as lines 309-315, the right singular vectors do not change the DNC1 metric (with a ""full"" SVD where $U$ and $V$ are invertible). Similarly, if I were to divide operations in the network as $V^T\sigma$ and $US$ as opposed to $\sigma U$ and $SV^T$, I should see that it is now $US$ which is responsible for neural collapse (again with a full SVD). This conclusion also depends on the chosen metric for evaluating collapse. Why do the authors consider the ratios of traces of between- and within-class covariances, rather than the trace of their ratio (the Fisher linear discriminant)? It seems that it would reverse one of the conclusions of the analysis, since the trace of the Fisher discriminant ratio $\mathrm{tr}(\Sigma_W^{-1} \Sigma_B)$ is invariant to invertible linear transformations, and decreases under non-invertible linear transformations, so can only be improved through the non-linearity. If the conclusion of which part of the network is responsible for DNC depends strongly on the chosen metric, can we really ascribe meaning to the question? It seems to me that it is really the sequence of weights and non-linearity which _together_ induce DNC, and trying to separate their effects is not really possible.

Finally, Proposition A.1 was first published by Cho and Saul in _Kernel Methods for Deep Learning_, NIPS 2009. Besides, the expression of the kernel in eq. (5) can be simplified with algebra and trigonometry (compare with their eq. (6)).

Minor notes and suggestions:
- I suggest using a so-called ""diverging"" colormap (such as ""bwr"") in Figure 1 to clearly separate positive from negative correlations, and use the same range for both datasets.
- I suggest replacing ""Gram matrix"" with ""(uncentered) covariance"" to refer to $W^TW$, as weight matrices $W$ are generally decomposed in rows which correspond to individual neurons.
- The notation $||X||$ to refer to the vector in $\mathbb R^N$ of column norms of a matrix $X \in \mathbb R^{d\times N}$ is never introduced (and clashes with the usual convention that this is a matrix norm).
- Why is the last layer denoted $W_{L+1}$ instead of $m_{L+1}$?
- The choice of layer-indexing is confusing and seems inconsistent throughout the paper. Contrarily to what is stated in section 3.1, isn't $X_l$ the features after $l-1$ network layers? I suggest to denote the input as $X_0$ instead of $X_1$ to simplify the notations. Also, it seems that $M_l^{1/2} X_l$ should be referred to as $\tilde X_{l+1}$ rather than $\tilde X_l$ given the chosen conventions.
- Typo: missing a norm in the definition of $\bar H_l$ line 128.
-In section 4.2, I suggest defining activations before the kernels, e.g., $\tilde X_{l+1} = \kappa^{-1/2} M_l^{1/2} X_l$ and $X_{l+1} = \Phi_{\rm lin}(\tilde X_{l+1})$. I also suggest choosing a different notation for $k_{\rm lin}$ and $\Phi_{\rm lin}$ which are confusing as they imply linearity, and to avoid the awkward ""non-linear feature map $\Phi_{\rm lin}$"".
- Typo line 260: the output space of $k_M$ should be $\mathcal R^{C\times C}$.
- I suppose that $\lambda = \mu$ in section 4.3.
- In the caption of Figure 2, I suppose that ""fully-connected"" should be removed in the case of ResNet.

Limitations:
See weaknesses.

In its current state, I think that the paper is slightly below the acceptance bar and would require minor, if not major, changes before it can be fully appreciated by the NeurIPS community. I would be happy to raise my score if the authors address the points raised above.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The submission introduces a mechanism for Deep Neural Collapse (DNC) using the average gradient outer product (AGOP). The authors also propose the Deep Recursive Feature Machines (Deep RFM) model, which employs AGOP in its architecture to empirically and theoretically demonstrate DNC. The main contribution is that AGOP-based explanation is a data-based approach while prior work focused on data-agnostic explanations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* Using a data-based approach based on AGOP to explain DNC is novel to the best of my knowledge
* The paper offers both theoretical analysis and empirical evidence supporting the role of AGOP in inducing DNC
* The experiments are performed on different architectures and datasets

Weaknesses:
*  I found the paper challenging to read
* I am unsure about the practical implications of this work

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study two effects associated with neural collapse: the within class variability going to zero and the orthogonality/tight-frame of the class means. They study the deep recursive feature machine model, and show that neural collapse forms in that setting as well, due to the projection of the data onto the average-gradient outer product (AGOP). They show both empirical and theoretical results on this phenomenon, leveraging high-dimensional gaussian equivalence of nonlinear random feature models. Further, they show that the right singular vectors of the weight matrices are responsible for most the within-class variability collapse, projecting onto the subspace spanned by the gradients.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The writing of the paper is for the most part quite readable. The literature review is thorough and puts the results of this paper in a good context. The empirics are extensive and compelling. Moreover, the theoretical ideas leveraging the equivalence of nonlinear random features to a linear kernel with additional identity term make for a compelling argument about the mechanism for neural collapse in RFMs. Given the good mix of theory and experiment, I recommend this paper for acceptance.

Weaknesses:
Section 4 is doing many things at the same time. It may be better to split it into an empirical evidence section, and then do a section on the theoretical results. In particular, it would be good to give an idea of where the theoretical ideas are going at the start of 4.2 before setting out to prove the deep neural collapse results. This would substantially improve the readability of this section. 

This goes double for section 4.3. The opening paragraph of that section is unreadable:

*""Next, we show that the formation of the neural collapse is not only implicitly given by the specific
optimization procedure of the Deep RFM, but is also implicitly regularized for in the parametrized
kernel ridge regression, a model class that includes RFM (i.e., a single layer of Deep RFM)""*

I don't really understand what this is saying, or even what you're trying to accomplish in the entire subsection. I tried many times to read it. The whole subsection should be rewritten. There are many sentences there that make no sense to me. Here is another one:

*""Since we do not explicitly regularize M, we will drop the dependence on it, treat k as
 a free optimization variable and compute the optimal value of the following relaxed problem:""*

This is certainly not something you can do generally. For example if I had a matrix parameterized as $A = M_1 M_2$ and optimized just the $M_i$ with no explicit regularization, there are many cases where this isn't the same as optimizing $A$. Maybe you mean to say something else but once again I can't understand what you're trying to say. The prior subsection was compelling enough that I am discounting this rather poor form of writing. Please rewrite this section. 

More generally, there are many sentences throughout the paper that are not well-worded and seem to run on. Improving the writing would benefit the quality, precision, and reach of this otherwise strong paper. If in your rebuttal you can provide examples of improved presentation, I may raise my score higher.

Limitations:
This work elucidates an important phenomenon in deep learning theory. Developing a principled understanding of feature learning is likely to have implications for the interpretability and reliability of AI systems.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wTIzpqX121;"REVIEW 
Summary:
This work introduces a VAE variant of GraphCast for global medium-range weather forecasting and a VAE variant of a UNet (that is formulated as a GNN) for limited area modeling over Scandinavia. For this, they adapt GraphCast to have a similar hierarchical structure to UNets, and then treat the coarsest hierarchical layer (the bottleneck) as a latent variable representing the mean of isotropic Gaussians. The ensemble predictions from the model are similarly fast as a single deterministic prediction, achieved through batching. Their calibration can be good for some variables (e.g. global t2m for 10 day lead time has a spread/skill ratio of 0.99), while poorer for others (e.g. local wvint has a spread/skill ratio of 0.57 for 24h lead time).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed VAE extension to GraphCast is significantly faster than diffusion-based approaches (like the GenCast model).
2. The work does not limit itself to just global weather forecasting, but also presents results for limited area modeling, which is the class of models used by many national weather services.
3. The paper is reasonably well written, keeping a good amount of detail in the main paper, and presenting many additional details in the appendix.

Weaknesses:
Major points:
1. Questionable baselines: I am unsure if the chosen baselines are very strong, let me name a few reasons for this:
    - Tab 1 presents performance for GraphCast, e.g. RMSE=387 for z500, 5 day leadtime. However, if I check the headline scores in the WeatherBench 2 (https://sites.research.google/weatherbench/) for GraphCast, i see RMSE=274 for z500, 5 day leadtime, which is significantly higher and beats all models presented in this study.
    - Both Tab 1 and Tab 2 do not include scores for the conventional weather models. I would expect Tab 1 to include IFS & IFS-ENS scores and Tab 2 to include MEPS scores.
    - Since this work introduces a probabilistic weather model, i would expect comparison with other recent works on probabilistic weather models, like the ones cited in this paper (e.g. GenCast).
    - Graph-FM has almost 6x the parameters compared to GraphCast* (Tab 5) - which quite possibly could be the major reason for its improved performance, and not the introduced architectural feature of hierarchical layers.
2. Overlooked connection to UNets: The Graph-FM that was introduced for the LAM setting looks to me as equivalent to a UNet:
    - The input data comes on a regular grid with 236 x 268 pixels. Which is subsequently downsampled using 3x3 windows. Processing at each depth level is done with a locally connected layer (in other words: a local convolutional filter). A semantically simpler description of such a model would be a UNet with 3x3 pooling (learned in this case) and 3x3 conv filters at each stage. Possibly, implementing it as a UNet could also be computationally advantageous, making use of the highly optimized kernels for 2d convolutions and pooling operations, instead of GNN layers that rely on scatter sums.
    - UNets have been previously used for Weather forecasting: e.g. https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2018GL080704 & https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020MS002203
3. Proposed VAE implementation physically not meaningful? Your VAE is implemented with a latent variable at the coarsest level. This is supposed to capture epistemic uncertainty related to the forward model (and not due to initial state). However, one may argue for atmospheric models most model uncertainty comes from the subgrid-scale parametrizations and not from the coarse-scale representation of atmospheric dynamics. Hence, to me it seems far more intuitive to introduce the stochasticity at the finest level, representing the small scales. I assume you chose the hierarchical description mostly for computational reasons, but given a lack of physical basis, i would at least expect a more thorough investigation of potential errors introduced by this, e.g. is the ensemble variability too smooth?
4. Missing reference to previously published work? A workshop paper at last years NeurIPS has introduced both the hierarchical GNN and the MEPS dataset https://arxiv.org/abs/2309.17370 , if I am not misstaken. I am not really sure about NeurIPS policy here, but even if this work is a direct extension of the previous work and the previous work is to be considered as non-archival, I still believe you should at least cite the workshop paper.

Minor points:
1. GraphCast + SWAG: This is a baseline with poor performance, that is somewhat arbitrarily picked from many possible approaches to obtain ensemble predictions from neural networks. I see two options here: Either you keep it, but also introduce many other such baselines, to make clear that you did not cherrypick a particularly weak one. Other approaches  that should not be prohibitively expensive to run could e.g. be MC-Dropout or Laplace Approximation. Or, you simply drop it, as is, it does not  add much to the paper.
2. Introduction lacks motivation for LAM: This is an ML conference that you are submitting to. It would probably be good to briefly motivate why doing LAM is even necessary (i.e., why can't we just rely on global models instead)?
3. Extreme Weather evaluation / case study: One key reason for ensemble prediction is capturing the tails, i.e. the extremes. You state in Appendix A that this is out-of-scope for the work. I would argue you are making your life too easy here. Since the presented models are likely not useful unless they display decent performance also for extreme weather, it would be important to evaluate just that. It may be enough for this paper to e.g. study a single extreme event as a case study.

Limitations:
The section on limitations is somewhat short. I believe the key limitation of this work is in the evaluation, i.e. it is unclear to me after reading the work how well the models perform. E.g., are these models robust for longer rollouts or if applied at prediction time further away from training time?
Moreover, the work does transparently communicate a limitation of their multi-scale Graph-EFM: visual artifacts. But, this is strange to me, as it could mean two things: a) the VAE formulation and multiscale edges simply don't work well together or b) since GraphCast did not observe such issues, the presented Graph-EFM (ms) is simply not trained sufficiently or suffers from a buggy implementation. While a) would be an interesting finding, i believe b) can not be ruled out given the presented results in this paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new method for predicting weather using advanced deep learning models. The approach, called Graph-EFM, improves accuracy and better handles uncertainties in weather forecasts. It uses a 1.5 degree version of ERA5 and making weather predictions more reliable and useful for real-world applications.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper's strengths include the innovative use of Graph-EFM for accurate probabilistic weather forecasting, detailed experiments on large datasets, and clear presentation of methods. Graph-EFM significantly enhances uncertainty estimation and forecast reliability adding value to both research and practical weather prediction applications.

Weaknesses:
What happened if 0.25 degree ERA5 is used?

Limitations:
Not many, this is a fantastic piece of work

Rating:
10: strong accept, should be highlighted at the conference

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors propose a graph-based ensemble forecasting model (Graph-EFM) to provide weather prediction with a hirearchical GNN framework. They used a hierarchical mesh graph to handle the challenges of capturing processes unfolding over different spatial scales and modeling the uncertainty in the chaotic system. The Graph-EFM provides a probabilistic weather forecasting with the benefit of capturing forecast uncertainity. The experiment results show the effectiveness and advantages of Graph-EFM compared to other deterministic models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The hierarchical mesh graph provides a reasonable idea to handle different spatial scales for weather forecasting, which could inspire other researchers to handle problems in different domains.
The spatial dependencies are considered and handled within GNN layers.
Using ensumble-based model could capture the uncetainty of weather system.

Weaknesses:
In Figure 3, it seems like the selected ensemble members vary a lot, and how close is your forecast to the ground truth. Possibly, explaining a little bit of the underlying meaning of the measures in table 1 & 2 in the paper.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes Graph-EFM, a method that combines a hierarchical multi-scale graph neural network with a variational objective for probabilistic weather forecasting. The method performs on par with Graphcast on deterministic metrics with the extra benefit of uncertainty estimation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The paper tackles probabilistic weather forecasting, which is an important problem in the field.
- The proposed method is intuitive and makes sense. Overall, generative modeling is a potential direction for probabilistic weather forecasting. People have used GANs and diffusion, so a latent variable model is a natural addition to the literature.
- The performance looks promising, and it is more efficient than existing methods using diffusion.

Weaknesses:
- The authors should replace Table 1 with a line graph figure instead, as it allows comparison across different variables and lead times.
- Please see my questions below.

Limitations:
See above.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work introduces a VAE variant of GraphCast for global medium-range weather forecasting and a VAE variant of a UNet (that is formulated as a GNN) for limited area modeling over Scandinavia. For this, they adapt GraphCast to have a similar hierarchical structure to UNets, and then treat the coarsest hierarchical layer (the bottleneck) as a latent variable representing the mean of isotropic Gaussians. The ensemble predictions from the model are similarly fast as a single deterministic prediction, achieved through batching. Their calibration can be good for some variables (e.g. global t2m for 10 day lead time has a spread/skill ratio of 0.99), while poorer for others (e.g. local wvint has a spread/skill ratio of 0.57 for 24h lead time).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed VAE extension to GraphCast is significantly faster than diffusion-based approaches (like the GenCast model).
2. The work does not limit itself to just global weather forecasting, but also presents results for limited area modeling, which is the class of models used by many national weather services.
3. The paper is reasonably well written, keeping a good amount of detail in the main paper, and presenting many additional details in the appendix.

Weaknesses:
Major points:
1. Questionable baselines: I am unsure if the chosen baselines are very strong, let me name a few reasons for this:
    - Tab 1 presents performance for GraphCast, e.g. RMSE=387 for z500, 5 day leadtime. However, if I check the headline scores in the WeatherBench 2 (https://sites.research.google/weatherbench/) for GraphCast, i see RMSE=274 for z500, 5 day leadtime, which is significantly higher and beats all models presented in this study.
    - Both Tab 1 and Tab 2 do not include scores for the conventional weather models. I would expect Tab 1 to include IFS & IFS-ENS scores and Tab 2 to include MEPS scores.
    - Since this work introduces a probabilistic weather model, i would expect comparison with other recent works on probabilistic weather models, like the ones cited in this paper (e.g. GenCast).
    - Graph-FM has almost 6x the parameters compared to GraphCast* (Tab 5) - which quite possibly could be the major reason for its improved performance, and not the introduced architectural feature of hierarchical layers.
2. Overlooked connection to UNets: The Graph-FM that was introduced for the LAM setting looks to me as equivalent to a UNet:
    - The input data comes on a regular grid with 236 x 268 pixels. Which is subsequently downsampled using 3x3 windows. Processing at each depth level is done with a locally connected layer (in other words: a local convolutional filter). A semantically simpler description of such a model would be a UNet with 3x3 pooling (learned in this case) and 3x3 conv filters at each stage. Possibly, implementing it as a UNet could also be computationally advantageous, making use of the highly optimized kernels for 2d convolutions and pooling operations, instead of GNN layers that rely on scatter sums.
    - UNets have been previously used for Weather forecasting: e.g. https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2018GL080704 & https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020MS002203
3. Proposed VAE implementation physically not meaningful? Your VAE is implemented with a latent variable at the coarsest level. This is supposed to capture epistemic uncertainty related to the forward model (and not due to initial state). However, one may argue for atmospheric models most model uncertainty comes from the subgrid-scale parametrizations and not from the coarse-scale representation of atmospheric dynamics. Hence, to me it seems far more intuitive to introduce the stochasticity at the finest level, representing the small scales. I assume you chose the hierarchical description mostly for computational reasons, but given a lack of physical basis, i would at least expect a more thorough investigation of potential errors introduced by this, e.g. is the ensemble variability too smooth?
4. Missing reference to previously published work? A workshop paper at last years NeurIPS has introduced both the hierarchical GNN and the MEPS dataset https://arxiv.org/abs/2309.17370 , if I am not misstaken. I am not really sure about NeurIPS policy here, but even if this work is a direct extension of the previous work and the previous work is to be considered as non-archival, I still believe you should at least cite the workshop paper.

Minor points:
1. GraphCast + SWAG: This is a baseline with poor performance, that is somewhat arbitrarily picked from many possible approaches to obtain ensemble predictions from neural networks. I see two options here: Either you keep it, but also introduce many other such baselines, to make clear that you did not cherrypick a particularly weak one. Other approaches  that should not be prohibitively expensive to run could e.g. be MC-Dropout or Laplace Approximation. Or, you simply drop it, as is, it does not  add much to the paper.
2. Introduction lacks motivation for LAM: This is an ML conference that you are submitting to. It would probably be good to briefly motivate why doing LAM is even necessary (i.e., why can't we just rely on global models instead)?
3. Extreme Weather evaluation / case study: One key reason for ensemble prediction is capturing the tails, i.e. the extremes. You state in Appendix A that this is out-of-scope for the work. I would argue you are making your life too easy here. Since the presented models are likely not useful unless they display decent performance also for extreme weather, it would be important to evaluate just that. It may be enough for this paper to e.g. study a single extreme event as a case study.

Limitations:
The section on limitations is somewhat short. I believe the key limitation of this work is in the evaluation, i.e. it is unclear to me after reading the work how well the models perform. E.g., are these models robust for longer rollouts or if applied at prediction time further away from training time?
Moreover, the work does transparently communicate a limitation of their multi-scale Graph-EFM: visual artifacts. But, this is strange to me, as it could mean two things: a) the VAE formulation and multiscale edges simply don't work well together or b) since GraphCast did not observe such issues, the presented Graph-EFM (ms) is simply not trained sufficiently or suffers from a buggy implementation. While a) would be an interesting finding, i believe b) can not be ruled out given the presented results in this paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new method for predicting weather using advanced deep learning models. The approach, called Graph-EFM, improves accuracy and better handles uncertainties in weather forecasts. It uses a 1.5 degree version of ERA5 and making weather predictions more reliable and useful for real-world applications.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper's strengths include the innovative use of Graph-EFM for accurate probabilistic weather forecasting, detailed experiments on large datasets, and clear presentation of methods. Graph-EFM significantly enhances uncertainty estimation and forecast reliability adding value to both research and practical weather prediction applications.

Weaknesses:
What happened if 0.25 degree ERA5 is used?

Limitations:
Not many, this is a fantastic piece of work

Rating:
10: strong accept, should be highlighted at the conference

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors propose a graph-based ensemble forecasting model (Graph-EFM) to provide weather prediction with a hirearchical GNN framework. They used a hierarchical mesh graph to handle the challenges of capturing processes unfolding over different spatial scales and modeling the uncertainty in the chaotic system. The Graph-EFM provides a probabilistic weather forecasting with the benefit of capturing forecast uncertainity. The experiment results show the effectiveness and advantages of Graph-EFM compared to other deterministic models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The hierarchical mesh graph provides a reasonable idea to handle different spatial scales for weather forecasting, which could inspire other researchers to handle problems in different domains.
The spatial dependencies are considered and handled within GNN layers.
Using ensumble-based model could capture the uncetainty of weather system.

Weaknesses:
In Figure 3, it seems like the selected ensemble members vary a lot, and how close is your forecast to the ground truth. Possibly, explaining a little bit of the underlying meaning of the measures in table 1 & 2 in the paper.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes Graph-EFM, a method that combines a hierarchical multi-scale graph neural network with a variational objective for probabilistic weather forecasting. The method performs on par with Graphcast on deterministic metrics with the extra benefit of uncertainty estimation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The paper tackles probabilistic weather forecasting, which is an important problem in the field.
- The proposed method is intuitive and makes sense. Overall, generative modeling is a potential direction for probabilistic weather forecasting. People have used GANs and diffusion, so a latent variable model is a natural addition to the literature.
- The performance looks promising, and it is more efficient than existing methods using diffusion.

Weaknesses:
- The authors should replace Table 1 with a line graph figure instead, as it allows comparison across different variables and lead times.
- Please see my questions below.

Limitations:
See above.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
wT6GHk5ShC;"REVIEW 
Summary:
The authors provide a theoretical perspective on the stability of in context learning via implicit gradient descent trajectories. Ultimately, the analysis suggests that high condition numbers of the weight matrices belonging to layers with a high index can be pruned in order to achieve a model which performs better on ICL tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- In context learning is important, and something which has not been studied as deeply as other topics of ML due to the recent rise of transformers and ICL in general.
- The method intuitively makes sense and is something which can be conditionally tuned after training based on specific tasks if a validation set is available.

Weaknesses:
- It would be good to define deep and shallow, as these are subjective terms depending on the reference frame.
- Figure 1 cpation says: ""We operate on the whole of MLP or ATTN."" What does this mean?
- If as figure 1 states, you can clip 99.5% of the original weights, what happens if you just drop that layer entirely? Recent work has shown that the deeper layers can be completely dropped without much effect. [1]
  - I cannot see much benefit gained from pruning part of the weights with SVD when it seems that the in nearly all cases, the benefit can be had by dropping the layer entirely.
  
- Is the mask on L138 supposed to represent a causal mask? If so, I do not think the notation is correct, as the Identity matrix would only have $N$ binary values which is much less than is needed for a causal mask.
- How can equation 1 and 2 use the same mask?
- Example 1 appears to be incorrect:
  - There is no parentheses around $W_{V_r}^k + \delta_V h_i^{k-1}$ in the first line.
  - The triangle inequality seems to say that line 2 $\geq$ line 1
  - Given the above, I do not see what conclusion can be drawn from this equation.
  - Have I missed something here?

Limitations:
The limitations have been discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the effect of singular value decomposition (SVD)-based weight pruning on the in-context learning (ICL) performance of large language models. 

The Authors show that SVD-based pruning can enhance ICL performance, with deeper layers showing more stable improvements. 
They provide theoretical analysis to explain these findings, presenting implicit gradient descent trajectories for ICL and deriving generalization bounds. 

Based on their insights, they propose a simple algorithm for enhancing ICL inference in downstream tasks.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The Authors provide a theoretical analysis to explain their empirical findings, including the derivation of implicit gradient descent trajectories and generalization bounds for ICL.

- Furthermore, they propose a simple, derivative-free algorithm for enhancing ICL performance in downstream tasks, demonstrating the practical value of their theoretical insights.

Weaknesses:
- The theoretical analysis primarily focuses on linear attention, which may not fully capture the complexities of standard Softmax attention used in most transformer models

- The proposed algorithm is derivative-free, but the search for optimal clipping rates may still be computationally expensive for very large models or datasets

- There is a substantial lack of comparison with other pruning methods: the study focuses on SVD-based pruning but doesn't compare it with other pruning techniques, which could provide context for the method's effectiveness

- Poor language, frequent typos, and grammatical errors are significant issues in this paper. This does not help readability, and would likely be a barrier to publication in its current form.

- An essential part of the paper, which is the discussion of related works is not part of the main text. Furthermore, this discussion is prone to criticism. For example, quoting the seminal paper by Frankle and Carbin as an example of low-rank properties of neural networks is clearly misleading. I think that this discussion should be an essential part of the main text, and should also be substantially revised in order to avoid conceptual confusions.

Limitations:
Limitations are discussed in the final section (4).

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper demonstrates that (1) SVD-based weight pruning can sometimes achieve better in-context learning performance, and (2) pruning weights in deeper layers often results in more stable outcomes compared to shallow layers. The authors explain their findings through theoretical analysis and propose an intuitive matrix condition number-based weight pruning algorithm to achieve both stable and improved performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work conducts an in-depth analysis to explain the ""stability"" of transformer weight pruning across different layers. The framework is interesting and validated through experiments. Moreover, the theoretical analysis can be applied to design new algorithms like algorithm 1 in this paper .

Weaknesses:
Despite adopting various simplifications (such as using a linear attention transformer without MLP and layer normalization, treating each in-context example as a single vector, implementing attention masks for query tokens, and using meta-gradients for in-context learning) in their theoretical analysis, the results are still limited. They only explain why SVD-based weight pruning can achieve ""stable"" performance, leaving the more intriguing question of why transformers can achieve ""better"" performance with pruning unclear. Additionally, even with detailed hyperparameter tuning, the effectiveness of Algorithm 1 remains uncertain. Further details are provided in the questions section.

Limitations:
See Weaknesses and Questions. Besides, there seem many mistakes on pages 7 and 8, for example, the equation between line 228 and 229, and the equation for about F-norm in lines 230,231,232. These inaccuracies cast doubt on the overall reliability of the paper's findings. If there are any misunderstandings on my part, please point them out, and I will reconsider my evaluation of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses the phenomenon: SVD-based weight pruning can increase the in-context learning abilities of transformer based LLMs. In this paper, the authors conduct theorectical analysis by presenting the implicit gradient descent trajectories of ICL and providing the generation bounds visa full implicit gradient descent trajectories. This paper also provide a simple yet effective algorithm to clip the LLM by SVD to enhance ICL inference.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
First, this paper has a clear writing and is easy to follow. 

It provides a detailed theoretical analysis on why SVD based weight pruning will improve ICL performance by leveraging the implicit gradient descent trajectories. It also provides the generalization bounds of ICL, in Theorem 2, it can be inferred that the noise level and the norm of of gradient contribute to the error bound. It provides the theoretical insight of SVD based method.

The authors provides a simple algorithm to leverage the discovered phenomenon to improve ICL performance of LLM in a gradient-freee way. The ratio between $\sigma_{max} $ and $\sigma_{min}$ is a good choice of heuristic conditional number.

Weaknesses:
1. More details of algorithms is not shared. e.g. the range / number of clipping rate candidates set. 
2. In experiments result of C.5, the optimal $\xi$ varies a lot across different tasks and different modules. However, this phenomenon is not touched in the theoretical part.

Limitations:
No

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors provide a theoretical perspective on the stability of in context learning via implicit gradient descent trajectories. Ultimately, the analysis suggests that high condition numbers of the weight matrices belonging to layers with a high index can be pruned in order to achieve a model which performs better on ICL tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- In context learning is important, and something which has not been studied as deeply as other topics of ML due to the recent rise of transformers and ICL in general.
- The method intuitively makes sense and is something which can be conditionally tuned after training based on specific tasks if a validation set is available.

Weaknesses:
- It would be good to define deep and shallow, as these are subjective terms depending on the reference frame.
- Figure 1 cpation says: ""We operate on the whole of MLP or ATTN."" What does this mean?
- If as figure 1 states, you can clip 99.5% of the original weights, what happens if you just drop that layer entirely? Recent work has shown that the deeper layers can be completely dropped without much effect. [1]
  - I cannot see much benefit gained from pruning part of the weights with SVD when it seems that the in nearly all cases, the benefit can be had by dropping the layer entirely.
  
- Is the mask on L138 supposed to represent a causal mask? If so, I do not think the notation is correct, as the Identity matrix would only have $N$ binary values which is much less than is needed for a causal mask.
- How can equation 1 and 2 use the same mask?
- Example 1 appears to be incorrect:
  - There is no parentheses around $W_{V_r}^k + \delta_V h_i^{k-1}$ in the first line.
  - The triangle inequality seems to say that line 2 $\geq$ line 1
  - Given the above, I do not see what conclusion can be drawn from this equation.
  - Have I missed something here?

Limitations:
The limitations have been discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the effect of singular value decomposition (SVD)-based weight pruning on the in-context learning (ICL) performance of large language models. 

The Authors show that SVD-based pruning can enhance ICL performance, with deeper layers showing more stable improvements. 
They provide theoretical analysis to explain these findings, presenting implicit gradient descent trajectories for ICL and deriving generalization bounds. 

Based on their insights, they propose a simple algorithm for enhancing ICL inference in downstream tasks.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The Authors provide a theoretical analysis to explain their empirical findings, including the derivation of implicit gradient descent trajectories and generalization bounds for ICL.

- Furthermore, they propose a simple, derivative-free algorithm for enhancing ICL performance in downstream tasks, demonstrating the practical value of their theoretical insights.

Weaknesses:
- The theoretical analysis primarily focuses on linear attention, which may not fully capture the complexities of standard Softmax attention used in most transformer models

- The proposed algorithm is derivative-free, but the search for optimal clipping rates may still be computationally expensive for very large models or datasets

- There is a substantial lack of comparison with other pruning methods: the study focuses on SVD-based pruning but doesn't compare it with other pruning techniques, which could provide context for the method's effectiveness

- Poor language, frequent typos, and grammatical errors are significant issues in this paper. This does not help readability, and would likely be a barrier to publication in its current form.

- An essential part of the paper, which is the discussion of related works is not part of the main text. Furthermore, this discussion is prone to criticism. For example, quoting the seminal paper by Frankle and Carbin as an example of low-rank properties of neural networks is clearly misleading. I think that this discussion should be an essential part of the main text, and should also be substantially revised in order to avoid conceptual confusions.

Limitations:
Limitations are discussed in the final section (4).

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper demonstrates that (1) SVD-based weight pruning can sometimes achieve better in-context learning performance, and (2) pruning weights in deeper layers often results in more stable outcomes compared to shallow layers. The authors explain their findings through theoretical analysis and propose an intuitive matrix condition number-based weight pruning algorithm to achieve both stable and improved performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work conducts an in-depth analysis to explain the ""stability"" of transformer weight pruning across different layers. The framework is interesting and validated through experiments. Moreover, the theoretical analysis can be applied to design new algorithms like algorithm 1 in this paper .

Weaknesses:
Despite adopting various simplifications (such as using a linear attention transformer without MLP and layer normalization, treating each in-context example as a single vector, implementing attention masks for query tokens, and using meta-gradients for in-context learning) in their theoretical analysis, the results are still limited. They only explain why SVD-based weight pruning can achieve ""stable"" performance, leaving the more intriguing question of why transformers can achieve ""better"" performance with pruning unclear. Additionally, even with detailed hyperparameter tuning, the effectiveness of Algorithm 1 remains uncertain. Further details are provided in the questions section.

Limitations:
See Weaknesses and Questions. Besides, there seem many mistakes on pages 7 and 8, for example, the equation between line 228 and 229, and the equation for about F-norm in lines 230,231,232. These inaccuracies cast doubt on the overall reliability of the paper's findings. If there are any misunderstandings on my part, please point them out, and I will reconsider my evaluation of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses the phenomenon: SVD-based weight pruning can increase the in-context learning abilities of transformer based LLMs. In this paper, the authors conduct theorectical analysis by presenting the implicit gradient descent trajectories of ICL and providing the generation bounds visa full implicit gradient descent trajectories. This paper also provide a simple yet effective algorithm to clip the LLM by SVD to enhance ICL inference.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
First, this paper has a clear writing and is easy to follow. 

It provides a detailed theoretical analysis on why SVD based weight pruning will improve ICL performance by leveraging the implicit gradient descent trajectories. It also provides the generalization bounds of ICL, in Theorem 2, it can be inferred that the noise level and the norm of of gradient contribute to the error bound. It provides the theoretical insight of SVD based method.

The authors provides a simple algorithm to leverage the discovered phenomenon to improve ICL performance of LLM in a gradient-freee way. The ratio between $\sigma_{max} $ and $\sigma_{min}$ is a good choice of heuristic conditional number.

Weaknesses:
1. More details of algorithms is not shared. e.g. the range / number of clipping rate candidates set. 
2. In experiments result of C.5, the optimal $\xi$ varies a lot across different tasks and different modules. However, this phenomenon is not touched in the theoretical part.

Limitations:
No

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wT5AgMVkaJ;"REVIEW 
Summary:
This paper studies the problem of aligning vision models with human aesthetic standards in a retrieval system. There are three key parts in the proposed model including LLM rephrasing, re-ranking, and RL fine-tuning. Two novel benchmarks are also introduced to integrate aesthetic quality into evaluation metrics. Experimental results demonstrate the effectiveness of the proposed method and the benchmarks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper addresses the aesthetic quality issue in image retrieval systems and introduces a reinforcement learning fine-tuning strategy that enables the retrieval model to directly retrieve images based on both semantics and aesthetic quality, eliminating the need for multi-stage filtering. This approach holds significant value.
2. The paper introduces two evaluation benchmarks, addressing the limitation of current image retrieval benchmarks that fail to evaluate aesthetic quality.
3. The experiments are comprehensive, validating the importance of each component in the proposed method.

Weaknesses:
1. The methodological process described in the article is somewhat cumbersome, with Figure 2 merely outlining key processes and concepts in a rudimentary manner, thereby increasing the difficulty for readers to comprehend.
2. The authors appear to conflate ""no-reference image quality assessment"" with ""image aesthetic quality assessment."" While these tasks are indeed closely related, they are distinct. MANIQA, for instance, should not be regarded as an aesthetic quality assessment model, and its paper does not evaluate the model's performance on aesthetic datasets.
3. There remain some details in the article that are inadequately explained. It is peculiar that in Appendix Table 7, the same stride seemingly yields a different number of images.
4. The manuscript contains typos. For example, the indicator function symbol in Equation 11 is clearly garbled.

Limitations:
The paper does not discuss the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper looks into the alignment task for vision and language models within retrieval models where properties such as visual aesthetic comes to play. To achieve this, the paper collects some data to design a metric suitable for taking into account human aesthetic evaluation. And employs an RL-based technique to exploit the human opinion for better aligning the retrieved images with human aesthetic preferences.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* It is well-written paper
* The concept of aligning vision with aesthetic preferences is interesting and useful in some applications.
* The experiments are well-designed and quite convincing. 
* It is interesting that LLM rephrasing could improve the quality of results

Weaknesses:
* The proposed metric could be elaborated better and maybe explained how the study ensured the metric is not designed under influence of the model.

Limitations:
The paper does not discuss any limitations with respect to the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work aims to align vision models with human aesthetic standards in a retrieval system. To do this, the authors propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. The authors further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The idea of aligning vision models with human aesthetics in retrieval is interesting. This work has potential applications in various real-life applications. 
2.	The authors’ motivation of utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations is insightful. 
3.	The paper is well-written and informative.
4.	The proposed dataset HPIR can be used by fellow researchers in the related fields.

Weaknesses:
I feel it can be further improved in the following ways. 

1.	For benchmarking human preferences, it might be better to record down the human variance in their annotations. I understand the authors used multiple annotations to ensure robustness, but since aesthetics is a subjective concept, human variance itself tells something.
2.	Following point 1, I feel the work can be made more solid if it includes some human evaluation studies on the experimental results. For example, in Fig. 5, it does not seem so obvious to me on the respective enhancement with finetuning. 

Without the above two points, I feel the paper has somewhat overclaimed the ""alighing vision models with human aesthetics"".

Limitations:
The authors did not indicate limitations in their paper, and mentioned that they will discuss it in future. I feel this paper has clear limitations such as the indication of human variance and the evaluation of the results.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aligns the vision models with human values by leveraging LLM for query rephrasing and introducing preference-based reinforcement learning. The paper also presents a novel dataset named HPIR to benchmark the alignment with human aesthetics.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper introduces a novel approach to align visual models with human aesthetics, combining LLM rewriting to enhance query understanding and using preference-based reinforcement learning to fine-tune the model. The paper is comprehensive in experiments and introduces the HPIR dataset for benchmarking. The paper is well-structured and the methods are clearly explained. Key concepts are well defined and the use of diagrams helps to effectively illustrate the results. And this paper improves the aesthetic quality of results in image retrieval by aligning visual models with human preferences. The proposed method and dataset provide valuable ideas for future research in this area.

Weaknesses:
[W1] This paper lacks a detailed user study to validate the actual effectiveness of the proposed method. Including a user study with different participants to evaluate the subjective improvement of aesthetic alignment could provide stronger evidence for the actual effectiveness of the method.
[W2] Placing the related work in Section 6 makes it difficult for readers to have a clear understanding of the problem domain and existing research results before reading the specific methods and experiments, which is not conducive to the coherence of the paper structure.

Limitations:
Exploring possible negative social impacts, such as implications for privacy or how the technology might be misused in unintended ways Research could benefit from deeper analysis of how biases in training data affect model outputs beyond just aesthetics, particularly with respect to cultural and demographic diversity.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of aligning vision models with human aesthetic standards in a retrieval system. There are three key parts in the proposed model including LLM rephrasing, re-ranking, and RL fine-tuning. Two novel benchmarks are also introduced to integrate aesthetic quality into evaluation metrics. Experimental results demonstrate the effectiveness of the proposed method and the benchmarks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper addresses the aesthetic quality issue in image retrieval systems and introduces a reinforcement learning fine-tuning strategy that enables the retrieval model to directly retrieve images based on both semantics and aesthetic quality, eliminating the need for multi-stage filtering. This approach holds significant value.
2. The paper introduces two evaluation benchmarks, addressing the limitation of current image retrieval benchmarks that fail to evaluate aesthetic quality.
3. The experiments are comprehensive, validating the importance of each component in the proposed method.

Weaknesses:
1. The methodological process described in the article is somewhat cumbersome, with Figure 2 merely outlining key processes and concepts in a rudimentary manner, thereby increasing the difficulty for readers to comprehend.
2. The authors appear to conflate ""no-reference image quality assessment"" with ""image aesthetic quality assessment."" While these tasks are indeed closely related, they are distinct. MANIQA, for instance, should not be regarded as an aesthetic quality assessment model, and its paper does not evaluate the model's performance on aesthetic datasets.
3. There remain some details in the article that are inadequately explained. It is peculiar that in Appendix Table 7, the same stride seemingly yields a different number of images.
4. The manuscript contains typos. For example, the indicator function symbol in Equation 11 is clearly garbled.

Limitations:
The paper does not discuss the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper looks into the alignment task for vision and language models within retrieval models where properties such as visual aesthetic comes to play. To achieve this, the paper collects some data to design a metric suitable for taking into account human aesthetic evaluation. And employs an RL-based technique to exploit the human opinion for better aligning the retrieved images with human aesthetic preferences.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* It is well-written paper
* The concept of aligning vision with aesthetic preferences is interesting and useful in some applications.
* The experiments are well-designed and quite convincing. 
* It is interesting that LLM rephrasing could improve the quality of results

Weaknesses:
* The proposed metric could be elaborated better and maybe explained how the study ensured the metric is not designed under influence of the model.

Limitations:
The paper does not discuss any limitations with respect to the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work aims to align vision models with human aesthetic standards in a retrieval system. To do this, the authors propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. The authors further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The idea of aligning vision models with human aesthetics in retrieval is interesting. This work has potential applications in various real-life applications. 
2.	The authors’ motivation of utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations is insightful. 
3.	The paper is well-written and informative.
4.	The proposed dataset HPIR can be used by fellow researchers in the related fields.

Weaknesses:
I feel it can be further improved in the following ways. 

1.	For benchmarking human preferences, it might be better to record down the human variance in their annotations. I understand the authors used multiple annotations to ensure robustness, but since aesthetics is a subjective concept, human variance itself tells something.
2.	Following point 1, I feel the work can be made more solid if it includes some human evaluation studies on the experimental results. For example, in Fig. 5, it does not seem so obvious to me on the respective enhancement with finetuning. 

Without the above two points, I feel the paper has somewhat overclaimed the ""alighing vision models with human aesthetics"".

Limitations:
The authors did not indicate limitations in their paper, and mentioned that they will discuss it in future. I feel this paper has clear limitations such as the indication of human variance and the evaluation of the results.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aligns the vision models with human values by leveraging LLM for query rephrasing and introducing preference-based reinforcement learning. The paper also presents a novel dataset named HPIR to benchmark the alignment with human aesthetics.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper introduces a novel approach to align visual models with human aesthetics, combining LLM rewriting to enhance query understanding and using preference-based reinforcement learning to fine-tune the model. The paper is comprehensive in experiments and introduces the HPIR dataset for benchmarking. The paper is well-structured and the methods are clearly explained. Key concepts are well defined and the use of diagrams helps to effectively illustrate the results. And this paper improves the aesthetic quality of results in image retrieval by aligning visual models with human preferences. The proposed method and dataset provide valuable ideas for future research in this area.

Weaknesses:
[W1] This paper lacks a detailed user study to validate the actual effectiveness of the proposed method. Including a user study with different participants to evaluate the subjective improvement of aesthetic alignment could provide stronger evidence for the actual effectiveness of the method.
[W2] Placing the related work in Section 6 makes it difficult for readers to have a clear understanding of the problem domain and existing research results before reading the specific methods and experiments, which is not conducive to the coherence of the paper structure.

Limitations:
Exploring possible negative social impacts, such as implications for privacy or how the technology might be misused in unintended ways Research could benefit from deeper analysis of how biases in training data affect model outputs beyond just aesthetics, particularly with respect to cultural and demographic diversity.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wT2TIfHKp8;"REVIEW 
Summary:
This paper addresses the challenge of predicting less frequently visited points-of-interest (POIs) in human mobility data, a problem known as the long-tail issue in spatial distribution. The authors introduce a new framework called Long-Tailed Adjusted POI Prediction (LoTNext), which includes two main components: long-tailed graph adjustment module and long-tailed loss adjustment module. Additionally, the framework employs an auxiliary prediction task to enhance the model's generalization and overall accuracy. The effectiveness of LoTNext is demonstrated through experiments on two real-world trajectory datasets, where it significantly outperforms existing state-of-the-art methods in human mobility prediction.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The code has been provided, which makes the reproducibility of this paper good.
2. The paper is generally well-writern and easy to follow.
3. The proposed method is motivation-grounded.

Weaknesses:
1. The presentation quality of this paper can be further enhanced.
2. The authors are encouraged to conduct experiments on more datasets and provide more detailed analysis.
3.  This paper can supplement more theoretical analysis to guarantee the proposed method's effectiveness.

Limitations:
See above weaknesses and questions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents the Long-Tail Adjusted Next POI Prediction (LoTNext) framework to address the long-tail problem in next POI prediction. This problem refers to the uneven spatial and temporal distribution of POI visits, making it challenging for prediction models to predict less frequently visited POIs. LoTNext combines a Long-Tailed Graph Adjustment module to reduce the noise and impact of long-tailed nodes in the user-POI interaction graph and a Long-Tailed Loss Adjustment module to balance the loss between head and tail POIs. Additionally, an auxiliary prediction task is employed to enhance generalization and accuracy. The proposed method was evaluated on two real-world trajectory datasets, Gowalla and Foursquare, where it significantly outperformed existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- LoTNext introduces a unique combination of graph adjustment and loss adjustment modules to tackle the long-tail problem, which is a significant contribution to the field of human mobility prediction.
- The framework is evaluated on two real-world datasets and compared with ten existing methods, demonstrating superior performance across multiple metrics.
- The paper provides a thorough explanation of the methodology, including the embedding generation, transformer encoder, spatial contextual attention layer, and the overall optimization process, making it reproducible and transparent.

Weaknesses:
- The proposed model is complex and involves multiple components and adjustments, but it is not clear how computationally expensive it would be to make predictions in services and elsewhere.
- The model performed well on the dataset used, but it is unclear under what conditions the proposed method will perform well, such as visit intervals and frequency of visits.

Limitations:
The authors have adequately addressed the limitation of their work, particularly the potential privacy risks associated with the extensive use of user trajectory data.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the LoTNext framework, which is designed to improve the prediction of human mobility patterns, specifically addressing the challenge of long-tail distribution in POI visitations. The authors propose a novel approach that includes a Long-Tailed Graph Adjustment module and a Long-Tailed Loss Adjustment module, along with an auxiliary prediction task, to enhance the model's ability to predict less frequently visited POIs. The paper demonstrates the effectiveness of LoTNext through comprehensive experiments on two real-world datasets, showing significant improvements over existing state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I like the research gap proposed by this paper. This is a worthwhile issue to study.

Weaknesses:
(1) The evaluation could be expanded to include a broader range of metrics to further validate the generalizability of the LoTNext framework.
(2) It's better to have more explainability related experiments.
(3) A more detailed literature review is needed (at least in the appendix) so that the novelty of the method could be better evaluated. 
(4) The comparison methods used are somewhat outdated. Why didn't you use the latest methods, such as TPG (https://arxiv.org/abs/2304.04151) or LLM-Move (https://arxiv.org/pdf/2404.01855), for comparison?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This study proposes the Long-Tail Adjusted Next Point-of-Interest Prediction (LoTNext) framework. By combining a Long-Tailed Graph Adjustment module and a Long-Tailed Loss Adjustment module, it reduces the impact of long-tailed nodes in the user-POI interaction graph and adjusts loss through logit score and sample weight adjustment strategies. Experimental results show that LoTNext outperforms several existing methods on two real-world datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The structure and organization of this paper are well-designed, and the writing is clear and easy to comprehend.
2. This paper investigates the long-tail problem by proposing a general framework for next POI recommendation, filling the gap in addressing the long-tail issue in POI recommendation. This work is meaningful and valuable.
3. To enhance the readability of the paper, the authors provide detailed results analysis, parameter settings, and the motivation behind the design of each module in the appendix.

Weaknesses:
1.  In the related work section, the authors review common methods for addressing the long-tail problem in recommendation systems. Since this paper focuses on addressing the long-tail problem, adding several baselines that tackle the long-tail issue in recommendation systems (e,g,, [1]) would better demonstrate the effectiveness of the proposed method.
2. The novelty of this paper is not very strong. The long tail effect of check-in data, such as the POI frequency distributions, has been studied before. 
3. Additional comparative analyses should be included to illustrate the shortcomings of baselines in handling the long-tail issue. For instance, comparing the proposed model's performance with all baselines (not just Graph-Flashback) on long-tail POIs would better demonstrate its effectiveness in addressing the long-tail problem.
4. The experimental results are not convincing enough, as the compared methods are not the SOTA method. More recent baselines should be compared (e.g., [2-4]).

[1] Meta graph learning for long-tail recommendation, SIGKDD, 2023.
[2] EEDN: Enhanced Encoder-Decoder Network with Local and Global Context Learning for POI Recommendation， SIGIR-23
[3] Adaptive Graph Representation Learning for Next POI Recommendation， SIGIR-23
[4] Spatio-Temporal Hypergraph Learning for Next POI Recommendation， SIGIR-23

Limitations:
The authors have already pointed out a limitation of this method: LoTNext relies on extensive user trajectory data, which, if deployed by certain institutions or companies, may pose a potential risk of privacy breaches, potentially leading to negative social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of predicting less frequently visited points-of-interest (POIs) in human mobility data, a problem known as the long-tail issue in spatial distribution. The authors introduce a new framework called Long-Tailed Adjusted POI Prediction (LoTNext), which includes two main components: long-tailed graph adjustment module and long-tailed loss adjustment module. Additionally, the framework employs an auxiliary prediction task to enhance the model's generalization and overall accuracy. The effectiveness of LoTNext is demonstrated through experiments on two real-world trajectory datasets, where it significantly outperforms existing state-of-the-art methods in human mobility prediction.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The code has been provided, which makes the reproducibility of this paper good.
2. The paper is generally well-writern and easy to follow.
3. The proposed method is motivation-grounded.

Weaknesses:
1. The presentation quality of this paper can be further enhanced.
2. The authors are encouraged to conduct experiments on more datasets and provide more detailed analysis.
3.  This paper can supplement more theoretical analysis to guarantee the proposed method's effectiveness.

Limitations:
See above weaknesses and questions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents the Long-Tail Adjusted Next POI Prediction (LoTNext) framework to address the long-tail problem in next POI prediction. This problem refers to the uneven spatial and temporal distribution of POI visits, making it challenging for prediction models to predict less frequently visited POIs. LoTNext combines a Long-Tailed Graph Adjustment module to reduce the noise and impact of long-tailed nodes in the user-POI interaction graph and a Long-Tailed Loss Adjustment module to balance the loss between head and tail POIs. Additionally, an auxiliary prediction task is employed to enhance generalization and accuracy. The proposed method was evaluated on two real-world trajectory datasets, Gowalla and Foursquare, where it significantly outperformed existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- LoTNext introduces a unique combination of graph adjustment and loss adjustment modules to tackle the long-tail problem, which is a significant contribution to the field of human mobility prediction.
- The framework is evaluated on two real-world datasets and compared with ten existing methods, demonstrating superior performance across multiple metrics.
- The paper provides a thorough explanation of the methodology, including the embedding generation, transformer encoder, spatial contextual attention layer, and the overall optimization process, making it reproducible and transparent.

Weaknesses:
- The proposed model is complex and involves multiple components and adjustments, but it is not clear how computationally expensive it would be to make predictions in services and elsewhere.
- The model performed well on the dataset used, but it is unclear under what conditions the proposed method will perform well, such as visit intervals and frequency of visits.

Limitations:
The authors have adequately addressed the limitation of their work, particularly the potential privacy risks associated with the extensive use of user trajectory data.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the LoTNext framework, which is designed to improve the prediction of human mobility patterns, specifically addressing the challenge of long-tail distribution in POI visitations. The authors propose a novel approach that includes a Long-Tailed Graph Adjustment module and a Long-Tailed Loss Adjustment module, along with an auxiliary prediction task, to enhance the model's ability to predict less frequently visited POIs. The paper demonstrates the effectiveness of LoTNext through comprehensive experiments on two real-world datasets, showing significant improvements over existing state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I like the research gap proposed by this paper. This is a worthwhile issue to study.

Weaknesses:
(1) The evaluation could be expanded to include a broader range of metrics to further validate the generalizability of the LoTNext framework.
(2) It's better to have more explainability related experiments.
(3) A more detailed literature review is needed (at least in the appendix) so that the novelty of the method could be better evaluated. 
(4) The comparison methods used are somewhat outdated. Why didn't you use the latest methods, such as TPG (https://arxiv.org/abs/2304.04151) or LLM-Move (https://arxiv.org/pdf/2404.01855), for comparison?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This study proposes the Long-Tail Adjusted Next Point-of-Interest Prediction (LoTNext) framework. By combining a Long-Tailed Graph Adjustment module and a Long-Tailed Loss Adjustment module, it reduces the impact of long-tailed nodes in the user-POI interaction graph and adjusts loss through logit score and sample weight adjustment strategies. Experimental results show that LoTNext outperforms several existing methods on two real-world datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The structure and organization of this paper are well-designed, and the writing is clear and easy to comprehend.
2. This paper investigates the long-tail problem by proposing a general framework for next POI recommendation, filling the gap in addressing the long-tail issue in POI recommendation. This work is meaningful and valuable.
3. To enhance the readability of the paper, the authors provide detailed results analysis, parameter settings, and the motivation behind the design of each module in the appendix.

Weaknesses:
1.  In the related work section, the authors review common methods for addressing the long-tail problem in recommendation systems. Since this paper focuses on addressing the long-tail problem, adding several baselines that tackle the long-tail issue in recommendation systems (e,g,, [1]) would better demonstrate the effectiveness of the proposed method.
2. The novelty of this paper is not very strong. The long tail effect of check-in data, such as the POI frequency distributions, has been studied before. 
3. Additional comparative analyses should be included to illustrate the shortcomings of baselines in handling the long-tail issue. For instance, comparing the proposed model's performance with all baselines (not just Graph-Flashback) on long-tail POIs would better demonstrate its effectiveness in addressing the long-tail problem.
4. The experimental results are not convincing enough, as the compared methods are not the SOTA method. More recent baselines should be compared (e.g., [2-4]).

[1] Meta graph learning for long-tail recommendation, SIGKDD, 2023.
[2] EEDN: Enhanced Encoder-Decoder Network with Local and Global Context Learning for POI Recommendation， SIGIR-23
[3] Adaptive Graph Representation Learning for Next POI Recommendation， SIGIR-23
[4] Spatio-Temporal Hypergraph Learning for Next POI Recommendation， SIGIR-23

Limitations:
The authors have already pointed out a limitation of this method: LoTNext relies on extensive user trajectory data, which, if deployed by certain institutions or companies, may pose a potential risk of privacy breaches, potentially leading to negative social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wT2KhEb97a;"REVIEW 
Summary:
This paper considers the approximate personalized page rank. Classical results for this problem have a runtime that is linear in $1/\alpha\epsilon$ where $\alpha$ is the damping factor and $\epsilon$ is the error parameter. The authors show that APPR is simply a local variant of Gauss-Seidel Successive Overrelaxation. Using this connection, the authors derive new run time bounds for APPR and also propose a new algorithm based on Gradient Descent. The execution time for both these are, in the worst-case, identical to the previous bounds. However, they are more sensitive to the state of execution of the algorithms (depend on the active nodes) and seem to mirror the actual performance of these algorithms. Also, under certain assumptions, they improve the worst-case execution time.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses an important problem, provides deeper insights into an existing algorithm, provides a new algorithm and also reanalyzes the algorithm in a more fine-grained way. All of this is done via connection to GSSOR which seems to be new.

I find the result quite interesting. However, I am not very familiar with recent work on personalized page rank.  For this reason, I recommend accepting but with a low confidence.

Weaknesses:
NA

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the study of local algorithms for graph clustering which is an important problem in the field of graph data analysis. In particular this paper is considers the task of computing Personalized Page Rank (PPR) vectors for a given graph. In this problem the algorithm is given a graph in the form of its adjacency and degree matrices, the goal is to approximate the Personalized Page Rank vector for a given starting vertex and dampening factor $\alpha$ up to precision $\epsilon$ without accessing the entire graph. The classical algorithm of Andersen, Chung and Lang runs in time $O(1/\alpha \epsilon)$, which independent of the graph size. The central question posed by subsequent works is whether the dependence on $\alpha$ can be improved to $1/\sqrt{\alpha}$. The main contribution of the paper is to propose a new algorithmic framework based on the locally evolving set process. Under this framework they are able to implement existing algorithms such as Andersen et al.'s APPR algorithm as well as localized implementation of standard gradient descent. They are also able to develop localized versions of chebyshev and heavy ball methods that do achieve the $1/\sqrt{\alpha}$ dependence for some fixed constant value of $\epsilon$. Finally they show that on several large scale graphs, their new localized chebyshev and heavy ball methods do outperform APPR and related methods empirically.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main strengths of this paper are to develop a new algorithmic framework that can not only encompass existing algorithms but lead to the development of better ones that overcome previously known limitations for designing local graph clustering algorithms. They also back their theoretical analysis with the practical implementation of their method which is also shown to be superior to previous algorithms.

Weaknesses:
One weakness is that the paper is only able to obtain a quadratic improvement in the dependence on the parameter $\alpha$, obtained by the local implementation of the Chebyshev and Heavy-ball method, only for a value of $\epsilon$ and not for all.

Limitations:
Authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper uses the evolving set procedure to give a local PageRank algorithm whose dependence on \alpha (the reset probability) is \sqrt{\alpha}.

It proposes accelerated local iterative methods with coefficients given by Chebyshev iteration. The convergence of this algorithm in both graph theoretic and general sparse linear systems settings are analyzed in detail. Discussions of the relations between this method and other local iterative algorithms are also given in detail.

The method was implemented and tested on a range of graphs, mostly coming from social networks. This includes two large ones with edges in the billions. On moderate ranges of \alpha (reset probability), the experiments show significant speedups (factor of about 3) and convergences (factor of 10) on most graphs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Local algorithms are widely used in graph analytics. The question studied is natural, and has been proposed before.

The method is theoretically well-founded, and has significant technical depth.

The experiments are thorough and well documents, and clearly demonstrate the advantages of this method in multiple parameter regimes.

Weaknesses:
The gains only kick in at a relatively large number of steps: it's not clear to me that these are the parameter regimes in which local algorithms actually get used.

Ideally for the empirical works I'd also like to see comparisons of downstream tasks and effects on overall accuracies (e.g. F-1 score), but the paper itself has already covered a lot of ground.

Limitations:
yes, limitations have been addressed, and are entirely theoretical w.r.t. some graph parameter regimes.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers the approximate personalized page rank. Classical results for this problem have a runtime that is linear in $1/\alpha\epsilon$ where $\alpha$ is the damping factor and $\epsilon$ is the error parameter. The authors show that APPR is simply a local variant of Gauss-Seidel Successive Overrelaxation. Using this connection, the authors derive new run time bounds for APPR and also propose a new algorithm based on Gradient Descent. The execution time for both these are, in the worst-case, identical to the previous bounds. However, they are more sensitive to the state of execution of the algorithms (depend on the active nodes) and seem to mirror the actual performance of these algorithms. Also, under certain assumptions, they improve the worst-case execution time.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses an important problem, provides deeper insights into an existing algorithm, provides a new algorithm and also reanalyzes the algorithm in a more fine-grained way. All of this is done via connection to GSSOR which seems to be new.

I find the result quite interesting. However, I am not very familiar with recent work on personalized page rank.  For this reason, I recommend accepting but with a low confidence.

Weaknesses:
NA

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the study of local algorithms for graph clustering which is an important problem in the field of graph data analysis. In particular this paper is considers the task of computing Personalized Page Rank (PPR) vectors for a given graph. In this problem the algorithm is given a graph in the form of its adjacency and degree matrices, the goal is to approximate the Personalized Page Rank vector for a given starting vertex and dampening factor $\alpha$ up to precision $\epsilon$ without accessing the entire graph. The classical algorithm of Andersen, Chung and Lang runs in time $O(1/\alpha \epsilon)$, which independent of the graph size. The central question posed by subsequent works is whether the dependence on $\alpha$ can be improved to $1/\sqrt{\alpha}$. The main contribution of the paper is to propose a new algorithmic framework based on the locally evolving set process. Under this framework they are able to implement existing algorithms such as Andersen et al.'s APPR algorithm as well as localized implementation of standard gradient descent. They are also able to develop localized versions of chebyshev and heavy ball methods that do achieve the $1/\sqrt{\alpha}$ dependence for some fixed constant value of $\epsilon$. Finally they show that on several large scale graphs, their new localized chebyshev and heavy ball methods do outperform APPR and related methods empirically.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main strengths of this paper are to develop a new algorithmic framework that can not only encompass existing algorithms but lead to the development of better ones that overcome previously known limitations for designing local graph clustering algorithms. They also back their theoretical analysis with the practical implementation of their method which is also shown to be superior to previous algorithms.

Weaknesses:
One weakness is that the paper is only able to obtain a quadratic improvement in the dependence on the parameter $\alpha$, obtained by the local implementation of the Chebyshev and Heavy-ball method, only for a value of $\epsilon$ and not for all.

Limitations:
Authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper uses the evolving set procedure to give a local PageRank algorithm whose dependence on \alpha (the reset probability) is \sqrt{\alpha}.

It proposes accelerated local iterative methods with coefficients given by Chebyshev iteration. The convergence of this algorithm in both graph theoretic and general sparse linear systems settings are analyzed in detail. Discussions of the relations between this method and other local iterative algorithms are also given in detail.

The method was implemented and tested on a range of graphs, mostly coming from social networks. This includes two large ones with edges in the billions. On moderate ranges of \alpha (reset probability), the experiments show significant speedups (factor of about 3) and convergences (factor of 10) on most graphs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Local algorithms are widely used in graph analytics. The question studied is natural, and has been proposed before.

The method is theoretically well-founded, and has significant technical depth.

The experiments are thorough and well documents, and clearly demonstrate the advantages of this method in multiple parameter regimes.

Weaknesses:
The gains only kick in at a relatively large number of steps: it's not clear to me that these are the parameter regimes in which local algorithms actually get used.

Ideally for the empirical works I'd also like to see comparisons of downstream tasks and effects on overall accuracies (e.g. F-1 score), but the paper itself has already covered a lot of ground.

Limitations:
yes, limitations have been addressed, and are entirely theoretical w.r.t. some graph parameter regimes.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wSqpNeMVLU;"REVIEW 
Summary:
This paper presents a theoretical study on speculative decoding, an efficient inference method for large autoregressive models. It highlights practical implications, proposing a Pareto-optimal solution for the rejection-distribution bias tradeoff.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The authors provide a robust theoretical foundation, illustrating the practical implications of speculative decoding, such as the improvement of rejection accuracy, which cannot be achieved by simply changing the acceptance probability.
 - The study explores the trade-offs between inference cost and quality degradation, supported by an optimization model. This analysis is valuable for practical applications.

Weaknesses:
- The main figure does not clearly communicate the core concept of speculative decoding. It might lead readers to believe that speculative decoding primarily addresses hallucination, which is not its main advantage.
 - The experimental results are not distinctly highlighted, and the authors do not explain how these results support their theoretical analysis. While the theoretical contributions are significant, the paper would benefit from more extensive empirical validation.

Limitations:
The results may not guarantee optimality in practical situations because real-world circumstances are more complex and varied than those considered in the theoretical analysis.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a theoretical perspective on speculative sampling. Through Theorems 1 and 2, the authors demonstrate that the sampling method employed by speculative sampling is optimal and unbiased. Subsequently, Theorem 3 introduces a multi-candidate approach to enhance the acceptance rate of speculative sampling.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The writing is very clear, with takeaways provided under each theorem to explain the theory.

Theorems 1 and 2 are crucial for speculative sampling. In paper [23], the authors showed that speculative sampling is unbiased but did not prove its efficiency compared to other rejection sampling methods. The proof provided here is very important.

Weaknesses:
The experiments are not sufficient. I would like to see improvements in batch speculative sampling in real-world scenarios.

I am curious if batch speculative sampling can be combined with tree-style methods, e.g., [1] CaPE and [2] Medusa?

[1] Du, C., Jiang, J., Yuanchen, X., Wu, J., Yu, S., Li, Y., ... & You, Y. GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding. In Forty-first International Conference on Machine Learning.
[2] Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., & Dao, T. (2024). Medusa: Simple LLM inference acceleration framework with multiple decoding heads. In Forty-first International Conference on Machine Learning.

Limitations:
na

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The author aim to develop theoretical understanding of speculative decoding. The authors assume that given a large and small model participating in speculative decoding, the computation complexity of the small model is negligible. Under this assumption, they characterize the expected rejection rate of speculative decoding. They show that this bound depends on the total variation distance between the generations from small and large model. Next, the authors show that spectral decoding gives optimal rejection bounds in class of all rejection based methods. Motivated by recent works analyzing batch speculative decoding, where the rejection is done only if M tokens are rejected from a given sample. Finally, given an acceptance probability, the authors show an optimal solution solution to the total variation loss between the distributions of large model and the one found by speculative decoding. This objective changes linearly with the rejection probability. This provides insights on selecting the optimal value of rejection threshold as per requirement. The presented theoretical results are backed up with appropriate experiments validating them.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) The theoretical analysis presented by the authors provide several interesting insights about the inference efficiency observed by spectral decoding.

2) All the results are backed up with simulation experiments, which strengthen the results presented in the paper.

Weaknesses:
1) It is not completely clear, why making the assumption about negligible compute of the small model is not a strong assumption. Since the small model needs to generate the tokens autoregressively therefore even though its single pass could be small as compared to the larger model but it the context length is high i.e. several autoregressive passes are made, the compute of small model might not be negligible. It would be great if the authors can provide some empirical evidence to justify this assumption.

2) It would have been great if the authors provided evidence using real world models in support of their theory. Although, this is not a major weakness, but authors should consider it in the camera ready version.

Limitations:
Yes, the authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides detailed analysis to speculative decoding and batch speculative decoding. The conclusions of the paper are: (1) speculative decoding is unbiased and it shows the expected rejection rate; (2) speculative decoding has the lowest rejection rate in all the unbiased algorithm that belongs to the familty defined in Algorithm 2; (3) batch speculative decoding has lower rejection rate than speculative decoding; (4) it analyzes the trade-off between efficiency and effectiveness of the family of algorithm defined in Algorithm 2.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper provides comprehensive theoretical analysis.

2. The findings in Theorem 4 and 5 are interesting.

3. The paper is easy to understand.

Weaknesses:
1. Although the paper provides lots of theoretical analysis. But I find only Theorem 4 and 5 are somewhat interesting. Theorem 1 is already derived in the original speculative decoding paper. For Theorem 2, although speculative decoding is proven to be optimal in the family of algorithms defined in Algorithm 2, but I don't think there are a lot of existing algorithms can be formulated in Algorithm 2. In fact, is there any algorithm that belongs to Algorithm 2 and is unbiad and it not speculative decoding? For Theorem 3, the finding that batch speculative decoding has lower rejection rate than vanilla speculative decoding is not surprising. 

2. Although Theorem 4 and Theorem 5 are interesting, it only solves half of the problem: given b, what should P be. It would be better if the authors could also discuss the design of b.

3. I think the paper can also be improved if the authors could summarize a new speculative algorithm from Theorem 4 and 5 and running experiments to compare with vanilla speculative decoding.

Limitations:
see weakness above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a theoretical study on speculative decoding, an efficient inference method for large autoregressive models. It highlights practical implications, proposing a Pareto-optimal solution for the rejection-distribution bias tradeoff.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The authors provide a robust theoretical foundation, illustrating the practical implications of speculative decoding, such as the improvement of rejection accuracy, which cannot be achieved by simply changing the acceptance probability.
 - The study explores the trade-offs between inference cost and quality degradation, supported by an optimization model. This analysis is valuable for practical applications.

Weaknesses:
- The main figure does not clearly communicate the core concept of speculative decoding. It might lead readers to believe that speculative decoding primarily addresses hallucination, which is not its main advantage.
 - The experimental results are not distinctly highlighted, and the authors do not explain how these results support their theoretical analysis. While the theoretical contributions are significant, the paper would benefit from more extensive empirical validation.

Limitations:
The results may not guarantee optimality in practical situations because real-world circumstances are more complex and varied than those considered in the theoretical analysis.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a theoretical perspective on speculative sampling. Through Theorems 1 and 2, the authors demonstrate that the sampling method employed by speculative sampling is optimal and unbiased. Subsequently, Theorem 3 introduces a multi-candidate approach to enhance the acceptance rate of speculative sampling.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The writing is very clear, with takeaways provided under each theorem to explain the theory.

Theorems 1 and 2 are crucial for speculative sampling. In paper [23], the authors showed that speculative sampling is unbiased but did not prove its efficiency compared to other rejection sampling methods. The proof provided here is very important.

Weaknesses:
The experiments are not sufficient. I would like to see improvements in batch speculative sampling in real-world scenarios.

I am curious if batch speculative sampling can be combined with tree-style methods, e.g., [1] CaPE and [2] Medusa?

[1] Du, C., Jiang, J., Yuanchen, X., Wu, J., Yu, S., Li, Y., ... & You, Y. GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding. In Forty-first International Conference on Machine Learning.
[2] Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., & Dao, T. (2024). Medusa: Simple LLM inference acceleration framework with multiple decoding heads. In Forty-first International Conference on Machine Learning.

Limitations:
na

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The author aim to develop theoretical understanding of speculative decoding. The authors assume that given a large and small model participating in speculative decoding, the computation complexity of the small model is negligible. Under this assumption, they characterize the expected rejection rate of speculative decoding. They show that this bound depends on the total variation distance between the generations from small and large model. Next, the authors show that spectral decoding gives optimal rejection bounds in class of all rejection based methods. Motivated by recent works analyzing batch speculative decoding, where the rejection is done only if M tokens are rejected from a given sample. Finally, given an acceptance probability, the authors show an optimal solution solution to the total variation loss between the distributions of large model and the one found by speculative decoding. This objective changes linearly with the rejection probability. This provides insights on selecting the optimal value of rejection threshold as per requirement. The presented theoretical results are backed up with appropriate experiments validating them.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) The theoretical analysis presented by the authors provide several interesting insights about the inference efficiency observed by spectral decoding.

2) All the results are backed up with simulation experiments, which strengthen the results presented in the paper.

Weaknesses:
1) It is not completely clear, why making the assumption about negligible compute of the small model is not a strong assumption. Since the small model needs to generate the tokens autoregressively therefore even though its single pass could be small as compared to the larger model but it the context length is high i.e. several autoregressive passes are made, the compute of small model might not be negligible. It would be great if the authors can provide some empirical evidence to justify this assumption.

2) It would have been great if the authors provided evidence using real world models in support of their theory. Although, this is not a major weakness, but authors should consider it in the camera ready version.

Limitations:
Yes, the authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides detailed analysis to speculative decoding and batch speculative decoding. The conclusions of the paper are: (1) speculative decoding is unbiased and it shows the expected rejection rate; (2) speculative decoding has the lowest rejection rate in all the unbiased algorithm that belongs to the familty defined in Algorithm 2; (3) batch speculative decoding has lower rejection rate than speculative decoding; (4) it analyzes the trade-off between efficiency and effectiveness of the family of algorithm defined in Algorithm 2.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper provides comprehensive theoretical analysis.

2. The findings in Theorem 4 and 5 are interesting.

3. The paper is easy to understand.

Weaknesses:
1. Although the paper provides lots of theoretical analysis. But I find only Theorem 4 and 5 are somewhat interesting. Theorem 1 is already derived in the original speculative decoding paper. For Theorem 2, although speculative decoding is proven to be optimal in the family of algorithms defined in Algorithm 2, but I don't think there are a lot of existing algorithms can be formulated in Algorithm 2. In fact, is there any algorithm that belongs to Algorithm 2 and is unbiad and it not speculative decoding? For Theorem 3, the finding that batch speculative decoding has lower rejection rate than vanilla speculative decoding is not surprising. 

2. Although Theorem 4 and Theorem 5 are interesting, it only solves half of the problem: given b, what should P be. It would be better if the authors could also discuss the design of b.

3. I think the paper can also be improved if the authors could summarize a new speculative algorithm from Theorem 4 and 5 and running experiments to compare with vanilla speculative decoding.

Limitations:
see weakness above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wQpNG9JnPK;"REVIEW 
Summary:
This paper addresses the problem of spurious correlations caused by environments from where data are collected.
The proposed method applies a mask to input data to separate spurious and semantic features.
The masked input data are fed into a local model specialized to each environment.
Each local model is trained to induce neural collapse for OOD generalization.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- S1: Making use of neural collapse for OOD generalization is interesting.

Weaknesses:
- W1: Comparison with not only OOD generalization methods but also spurious correlation (sometimes called bias or shortcut) methods is necessary. Methods that can automatically detect and split spurious and semantic features have been developed [a-e].
- W2: Types of spurious features that the proposed method can handle need to be clarified. Can the proposed method handle spurious features in superposition, e.g., objects and textures?
- W3: The rationale behind the proposed method needs to be clarified. For instance, it is unclear why the method adds the noise to the mask when learning it.
- W4: Deeper analyses in the experiments would make the paper more interesting. For example,      
  - Whether the neural collapse is achieved by the proposed method should be confirmed in the experiment.    
  - Visualizing learned masks would produce more valuable insights.
- W5: What is described in the introduction and what is done in the proposed method seems to be different. Although L42 states that `we propose to compute the Frobenius norm (F-norm) of the difference between the feature prototypes and the standard simplex ETF`, the F-norm does not appear in the proposed method.
- W6: Writing and formatting can be improved. There are many inconsistent spellings. For example,    
  - Is ""variable features"" in L153 the same as spurious features?  
  - The meaning of ""interaction"" in L189, 192, and so on is unclear. Maybe ""training?""  
  - Such inconsistent spellings occur from Section 4.

[a] Tiwari, Rishabh, and Pradeep Shenoy. ""Overcoming simplicity bias in deep networks using a feature sieve."" ICML2023.    
[b] Bahng, Hyojin, et al. ""Learning de-biased representations with biased representations."" ICML2020.    
[c] Yang, Wanqian, et al. ""Chroma-vae: Mitigating shortcut learning with generative classifiers."" NeurIPS2022.    
[d] Liu, Evan Z., et al. ""Just train twice: Improving group robustness without training group information."" ICML2021.    
[e] Nam, Junhyun, et al. ""Learning from failure: De-biasing classifier from biased classifier."" NeurIPS2020.

Limitations:
Discussed in Section 6.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper leverages the neural collapse inspired ETF behavior to simulate different environments in datasets, and uses it for OOD classification.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper uses a phenomenon that's apparent in the standard setting, for a task that varies from the standard setting. It uses intuitive notions to tackle the task of OOD classification. The paper experiments are generally convincing.

Weaknesses:
The paper seems generally consistent and well merited. The experiments are a bit lacking, but are convincing.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The spurious correlation between image background features and their labels is a significant research problem, and the existing research suffers from the issue of difficult decoupling. In this paper, we propose a new approach to solve the spurious association problem by alternately performing environment segmentation and learning semantic masks from the perspective of neural collapse. Extensive experiments are conducted on four datasets and the results show that the proposed method significantly improves the out-of-distribution performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper explores an important and widespread problem in real-world applications with solid and extensive experiments. The writing is clear and the narrative is easy to follow, facilitating an understanding of the spurious correlations problem. The use of neural collapse is particularly innovative.

Weaknesses:
W1:  In lines 48-50, it is mentioned that IRM-based methods learn similar representations from different environments, indicating a lack of proper alignment. Could you provide a corresponding experiment to demonstrate this phenomenon?

W2: In Figure 3, the explanation of the middle module that uses logits to judge the environment is unclear. Could you please clarify the structure of the local models, the number of local models used, and the specific meaning of the logit values?

W3: Could you explain the differences between masks based on pixel-level and feature-level approaches? If using feature-level masks, what is the impact of different network-layer features on model performance?

This work addresses an important and interesting question by introducing neural collapse from an invariant perspective, which I believe can provide valuable insights to the community. However, my main concern is that the same mask is used to learn both invariant and variable feature information. What are the advantages of the mask learning mechanism proposed in this paper compared to HRM's [1] mask mechanism?

[1] Heterogeneous Risk Minimization

Limitations:
Yes, the authors have adequately described the limitations in their submission.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper addresses the problem of spurious correlations caused by environments from where data are collected.
The proposed method applies a mask to input data to separate spurious and semantic features.
The masked input data are fed into a local model specialized to each environment.
Each local model is trained to induce neural collapse for OOD generalization.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- S1: Making use of neural collapse for OOD generalization is interesting.

Weaknesses:
- W1: Comparison with not only OOD generalization methods but also spurious correlation (sometimes called bias or shortcut) methods is necessary. Methods that can automatically detect and split spurious and semantic features have been developed [a-e].
- W2: Types of spurious features that the proposed method can handle need to be clarified. Can the proposed method handle spurious features in superposition, e.g., objects and textures?
- W3: The rationale behind the proposed method needs to be clarified. For instance, it is unclear why the method adds the noise to the mask when learning it.
- W4: Deeper analyses in the experiments would make the paper more interesting. For example,      
  - Whether the neural collapse is achieved by the proposed method should be confirmed in the experiment.    
  - Visualizing learned masks would produce more valuable insights.
- W5: What is described in the introduction and what is done in the proposed method seems to be different. Although L42 states that `we propose to compute the Frobenius norm (F-norm) of the difference between the feature prototypes and the standard simplex ETF`, the F-norm does not appear in the proposed method.
- W6: Writing and formatting can be improved. There are many inconsistent spellings. For example,    
  - Is ""variable features"" in L153 the same as spurious features?  
  - The meaning of ""interaction"" in L189, 192, and so on is unclear. Maybe ""training?""  
  - Such inconsistent spellings occur from Section 4.

[a] Tiwari, Rishabh, and Pradeep Shenoy. ""Overcoming simplicity bias in deep networks using a feature sieve."" ICML2023.    
[b] Bahng, Hyojin, et al. ""Learning de-biased representations with biased representations."" ICML2020.    
[c] Yang, Wanqian, et al. ""Chroma-vae: Mitigating shortcut learning with generative classifiers."" NeurIPS2022.    
[d] Liu, Evan Z., et al. ""Just train twice: Improving group robustness without training group information."" ICML2021.    
[e] Nam, Junhyun, et al. ""Learning from failure: De-biasing classifier from biased classifier."" NeurIPS2020.

Limitations:
Discussed in Section 6.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper leverages the neural collapse inspired ETF behavior to simulate different environments in datasets, and uses it for OOD classification.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper uses a phenomenon that's apparent in the standard setting, for a task that varies from the standard setting. It uses intuitive notions to tackle the task of OOD classification. The paper experiments are generally convincing.

Weaknesses:
The paper seems generally consistent and well merited. The experiments are a bit lacking, but are convincing.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The spurious correlation between image background features and their labels is a significant research problem, and the existing research suffers from the issue of difficult decoupling. In this paper, we propose a new approach to solve the spurious association problem by alternately performing environment segmentation and learning semantic masks from the perspective of neural collapse. Extensive experiments are conducted on four datasets and the results show that the proposed method significantly improves the out-of-distribution performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper explores an important and widespread problem in real-world applications with solid and extensive experiments. The writing is clear and the narrative is easy to follow, facilitating an understanding of the spurious correlations problem. The use of neural collapse is particularly innovative.

Weaknesses:
W1:  In lines 48-50, it is mentioned that IRM-based methods learn similar representations from different environments, indicating a lack of proper alignment. Could you provide a corresponding experiment to demonstrate this phenomenon?

W2: In Figure 3, the explanation of the middle module that uses logits to judge the environment is unclear. Could you please clarify the structure of the local models, the number of local models used, and the specific meaning of the logit values?

W3: Could you explain the differences between masks based on pixel-level and feature-level approaches? If using feature-level masks, what is the impact of different network-layer features on model performance?

This work addresses an important and interesting question by introducing neural collapse from an invariant perspective, which I believe can provide valuable insights to the community. However, my main concern is that the same mask is used to learn both invariant and variable feature information. What are the advantages of the mask learning mechanism proposed in this paper compared to HRM's [1] mask mechanism?

[1] Heterogeneous Risk Minimization

Limitations:
Yes, the authors have adequately described the limitations in their submission.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
wK0Z49myyi;"REVIEW 
Summary:
The manuscript #3263 entitled ""CRAYM: Neural Field Optimization via Camera RAY Matching"" proposes a novel uncalibrated NeRF strategy based on prior keypoints matching across images. Specifically, the authors propose two novelties to improve the quality of the reconstruction and the pose estimation of the cameras: 1) Enriched ray features using surrounding rays sampled around the keypoints and 2) a ray matching index which can be used to re-weight the color regression part, leading to better robustness to occlusions.
The proposed technique has been evaluated across various standard datasets and against meaningful NeRF-like algorithms.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The idea of separating key rays and auxiliary rays is interesting and meaningful.
- Numerous and conclusive results.
- Assessment on a large number of datasets.
- Good ablation study underlying the benefit of each novelty.

Weaknesses:
- The robustness of the approach against outlier matches is not evaluated. Introducing artificial outliers (wrongly matched keypoints) into the dataset to assess how well the technique can handle mismatches would be of some interest.
*Question*: Would the matched rays consistency and the epipolar geometry compensate for that? Or would the training diverge?

- As stated in the literature review of this manuscript, other approaches taking advantage of the epipolar geometry and prior matching have already been designed in this context. I have difficulty understanding what is significantly different with this work apart from the sampling of additional surrounding rays and the matching ray index used to weight color prediction using image pairs. These two novelties seem rather incremental, but they nonetheless lead to strongly improved results.
*Question*: I assume that the other keypoints-based approaches are not ""self-calibrated"". Is the proposed technique the first ""keypoint-based"" calibration-free NeRF? If it is not the case, it would be meaningful to compare against such techniques too.

- Adding surrounding rays around a key ray appears to be quite effective; however, the sampling of auxiliary rays is not well described in the paper.
*Question*: How are the rays sampled?

- The initialization of the pose lacks details.
 *Question*: What is the effect of the pose initialization on the result?

- The intrinsic parameters of the camera could additionally be optimized.
*Question*: Just out of curiosity, have you conducted such an experiment?

- In equation (4), it seems that the proposed solution considers only pairs of images. 
*Question*: How are those pairs selected?

The proposed approach is inspired by existing techniques integrating matched keypoints (like using the epipolar loss) and other techniques, such as NeuS.

- The loss function contains many regularization factors.
*Question*: Is the final loss hard to balance?


Overall, the paper is interesting and proposes a few contributions that seem to lead to strongly improved results. Moreover, the approach has been evaluated on various standard datasets and against representative methods. However, this novel approach remains relatively incremental, and many points remain to be clarified regarding the robustness of the technique. For all the above-mentioned reasons, I would like to issue a rather mixed opinion regarding the acceptance of this work for this conference.

Limitations:
The limitations of the paper may not have been entirely investigated, specifically in terms of robustness. For instance, the influence of the initial pose and outliers has not been demonstrated.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a new technique called camera ray matching, which is integrated into the joint optimization of camera poses and a neural field. The method utilizes an uncalibrated set of images as input, incorporating photometric and geometric constraints through key points and key rays matching, with the aim of enhancing the quality of novel view rendering and 3D surface reconstruction. The approach comprises two simple modules and is implemented using grid-based representation (iNGP). Photometric experiments were exclusively compared with MLP-based methods, specifically NeRF-like, on the synthetic dataset of the vanilla NeRF, while geometric experiments demonstrate some positive results. The authors provide additional results in the appendix.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work is a positive extension to the field of neural reconstruction (like NeRF and SDF) under the setting of images captured with noisy poses. Authors make efforts to simultaneously solve the problems involving the camera pose, detailed renderings, and accurate surface reconstruction. Experiments show good results.

Weaknesses:
Too many factors are taken into account in the writing simultaneously, which leads to a lack of clear theme or a clear academic or technical problem to be addressed in this paper. The work appears to build incrementally upon previous research and offers limited novelty. The so-called Epipolar loss and Point-alignment loss are actually based on Bundle Adjustment (BA), using key points matching, which has been previously applied in the optimization of neural reconstruction in works such as SCNeRF, BARF, L2G, and Level2sfm. The proposed two modules do not bring significant innovation. It is also confusing that this work is implemented using a grid-based representation (i.e., iNGP), while the compared methods are implemented using MLP-based representation, which does not allow for a precise and fair comparison. I suggest that the authors refer to ZipNeRF for guidance on how to formulate research problems and conduct appropriate comparisons.

Limitations:
The authors have identified a limitation where the meshes extracted from the constructed SDFs may still contain messy inner structures over invisible areas. I recommend that the authors explore the possibility of finding the SDFs of surface points instead of the SDFs of each sampled point along rays. This would involve assessing whether the depth accumulated by all points along the rays is more accurate than the output surface generated by all sampled points.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work suggests a novel neural representation and training scheme that jointly solves for the scene representation and the multi-view camera localization. It is done using several new ideas that generalize existing NeRF based methods. 
The representation itself is a combination of a geometry-network, which predicts a signed-distance-function (SDF) and a feature vector, that are fed into the texture-network that predicts the usual color and density values.
The main key novelty, is that the optimization is done over matching rays, obtained from matching keypoints using a pretrained network. The standard photometric loss function is extended to incorporate an epipolar loss (that constrains the camera positions) and a point-alignment loss that ensures the ray intersect at the predicted depth estimates along the rays. Another strong addition, is the use of 'auxiliary' rays around each matched pair of rays, from which features are fused to produce a more robust representation, that can aid the optimization under errors in matching and camera poses.
Extensive experiments demonstrate the importance of each component and the strong performance of their combination.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* The paper presents an extension of the NeRF framework, based on several novel and interesting additions that are framed in a single pipeline. The experimental results show that these contributions work well together and yield new state-of-the-art results, across the board.
* One promising idea, in my view, is the joint optimization of both geometry and texture networks, which clearly complement eachother and are helpful in obtaining stronger and more accurate constraints on the scene understanding (as opposed to most NeRF pipelines that focus on image reconstruction and are less accurate for 3D reconstruction).  
* The other strong idea, is the joint optimization of matching rays, once again - imposing consistency contraints (on both camera and surface locations) that were not previously exploited to such an extent in prior work.
* The paper is well written and the contributions are very clearly highlighted, while the understanding of the conventional parts is left for the reader (which is mostly fine).

Weaknesses:
* Reproducibility - I believe that many details are missing (including from the appendix) for one to be able to implement the proposed method. For example:
   * What are the settings of the preprocessing SuperPoint and SuperGlue matching? What is the typical match density?
   * How are the auxiliary rays sampled? How many and under which distribution?
   * What is the function g in Eq 2 that fuses the key and auxiliary features?
   * What are the balance weights in the final loss (Eq. 7)?
   * How are poses initialized?
* Complexity - There is no discussion what so ever about the impact of the suggested changes on memory and runtime complexity, but at traning and in inference.
* Qualitative results are relatively limited. 
   * Synthesized images are all very small, so it is difficult to appreciate the fidelity.
   * No depth images are shown
   * No examples of key and auxiliary point matches are shown (over entire images)

Limitations:
Adequately discussed.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces Camera Ray Matching for optimizing camera poses and neural fields from multi-view images. The optimized feature volume supports novel view synthesis and 3D geometry reconstruction by probing camera rays, which carry both geometric and photometric information. CRAYM claims to improves efficiency and accuracy by focusing on keypoints and integrating multi-view consistencies, enhancing both geometric reconstruction and photorealistic rendering. The method shows result in NVS and geometry reconstruction compared to baseline methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper is well-structured and easy to follow.

Weaknesses:
- Experiments were only conducted on NeRF-synthetic datasets and not on LLFF datasets.
- Comparison is made with older baseline methods (e.g., SPARF, BARF, L2G) which are more than 2 years old. It’s recommended to include more recent methods such as NoPe-NeRF and BAA-NGP.
- It is suggested that the authors perform Neural Image Alignment to enhance the evaluation.

Limitations:
YES

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript #3263 entitled ""CRAYM: Neural Field Optimization via Camera RAY Matching"" proposes a novel uncalibrated NeRF strategy based on prior keypoints matching across images. Specifically, the authors propose two novelties to improve the quality of the reconstruction and the pose estimation of the cameras: 1) Enriched ray features using surrounding rays sampled around the keypoints and 2) a ray matching index which can be used to re-weight the color regression part, leading to better robustness to occlusions.
The proposed technique has been evaluated across various standard datasets and against meaningful NeRF-like algorithms.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The idea of separating key rays and auxiliary rays is interesting and meaningful.
- Numerous and conclusive results.
- Assessment on a large number of datasets.
- Good ablation study underlying the benefit of each novelty.

Weaknesses:
- The robustness of the approach against outlier matches is not evaluated. Introducing artificial outliers (wrongly matched keypoints) into the dataset to assess how well the technique can handle mismatches would be of some interest.
*Question*: Would the matched rays consistency and the epipolar geometry compensate for that? Or would the training diverge?

- As stated in the literature review of this manuscript, other approaches taking advantage of the epipolar geometry and prior matching have already been designed in this context. I have difficulty understanding what is significantly different with this work apart from the sampling of additional surrounding rays and the matching ray index used to weight color prediction using image pairs. These two novelties seem rather incremental, but they nonetheless lead to strongly improved results.
*Question*: I assume that the other keypoints-based approaches are not ""self-calibrated"". Is the proposed technique the first ""keypoint-based"" calibration-free NeRF? If it is not the case, it would be meaningful to compare against such techniques too.

- Adding surrounding rays around a key ray appears to be quite effective; however, the sampling of auxiliary rays is not well described in the paper.
*Question*: How are the rays sampled?

- The initialization of the pose lacks details.
 *Question*: What is the effect of the pose initialization on the result?

- The intrinsic parameters of the camera could additionally be optimized.
*Question*: Just out of curiosity, have you conducted such an experiment?

- In equation (4), it seems that the proposed solution considers only pairs of images. 
*Question*: How are those pairs selected?

The proposed approach is inspired by existing techniques integrating matched keypoints (like using the epipolar loss) and other techniques, such as NeuS.

- The loss function contains many regularization factors.
*Question*: Is the final loss hard to balance?


Overall, the paper is interesting and proposes a few contributions that seem to lead to strongly improved results. Moreover, the approach has been evaluated on various standard datasets and against representative methods. However, this novel approach remains relatively incremental, and many points remain to be clarified regarding the robustness of the technique. For all the above-mentioned reasons, I would like to issue a rather mixed opinion regarding the acceptance of this work for this conference.

Limitations:
The limitations of the paper may not have been entirely investigated, specifically in terms of robustness. For instance, the influence of the initial pose and outliers has not been demonstrated.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a new technique called camera ray matching, which is integrated into the joint optimization of camera poses and a neural field. The method utilizes an uncalibrated set of images as input, incorporating photometric and geometric constraints through key points and key rays matching, with the aim of enhancing the quality of novel view rendering and 3D surface reconstruction. The approach comprises two simple modules and is implemented using grid-based representation (iNGP). Photometric experiments were exclusively compared with MLP-based methods, specifically NeRF-like, on the synthetic dataset of the vanilla NeRF, while geometric experiments demonstrate some positive results. The authors provide additional results in the appendix.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work is a positive extension to the field of neural reconstruction (like NeRF and SDF) under the setting of images captured with noisy poses. Authors make efforts to simultaneously solve the problems involving the camera pose, detailed renderings, and accurate surface reconstruction. Experiments show good results.

Weaknesses:
Too many factors are taken into account in the writing simultaneously, which leads to a lack of clear theme or a clear academic or technical problem to be addressed in this paper. The work appears to build incrementally upon previous research and offers limited novelty. The so-called Epipolar loss and Point-alignment loss are actually based on Bundle Adjustment (BA), using key points matching, which has been previously applied in the optimization of neural reconstruction in works such as SCNeRF, BARF, L2G, and Level2sfm. The proposed two modules do not bring significant innovation. It is also confusing that this work is implemented using a grid-based representation (i.e., iNGP), while the compared methods are implemented using MLP-based representation, which does not allow for a precise and fair comparison. I suggest that the authors refer to ZipNeRF for guidance on how to formulate research problems and conduct appropriate comparisons.

Limitations:
The authors have identified a limitation where the meshes extracted from the constructed SDFs may still contain messy inner structures over invisible areas. I recommend that the authors explore the possibility of finding the SDFs of surface points instead of the SDFs of each sampled point along rays. This would involve assessing whether the depth accumulated by all points along the rays is more accurate than the output surface generated by all sampled points.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work suggests a novel neural representation and training scheme that jointly solves for the scene representation and the multi-view camera localization. It is done using several new ideas that generalize existing NeRF based methods. 
The representation itself is a combination of a geometry-network, which predicts a signed-distance-function (SDF) and a feature vector, that are fed into the texture-network that predicts the usual color and density values.
The main key novelty, is that the optimization is done over matching rays, obtained from matching keypoints using a pretrained network. The standard photometric loss function is extended to incorporate an epipolar loss (that constrains the camera positions) and a point-alignment loss that ensures the ray intersect at the predicted depth estimates along the rays. Another strong addition, is the use of 'auxiliary' rays around each matched pair of rays, from which features are fused to produce a more robust representation, that can aid the optimization under errors in matching and camera poses.
Extensive experiments demonstrate the importance of each component and the strong performance of their combination.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* The paper presents an extension of the NeRF framework, based on several novel and interesting additions that are framed in a single pipeline. The experimental results show that these contributions work well together and yield new state-of-the-art results, across the board.
* One promising idea, in my view, is the joint optimization of both geometry and texture networks, which clearly complement eachother and are helpful in obtaining stronger and more accurate constraints on the scene understanding (as opposed to most NeRF pipelines that focus on image reconstruction and are less accurate for 3D reconstruction).  
* The other strong idea, is the joint optimization of matching rays, once again - imposing consistency contraints (on both camera and surface locations) that were not previously exploited to such an extent in prior work.
* The paper is well written and the contributions are very clearly highlighted, while the understanding of the conventional parts is left for the reader (which is mostly fine).

Weaknesses:
* Reproducibility - I believe that many details are missing (including from the appendix) for one to be able to implement the proposed method. For example:
   * What are the settings of the preprocessing SuperPoint and SuperGlue matching? What is the typical match density?
   * How are the auxiliary rays sampled? How many and under which distribution?
   * What is the function g in Eq 2 that fuses the key and auxiliary features?
   * What are the balance weights in the final loss (Eq. 7)?
   * How are poses initialized?
* Complexity - There is no discussion what so ever about the impact of the suggested changes on memory and runtime complexity, but at traning and in inference.
* Qualitative results are relatively limited. 
   * Synthesized images are all very small, so it is difficult to appreciate the fidelity.
   * No depth images are shown
   * No examples of key and auxiliary point matches are shown (over entire images)

Limitations:
Adequately discussed.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces Camera Ray Matching for optimizing camera poses and neural fields from multi-view images. The optimized feature volume supports novel view synthesis and 3D geometry reconstruction by probing camera rays, which carry both geometric and photometric information. CRAYM claims to improves efficiency and accuracy by focusing on keypoints and integrating multi-view consistencies, enhancing both geometric reconstruction and photorealistic rendering. The method shows result in NVS and geometry reconstruction compared to baseline methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper is well-structured and easy to follow.

Weaknesses:
- Experiments were only conducted on NeRF-synthetic datasets and not on LLFF datasets.
- Comparison is made with older baseline methods (e.g., SPARF, BARF, L2G) which are more than 2 years old. It’s recommended to include more recent methods such as NoPe-NeRF and BAA-NGP.
- It is suggested that the authors perform Neural Image Alignment to enhance the evaluation.

Limitations:
YES

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wJaCsnT9UE;"REVIEW 
Summary:
This paper presents introduces a training approach for ensemble learning called SharpBalance to balance sharpness and diversity within ensembles. This paper shows theoretically that SharpBalance achieves a better sharpness-diversity trade-off.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Ensemble learning is an important research direction.
2. Understanding of sharpness and diversity within deep ensembles is important for the study of generalization to both in-distribution and out-of-distribution data.
3. The paper is technically sound.

Weaknesses:
1. Since SharpBalance focuses ""on a diverse subset of the sharpest training data samples"", it may not apply in small datasets where available data is already sparse.
2. Empirical improvement over existing methods is marginal.

Limitations:
Limitations are adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the sharpness and diversity within deep ensembles. Specifically, it identifies the trade-off phenomenon between sharpness and diversity with both theoretical and empirical evidence. Additionally, it proposes a method called SharpBalance, which trains individuals using selective 'sharp' subsets. Conducted experiments have demonstrated the effectiveness of the proposed SharpBalance when applied to deep ensembles.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
There are several strengths in this paper:

- The exploration of sharpness and diversity in deep ensembles is both interesting and novel.

- Sufficient theoretical and empirical evidence has been provided for validation.

- The proposed method is simple, effective, and accompanied by code for verification.

Weaknesses:
However, I still have the following concerns:

 - The evaluation seems a bit weak. The authors should consider comparing with more ensemble baselines.

 - What is the scale of $D_{SAM}^i$ and how does it change during training? Providing some details on this would help in understanding the proposed method.

 - Refer to Line 166: How do the authors train individuals with the full datasets? Are these individuals trained with different initializations?

 - (Optional) As described, the model's generalization is not merely correlated with sharpness, which aligns with some recent advanced SAM variants. Thus, integrating these advanced variants [1][2] with SharpBal would be more beneficial for studying the trade-off between sharpness and diversity.

References:

[1] Random Sharpness-Aware Minimization. In NeurIPS 2022.

[2] Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization. In CVPR 2023.

Limitations:
The authors have provided **Limitations** section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes SharpBalance, that is a method aiming to investigate the relationship between sharpness and diversity for deep ensembles.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- SharpBalance looks quite effective for the out-of-distribution setting. The goal of balancing sharpness and diversity within ensembles is an important idea.  
- Great theoretical analysis

Weaknesses:
- The authors are aware of the paper called “Diversity-Aware Agnostic Ensemble of Sharpness Minimizers” [1], the idea is quite like the proposed paper, they aim to investigate the relations between sharpness and diversity on ensemble learning. I suggest the authors to discuss the main differences between both.    

[1] Anh Bui, Vy Vo, Tung Pham, Dinh Phung and Trung Le, Diversity-Aware Agnostic Ensemble of Sharpness Minimizers, arXiv:2403.13204. 

- Regarding the baselines the authors only compare SharpBalance with SAM. Nevertheless, newer, and stronger baselines like GSAM [2] and OBF [3] should also be benchmarked since they are the current state-of-the-art. 

[2] Zhuang, J., Gong, B., Yuan, L., Cui, Y., Adam, H., Dvornek, N., Tatikonda, S., Duncan, J., and Liu, T. Surrogate gap minimization improves sharpness-aware training. arXiv preprint arXiv:2203.08065, 2022. 

[3] Vani, A; Tung, F; Oliveira G; Sharifi H. Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics, International Conference on Machine Learning (ICML) 2024.  

- Another point to improve are the datasets. I strongly suggest the authors to benchmark with at least a couple large scale datasets. Options are ImageNet-V1 [4] for training and ImageNet-Real [5] and ImageNet-V2 [6] for testing, ImageNet-R [7] for out-of-distribution robustness benchmark and ImageNet-Sketch [8].  

[4] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. IEEE, 2009 

[5] Beyer, L., He ́naff, O. J., Kolesnikov, A., Zhai, X., and Oord, A. v. d. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020. 

[6] Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do imagenet classifiers generalize to imagenet? In Interna- tional conference on 
machine learning, pp. 5389–5400. PMLR, 2019. 

[7] Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340–8349, 2021. 

[8] Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pp. 10506–10518, 2019.

Limitations:
- The proposed method is not a significant improvement for ID scenarios as the authors claim. I would tone that down over the whole text. Maybe clearly state that SharpBalance is indeed superior to OOD scenarios and competitive on ID settings.  

- I would not claim the phenomenon called sharpness-diversity trade-off is a discovery, paper [1] is addressing the same phenomena and it was publicly available on arxiv before submission and on openreview since the beginning of the year.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Ensemble methods and sharpness-aware optimization techniques are well-known strategies for improving generalization. This work identifies a trade-off between sharpness and diversity, observing that reducing sharpness can diminish diversity and harm ensemble performance. Through theoretical and empirical analysis of this sharpness-diversity trade-off, the authors present SharpBalance, an algorithm for training ensembles with sharpness-aware solutions without sacrificing diversity. Evaluation results on CIFAR-10/100, TinyImageNet, and their corrupted variants confirm the effectiveness of SharpBalance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Ensemble methods and sharpness-aware optimization techniques are both prominent approaches for improving generalization. The aim of this work, which combines these two approaches, is well-motivated.
- While the theoretical analysis uses the variance metric to indicate diversity, the experimental results show consistent trends across different diversity metrics. It suggests that the proposed analysis is widely applicable to the general concept of diversity.
- Extensive empirical results effectively validate the theoretical analysis. The summary plots of the results are generally highly readable.

Weaknesses:
- The evaluation results are centered exclusively on classification accuracy; since ensembling usually highlights both predictive accuracy and uncertainty, relying solely on accuracy to assess overall performance is insufficient. 
- Specifically, for the corrupted CIFAR benchmark, uncertainty metrics like negative log-likelihood or expected calibration error are more important than test accuracy, but these aspects are not currently considered.
- It seems that all experiments were conducted exclusively with residual networks. It is essential to verify if the proposed analysis and algorithm are applicable to other architecture families as well.

Limitations:
Section 5 addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents introduces a training approach for ensemble learning called SharpBalance to balance sharpness and diversity within ensembles. This paper shows theoretically that SharpBalance achieves a better sharpness-diversity trade-off.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Ensemble learning is an important research direction.
2. Understanding of sharpness and diversity within deep ensembles is important for the study of generalization to both in-distribution and out-of-distribution data.
3. The paper is technically sound.

Weaknesses:
1. Since SharpBalance focuses ""on a diverse subset of the sharpest training data samples"", it may not apply in small datasets where available data is already sparse.
2. Empirical improvement over existing methods is marginal.

Limitations:
Limitations are adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the sharpness and diversity within deep ensembles. Specifically, it identifies the trade-off phenomenon between sharpness and diversity with both theoretical and empirical evidence. Additionally, it proposes a method called SharpBalance, which trains individuals using selective 'sharp' subsets. Conducted experiments have demonstrated the effectiveness of the proposed SharpBalance when applied to deep ensembles.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
There are several strengths in this paper:

- The exploration of sharpness and diversity in deep ensembles is both interesting and novel.

- Sufficient theoretical and empirical evidence has been provided for validation.

- The proposed method is simple, effective, and accompanied by code for verification.

Weaknesses:
However, I still have the following concerns:

 - The evaluation seems a bit weak. The authors should consider comparing with more ensemble baselines.

 - What is the scale of $D_{SAM}^i$ and how does it change during training? Providing some details on this would help in understanding the proposed method.

 - Refer to Line 166: How do the authors train individuals with the full datasets? Are these individuals trained with different initializations?

 - (Optional) As described, the model's generalization is not merely correlated with sharpness, which aligns with some recent advanced SAM variants. Thus, integrating these advanced variants [1][2] with SharpBal would be more beneficial for studying the trade-off between sharpness and diversity.

References:

[1] Random Sharpness-Aware Minimization. In NeurIPS 2022.

[2] Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization. In CVPR 2023.

Limitations:
The authors have provided **Limitations** section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes SharpBalance, that is a method aiming to investigate the relationship between sharpness and diversity for deep ensembles.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- SharpBalance looks quite effective for the out-of-distribution setting. The goal of balancing sharpness and diversity within ensembles is an important idea.  
- Great theoretical analysis

Weaknesses:
- The authors are aware of the paper called “Diversity-Aware Agnostic Ensemble of Sharpness Minimizers” [1], the idea is quite like the proposed paper, they aim to investigate the relations between sharpness and diversity on ensemble learning. I suggest the authors to discuss the main differences between both.    

[1] Anh Bui, Vy Vo, Tung Pham, Dinh Phung and Trung Le, Diversity-Aware Agnostic Ensemble of Sharpness Minimizers, arXiv:2403.13204. 

- Regarding the baselines the authors only compare SharpBalance with SAM. Nevertheless, newer, and stronger baselines like GSAM [2] and OBF [3] should also be benchmarked since they are the current state-of-the-art. 

[2] Zhuang, J., Gong, B., Yuan, L., Cui, Y., Adam, H., Dvornek, N., Tatikonda, S., Duncan, J., and Liu, T. Surrogate gap minimization improves sharpness-aware training. arXiv preprint arXiv:2203.08065, 2022. 

[3] Vani, A; Tung, F; Oliveira G; Sharifi H. Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics, International Conference on Machine Learning (ICML) 2024.  

- Another point to improve are the datasets. I strongly suggest the authors to benchmark with at least a couple large scale datasets. Options are ImageNet-V1 [4] for training and ImageNet-Real [5] and ImageNet-V2 [6] for testing, ImageNet-R [7] for out-of-distribution robustness benchmark and ImageNet-Sketch [8].  

[4] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. IEEE, 2009 

[5] Beyer, L., He ́naff, O. J., Kolesnikov, A., Zhai, X., and Oord, A. v. d. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020. 

[6] Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do imagenet classifiers generalize to imagenet? In Interna- tional conference on 
machine learning, pp. 5389–5400. PMLR, 2019. 

[7] Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340–8349, 2021. 

[8] Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pp. 10506–10518, 2019.

Limitations:
- The proposed method is not a significant improvement for ID scenarios as the authors claim. I would tone that down over the whole text. Maybe clearly state that SharpBalance is indeed superior to OOD scenarios and competitive on ID settings.  

- I would not claim the phenomenon called sharpness-diversity trade-off is a discovery, paper [1] is addressing the same phenomena and it was publicly available on arxiv before submission and on openreview since the beginning of the year.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Ensemble methods and sharpness-aware optimization techniques are well-known strategies for improving generalization. This work identifies a trade-off between sharpness and diversity, observing that reducing sharpness can diminish diversity and harm ensemble performance. Through theoretical and empirical analysis of this sharpness-diversity trade-off, the authors present SharpBalance, an algorithm for training ensembles with sharpness-aware solutions without sacrificing diversity. Evaluation results on CIFAR-10/100, TinyImageNet, and their corrupted variants confirm the effectiveness of SharpBalance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Ensemble methods and sharpness-aware optimization techniques are both prominent approaches for improving generalization. The aim of this work, which combines these two approaches, is well-motivated.
- While the theoretical analysis uses the variance metric to indicate diversity, the experimental results show consistent trends across different diversity metrics. It suggests that the proposed analysis is widely applicable to the general concept of diversity.
- Extensive empirical results effectively validate the theoretical analysis. The summary plots of the results are generally highly readable.

Weaknesses:
- The evaluation results are centered exclusively on classification accuracy; since ensembling usually highlights both predictive accuracy and uncertainty, relying solely on accuracy to assess overall performance is insufficient. 
- Specifically, for the corrupted CIFAR benchmark, uncertainty metrics like negative log-likelihood or expected calibration error are more important than test accuracy, but these aspects are not currently considered.
- It seems that all experiments were conducted exclusively with residual networks. It is essential to verify if the proposed analysis and algorithm are applicable to other architecture families as well.

Limitations:
Section 5 addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wJAF8TGVUG;"REVIEW 
Summary:
The paper introduces S-MolSearch, a framework for ligand-based virtual screening in drug discovery that addresses the challenges of limited and noisy binding affinity data. By utilizing molecular 3D information and semi-supervised contrastive learning, S-MolSearch processes both labeled and unlabeled data to train molecular structural encoders and generate soft labels for the unlabeled data, drawing on inverse optimal transport principles. The framework outperforms existing structure-based and ligand-based virtual screening methods, as evidenced by its superior performance on the LIT-PCBA and DUD-E benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Well-written
- Well-organized experimental settings and comparison methods

Weaknesses:
- There is a lack of discussion on the reasons behind the performance differences and improvements, with only numerical comparisons of the experimental results.
- There is insufficient experimentation and consideration regarding the time required for virtual screening.
- There are no results for experimental metrics such as AUROC or BEDROC, which were used in previous studies.

Limitations:
The paper addressed limitations and potential social impacts in Appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ""S-MolSearch,"" a semi-supervised contrastive learning framework designed for ligand-based virtual screening in drug discovery. This framework uniquely leverages labeled binding affinity information to produce soft labels for unlabeled molecules, integrating 3D molecular structures and binding affinity data. The paper also proposes a novel semi-supervised learning paradigm that combines contrastive learning with Inverse Optimal Transport (IOT).

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The supervision idea is novel and useful, and the target application is very impactful with broad implications.
2. The paper is well-written and the experiments are comprehensive.

Weaknesses:
1.	Memory Consumption Concerns: The model employs a parallel architecture with two f_\theta encoders and one g_\phi encoder, based on the Uni-Mol framework. Although utilizing pretrained models has shown significant performance benefits, the paper should address potential memory management strategies, especially for future applications involving molecules with a greater number of atoms.
2.	Utilization of 3D Structures: The paper promotes a novel semi-supervised contrastive learning paradigm, yet the core contribution does not seem to revolve around the innovative use of 3D structures, as this capability primarily stems from the Uni-Mol architecture. It would be beneficial if the authors could clarify any specific enhancements made to ensure the effective preservation and utilization of geometric information within the model. Absent such enhancements, clearer distinctions should be made regarding the role of 3D structures to prevent misconceptions about the paper presenting a new geometric deep learning technique.
3.	Clarity in Section 3.4: The explanation of how $\Gamma$, which approximates the distribution of $C$ under constraints from $U(p,q)$, relates to the continuous optimal transport problem is not clear. Moreover, the motivation and necessity of soft labels, beyond experimental justifications, needs further elaboration. The section would benefit from additional visual aids or high-level descriptions, akin to the clarifications provided in sections 3.3 and 3.5, to aid in comprehension.
4.	Component Efficacy in Table 3: There appears to be a discrepancy in the impact of model components across different benchmarks—soft labels are pivotal for DUD-E, whereas pretraining is more crucial for LIT-PCBA, with soft labels showing minimal importance. Insights into this inconsistency would be valuable. Furthermore, an evaluation of how the Uni-Mol encoder alone performs on these tasks would provide additional context on the effectiveness of the proposed enhancements

Minor points and typos:
L153-154 is not clear.
L162: It would be beneficial to include illustrations of $M_{sup}$ in the figures for clarity.
Formula 2 and L 168: It is better to give intuitive explanations of $1_N$.
L184: Inconsistent notation. $g(\psi)$ or $g_\psi$?
L281: Misplaced comma.

Limitations:
The focus of the paper is predominantly on molecule binding affinity, heavily relying on a pretrained encoder. This reliance could limit the model's applicability across a broader spectrum of bioinformatics data. A more detailed discussion on the dependency on pretrained models and potential strategies to mitigate this limitation would enhance the paper's breadth and applicability.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed a Ligand-based Virtual Screening method S-MolSearch. which can  leverages molecular 3D information and affinity information in semi-supervised contrastive learning.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The method is able to leverage both labeled and unlabeled data simultaneously and achieves excellent performance on DUDE and Lit-PCBA benchmarks.
2. The approach of using the principles of inverse optimal transport for semi-supervised learning is quite innovative and worth adopting.
3. The ablation experiments are sufficient, and the experimental section is quite robust.

Weaknesses:
1. In the method section, it is unclear to me whether during inference only encoder$g_{\psi}$ is used, or both $\psi$ and $f_{\theta}$ are used  simultaneously?
2.  If the application scenario involves a newly provided protein without reference molecules, how should ligand-based virtual screening methods handle this situation?

Limitations:
S-MolSearch predominantly focuses on the molecular affinity data, omitting broader biochemical interactions, which suggests a potential area for improvement.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method for ligand-based virtual screening based on contrastive learning and inverse optimal transport. Two molecule encoders are trained. The first encoder is trained using a contrastive loss function on the ChEMBL data by pairing compounds that are active toward the same protein, and compounds active toward different targets are treated as negative pairs. Next, the second encoder is trained by using the pseudo-labels produced by the first model. The proposed model is tested on two benchmark datasets, DUD-E and LIT-PCBA. Additionally, an ablation study is conducted, and the impact of the labeled data scale is visualized.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Originality:
- The approach seems to be novel. I have not found any similar papers that use optimal transport for the ligand-based virtual screening task.

Quality:
- The theory described in the paper is formally proven in the Appendix.
- The proposed method obtains excellent results in both tested benchmarks.
- The quality of the learned representation is demonstrated in Figure 2.

Clarity:
- The paper is written clearly and is easy to follow.
- Figure 1 shows the idea of the model very clearly.

Significance:
- The presented method is an interesting and effective way to utilize all the available public data to build a strong model for ligand-based virtual screening.

Weaknesses:
Quality:
- It would be interesting to see some qualitative examples of molecules that were found to be similar to the active compounds in the virtual screening process. Do the trained similarities correlate with the Tanimoto similarity?

Clarity:
- Does the “sup” subscript in Section 3.4 correspond to the “label” subscript in Proposition 1? What is the difference between these two sets?

Minor comments:
- A typo in line 151, “we employs InfoNCE.”
- In line 183, something is missing before “1”.

Limitations:
The limitations have been described.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces S-MolSearch, a framework for ligand-based virtual screening in drug discovery that addresses the challenges of limited and noisy binding affinity data. By utilizing molecular 3D information and semi-supervised contrastive learning, S-MolSearch processes both labeled and unlabeled data to train molecular structural encoders and generate soft labels for the unlabeled data, drawing on inverse optimal transport principles. The framework outperforms existing structure-based and ligand-based virtual screening methods, as evidenced by its superior performance on the LIT-PCBA and DUD-E benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Well-written
- Well-organized experimental settings and comparison methods

Weaknesses:
- There is a lack of discussion on the reasons behind the performance differences and improvements, with only numerical comparisons of the experimental results.
- There is insufficient experimentation and consideration regarding the time required for virtual screening.
- There are no results for experimental metrics such as AUROC or BEDROC, which were used in previous studies.

Limitations:
The paper addressed limitations and potential social impacts in Appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ""S-MolSearch,"" a semi-supervised contrastive learning framework designed for ligand-based virtual screening in drug discovery. This framework uniquely leverages labeled binding affinity information to produce soft labels for unlabeled molecules, integrating 3D molecular structures and binding affinity data. The paper also proposes a novel semi-supervised learning paradigm that combines contrastive learning with Inverse Optimal Transport (IOT).

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The supervision idea is novel and useful, and the target application is very impactful with broad implications.
2. The paper is well-written and the experiments are comprehensive.

Weaknesses:
1.	Memory Consumption Concerns: The model employs a parallel architecture with two f_\theta encoders and one g_\phi encoder, based on the Uni-Mol framework. Although utilizing pretrained models has shown significant performance benefits, the paper should address potential memory management strategies, especially for future applications involving molecules with a greater number of atoms.
2.	Utilization of 3D Structures: The paper promotes a novel semi-supervised contrastive learning paradigm, yet the core contribution does not seem to revolve around the innovative use of 3D structures, as this capability primarily stems from the Uni-Mol architecture. It would be beneficial if the authors could clarify any specific enhancements made to ensure the effective preservation and utilization of geometric information within the model. Absent such enhancements, clearer distinctions should be made regarding the role of 3D structures to prevent misconceptions about the paper presenting a new geometric deep learning technique.
3.	Clarity in Section 3.4: The explanation of how $\Gamma$, which approximates the distribution of $C$ under constraints from $U(p,q)$, relates to the continuous optimal transport problem is not clear. Moreover, the motivation and necessity of soft labels, beyond experimental justifications, needs further elaboration. The section would benefit from additional visual aids or high-level descriptions, akin to the clarifications provided in sections 3.3 and 3.5, to aid in comprehension.
4.	Component Efficacy in Table 3: There appears to be a discrepancy in the impact of model components across different benchmarks—soft labels are pivotal for DUD-E, whereas pretraining is more crucial for LIT-PCBA, with soft labels showing minimal importance. Insights into this inconsistency would be valuable. Furthermore, an evaluation of how the Uni-Mol encoder alone performs on these tasks would provide additional context on the effectiveness of the proposed enhancements

Minor points and typos:
L153-154 is not clear.
L162: It would be beneficial to include illustrations of $M_{sup}$ in the figures for clarity.
Formula 2 and L 168: It is better to give intuitive explanations of $1_N$.
L184: Inconsistent notation. $g(\psi)$ or $g_\psi$?
L281: Misplaced comma.

Limitations:
The focus of the paper is predominantly on molecule binding affinity, heavily relying on a pretrained encoder. This reliance could limit the model's applicability across a broader spectrum of bioinformatics data. A more detailed discussion on the dependency on pretrained models and potential strategies to mitigate this limitation would enhance the paper's breadth and applicability.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed a Ligand-based Virtual Screening method S-MolSearch. which can  leverages molecular 3D information and affinity information in semi-supervised contrastive learning.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The method is able to leverage both labeled and unlabeled data simultaneously and achieves excellent performance on DUDE and Lit-PCBA benchmarks.
2. The approach of using the principles of inverse optimal transport for semi-supervised learning is quite innovative and worth adopting.
3. The ablation experiments are sufficient, and the experimental section is quite robust.

Weaknesses:
1. In the method section, it is unclear to me whether during inference only encoder$g_{\psi}$ is used, or both $\psi$ and $f_{\theta}$ are used  simultaneously?
2.  If the application scenario involves a newly provided protein without reference molecules, how should ligand-based virtual screening methods handle this situation?

Limitations:
S-MolSearch predominantly focuses on the molecular affinity data, omitting broader biochemical interactions, which suggests a potential area for improvement.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method for ligand-based virtual screening based on contrastive learning and inverse optimal transport. Two molecule encoders are trained. The first encoder is trained using a contrastive loss function on the ChEMBL data by pairing compounds that are active toward the same protein, and compounds active toward different targets are treated as negative pairs. Next, the second encoder is trained by using the pseudo-labels produced by the first model. The proposed model is tested on two benchmark datasets, DUD-E and LIT-PCBA. Additionally, an ablation study is conducted, and the impact of the labeled data scale is visualized.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Originality:
- The approach seems to be novel. I have not found any similar papers that use optimal transport for the ligand-based virtual screening task.

Quality:
- The theory described in the paper is formally proven in the Appendix.
- The proposed method obtains excellent results in both tested benchmarks.
- The quality of the learned representation is demonstrated in Figure 2.

Clarity:
- The paper is written clearly and is easy to follow.
- Figure 1 shows the idea of the model very clearly.

Significance:
- The presented method is an interesting and effective way to utilize all the available public data to build a strong model for ligand-based virtual screening.

Weaknesses:
Quality:
- It would be interesting to see some qualitative examples of molecules that were found to be similar to the active compounds in the virtual screening process. Do the trained similarities correlate with the Tanimoto similarity?

Clarity:
- Does the “sup” subscript in Section 3.4 correspond to the “label” subscript in Proposition 1? What is the difference between these two sets?

Minor comments:
- A typo in line 151, “we employs InfoNCE.”
- In line 183, something is missing before “1”.

Limitations:
The limitations have been described.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wIE991zhXH;"REVIEW 
Summary:
This paper considers bandits with preference feedback. It first constructs a novel confidence set that covers the ground truth with high probability. Then from a Stackelberg game perspective, it proposes an efficient algorithm that enjoys tighter regret bound than SOTA.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The technique used to construct the confidence set is interesting. The resulting confidence set is tighter.
2. The Stackelberg game perspective is interesting, and allows the author(s) to design the algorithm with better exploration-exploitation trade-off as demonstrated in the experiment.

Weaknesses:
The major concern is the practical applicability of the algorithm. Seems that the proposed algorithm can hardly scale up to a higher dimension (e.g., dimension equals to 7). In the proposed algorithm, a complicated sequential optimization problem needs to be solved. Notably, the experiment only considers two-dimensional problem, in sharp contrast to recent works (e.g., 12-dimensional problem considered in Xu et al. [2024]).

Limitations:
Some major comments:
1. The paper discusses the comparisons to [Xu et al. 2024]. Seems that the theoretical gain of $T^{\frac{1}{4}}$ mainly comes from tighter confidence set rather than the design of the algorithm. If the algorithm POP-BO in [Xu et al. 2024] is equipped with this tighter confidence set, it could also achieve the same regret bound. To see which algorithm is empirically better, it would be interesting to compare the proposed algorithm with POP-BO equipped with the tighter confidence set in this paper.

Besides the points mentioned above, here are some minor comments:
1. In line 109, I guess it should be {$0, 1$} instead of $[0, 1]$.
2. In line 178 to 179, typo in ""ridge estimator estimator"".
3. In line 574, abuse of the notation $s$.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers novel game-theoretic acquisition function for pairwise action selection with preference feedback. It is tailored to the setting with infinite domains and nonlinear kernelized rewards. The preference-based confidence sequences for kernelized utility functions are shown to be tight and anytime valid. The proposed algorithm MAXMINLCB is shown to satisfy a sublinear regret. Various simulations were conducted to showcase the advantage of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Although preference-based bandit optimization with linear utility functions is fairly well understood, such approaches cannot capture real-world problems with complex nonlinear utility functions. This paper aims to close this gap. The considered problem is timing and interesting. 

2. The technical contribution is non-trivial. Although there have been attempts to prove convergence of kernelized algorithms for preference-based bandits, such works employ a regression likelihood model which requires them to assume that both the utility and the probability of preference lie in an RKHS. Moreover, a sample-efficient algorithm is lacking for such approaches. In contrast, this work uses a kernelized logistic negative log-likelihood loss to infer the utility function, and provide confidence sets for its minimizer.

3. Some theoretical result, like Kernelized Logistic Confidence Sequences in Theorem 2, is also of independent interest. 

4. In spite of a theoretical paper, it is well written and is easy to follow.

Weaknesses:
1. In practice, how to determine the hyper-parameters like $\gamma_t$, $L$, and $B$ in (5)? Is there any data-driven way to select them?

2. In the main Theorem 6, the regret bound is $\gamma_T^{D}\sqrt{T}$. The term $\gamma_T^{D}$ is the T-step information gain of kernel, which is also a function of $T$. The authors claim that this rate improves that of Xu et al. (2024) by a factor of $T^{1/4}$. However, the cumulative regret bound in Theorem 5.2 of Xu et al. (2024) is of a similar order. Xu et al. (2024) also provided explicit regret upper bounds for various common kernels in Theorem 5.5. Hence, it is also interesting to provide an explicit form of $\gamma_T^{D}$ for some common kernels, and to compare these regret upper bounds in a fair way. 

3. In Figure 1, the authors presented the result for the Ackley function, which shows a clear advantage of the proposed method. However, in more extensive simulations (e.g., Matyas function in Figure 6, ) in the appendix, the proposed method is outperformed by the competitors. It is helpful to provide some discussion on these results, and offer some insights on when the proposed method would work well. This is helpful for practitioners.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper examines the problem of bandit optimization with preference feedback in large domains and nonlinear (kernelized) rewards. It introduces MAXMINLCB, which adopts a game-theoretic approach to action selection under comparative feedback. Additionally, it proposes kernelized preference-based confidence sets, which can be utilized in related problems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
(1) Rather than jointly selecting the arms in dueling bandits, the proposed method jointly optimizes both actions by choosing them as the equilibrium of a two-player zero-sum Stackelberg game. This approach enables a more efficient exploration/exploitation trade-off.

(2) The regret guarantee presented in this paper is tighter by a factor of  $T^{1/4}$ compared to Xu et al. (2024).

Weaknesses:
(1) Although the paper uses a kernelized logistic model to approximate the rewards, this approach may remain too simplistic for capturing the complexity of rewards in real-world applications.

(2) The paper lacks a comparison in the experiments with the related work by Xu et al. (2024).

(3)  In real applications, it is more common to rank between two state-action pairs. However, the paper does not consider contextual information and solely focuses on the multi-armed setting, which is less interesting and useful.

Limitations:
Please see weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers bandit optimization with preference feedback over continuous action spaces and kernelized reward function. The goal in this problem is to minimize the dueling regret against an optimal action over a finite time-horizon. Previous works on this problem are either restricted to finite action spaces or linear reward functions. The proposed algorithm casts the problem as kernalized logistic regression and designs confidence sets for the relative preference between two actions. It then proposes an action selection strategy based on a game theoretic Leader-Follower formulation that utilizes these confidence intervals. The paper provides a regret bound as well as empirical evaluation for the proposed algorithm.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The main contributions of the paper are two-fold:

1. Expanding the existing literature on dueling bandits by studying kernelized reward functions under infinite and continuous action sets. This requires new techniques to bound the confidence intervals. 

2. Proposing a principled game-theoretic approach to action selection in dueling bandits that can be of further interest.

In my opinion these are two important contributions to the literature. Since these ideas are likely to be relevant to other learning problems with preference feedback such as RLHF, I think that the results in this paper have a good scope. The paper is well-written and the contributions are clear.

Weaknesses:
Experimental evaluation can include other algorithms that are known to perform better than RUCB such as RMED (Komiyama et al., 2015) and Double Thompson Sampling (Wu and Liu, 2016).

Limitations:
The paper can include a section of limitations of the current work in terms of social impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers bandits with preference feedback. It first constructs a novel confidence set that covers the ground truth with high probability. Then from a Stackelberg game perspective, it proposes an efficient algorithm that enjoys tighter regret bound than SOTA.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The technique used to construct the confidence set is interesting. The resulting confidence set is tighter.
2. The Stackelberg game perspective is interesting, and allows the author(s) to design the algorithm with better exploration-exploitation trade-off as demonstrated in the experiment.

Weaknesses:
The major concern is the practical applicability of the algorithm. Seems that the proposed algorithm can hardly scale up to a higher dimension (e.g., dimension equals to 7). In the proposed algorithm, a complicated sequential optimization problem needs to be solved. Notably, the experiment only considers two-dimensional problem, in sharp contrast to recent works (e.g., 12-dimensional problem considered in Xu et al. [2024]).

Limitations:
Some major comments:
1. The paper discusses the comparisons to [Xu et al. 2024]. Seems that the theoretical gain of $T^{\frac{1}{4}}$ mainly comes from tighter confidence set rather than the design of the algorithm. If the algorithm POP-BO in [Xu et al. 2024] is equipped with this tighter confidence set, it could also achieve the same regret bound. To see which algorithm is empirically better, it would be interesting to compare the proposed algorithm with POP-BO equipped with the tighter confidence set in this paper.

Besides the points mentioned above, here are some minor comments:
1. In line 109, I guess it should be {$0, 1$} instead of $[0, 1]$.
2. In line 178 to 179, typo in ""ridge estimator estimator"".
3. In line 574, abuse of the notation $s$.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers novel game-theoretic acquisition function for pairwise action selection with preference feedback. It is tailored to the setting with infinite domains and nonlinear kernelized rewards. The preference-based confidence sequences for kernelized utility functions are shown to be tight and anytime valid. The proposed algorithm MAXMINLCB is shown to satisfy a sublinear regret. Various simulations were conducted to showcase the advantage of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Although preference-based bandit optimization with linear utility functions is fairly well understood, such approaches cannot capture real-world problems with complex nonlinear utility functions. This paper aims to close this gap. The considered problem is timing and interesting. 

2. The technical contribution is non-trivial. Although there have been attempts to prove convergence of kernelized algorithms for preference-based bandits, such works employ a regression likelihood model which requires them to assume that both the utility and the probability of preference lie in an RKHS. Moreover, a sample-efficient algorithm is lacking for such approaches. In contrast, this work uses a kernelized logistic negative log-likelihood loss to infer the utility function, and provide confidence sets for its minimizer.

3. Some theoretical result, like Kernelized Logistic Confidence Sequences in Theorem 2, is also of independent interest. 

4. In spite of a theoretical paper, it is well written and is easy to follow.

Weaknesses:
1. In practice, how to determine the hyper-parameters like $\gamma_t$, $L$, and $B$ in (5)? Is there any data-driven way to select them?

2. In the main Theorem 6, the regret bound is $\gamma_T^{D}\sqrt{T}$. The term $\gamma_T^{D}$ is the T-step information gain of kernel, which is also a function of $T$. The authors claim that this rate improves that of Xu et al. (2024) by a factor of $T^{1/4}$. However, the cumulative regret bound in Theorem 5.2 of Xu et al. (2024) is of a similar order. Xu et al. (2024) also provided explicit regret upper bounds for various common kernels in Theorem 5.5. Hence, it is also interesting to provide an explicit form of $\gamma_T^{D}$ for some common kernels, and to compare these regret upper bounds in a fair way. 

3. In Figure 1, the authors presented the result for the Ackley function, which shows a clear advantage of the proposed method. However, in more extensive simulations (e.g., Matyas function in Figure 6, ) in the appendix, the proposed method is outperformed by the competitors. It is helpful to provide some discussion on these results, and offer some insights on when the proposed method would work well. This is helpful for practitioners.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper examines the problem of bandit optimization with preference feedback in large domains and nonlinear (kernelized) rewards. It introduces MAXMINLCB, which adopts a game-theoretic approach to action selection under comparative feedback. Additionally, it proposes kernelized preference-based confidence sets, which can be utilized in related problems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
(1) Rather than jointly selecting the arms in dueling bandits, the proposed method jointly optimizes both actions by choosing them as the equilibrium of a two-player zero-sum Stackelberg game. This approach enables a more efficient exploration/exploitation trade-off.

(2) The regret guarantee presented in this paper is tighter by a factor of  $T^{1/4}$ compared to Xu et al. (2024).

Weaknesses:
(1) Although the paper uses a kernelized logistic model to approximate the rewards, this approach may remain too simplistic for capturing the complexity of rewards in real-world applications.

(2) The paper lacks a comparison in the experiments with the related work by Xu et al. (2024).

(3)  In real applications, it is more common to rank between two state-action pairs. However, the paper does not consider contextual information and solely focuses on the multi-armed setting, which is less interesting and useful.

Limitations:
Please see weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers bandit optimization with preference feedback over continuous action spaces and kernelized reward function. The goal in this problem is to minimize the dueling regret against an optimal action over a finite time-horizon. Previous works on this problem are either restricted to finite action spaces or linear reward functions. The proposed algorithm casts the problem as kernalized logistic regression and designs confidence sets for the relative preference between two actions. It then proposes an action selection strategy based on a game theoretic Leader-Follower formulation that utilizes these confidence intervals. The paper provides a regret bound as well as empirical evaluation for the proposed algorithm.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The main contributions of the paper are two-fold:

1. Expanding the existing literature on dueling bandits by studying kernelized reward functions under infinite and continuous action sets. This requires new techniques to bound the confidence intervals. 

2. Proposing a principled game-theoretic approach to action selection in dueling bandits that can be of further interest.

In my opinion these are two important contributions to the literature. Since these ideas are likely to be relevant to other learning problems with preference feedback such as RLHF, I think that the results in this paper have a good scope. The paper is well-written and the contributions are clear.

Weaknesses:
Experimental evaluation can include other algorithms that are known to perform better than RUCB such as RMED (Komiyama et al., 2015) and Double Thompson Sampling (Wu and Liu, 2016).

Limitations:
The paper can include a section of limitations of the current work in terms of social impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wHFaAH3E8z;"REVIEW 
Summary:
This paper introduces FasMe, a meta-learning approach for efficient precision matrix estimation in small sample settings. By leveraging meta-knowledge and maximum determinant matrix completion, FasMe reduces sample size requirements and improves computational efficiency. Experimental results show FasMe to be significantly faster and more accurate than existing methods, particularly in low-data environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) Paper investigates a key issue in precision matrix estimation and proposes a reasonable method to address the problem.

2) Paper provides thorough theoretical and experimental analyses to justify the method’s ability to reduce the sample requirement and enhance learning efficiency.

3) Paper has good representation.

Weaknesses:
I have few doubts about the method and experiments presented in the article.

Limitations:
nan

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a method to estimate sparse precision matrices from few samples. Theoretical properties of the proposed method are studied, and experiments on synthetic and brain fMRI data are presented.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths:
* The paper is overall well written, and fairly easy to follow and comprehend.
* Theoretical guarantees for sub-Gaussian distributed random variables are presented, and they seem to be novel contribution. 
* The experiments on the synthetic dataset clearly demonstrate improvement over the relevant competing methods.

Weaknesses:
Weaknesses:
* Currently quantitative results are presented for synthetic data. It would be nice to see more quantitative evaluations on benchmark datasets.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a meta-learning method for estimating the precision matrix on a new task with small data.
The proposed method uses common edges estimated from multiple auxiliary datasets as meta-knowledge. Then, it estimates the precision matrix on the new task, assuming its true edges contain all the estimated common edges (meta-knowledge). Some theoretical guarantees are also provided.
Experiments with synthetic and real-world datasets show the effectiveness and efficiency of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well-written and easy to follow.
- Concrete algorithm and its theoretical guarantees are presented (but, I didn't read their proof).
- Strong performance in terms of both accuracy and efficiency in the experiments.

Weaknesses:
- As the authors stated in the Limitation section, the assumption of Eq. 8 might not be held in general machine learning tasks, although it fits well in biological domains.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces FasMe, a meta-learning approach for efficient precision matrix estimation in small sample settings. By leveraging meta-knowledge and maximum determinant matrix completion, FasMe reduces sample size requirements and improves computational efficiency. Experimental results show FasMe to be significantly faster and more accurate than existing methods, particularly in low-data environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) Paper investigates a key issue in precision matrix estimation and proposes a reasonable method to address the problem.

2) Paper provides thorough theoretical and experimental analyses to justify the method’s ability to reduce the sample requirement and enhance learning efficiency.

3) Paper has good representation.

Weaknesses:
I have few doubts about the method and experiments presented in the article.

Limitations:
nan

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a method to estimate sparse precision matrices from few samples. Theoretical properties of the proposed method are studied, and experiments on synthetic and brain fMRI data are presented.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths:
* The paper is overall well written, and fairly easy to follow and comprehend.
* Theoretical guarantees for sub-Gaussian distributed random variables are presented, and they seem to be novel contribution. 
* The experiments on the synthetic dataset clearly demonstrate improvement over the relevant competing methods.

Weaknesses:
Weaknesses:
* Currently quantitative results are presented for synthetic data. It would be nice to see more quantitative evaluations on benchmark datasets.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a meta-learning method for estimating the precision matrix on a new task with small data.
The proposed method uses common edges estimated from multiple auxiliary datasets as meta-knowledge. Then, it estimates the precision matrix on the new task, assuming its true edges contain all the estimated common edges (meta-knowledge). Some theoretical guarantees are also provided.
Experiments with synthetic and real-world datasets show the effectiveness and efficiency of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well-written and easy to follow.
- Concrete algorithm and its theoretical guarantees are presented (but, I didn't read their proof).
- Strong performance in terms of both accuracy and efficiency in the experiments.

Weaknesses:
- As the authors stated in the Limitation section, the assumption of Eq. 8 might not be held in general machine learning tasks, although it fits well in biological domains.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wGjSbaMsop;"REVIEW 
Summary:
This paper explores the possibility of boosting the recommendation of a song in an automatic playlist continuation system by using a collective strategy of adding the song into the training playlists of the APC at a specific position. The paper shows that adopting a strategy that targets low frequency context makes it possible to very significantly boost the exposition of the song in the output of the APC.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
1: poor

Strengths:
- Very interesting idea which is novel in the domain of music recommendation
- Quite surprising experimental result that is worth sharing (possibility of boosting exposition of a song with the DirLof strategy).

Weaknesses:
- Limited scope:
    - The idea is tested in the limited context of automatic playlist continuation and is likely not adaptable to broader applications.
    - Only one APC model was tested, so the results may be specific to this model. It would have been a good idea to test the impact of the collective action on other models (such as the baseline proposed in the reference paper for the APC model).  Also, it’s unclear whether the effect would be robust to hyperparameter changes in the APC model.
    - the method is tested in a static context (not in the real world), so the actual impact of the method on usage (for instance, whether a user exposed through the APC to a song boosted by the collective strategy would listen to it or not) is not tested (this would require access to the production of a streaming music service, though)
- Very limited insights on the design of the efficient strategy (DirLof) and why it works. The presentation of the strategy is actually quite unclear, while it’s central to the paper.
- There may be other simpler baselines that would be worth testing. The paper shows that inserting the song at the end of the playlists is less performance than inserting the song randomly, which suggests that inserting it earlier in the playlist sequence may help. So a baseline that would always insert the song at the beginning of the sequence could have decent results (the first place is likely to be avoided as it would mean that the song would never appear as the target in the training of the APC, but low positions that appear regularly as targets in the training could be considered).

Limitations:
- As mentioned in the weaknesses, there are other simple baselines that would be worth testing. That would help support the claim that the specially designed DirLof strategy is actually efficient.
- Also the authors should comment on the possible transferability of their results to other APC models, and to modifications of hyperparameters of the model.

On the ethical side, the method is a bit too much presented as a way for artists to attack a recommender system, somewhat rerank their songs and make money out of it, which has some important ethical implications in terms of fairness among artists: As the payment system of most streaming services is based on subscription (so a fixed amount of money), artificially boosting an artist comes at the detriment of other artists.
I would then rather avoid this message of ""opportunity for artists"" in the paper (and even in the title) and turn it instead as a warning to music streaming platforms that recommender systems may lack robustness to attacks and that they should be aware of it and take action. I think presenting the paper in this second way would solve this ethical issue, but the current message is, to me, ethically borderline.

Minor comments
- The authors claim that “most large platforms have shifted from relying on collaborative-based models for APC to building deep learning based recommenders …” but 1) the references are only Spotify based 2) the references are research papers which doesn’t mean that the shift actually happened and that platforms no longer use collaborative-based models part in their complex architecture.

- In equation (1):
    - The authors likely want to specify that the recommended song shouldn’t be part of the playlist i.e S’ **∩** p = **∅** in the argmax
    - The argmax actually corresponds to the top K elements in terms of similarity, which is quite straightforward. But the sum in the equation makes it not very clear at first read. It could be worth it to state it explicitly before the equation.
- There are several notations that use the letter h (song embedding, playlist embedding, playlist mapping). I think using different letters may help make things clearer.
- In Figure 3, it seems like s* is subtituted to s_i (there is no s_i in (b)) while just before equation 4, it’s said that only insertions are considered.
- “*Existing data poisoning techniques, for example, operate in different settings, not complying with Constraint 1 and typically assuming white-box access to the model or involving test-time profile manipulations.*” ⇒ this needs references.
- *“Thus, collective action can make a tremendous difference for these artists: suppose an artist’s song is streamed 10,000 times, yielding a revenue for the artist of $40 for royalties of $0.004 per stream [32]; an amplification of 25 would increase this revenue to $1, 000.”* I understand this is purely illustrative, but the figures won’t reflect much the truth. First 0.004$ per stream is an average that depends on the platform, but also on the kind of registration of the user and on the country of the user with quite important variations. Second, several platforms (Spotify, Deezer…) are shifting to a payment model were all streams don’t have the same value, especially recommended streams are “discounted”, and some “real” artist may get a boost. Once again, I get it’s for illustrative purpose, but it brings confusion on how music payment system works, and then should be avoided in my opinion.
- *“Moreover, when considering s* as a relevant recommendation, collective action even enhances the system’s performance”* Isn’t this completely obvious? if presentation of s* is boosted (which happens, given the amplification), other recommendations are barely affected (as shown on figure 6, solid line), and you consider s* a valid recommendation, then for sure recommendation metrics will increase, won’t they?

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
They proposed a strategy for the streaming platform users to collectively act in order to promote targeted songs. The promotion efficacy is measured by the targeted songs' recommendation frequency boost at testing time. This strategy is approved to be effective through simulation experiment.  Another finding is that this strategy has minimum impact to the performance of the recommendation system as a whole, i.e., by preserving user experience to other non-targeted songs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* This is a novel idea, presented in an interesting domain with a good amount of related work.
* The writing of the paper is clear. The motivation is sound. 
* The experiment performed by the author successfully verified the efficacy of the collective strategy.

Weaknesses:
* Limited scope. See limitations below.
* Lack of technical novelty and contribution. The evaluation result would be a good report but I would recommend the author to seek publication in a different conference.

Limitations:
The strategy could be effective but it is built on top of the assumption that the serving platform has deployed minimum control against collective behavior. As I am aware, there are various user-side anomaly detection mechanisms usually deployed in production in streaming services like what is described in the paper. As an example, there could be real time monitoring of such collective behaviors. Such an anomaly detection system could be constructed by building a user-entity graph. In this use case, we could use artist or songs as the entity. When in a short period of time, there are bursty events that are related to an entity that happens (user promoting a song in their playlist), it could be an important indicator that something unusual happens (because this is not a popular song, it does not receive so many promotes on average). A follow-up action in the system could tag those relevant data as ""spurious"" thus they never go to the training data of the transformer model (for continuous training). As a result, the collective action could have a significantly less effect when such monitoring is turned on; or depending on what threshold they are set up.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper shows that strategic collective action by a small fraction of the population can lead to significant amplification of a particular song in a recommender system.  The authors propose two strategies for the collective (for a transformer-based song recommender) that achieve this amplification.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The setup is original and focuses on strategic collective action by a fraction of users in a recommender system and how this can increase a target song’s reach.
- The performance loss for the platform is negligible, the collective’s strategies are not adversarial (for e.g. fake profiles, artificial perturbations) and based on 1-edit distance to the original playlist. The paper shows that recommendations are largely preserved.

Weaknesses:
- The experiments follow the MF-Transformer in [7], to make the paper self-contained it would be beneficial to have a description of $\phi(.)$ and of $g(.)$ and the loss function in Section 2.1 or the Appendix. 
- I found the strategies in Sec 3.2 hard to parse, perhaps a figure showing the original playlist and the possible changes a user in the collective can do under the two strategies would be helpful. 
- Minor:  I think the notation h(.) is overloaded for the song/playlist embedding in 2.1 and for the strategy mapping which inserts s* into a playlist. 	Also, fig 6 could use different colors for the different strategies.

Limitations:
The authors discuss limitations in Appendix  A.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This research work proposes a novel solution to promote songs on music streaming platforms strategically. 

Under the following assumptions:

1. Fans can collaborate to promote a specific song by collectively reordering playlists.
2. The visibility of a song in a playlist affects its recommendation frequency.
3. Users are influenced by the position of songs in playlists when making listening choices.
4. The impact of collective action on song visibility is measurable and significant.

The authors suggest that fans strategically reorder playlists to promote a targeted song, thereby increasing its visibility in the recommender system. By leveraging algorithmic collective action, even small groups of fans can substantially impact the recommendation frequency of the promoted song. This strategy aims to enhance the visibility (capability of being discovered) of songs and artists, which will benefit both fans and musicians in the music streaming industry. 

 The evaluation focuses on quantifying the amplification of recommendations achieved by strategically placing songs in playlists, using metrics such as recommendation probability and change in the number of recommendations for a song. The evaluation also includes the impact on the recommendations of other songs and the overall performance of the recommender system. The analysis of results reveals that the collective action strategies can lead to a substantial increase in the recommendation frequency of the targeted song, with up to 25x  higher recommendation probability compared to average songs.

The main contributions are:

1. The paper introduces **two innovative collective** action strategies where participants strategically insert a target song into their playlists to promote an emerging artist. These strategies aim to increase the recommendations of the targeted song at test time, thereby boosting the artist's visibility on the platform.

2. The research demonstrates that even **small collectives**, controlling less than 0.01% of the training data, **can achieve significant amplification of recommendations** by strategically placing songs in playlists. This finding highlights the effectiveness of algorithmic collective action in promoting songs without major disruptions to the user experience.

3. Preservation of Other Recommendations, as the study reveals that while promoting a specific song through collective action, the recommendations of other songs are largely preserved. This indicates that the proposed strategies can enhance the visibility of targeted songs without significantly compromising the overall recommendation system's performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- Its innovation is a significant strength as it provides a new approach to increasing the visibility of emerging artists in music streaming platforms.
- Empirical Validation and Real-World Application: the research is empirically validated using an open-source APC model deployed on Deezer, a platform with millions of users. 
- important result: the study demonstrates that even small collectives can achieve substantial amplification of recommendations by strategically placing songs in playlists
- The findings show the potential for diverse artist promotion, which can make fairer use of the platforms but also fights against the long-tail problem in recommender systems. It can also help the serendipity effect.

Weaknesses:
-  The paper assumes that users are influenced by the position of songs in playlists when making listening choices. This assumption **may oversimplify user behavior and overlook other factors** that influence song recommendations and user engagement, potentially leading to biased results.

Limitations:
The paper does not explicitly discuss possible limitations of the approach to addressing problems of privacy and fairness. However, considering the ethical implications of data manipulation and collective action in recommender systems is crucial for ensuring transparency and equity in algorithmic interventions.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores the possibility of boosting the recommendation of a song in an automatic playlist continuation system by using a collective strategy of adding the song into the training playlists of the APC at a specific position. The paper shows that adopting a strategy that targets low frequency context makes it possible to very significantly boost the exposition of the song in the output of the APC.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
1: poor

Strengths:
- Very interesting idea which is novel in the domain of music recommendation
- Quite surprising experimental result that is worth sharing (possibility of boosting exposition of a song with the DirLof strategy).

Weaknesses:
- Limited scope:
    - The idea is tested in the limited context of automatic playlist continuation and is likely not adaptable to broader applications.
    - Only one APC model was tested, so the results may be specific to this model. It would have been a good idea to test the impact of the collective action on other models (such as the baseline proposed in the reference paper for the APC model).  Also, it’s unclear whether the effect would be robust to hyperparameter changes in the APC model.
    - the method is tested in a static context (not in the real world), so the actual impact of the method on usage (for instance, whether a user exposed through the APC to a song boosted by the collective strategy would listen to it or not) is not tested (this would require access to the production of a streaming music service, though)
- Very limited insights on the design of the efficient strategy (DirLof) and why it works. The presentation of the strategy is actually quite unclear, while it’s central to the paper.
- There may be other simpler baselines that would be worth testing. The paper shows that inserting the song at the end of the playlists is less performance than inserting the song randomly, which suggests that inserting it earlier in the playlist sequence may help. So a baseline that would always insert the song at the beginning of the sequence could have decent results (the first place is likely to be avoided as it would mean that the song would never appear as the target in the training of the APC, but low positions that appear regularly as targets in the training could be considered).

Limitations:
- As mentioned in the weaknesses, there are other simple baselines that would be worth testing. That would help support the claim that the specially designed DirLof strategy is actually efficient.
- Also the authors should comment on the possible transferability of their results to other APC models, and to modifications of hyperparameters of the model.

On the ethical side, the method is a bit too much presented as a way for artists to attack a recommender system, somewhat rerank their songs and make money out of it, which has some important ethical implications in terms of fairness among artists: As the payment system of most streaming services is based on subscription (so a fixed amount of money), artificially boosting an artist comes at the detriment of other artists.
I would then rather avoid this message of ""opportunity for artists"" in the paper (and even in the title) and turn it instead as a warning to music streaming platforms that recommender systems may lack robustness to attacks and that they should be aware of it and take action. I think presenting the paper in this second way would solve this ethical issue, but the current message is, to me, ethically borderline.

Minor comments
- The authors claim that “most large platforms have shifted from relying on collaborative-based models for APC to building deep learning based recommenders …” but 1) the references are only Spotify based 2) the references are research papers which doesn’t mean that the shift actually happened and that platforms no longer use collaborative-based models part in their complex architecture.

- In equation (1):
    - The authors likely want to specify that the recommended song shouldn’t be part of the playlist i.e S’ **∩** p = **∅** in the argmax
    - The argmax actually corresponds to the top K elements in terms of similarity, which is quite straightforward. But the sum in the equation makes it not very clear at first read. It could be worth it to state it explicitly before the equation.
- There are several notations that use the letter h (song embedding, playlist embedding, playlist mapping). I think using different letters may help make things clearer.
- In Figure 3, it seems like s* is subtituted to s_i (there is no s_i in (b)) while just before equation 4, it’s said that only insertions are considered.
- “*Existing data poisoning techniques, for example, operate in different settings, not complying with Constraint 1 and typically assuming white-box access to the model or involving test-time profile manipulations.*” ⇒ this needs references.
- *“Thus, collective action can make a tremendous difference for these artists: suppose an artist’s song is streamed 10,000 times, yielding a revenue for the artist of $40 for royalties of $0.004 per stream [32]; an amplification of 25 would increase this revenue to $1, 000.”* I understand this is purely illustrative, but the figures won’t reflect much the truth. First 0.004$ per stream is an average that depends on the platform, but also on the kind of registration of the user and on the country of the user with quite important variations. Second, several platforms (Spotify, Deezer…) are shifting to a payment model were all streams don’t have the same value, especially recommended streams are “discounted”, and some “real” artist may get a boost. Once again, I get it’s for illustrative purpose, but it brings confusion on how music payment system works, and then should be avoided in my opinion.
- *“Moreover, when considering s* as a relevant recommendation, collective action even enhances the system’s performance”* Isn’t this completely obvious? if presentation of s* is boosted (which happens, given the amplification), other recommendations are barely affected (as shown on figure 6, solid line), and you consider s* a valid recommendation, then for sure recommendation metrics will increase, won’t they?

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
They proposed a strategy for the streaming platform users to collectively act in order to promote targeted songs. The promotion efficacy is measured by the targeted songs' recommendation frequency boost at testing time. This strategy is approved to be effective through simulation experiment.  Another finding is that this strategy has minimum impact to the performance of the recommendation system as a whole, i.e., by preserving user experience to other non-targeted songs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* This is a novel idea, presented in an interesting domain with a good amount of related work.
* The writing of the paper is clear. The motivation is sound. 
* The experiment performed by the author successfully verified the efficacy of the collective strategy.

Weaknesses:
* Limited scope. See limitations below.
* Lack of technical novelty and contribution. The evaluation result would be a good report but I would recommend the author to seek publication in a different conference.

Limitations:
The strategy could be effective but it is built on top of the assumption that the serving platform has deployed minimum control against collective behavior. As I am aware, there are various user-side anomaly detection mechanisms usually deployed in production in streaming services like what is described in the paper. As an example, there could be real time monitoring of such collective behaviors. Such an anomaly detection system could be constructed by building a user-entity graph. In this use case, we could use artist or songs as the entity. When in a short period of time, there are bursty events that are related to an entity that happens (user promoting a song in their playlist), it could be an important indicator that something unusual happens (because this is not a popular song, it does not receive so many promotes on average). A follow-up action in the system could tag those relevant data as ""spurious"" thus they never go to the training data of the transformer model (for continuous training). As a result, the collective action could have a significantly less effect when such monitoring is turned on; or depending on what threshold they are set up.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper shows that strategic collective action by a small fraction of the population can lead to significant amplification of a particular song in a recommender system.  The authors propose two strategies for the collective (for a transformer-based song recommender) that achieve this amplification.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The setup is original and focuses on strategic collective action by a fraction of users in a recommender system and how this can increase a target song’s reach.
- The performance loss for the platform is negligible, the collective’s strategies are not adversarial (for e.g. fake profiles, artificial perturbations) and based on 1-edit distance to the original playlist. The paper shows that recommendations are largely preserved.

Weaknesses:
- The experiments follow the MF-Transformer in [7], to make the paper self-contained it would be beneficial to have a description of $\phi(.)$ and of $g(.)$ and the loss function in Section 2.1 or the Appendix. 
- I found the strategies in Sec 3.2 hard to parse, perhaps a figure showing the original playlist and the possible changes a user in the collective can do under the two strategies would be helpful. 
- Minor:  I think the notation h(.) is overloaded for the song/playlist embedding in 2.1 and for the strategy mapping which inserts s* into a playlist. 	Also, fig 6 could use different colors for the different strategies.

Limitations:
The authors discuss limitations in Appendix  A.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This research work proposes a novel solution to promote songs on music streaming platforms strategically. 

Under the following assumptions:

1. Fans can collaborate to promote a specific song by collectively reordering playlists.
2. The visibility of a song in a playlist affects its recommendation frequency.
3. Users are influenced by the position of songs in playlists when making listening choices.
4. The impact of collective action on song visibility is measurable and significant.

The authors suggest that fans strategically reorder playlists to promote a targeted song, thereby increasing its visibility in the recommender system. By leveraging algorithmic collective action, even small groups of fans can substantially impact the recommendation frequency of the promoted song. This strategy aims to enhance the visibility (capability of being discovered) of songs and artists, which will benefit both fans and musicians in the music streaming industry. 

 The evaluation focuses on quantifying the amplification of recommendations achieved by strategically placing songs in playlists, using metrics such as recommendation probability and change in the number of recommendations for a song. The evaluation also includes the impact on the recommendations of other songs and the overall performance of the recommender system. The analysis of results reveals that the collective action strategies can lead to a substantial increase in the recommendation frequency of the targeted song, with up to 25x  higher recommendation probability compared to average songs.

The main contributions are:

1. The paper introduces **two innovative collective** action strategies where participants strategically insert a target song into their playlists to promote an emerging artist. These strategies aim to increase the recommendations of the targeted song at test time, thereby boosting the artist's visibility on the platform.

2. The research demonstrates that even **small collectives**, controlling less than 0.01% of the training data, **can achieve significant amplification of recommendations** by strategically placing songs in playlists. This finding highlights the effectiveness of algorithmic collective action in promoting songs without major disruptions to the user experience.

3. Preservation of Other Recommendations, as the study reveals that while promoting a specific song through collective action, the recommendations of other songs are largely preserved. This indicates that the proposed strategies can enhance the visibility of targeted songs without significantly compromising the overall recommendation system's performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- Its innovation is a significant strength as it provides a new approach to increasing the visibility of emerging artists in music streaming platforms.
- Empirical Validation and Real-World Application: the research is empirically validated using an open-source APC model deployed on Deezer, a platform with millions of users. 
- important result: the study demonstrates that even small collectives can achieve substantial amplification of recommendations by strategically placing songs in playlists
- The findings show the potential for diverse artist promotion, which can make fairer use of the platforms but also fights against the long-tail problem in recommender systems. It can also help the serendipity effect.

Weaknesses:
-  The paper assumes that users are influenced by the position of songs in playlists when making listening choices. This assumption **may oversimplify user behavior and overlook other factors** that influence song recommendations and user engagement, potentially leading to biased results.

Limitations:
The paper does not explicitly discuss possible limitations of the approach to addressing problems of privacy and fairness. However, considering the ethical implications of data manipulation and collective action in recommender systems is crucial for ensuring transparency and equity in algorithmic interventions.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wGP1tBCP1E;"REVIEW 
Summary:
This paper derives an upper bound of the Lipschitz constant for diffusion classifiers. Then, it proposes Exact Posterior Noised Diffusion Classifier (EPNDC) and Approximated Posterior Noised Diffusion Classifier (APNDC) by deriving ELBO upper bounds on $\log p (x_\tau)$ and thereby enabling classifying noisy images. The APNDC achieves state-of-the-art certified robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The theory is cool and the math is intriguing. I like this direction because it leverages the shared Gaussian structure in diffusion models and randomized smoothing while circumventing the challenges of attacking diffusion models. The empirical evaluation results (especially Figure 2a) are also impressive.

Weaknesses:
In my opinion, some of the contents are not explained very clearly. Please see below and the contents of the ""Questions"" section.

- Table 4 is nice. However, I wish it was in the main text instead of the appendix, because the current main text misses the discussion on how to calculate the certified robustness for the proposed models.
- Figure 2 doesn't present the certified radius with a conventional diffusion classifier, as derived in Eq. (11). Since (11) is an important contribution of this work, I believe it should be included.
- I would like to see an ablation study on $\sigma_\tau$, but could not find this result.

Overall, this is still a nice paper.

Limitations:
As is the case for numerous diffusion classifier paper, the computation complexity, while improved in this paper, is far from ideal. This paper evaluates the method on a small subset of CIFAR-10 and ImageNet, probably due to this reason.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors investigate the certified robustness of diffusion classifiers. For this purpose, they first show that these classifiers have O(1) Lipschitzness and subsequently achieve tighter robustness bounds through Bayes' theorem and the ELBO.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1: Using diffusion models to generate large amounts of synthetic data is one of the most promising approaches to improve empirical and certified robustness in recent years. The authors utilize diffusion models directly to achieve high certified robustness. 

S2: While prior work has investigated the robustness of diffusion classifiers, they do not provide certified guarantees. This gap is addressed in this work.

S3: The work provides both relevant empirical and theoretical contributions

Weaknesses:
W1: References could be ordered by appearance (minor)

W2: The nature of diffusion classifiers induces a considerable computational overhead compared to standard classifiers. However, the authors try to address this issue through their sift-and-refine algorithm. Still a comparison between different methods w.r.t. inference time would have been informative. (could also include standard classifiers). Note that I would not consider large computational cost as a negative point concerning paper acceptance I just believe that a comparison would be helpful for the reader. Still the appendix provides some information w.r.t. time complexity so I view this as a minor issue. 

W3: Appendix D is very short and could be incorporated into the paper (at least in the camera-ready version)

Limitations:
Limitations are included in the appendix.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work proves that diffusion classifiers possess inherent robustness to adversarial attacks by demonstrating their O(1) Lipschitzness and establishing their certified resilience. By generalizing these classifiers to handle Gaussian-corrupted data and using evidence lower bounds for likelihood approximation, the research demonstrates the superior certified robustness of Noised Diffusion Classifiers (NDCs).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper showcases the robustness of the proposed Noised Diffusion Classifiers (NDCs), achieving high certified robustness on the CIFAR-10 and ImageNet 64x64 datasets. The study also provides a proof of O(1) Lipschitzness for diffusion classifiers.

Weaknesses:
1. The proposed method combines two existing techniques, diffusion classifiers and randomized smoothing, which is not sufficiently novel. The paper needs to better highlight what sets this approach apart from existing methods and how it fundamentally advances the field. Although the authors attempt to establish a theoretical framework, the derivation of the Lipschitz constant and its implications are not sufficiently detailed, leaving unanswered questions about the robustness guarantees.

2. The experimental evaluation relies heavily on the small CIFAR-10 and ImageNet 64x64 datasets. Expanding the experiments to include larger datasets, such as ImageNet-1K, would provide a more comprehensive assessment.

3. The paper discusses techniques to reduce time complexity but does not convincingly demonstrate the practicality of the proposed methods with experimental results, such as throughput or inference latency. A more detailed analysis and comparisons of computational efficiency, especially in relation to existing methods, are needed.

Limitations:
No potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a theoretical analysis of the enhanced robustness in diffusion-based classifiers and introduces a generalized Noised Diffusion Classifier, EPNDC. The authors utilize the Evidence Lower Bound (ELBO) of each conditional log-likelihood $\log p(x_\tau | y) $and Bayes' theorem as the logits for each class. They identified that EPNDC is time-consuming due to the iterative computation of two conditional ELBOs. To address this, they leverage the ELBO of an ensemble of EPNDC to approximate the expected likelihood as logits without additional computational cost. Additionally, they developed variance reduction and sift-and-refine techniques to reduce time complexity. Experimental results demonstrate that APNDC achieves significantly better robustness without requiring extra training data, fewer diffusion steps, and a reduced number of samples needed to estimate the Lipschitz bound.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The entire paper is logically structured with a clear progression, enabling readers to understand it well. From Algorithm 1 to Algorithm 2 to Algorithm 5, the authors continuously explore problems, improve algorithms, and provide thorough analysis and theoretical proofs.

2. The experiments are comprehensive, and compared to the benchmark, EPNDC shows significant improvements in certified accuracy. This demonstrates EPNDC's high scalability in handling large datasets with numerous categories.

Weaknesses:
1. Some causal relationships are unclear or lack citations, requiring further explanation from the authors. For instance, in Line 156: What does the ""nabla operator"" refer to? It is neither explained nor cited. In Lines 161-164: ""However, similar to the weak law of randomized smoothing, such certified robustness has limitations because it assumes the maximum Lipschitz condition is satisfied throughout the entire perturbation path. As a result, the robust radius is less than half of the reciprocal of the maximum Lipschitz constant."" The causal relationship here is unclear and needs further clarification.

2. Although the diffusion classifier is highly scalable and robust, its clean accuracy on ImageNet is still far behind the current state-of-the-art (90%+). More details can be found at [https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet).

Limitations:
The authors didn't address the limitations of APNDC, but Diffusion Classifiers are still far behind the current SOTA in classification accuracy. These are some potential limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper derives an upper bound of the Lipschitz constant for diffusion classifiers. Then, it proposes Exact Posterior Noised Diffusion Classifier (EPNDC) and Approximated Posterior Noised Diffusion Classifier (APNDC) by deriving ELBO upper bounds on $\log p (x_\tau)$ and thereby enabling classifying noisy images. The APNDC achieves state-of-the-art certified robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The theory is cool and the math is intriguing. I like this direction because it leverages the shared Gaussian structure in diffusion models and randomized smoothing while circumventing the challenges of attacking diffusion models. The empirical evaluation results (especially Figure 2a) are also impressive.

Weaknesses:
In my opinion, some of the contents are not explained very clearly. Please see below and the contents of the ""Questions"" section.

- Table 4 is nice. However, I wish it was in the main text instead of the appendix, because the current main text misses the discussion on how to calculate the certified robustness for the proposed models.
- Figure 2 doesn't present the certified radius with a conventional diffusion classifier, as derived in Eq. (11). Since (11) is an important contribution of this work, I believe it should be included.
- I would like to see an ablation study on $\sigma_\tau$, but could not find this result.

Overall, this is still a nice paper.

Limitations:
As is the case for numerous diffusion classifier paper, the computation complexity, while improved in this paper, is far from ideal. This paper evaluates the method on a small subset of CIFAR-10 and ImageNet, probably due to this reason.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors investigate the certified robustness of diffusion classifiers. For this purpose, they first show that these classifiers have O(1) Lipschitzness and subsequently achieve tighter robustness bounds through Bayes' theorem and the ELBO.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1: Using diffusion models to generate large amounts of synthetic data is one of the most promising approaches to improve empirical and certified robustness in recent years. The authors utilize diffusion models directly to achieve high certified robustness. 

S2: While prior work has investigated the robustness of diffusion classifiers, they do not provide certified guarantees. This gap is addressed in this work.

S3: The work provides both relevant empirical and theoretical contributions

Weaknesses:
W1: References could be ordered by appearance (minor)

W2: The nature of diffusion classifiers induces a considerable computational overhead compared to standard classifiers. However, the authors try to address this issue through their sift-and-refine algorithm. Still a comparison between different methods w.r.t. inference time would have been informative. (could also include standard classifiers). Note that I would not consider large computational cost as a negative point concerning paper acceptance I just believe that a comparison would be helpful for the reader. Still the appendix provides some information w.r.t. time complexity so I view this as a minor issue. 

W3: Appendix D is very short and could be incorporated into the paper (at least in the camera-ready version)

Limitations:
Limitations are included in the appendix.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work proves that diffusion classifiers possess inherent robustness to adversarial attacks by demonstrating their O(1) Lipschitzness and establishing their certified resilience. By generalizing these classifiers to handle Gaussian-corrupted data and using evidence lower bounds for likelihood approximation, the research demonstrates the superior certified robustness of Noised Diffusion Classifiers (NDCs).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper showcases the robustness of the proposed Noised Diffusion Classifiers (NDCs), achieving high certified robustness on the CIFAR-10 and ImageNet 64x64 datasets. The study also provides a proof of O(1) Lipschitzness for diffusion classifiers.

Weaknesses:
1. The proposed method combines two existing techniques, diffusion classifiers and randomized smoothing, which is not sufficiently novel. The paper needs to better highlight what sets this approach apart from existing methods and how it fundamentally advances the field. Although the authors attempt to establish a theoretical framework, the derivation of the Lipschitz constant and its implications are not sufficiently detailed, leaving unanswered questions about the robustness guarantees.

2. The experimental evaluation relies heavily on the small CIFAR-10 and ImageNet 64x64 datasets. Expanding the experiments to include larger datasets, such as ImageNet-1K, would provide a more comprehensive assessment.

3. The paper discusses techniques to reduce time complexity but does not convincingly demonstrate the practicality of the proposed methods with experimental results, such as throughput or inference latency. A more detailed analysis and comparisons of computational efficiency, especially in relation to existing methods, are needed.

Limitations:
No potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a theoretical analysis of the enhanced robustness in diffusion-based classifiers and introduces a generalized Noised Diffusion Classifier, EPNDC. The authors utilize the Evidence Lower Bound (ELBO) of each conditional log-likelihood $\log p(x_\tau | y) $and Bayes' theorem as the logits for each class. They identified that EPNDC is time-consuming due to the iterative computation of two conditional ELBOs. To address this, they leverage the ELBO of an ensemble of EPNDC to approximate the expected likelihood as logits without additional computational cost. Additionally, they developed variance reduction and sift-and-refine techniques to reduce time complexity. Experimental results demonstrate that APNDC achieves significantly better robustness without requiring extra training data, fewer diffusion steps, and a reduced number of samples needed to estimate the Lipschitz bound.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The entire paper is logically structured with a clear progression, enabling readers to understand it well. From Algorithm 1 to Algorithm 2 to Algorithm 5, the authors continuously explore problems, improve algorithms, and provide thorough analysis and theoretical proofs.

2. The experiments are comprehensive, and compared to the benchmark, EPNDC shows significant improvements in certified accuracy. This demonstrates EPNDC's high scalability in handling large datasets with numerous categories.

Weaknesses:
1. Some causal relationships are unclear or lack citations, requiring further explanation from the authors. For instance, in Line 156: What does the ""nabla operator"" refer to? It is neither explained nor cited. In Lines 161-164: ""However, similar to the weak law of randomized smoothing, such certified robustness has limitations because it assumes the maximum Lipschitz condition is satisfied throughout the entire perturbation path. As a result, the robust radius is less than half of the reciprocal of the maximum Lipschitz constant."" The causal relationship here is unclear and needs further clarification.

2. Although the diffusion classifier is highly scalable and robust, its clean accuracy on ImageNet is still far behind the current state-of-the-art (90%+). More details can be found at [https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet).

Limitations:
The authors didn't address the limitations of APNDC, but Diffusion Classifiers are still far behind the current SOTA in classification accuracy. These are some potential limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wFzIMbTsY7;"REVIEW 
Summary:
This paper investigates an emerging foundation model, Mamba, in Reinforcement Learning (RL) scenarios and compares it with Transformer in terms of effectiveness and efficiency. The authors find that in-context RL methods with Mamba as the backbone are generally more efficient than Transformer, but there is no significant improvement in effectiveness. Then, this paper proposes a Hybrid Mamba (HM) with the merits of transformers and Mamba in high-quality prediction and long-term memory. Finally, this paper conducts experiments on three benchmarks to exhibit its improved effectiveness and efficiency.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.	The paper is commendably well-written and coherent, effectively explaining complex ideas in an accessible manner. The authors explored the potential of the widely discussed model Mamba in the context of RL and compared it with Transformer in terms of effectiveness and efficiency.
2.	The authors proposed a novel hybrid model that inherits the merits of both Transformer and Mamba in a goal-conditional manner. The main advantage of using the hybrid structure is that when the time horizon is very long, as in the D4RL tasks, several episodes/trials are required for good in-context learning, as in the larger Dark Room and Tmaze environments.
3.	HM improves training and inference speed by reducing the horizon of the transformer model. This can be particularly important in applications such as robotics, which require high-frequency control.
4.	The experimental evaluation, meticulously designed to include several baselines and diverse tasks, demonstrates the algorithm's strengths.

Weaknesses:
1.	The baseline AD (Mamba) in Figure 2 and the baseline DM in Figure 3, which appear to be AD (Transformer) and DT variants, are crucial for the readers' understanding of how Mamba replaces the Transformer architecture. However, the lack of explanation of these two baselines in the experimental setup section might confuse readers.
2.	Some experimental settings are not explained clearly. In Section 5.3, the authors do not explain what GPU device they used. Although the device is introduced in Appendix A, it is recommended that it be explained clearly in the main text.

Limitations:
The author discusses limitations and potential negative societal impacts in Section 6.

Rating:
10: strong accept, should be highlighted at the conference

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents Hybrid Mamba (HM), a method that combines the Mamba model and Transformer to enhance reinforcement learning (RL) performance. HM leverages Mamba to generate high-value sub-goals, which then condition the transformer, leading to significant improvements in online testing efficiency and task-solving capabilities.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and clear to read.
2. HM significantly accelerates testing speed, achieving up to 28 times faster results than baseline methods.
3. HM demonstrates superior performance across various benchmarks, such as D4RL, Grid World, and Tmaze.

Weaknesses:
1. This paper claims to present a in-context RL approach. The motivation of this paper is concerned with the problems encountered with the no-gradient updates in-context approach (line 28), where the policy network does not require parameter updates. However, this paper uses a global update approach, which is closer to gradient-based and conditional-based offline RL. It seems to contradict the original intention of this paper. 

2. HM benefits from using a powerful subgoal encoder (Mamba in this case) and conditioning the policy network with subgoals. The performance improvement is expected and unsurprising due to the advantages inherent in conditional-based RL algorithms. Hence, it is necessary to further explain the unique contributions of combining Mamba and causal transformer in this paper.

3. If the sub-goal encoder are replaced with other advanced and efficient sequence encoders (e.g., flash-attention1/2 [1,2], x-lstm [3]), would it also yield better or more efficient performance? 

4. The experiments demonstrating HW's efficacy in capturing long-term dependencies are unconvincing. Achieving good results in tasks with an arbitrarily horizon (e.g., Tmaze) does not necessarily prove effective long-term memory embedding. It is crucial to test the stability and performance of HM with varying horizon lengths or other length-based settings. For example, Mamba’s original paper [4] demonstrated the ability to capture long-term dependencies through the scaling laws.

5. Could the authors clarify in which specific aspects HM's training time is faster than DT's? Since HM appears to be a combination of Mamba and DT.

6. There are parts of the paper that are not clearly explained. For instance, in lines 228-233, it is mentioned that the transformer predicts a c-step action sequence (named $a_1$ here) through the sub-goal $z_t$ and another c-step action sequence (named $a_2$) through valuable sub-goals from offline data. How are $a_1$ and $a_2$ subsequently updated or processed? 

7. (minor) The paper contains some typos and inconsistencies in tense usage. For example, in the related work section, the section on Mamba uses the present tense, while the section on in-context RL uses the past tense. These should be corrected for consistency. In addition, what's the meaning of the different gaussian distribution figures in Figure 1?

*Reference:*

[1] Dao T, Fu D, Ermon S, et al. Flashattention: Fast and memory-efficient exact attention with io-awareness. NeurIPS 2022.

[2] Dao T. Flashattention-2: Faster attention with better parallelism and work partitioning. ICLR 2024.

[3] Beck M, Pöppel K, Spanring M, et al. xLSTM: Extended Long Short-Term Memory. arXiv 2024.

[4] Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv 2023.

Limitations:
The authors raise some limitaions, for example, how to control the setting of hyperparameter $c$, which is not addressed in this paper but is claimed to be solved in the future.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates to utilize the Mamba [1] architecture for In-Context RL task. Addressing this task with Transformer architecture is effective while it is very inefficient due to the quadratic computation overhead of Transformer. The Mamba can reduce this overhead dramatically while sustain the performance somewhat. The application of State-Space Models (SSMs) to In-Context RL task is studied in [2], but different from [2], they combinationally utilize Mamba and Transformer as high-level memory and low-level (short-term) memory. Additionally, as Mamba predicts the sub-goal for the Transformer short-term memory, they improved the performance. Through this modeling, they can achieve better performance than previous works while improving the efficiency.

[1] Gu, Albert, and Tri Dao. ""Mamba: Linear-time sequence modeling with selective state spaces."" arXiv preprint arXiv:2312.00752 (2023).

[2] Lu, Chris, et al. ""Structured state space models for in-context reinforcement learning."" Advances in Neural Information Processing Systems 36 (2024).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Appropriate modeling is applied in this study. While the effectiveness of hybrid modeling of SSMs and local Attention has been previously explored in [1], the authors effectively implement this concept for the In-Context RL task with new functionalities, such as predicting high-value sub-goals.
- The introduction and methodology sections are well written. The motivation is clearly articulated, and the logical flow of their method proposal is coherent. The empirical analysis comparing Mamba and Transformer in RL tasks convincingly demonstrates the need for more advanced modeling.
- The paper provides extensive empirical analyses. It shares experimental results on multiple benchmarks, including ablation studies and performance changes with varying hyperparameter values.

[1] De, Soham, et al. ""Griffin: Mixing gated linear recurrences with local attention for efficient language models."" arXiv preprint arXiv:2402.19427 (2024).

Weaknesses:
- The high-level encoding is done by encoding the intervalled trajectories (e.g., every $c$ -th trajectory), which might miss important information in the middle of the interval.
- The section on Hybrid Mamba with Valuable Sub-goals is initially confusing, especially regarding the relationship between Mamba’s sub-goal prediction and the collected valuable sub-goals. Discussing this relationship at the beginning of the Valuable Sub-goal section could help readers understand the content more easily.
- One of the experimental results differs from my expectations, but the paper does not provide an analysis for this. I will address this in the Questions section.

Limitations:
The authors properly addressed their limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes Hybrid Mamba (HM) for in-context RL. Existing in-context RL methods are predominantly based on the Transformer architecture. Transformers come with quadratic complexity of self-attention and are computationally costly. Consequently, the authors propose a hybrid architecture that uses Mamba to compute sub-goals from long-context, which are fed into a low-level Transformer policy. The authors conduct experiments on grid-worlds and D4RL to evaluate their method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Relevance**

The paper aims at deploying the Mamba architecture for in-context RL, which is very relevant given the quadratic complexity of the Transformer architecture.
This results in clear benefits in terms of time complexity.

**Experimental results**

Empirical results on simple gridworld environments and D4RL seem convincing and their method exhibits significant gains compared to Transformers.

Weaknesses:
**Presentation**

The methodology raises some questions and should be improved, in particular:
 - What is the reasoning behind sampling the sub-goal from a multi-variate Gaussian?
 - How does this compare to using a fixed representation? (e.g., similar to CLS token)
 - Why is the done-flag in Hybrid Mamba necessary? Do other methods (e.g., AD [1]) use this as well?
 - What does “Extra high-value states” mean?
 - What is the intuition behind removing actions from the Mamba context?
 - What effect would dropping actions have in other methods?

Furthermore, the construction of “valuable sub-goals” is unclear.
One way to improve clarity would be to shorten the section on preliminaries and instead add more details to the Method section.
Figure 2 and Table 2 are missing the performance curves/scores for HM without valuable subgoals.
Finally, Figure 1 can be improved to enhance clarity.

**Significance of results**

The authors evaluate primarily on simple grid-world environments and rather simple robotics tasks. However, it is unclear how well HM generalizes to more complex tasks as used in other works [2].

**Evaluation**

The authors change their evaluation methodology from improvement curves on gridworlds (Figure 2) to average performance scores on D4RL (Table2).
On D4RL, HM seems to clearly outperform other methods.
However, the authors do not show in-context improvemenst which raises the question whether HM actually learns to improve in-context. Can the authors clarify, why no in-context improvement curves are shown for D4RL?

**Ablation studies**

Some ablation studies are missing and would add more depth to understanding the proposed method, in particular:
- What is the impact on performance of including the done-flag in Mamba?
- What effect does it have on other methods?
- What is the impact on performance of removing the action condition in HM?
- What effect does the same intervention have on other methods?


 [1] Laskin et al., In-context Reinforcement Learning with Algorithm Distillation, ICLR 2023
 [2] Raparthy et al., Generalization to New Sequential Decision Making Tasks with
In-Context Learning, ICML 2024

Limitations:
The authors highlight that setting the context length c to a fixed value as a current limitation of their method. However, a notable limitations are missing, namely that their evaluation is limited to simple environments, while it is unclear how well HM performs on more complex or new tasks.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates an emerging foundation model, Mamba, in Reinforcement Learning (RL) scenarios and compares it with Transformer in terms of effectiveness and efficiency. The authors find that in-context RL methods with Mamba as the backbone are generally more efficient than Transformer, but there is no significant improvement in effectiveness. Then, this paper proposes a Hybrid Mamba (HM) with the merits of transformers and Mamba in high-quality prediction and long-term memory. Finally, this paper conducts experiments on three benchmarks to exhibit its improved effectiveness and efficiency.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.	The paper is commendably well-written and coherent, effectively explaining complex ideas in an accessible manner. The authors explored the potential of the widely discussed model Mamba in the context of RL and compared it with Transformer in terms of effectiveness and efficiency.
2.	The authors proposed a novel hybrid model that inherits the merits of both Transformer and Mamba in a goal-conditional manner. The main advantage of using the hybrid structure is that when the time horizon is very long, as in the D4RL tasks, several episodes/trials are required for good in-context learning, as in the larger Dark Room and Tmaze environments.
3.	HM improves training and inference speed by reducing the horizon of the transformer model. This can be particularly important in applications such as robotics, which require high-frequency control.
4.	The experimental evaluation, meticulously designed to include several baselines and diverse tasks, demonstrates the algorithm's strengths.

Weaknesses:
1.	The baseline AD (Mamba) in Figure 2 and the baseline DM in Figure 3, which appear to be AD (Transformer) and DT variants, are crucial for the readers' understanding of how Mamba replaces the Transformer architecture. However, the lack of explanation of these two baselines in the experimental setup section might confuse readers.
2.	Some experimental settings are not explained clearly. In Section 5.3, the authors do not explain what GPU device they used. Although the device is introduced in Appendix A, it is recommended that it be explained clearly in the main text.

Limitations:
The author discusses limitations and potential negative societal impacts in Section 6.

Rating:
10: strong accept, should be highlighted at the conference

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents Hybrid Mamba (HM), a method that combines the Mamba model and Transformer to enhance reinforcement learning (RL) performance. HM leverages Mamba to generate high-value sub-goals, which then condition the transformer, leading to significant improvements in online testing efficiency and task-solving capabilities.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and clear to read.
2. HM significantly accelerates testing speed, achieving up to 28 times faster results than baseline methods.
3. HM demonstrates superior performance across various benchmarks, such as D4RL, Grid World, and Tmaze.

Weaknesses:
1. This paper claims to present a in-context RL approach. The motivation of this paper is concerned with the problems encountered with the no-gradient updates in-context approach (line 28), where the policy network does not require parameter updates. However, this paper uses a global update approach, which is closer to gradient-based and conditional-based offline RL. It seems to contradict the original intention of this paper. 

2. HM benefits from using a powerful subgoal encoder (Mamba in this case) and conditioning the policy network with subgoals. The performance improvement is expected and unsurprising due to the advantages inherent in conditional-based RL algorithms. Hence, it is necessary to further explain the unique contributions of combining Mamba and causal transformer in this paper.

3. If the sub-goal encoder are replaced with other advanced and efficient sequence encoders (e.g., flash-attention1/2 [1,2], x-lstm [3]), would it also yield better or more efficient performance? 

4. The experiments demonstrating HW's efficacy in capturing long-term dependencies are unconvincing. Achieving good results in tasks with an arbitrarily horizon (e.g., Tmaze) does not necessarily prove effective long-term memory embedding. It is crucial to test the stability and performance of HM with varying horizon lengths or other length-based settings. For example, Mamba’s original paper [4] demonstrated the ability to capture long-term dependencies through the scaling laws.

5. Could the authors clarify in which specific aspects HM's training time is faster than DT's? Since HM appears to be a combination of Mamba and DT.

6. There are parts of the paper that are not clearly explained. For instance, in lines 228-233, it is mentioned that the transformer predicts a c-step action sequence (named $a_1$ here) through the sub-goal $z_t$ and another c-step action sequence (named $a_2$) through valuable sub-goals from offline data. How are $a_1$ and $a_2$ subsequently updated or processed? 

7. (minor) The paper contains some typos and inconsistencies in tense usage. For example, in the related work section, the section on Mamba uses the present tense, while the section on in-context RL uses the past tense. These should be corrected for consistency. In addition, what's the meaning of the different gaussian distribution figures in Figure 1?

*Reference:*

[1] Dao T, Fu D, Ermon S, et al. Flashattention: Fast and memory-efficient exact attention with io-awareness. NeurIPS 2022.

[2] Dao T. Flashattention-2: Faster attention with better parallelism and work partitioning. ICLR 2024.

[3] Beck M, Pöppel K, Spanring M, et al. xLSTM: Extended Long Short-Term Memory. arXiv 2024.

[4] Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv 2023.

Limitations:
The authors raise some limitaions, for example, how to control the setting of hyperparameter $c$, which is not addressed in this paper but is claimed to be solved in the future.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates to utilize the Mamba [1] architecture for In-Context RL task. Addressing this task with Transformer architecture is effective while it is very inefficient due to the quadratic computation overhead of Transformer. The Mamba can reduce this overhead dramatically while sustain the performance somewhat. The application of State-Space Models (SSMs) to In-Context RL task is studied in [2], but different from [2], they combinationally utilize Mamba and Transformer as high-level memory and low-level (short-term) memory. Additionally, as Mamba predicts the sub-goal for the Transformer short-term memory, they improved the performance. Through this modeling, they can achieve better performance than previous works while improving the efficiency.

[1] Gu, Albert, and Tri Dao. ""Mamba: Linear-time sequence modeling with selective state spaces."" arXiv preprint arXiv:2312.00752 (2023).

[2] Lu, Chris, et al. ""Structured state space models for in-context reinforcement learning."" Advances in Neural Information Processing Systems 36 (2024).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Appropriate modeling is applied in this study. While the effectiveness of hybrid modeling of SSMs and local Attention has been previously explored in [1], the authors effectively implement this concept for the In-Context RL task with new functionalities, such as predicting high-value sub-goals.
- The introduction and methodology sections are well written. The motivation is clearly articulated, and the logical flow of their method proposal is coherent. The empirical analysis comparing Mamba and Transformer in RL tasks convincingly demonstrates the need for more advanced modeling.
- The paper provides extensive empirical analyses. It shares experimental results on multiple benchmarks, including ablation studies and performance changes with varying hyperparameter values.

[1] De, Soham, et al. ""Griffin: Mixing gated linear recurrences with local attention for efficient language models."" arXiv preprint arXiv:2402.19427 (2024).

Weaknesses:
- The high-level encoding is done by encoding the intervalled trajectories (e.g., every $c$ -th trajectory), which might miss important information in the middle of the interval.
- The section on Hybrid Mamba with Valuable Sub-goals is initially confusing, especially regarding the relationship between Mamba’s sub-goal prediction and the collected valuable sub-goals. Discussing this relationship at the beginning of the Valuable Sub-goal section could help readers understand the content more easily.
- One of the experimental results differs from my expectations, but the paper does not provide an analysis for this. I will address this in the Questions section.

Limitations:
The authors properly addressed their limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes Hybrid Mamba (HM) for in-context RL. Existing in-context RL methods are predominantly based on the Transformer architecture. Transformers come with quadratic complexity of self-attention and are computationally costly. Consequently, the authors propose a hybrid architecture that uses Mamba to compute sub-goals from long-context, which are fed into a low-level Transformer policy. The authors conduct experiments on grid-worlds and D4RL to evaluate their method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Relevance**

The paper aims at deploying the Mamba architecture for in-context RL, which is very relevant given the quadratic complexity of the Transformer architecture.
This results in clear benefits in terms of time complexity.

**Experimental results**

Empirical results on simple gridworld environments and D4RL seem convincing and their method exhibits significant gains compared to Transformers.

Weaknesses:
**Presentation**

The methodology raises some questions and should be improved, in particular:
 - What is the reasoning behind sampling the sub-goal from a multi-variate Gaussian?
 - How does this compare to using a fixed representation? (e.g., similar to CLS token)
 - Why is the done-flag in Hybrid Mamba necessary? Do other methods (e.g., AD [1]) use this as well?
 - What does “Extra high-value states” mean?
 - What is the intuition behind removing actions from the Mamba context?
 - What effect would dropping actions have in other methods?

Furthermore, the construction of “valuable sub-goals” is unclear.
One way to improve clarity would be to shorten the section on preliminaries and instead add more details to the Method section.
Figure 2 and Table 2 are missing the performance curves/scores for HM without valuable subgoals.
Finally, Figure 1 can be improved to enhance clarity.

**Significance of results**

The authors evaluate primarily on simple grid-world environments and rather simple robotics tasks. However, it is unclear how well HM generalizes to more complex tasks as used in other works [2].

**Evaluation**

The authors change their evaluation methodology from improvement curves on gridworlds (Figure 2) to average performance scores on D4RL (Table2).
On D4RL, HM seems to clearly outperform other methods.
However, the authors do not show in-context improvemenst which raises the question whether HM actually learns to improve in-context. Can the authors clarify, why no in-context improvement curves are shown for D4RL?

**Ablation studies**

Some ablation studies are missing and would add more depth to understanding the proposed method, in particular:
- What is the impact on performance of including the done-flag in Mamba?
- What effect does it have on other methods?
- What is the impact on performance of removing the action condition in HM?
- What effect does the same intervention have on other methods?


 [1] Laskin et al., In-context Reinforcement Learning with Algorithm Distillation, ICLR 2023
 [2] Raparthy et al., Generalization to New Sequential Decision Making Tasks with
In-Context Learning, ICML 2024

Limitations:
The authors highlight that setting the context length c to a fixed value as a current limitation of their method. However, a notable limitations are missing, namely that their evaluation is limited to simple environments, while it is unclear how well HM performs on more complex or new tasks.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wDirCeTIoz;"REVIEW 
Summary:
The paper introduces Distributed Lion, a variant of the Lion optimizer, tailored for distributed training environments. Lion, known for its memory and computational efficiency, is adapted to reduce communication costs between workers and a central server. This is achieved by communicating binary or low-precision vectors rather than high-precision floating-point vectors. The paper presents theoretical convergence properties and empirical results that demonstrate Distributed Lion’s robustness and efficiency across various tasks, worker counts, and batch sizes. It shows comparable performance to standard Lion or AdamW optimizers but with significantly reduced communication bandwidth.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ Innovation in Communication Efficiency: The use of binary or low-precision vectors for communication significantly reduces bandwidth requirements, which is a critical factor in distributed training.
+ Theoretical Validation: The paper provides a solid theoretical foundation confirming the convergence properties of Distributed Lion.
+ Empirical Evidence: Extensive experiments demonstrate the robustness and efficiency of Distributed Lion across a variety of tasks, making a strong case for its practical applicability.

Weaknesses:
- Incompatible with Allreduce: after converting the gradients to binary or low-precision, Allreduce cannot be used for gradient synchronization. One of my concerns is about its communication efficiency in real-world distributed systems, especially training with a high number of workers.
- Computation Overhead: While the communication cost is reduced, the overhead of converting updates to binary or low-precision vectors and back might offset some of the gains in certain scenarios. It helps if the end-to-end training throughput comparison is reported.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Large-scale AI model training has increasingly higher requirements on time, cost and environmental impact, so it is crucial to develop efficient optimizers. As an emerging optimizer, Lion optimizer has advantages in memory, computation and sample efficiency compared with AdamW. Distributed Lion: The paper proposes Distributed Lion, which is an innovative adaptation of Lion optimizer in distributed training environment. Using symbolic operations in Lion, Distributed Lion only requires binary or low-precision vectors to be communicated between working nodes and central servers, significantly reducing communication costs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Distributed Lion significantly reduces communication overhead by communicating only binary or low-precision vectors between workers, which is particularly beneficial for large-scale distributed training.
2. The paper provides theoretical analysis to prove the convergence of Distributed Lion.
3. Experimental results show that Distributed Lion can achieve comparable performance to the standard Lion or AdamW optimizer while reducing communication bandwidth.

Weaknesses:
1. The actual updating on local worker parameters is gradients, while the communicated message is signs. While the theoretical analysis shows this updating can guarantee the convergence, the actual updating style looks like the quantization. The important baselines like QSGD and SignSGD are missed. 
2. The performance of Distributed Lion can be sensitive to hyperparameter choices, especially those related to communication and aggregation strategies.
3. The code is not provided. Thus the reproducibility of the experiments is weakened.
4. The experiment performance on the CIFAR-10 is very low. Considering that the well-known validation performance of CIFAR-10 can be achieved as 94%, the proposed results are around 90%. Why the performance decreases?
5. The important baseline SGD with momentum is not provided.
6. The convergence curves on training with ImageNet and OpenWebText are not provided. This makes it hard to identify the convergence speedup between different optimizers.
7. The wall-clock time is not provided. The quantization operation and the majority vote require extra time, it will be better to show this optimizer can reduce the real-world throughputs.

Limitations:
Two minor points considering the realistic device constraints:
1. The experiment scalability is with 32 GPUs, which is not a large scale distributed training setting.
2. The training model is small-scale with less than 1B parameters.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper extends the Lion optimizer to data parallel distributed training. Unlike optimizers like SGD and Adam, the binary update in Lion can be exploited to minimize the communication. They investigate two cost effective methods for the communication of  binary updates; averaging and majority vote. Experimental results show that both methods yield competitive results to global Lion and AdamW.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The convergence analysis provided in Section 3 gives some reassurance to this non-conventional optimization method. Results are promising and experimental conditions seem adequate.

Weaknesses:
The proposed method is a trivial extension of Lion to data parallel distributed training, so the only interesting contribution seems to be the convergence analysis. 

The main contribution of this work is supposed to be the reduction of communication overhead, but there are no results showing the actual breakdown of the training time. Therefore, it is not possible to determine whether the reduction of communication volume is actually contributing to the reduction of the overall training time. Since the results seem to vary quite a bit for different models and datasets, such information is useful for determining whether the experiments are conducted for configurations that actually show a significant impact on the training time. There remains a possibility that the current method does not work as well for extremely large models trained with ZeRO 3 data parallelism, which is where the communication overhead really becomes a problem.

Limitations:
The limitations pointed out above are not explicitly stated in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes Distributed Lion, a new variant of Lion optimizer for distributed training. The proposed algorithm only requires to communicate binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. The theoretical analysis proves the convergence of the proposed algorithms. The empirical results show that the proposed algorithms have comparable model performance on CV/NLP applications but with significantly less communication overhead compared to the baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes Distributed Lion, a new variant of Lion optimizer for distributed training.

2. The proposed algorithm only requires to communicate binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost.

3. The theoretical analysis proves the convergence of the proposed algorithms. The empirical results show that the proposed algorithms have comparable model performance on CV/NLP applications but with significantly less communication overhead compared to the baselines.

Weaknesses:
1. According to Assumption 3.1, the convergence requires i.i.d. local datasets, while real-world distributed training typically uses non-i.i.d. local data.

2. In the empirical results, there seems to be no wall-clock time for training is reported. Note that the overall goal of communication reduction is to reduce the training time. Thus, it is important to report loss/acc vs. wall-clock time in the experiments.

Limitations:
The limitations are well discussed and addressed in this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Distributed Lion, a variant of the Lion optimizer, tailored for distributed training environments. Lion, known for its memory and computational efficiency, is adapted to reduce communication costs between workers and a central server. This is achieved by communicating binary or low-precision vectors rather than high-precision floating-point vectors. The paper presents theoretical convergence properties and empirical results that demonstrate Distributed Lion’s robustness and efficiency across various tasks, worker counts, and batch sizes. It shows comparable performance to standard Lion or AdamW optimizers but with significantly reduced communication bandwidth.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ Innovation in Communication Efficiency: The use of binary or low-precision vectors for communication significantly reduces bandwidth requirements, which is a critical factor in distributed training.
+ Theoretical Validation: The paper provides a solid theoretical foundation confirming the convergence properties of Distributed Lion.
+ Empirical Evidence: Extensive experiments demonstrate the robustness and efficiency of Distributed Lion across a variety of tasks, making a strong case for its practical applicability.

Weaknesses:
- Incompatible with Allreduce: after converting the gradients to binary or low-precision, Allreduce cannot be used for gradient synchronization. One of my concerns is about its communication efficiency in real-world distributed systems, especially training with a high number of workers.
- Computation Overhead: While the communication cost is reduced, the overhead of converting updates to binary or low-precision vectors and back might offset some of the gains in certain scenarios. It helps if the end-to-end training throughput comparison is reported.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Large-scale AI model training has increasingly higher requirements on time, cost and environmental impact, so it is crucial to develop efficient optimizers. As an emerging optimizer, Lion optimizer has advantages in memory, computation and sample efficiency compared with AdamW. Distributed Lion: The paper proposes Distributed Lion, which is an innovative adaptation of Lion optimizer in distributed training environment. Using symbolic operations in Lion, Distributed Lion only requires binary or low-precision vectors to be communicated between working nodes and central servers, significantly reducing communication costs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Distributed Lion significantly reduces communication overhead by communicating only binary or low-precision vectors between workers, which is particularly beneficial for large-scale distributed training.
2. The paper provides theoretical analysis to prove the convergence of Distributed Lion.
3. Experimental results show that Distributed Lion can achieve comparable performance to the standard Lion or AdamW optimizer while reducing communication bandwidth.

Weaknesses:
1. The actual updating on local worker parameters is gradients, while the communicated message is signs. While the theoretical analysis shows this updating can guarantee the convergence, the actual updating style looks like the quantization. The important baselines like QSGD and SignSGD are missed. 
2. The performance of Distributed Lion can be sensitive to hyperparameter choices, especially those related to communication and aggregation strategies.
3. The code is not provided. Thus the reproducibility of the experiments is weakened.
4. The experiment performance on the CIFAR-10 is very low. Considering that the well-known validation performance of CIFAR-10 can be achieved as 94%, the proposed results are around 90%. Why the performance decreases?
5. The important baseline SGD with momentum is not provided.
6. The convergence curves on training with ImageNet and OpenWebText are not provided. This makes it hard to identify the convergence speedup between different optimizers.
7. The wall-clock time is not provided. The quantization operation and the majority vote require extra time, it will be better to show this optimizer can reduce the real-world throughputs.

Limitations:
Two minor points considering the realistic device constraints:
1. The experiment scalability is with 32 GPUs, which is not a large scale distributed training setting.
2. The training model is small-scale with less than 1B parameters.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper extends the Lion optimizer to data parallel distributed training. Unlike optimizers like SGD and Adam, the binary update in Lion can be exploited to minimize the communication. They investigate two cost effective methods for the communication of  binary updates; averaging and majority vote. Experimental results show that both methods yield competitive results to global Lion and AdamW.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The convergence analysis provided in Section 3 gives some reassurance to this non-conventional optimization method. Results are promising and experimental conditions seem adequate.

Weaknesses:
The proposed method is a trivial extension of Lion to data parallel distributed training, so the only interesting contribution seems to be the convergence analysis. 

The main contribution of this work is supposed to be the reduction of communication overhead, but there are no results showing the actual breakdown of the training time. Therefore, it is not possible to determine whether the reduction of communication volume is actually contributing to the reduction of the overall training time. Since the results seem to vary quite a bit for different models and datasets, such information is useful for determining whether the experiments are conducted for configurations that actually show a significant impact on the training time. There remains a possibility that the current method does not work as well for extremely large models trained with ZeRO 3 data parallelism, which is where the communication overhead really becomes a problem.

Limitations:
The limitations pointed out above are not explicitly stated in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes Distributed Lion, a new variant of Lion optimizer for distributed training. The proposed algorithm only requires to communicate binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. The theoretical analysis proves the convergence of the proposed algorithms. The empirical results show that the proposed algorithms have comparable model performance on CV/NLP applications but with significantly less communication overhead compared to the baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes Distributed Lion, a new variant of Lion optimizer for distributed training.

2. The proposed algorithm only requires to communicate binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost.

3. The theoretical analysis proves the convergence of the proposed algorithms. The empirical results show that the proposed algorithms have comparable model performance on CV/NLP applications but with significantly less communication overhead compared to the baselines.

Weaknesses:
1. According to Assumption 3.1, the convergence requires i.i.d. local datasets, while real-world distributed training typically uses non-i.i.d. local data.

2. In the empirical results, there seems to be no wall-clock time for training is reported. Note that the overall goal of communication reduction is to reduce the training time. Thus, it is important to report loss/acc vs. wall-clock time in the experiments.

Limitations:
The limitations are well discussed and addressed in this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
