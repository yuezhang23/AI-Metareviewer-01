id;text;label
Jkt42QYyEH;"REVIEW 
Summary:
This paper tackles an important problem in reconstructing interactive 3D scenes with language grounding. The authors proposed to use object-based modeling of different deformation fields over the dynamic NeRF pipeline and equip it with language embeddings for grounding interactions. The authors constructed two synthetic datasets OmniSim and InterReal for data collection and evaluations. Experimental results show that their methods significantly outperform prior methods on reconstruction and language grounding.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The problem of reconstructing interactive scenes with language grounding is an important topic to embodied AI, we have seen many related works that boost the development of robot perception.
- The construction of OmniSim and InterReal datasets could be beneficial for research in dynamic reconstruction and robotics.
- The authors showed significant performance improvement on their constructed dataset, outperforming existing articulated object reconstruction models by a large margin.

Weaknesses:
- Despite the good motivation, one major concern about its paper is its poor presentation in terms of notations and details. Several key detail designs are omitted in both Sec.3/4 and supplementary.
  - How are disjoint regions $\mathcal{R}$ defined? it seems from the start of the description that this knowledge is already given. How do we determine the number of subregions? Any prior used for learning the deformation in each subregion? Since jointly optimizing the belonging relationship of each point to each region and the deformation for each field is pretty difficult as far as I know. 
  - The notations used are confusing, especially in Sec.3.2 which is an important portion of text to help understand the methodology. In Eq.3, what does $\Theta$ mean? It is different from the $\Theta$ in Eq.2 but is it just another MLP prediction? how to determine them for each region?
  - As for the dataset curation, in each data sample will there be multiple objects being interacted (since you modeled many sub-deformation fields)?
  - The authors should mention if any priors were added to the implemented baselines as well because methods like CoGS were not originally designed to handle multiple objects if I'm understanding it correctly.
- The current dynamic reconstruction model still stays at the rendering level, some explorations on extending it to 3D meshes or simulated environments might provide more insights on using this model for future research.

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed LiveScene, a NeRF-based approach to enable indoor-scale controllable scene reconstruction and novel view synthethis. By extending K-Panes with a object-aware multi-scale space factorization, scene-level 3D space with articulated objects could be modeled with motion patterns via densely collected monocular video with camera poses. On exsiting benchmark datasets and newly proposed datasets OmniSim/InterReal, the porposed method LiveScene achieves the best overall performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The overall method is well motivated to address the more challenging indoor-scene level controllable NeRF. The introduction of control variables and their space spaces make the overall training feasible.

- The extensive experiments as well as demos proves the effectiveness of LiveScene, both quanlitatively and quantitaitively.

Weaknesses:
Though the overall resutls seem promising, I still have several concerns regarding the formulation and lack of clarifications in certain key aspects.

- The only additional attribute of overll space is modelled as a 3+alpha dimension space, how to cope with time variations? Are the time dimension implicitly encoded within the contro variables to cope with motions (such as open/close the door)? Or is there any explicit formulation of time dimension?

- What is the potential maximum number of objects within the scene? And what is the potential limitations when scaling up to more objects? In Tab.6 of supplement, 6 objects at most is validated. How about more diverse objects?

- Is it possible to encode more complex or fine-grained object control (e.g.,open left side door of a double-open fridge), especially when the training data mainly contains the fully open and fully-close state. Specifically, I am wondering what is the interpolation capability of the propsoed interaction-aware feature space and its generalization capability to unseen but correlated states.

- As mentioned in Appdix D, ‘Interaction Variable MSE’ mentions that the interaction values are fully supervised and GT labels are also used during inferenced to enble control. It would be good to see, in practical cases, without GT interaction values, what is the performance degradations, which could further strengthen the potential applciations and reveal potential limitaions.

Limitations:
Limitations have been partly addressed in the main paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
They introduce a language-embedded ""interactive neural radiance field"" that efficiently reconstructs and controls multiple objects within scenes. Factorization decomposes the scene into more local fields that can achieve local deformation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
My sense is that the technical novelty of this paper is high, though I'm not an expert in this domain. Additionally the evaluation seems thorough and the method seems well-considered.

Weaknesses:
At times, I feel the the language of the methods section can be made a little clearer. I had trouble following the motivation for a lot of the design decisions.

Limitations:
No issues here.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the complex challenge of reconstructing and controlling multiple interactive objects in complex scenes from monocular videos without prior modeling of geometry and kinematics. This task is critical for advancing fields like virtual reality, animation, and robotics, where understanding and interacting with 3D environments are essential. The proposed framework decomposes interactive scenes into local deformable fields. This factorization allows for precise modeling of individual objects’ interactions. Additionally, a multi-scale interaction probability sampling strategy is introduced to accurately sample interaction-relevant 3D points within these fields, enabling effective control over object dynamics in complex environments. The interaction-aware language embedding method generates dynamic language embeddings that adapt to varying interaction states. This allows users to control interactive objects through natural language commands, enhancing the interface's intuitiveness and accessibility. Authors also contributed OmniSim and InterReal datasets. These datasets are the first to offer scene-level physical interaction data, comprising 28 scenes with a total of 70 interactive objects. They provide a valuable resource for evaluating the performance of interactive scene modeling methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ The paper is well motivated and sets a clear difference from previous works. Results look good and promising. Figures and illustrations are helpful and informative.
+ The factorization technique that decomposes complex scenes into local deformable fields allows for more granular control and precise modeling of individual object interactions within a complex scene, addressing the high-dimensional challenge that previous methods struggled with.
+ By embedding language control within the interactive radiance fields, the framework allows users to manipulate and interact with 3D environments using simple language commands, greatly enhancing user accessibility and interaction fidelity.
+ Authors provided abundant demos on their project page, which is helpful

Weaknesses:
- Might need to slightly enlarge texts in figures.
- Not necessarily a weakness but authors can consider visualize some latent features (instead of illustrations like in fig2-4) to better show the decomposition of the high-dimensional feature space.
- Lack some qualitative results on real-world dataset and existing public dataset. Also, in the only InterReal qualitative results (fig.11), k-planes results were missed.

Limitations:
Authors discussed limitations in terms of closed vocabulary, caused by OpenCLIP. Given that authors did not show much real-world scene manipulation results, potential limitations in real-world scenes should also be discussed as such scenes are in general more complicated. No societal impact was discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zBG7WogAvm;"REVIEW 
Summary:
This paper proposes a method for decision-aware Bayesian experimental design, where the design is not optimized with respect to the most accurate posterior distribution of the latent parameters but rather with respect to the expected utility gain of the actual (down-stream) decision task.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
This is an innovative paper with high practical relevance. The proposed method appears sound and the corresponding neural networks well designed to suit the goal. Despite my questions and concerns (see below), I am positive about this paper overall and eager to increase my score should my points be addressed.

Weaknesses:
- The presentation of p(y_Xi | h_t) between Eq 3 and 4 is partially unclear to me. From the definition, it seems this is not actually a distribution but a set of distributions. To me, then notation p(y_Xi | h_t) appears to be quite the abuse of notation because we cannot readily read this it as a single distribution. Can you perhaps think about a different notation that makes this easier to parse and understand? Relatedly, in Equation 4, it appears that we compute an expectation over p(y_Xi | h_t). But how do we compute an expectation over a set of distributions? I think I get what the authors do and want to imply but to me this notation doesn’t help in understanding it.
- Equation 7: It seems we approximate the predictive distribution always by a Gaussian. I mean this of course works if the true underlying function is some kind of GP, but what if the true predictive distribution is far away from Gaussian? I don’t see this choice to be discussed properly so I consider it a weakness of this paper for now.
- The discussion of training and inference time can only be found in the appendix. Specifically, training speed seems to be substantial, which of course makes sense for an amortized method. However, I don’t see any discussion for when the training actually amortizes. That is, how many BED tasks do we need to run at minimum before the total (training + “inference”) time of the new method becomes better than those of the competing methods. More generally, I think a discussion of speed should be more prominent in the paper.
- 6.1 toy example was hard for me to understand at first. Is this just a standard BO task to find the point where the unknown function is maximal?

Limitations:
The paper discusses several limitations. I am missing a discussion on the initial overhead of training, which is usually substantial in amortized methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper looks at the problem of designing Bayesian optimal experiments taking into account the downstream decision making. At the core is a Transformer Neural Decision Process (TNDP) architecture that is trained to amortise the experimental design process whilst simultaneously inferring the optimal downstream decision.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- Relevant and interesting topic: Downstream decision making is what ultimately matters, so taking this into account when designing experiments to collect data can result in more cost- and sample-efficient learning.  

- Motivation for the paper as well as clarity of writing are excellent. Contextualisation relative to prior work can be improved as outlined in the next section.

- The proposed Transformer Neural Decision Process (TNDP) architecture is tailored to the BED problem, is well-explained and adds some novelty to the architectures typically used in the field.

Weaknesses:
### Sections 2.2 & 3.2 and Lindley's decision-theoretic BED [1]:

My main issue with the paper is the presentation of DUG and EDUG as novel. This framework was first formulated in [1], and is very well summarised in Section 1.3 of [2]. I strongly recommend the authors read that section, and present their Section 3.2 accordingly, acknowledging they follow Lindley, 1972. The questions/comments in the next 2 bullets are a consequence of this omission of literature.

- Second paragraph of Sec 2.2: I am not sure how the predictive distribution $p(y | \xi, h_t)$ is defined. I would think it is $p(y | \xi, h_t) = \mathbb{E}_{p(\theta |h_t)} [p(y | \xi, \theta)]$. Whether or not you compute/approximate the posterior $p(\theta |h_t)$, or seek to directly approximate $p(y | \xi, h_t)$ (eg variationally), I think you should explicitly define what this quantity is. 

- I am not sure how the utility $u(y_\Xi, a)$ is defined. From a Bayesian decision-theoretic approach, the utility has to depend on the state of the world $\theta$, as well as the experiments $\xi$ you are going to perform (which I guess is implicit in $y_\Xi$). So shouldn't the ""lowest level"" utility be a function $u(y, \theta, \xi, a)$, which you then integrate over $p(\theta|h_t)$, to obtain $u(y, \xi, a) = \mathbb{E}_{p(\theta|h_t)} [u(y, \theta, \xi, a)]$, then take $\max$ wrt $a$,  and finally integrate over the predictive $p(y |\xi, h_t)$ to obtain an expected utility, which can then act a design ranking criterion, as you do in Eq 4 and (cf Eq 2 in [2]).

### Related work: 

For a field that has such rich history and renewed interest from the ML community recently, the related works section is quite short and sparse on citations. Some areas that are missing include:
- Decision-theoretic BED: as previously discussed, the general framework of utility-based BED was developed by Lindley (1972).
- BED + RL: this work touches on some aspects of RL; It might be good to discuss relations recent works in the intersection such as [5] and [6] (in addition to those mentioned)
- Decision-theoretic approaches in related fields such as Bayesian Optimisation, e.g. [7], [8]
- Finally, I'm not too familiar with this line of literature, but  more recent work around decision transformers---is there any relation between TNDP with works like [9] and [10]?

### Other:

- Line 6: ""most recent BED methods use amortised inference with a policy network"" is not quite correct in the sense that no ""real inference"" (posterior updates on the parameters $\theta$) are performed. 
- Line 179: ""to ensure the framework satisfied the permutation invariance property of sequential BED"": not all BED problems are permutation invariant. For example, designing experiments for time series models (e.g SIR in [3] and [4]), permutation invariance does not hold. This aspect has been discussed in e.g. Section 3.3 of [3].
- Assuming you do want a permutation invariant architecture (most design problems fall in that category): by conditioning on $t$ as part of the global information (GI) set, I think you actually break that invariance. This is because encoding $(\xi, y)$ at time $t$ or at time $s$ will give you different outputs. As far as I can tell from Fig2b), $D_c$ does attend to GI. Could you please explain if that's the case or I have misunderstood something?

-----
#### References

[1] Lindley, D. V. (1972). Bayesian statistics: A review. Society for industrial and applied mathematics.

[2] Chaloner, K., & Verdinelli, I. (1995). Bayesian experimental design: A review. Statistical science, 273-304.

[3] Ivanova, D. R., Foster, A., Kleinegesse, S., Gutmann, M. U., & Rainforth, T. (2021). Implicit deep adaptive design: Policy-based experimental design without likelihoods. Advances in neural information processing systems, 34, 25785-25798.

[4] Kleinegesse, S., & Gutmann, M. U. (2019, April). Efficient Bayesian experimental design for implicit models. In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 476-485). PMLR.

[5] Mehta, V., Paria, B., Schneider, J., Ermon, S., & Neiswanger, W. (2021). An experimental design perspective on model-based reinforcement learning. arXiv preprint arXiv:2112.05244.

[6] Mehta, V., Char, I., Abbate, J., Conlin, R., Boyer, M., Ermon, S., ... & Neiswanger, W. (2022). Exploration via planning for information about the optimal trajectory. Advances in Neural Information Processing Systems, 35, 28761-28775.

[7] Neiswanger, W., Yu, L., Zhao, S., Meng, C., & Ermon, S. (2022). Generalizing Bayesian optimization with decision-theoretic entropies. Advances in Neural Information Processing Systems, 35, 21016-21029.

[8] Ivanova, D. R., Jennings, J., Rainforth, T., Zhang, C., & Foster, A. (2023, July). CO-BED: information-theoretic contextual optimization via Bayesian experimental design. In International Conference on Machine Learning (pp. 14445-14464). PMLR.

[9] Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., ... & Mordatch, I. (2021). Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 15084-15097.

[10] Zheng, Q., Zhang, A., & Grover, A. (2022, June). Online decision transformer. In international conference on machine learning (pp. 27042-27059). PMLR.

Limitations:
Some limitations of the work were outlined in the Discussion section of the paper. Regarding negative societal impact, the field of experimental design (which boils down to efficient data collection), generally warrants some discussion. 

The experiments presented in this paper mostly use synthetic data and do not have negative impact; the HPO experiment, which uses real data does not (directly) represent an application with negative impact. However, applying these methods in real-world applications, particularly if decisions directly affect humans, as in e.g. personalised medicine, could raise concerns around bias, fairness, explainability and privacy. 

I would suggest to the authors to add 1-2 sentences in their limitations section to acknowledge 1) the synthetic or semi-synthetic nature of the experiments, and 2) potential concerns that might arise when applying their method in real-world applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a transformer-based architecture for jointly sampling designs and decisions in Bayesian Experiment Design (BED) using a forward-looking criterion. The latter considers the improvement in maximum expected utility brought about by a new design-outcome pair, where the expectation is taken with respect to the predictive distribution of the model. The main innovation of the paper lies in the coupling between information gain and utility maximization in an amortized, transformer-based framework in the spirit of attentive neural processes. The performance of the new architecture is evaluated on a toy regression task and two more representative models, exhibiting stable performance gains over contender methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is clearly written, the ideas and formulations are stringent and well-justified, overall making it easy to follow and a pleasure to read (with the exception of Section 4.1, see below).

- The proposed architecture and training objectives are novel and seem to unlock both qualitative and quantitative improvements over existing methods. 

- The results indicate superior and stable performance of the proposed architecture on two interesting tasks, along a toy 1D GP model which seems to be a standard proof-of-concept task in the neural process (NP) literature.

Weaknesses:
- Some notational confusion can be avoided by consistently using the notation $a_{1:t}$ to denote a sequence of $t$ elements and $a_t$ to denote the $t$-th element in the sequence. Currently, $h_t$ denotes a sequence, but, e.g., $y_t$ denotes an element, and then again $\theta_{1:L}$ also represents a sequence. Also, P4L126 is an abuse of notation with slightly confusing wording, such as “the predictive posterior distribution over all possible designs”, whereas the predictive distribution(s) are over future \textit{outcomes}. This is in no way different than the posterior predictive in Bayesian (non-linear or linear) regression, where the posterior predictive is conditioned on the training data set and the set of (unlabeled) predictors available at test time. Hence, I struggle to understand the need for the convoluted abuse of notation, but I may be missing something. Also section 4.1 suddenly starts using bold font for vectors, which was not the case in the preceding sections. 

- Figure 2 is not particularly informative for the data flow, as it does not clearly communicate weight sharing, input-output operations and dependencies (left panel); the right panel comes out of the blue and is not well explained (i.e., what are the elements on the “left” and on the “top”); the description below on P6 does indeed disambiguate the idea behind the construction of the masks, but I believe it is best when figures support and enhance the text and not vice versa.

- Overall, I feel that Section 4.1 is the weakest link in the paper, and I believe the authors can think about optimizing the ratio of details dispersed between the main text and the appendix. For instance, there is no need to reiterate established transformer-based computations, but it could be helpful to explicate the construction of the masks, the representation types (e.g., vectors, sequences of vectors,...?), and the precise partitioning of the components into keys, queries, and values.

- According to my understanding, none of the contender methods in the experiments is an amortized method. Wouldn’t some of the existing amortized BED methods (e.g., as highlighted in the Related Work) make for suitable benchmarks, despite not optimizing for future decisions?

- The topic of model misspecification is never mentioned in the paper, even though the comprehensive review paper [1] states that it remains a major unsolved issue in BED and in amortized Bayesian inference more generally [2]. I believe this should also be acknowledged in the current paper and the authors can potentially think about quantifying the impact of model misspecification in a small ablation study in the final version of the manuscript.
 
I am happy to discuss these points with the authors and increase my score if they are addressed / clarified.

[1] Rainforth, T., Foster, A., Ivanova, D. R., and Bickford Smith, F. (2024). Modern Bayesian
429 experimental design. Statistical Science, 39(1):100–114.

[2] Schmitt, M., Bürkner, P. C., Köthe, U., & Radev, S. T. (2024). Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks: An Extended Investigation. arXiv preprint arXiv:2406.03154.

Limitations:
The authors openly discuss the current limitations of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tackles an important problem of designing experiments in a way that directly optimizes downstream decision-making tasks, going beyond just inferring parameters of interest. The authors make several valuable contributions:

1. They introduce the concept of Decision Utility Gain (DUG) to quantify how much an experimental design improves the expected utility of the downstream decision. 

2. They propose a novel neural architecture called the Transformer Neural Decision Process (TNDP) that amortizes both the experimental design selection and the approximation of the predictive distribution needed for decision-making. This unified amortized framework is a key innovation.

3. The authors develop a non-myopic training objective that looks beyond just the immediate decision utility to account for effects of the current design on future rewards.

4. Empirically, they demonstrate TNDP's effectiveness over traditional methods on various tasks like active learning, hyperparameter optimization, showing it can find informative designs and make accurate downstream decisions.

In summary, this work makes valuable conceptual and technical contributions to the area of Bayesian experimental design by pioneering decision-aware amortized methods. It opens up new research directions for further enhancing real-world decision-making via optimized experimental data acquisition.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a novel problem formulation by introducing the concept of Decision Utility Gain (DUG), which shifts the focus of experimental design from reducing parameter uncertainty to directly optimizing downstream decision utility. This new perspective is a creative departure from traditional Bayesian experimental design (BED) approaches.
- The application of amortized inference techniques to decision-aware experimental design can be considered an original contribution, as it represents a new domain for these methods beyond traditional BED.
- The empirical evaluation is comprehensive, spanning diverse tasks such as active learning, hyperparameter optimization, and synthetic regression problems. The results demonstrate the consistent superiority of TNDP over traditional methods.

Weaknesses:
- The authors could provide a more rigorous analysis of the properties and characteristics of the TNDP architecture, such as its convergence behavior, sample complexity, and theoretical guarantees (if any) regarding the quality of the proposed designs and decisions.
- The experimental evaluation, while comprehensive, focuses primarily on synthetic and benchmark datasets. While these serve as important proof-of-concept demonstrations, the paper could benefit from including real-world case studies or applications to further validate the practical utility of the proposed framework.
- While the amortized nature of TNDP is highlighted as a key advantage, the paper could provide a more detailed analysis of the computational complexity and scalability of the proposed approach. This analysis could include factors such as the training time required for different problem sizes, the memory footprint, and the scalability of the attention mechanisms used in the Transformer architecture.

Limitations:
- The authors mention the use of a basic REINFORCE algorithm for training the query head, which can lead to unstable training, especially in tasks with sparse reward signals. While they suggest the use of more advanced reinforcement learning methods as a potential solution, a more detailed discussion on the specific challenges faced during training and the trade-offs involved in selecting different RL algorithms would be beneficial.
- The authors mention that their model is trained on a fixed-step length, assuming a finite horizon for the experimental design process. A discussion on the limitations of this assumption and the potential difficulties in extending their approach to infinite horizon or open-ended experimental scenarios would be valuable.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zAuerb1KGx;"REVIEW 
Summary:
This paper proposes an improved approach to multi-label learning using $\mathcal{H}$-consistency bounds by introducing the multi-label logistic loss to effectively handle label correlations. It extends to various multi-label losses, ensuring Bayes-consistency across diverse settings, and includes efficient gradient computation algorithms for minimizing the proposed loss function. This work offers a unified framework with robust consistency guarantees, advancing beyond traditional methods in multi-label learning.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Introducing the multi-label logistic loss, which effectively addresses label correlations often overlooked by traditional binary relevance surrogates under Hamming loss.

- The paper establishes $\mathcal{H}$-consistency bounds for a wide range of multi-label losses, ensuring Bayes-consistency across diverse multi-label learning scenarios. This extends beyond previous research that primarily focused on specific loss functions.

- It offers a unified framework that accommodates various multi-label losses, including novel extensions and adaptations from standard classification. This is supported by efficient gradient computation algorithms specifically designed for minimizing the proposed multi-label logistic loss.

Weaknesses:
- The motivation and background of this paper lack clear logic and hierarchy. It is suggested to first outline the shortcomings of existing methods and then clearly present the research questions addressed in this paper.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores surrogate losses and algorithms for multi-label learning, focusing on \( \mathcal{H} \)-consistency bounds. It identifies the limitations of Hamming loss and introduces a new multi-label logistic loss that accounts for label correlations. The study extends this to a broader family of multi-label losses and adapts comp-sum losses from standard classification to multi-label learning. The authors propose a unified framework providing strong consistency guarantees for multi-label losses and describe efficient gradient computation methods for minimizing these losses.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors conduct a detailed analysis of the popular Hamming loss in multi-label learning when using smooth losses. They identify its sub-optimal dependency on the number of labels and its failure to account for label correlations, providing valuable insights into the limitations of existing loss functions. 
1. The authors introduce an improvement by presenting a novel surrogate loss, the multi-label logistic loss, which accounts for label correlations and benefits from label-independent \( \mathcal{H} \)-consistency bounds. This innovation addresses the identified drawbacks of existing loss functions and broadens the analysis to include a more extensive family of multi-label losses, including a new extension based on linear-fractional functions related to the confusion matrix.
1. The authors extend their work by adapting multi-label logistic losses to more comprehensive multi-label comp-sum losses. By demonstrating that this family of surrogate losses benefits from \( \mathcal{H} \)-consistency bounds and Bayes-consistency across any general multi-label loss, they propose a unified surrogate loss framework. This expands upon previous work that only established consistency for specific loss functions, showcasing the applicability of their approach.
1. The authors' writing is clear and well-structured, with each theoretical assumption and conclusion articulated distinctly.

Weaknesses:
1. In section 4, although the excellent properties of the proposed multi-label logistic loss are proven, providing a detailed explanation of each component of this loss would further enhance the reader's understanding of its superiority.
2. If the advantages of this loss could be demonstrated through experimental validation, it would be more intuitive for readers.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study surrogate losses and algorithms for multi-label learning via H-consistency bounds and introduce a novel surrogate loss, multi-label logistic loss in this paper. By broadening the H-consistency bounds analyses to more general multi-label losses and extending to multi-label comp-sum losses, the authors provide a unified surrogate loss framework for H-consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to follow.
2. The authors make comprehensive reviews of related works, including their pros and cons.
3. The authors provide rigorous theoretical analyses of the limitations of existing binary relevance loss, the H-consistency of the proposed multi-label logistic loss, and the extensions to more general multi-label losses. The theoretical contribution is important for multi-label learning.
4. The authors demonstrate the efficient computation of the gradient for the proposed multi-label logistic loss and conduct time complexity analyses.

Weaknesses:
1. I understand that this is a theoretical work, and experiments of empirical evaluations are not its focus. However, adding experiments to compare the proposed loss with commonly used multi-label losses on standard datasets would make the paper more comprehensive and appealing. Besides, it can also verify whether the proposed loss is effective in practice.
2. There is a typo in line 300.($1-\bar{L}_{ham}(\cdot, y)$).

Limitations:
Yes, the authors have adequately addressed the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper derives H-consistency bounds for binary-relevance style surrogate losses, as well as a new surrogate, for mutli-label learning problems, showing that the proposed multi-label logistic loss whose upper-bound on the Hamming loss is independent of the number of labels.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The $H$-consistency bounds provided in the paper are more informative than existing Bayes-consistency results, as they hold not just in the infinite limit.

The novel multi-label logistic loss allows upper-bounds that do not depend on the number of labels.

Weaknesses:
The paper does not provide any experiments. While this is OK for a theory paper, it does mean that the question of whether the new surrogate works better in practice remains unanswered (which should be reflected in the conclusion section, at least), for two reasons:
a) all the theory provides are upper-bounds, which might not be indicative of actual performance
b) while the theory provides better guarantees for the task loss if the surrogate is reduced to the level $\epsilon$, it might be that reducing the new surrogate is just much more difficult than optimizing binary relevance. In particular, if the computational cost for reducing the multi-label logistic loss to the same level $\epsilon$ as binary relevance is larger by at least $\sqrt{l}$, then, normalized for compute, the advantage of the new surrogate vanishes.

It is claimed that the gradient of the multi-label logistic loss can be computed efficiently, yet the presented formulas still contain sums over the entire $2^l$ entries of the label space. Even if they can be precomputed once, already at moderate label space sizes of l ~ 100 would these quantities be intractable.

It is annoying that most equations are unnumbered. Even if they are not referred to in the paper, your readers and reviewers might want to reference them.

the equation after l. 328 switches between $\mathbf{\mathsf{y}}'$ and $y'$; and $y''$ changes to $y$

l. 114: I'm not sure what the point here is of introducing the threshold $t$, if it is set to $0$ in the same sentence? Couldn't $t$ be simply absorbed into $h$?

l. 178-180; 208: Arguably, completeness does _not_ hold in practice, because there is some form of upper-bound (e.g., weights representable in the given floating-point format)

l. 231. Binary relevance is not just Bayes-consistent w.r.t. the Hamming-loss, but also works for precision-at-$k$

In the equation after line 542, I think $\bar{L}$ should be $\bar{L}_\mathrm{ham}$? 

l. 503: I think $q$ should be $q_i$, and there is a weird subscript on that line.

l. 174 consist -> consisting

Limitations:
I'm not sure if the proposed surrogate actually is tractable for label spaces with more than 50 labels.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zApFYcLg6K;"REVIEW 
Summary:
This paper introduces a new algorithm for constructing U-statistics under central DP. Compared to the naive method, the proposed estimator exhibits lower variance. The authors also derive a lower bound for private algorithms. Several statistical applications are presented to illustrate the methodology.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
U-statistics are widely applied in statistical inference. The improvements in private estimation presented in this paper are useful, and the theoretical results are solid.

Weaknesses:
The calculation of the privacy budget lacks precision.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the problem of private estimation of U-statistics. The authors propose a new thresholding-based approach using local Hájek projections to achieve nearly optimal private error in both non-degenerate and degenerate settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper provides solid theoretical foundations, including lower bounds for the private error and theoretical guarantees for the proposed algorithm.
2. The proposed method is applicable to a wide range of U-statistics problems, from hypothesis testing to subgraph counting in random geometric graphs.
3. The method aims to provide private confidence intervals for U-statistics, addressing a gap in existing literature.

Weaknesses:
1. The paper is difficult to read due to the heavy use of parameters and notations, many of which are not well-defined or explained, particularly in the algorithmic sections.
2. The manuscript provides non-asymptotic results for the DP estimators, but lacks the asymptotic normality results typical for non-private version of U-statistics, which are crucial for practical applications. I think the asymptotic variance of the private U-statistics will change compared to the non-private version. More discussion on expected on this difference. 
3. To provide private confidence intervals, the variance should also be estimated privately. This aspect is not thoroughly discussed, making the testing problem in Section 5 less meaningful.
4. There are no experimental results to demonstrate the practical performance of the proposed algorithms, which is a significant omission.
5. The paper only consider the 1-dimensional data $X$ throughout the paper. A general discussion of d-dimensional vector are needed because it may suffer from the curse of dimensionality, which will affect the generalizability of the results.

Limitations:
1. The paper should discuss the differences and potential advantages of the proposed method compared to directly adding noise to the estimators.
2. The authors should include asymptotic normality results for the DP estimators, similar to those available for non-private U-statistics.
3. The paper would benefit significantly from experiments that validate the theoretical findings and demonstrate the practical applicability of the proposed methods.
4. The author should specify the dimension of $X$ and discuss its impact on the results.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem of estimating U statistics under central differential privacy. U statistics are established minimum variance unbiased estimators for estimable parameters in the form $\mathbb{E} h (X_1, ..., X_k)$, where $h$ is a kernel and for all $i$ $X_i$ is i.i.d. from some underlying distribution. In other words, U statistics estimate averages of kernels applied to subsets of the data of degree (size) $k$. This type of problem arises in multiple statistical tests such as goodness-of-fit tests and Pearsons's chi-squared tests, uniformity testing, subsampling and other scenarios. While many methods have been studied for differentially private mean estimation, the research on private U statistics is in its early stage and has so far mainly focused on local differential privacy models and discrete data. This paper seeks to provide differentially private U statistics estimators achieving nearly optimal private error for both the case of non-degenerate kernels and degenerate kernels.

The main contributions of this paper are: i) it derives the lower bound for private algorithms for the non-degenerate kernel case (Theorem 1); ii) it finds that applying off-the-shelf private mean estimation procedures to U statistics estimation yields suboptimal error; iii) it proposes an algorithm that achieves nearly optimal private error in the non-degenerate kernel case, and evidence of near optimality for bounded degenerate kernels. 

The proposed algorithm (Algorithm 1) is based on representing U statistics via the Hájek projection, and leverages the fact that local Hájek projections enjoy strong concentration around the conditional mean. Basically, if all local Hájek projections $\hat h(i)$ are within a certain threshold distance from the pre-computed empirical mean $A_n$, the output $\tilde{A}_n$ on line 14 is going to be equal to $A_n$; if not, for every subset $S$ containing a bad index, $h(S)$ is replaced by a weighted combination of $h(S)$ and $A_n$. The choice of threshold $\xi$ ensures $L = 1$ with high probability, maintaining a balance between excluding bad data and preserving good data, while also keeping the sensitivity of the final adjusted mean $\tilde{A}_n$ small​, which is crucial for differential privacy. A lower bound for sub-Gaussian non-degenerate kernels is provided (Corollary 1) and Algorithm 1 is proven to match this lower bound. It is also shown that Algorithm 1 matches the lower bound for bounded degenerate kernels (Corollary 2).

The paper discusses a wide range of applications of the proposed method to uniformity testing, goodness-of-fit tests, Pearson’s chi-squared tests, symmetry testing, and sparse graph statistics.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is clear, well-structured and provides rigorous derivations and proofs to back the proposed methods and claims. 

The paper addresses a notable gap in current differential privacy research, which is U statistics under differential privacy. The authors derive lower bounds for both the private sub-Gaussian non-degenerate kernel case and the private bounded degenerate kernel case. These bounds support the proofs that the proposed method achieves i) near-optimality for sub-Gaussian non-degenerate kernels and ii) strong evidence of near optimality for the bounded degenerate case. These results are valuable in the context of the differential privacy research community.

The contributions are clearly highlighted.

I appreciate the effort by the authors to make the results as clear as possible for the reader. In particular, the table summary of the error of different private methods in Table 1 makes it easy to understand the relative error performance of different methods at glance; similarly, in a couple of instances the authors provide key intuitions behind the proposed methods, which helps break down important technical steps that are fundamental to the proposed method. The notation is also clear and consistent.

The proposed method has wide applicability, as demonstrated in the Applications section, where the authors describe the usefulness of the method spanning multiple statistical tests and sparse graph statistics. 

Computational complexity and alternative computationally efficient approximations of U statistics are also discussed.

Extensive proofs and supporting technical derivations are provided in Appendix, although I did not review it in detail due to time constraints.

Weaknesses:
I didn’t find any significant weaknesses in this paper. The paper is highly technical and notation-heavy, but as I described in the previous section, it still reads very clearly. A few of minor notes:

- Since [53] appears to be foundational to the development of the main proposed method, it is worth dedicating a short description of it and/or specification of which ideas in [53] have been built upon.
- Theorem 2 is not followed by a pointer to its proof in Appendix. Please reference the proof in Appendix.
- Limitations of the proposed methods are briefly mentioned throughout the paper, but I would prefer if they were addressed separately in a short dedicated paragraph or subsection, making them more easily identifiable by a reader skimming through the paper.

Limitations:
Limitations of the proposed method are sparsely mentioned throughout the paper. As I mentioned under ""Weaknesses"", it would be preferable to add a dedicated paragraph to the limitations, even if short.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies differentially private estimation of U-statistics (estimators for such statistics are averages of functions $h$ that depend on a number of i.i.d. samples $X_1,\dots,X_k$). This is a generalization of the commonly studied mean estimation problem where $k=1$ and such estimators with $k>1$ are widely applied across statistics. The authors are primarily interested in cases where $h$ is a subgaussian kernel i.e. the distribution of $h(X^k)$ is subgaussian or cases where the range of $h$ is bounded (and satisfies a certain degeneracy property).

The main contributions of the paper are as follows:
1) They first consider approaches that reduce differentially private U-statistics to differentially private mean estimation and argue that natural approaches result in estimators that are either suboptimal in either the non-private error terms or the private error-terms. The estimators they consider are a naive estimator that reduces to the i.i.d. case by computing the function $h$ on a partition of the dataset before applying a subgaussian mean estimation algorithm on the resulting sample of function values, and a more complicated estimator that generalizes the CoinPress algorithm to work with weakly dependent samples. The former has suboptimal non-private error while the latter has a suboptimal privacy term (the dependence on $k$ is suboptimal).

2) They then consider a different strategy inspired by work on privately estimating the sampling probability for Erdos-Renyi graphs. This strategy exploits the concentration of the 'local Hajek projections' around the true mean.  The idea is to classify coordinates into good and bad coordinates respectively based on how close their projections are to the optimal non-private statistic, and reduce the local sensitivity of the average being computed by reducing the influence of bad coordinates by reducing the weight of the corresponding terms in the average. They can then compute an appropriate smooth upper bound to the local sensitivity of this average and add less noise. They use this idea to obtain a general result for bounded kernels, and then use it to get the optimal rate for subgaussian-nondegenerate kernels, and a bound for general degenerate bounded kernels. They also provide some indication that their bound for general degenerate bounded kernels may be optimal.

3) They also show that their results can be used to privatize 'subsampled' estimators with similar error rates that are computationally much more efficient. Finally, they apply these results to settings where U statistics are used such as various hypothesis testing problems.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1) U-statistics are widely used across statistical testing and estimation, and have been relatively understudied in the privacy literature. This paper explores them quite generally and does a good job of suggesting problems for future work.

2) They do a good job of explaining how natural extensions of traditional DP mean estimators perform sub-optimally in estimating U-statistics.

3) The estimator based on local Hajek projections (and smooth sensitivity) seems quite technically novel and interesting.

Weaknesses:
1) In the applications section, it would be good to discuss existing private algorithms for the corresponding tasks (if there are any) and compare the bounds that are obtained.

2) In the Hajek projection algorithm, it would be nice if they explained how they build on the techniques from [Ullman Sealfon NeurIPS 2019]- which parts are borrowed from that work and which parts are new.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
z86knmjoUq;"REVIEW 
Summary:
The paper presents a new approach called Prompt Evolution with Graph ODE (PURE) for non-distributed fluid dynamics modeling. PURE first learns from historical observations and system parameters in the frequency domain to explore multi-view contextual information, which can efficiently initialize the cue embedding. Interpolations of the observation sequences are then merged into the graph ODE so that the time evolution of the model-adaptive cue embeddings can be captured. These time-evolving cue embeddings are then incorporated into the underlying predictive model to overcome spatio-temporal distributional variations. In addition the paper minimizes the mutual information between the cue embeddings and the observation embeddings to enhance the robustness of the model to different distributions. Finally, extensive experiments conducted on various kinds of benchmark datasets validate the superiority of the proposed PURE compared to various baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The idea of the paper is novel. It is the first to link prompt learning to dynamic system modeling for out-of-distribution problems. 
 
2. This paper is technically sound. PURE first learns initialized prompt embeddings from historical observations and system parameters, and then employs a graph ODE with interpolated observation sequences to capture the continuous evolution of their model adaptation under out-of-distribution changes.  

3. The experimental results show the effectiveness of PURE in different challenging environments.

Weaknesses:
1. The contribution of the proposed method in dealing with the OOD problem needs to be further clarified since the advantages of PURE over the previous efforts, such as Refs. [7, 67, 14, 72], etc., to address the OOD problem are not listed.

2. The writing of the paper needs to be improved. Some of the symbols in the method description section are not defined, e.g., what do P and N in Equation 9 refer to?

3. The experiment is not comprehensive enough. (a) The reasons for selecting baselines are not explained. Data augmentation [66, 7], invariant feature learning [39, 69, 38], adversarial training [67, 7], and domain adaptation [32, 14] are mentioned in the paper in related work for solving the OOD problem, but they are not be compared as baselines in the experiment. (b) The experiments in this paper do not state whether noisy data are considered. (c) The authors just give a brief description of the results without analyzing the reasons behind the high performance.

Limitations:
The authors list limitations in the appendix but do not mention them in the main text. It is recommended that the author make a description in the main text.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
- The paper aims to improve the out-of-distribution (OOD) generalization of fluid dynamics modeling.

- Two types of OOD scenarios are targeted: OOD across different systems and OOD within the same system across different timestamps.

- The paper proposes a framework named PURE, composed of modules including:
    - Multi-view Context Exploration, which explores spatio-temporal data using both the attention mechanism and the frequency domain;
    - Time-evolving Prompt Learning, which incorporates the interpolation of observation sequences;
    - Model Adaptation with Prompt Embeddings, which leverages time-evolving prompts to mitigate temporal distribution shifts.

- Extensive experiments on a range of fluid dynamics datasets support the claim.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Significant topic: OOD generalization in fluid dynamics modeling.

- Well-motivated, as OOD generalization is a crucial challenge in this field.

- The presentation effectively delivers the message.

- Extensive experiments have been conducted.

Weaknesses:
- My major concern with the paper is that the OOD challenge in dynamics modeling is not well-formulated. The paper describes the OOD scenario verbally as ""*different dynamical systems could involve different parameters in underlying rules*"" and ""*during long-term auto-regressive forecasting, the input data distribution could vary hugely during temporal evolution,*"" which is straightforward and easy to understand. However, the mathematical formulation of these scenarios is absent. This formulation should be the foundational basis of the topic, as we need to clearly define the problem before addressing it.

- Given the lack of mathematical formulation of the challenge, I find myself lost in the proposed approach section, unsure of the necessity for specific components. While I understand the function of each component, I cannot see why it is needed or which gaps it aims to bridge in the absent mathematical framework.

- Why is the proposed method termed ""prompt""? Is there a connection to prompt tuning in large language models?

- How do you quantify the distribution shift in dynamics modeling? Can you rank the 'difficulty level' of OOD generalization in your experiments and analyze in which scenarios your method stands out and why?

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper pioneers the connection of prompt learning with dynamical system modeling to address the challenge of out-of-distribution shifts. The proposed PURE method initializes prompt embeddings by learning from historical observations and system parameters.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The paper is easy to follow. 
2.The proposed method is sound and innovative.
3. The authors provide theoretical proof and show comprehensive experimental comparisons.

Weaknesses:
1. Some results may be incorrectly labeled as suboptimal in table, and there are errors in the use of some symbols.
2. The explanation of the experimental results is not detailed enough, making some experiments difficult to understand.
3. The proposed method is aimed at OOD (Out-Of-Distribution), but the experiments lack comparison and discussion with methods specifically targeting OOD, such as [1] and [2].
Reference:
[1] Kirchmeyer, Matthieu, et al. ""Generalizing to new physical systems via context-informed dynamics model."" International Conference on Machine Learning. PMLR, 2022.
[2] Yin, Yuan, et al. ""LEADS: Learning dynamical systems that generalize across environments."" Advances in Neural Information Processing Systems 34 (2021): 7561-7573.

Limitations:
This method does not apply to real-world scenarios, such as rigid dynamics modeling and traffic flow forecasting.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a graph ODE-based approach for OOD fluid dynamics modeling. PURE aims to learn time-evolving prompts via graph ODE for adaptation of spatio-temporal forecasting models on OOD scenarios. To address temporal distribution shifts, the interpolation of obersvation sequences are combined into graph ODE framework to learn evolution of prompt embeddings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper proposes a new approach that connects prompt learning and dynamical system modeling which addresses OOD shifts.
- By learning time-evolving prompts that adapt to changes in system parameters and temporal evolution, the approach can enhance model robustness.
- The paper provides theoretical analysis on incorporating observations during evolution.
- Experiments on diverse benchmarks show generalization ability to OOD and different prediction length.

Weaknesses:
As I am not an expert in this field, I am unable to find major concerns or weakness of the approach.
- As the method is based on attention, the proposed approach may have limited scalability and take long computation time. Is there a comparison on these with the previous works?

Limitations:
The limitations are explained in Appendix I.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
z7h7zMgyPJ;"REVIEW 
Summary:
This paper presents an efficient and simple weak-to-strong learner that has optimal in-expectation error. In weak-to-strong learning, we are given a dataset of $m$ points from a distribution, and a $\gamma$-weak learner that returns hypotheses from a class of VC dimension $d$. AdaBoost, which is a textbook weak-to-strong learner, makes $O(\ln(m)/\gamma^2)$ total invokations to the weak learner, and the best-known analysis for it shows that it suffers an in-expectation error $O\left(\frac{d\ln(m/d)\ln(m)}{\gamma^2 m}\right)$. Larsen and Ritzert (2022) constructed a weak-to-strong learner, that has expected error $O(d/\gamma^2 m)$. Furthermore, they showed that this is the optimal error that one can obtain from $m$ training examples and a $\gamma$-weak learner. However, the weak-to-strong learner by Larsen and Ritzert (2022) makes $O(m^{0.8}/\gamma^2)$ invokations to the weak learner --- which is exponentially worse than AdaBoost. Another bagging-based-boosting algorithm due to Larsen (2023), which also achieves the optimal expected error of $O(d/\gamma^2m)$, makes only $O((\ln m)^2/\gamma^2)$ invokations to the weak-learner. This is still a log factor worse than AdaBoost. Could we then hope to obtain a tighter analysis of the error of AdaBoost, and show that it obtains the optimal error with only $O(\ln(m)/\gamma^2)$ invokations to the weak learner? Unfortunately, no. Høgsgaard et al. (2023) showed that AdaBoost necessarily suffers an expected error which is at least $\Omega(d\ln(m)/\gamma^2 m)$.

Can we then at least shoot for a different weak-to-strong learner that attains the optimal expected error of $O(d/\gamma^2m)$, and also invokes the weak learner only $O(\ln(m)/\gamma^2)$ many times (which is the AdaBoost gold standard)? This paper answers the question in the affirmative, with a remarkably simple weak-to-strong learner that they call Majority-of-29. The algorithm is exceedingly simple to describe: Partition the training dataset into 29 disjoint sub-samples of size $m/29$ each. Run AdaBoost on each subsample, and return the majority vote over the AdaBoosts. Since each AdaBoost makes only $O(\ln(m)/\gamma^2)$ calls to the weak learner, and we run a constant (29) many AdaBoosts, the total number of calls to the weak learner is $O(\ln(m)/\gamma^2)$ as required. Further, using an analysis similar to the recent majority-of-3-ERMs algorithm of Aden-Ali et al. (2023), the authors are able to show that the expected error of Majority-of-29 is $O(d/\gamma^2m)$. The analysis from that work does not extend in a trivial manner, and the authors are required to make appropriate technical modifications and enhancements. The number 29 emerges from the analysis --- the authors require showing a new generalization bound for margin-based classifiers (they show a generalization bound of the order $O((d/\gamma^2m)^{\alpha})$), for $\alpha=1/14$, and this lets them obtain the result for Majority-of-$g(\alpha)$, where $g(\alpha)=2/\alpha+1$. The authors conjecture that the analysis of the generalization bound could be improved, and a Majority-of-3 might well suffice for optimal error.

Finally, the authors also do a (somewhat-limited) empirical comparison of the the performances of the three optimal weak-to-strong learners mentioned above (LarsenRitzert, Bagging-based-boosting, Majority-of-29) as well as AdaBoost. The authors find that for large datasets, Majority-of-29 outperforms the other optimal weak-to-strong learners. On the smaller datasets, the authors find that Bagging-based-boosting outperforms Majority-of-29.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The weak-to-strong learner that the authors propose is optimal, and also requires the fewest calls to the weak learner among all optimal weak-to-strong learners that we know. More importantly, it is exceedingly simple and elegant. It also empirically outperforms the other optimal weak-to-strong learners (at least in the experiments performed by the authors). It is also nice to see that the analysis technique from Aden-Ali et al. (2023) finds new applications. The paper is well-written, sets up the stage (along with relevant prior work) well in the first two sections, and provides a nice high-level summary of the formal analysis in Section 3.

Weaknesses:
While the theoretical contribution is substantial and undeniable, arguably, the experimental section is extremely limited (which is okay, and the authors admit this at the end, but this is still a limitation, especially if we want to draw conclusions about the empirical performance of the different weak-to-strong learners). The authors only perform experiments on 4 real-world datasets---there are admittedly many more out there, even just in the UCI repository. Could the authors at least elaborate on their rationale behind choosing the datasets that they did? (e.g., was it a random subset of 4? was it the first 4? was it the best 4 from 20 that they observed this trend on?) How might one believe that there is no cherry-picking of datasets involved? The authors make two conclusions from their experiments: 1) on larger datasets, Majority-of-29 outperforms both Bagging-based-boosting and LarsenRitzert. 2) on smaller datasets, Bagging-based-boosting outperforms Majority-of-29. Importantly, the former conclusion is drawn from results on just 3 datasets, and the latter is drawn from just 1! This can really make one skeptical about whether they should truly believe these conclusions. It is okay that this is just a pilot empirical study, but such claims call for significantly larger empirical validation. Also, please see the questions below.

Limitations:
The authors have adequately addressed any limitations that I can foresee.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new Boosting algorithm, MAJORITY-OF-29, which achieves provably optimal sample complexity and is remarkably simple to implement. The algorithm partitions the training data into 29 disjoint subsets, applies AdaBoost to each subset, and combines the resulting classifiers through a majority vote. This approach not only matches the asymptotic performance of AdaBoost but also improves upon previous weak-to-strong learners in terms of simplicity and runtime efficiency.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
1. The paper introduces a novel method and provides detailed theoretical analysis.

Weaknesses:
2. Existing experiments fail to demonstrate the effectiveness of the proposed method, and there is a lack of analysis and discussion on current experimental results.

Limitations:
Limitations are discussed.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a new boosting algorithm: partition training data into 29 pieces of equal size, run AdaBoost on each, and output the majority vote over them. The authors prove that the sample complexity of MajorityVote29 is optimal and its running time is the same order as AdaBoost. Experimental results are also attached, which corroborate their theoretical findings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Very strong and interesting result
- Mathematically sound, based by my judgement
- Good presentation, self-contained and well-structured

Weaknesses:
N/A

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
z6reLFqv6w;"REVIEW 
Summary:
The article proposes a learning scheme aimed at detecting emergent quantities from time series data of systems made of many interacting parts, such as the brain. To this end the authors combine ""minimum mutual information"", a previously introduced emergence criterion, with SMILE, a differentiable lower bound estimator for mutual information. Differentiability is crucial for the loss function to be optimizable efficiently. They apply this architecture to two examples: First, a series of random bit strings with time-correlated parity, where parity is considered the emergent quantity. Second, real-world data of macaque brain activity. The approach successfully identifies parity in the first example. The authors claim that an emergent feature has been learned for the second example also.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
While the individual parts of the learning scheme are not new, their combination into a differentiable architecture is original and seems like a promising direction to me. The analysis seems sound, even though I found the presentation at times a bit hard to follow as some parts seem to be missing. The individual quantities are mostly clearly defined and the individual results are statistically significant in terms of error bars.

Weaknesses:
From the article alone, I could not fully understand the architecture and its training procedure that is illustrated in Fig. 1. I could not find code that would reproduce it, or a detailed pseudo-code description of the algorithm. It is unclear to me what emergent feature was found for the monkey example, or how Fig. 4 proves that any such feature was found. While the architecture and direction seems promising to me, a few more benchmarks would help make the case that this scheme can find emergent features in many settings. The two examples shown are one toy example with unnatural time dynamics and one real world example where it is hard to understand the dynamics from first principles. Benchmarking this new method on more standard examples with emergent behavior, such as Ising models, would be more convincing.

Limitations:
There is no open access to the code.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a method for learning the causally emergent representation of time series data. Based on the Partial Information Decomposition (PID) and ΦID definition of emergent variables, the paper utilizes variational information lower bounds to estimate and optimize the emergence objective function. The paper further includes a Minimum Mutual Information term and a penalty term, to reduce redundancy and discover diverse emergent variables, respectively. Experiments on a synthetic dataset and a primate brain activity dataset show that the method is able to discover diverse causally emergent representation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Discovering causally emergent representations is a very interesting topic, and has significance in a wide range of scientific disciplines. The paper is inspirational and written clearly. Although the components of the method, i.e. definition of emergence objective function, and variational bounds for mutual information, are not new, their combination to discover causally emergent representations in a learnable way is interesting, and to my knowledge, novel.

Weaknesses:
As discussed above, the novelty is a little limited. This can be compensated by solid evaluations with a wide range of interesting datasets. I think the place the paper needs most improvements is more diverse and extensive evaluations. The paper can benefit from a few more datasets, both synthetic and real world, including the other datasets used in [1] and other references. If there exists baselines for discovering causally emergent representations, those baselines should also be compared against.



Reference:

1. Rosas, Fernando E., et al. ""Reconciling emergences: An information-theoretic approach to identify causal emergence in multivariate data."" PLoS computational biology 16.12 (2020): e1008289.

Limitations:
The authors did not explicitly state the limitation of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a method for identifying emergent variables in time series data through a novel machine learning architecture. It uses unsupervised learning for representation and information theory to find emergent properties in systems, which are often complex and not easily describable at the microscale level. The paper is motivated by the fact that unsupervised learning can be a powerful tool for identifying emergent properties, but current approaches limit to only information theory.

The method rests on maximizing an objective defined by subtracting the mutual information of state variables at time t and the coarse graining at time t + 1 from the mutual information of the coarse grainings at t and t + 1. In other words, the amount of emergent information. Information theoretic definition of emergence is thus used to facilitate unsupervised learning. The method is tested on synthetic data and real-world ECoG brain activity data, demonstrating its ability to identify known and novel emergent features effectively.

Experiments are conducted on a synthetic dataset and a macaque brain activity dataset. For the synthetic dataset, the method is able to estimate the ground-truth value of psi (the difference that is central to the objective function). For ECoG data, skip connections were introduced into the architecture, and once again found emergent representations. 

The paper concludes with a discussion on related (info theory) work, limitations, and future steps.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
### Clarity 
- Diagram features are well designed and results features are clear and salient
- Though writing is somewhat unstructured, the shorter-range explanations are well-done
- Methodology is given in detail. Lots of helpful explanation of relevant information theory, as well as the overall approach

### Quality
- Good to have primate brain data, though more interpretation would help 
- Covers all the basic needs for a new method: real data, novel setup, suitable metrics (though they need more explanation)
- Experimental setup is well-designed to demonstrate that emergent variables are being learned 

### Originality
- As far as I know, applying the information theoretic definition of emergent variables as an objective and training in this setting is novel

### Significance
- An innovative idea that shows promise. While there could be more experimentation, this is a promising and new direction.

Weaknesses:
### Clarity
- It's not immediately clear how to interpret results. The paper shows figures, but it doesn't explain them much. Interpreting them requires a lot of re-reading the methods section
- Writing is somewhat verbose and unstructured, and occasionally reads like a process statement

### Quality 
- This idea is compelling and innovative! The loss built on MI of coarse grainings and state variables is intuitive while creating a solid foundation for taking advantage of the capabilities of unsupervised learning
- On the ECoG dataset, giving intuition/semantic understanding of emergent features (or at least attempting interpretation) would be cool
- Limited experiments on real data in general - ultimately, only one experimental setting is shown as far as I understand. The synthetic problem, while useful, is simple
- Lacking baselines or extensive comparison to existing methods, even if purely information-theoretic

### Significance
- It would help to have clearer comparison to existing methods so that we could see the value-add of this innovation, not just the novelty and value alone

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel objective function and deep learning architecture that are targeted to extract emergent latent representations from time series data. Motivation is very clear. The definition of emergent latent representation interesting and useful. The utilization of mutual information estimators (lower bounds thereof) is smart. Evaluations are restricted to a fully artificial and a fully neurobiological dataset.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The study of emergence and its conceptual and mathematical formalization is of general interest to neural information processing and the involved (part-of) cognitive science subdiscipline within. The authors utilize an existing definition thereof [30] as well as an approximation technique of a lower bound on mutual information (SMILE, [32]), which they combine in a highly elegant manner to yield their learning architecture. 

The usage of a linear temporal predictor with learnable residuals is a great way to bootstrap the system’s abilities. 

Even multiple emergent latents can be successfully extracted. 

A real-world dataset indicates applicability beyond artificial data. 

Paper is very well written – relatively easy to comprehend and all steps taken are very well motivated and sufficient background is provided.

Weaknesses:
System evaluations are minimalistic and not as convincing as I had hoped. Both, comparisons to potential baseline algorithms as well as more ablations are missing. 

Furthermore, one artificial dataset and one not well-motivated real-world neural dataset seems not enough to warrant publication. 

In particular, I would have expected at least one if not multiple DL/Neural Network baselines that do not pursue the information-theoretic objective but simply a standard temporal prediction objective. Those probably do not work on the parity problem at all but at least an attempt seems needed. That is, use a DREAMER-like world model learning architecture with probabilistic latents and see if structure emerges. 

Ablations could have explored more than just the same architecture without the penalty / adjustment term or without the macroscopic estimator. Further, in the artificial dataset the correlation coefficients $\gamma$ are quite high – was this necessary? When does this break down? Evaluations with a non-linear prediction pipeline would also be useful.

Limitations:
Unclear how robust this is as ablations and comparisons are not very extensive.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z6KNvOe9zQ;"REVIEW 
Summary:
This paper aims to explore the use of weak supervision signals in multimodal interleaved image-text data to pretrain visual encoder, compressing the distribution of high-level features into the visual encoder. The paper employs contrastive loss and autoregressive loss to train the model. To prevent the collapse of visual representations, an entropy maximization constraint is applied. The paper derives the equivalence of maximizing the mutual information between the model's input and output as a latent compression and entropy constraint. The proposed pre-training method, called LCL, achieves performance comparable to CLIP on paired data while better utilizing the supervision information in interleaved data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper explores how to use weak supervision signals in more general interleaved image-text data to accomplish visual pre-training. Its advantages are as follows:
1. Unlike previous approaches that fine-tune pre-trained visual models to align visual representations with the text space (Flamingo, LLaVA), this paper explores how to train visual models from scratch using interleaved image-text data. This is a meaningful exploration.
2. To prevent the collapse of visual representations, where autoregressive generation relies solely on textual information, this paper imposes an entropy constraint and further derives it as optimizing mutual information. This approach aids in model training.
3. Extensive quantitative experiments have validated the effectiveness of the visual models trained using this approach.

Weaknesses:
This paper has the following areas for improvement:
1. In some cases, the textual context may have little relevance to the image. It is worth investigating whether such data could harm the model's performance.
2. The paper lacks qualitative experiments to further demonstrate the effectiveness of the method. Designing reasonable visualization analyses would help to further elucidate the advantages of the approach.
3. Similar to CLIP, further demonstrating the model's transfer learning performance through domain adaptation tests and few-shot metrics would be beneficial.

Limitations:
The paper's discussion on data bias and energy consumption limitations is commendable; however, it could further explore issues related to data privacy.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a vision backbone pre-training method named Latent Compression Learning (LCL) to utilize interleaved image-text data. The proposed LCL approach maximizes mutual information between the inputs and outputs of a GPT-like model in autoregressive manner. The proposed method integrate both discriminative and generative objectives by contrasting preceding context and generate subsequent text based on visual representation. The extensive experiments demonstrate that LCL not only matches the performance of existing models like CLIP on paired datasets (e.g., LAION) but also effectively leverages interleaved pre-training data (e.g., MMC4) to learn robust visual representations from scratch.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well written and easy to follow.

2. The paper introduces a new pre-training method, Latent Compression Learning (LCL), which utilizes interleaved image-text data for visual backbone pre-training for the first time. And this can effectively leveraging large-scale web-crawled data, which is easier to crawl compared to the image-text pairs.

3. Extensive experiments are conducted, demonstrating the effectiveness of the proposed method on both paired datasets (e.g., LAION) and interleaved datasets (e.g., MMC4).

Weaknesses:
1. From Table 5, it appears that solely leveraging image-text pairs with LCL does not provide benefits over the CLIP baseline. However, when using the MMC4 dataset, which is manually composed of interleaved text, there is significant performance improvement on downstream tasks. I am curious whether this performance gain results from the increased number of training samples (i.e., the total number of images used during training).

2. According to Table 3, utilizing original interleaved datasets such as Obelics does not yield any performance gain. In comparison, the MMC4 dataset requires more computation for data filtering with the CLIP score and the use of image-text pairs to create interleaved data. It is unclear how to efficiently utilize the original interleaved data directly crawled from the web. Do you have any insights on the differences between these two types of interleaved datasets?

Limitations:
The authors have addressed the limitation in their manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tackles the problem of vision model pre-training. More exactly, it aims to exploit the interleaved image-text data that is very prevalent on the Internet. It proposes Latent Compression Learning that maximises the mutual information between the inputs and outputs of a causal attention model. When visual pre-training is applied to interleaved image-text data, visual latents are extracted using a visual encoding network and then combined with the text and fed to the causal model.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper tackles an important task and proposes an interesting method that may be of interest to the research community.

Weaknesses:
While the method seems interesting, my main concern is related to the experimental part that I find confusing. For example for BEiT3 the numbers reported are different from the ones reported in the paper.

Also, I think that for Tab 6, more multi-modal LLMs need to be included. While there can be a debate on fair vs unfair comparison, I think that you present results on a dataset these need to be complete. So, they can be greyed out, put in a different section, etc and explained why the comparison is not fair, but I don't think it's suitable for models that perform better to not be included at all. So, missing comparisons:

Fang, Yuxin, et al. ""Eva: Exploring the limits of masked visual representation learning at scale."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
Zou, Xueyan, et al. ""Generalized decoding for pixel, image, and language."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

or even some very recent ones for the sake of completeness:
Sun, Quan, et al. ""Generative multimodal models are in-context learners."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
Liu, Haotian, et al. ""Improved baselines with visual instruction tuning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

Limitations:
Limitations are barely discussed at the end of the conclusions. Some limitations can be inferred from the rest of the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper pre-trains models with a combination of a contrastive image-text objective and a generative language objective. The authors provide many results on image classification and vision-language tasks suggesting the competitiveness of the method in controlled settings.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1. The paper is well framed and motivates nicely the need to pre-training on interleaved data.

S2. The paper gives good intuition about what the various equations mean, making the manuscript more accessible.

S3. Consideration of many pre-training datasets including LAION-400M, MMC4, and OBELICS.

S4. Extensive objective ablations spanning both contrastive and generative losses.

Weaknesses:
W1. [MAJOR] The paper presents the objective as novel (L44-54); however, it seems similar to CoCa (Yu et al., 2022.), which also employs a contrastive loss and a next token prediction loss. Can the authors clarify the differences and why the formulation is novel?

W2. It seems equation 3 appears in prior work; however, when it is first presented it seems to be presented as a novel insight. I recommend making the attribution to prior work more clear before introducing the equation.

W3. In the relation to previous pre-training tasks, it is important to also relate to CoCa. It seems the objective is pretty much the same suggesting that the objective is not actually a contribution of the work. Is there any reason CoCa is not mentioned here given the similarities?

W4. Make sure it is clear that you train on sequences with more than one image per sample (I am assuming this is true because you train on MMC4, but when explaining the objectives you include only one sequence for simplicity). 3.3 is a nice place to add this information. Also any special tricks to get multi-image to work? If so, it could also be nice to mention this.

W5. Why are the numbers for Flamingo in Tab 1 for IN-1k so low? Flamingo uses a pre-trained vision backbone, so I expect numbers to be good here.

W6. Is the COCO CIDEr evaluation protocol zero-shot? If so the number in table 4(a) of 87.5 looks extremely high relative to open flamingo and Idefics. Please double check this number and if few-shot prompting is used here, please make this clear. Also why is Gen. only worse than Con. only for captioning. How is contrastive learner able to do captioning?

W7. In the frozen transfer setting in Tab. 6 are all models fine-tuned on the same data? If so, what data? The specifics of the experiment are not clear to me, making it hard to interpret the results.

Limitations:
The authors address limitations in a dedicated paragraph.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z4eVwH484M;"REVIEW 
Summary:
This paper aims to improve vectorized HD map construction for autonomous driving. Inspired by the global feature association in traditional offline HD mapping, the proposed MapUnveiler processes input frames in a clip-based manner and hopes to resolve occlusions using information from previous frames. Built up MapTRv2, MapUnveiler introduces clip tokens together with the Inter-clip and Intra-clip Unveiler modules to update the map queries with temporal information. Experiments on nuScenes and Argoverse2 datasets demonstrate the superior performance of the proposed method, especially on highly-occluded scenes.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The idea of incorporating and aggregating clip-level information for online vectorized HD mapping is reasonable and is more akin to how humans drive. The proposed method has more thoughtful designs than early works such as StreamMapNet to better handle occlusions and incorporate long-range information. 

2. The proposed MapUnveiler obtains state-of-the-art results in various experimental settings. The improvements over previous methods are especially prominent in the large 100mx50m setting and the highly-occluded scenes collected by the authors. 

3. Extensive ablation studies enumerate the choices of almost all hyper-parameters or model components, which helps better understand and break down each element's contributions.

Weaknesses:
1. The clarity of the method description is poor, making it very hard to thoroughly understand the proposed architecture. Details are discussed below:
    
    - The method explanation is not self-contained:  i) The Inter-clip Unveiler section refers to the TTM and directly skips all details. There is no information at all about how is the compact memory token generated from the denser map queries;  ii) The ""loss"" section refers to MapTRv2 and again skips all details. The authors should not assume the general audience to be aware of the concrete details of TTM and MapTRv2. The core formulation of these components should be elaborated with texts or equations, while full details can go to the appendix.
    - The definitions of the temporal window T and the stride S are unclear. Based on the text descriptions and the common definition of stride, my understanding of ""T=3 and S =2"" is that ""each clip has 3 frames, and every two consecutive frames have a temporal gap of 1."" However, the symbols in L177-178 seem to suggest other meanings of T and S.
    - The description of the inference mechanism is also vague. Is the MapUnveiler executed per frame or per clip? Figure 2 seems to suggest the per-clip inference where the predictions of T frames are obtained together. If this is the case, does it hurt the actual response frequency? 
    
     In short, Section 3 of the paper lacks significant details, and I cannot properly understand MapUnveiler's exact formulation. Given that the authors answer ""No"" to Question 5 of the Checklist, I have to raise concerns about the paper's reproducibility.

2. There is no detail on how the pre-training and fine-tuning are conducted. Do you initialize the MapNet by training MapTRv2? If this is the case, how are the training epochs split for the MapNet pre-training and the end-to-end MapUnveiler fine-tuning?  If the 24/6 epochs for nuScenes/Argo2 are only for the fine-tuning stage, then the comparisons in the main table are unfair, as other methods in the table have not fully converged. 

3. The main comparison results are incomplete. Most previous papers provide the nuScenes results of both short and long training schedules, but the main table only presents short-schedule results. Considering the last question about the pre-training and fine-tuning, the authors should complement the table with long-schedule results to show that MapUnveiler can obtain consistent performance boosts when all the methods are fully converged. This concern is backed up by the fact that MapUnveiler's improvement is much smaller on Argo2 compared to nuScenes -- based on my empirical experience, previous methods like MapTRv2 and its followups converge faster on Argo2, and training for 6 epochs is close to convergence. This probably suggests that the large performance gaps on nuScenes come from unfair training settings. 

4. Your interpretation of StreamMapNet and SQD-MapNet's Argo2 training epochs is wrong. These two methods employ a different frame sampling strategy at training time compared to MapTRv2, but their effective number of training samples is the same as MapTRv2. Therefore, the claim about the ""longer training schedules"" in the main table's caption is misleading.

5. The venues in the main table are not accurate. HIMap[49] and MGMap[24] are accepted by CVPR2024, and the information was already available at the time of NeurIPS submission. Furthermore, a recent HD map construction method, MapTracker[A], also studies temporal modeling and should be very relevant, but it is missing in the discussion and related works.   

    [A] MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping, arXiv:2403.15951

Limitations:
The limitations and broader impacts are adequately discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors propose a new approach for constructing vectorized high-definition maps that exploits temporal information across adjacent input frames.  The model, which they call MapUnveiler, operates at the clip-level and consists of an intra-clip unveiler which generates vectorized maps for T frames and an inter-clip unveiler which uses a memory module to aggregate information between clips. The authors present results on two standard benchmarks, vectorized HD map construction benchmarks (nuScenes and Argoverse2) and demonstrate the model’s superior quantitive performance to several previously proposed approaches. They also show several qualitative examples of how MapUnveiler can better handle occlusions in the input images.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The paper is well-written and contextualized well within prior work.
- The methodology is novel and well-motivated.
- The results are strong on the two tested datasets, both quantitatively and qualitatively.
- Many different analyses and ablations were included to justify the design decisions used within MapUnveiler and show its strengths.

Weaknesses:
1. The methods is dense and a bit hard to read. The architecture figures help but are also a bit difficult to parse through. It would be helpful to try to weave more intuition into the text.
2. Claiming ""-9.8%"" is significant but ""-6.0%"" is comparable in the robustness to occlusion section seems a bit arbitrary (and potentially overstating MapUnveiler's performance, as a 6% drop is still considerable). I suggest the authors rephrase this sentence (and address similar claims in the paper).

There are several typos throughout the paper. I have enumerated some here, but encourage the authors to do a detailed proofread:
- 127: With there
- 129: mapnet -> MapNet
- 161: bev -> BEV
- 167 parenthesis 
- 192 backwards parenthesis 
- 294: In addition, if we choose too short

Limitations:
Only one limitation is included. I encourage the authors to think through other potential limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents a method called MapUnveiler, which aims to improve the construction of vectorized HD maps for autonomous driving. MapUnveiler uses a novel clip-level pipeline to unveil occluded map elements by relating dense image representations with efficient clip tokens and propagating inter-clip information. This approach leverages temporal information across adjacent input frames, addressing the limitations of single-frame and streaming inference methods. The model achieves state-of-the-art performance on the nuScenes and Argoverse2 benchmark datasets, demonstrating promising improvements in challenging scenarios with longer perception ranges and heavy occlusions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The introduction of a clip-level pipeline for vectorized HD map construction effectively addresses occlusion issues and leverages temporal information across multiple frames.
2. The method utilizes clip tokens to propagate map information efficiently, reducing redundant computations and enhancing prediction consistency.
3. Extensive experiments demonstrate that MapUnveiler achieves state-of-the-art performance on nuScenes and Argoverse2 benchmarks, particularly in challenging scenarios.

Weaknesses:
1. The community has noticed a severe data leakage issue with utilizing nuScenes and Argoverse2 datasets for online mapping evaluation {1, 2}, as these datasets are not intentionally built for online mapping. It might also be necessary to validate the proposed method on geo-disjoint training and validation sets.
2. It would be good to see the analysis of added model compacity due to the introduction of the proposed intra-clip unveiler and inter-clip unveiler.
3. It seems the proposed intra-clip unveiler and inter-clip unveiler are adaptable to any single-frame inference online mapping methods. It would be good to validate the effectiveness of the proposed modules on other baseline methods.
4. The authors are encouraged to investigate the consistency of estimated HD maps across frames of the proposed method compared to existing methods with ""inconsistent and suboptimal prediction results"" (mentioned in Line 7).
{1} Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps.
{2} Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It.

Limitations:
The limitation of dependency on temporally consecutive frames is discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a clip-based vectorized HD map construction paradigm for the processing of long temporal sequence, in which occluded map elements are unveiled explicitly by efficient clip tokens. Through clip token propagation, MapUnveiler achieves effective utilization of long-term temporal map information by associating inter-clip information, in which clip tokens are propagated rather than dense BEV features. Experiments demonstrate that MapUnveiler boosts the performance on public benchmark datasets, also for more challenging setting like long-range perception and heavily occluded driving scenes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy-to-follow. Figures clearly conveys the intended message.
2. “Unveiling the hidden” and clip token propagation are reasonable and effective strategy for static Map element detection, which is practical and alleviates the problem to some extent.
3. The proposed method demonstrates strong performance on benchmark dataset, comprehensive experiments and ablation studies justify the model design.

Weaknesses:
1. As mentioned at line 227, this work is built on pretrained frame-level MapTRV2 and fine-tuned, thus the comparison can be unfair. Results without pretraining are required to verify your effectiveness.
2. At line 53 and BEV Updater in line 151, for occluded features, how to select the tokens that are visible in certain frames? Seems tokens within the temporal window are fully utilized for BEV update by cross attention, how to determine whether these tokens contain unblocked information? More explanations are required.

Limitations:
Yes. The authors mentioned the weakness of their approach on the corrupted input.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z4duW3KzlD;"REVIEW 
Summary:
The paper presents a deep state-space model architecture with non-linear transitions and emissions. The model disentangles the latent representation for the dynamics and the one for the observed data at each time step - allowing therefore effective state estimation at future time steps and the ability to deal with missing data imputation.
Inference is performed with a deep Extended Kalman Filter, that relies on a RNN architecture to make a more efficient approximate computation of the Kalman Gain (KG) and smoothing gain (SG).
The method is tested on a number of simulated and realistic approaches, and it outperforms competing architectures.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Non-linear/deep state-space models are being used more and more in many applications. Parameter learning and state estimation is however challenging in this setting, and this paper provides an interesting method for this
2. The method is more scalable than comptering KF-based methods thanks to the dynamics network approximation, but still effective despite the aproximation
3. The method builds on some models in the literature, but provides some useful novel components
4. The authors did extensive and well-though experiments/ablations, comparing with many SOTA models
5. There is an extensive appendix covering many details that did not fit in the main text. I particularly appreciated ""A.11.1 Python intuitive code.""

Weaknesses:
The paper is not straightforward to read (had to read it carefully twice), mostly because of the way the required derivations are presented.
The notation used is somewhat not conventional within the ML-heavy NeurIPS community, and should be improved/clarified:
1. In Section 4 you use o^+ notation which is not common in the ML community. Can you clarify what it means and why you need it? This explanation needs to be done in section 4, not referring to a different section.
2. Similarly, what is s in line 219 and why do you need to introduce this notation? The sentence in line 219-221 is key but unclear
3. Why do you need to define the ""gt"" in line 229?
4. Not sure the SI perspective helps in the ML-heavy NeurIPS community, it brings confusion. Maybe can be added in the appendix?

In terms of novelty, the final model seems to me more similar to the Kalman VAE (KVAE) model than what the authors claim. Your model can almost be seen as a modification/extension to the KVAE in which you add the RNN approximation to avoid the O(n^3) complexity, model the transition noise covariance and use a slightly different parameterization for the dynamics network.
Can you clarify the differences between your model and the KVAE? 
In line 96 and Table 6 in the appendix, I don't think your KVAE description is correct: it has a setup very similar to your model which allows to estimate the state dynamics, and allows for direct optimization unlike what you claim.
I am not as familiar with the other KF-based methods mentioned, but make sure your description is correct.

Minor comments:
1. Line 53: typo ""To to""
2. Line 71: typo ""We"" -> ""we""
3. You introduce gamma in line 144, but only say what it is in line 150, making the reader wonder if it was defined above and look for it
4. Line 283: you say ""with n=3m"" without specifying what n and m are. Even if they are defined before, being a notation-heavy paper, better to remind the reader what n and m are.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a very well theoretically motivated State-Space Model learning approach, which is implemented by a gated inference network. The network implements a Hammerstein-Wiener model within a modularized deep learning architecture. It uses GRU cells to mimic Kalman Filtering operations. Forward as well as forward-inverse processing routines optimize the hidden state space estimations. Several theoretical components add to the paper contribution. Evaluations show superior performance on several challenging toy problems with noisy data (pendulum, double pendulum, ball bouncing in irregular polygon, as well as odometry prediction from kiti data) evaluating state estimation and imputation tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Paper is very well-structured. The work is also very well-motivated and well-embedded into the literature. 

The theoretical motivation and system derivations are impressive and usefully embed the author’s GIN system into the Kalman Filtering background. Approximating everything in a variational inference manner via estimations of Gaussians and their Covariances is efficient.

Theorems 3 and 4 offer a theoretical derivation for ensuring stability of the unfolding recurrence. 

The evaluations contain sufficiently challenging problems. Performance is compared with many alternatives, showing superior performance nearly throughout. Only in Table 3 GIN was partially beaten by DeepVO.

Weaknesses:
The theorems 3 and 4 are not really experimentally evaluated. Is instability observed when the recurrent matrix is not modified as proposed? The theorem’s proposal should be verified experimentally. 

Even more elaborate evaluations would of course be great. Seeing the great content and the importance of the theoretical derivations, though, I consider this a very minor point, which can be tackled in subsequent work.

Limitations:
Further evaluations and ablations to truly identify the core components that yield the great results of the GIN system.

Confirm theory components experimentally.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper advances temporal reasoning in dynamic, high-dimensional, noisy environments by introducing a novel architecture for latent variable state space models. The architecture permits efficient Bayesian inference with nonlinear transitions and emissions. Experiments are performed on toy datasets and a simple real-world dataset for state estimation and missing data imputation, showing that it beats benchmarks relative to competing models like RNNs, autoregressive models, and latent variable approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Clear exposition of model architecture and inference algorithm. Theoretical analysis in Section 6.

Weaknesses:
I think one thing that could really strengthen this paper is showing an experiment on a more challenging data set / problem. The first two experiments are on toy problems.

I think another thing is to explain more clearly how this architecture is differentiated from others, i.e. the technical novelty. E.g. what is the relation of your model to other SSMs incorporating RNNs like the Variational RNN (which you benchmark against in the experiments), and what is it about that change that improves inference?

Limitations:
Yes, limitations adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ywEQkCmImh;"REVIEW 
Summary:
This work proposes a new task named Multi-Domain Learning Video Anomaly Detection, which aims to learn a general VAD model across domains. The work finds that abnormal conﬂict is a critical challenge in the task. Then, the work establishes a new benchmark, designs an effective baseline and conducts extensive experiments to investigate this challenge. The results shown on the benchmark demonstrate that the abnormal conﬂict is alleviated.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The work proposes a new task, which is interesting. 
2. The work establishes a new benchmark to evaluate the new task. 
3. The motivation of the proposed baseline, i.e., abnormal conﬂict, is clear and makes sense.

Weaknesses:
I have some concerns about the proposed method, and I think more comparison experiments are needed to demonstrate the effectiveness. Despite this, I think the abnormal conflict issue is interesting, thus I am willing to raise my rating if my major concerns are addressed. My concerns are as follows:

1. Why the proposed Abnormal Conﬂict (AC) classifer can address the abnormal conﬂict problem? Why the label is determined by the discrepancy in Eq. (6)? It seems that there are some mistakes in the formula (inconsistent with that in Fig. 2). 
2. I would like to see the results of more baselines, in addition to MIL, Null-MIL and NullAng-MIL. 
3. More detailed discussions about related works are needed, e.g., virual video anomaly detection datasets [1] and related techniques utilizing virtual datasets [2]. 

[1] Ubnormal: New benchmark for supervised open-set video anomaly detection, CVPR 2022

[2] Generating Anomalies for Video Anomaly Detection with Prompt-based Feature Mapping, CVPR 2023

Limitations:
The paper has discussed the limitations and potential impacts of the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, authors proposed a new task called Multiple Domain VAD (MDVAD), along with a benchmark and new evaluation protocols. Authors' goal is to construct a general VAD model by conducting multi-domain learning while recognizing abnormal conflicts and exploring representations of general normality and abnormality. Authors introduced a baseline for MDVAD and proposed a new framework with multi-head to mitigate abnormal conflicts and proposed Null-Multiple Instance Learning (Null-MIL) and NullAngular-MIL (NullAng-MIL) losses for multi-domain training. Additionally authors suggested an Abnormal Conflict (AC) Classifier to explore general features while being aware of abnormal conflicts. Authors analyzed the primary issues of MDVAD and proposed a baseline for this new task.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. According to the analysis, authors believed that the abnormal conflict and the scene discrepancy are the two main issues and designed a framework with multi-head to deal with these problems. 

2. Null-MIL and NullAng-MIL methods are designed for multi-domain learning, and an AC classifier is proposed for learning general features while abnormal conflicts exists.

3. Authors provided sufficient experiment results for this task and create a new baseline.

Weaknesses:
1. The proposed framework with multi-head for multi-domain seems not flexible enough during the domain changes, such as adding a new dataset with extra abnormal conflicts. And for the abnormal conflicts, will the proposed method performs better compared to make anomaly categories classifications for all anomaly events type of all domains?
2. In my opinion, traditional WS-VAD methods are designed to detect abnormal events in single domain without abnormal conflicts, and when abnormal conflicts exists, it will be better to use other paradigms such as temporal action localization or video grounding. And for the current WS-VAD datasets, the annotations are video-level, or even without category information, which is too weak for higher level anomaly detection. Training model with the current MDVAD paradigm is likely to not achieve good results.
3. Maybe using visual-language model with multimodal alignment can deal with the above issues? These models contain more knowledges for more event categories and higher generalization ability, which are likely to have the ability to individually detect conflicting anomalies. Compared to multi-head regression, is VL alignment a better approach for MDVAD task?

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The manuscript addresses the limitations of existing Video Anomaly Detection (VAD) models that are confined to single-domain learning. The primary contribution of the paper is the introduction of a new task called Multi-Domain Learning for VAD (MDVAD), which aims to develop a general model capable of identifying abnormal events across multiple domains. The manuscript conducts experiments using the MDVAD benchmark and demonstrates the limitations of traditional multi-domain learning. It shows the effectiveness of the proposed baselines in handling abnormal conflicts and achieving robust performance across multiple domains.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The manuscript proposes a new task, Multiple Domain Video Anomaly Detection (MDVAD), which solves the problem that the existing model is limited to a single domain and provides a new idea for the development of domain-generalized models.
2.The MDVAD method proposes domain-specific multiple head mechanism and Null-Multiple Instance Learning Method (Null-MIL), which effectively solves the problem of anomaly conflict between different domains.
3. The MDVAD method constructs a new benchmark containing six representative VAD datasets, which fills the gap of the lack of unified evaluation standard in multi-domain learning tasks.
4. The MDVAD method designs four evaluation protocols (held-in, leave-one-out, low-shot domain adaptation, and full fine-tuning) to systematically evaluate the generalization ability of the model.

Weaknesses:
1. MDVAD introduces the domain-specific multi-head mechanism and the Null-MIL method, which increases the complexity and computational cost of the model, and may place higher demands on the computational resources in practical applications.
2. The multi-domain learning task itself is difficult to train, and with the proposed method further increasing the complexity of training, MDVAD may require longer training time and higher technical requirements.
3. Although the theoretical background and analysis are provided, the theoretical basis and derivation process of some of the methods of MDVAD are slightly weak and need to be further explored and verified in depth. Part of the theoretical analysis is based on specific assumptions, and these assumptions may not be fully valid in practical applications, affecting the applicability of the theoretical analysis.
4. Although new benchmarks and assessment protocols are proposed, MDVAD lacks comparative experiments with other state-of-the-art methods, making it difficult to objectively assess the relative advantages of the proposed methods.

Limitations:
See Weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new task called MDVAD, the goal of which is to effectively learn from multiple domains with different data distributions and definitions of abnormality without confusion, resulting in a general VAD model. To achieve this, the authors expand the traditional single-head framework to multiple-head framework for learning different knowledge and design an AC classifier to handle abnormal conflicts. The experimental results prove the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper focuses on the problem of learning a generalizable VAD model, which is an important task.
2. The experiments conducted by the author are relatively compr

Weaknesses:
1. This paper proposes a new task called MDVAD to achieve generalizable VAD by resolving conflicts in anomaly definitions. However, for any VAD application, the definition of normal or abnormal events should be explicitly determined according to the scenario requirements, rather than simply combining multiple datasets and resolving the abnormal conflicts. I find it difficult to understand under what practical scenario a VAD model trained using multiple datasets with abnormal conflicts is needed.
2. The writing of this paper is not clear enough, where some necessary training and inference details are missed. For example, the normal head training mentioned in NullAng-MIL is confusing.
3. This paper lacks a detailed description of the experimental setup. For example, if an anomalous event is determined to be a conflict, how should the model handle such an event?

Limitations:
I do not recognize obvious potential negative societal impact of this work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
GLUIuli3Sm;"REVIEW 
Summary:
This submission studies the convergence guarantees of and bounds on the expected number of samples used when using loss based active learning. They additionally propose a new sampling scheme that combines loss based sampling and a Polyak step size and provide convergence guarantees. Their analysis covers multiple models and loss functions. They proposed methods further evaluated with numerical experiments on multiple datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The problem addressed is interesting and has not been addressed in the literature. In order for loss  based sampling strategies to be effectively deployed in the wild this sort of analysis is necessary. The algorithmic contributions are also of interest, and borrow a well known approach to setting step sizes (Polyak step sizes) from the optimization community to define their Adaptive-Weight Sampling scheme. This combination is a novel idea and a starting point to explore other methods of setting step sizes in this active learning context.

Weaknesses:
While  the technical contributions of this paper are interesting, the primary weakness is the communication of results. This reviewer has an optimization background rather than an active learning one, but even accounting for this the organization and exposition of results was challenging follow. For example, rather than defining algorithms in a LaTeX algorithm block as is standard in literature, authors simply  refer to modifying the (projected) SGD update. While the general idea of active learning is simple, it is also unclear which variant of active learning the authors are studying. Based on the step size being defined as a Bernoulli variable and the experiments conducting one pass over the dataset, it appears that the authors are studying a “streaming” approach to active learning, where the decision to evaluate the label or not is made upon encountering each datapoint. It appears that the selection operation is to ignore when the Bernoulli sample is zero and evaluate the loss otherwise. This is not made clear in the writing, and an optimization audience may be confused as to why some steps will have step size zero. This understanding could be incorrect, which is likely due to the lack of clarity and motivation of the definitions and algorithms. The authors are encouraged to be more explicit about what problem they are trying to solve, and what the exact definitions of their algorithm is.

Limitations:
The authors have addressed some limitations in their work and potential future directions.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This is a technical paper, whose subject of interest is the convergence of stochastic gradient-based learning algorithms which include a stochastic step size mechanism, whose value is allowed to be influenced by losses or other ""uncertainty"" related quantities that are computed at training time.

Their main theoretical results can be roughly broken into two categories based on the assumptions placed on the step-size mechanism. The first category is where the step size is a re-scaled Bernoulli random variable, taking values in $\\{0, \\gamma\\}$ in their notation, with $\\gamma$ fixed throughout but the probability of a non-zero step size (i.e., $z\_{t} = \\gamma$) can change depending on the data/parameter at each step in the training routine. They start with an argument centered around a monotonic loss function and linear binary classifiers, but also consider an ""equivalent loss"" type of strategy (like in Liu an Li '23), again where a convenient monotonicity assumption (here on $\\pi$) preserves convexity and aids in analysis. Their main bounds are in-expectation upper bounds on the loss incurred by the average of iterates generated using this Bernoulli step size.

The second category is similar, but allows the actual step size to be impacted by loss/gradient values in an ""adaptive"" way, while retaining a certain probability of step size 0. This combination of Bernoulli step sizes with an adaptive step size is what they call ""Adaptive-Weight Sampling (AWS)"", and they provide conditions to obtain upper bounds on the (empirical) objective function of interest (i.e., the average loss). 

Their theoretical results are complemented by empirical analysis, in which they compare uniform random sampling (of points for SGD), ""traditional"" loss-based sampling, and their AWS approach (their Fig 1). This setup assumes loss access, i.e., this is not active learning. On the other hand, for active learning scenarios, a loss estimator needs to be plugged in; they consider the impact of the quality of such an estimator in their second batch of tests (their Fig 2).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Overall, the paper is quite well-written and has a natural logical structure which is easy to follow. The authors have obtained a flexible set of conditions for evaluating SGD procedures with a stochastic loss-dependent step size mechanism, which appear to build quite directly upon the existing literature, which they are good about citing (e.g., Raj and Bach '22, Liu and Li '23).

The paper is a mixture of different efforts, some new convergence theory, a new proposed algorithm (AWS), plus formal/empirical analysis of this algorithm, and I think there is potential for this work to have an audience at a conference like NeurIPS.

Weaknesses:
I am not familiar with the related technical literature, so I will not comment on the novelty or theoretical prowess required to obtain the results here.

I would personally highlight two main points I feel need improvement. The first point is that the narrative of this paper feels really bloated. To the best of my reading, all the talk of ""active learning"" in the title and throughout the paper is totally irrelevant to the entire paper, save for the last paragraph of section 4 plus Figure 2. Yes, there are obvious links between the procedure of interest here and active learning settings, but the core problem setting stands on its own just fine. There is no reason to structure the paper around active learning, it just makes things confusing and downplays the substantive results. I feel like I can say the exact same thing about ""uncertainty-based"" methods. The only uncertainty-related formulation I can find is Corollary 3.8. Having this is great, but why put uncertainty-based and loss-based methods on the same footing when writing the paper?

The second point is related to technical exposition. For the most part the work seems to be well done, but for a first-time reader, certain parts feel rushed and sloppy. I'll make a list of points I tripped up on in the following section.

Limitations:
Not applicable.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper considers the active learning algorithms based on uncertainty and loss functions. The learner queries the label of an unlabeled sample with probability proportional to (some function of) the uncertainty/loss and updates the parameter according to some step size scheme. The authors generalize previous results under the strictly separable binary classification setting and general classification setting with convex loss and smooth equivalent loss. The authors later propose a Polyak-type step size scheme called Adaptive-Weight Sampling and prove its convergence. Numerical experiments verify the efficiency of AWS under both oracle and estimation of loss functions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The analysis is solid;
2. The presentation is clear;
3. The Adaptive-Weight Sampling (AWS) algorithm provides a novel perspective for active learning literature.

Weaknesses:
1. The generalization of previous results seems not to be very essential;
2. The assumption of access to the exact loss function before querying seems too strong for theoretical analysis.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z4FaPUslma;"REVIEW 
Summary:
The paper uses Riemannian optimization to guide the final layer weights (the linear classifier) toward the nearest simplex ETF orientation. In particular, consider the two common approaches of training a deep classifier network:

1. The standard training strategy where the final layer weights are updated by backpropagation.

2. The final layer weights are fixed as a simplex ETF (which has been well-studied in previous works).

The proposed approach leverages the duality between penultimate layer features and the final layer weights (to form a simplex ETF orientation) and gradually guides the latter to an optimal simplex ETF per training step.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The proposed approach frames the gradual transition of weights to a simplex ETF as a Riemannian optimization problem, which can be differentiated. Thus, allowing for an end-end training pipeline. The combination of these techniques is novel to the neural collapse setting.

2. The experimental results are presented for the simple UFMs as well as practical networks and datasets to showcase the convergence benefits.

Weaknesses:
The authors do not provide numerical data for the extra memory and step-time that is required by the extra deep declarative layer. A brief discussion is presented in Section 5 but I believe further details would strengthen the paper. For instance:
- By what percentage does the step time and memory increase when adding this layer?
- When should one avoid the backward pass through this layer and consider only the forward pass?
- What is the dependence on the memory and step time growth with the feature dimension and the number of classes? Maybe a simple UFM-based analysis should suffice.

see more questions below.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a novel algorithm for neural network training. The algorithm is motivated by the recent discovery on the neural collapse phenomenon, which demonstrates that the last layer of neural network classifier will converge to a specific structure named simplex ETF. The authors propose to guide the network parameters to the ETF structure via explicitly penalizing on the distance to the ETF, and further address the non-uniqueness of the solution via adding a proximal term. Experimental results on various neural network architecture and real world datasets are presented, and the proposed algorithm can universally improve the training and testing accuracy over the standard training.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The proposed algorithm is novel and well motivated, and it shows universal and significant improvement over multiple choices of network architecture and datasets. The contribution of this work is solid, it helps the community to understand the benefit of the neural collapse phenomenon, and can potentially improve the standard paradigm of neural network training.

Weaknesses:
1. The presentation should be improved, see questions for detail. In general the authors should give more detailed information about how the algorithm is implemented.

2. Although the accuracy on train and test dataset exhibits significant improvement within the fixed number of training epochs, the proposed algorithm are much more complicated to compute. Therefore it makes more sense to compare the running time and computational cost with Standard and Fixed ETF.

3. Proper ablation study is missing. The authors add many additional techniques, such as exponential moving average, stratified batch sampling, deep declarative layer to improve the training. It is not clear how much the improvement indeed comes from the nearest ETF optimization.

Limitations:
The authors have discussed the limitation properly in the paper. A more detailed discussion with empirical results on the computational cost will be helpful.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
One of the key aspects of neural collapse (NC) is that the penultimate class feature means form a simplex Equiangular Tight Frame (ETF). The main idea of this paper is to leverage this insight and improve training by further encouraging this property during training. The authors suggest doing this by solving a Riemannian optimization at a given iteration. The way it works is that the classifier weights are set to the nearest simplex ETF obtained by solving this inner Riemannian optimization problem. The classifier weights are dynamically updated during training using this Riemannian opitmization problem at each iteration (rather than trained using gradient descent) using a ""deep declarative node"" this allows gradients to propagate through the Riemannian optimization. 

They show that this approach indeed speeds up training convergence and improves training stability. Their experiments include both synthetic and real-world classification tasks and architectures. 

Overall the authors present a nice idea and it is a well-written paper. However, there are a few issues related to the experiments that I outline below. 

From my viewpoint, the value of this paper and their method (to me) is less the improved test accuracy and more the improved stability and speed of convergence. It's important to note that this speed up also comes at an additional cost (i.e. in performing the Riemannian optimization). Therefore, the improvements to stability or speed of convergences should be weighted against this caveat. I think it would help to highlight this tradeoff more upfront and make that more clear/transparent.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This is a thoughtful and well-written paper. The authors suggest a nice idea to leverage this insight of NC in deep learning and their approach has clear benefits. It is a nice idea and very well executed. 

There are clear improvements to the current methods; e.g., their improvement upon [74] by solving the inner Riemannian optimization instead of requiring the model backbone to do the work of matching to a chosen fixed ETF.

The theory and the idea is very compelling. The implementation is good and well explained. Beyond the theory and the novelty of the idea, the main strength of the paper is the value added wrt convergence speed in terms of the number of epochs required for the network to converge.

Good work.

Weaknesses:
The main points of concern for me are in regards to the experiments and how the results are reported in the paper.

Table 2 looks good but is a bit misleading particularly when comparing the ranges of the test top-1 accuracy.
The results are still interesting but it's not such a strong/clear winner; that is, when looking at the ranges, it's not so obvious. The authors point this out and clarify that the advantages are speed to convergence and decreased variability which I agree are definite plusses.

The test top-1 accuracies reported in Table 2 aren't competitive with what can be obtained on these benchmark datasets, particularly for the Resnet models. For example, looking at 200 epochs or training, STL on ResNet50 should be able to achieve 85-90% test accuracy, even for Resnet18 the test top-1 accuracy for STL should be upwards of 75%. Similarly, for CIFAR100 on Resnet50, the test accuracies aren't competitive. It'd be interesting to see if these claims about variability still hold when giving the baselines adequate chance to be competitive.

For Figure 4, also no error bars. Understanding compute restraints, it would be nice to see similar multiple seed runs for ImageNet experiments. 

Finally, one thing that is not reported here is an estimate of compute cost. Their method requires additional compute for each iteration. Perhaps when compared on this axis their implicit ETF and the Standard training method would be more fairly compared. 
The authors do mention this in the limitations section.

Limitations:
N/A The authors address any limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel approach to utilizing ETF geometry. Instead of fixing the weights or making them learnable, their approach dynamically adjusts the weights by solving a Riemannian optimization problem while allowing end-to-end training. They show that their approach outperforms both the fixed and learnable approaches in terms of convergence speed and generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality:
The idea of dynamically adjusting weights is not new, but in the context of neural collapse (NC), it is a natural extension. Fully learnable weights do not provide the ETF structure, and fixed weights are too restrictive. The proposed approach is a good compromise between the two and combines the best of both worlds.

Quality:
The paper is well-written, and the proposed approach is carefully supported by theorems and experiments.

Clarity:
The paper is well-written and easy to follow.

Significance:
Their approach is general and could be applied to a range of problems. The authors applied it to synthetic UFMs and some standard image benchmarks (CIFAR-10, CIFAR-100, STL-10, ImageNet). The authors plan to release code upon acceptance.

Weaknesses:
Overhead Cost:
The proposed method computes the exponential moving average of the feature means, performs a Riemannian optimization, and computes the gradient of DDN. These components introduce overhead in terms of epoch time. The authors claimed in the paper that the gradient of DDN is not computed, and the Riemannian optimization overhead is negligible. This unsupported claim should be backed up by an additional experiment that reports these extra computation times.

Standard Procedure:
""To ensure fair method comparison,"" the authors include classifier weight normalization and feature normalization for the standard procedure. This is usually not the case when using CE loss (see Fig 2). The authors should justify this choice by providing the results without these normalizations for the standard procedure.

Image Baselines Results are not SOTA:
The reported results are not state-of-the-art. For example, ResNet-18 trained on CIFAR-10 only reaches 80.47%. It seems that these baselines are not well-tuned, and the gain of the proposed approach is not clear and could potentially fade away with a better-tuned baseline. Can the authors comment on this? Additionally, the authors should include the results using ResNet-50 on ImageNet, which should provide a stronger reference point.

Fixed ETF Procedure:
The authors only used the canonical simplex ETF for the fixed procedure. The weight matrix results in many zeros and could lead to poor performance when used as the fixed classifier because some neurons will be inactive. The authors should include the results using the fixed ETF with a non-canonical (i.e., projection on a random basis).

Remarks:
The authors should directly clarify in Tables 1 and 2 the ResNet architecture used (18 or 50).

Limitations:
For large-scale problems, the computational cost of the proposed approach could be a limitation due to the high memory cost of computing the backward pass of the Riemannian optimization. Therefore, the authors did not compute the gradient calculation when reporting their results for the image benchmarks.
The authors claimed that they empirically observed no significant difference in performance for small-scale problems where the DDN gradient can be computed.
Including these results in the supplementary material would also be beneficial.
Moreover, we agree that future work should verify if this is true for large-scale problems.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z2739hYuR3;"REVIEW 
Summary:
The problem considered in this paper is online learning in MDPs where transition probabilities are modelled with a log-linear model (with ""multinomial logit function approximation""). The finite horizon, time-inhomogenous setting is considered. The problem is motivated by allowing a nonlinear transformation in modeling the MDP and yet maintaining both computational and information theoretic tractability. Inspired by results in the analogous bandit problems and algorithms developed for them, a number of gradually more complex, but (statistically) better performing algorithms are considered. In particular, while naive approaches give a poor dependence on a problem parameter $\kappa$ that characterizes the ""strength"" of nonlinearity, by adopting previous ideas to the MDP setting, new algorithms are designed that eliminate this poor dependence. A lower bound is also established, which nearly matches the upper bound (but considers infinite action spaces, while the main paper considers finite action spaces).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This is a reasonable problem setting; and the approach is also reasonable. It is nice to have a lower bound, even if there is a mismatch between the settings. It is nice to see that ideas that were developed for the bandit setting generalize to the MDP setting.

Weaknesses:
1. The novelty is limited by that we have seen the same story, same ideas playing out nicely in the closely related bandit setting. 
2. A new parameter, U, the number of next states that are reachable with positive probability in the worst case, appears in the analysis and will appear in the bounds.
3. It is an unpleasant surprise for the reader to discover this dependence only through carefully reading the paper, rather than being told upfront. It is not good that the opportunity to discuss whether this quantity needs to enter the regret bound, and that this quantity needs to be small for the algorithm to be tractable, is missed.
4. Line 83 and onward: The work of Uohamma is discussed but is mischaracterized. My reading of this work is that they do establish that their algorithm runs in polynomial time. It remains unclear why the exponential family model is incomparable with the one considered here; an explanation (with examples) is missing.
5. The paper could use some extra proofreading (e.g., the upper indices in the bottom of page 5, in the displayed equation are not correct); in line 149, in the definition of $U$, $|\cdot|$ is missing.

Limitations:
n.a.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers MDPs employing the MNL function for transition probability, following Hwang and Oh [2023]. The authors suggest efficient algorithms based on online Newton steps, inspired by [Hazan et al., 2014; Zhang et al., 2016; Oh and Iyengar, 2021]. Furthermore, to improve $\kappa$ dependency, they provide algorithms employing local learning with mirror descent inspired by [Zhang and Sugiyama, 2023; Lee and Oh, 2024]. The algorithms achieve $1/\sqrt{\kappa}$ or even detach the dependency of $\kappa$ from the leading term.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The suggested algorithms are computationally efficient and show improvement in $\kappa$ compared to the previous work of Hwang and Oh [2023].

Weaknesses:
- Their suggested algorithms do not seem novel because they are based on previously proposed methods for logistic or MNL bandits. Specifically, the online Newton update is widely studied for MNL or logistic bandits [Oh and Iyengar, 2021; Zhang and Sugiyama, 2023].

- Furthermore, the improvement on $\kappa$ is based on the mirror descent algorithm proposed in [Zhang and Sugiyama, 2023; Lee and Oh, 2024], and the proofs seem to follow the steps in [Zhang and Sugiyama, 2023; Lee and Oh, 2024] in the appendix.

- Lastly, the MNL model for transition probability may have an inherent weakness: the number of achievable states for each (k,n) must be finite, and it is required to know the state space of $S_{k,n}$.

[1] Faury, Louis, et al. ""Jointly efficient and optimal algorithms for logistic bandits."" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.

Limitations:
The authors discuss some interesting future work regarding regret bound. Additionally, I believe, as mentioned in Weaknesses, the MNL model for transition probability has an inherent weakness: the number of achievable states for each (k,n) must be finite, and it is required to know the state space of $S_{k,n}$.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the author analyzes a Markov Decision Process (MDP) model with non-linear function approximation. Specifically, in the finite-time horizon inhomogeneous episodic MDPs setting, the transition dynamics are unknown but the reward function is known. The author proposes using a multinomial logit (MNL) function approximation to estimate transition dynamics, which is superior to the linear function approximation if the model is misspecified in \cite{hwang2023model}. Additionally, the author proposes *UCRL-MBL-OL*, which adapts the previous work that is model-based and has large computational and storage complexity, to an online style that only consumes constant computation and storage resources. Moreover, the author has proven that the regret bound of *UCRL-MBL-OL* matches the state-of-the-art in Theorem 1. Its regret bound achieves $\tilde{O}(\kappa^{-1} dH^2\sqrt{K})$, where $H$ is the time horizon length, $K$ is the number of total episodes and $\kappa$ is considered as a parameter to control the sparsity of the transition dynamics and $d$ is the hidden dimensionality. Ignoring the logarithmic factor and $\kappa$, such a regret bound has only a $\sqrt{H}$ gap compared to the lower bound. After that, with additional assumption, the author utilizes the local information to propose another two algorithms, *UCRL-MNL-LL* and *UCRL-MNL-LL+* to remove the dependence on $\kappa$ and get a tighter regret bound as well as maintain good properties of *UCRL-MNL-OL*.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written. The author makes a clear improvement point compared to the literature.

2. The algorithm proposed by the author enjoys an online learning style that does not need to maintain a large historical set.

Weaknesses:
1. Although this paper focuses on reducing the computation complexity, I am curious about the sample complexity of *UCRL-MNL-OL*.
    
2. Since the algorithm builds up the estimation of the transition dynamics by using MNL function approximation, is it considered a model-based algorithm? More specifically, does it require storing the transition dynamics for each state-action pair in every step?

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the MNL function approximation inhomogeneous RL, achieves the $O(1)$ computation cost, and improves the regret guarantee with regard to $\kappa$. To improve the computation cost, this work employs the online Newton step instead of MLE estimation to estimate $\theta$. Then, they design a novel confidence set by making full use of local information to improve the dependence of $\kappa$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The use of local information instead of a uniform $\kappa$ is novel and useful to improve the dependence of $\kappa$.
2.	The UCRL-MNL-LL+ removes the $\kappa$ dependence on the lower-order term and almost matches the optimal regret results by using high-order Taylor expansion.

Weaknesses:
1.	[1] also use the online Newton step to improve the computation cost in the logit contextual bandits setting. It would be better to discuss the novelty of UCRL-MNL-OL.

[1] Oh, M. H., & Iyengar, G. (2021, May). Multinomial logit contextual bandits: Provable optimality and practicality. In Proceedings of the AAAI conference on artificial intelligence (Vol. 35, No. 10, pp. 9205-9213).

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the recently proposed MDPs that use multinomial logit function approximation for state distribution validness. The results and algorithms improve the prior work of Hwang and Oh [2023] in multiple aspects, including computation efficiency, storage, and statistical dependence on the problem-dependent quantity $\kappa$ that can be exponentially small. In addition, the authors establish a matching lower bound on $d$, the feature space dimension, and $K$, the number of episodes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and has clear logic flows. Readers can see how the authors approach the MDP problem and tackle the challenges. In particular, Table 1 is quite useful for demonstrating the advancements in the work.
- The improvements in both computation and storage efficiencies are essential for practical applications. In Theorem 2, the authors also improve the dependence on $kappa$ to $\sqrt{\kappa}$ without affecting efficiency. The enhancement seems significant, especially since the parameter can be exponentially small. 
- The lower bound established in the paper is the first to demonstrate the optimality of the authors' algorithms in the $d$-$K$ dependence. Per my understanding, it also confirms the results' optimality of Hwang and Oh [2023].

Weaknesses:
- The primary high-level techniques and tools (seem to) come from existing works and relevant fields, such as MNL contextual bandits. The authors should put more effort into highlighting the technical challenges and novelties besides the previous comparisons. 
- It would be beneficial to include experiments on synthetic and real-world datasets and compare the results to existing baselines and relevant works. In particular, the new algorithms seem more involved than prior ones, which may affect their stability and adaptiveness.
- There is still a significant gap between the lower and upper bounds. Besides, I wonder how often $\kappa$ could be exponentially small in practical settings, though it's definitely of theoretical interest to approach the lower limits on parameter dependency.

Limitations:
The authors have made various comparisons and discussed the limitations of the results, which I'm satisfied with. I do not see any potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
z1GwaNoGnr;"REVIEW 
Summary:
This paper introduces XMask3D, a framework developed for open vocabulary 3D semantic segmentation. They propose the integration of the denoising UNet, derived from a pre-trained diffusion model, to generate geometry-aware segmentation masks conditioned on learnable implicit 3D embeddings. These binary 2D masks are used to filter mask-level embeddings of 3D representations and apply mask regularization, thereby improving the open vocabulary capacity of 3D features.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	The motivation is clear.
2.	The proposed method is intuitive, and the experiments have validated their contributions.

Weaknesses:
1.	The organization should be improved. Section 3.1 provides an overview, while section 3.2 includes design insights and preliminary findings. The flow of these writings has puzzled me, making it difficult to grasp your key contribution.

Limitations:
1.	The authors have addressed the limitations and potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a precise and consistent mask-level alignment between 3D features and the 2D-text embedding space through a method called cross-modal mask reasoning. The proposed XMask3D model includes a 3D branch for capturing geometric features, a 2D branch for generating vision-language aligned masks, and a fusion block to combine 3D with 2D. Using a pre-trained text-to-image diffusion model as the 2D mask generator, the model leverages three techniques: 3D-to-2D mask generation, 2D-to-3D mask regularization, and 3D-2D mask feature fusion.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1- The idea is novel, the author propose to merge 2D which provides high OV capabilities, with 3D features shich endoces 3D geometry. 

2- The method performs remarkably better than the reported models, namely OpenScene. The experiments are also well structure

Weaknesses:
1- The authors don't compare with state-of-the-art 3D semantic segmentation OV3D[1]

2- The authors highlighed fututre work in the limitation, it would be good if you can expand it with some limitation on the technical side or some failure cases.

[1] Jiang, Li, Shaoshuai Shi, and Bernt Schiele. ""Open-Vocabulary 3D Semantic Segmentation with Foundation Models."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

Limitations:
Needs to be expanded

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the limitations of current open vocabulary 3D semantic segmentation methods, which primarily focus on creating a unified feature space for 3D, 2D, and textual modalities but struggle with fine-grained segmentation boundaries. To overcome these limitations, the authors propose XMask3D, a cross-modal mask reasoning framework that achieves more precise mask-level alignment between 3D features and the 2D-text embedding space.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The part ""incorporating a 2D mask generator to create geometry-aware open masks and apply fine-grained mask-level regularization on 3D features"" seems reasonable and novel.
2. The paper is well-structured and easy to follow.
3. Analysis is thorough and insightful.

Weaknesses:
1. The paper evaluates the proposed method on a limited set of benchmarks (ScanNet20, ScanNet200, S3DIS), all of which are indoor scene datasets. Authors could discuss how the method might perform on outdoor datasets. Additionally, the authors could provide a qualitative analysis of the model's potential limitations when applied to different environments.
2. The reliance on the denoising UNet from a pre-trained diffusion model could be seen as a potential weakness or limitation, especially given the computational resources required for training and inference.

Limitations:
The authors have addressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of open-vocabulary 3D semantic segmentation by utilizing 3D geometric features, 2D semantic embeddings, and text modality. The proposed approach adapts the ODISE method to the 3D domain, aiming to distill open-vocabulary semantic segmentation knowledge from a pre-trained text-to-image denoising diffusion model to a 3D segmentation model. Initially, an input point cloud is fed into a 3D encoder-decoder segmentation network, producing point-wise geometric features. Simultaneously, a pre-trained visual-language diffusion model generates 2D masks and embeddings from posed images of the same scene, conditioned on the 3D global feature of the 3D branch’s encoder. Unlike the ODISE method, an implicit $3D$ captioner is introduced to produce geometry-aware 2D masks while also distilling information from the 2D branch network to the 3D encoder. To further regularize the 3D network, a distillation loss ($\mathcal{L}_{mask}$) is applied to the 3D mask embeddings, derived from the per-point features and the 2D masks back-projected to the point cloud as 3D binary masks. By obtaining ground truth mask features from a pre-trained CLIP model, the 3D masked embeddings are aligned with the image-text joint embedding space, through a cosine similarity loss. This alignment leads to more coherent segmentation results and enhances the model's open-vocabulary capabilities. Finally, the per-point features are combined with the pseudo mask 2D features (formed by the back-projected 3D mask and 2D mask embeddings), resulting in a fused per-point representation that incorporates the geometric information from the 3D segmentation network and the semantic open-vocabulary capabilities of the 2D branch. The approach is evaluated on three semantic segmentation benchmarks (ScanNet, ScanNet200, and S3DIS) and demonstrates superior performance compared to competing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The XMask3D effectively aligns 3D geometric features with 2D and textual modailities through knowledge distillation from visual-text joint embedding spaces inherent in the pre-trained 2D denoising UNet and the CLIP model. As evident by the ablation, the implicit 3D captioner is a crucial step in the overall pipeline, and it outperforms vanilla text conditioning or the implicit 2D captioner of ODISE, in both base and novel semantic categories. Moreover, the 2D-to-3D mask regularization is also essential, since it significantly improves the accuracy of the proposed method esp. in novel categories. This justifies the need for this additional distillation step from the CLIP joint space, to further enhance the open-vocabulary capabilities of the XMask3D method. Finally, the discussion on modality fusion, both in the main paper and supplementary, is highly appreciated. By dissecting the method and providing qualitative and quantitative results for each step, the authors make it easier for readers to understand and gain intuition about the presented approach.

Weaknesses:
While the method exhibits superior performance w.r.t. competing methods, it seems that the output fused embeddings yields to geometric inconsistent features for semantic classes that cover large areas of the point cloud such as wall, ceiling and floor. This is evident in both partitioning settings when the class is either base or novel (Table 5 (a) and (b) in supp.).

Limitations:
Yes, the authors have discussed the method's limitations in detail in Section 4.4.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z0I2SbjN0R;"REVIEW 
Summary:
This paper introduces diffusion methods to tackle the partially observed PDEs, named DiffusionPDE. By learning the joint distribution of solution and coefficient space, the proposed model can handle both forward and inverse problems. The authors experiment with diverse PDEs and settings to demonstrate the model's effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
-	This paper successfully utilizes the diffusion methods in solving PDEs, covering both forward and inverse problems.

-	The main text and supplementary materials provide diverse experiment settings, which can well support the model’s effectiveness on partial observations.

-	This paper is overall clear and well-written.

Weaknesses:
1.	The technical contribution is limited.

From a technical view, this paper is an application of the diffusion model in PDE solving. There are also some previous methods that also use diffusion methods and leverage the PDE loss [1]. Thus, I think the technical novelty is limited.

[1] A Physics-informed Diffusion Model for High-fidelity Flow Field Reconstruction, JCP 2023

2. Some powerful baselines are missing.

- According to Figure 1, I think the base model of DiffusionPDE is U-Net. How about comparing it with a single U-Net? I think U-Net could be a powerful baseline.

- There are also some latest models that are good at processing partially observed or irregularly placed PDEs, such as OFormer [1] and Transolver [2]. They should include them as baselines.

[1] Transformer for Partial Differential Equations' Operator Learning, TMLR 2023

[2] Transolver: A Fast Transformer Solver for PDEs on General Geometries, ICML 2024

3. Model efficiency comparisons are needed, including GPU memory and running time.

4. I think the proposed model cannot predict the future evolution of a time-dependent PDE. Current tasks are all about “reconstruction” or “imputation”.

Limitations:
I appreciate that they have discussed the limitations. But I think the mentioned issues about efficiency and limitations on temporal modeling are not trivial. More discussions are expected.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes to solve PDEs given only sparse measurements by jointly modeling the solution and coefficient space (e.g. the initial conditions) using a diffusion model. By applying diffusion posterior sampling (DPS) the authors obtain samples that are consistent with the sparse measurements and the underlying PDE equations. Several experiments show superior performance of the method compared to standard baselines such as PINNs and FNOs.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Solving PDEs under partial observation is an important problem in real-world applications
- The proposed method is technically sound and improves upon existing baseline methods (PINN, FNO) that do not work well for sparse measurements
- Leveraging a pretrained diffusion model as a generative prior to model the joint distribution of solution and coefficient space is a good idea
- The presentation of the method is clear and supported by concise algorithms and equations. The paper is well written overall
- Experiments consider standard baseline methods for PDEs and cover a sufficient range of different dynamics

----

Post-rebuttal: the authors have addressed quite a few of the initial concerns, and while some concerns (e.g. about the magnitude of the contributions remain), I'd be happy to support an accept. I've raised my score accordingly.

Weaknesses:
- The main weakness of the method is the limited novelty. Both sparse measurements and physics-based losses have been considered together with diffusion models, see e.g. Shu et al. (2023). So it seems to me that the main technical novelty is to apply diffusion models to model the joint distribution of two simulation states at different points in time and apply DPS during inference for consistency with the sparse measurements and PDE constraints.     
- The experiments do not take into account any stochasticity or uncertainty. In principle, DPS will give a distribution of solutions, which is not the case for the other baseline methods, but this is not explored further in the paper. 
- Since the joint distribution models two states at time 0 and time T (for all experiments except Burgers' equation) and $0 \ll T$, the authors need to simplify the PDE loss $\mathcal{L}_{pde}$ to drop any time derivatives. This is a serious limitation.   
- It is not clear if DPS works better than classifier-free guidance, as used e.g. in Shu et al. (2023), or other methods for solving inverse problems with diffusion models. 
- DPS requires a lot of compute during inference for calculating $\mathcal{L}_{pde}$. For a fair comparison, it would be important to show the number of parameters, training time and inference time for all methods.

Limitations:
The authors have mentioned slow sampling speed as a limitation in the conclusion, but I think an extended discussion of this would be important to include.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work uses a guided diffusion process to solve the PDE forward and inverse problems with partial observations. Instead of learning the parameter-to-solution map ($a\rightarrow u$) as in Neural Operators, the method learns the diffusion process on the joint distribution $(a,u)$, and use guided diffusion for inference under sparse observations. Compared with several baseline method, the proposed method shows improved performance for solving forward and inverse problem with sparse observations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The work uses a guided diffusion process to solve the PDE forward and inverse problems with partial observations.The authors compare with several baseline methods. The idea is clearly presented, and might be useful for the community.

Weaknesses:
The paper presents an interesting approach to solving PDE forward or inverse problems with sparse observations, which is an appealing concept given the minimal data requirement. However, this approach raises some concerns about the well-posedness of the problem. For example, in forward problems where sparse observations of the parameter $a(x_i)$ are available, there are infinitely many ways to interpolate $a$ and solve the PDE to obtain $u$. They are all valid solutions that satisfy both the PDE and the observations. This suggests that the method's ability to achieve good recovery might heavily rely on the strong regularization imposed by the training dataset, potentially limiting its practical utility as it may only favor solutions resembling those in the training set.

Additionally, in Appendix C, Table 2, the weightings for observation and PDE loss are significantly higher (by two to six orders of magnitude) than those for $\nabla_x \log(p(x))$ as described in Equation 8, which might indicate a predominance of data fitting over the diffusion process. It would be beneficial if the authors could provide more guidance on how these weights were chosen and discuss the implications of using smaller weights. Understanding the rationale behind these choices could help clarify the model's dependency on these parameters and their impact on the solution's behavior.

Limitations:
The author mention several limitation in the conclusion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper uses score based generative diffusion models to find the forward and backwards solution of a set of PDEs given partial observations of the solution and/or incomplete knowledge of the coefficients. The method performs well, and outperforms other ML  methods such as FNO, as well as 'standard' FE type methods, for a range of standard test problems. The method reconstructs This is a novel approach, which delivers good performance, with low errors  at a competitive speed. Extensive tests are given, with careful analysis of the results.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The use of score based generative methods in this context, where both the solution and the parameter estimates are updated, is novel. The method is clearly effective for the problems considered and should have good applications to real world examples. Extensive tests on a series of standard test problems show that the errors of the method are much lower than other ML based methods such as FNO.

Weaknesses:
This paper suffers as do many similar papers from a limited range of examples. It concentrates on the usual examples of PDEs such as NS and Bergers, and in both cases of these it looks at problems with quite moderate viscosity, which are realtively east to solve. This is more or less inevitable for such a short paper as this, especially as comparisons are needed with other method. But I would have liked to have seen more novel examples than the usual ones. This is not really a criticism of this paper, but is something to consider for future work. It would be imporoved by a fairer comparison with other methods which work with incomplete data and measurements. A clear exanple of this being the data assimilation widely used in physical modelling for just this range of problems. These should be descibed somewhere in the introduction and in Section 2. (Although of course these latter methods are slow in comparison.) The method is also limited (see later) to looking at certain slices of the solution.

Limitations:
The model as described only looks at slices of solutions of 2D problems. This has been clearly identified by the authors. In this sense it is vrather limited when compared to other ML based approaches, and of course traditional FE based methods. I am pleased that this is recognised and that the authors plan to address this. The DiffusionPDE method will only be truly competitive when this is done, but this paper is a good step in this direction.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
yzviAnpvU6;"REVIEW 
Summary:
From my understanding, this paper give a zero-th order algorithm with application to popular vision tasks neural architecture search and black-box adversarial attacks. The authors derive a closed-form solution after modeling the gradient estimation as a quadratically constrained linear program problem. The key idea is to try to decouple the required sample size from the variable dimension without extra conditions required, making it able to leverage the queries in the prior iterations. The speedup is further technically achieved by directly indexing some of the intermediate variables that contribute to the gradient estimation. The theoretical studies are given for its convergence speed and its cost-effectiveness is verified on benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Clear motivation with clearly derived approach, and it is a new zero-th order algorithm indeed and the authors also contextualize well the proposed method with related work discussion.
2. Strong empirical performance on representative vision tasks with rich testbeds and settings.
3.  The approach by its design, could enjoy the efficiency of smoothing techniques while maintaining estimation accuracy. Table 4 in the appendix is informative.
4.  The paper gives comprehensive results and technical details in both main paper and appendix.

Weaknesses:
1. As remarked by the authors, it has few constraints on the sample size, similar to the smoothing techniques; and it requires the estimation of the gradients which involves solving a linear program problem.
2. As a zero-th order algorithm, it may still not be suited for large-scale application e.g. network training.

Limitations:
No.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ReLIZO, a novel zeroth-order optimization method leveraging linear interpolation to estimate gradients efficiently. It reduces the complexity of gradient estimation by reusing prior queries without additional conditions on sample size, decoupling it from variable dimension constraints. ReLIZO models gradient estimation as a quadratically constrained linear program, solving it analytically to reduce computation complexity. Experimental results demonstrate ReLIZO's efficacy in various scenarios, including black-box adversarial attacks and neural architecture search, showcasing faster convergence and better solutions compared to existing methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The paper is well-written, with clear and easy-to-follow explanations.
* The paper introduces a method for estimating gradients using arbitrarily sampled vectors without requiring orthogonal conditions or adherence to a specific distribution, enabling the reuse of queries to accelerate the zeroth-order (ZO) optimization process.
* Extensive experiments on simulation benchmarks and real-world applications validate the method’s performance.
* The paper highlights that ReLIZO can be viewed as a generalized version of traditional linear interpolation methods, capable of handling both equal and smaller sample sizes compared to variable dimensions. This demonstrates ReLIZO's theoretical soundness and enhanced flexibility in gradient estimation.

Weaknesses:
* The effectiveness of reusing queries depends on the choice of the reusable distance bound, which might require fine-tuning for different applications, adding complexity to its implementation.
* While the method reduces the number of function queries, the process of solving the quadratically constrained linear program might introduce additional computational overhead for large $n$.

Limitations:
The authors have adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study introduces a novel gradient estimation algorithm that operates solely on forward function evaluations. The method employs a Quadratically Constrained Linear Program (QCLP) to determine the optimal linear approximation of sample vectors. The authors present performance enhancement strategies, including sample reuse and efficient inverse matrix computation within the QCLP framework. Empirical evaluations conducted on black-box adversarial attacks and neural architecture search demonstrate the proposed algorithm's superiority over existing zeroth-order methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method is natural. Approximating the gradient using linear combinations of samples and formulating as the QCLP is an intuitive idea, and the auxiliary techniques employed in this study are both judicious and pertinent to the research objectives.

2. The paper is well-written and easy to follow.

Weaknesses:
1. Zeroth-order gradient estimation has a relatively limited impact. While the proposed zeroth-order gradient estimation method demonstrates superiority over existing algorithms in its class, its overall impact on solving underlying optimization problems may be constrained. This limitation is exemplified in the NAS evaluation, where ReLIZO does not consistently achieve optimal performance.

2. According to my interpretation, in ReLIZO algorithm, obtaining new samples from the input space in each iteration is random and arbitrary. I feel there might be more effective strategies to sample new vectors based on known information. Could the authors comment on this?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yySpldUsU2;"REVIEW 
Summary:
It is known that usually deep neural networks will learn “easy examples"" that contain fast-learnable features first while learning more complex examples in a second time. The authors argue that mitigating such simplicity bias is the reason method like SAM are outperforming SGD. Based on such analysis, the authors introduce their methods coined as USEFUL that consists in two setups: 1) Identifying the examples with fast-learnable features using a clustering method based on layer output similarity 2) Upsampling by a constant factor the remaining examples with slow-learning features. By doing so, the authors can significantly increase model performances and training time on different classification tasks using different optimizers. They assess their methods across a wide range of dataset and different hyper-parameters and outperform random clustering baseline.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well motivated and written. The method seems to be sounded and I really appreciate that the authors assess their method using different hyper-parameters such as optimizer, batch size, datasets, upsampling factor, architectures, and data augmentation. It is also great that they ran a baseline with random clustering.

Weaknesses:
It is not clear when and why one should choose the last output activation vector to define the clustering instead of intermediate activation vector. It is also not clear at which epoch one should decide to do the clustering since for a dataset like CIFAR10 the optimal performances are achieved at epoch 8 while for CIFAR100 it is epoch 20. So, finding the correct hyper-parameters for the clustering might be costly and thus impact how fast convergence can really be (if we consider this needed additional ablation on clustering epoch). In addition, the authors mention that they are using an upscaling factor of 2, but I am wondering how robust this is when using long-tail distribution. For example, I am not sure that on something like ImageNet-LT or Inaturalist, we will get the best performances by using a constant factor.  I would also be a bit more cautious about some of the claims made in the papers. For example, the authors claim that their method is generalizing to OOD tasks while providing experiments on only the WaterBird dataset.  So, it would be better to write about promising preliminary results than claiming generalization on OOD.

Limitations:
The authors did not really discuss any limitations (outside the fact that their theoretical result does not extend to CNN) or societal impact. I think that one limitation that could have been highlighted is the smaller scale of the experiment and the focus on classification tasks. Another limitation is the lack of results on OOD or long-tail benchmarks which would seem to be well suited for this type of work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work aims to modify the training data distribution to improve in-distribution generalization. First, the authors theoretically analyse a 2-layer CNN and compare the feature learning dynamics (fast learnable and slow-learnable features) of Gradient Descent (GD) and Sharpness-Aware Minimization (SAM). It is then shown that SAM mitigates simplicity bias compared to GD. The authors then propose USEFUL (UpSample Early For Uniform Learning), a method that upsamples the examples in the training set that contains slow-learnable features.  USEFUL first clusters the examples with similar outputs early in the training and then upsamples the slow-learnable clusters. The main idea behind USEFUL is to learn features at a uniform speed (similar to SAM) by changing the training data distribution. USEFUL can be trained with SGD, SAM and SAM + Trivial Augment. Results on CIFAR-10, CIFAR-100, STL10, TinyImageNet indicate that USEFUL is across datasets and architectures. Additonal ablation and analysis show that USEFUL learns similar properties to SAM (for e.g less sharp solutions).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Originality: The question posed by the authors “Can we change the training data distribution such that the model trained on it has similar properties to SAM?” is interesting and novel. The proposed method is also well-motivated.
2. Results: The authors perform a comprehensive set of ablations and analysis on the proposed method USEFUL. Section 5.4 that shows that USEFUL’s solution has similar properties to SAM, which answers the question raised in the motivation of the paper. I also particularly like the ablations with upweighting loss and data selection method in Appendix D.6.
3. Overall, the paper is fairly well written. One minor point to address here is that the paper covers multiple concepts like SAM, simplicity bias, flat minima and uniform feature learning. It would be good to explain the relationship between these more clearly.

Weaknesses:
1. The authors explicitly mention that their focus in this paper is only on “in-distribution generalization”. I am a bit confused by this given the motivation of simplicity bias and learning features uniformly. To elaborate more on this point,
    - Springer et al [1] also show that SAM implicitly balances the quality of diverse features (similar to the observations made in Section 3 of this paper. The experimental results in [1] is focused more on datasets with multiple predictive features like CelebA, CIFAR-MNIST. 
    - Past work on simplicity bias and shortcut learning [2, 3, 4, 5] has focused on similar datasets like CelebA, Waterbirds, CIFAR-MNIST, Colored-MNIST to name a few.
    - While the authors have shown encouraging results on Waterbirds dataset in Appendix D5, it would be good to show the complete results on various groups and on other datasets as well.
2. Connection to [1]. Springer et al [1] made a very similar observation as to Section 3 in this paper. It would be great if the authors can clarify the differences with the observations in [1] and this work. Particularly, [1] also shows that SAM mitigates simplicity bias and that  SAM learns higher quality representations of hard-to-learn features. The authors briefly discuss this in Related Works section but a more detailed answer would be helpful.
3. I just wanted to understand the practical usefulness of the proposed method. This method has one additional hyperparameter i.e the separating epoch. The authors have reported the best separating epoch for all the datasets which is epoch 8 for CIFAR-10 and epoch 20 for CIFAR-100 (Appendix C.2). How is this hyperparameter chosen? Is there a separating epoch number that works across various datasets? This is especially relevant given that that the average gain on most of the datasets with USEFUL is less than 1% with additional cost for training. 
    
 [1] Springer, Jacob Mitchell, Vaishnavh Nagarajan, and Aditi Raghunathan. ""Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning."" The Twelfth International Conference on Learning Representations.

[2] Shah, Harshay, et al. ""The pitfalls of simplicity bias in neural networks."" Advances in Neural Information Processing Systems 33 (2020): 9573-9585.
    
 [3] Geirhos, Robert, et al. ""Shortcut learning in deep neural networks."" Nature Machine Intelligence 2.11 (2020): 665-673.
    
 [4] Kirichenko, Polina, Pavel Izmailov, and Andrew Gordon Wilson. ""Last layer re-training is sufficient for robustness to spurious correlations."" arXiv preprint arXiv:2204.02937 (2022).
    
 [5] Teney, Damien, et al. ""Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.

Limitations:
Yes, the authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
- Proves for a 2-layer CNN with fixed second layer weighsts trained on a toy dataset,  SAM learns slow-learnable and fast-learnable features more uniformly in the early epochs compared to SGD
- Based on this analysis, proposes a simple clustering-based upsampling strategy for reducing simplicity bias / excessive reliance on fast-learnable features. The results show that this improves in-distribution generalization of standard small-scale image classification tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Simple easy-to-implement method that uses SAM and upsampling to improve in-distribution generalization
- The method is well justified with theoretical analysis comparing SAM and SGD on a toy data distribution. This analysis indicates that SAM is less sensitive to simplicity bias.

Weaknesses:
- No baselines. There are several papers now that try to reduce simplicity bias in order to improve performance:
    - https://arxiv.org/abs/2105.05612
    - https://arxiv.org/abs/2301.13293
    - https://arxiv.org/abs/2107.09044 (does not focus on simplicity bias explicitly, but similar to method proposed in paper)
    - simpler baselines: there are several papers that propose “example difficulty” metrics (https://arxiv.org/abs/2106.09647). How well do this correlate with the clusters found in your method? If you just train on the k examples with the highest difficulty scores (per class), does this fare worse than the proposed method?
- Limited novelty due to findings in [64] (Sharpness-aware minimization enhances feature quality via balanced learning). This paper also shows that SAM improves feature diversity (on real datasets + backed up with analysis on a toy dataset) and improves performance on transfer-learning tasks.
- Lacking discussion about when this method would fail. I can imagine two scenarios where the method would not work:
    1. Most training examples have one or more slow-learnable features. In this case, the clustering approach would “remove” most of the points in the dataset, and train on very points for multiple epochs. This could result in overfitting and performance that is worse than training. There’s an implicit assumption that there is some sort of one-to-one relation between examples and features. In the case where all examples contain an “easy” (e.g. patch) and a “hard” feature (e.g. CIFAR), would this method improve performance over SGD? 
    2. In noisy datasets, low-quality examples or mislabeled examples would require more time to learn, and this method would cluster them and train on them for longer. That is, it would group examples that are “high-quality” and hard-to-learn with “low-quality” points. In this case, would the proposed method improve performance over SGD? 
- “SB of SGD has been long conjectured to be the reason for the superior generalization performance of overparameterized models, by providing capacity control or implicit regularization” This incorrectly cites https://arxiv.org/abs/2006.07710v2, which shows that too much simplicity bias can lead to robustness and in-distribution generalization issues.
- Unfair evaluation. The experiments compare SAM+TA augmentation and SAM+USEFUL+TA to SGD (no TA). I think there should be two plots, complaring {SGD, SAM, SAM+USEFUL} w/ and w/o TA.
- Experiments on larger datasets. The image classification used here are fairly small-scale. I would like to how well this method scales to ImageNet-scale datasets (TinyImageNet is not a good proxy..)
- Writing is repetitive at times, especially the theory section (3.3)

Limitations:
Please see strengths and weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes an algorithm for changing the distribution of training data to improve the generalization of the model on origin data distribution. The paper is inspired by Sharpness Aware Minimization, which aims at finding a flat minimum meaning that it has a good generalization capability. This paper divides features into two categories: fast-learnable features and slow-learnable features and derives some observations like ""SGD and SAM only learn fast-learnable or easy features early in training"" and ""SAM learns slow-learnable and fast-learnable features at a more uniform speed"". The authors propose the method dubbed as USEFUL to train the model on some slow-learnable features repeatedly. The experiments show the effectiveness of USEFUL on CIFAR10 and CIFAR100 datasets.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The paper has a theoretical analysis to analyze the learning progress and derive the proposed method.
- The experiments are abundant and comprehensive.

Weaknesses:
There are some questions based on the presentation of this paper, I will not hesitate to improve my score if the following question are solved.
- Difference between this paper and methods for long-tailed data distribution or measuring the difficulty of learning examples. Algorithms for long-tailed data distribution are usually based on resampling training data or reweighing loss value. The proposed USEFUL is similar to the resampling methods except that USEFUL focuses on the features that are hard/slow to learn. Some references for understanding: [Shi, Jiang-Xin, et al. ""How re-sampling helps for long-tail learning?."" Advances in Neural Information Processing Systems 36 (2023).](https://arxiv.org/pdf/2310.18236), [Shrivastava, Abhinav, Abhinav Gupta, and Ross Girshick. ""Training region-based object detectors with online hard example mining."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.](https://arxiv.org/pdf/1604.03540v1) and some references based on it, [A Re-Balancing Strategy for Class-Imbalanced Classification Based on Instance Difficulty](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_A_Re-Balancing_Strategy_for_Class-Imbalanced_Classification_Based_on_Instance_Difficulty_CVPR_2022_paper.pdf), [Active Teacher for Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Mi_Active_Teacher_for_Semi-Supervised_Object_Detection_CVPR_2022_paper.pdf), I believe a discussion of these references in paper should be helpful.
- The relation between the proposed USEFUL and SAM? It seems like the motivation of USEFUL is changing the data distribution to get a flat minimum like SAM. But the results in Appendix D.2, *i.e.*, 53.8 for SGD 41.8 for SGD+USEFUL 12.4 for SAM in Table 1($\lambda_{max})$, do not show effectiveness compared with SAM. It could show the effectiveness on SGD but it's far from being comparable to SAM. 
Some small questions:
- What's the exact formulation of the Data distribution?
- What's the ""patch"" meaning in Definition 3.1? Is that the same as the patch in ViT or the channel of the image? It's a little confusing.
- The experiments mainly focus on traditional architecture, e.g., n-Layer CNN, ResNet. More experiments on popular models and big datasets, e.g., Transformer ImageNet-1k, would be better.

Limitations:
N.A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yxjWAJzUyV;"REVIEW 
Summary:
This paper reduces the complex policy optimization procedure of alignment to a simple regression objective, using the relation between optimal policy and reward. The paper conduct detailed theoretical analysis in revealing the relation between the proposed algorithm *REBEL* and *NPG/MD*. Comprehensive experiments in both text and image generation exhibit the effectiveness of *REBEL*.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies simplified version of policy optimization in RLHF (compared to PPO), which is a research topic of interest.
2. The theoretical analysis of *REBEL* is detailed and insightful.
3. The presentation of this paper is logically clear and has good readability.
4. The experiments in this paper are comprehensive, and the experimental results are well presented.

Weaknesses:
1. The statement ""REBEL ... be extended to handle intransistive preferences ...."" in the abstract is not adequately presented in the main content of the paper. As the major influence brought by intransistive preferences is the degradation of reward score accuracy, which is not addressed by this paper.
2. I would suggest the authors to summarize the limitations of the proposed method in a separate ""Limitations"" section.

Limitations:
none

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes the REBEL algorithm that reduces policy optimization to iteratively solving squared loss regression problems on the difference in rewards between trajectories, based on DPO's analysis. The paper transforms the resulting equation for r(x, y) presented in DPO to a regression loss function, and avoids the intractable calculation of Z(x) by calculating the loss based on a pair of samples from the same input prompt x, i.e., (x, y) and (x, y'). One of the goals for REBEL is to serve as a simple and lightweight RL algorithm that eliminates the need for complex components like value functions and clipping heuristics used in PPO. The authors provide a theoretical analysis showing that Natural Policy Gradient can be seen as a special case of REBEL under some assumptions. The authors conduct two kinds of empirical analysis including language modeling and image generation tasks to demonstrate the performance of REBEL.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Originality:
    - This paper presents a new angle by transforming the analysis of the reward function presented in the DPO paper into a reward regression loss, leading to the proposed REBEL algorithm. 
    - The authors make connections between REBEL and existing RL methods like NPG considering some assumptions, showing that these algorithms can be seen as special cases or approximations of REBEL under certain conditions. 

- Quality:
    - The paper provides a thorough theoretical analysis comparing REBEL with existing RL approaches. 

- Clarity: 
    - The paper is well-written and easy to understand, with a clear logical flow from motivation to theoretical analysis to empirical validation. The authors do an good job of explaining the intuition behind REBEL and highlighting its connections to prior work.

- Significance:
    - The paper tackles the important problem of developing simpler and more efficient RL algorithms that can scale to large-scale generative model fine-tuning.

Weaknesses:
1. Insufficient experimental validation and limited baseline comparisons:
- While the paper presents empirical results on language modeling and image generation tasks, the experimental validation of REBEL could be more comprehensive. The authors should consider including a wider range of benchmarks and datasets to demonstrate the generality and robustness of their approach.
- The comparison with baseline algorithms like PPO and DPO is somewhat limited. The authors should provide more details on the hyperparameter settings and training procedures for the baselines to ensure a fair comparison. Moreover, the poor performance of DPO compared to PPO in the experiments raises questions about the implementation or hyperparameter choices.
- The authors claim that REBEL matches the strongest known theoretical guarantees in terms of convergence and sample complexity. However, the experiments only compare performance at a specific epoch without demonstrating improved sample efficiency. Convergence plots showing the performance of REBEL and baselines over the course of training would provide a clearer picture of the sample efficiency and convergence properties.

2. Lack of support for certain claims and limited exploration of key aspects:
- The paper makes several claims regarding the advantages of REBEL, such as its ability to handle intransitive preferences, incorporate offline datasets, and apply to deterministic MDPs. However, there is a lack of corresponding experimental evidence or theoretical analysis to substantiate these claims.
- The relationship between the regressor's performance and the quality of the dataset used for training is not explored in depth. Insights or experiments that investigate how dataset quality and diversity affect the regressor's ability to capture an improved policy would strengthen the paper.
- The choice of base distribution \mu is mentioned as a determining factor for whether REBEL is hybrid or fully online. However, the paper does not provide experimental results comparing different forms of \mu across various tasks or practical guidelines for choosing \mu in real-world applications.

3. Inconsistencies and potential conflicts with previous statements:
- The authors mention that critic-based variance reduction might be necessary for high-variance trajectory-level rewards in stochastic MDPs, which seems to contradict the criticism of PPO's complexity in the introductory section. The lack of experimental support for REBEL's performance in stochastic MDPs is a significant limitation, and the authors should provide preliminary results or theoretical insights to support their claims.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents REBEL, a minimalist reinforcement learning algorithm that does policy optimization by solving a sequence of regression problems using relative rewards as targets. Theoretical analysis shows that Natural Policy Gradient (NPG) is a variant of REBEL, and thus theoretical guarantees for NPG can be applied to REBEL.  Experimental results  show that REBEL matches or outperforms existing baselines, most notably PPO and RLOO, on multiple tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-organized and technically sound. The general flow of the paper is smooth and proposed methods are explained adequately. The paper has an appropriate number of citations and properly details existing work in the related work section. 
- The method is simple to implement and has little engineering overhead. Given the minimalist implementation, the results are impressive, surpassing even PPO, which typically requires significant engineering.

Weaknesses:
- There are no significant weaknesses in this work, barring some clarifying details. 
- I believe that at least a brief section on related work should be included in the main paper, the in-depth one can be deferred to the appendix. In terms of space, I personally do not think Section 2.2 adds much value to the main paper.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present REBEL, a method for solving contextual bandit problems (such as the alignment of language models) via regressing relative rewards. They first derive their objective by demonstrating that the use of paired responses means that you can get rid of the partition function, which is impossible to estimate. 

They then connect their method to previous methods in RL including detailing, but not . They demonstrate that under strong assumptions REBEL is equivalent to mirror descent, and that under assumptions of coverage by the reference policy, that REBEL produces returns close to an optimal policy. 

Finally the authors run experiments on summarisation, general chat and image alignment, demonstrating their method compares favourably to other methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The idea of using relative rewards to remove the partition function is a nice and simple idea
* The theoretical connections of their method to prior methods grounds their work nicely in existing RL approaches. 
* The empirical results seem to demonstrate their method is competitive or better than other approaches. 
* REBEL compares favourably in terms of runtime and memory usage with other, similarly performing methods. 

Overall the theoretical and empirical examinations of their method seems very thorough.

Weaknesses:
See questions

Limitations:
The authors discuss the limitations throughout their work at relevant stages.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yxOrSmS5wR;"REVIEW 
Summary:
The paper proposes AV-Cloud, a framework for high-quality spatial audio rendering in 3D scenes without relying on visual cues. AV-Cloud addresses issues in current audio-visual rendering methods, such as audio lag and dependence on visual rendering quality, by introducing Audio-Visual Anchors and the Audio-Visual Cloud Splatting module. These components facilitate the generation of viewpoint-specific spatial audio synchronized with visual content. The method demonstrates superior performance on multiple benchmarks, outperforming existing baselines in audio reconstruction accuracy, perceptual quality, and acoustic effects.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The concept of using Audio-Visual Anchors and Cloud Splatting to decouple audio rendering from visual rendering is interesting.
2. The paper demonstrates comprehensive experimentation and robust evaluation across multiple benchmarks.
3. The paper is well-structured and the presentation of the framework is clear. The figures and supplement examples help the readers better understand.
4. The proposed method addresses critical issues in real-time audio-visual rendering.

Weaknesses:
1. The mathematical formulation of the Audio-Visual Cloud Splatting module could be more detailed. For instance, Equation (2) introduces the softmax function applied to the relative vectors and visual features, but the reason behind this specific formulation and its implications are not sufficiently explained. Clarifying how the weights $a_{ki}$ are computed and how they influence the final output would enhance understanding.
2. The technical derivation of the Spatial Audio Render Head (SARH) lacks depth. Specifically, the process described in Equations (4) and (5), where the mixture mask $m_m$ and the difference mask $m_d$ are used to compute the left and right channel outputs, is not fully elaborated. The significance of these masks and their impact on the final audio quality are not clearly discussed. Additionally, the role and impact of the convolution modules within the residual structure (Figure 3) are not sufficiently explained.
3. While the method shows strong performance on benchmarks and some real-world examples, the provided examples are too idealized and lack challenging elements like interfering sound (e.g., crowd noise). I think the robustness of AV-Cloud in more complex and noisy real-world environments should also be validated.

Limitations:
The authors mention the limitations of their approach's challenges and potential drawbacks. The reliance on camera calibration and the potential issues with noise in real-world audio recordings are noted. Additional imitations can be found in the Weaknesses section

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
A novel approach for rendering high-quality spatial audio in 3D scenes, called AV-Cloud, is proposed. This method synchronizes with the visual stream without relying on or being explicitly conditioned by visual rendering, enabling immersive virtual tourism through real-time dynamic navigation of both audio and visual content. Unlike current audio-visual rendering methods that depend on visual cues and may suffer from visual artifacts causing audio inconsistencies, AV-Cloud overcomes these issues. It uses a set of sparse AV anchor points, forming an Audio-Visual Cloud derived from camera calibration, to represent the audio-visual scene. The Audio-Visual Cloud allows for the generation of spatial audio for any listener location. A novel module, Audio-Visual Cloud Splatting, decodes these AV anchor points into a spatial audio transfer function for the listener’s viewpoint, which is then applied by the Spatial Audio Render Head module to transform monaural input into viewpoint-specific spatial audio. This approach eliminates the need for pre-rendered images and efficiently aligns spatial audio with any visual viewpoint. The results are satisfying.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The AV anchors strategy seems to be interesting and effective for audio-visual scene representation. The Audio-Visual Cloud Splatting is novel for AV tasks but more likely to be a Q-former.
2. The experiment results are good and ablations are clear.

Weaknesses:
As I mentioned in the strengths, the Audio-Visual Cloud Splatting seems to be a Q-former like module.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores the problem of generating 3D audiovisual scenes – that is, generating 3D scenes with spatial audio. The proposed approach, AV Cloud, uses anchor points obtained from Structure-from-Motion (SfM) points. The anchors are then used with an AV Cloud splatting module which decodes the visuals and the audio. Experiments are done on RWAVS and Replay-NVAS with comparisons done with several prior works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
– 3d audiovisual scene generation is a really interesting problem to solve. WHile there is considerable literature on visual scene generation, generating 3d visual scene is an interesting problem with real-world applications. 

– The model claims to be able to generate the audio and the visuals in parallel. Essentially unlike prior work it decouples the generation of two modalities by not using the generated visuals for generating the audio. 

– On objective metrics, the paper claims to make good improvements

---- 
increased score after rebuttal

Weaknesses:
– The paper is a bit difficult to follow – especially the key part of AudioVisual anchor points. 

– First, a short primer on SfM is desirable, even if it is in Appendix. More importantly though, it is not clear why it makes sense to use SfM points and clustering on top of them to model AV anchor points and generation of spatial points. Why does it make sense to use SfM points or anchors derived from them as the starting point for AV generation ? What relation the anchors have with audio which motivates the fact that these anchors can be used for audio generation ? 

– Second, the details of AV anchor points are fuzzy. The visuals are used for SfM points which are then clustered to get the anchors. Where is the audio into picture here ? Are these anchors visual only ? If so, why are we calling it AV Anchors ? 

– In prior works, for example AV-Nerf, there is an an explicit AV-Mapper which learns the audio visual relations through which the spatial audio generatio happens. Here Visual2Audio splatting transformer is expected to model that ? 

– For the subjective tests, it would be good to actually get proper subjective ratings on the generated spatial audio. The current preference numbers are not very informative. Getting the spatial audio rated with respect to their quality and spatial characteristics would be much more meaningful. 

– Since NAF, INRAS and other works are considered here - I think it would be good to reference NACF ([R1]) below. NACF specifically focuses on using visuals and is ideal for comparison. 

[R1] Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Cqr6E81iB7;"REVIEW 
Summary:
Various results are proved about online learning of private learning algorithms, contrasting no DP, pure DP and approximate DP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper present some interesting new results on online learning with DP. I read up to section 3 and the writing is very clear and the results are important and purportedly novel. (I don't have background or recent experience in the area of online learning so I cannot independently confirm their novelty.)

Weaknesses:
I can't point to any weaknesses, but this paper is outside of my area, and I was only able to follow up to Section 3, so it is certainly possible there is something I missed. I am basically taking the paper at its word on the claims made in Sections 1 and 2.

Limitations:
No potential negative societal impact.

Rating:
8: accept, good paper

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies online learning of concept classes under DP (differential privacy) constraint. The paper makes progress towards understanding mistake bounds (mostly) in the realizable case in a few settings. Concretely, The paper shows that:
1. If the adversary is oblivious, then PAC pure DP learnability implies online pure DP learnability.
2. On the other hand, if the adversary is adaptive, then PAC pure DP learnability does not imply online pure DP learnability.
3. In contrast with the standard online learning model, for every non-trivial hypothesis class, the mistake bound depends on $T$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Assuming that the results are correct, the paper contains a significant improvement towards understanding mistake bounds in online learning under DP constraint.
2. The questions and results are interesting and in the scope of NeurIPS.
3. The proof techniques are explained in detail.

Weaknesses:
1.It is a bit hard to digest the results and understand the remaining gaps, because there are many settings considered in the paper (DP/non-DP, oblivious/adaptive, approximate/pure...), and no figure/meta-theorem that neatly explains all the relationships. Such a figure/meta-theorem would significantly improve the presentation of the paper.

2. As a consequence of the above, it is not exactly clear how tight the results are. If you prove a lower bound (as in Section 4), I think it is better to formally state, right after it, the best known lower bound (and the dual statement for proving a lower bound).

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper demonstrates that any function class that is offline PAC learnable with pure DP is also online learnable with pure DP against an oblivious adversary. In this context, a hypothesis class is considered online learnable in the realizable setting if there exists an algorithm with a sublinear mistake bound.

The paper also establishes a distinction between online learning with pure privacy for oblivious and adaptive adversaries. Specifically, it shows that the hypothesis class $point_N$ is privately online learnable against an oblivious adversary but not against adaptive adversaries. This finding also indicates a separation between pure and approximate private online learnability, as $point_N$ is online learnable with approximate DP against an adaptive adversary.

Additionally, the paper presents a general lower bound on DP online learning against an oblivious adversary for non-complementary function classes.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper offers general results on private online learnability, identifying conditions under which a hypothesis class is online learnable with pure DP against an oblivious adversary. This connection to Representation Dimension links to existing results on DP PAC learnability.
- It also explores different layers of separation using the function class $point_N$, contributing to a deeper understanding of the cost of privacy in online learning.

Weaknesses:
The proof of Theorem 4.3 is not clear

Limitations:
No significant limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies limits of pure DP and approximate DP in the context of online learning (with oblivious and adaptive adversaries).

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
The research questions are interesting and the results may have fundamental value.
I'm not an expert in all topics covered by the paper, but the contributions seem novel to me.
The text is in general well-written and understandable.

Weaknesses:
The paper has a lot of theorems and lemmas, which is interesting.  Still, the paper has no conclusions, discussion, further work, description of limitations or illustrative experiments or extensive examples.  This puts the task of understanding the value and applicability of the work to a large extent to the reader.

Limitations:
The authors don't discuss limitations or societal impact.
I believe there are no ethical concerns with this theoretical work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yvUHnBkCzd;"REVIEW 
Summary:
This paper introduces a personalized federated learning algorithm to address the challenges of real-time predictions in non-stationary environments. Clients fine-tune models online, combining their locally fine-tuned models with multiple federated models learned over time. This approach ensures efficient adaptation to evolving data streams, with theoretical analysis and experiments on real datasets demonstrating its effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed algorithm effectively addresses the challenge of making real-time predictions in non-stationary environments by allowing clients to fine-tune models online, ensuring continuous adaptation to evolving data streams.
- By combining locally fine-tuned models with multiple federated models, the approach enhances personalization and leverages the strengths of both local and federated learning, resulting in improved performance.
- The paper provides a solid theoretical analysis alongside experimental validation on real datasets, demonstrating the practical effectiveness and robustness of the proposed algorithm in real-world scenarios.

Weaknesses:
1. Contributions are suggested to list by items for clear summaries.
2. The baselines in Table 1 are all before the 2022 year, more latest related methods published in 2023 should be compared.
3. Fed-POE has limited improvements on Air and FMNIST datasets.
4. The process of combining locally fine-tuned models with multiple federated models may introduce significant computational overhead for clients, especially those with limited resources.
5. As the number of clients increases, managing and integrating multiple personalized models can become complex, posing scalability challenges for the proposed algorithm.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel personalized federated learning algorithm, Fed-POE, which is designed for adaptive prediction and model fine-tuning in dynamic environments. It addresses the challenge of real-time predictions on streaming data by constructing a personalized model that combines a locally fine-tuned model with multiple federated models. Theoretical analysis and experiments on real datasets demonstrate its effectiveness in achieving sublinear regret bounds and improved online prediction accuracy.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper proposes a unique ensemble method that dynamically combines local and federated models, which is a novel approach in the field of federated learning.

2. It provides a solid theoretical analysis, demonstrating sublinear regret bounds for convex models.

3. The paper is well organized.

Weaknesses:
1. Although the presented method is novel, it is simply a combination of the previous personalized federated learning approaches as well as ensemble learning and provides comparably little conceptual originality.  The contribution's main novelty seems to be that integrating results from prior models would be beneficial in mitigating catastrophic forgetting in online federated learning.

2. Experimental results show that the improvement in the accuracy of Fed-POE compared to other methods is not significant, but ensemble learning inevitably increases the computational overhead increase. The paper needs to analyze whether this trade-off is reasonable.

3. The paper needs more experiments to prove the effectiveness of the method, for example, for real-time predictions, the size of the old data replay is crucial, and the authors should design experiments to analyze the effect of batch size b on the experimental results. This paper also needs experiment results on the accuracy over the time step.

Limitations:
see the weaknesses and questions

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an interesting perspective about the role of ensembles of models in federated learning. The provocative claim is the fact that federated learning is not always better than locally-trained models. This is contextualized in the field of not IID data and time-varying data generating processes. To address this issue the paper introduces from the theoretical point of view how to quantify the regret in federated and locally trained models. In addition it includes an analysis of non-convex models by managing an history of models. The overall impression about the paper is positive even if some points could have been better explored (in particular the part related to not IID data that is somehow the core of the paper).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces a theoretical evaluation of the gain produced by federated models w.r.t. locally trained models. This results show that federated learning is relevant only when models can be considered iid (hence averaging providing better results). This is somehow a know results but I appreciated the theoretical analysis
- The proposed solution is to combine with a convex mean a locally-trained models with the federated models
- This is further extended in case of non-convex models by considering an ""history"" of models to be used when needed (i.e., according to the loss)

Weaknesses:
- The federated models somehow includes the locally-trained model. I would have appreciated a further analysis about the fact that the two ""sides"" of the average model are related each other. 
- The setting in which eta and eta_c scales with T prevents adaptation in the long run (which is somehow the core of the paper). How to deal with that?
- Federated learning typically takes also into account the complexity of the learning phase (i.e., the amount of info to be transmitted, e.g., the models). This is not quantified here. And this could be also a weak point in the fed-poe algorithm.

Limitations:
See Weaknesses box

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Fed-POE, a novel personalized federated learning algorithm tailored for online prediction and model fine-tuning. Fed-POE creates an ensemble by integrating local models with those periodically contributed by the server over time. Theoretical analysis confirms that Fed-POE attains sublinear regret. Empirical results demonstrate that Fed-POE consistently surpasses the performance of both local and federated models across all evaluated datasets, which indicates that Fed-POE effectively leverages the advantages of both local and federated models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The technical content of the paper appears to be accurate, although I did not check all the details carefully.
- This paper is generally well-written and structured clearly.
- The experiments substantiate the main theoretical analysis, and the proposed algorithm demonstrates superior performance over the baseline methods

Weaknesses:
My primary concern is that the assertion the proposed algorithm can effectively harness the combined advantages of federated and local models is not clearly demonstrated within the theoretical bounds. The paper presents two principal theoretical results: Theorem 2 provides the regret upper bound for the proposed algorithm in convex scenarios, while Theorem 3 addresses non-convex cases. Both theorems establish sublinear regret bounds that are consistent with those for federated learning using a straightforward online gradient descent approach.  I recommend enhancing the clarity of the proposed method's advantages in the theorems by incorporating assumptions about the data distributions.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yppcLFeZgy;"REVIEW 
Summary:
The paper presents MutaPLM, a framework designed to interpret and navigate protein mutations using protein language models. This approach utilizes a protein delta network to capture mutation representations and employs a transfer learning pipeline with a chain-of-thought strategy to leverage knowledge from biomedical texts.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper attempts to propose a general interpretable model for protein mutations.
2. This paper compiles a mutation-text multimodal dataset, providing an excellent benchmark for future work.
3. The code is available. Although I haven't had time to run it yet, I will try to run the code during the rebuttal phase to ensure the reproducibility of the experiments.

Weaknesses:
1. PLM representations used in this study is the residue-level or protein-level embedding? If the mutation has very few residues, such as a missense mutation, will using protein-level embedding result in h∆ being too small?
2. Is it possible to provide some more practical mutation-related downstream task benchmark results? For example, predicting changes in protein properties or PPI?
3. Is it possible to compare the proposed method with the predictive results of embeddings extracted by AF, since the description information of the mutation may already be included in the structural changes predicted by AF before and after the mutation?
4. I do not deny that this is a good work, but perhaps it is more suitable for the benchmark and dataset track, because its method has limited innovation, and it has not verified its interpretability and performance on actual tasks related to protein properties.

Limitations:
This paper discusses the limitations and points out the direction for future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In the paper entitled ""MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering,"" the authors proposed multimodal protein-textual language models for understanding the effect of mutation and performing protein engineering. They also build MutaDescribe, the first large-scale protein mutation dataset with rich textual annotations.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is generally well-written and easy to follow.
2. The authors have constructed the first comprehensive protein mutation dataset enriched with textual annotations. This dataset represents a significant foundation for future research in this field.
3. The MutaPLM framework introduced in this paper is innovative, particularly in its explicit modeling of mutations and its use of cross-modal transformers for multi-modal feature integration, enhancing its analytical capability.
4. By integrating large language models, the proposed framework significantly simplifies protein engineering, offering an intuitive tool that could be readily adopted by biologists for advanced research.

Weaknesses:
1. The paper lacks a comparison with fine-tuned protein language models. Finetuned PLMs (ESM-1, ESM-2) have been validated to be powerful for various downstream tasks. For example, MLAEP(https://www.nature.com/articles/s41467-023-39199-6) and AugmentedESM(https://www.nature.com/articles/s41587-021-01146-5)
2. The paper did not prove why the textural annotation is necessary. From the ablation study, one can conclude that the labeled information from the textual annotation makes the model powerful. 
3. The paper should add more discussion and experiments on why human-understandable notation is necessary. Human-understandable notations are not more informative compared with a conventional multi-label dataset. Moreover, LLMs may fail to deal with regression tasks, while finetuned PLMs can do better.

Limitations:
The authors addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework to 1). generate text-based mutation effects for mutated proteins and 2). propose new mutated sequences based on the function descriptions. The main module is an encoder-decoder network, which encodes the representations of mutated sequences and outputs the position and amino acid of the mutation. The network is first pretrained on the protein literatures and then fine-tuned on the mutation effects.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The problem studied in this paper is novel and well-motivated: generate mutated sequences conditioning on the instructions, and generate mutation effects conditioning on the sequences.
* The method is technically sound. 
* The paper is well-structured

Weaknesses:
Most issues are on the evaluation side. Rigorous evaluations are very important for the AI4Science applications. 
* Baseline Selection: The paper employs weak baselines for comparison. None of the baselines used have been specifically trained on mutations.  This makes it difficult to accurately assess the true effectiveness of the method.
* Lack of Temporal Evaluation: While the paper adopts a structural split for evaluation, which is acceptable, a temporal-based evaluation would be more ideal and realistic. A temporal split, where some proteins are held out based on their discovery time, would more accurately reflect real-world scenarios in scientific applications. 
* Weak Evaluation of Mutation Explanations: The use of GPT-4 to assess scientific explanations is not robust or scientifically sound.
* Missing experimental details. The paper omits several crucial experimental details, which harms reproducibility and thorough understanding of the methodology. Specific areas lacking detail include:
  1. explain in details how you tune the hyperparameters
  2. what is the dataset for protein literatures?
  3. When construct MutaDescribe, did you only use swissprot or the whole dataset? how did you extract the mutation explanations? How do you know whether it's expert-reviewed?

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ypggxVWIv2;"REVIEW 
Summary:
This paper tries to evaluate the strategic reasoning abilities of LLM. Therefore, 10 games are chosen where LLMs is trying to solve the game. This paper includes various open- and closed-source LLMs into consideration and build a benchmark for easy evaluation.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
Evaluating the strategic reasoning is important and the evaluation includes various LLMs into consideration.

Weaknesses:
The evaluation protocol is questionable. More comments and questions are in the following section.

Limitations:
More limitations should be discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a benchmark for evaluating the strategic reasoning of LLMs. The benchmark includes ten games of various types. The authors use these games to conduct competitive experiments between LLMs and traditional methods, as well as LLM-vs.-LLM. The paper then analyzes the experimental results and model behavior, and examines the game-theoretic properties of LLMs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is logically clear, understandable, and well-written.
2. The experiments are comprehensive. The authors evaluate comparisons between LLMs and traditional methods and LLM-vs.-LLM competitions. They include multiple open-source and closed-source models and tests of various prompting methods.
3. The authors evaluate game-theoretic properties, including Nash equilibrium with regret and Pareto efficiency.

Weaknesses:
I didn't find any significant weaknesses, only a few questions.

Limitations:
The authors have fully addressed the limitations in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a benchmark to understand the strategic reasoning capabilities of llms. The authors present a suite of game theoretic tasks with different structures to do this. They use different evaluation metrics like ELOs and Relative advantage to compare different llms and prompting methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is clearly written and well motivated. It provides some structure to the growing literature of strategic reasoning with llms.
- A wide range of closed source, open source models are tested. A good set of prompts are used to test the models too!
- I particularly liked table 1 and the selection of different tasks with different characteristics.
- The normalized relative advantage is a good, interpretable metric
- The framework and taxonomy are clear and easy to understand.
- Section 4.4 gave some good insight into the types of errors made by llms
- I also liked reading the analysis in section 4.3, in particular that code pretraining helps with strategic reasoning.

Weaknesses:
- Characterizing human performance would strengthen the paper
- Including some qualitative reasoning traces of successes and failures might be insightful.
- Minor: This paper would be an ideal fit for the datasets and benchmarks track, instead of the main track. I dont think it should be penalized for this though!

Typos

Line 79: Characterize

Line 171: dynamic gaming → dynamic game

Limitations:
The authors do a good job of addressing limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GTBench, a set of 10 different games to test how well large language models can think strategically. The author found that while LLMs struggle with complete and deterministic games like Tic-Tac-Toe and Connect-4, they perform better in incomplete uncertain games like poker and negotiation. Code-pretraining improves their strategic thinking abilities. However, advanced thinking methods like Chain-of-Thought and Tree-of-Thought don’t always help and can sometimes make things worse. The latest open-source models, like Llama-3, are getting closer in performance to commercial models like GPT-4. Common mistakes LLMs make include misunderstanding game rules, being over-confident, and making calculation errors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to understand. 
2. The problem of evaluating LLMs' strategic reasoning abilities is meaningful. Creating such a benchmark is valuable for the research community.
3. The paper provides a detailed evaluation of LLMs across different game tasks. These tasks indeed measure the strategic reasoning of LLMs, even if some models already understand the optimal algorithms for those games. (For example, you could ask GPT-4 about the optimal strategy for some of these games, and it knows the optimal algorithm.)
4. The authors conducted extensive experiments using various base models, including reasoning methods like ToT and CoT. They had some interesting findings and analysis (concluded in the summary).

Weaknesses:
1. The paper claims that measuring strategic reasoning capabilities with games is missing in existing benchmarks. However, there are other benchmarks, such as MAgIC released last year, that consider benchmarking LLMs' strategic behavior using games. While there are differences, this weakens the claim of novelty.
2. Some of the selected games, like Tic-Tac-Toe, have known optimal strategies and are not complex enough. These games might not fully challenge the advanced strategic reasoning capabilities of LLMs. Even though the current evaluation is useful, as a benchmark intended for future use, it should be capable of evaluating more advanced or adapted LLM agents.
3. The benchmark focuses on a set of 10 games. It’s unclear how well the findings generalize to other strategic scenarios, even similar types of tasks. The results appear to be quite case-by-case. A broader range of tasks and scalable evaluation frameworks would make the benchmark more comprehensive.
4. The experiments primarily involve LLMs and traditional solvers. There is a lack of evaluation against human opponents, which could provide more insights into the models' performance in real-world strategic interactions. As a benchmark, I also expect to have other opponents (for example, the optimal algorithm, the RL based agent).

Limitations:
If LLMs are trained on biased data, they might reinforce existing biases in strategic decision-making. Testing the LLM with different personas could show if this changes the game results and reveal potential biases.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ypaqE8UwsC;"REVIEW 
Summary:
This paper proposed the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm. The combination of offline RL and federated learning is interesting in addressing the training data insufficiency issue due to small pre-collected datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The originality of this paper is relatively good, since the proposed Federated Ensemble-Directed Offline Reinforcement Learning Algorithm is effective in offline reinforcement learning. The quality and clarity are also clear, and this paper is actually well-written. The significance of this paper is obvious, because offline reinforcement learning is important in real-world scenarios.

Weaknesses:
1. Some technical details need to be explained. For example, the ensemble learning and its role.
2. The novelty of this paper needs further clarification, and what is the main difference between this proposed method and existing studies? It seems that there is only a simple combination of two technologies.
3. Numerically, the authors could consider comparing their method with more baselines. There are some studies on federated learning for offline RL.

Limitations:
1. What is the technical drawback of the proposed method? E.g., the effectiveness of the agent weight by ensemble approach
2. Does this proposed method work for other RL algorithms?

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors identify fundamental challenges for Federated Offline Reinforcement Learning and present Fedora, an approach that tackles each of them. They perform extensive evaluation of the approach on Mujoco and real-world datasets showing improved performance over existing work.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is well-written and, importantly, the code has been shared. The authors run extensive experiments. The work is novel and the notion of federated optimism is particularly interesting. Federated offline RL is an important research area with vast real-world applicability. The algorithm has been shown to be robust to diverse/ heterogenous client datasets. It is also commendable that the approach was tested on a real-world robot.

Weaknesses:
No theoretical guarantees have been given for the algorithm though it does build upon foundational work.  I believe that the authors should explicitly discuss limitations/ opportunities for future work in the paper. It is important for the algorithm pseudocode to be included in the main material as is the norm in such papers. I believe that there are perhaps many experiments included the main paper meaning that the discussion/ hypotheses for results is somewhat diluted. 
Another minor issue is that the figures are placed very far away from where they are referred to in text.

Limitations:
Limitations should be explicitly stated. I feel that the authors could give a more balanced view of the algorithm by not only showing strengths but also assessing the limits of the work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA), a novel approach for collaborative learning of high-quality control policies in a federated offline reinforcement learning (RL) setting. The paper identifies key challenges in federated offline RL, including ensemble heterogeneity, pessimistic value computation, and data heterogeneity. To address these issues, FEDORA estimates the performance of client policies using only local data and, at each round of federation, produces a weighted combination of the constituent policies that maximize the overall offline RL objective, while maximizing the entropy of the weights. Besides the core idea, FEDORA also performs data pruning

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. This is a novel work proposing the first federated offline RL algorithm in the general case (without assuming linearity). The paper is very well written with clear motivations and detailed discussions on the insufficiency of existing, naive approaches. 

2. The experiments are also very thorough and convincing with experiments ranging from simple 2D environments to high-dimensional continuous control problems. The algorithm is also tested on a real-world robot platform, which is very impressive given the density of algorithmic contributions in the paper.

Weaknesses:
1. ""Collect wisdom"" can be replaced by more rigorous exposition. Same goes with ""ambitious targets"". 

2. The number of communication rounds needed for FEDORA to converge is still quite high. 

3. Given how well the algorithm does, some sort of theoretical analysis could further strengthen the work.

Limitations:
Yes, limitations are adequately discussed.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
m772lelDe3;"REVIEW 
Summary:
The paper studies the conditions that lead to consensus in matrix-weighted consensus networks when constant time delays are present. The analysis considers both leaderless and leader-follower settings. The paper considers single integrators with uniform time delays, heterogeneous time delays, and double integrators with two constant time delays. The paper derives the conditions for asymptotic convergence to a consensus or clustering configuration. The mathematical techniques include direct eigenvalue evaluation and application of the Lyapunov-Krasovkii theorem. The paper explains how the derived results can be applied to the problem of bearing-based network localization.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1. 1The paper provides a novel theoretical characterization of the asymptotic consensus for matrix-weighted consensus networks in several settings. In multi-agent networks  

S2.	The analysis is clearly presented and the arguments are logical and well-structured. The employed techniques are innovative.

Weaknesses:
W1.	Although the scope of NeurIPS is broad, and papers are encouraged from diverse fields, this paper makes little attempt to connect to any learning-based problem or application. The paper seems to be a poor match for a machine learning conference. There is not a single citation of a paper from one of the leading machine learning conferences or journals. 24 of the 37 references are associated with papers in control journals and conferences. The paper would be of much more interest to the control community. If the authors consider this to be a significant research contribution that furthers the understanding of matrix-weighted consensus, then why not submit it to Automatica (this is the forum for several other cited matrix-weighted consensus papers), or IEEE Trans. Automatic Control, or IEEE Trans. Control of Networked Systems. If there is a belief that the paper exposes a problem or a technique to the machine learning community, then there must be a much more convincing effort to highlight the connections – where/how would the machine learning community find the presented results useful? 

W2.	The main paper shows how the theoretical results are applicable to the bearing-based network localization problem. There is very little explanation of whether the proposed approach to localization is advantageous, and how the theoretical results are useful – whether it is for analysis of a network, or for design of a network. The appendix provides the results of simulations. But even there, the discussion is limited to simple observations regarding the behaviour of the simulated network (consensus/instability). The simulation analysis needs to be more convincing and explain in detail how the theoretical results are useful for this problem. Alternatively, additional theory could be provided that pertains to the bearings-based network localization task. 

W3.	The presentation of the paper should be improved. In particular, the figures on pages 16-18 are far too small. The text in these figures is illegible. It becomes almost impossible to understand what information the figures are supposed to convey.

Limitations:
The paper includes one or two sentences in the conclusion to discuss the limitations. The only acknowledged limitation is the restriction to the constant time delay setting. The paper would be strengthened by a much more thorough discussion of the limitations. For example, it would be helpful to understand whether the authors consider the constant time restriction to simply lead to conservative bounds, or whether the results would be completely inapplicable in a variable time delay setting.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Paper under review analyzes the consensus of agents over a network. The agents have arbitrary but identical state space dimension, so not just scalar dynamics. The communication between agents is delayed and can be heterogeneous. Lyapunov–Razumikhin functionals with an LMI (that grows with the size of the network) are the main analysis tools. The literature in this area is vast. The results may already be contained as special cases of more general results from the control literature that was not referenced in the original submission

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- originality: very crowded area of research
- quality: high quality except for not being very precise about what is different from other work, and lacks a detailed discussion about what impacts the LMI bound
- clarity: paper is well written
- significance: more general results on the same topic appear to already exist

Weaknesses:
- Authors discuss some of the relevant literature but are never explicit about what actually is different. This makes it very challenging to understand what is different and new about this contribution.
- Along the same lines as the above statement. Results and analysis techniques may be already present in the papers referenced below.   
	- Jiang, W., Liu, K., & Charalambous, T. (2022) in particular solves the more general problem of consensus with heterogenous delays where each agent is an arbitrary linear dynamical system (A,B,C). The results in this paper appear to be a subset of that class of systems.
- LMI grows with the size of the network making it not scale well
- No interpretation of the LMI once it is derived (but they are huge matrices which kind of make them hard to interpret)

Limitations:
limitations discussion is not really sufficient, they only discuss future directions

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates consensus conditions for matrix-weighted consensus networks, both leaderless and leader-follower, in the presence of constant time-delays. It explores delayed consensus algorithms for networks of single- and double-integrators using relative positions. The study derives conditions for networks to achieve consensus or clustering using eigenvalue evaluation and the Lyapunov-Krasovkii theorem. It also discusses an application in bearing-based network localization. Some numerical simulations are also provided to demonstrate effectiveness of the results.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper studied an important problem in the control theory, proposing novel algorithms and extending existing work to matrix-weighted networks with time-delays. They provided structured and rigorous mathematical analysis/proof of their results and provide helpful numerical simulations to validate their theoretical results.

Weaknesses:
In my opinion, the major weakness (in terms of publication at NeurIPS) of this paper is below. This paper has strong focus on control theory and consensus algorithms, which makes it less relevant to the core interests of the NeurIPS audience, since NeurIPS emphasizes more on machine learning methodologies and applications. The paper does not clearly establish connections to machine learning problems or provide good experimental results involving machine learning tasks.

This paper contains quite some valuable novel research results, but it seems to me it is more appropriate for publication on a traditional control journal or conference, rather than such a top tier machine learning conference like NeurIPS. It is just not a good fit, and it might be better to reserve the space (which is quite limited) for some other good candidate paper submissions more relevant to Machine Learning, which are more aligned with the interests of core audiences of NeurIPS.

Limitations:
N/A.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
lizRmKCnBp;"REVIEW 
Summary:
The manuscript introduces a neural compression paradigm for effectively compressing diverse sets of 3D geometry models. The authors propose a two-stage framework that first converts irregular mesh models into a regular 4D TSDF-Def volume representation and then employs a quantization-aware auto-decoder network to achieve redundancy elimination and compact representation. The method claims to compress a large number of 3D mesh models with high accuracy and preservation of geometric details, outperforming state-of-the-art methods both quantitatively and qualitatively.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper presents a unique method for compressing 3D geometry sets by leveraging neural networks, which is a significant advancement in the field. NeCGS achieves an impressive compression ratio, which is a critical metric for 3D geometry data compression.
- The method maintains high accuracy and preserves detailed geometric structures even at high compression ratios. The authors have conducted comprehensive experiments and ablation studies across various datasets, demonstrating the effectiveness of their approach.
- The inclusion of source code in the supplemental material enhances the reproducibility and transparency of the research.
- The paper is well-organized, with clear explanations of the methodology and results.

Weaknesses:
- The manuscript mentions that the optimization process for TSDF-Def volumes is time-consuming (over 15 hours), which could be a limitation for practical applications. The manuscript should address the long optimization time required for the TSDF-Def volumes. Future work could focus on accelerating this process to make the method more practical.
- While the method performs well on tested datasets, it is unclear how well it generalizes to other, more complex, or varied 3D geometry sets, such as some geometry with thin structures or open boundaries (cloth). 
- The choice of an auto-decoder network is effective, but the paper could benefit from a more detailed explanation of why this architecture was chosen over others.
- While the method outperforms existing techniques, a more thorough comparison in terms of trade-offs, especially related to computational resources, would be insightful.
- The paper could provide more insights into how the method scales with the size and complexity of the 3D geometry sets. The paper should include scalability tests to understand how the method performs with larger and more complex datasets.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a neural compression algorithm, NeCGS to significantly compress geometry datasets. The algorithm mainly consists of 2 components, 1) regular geometry representation: This is an optimization algorithm to optimize the TSDF field such that the error between the original geometries and the geometries reconstructed by the deformable marching cube algorithm is minimized and 2) compact neural representation: regresses the optimized TSDF-def fields from compressed latent states, quantizes the latent states and compresses them further into bitstreams. The trained decoder can then be used to reconstruct the TSDF-def fields and the geometries can be reconstructed using the DMC algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The NeCGS algorithm can provide high compression ratios with impressive reconstruction capability of the geometries. Better geometry representations can be achieved using the proposed optimization algorithm. This is evident from the ability of the DMC method to accurately reconstruct surfaces. The DMC algorithm is also significant and seems to provide better reconstruction of detailed structure in the geometries. Overall, the developed compression method has high potential and the results presented in the paper are very impressive.

Weaknesses:
The biggest weakness of the proposed approach is the computational cost of the method. The exorbitantly large times required to compress the datasets reduce the value proposition. Additionally, it is not clear how much the computational cost scales with the size of the geometry dataset.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method to compress 3D geometry of diverse categories of objects. In the first step, the paper proposes a method to first convert an irregular mesh to a regular representation like a 4D TSDF-Def volume that implicitly describes the geometry. After this, an auto-decoder is trained that learns to reconstruct the 4D TSDF-Def volume from a compressed feature vector which is unique for each shape. Hence, with this design the model can summarize the similarity of local geometric structures within and across different 3D meshes resulting in a compact representation. Results on AMA, DT4D and Thingi10K datasets shows that the model can achieve compression of 3D models to a reasonable extent.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) **Clarity:** the paper is well written with each component of the method explained clearly which is easy to understand.
2) **Reproducibility:** All the details to replicate the results are provided along with the code and architecture details in the supplementary material.

Weaknesses:
1. The intuition behind preferring TSDF-Def 4D volume over TSDF 3D volume is unclear, even though an ablation study shows better reconstruction for thin structures. The quantitative results in Table 2 only show marginal improvements. An brief intuitive explanation of the design choice is helpful.
2. There are lot of methods which try to compress a neural field. For e.g. Triplanes[1], HashGrid [2], Vector Quantization [3], TensoRF [4], Dictionary Fields [5]. It is not very clear why this method does not compare with all these techniques which can be used for compression? 
3. Can this method generalize? Can I use the trained auto-decoder setting to compress a new 3D mesh on which the model is not trained on? How about other methods with which the method compares.
4. The paper does not do a relative comparison of the compression time with the baseline methods. Given the optimization time shown in Table 3, I have concerns about the practical usage of this method.

[1] Peng, Songyou, et al. ""Convolutional occupancy networks."" ECCV, 2020. \
[2] Müller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives with a multiresolution hash encoding. ACM TOG, 2022. \
[3] Takikawa, Towaki, et al. ""Variable bitrate neural fields."" ACM SIGGRAPH, 2022. \
[4] Chen, Anpei, et al. ""Tensorf: Tensorial radiance fields."" ECCV, 2022. \
[5] Chen, Anpei, et al. ""Dictionary fields: Learning a neural basis decomposition."" ACM TOG, 2023.

Limitations:
Limitations are adequately discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
this paper looks at the problem of compressing 3d shapes (esp geometry). this paper proposes a two stage approach. the first stage is regular geometry representation. the second stage is compact neural compression. results show some improvements.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. compressing 3d shapes is important to many applications

Weaknesses:
1. this paper over claims what it does. in L1-3, it says that they made the first attempt to tackle the problem of compressing 3D geometry sets containing diverse categories. this isn't true. there are at least two papers doing geometry compression of 3D geometry [a], [b].  

[a] On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes https://arxiv.org/abs/2009.09808
[b] Neural Progressive Meshes https://arxiv.org/abs/2308.05741

2. [a] and [b] are very important references but they are not cited nor discussed. it's not necessary to compare the proposed method with [a] and [b], but at least the authors should acknowledge the existence of these two papers.

3. optimization time is too long

4. it is unclear whether the proposed method is reproducible

5. typo L43: Matching cubes -> Marching cubes

Limitations:
yes

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
lTUXlmjcva;"REVIEW 
Summary:
This paper proposes the affinity score, which measures the non-linearity of an activation function $\sigma(X)$ given the distribution of $X$.
The affinity score is defined based on how well the 2-Wasserstein distance $W_2(X, Y)$, where $Y=\sigma(X)$, is approximated by $W_2(N_X, N_Y)$, where $N_X$ and $N_Y$ are Gaussian approximations of the distributions of $X$ and $Y$, respectively.
Note that $W_2(N_X, N_Y)$ has a closed-form solution, and it holds that $W_2(X, Y) = W_2(N_X, N_Y)$ if the relation between $X$ and $Y$ is locally affine on the support of the given $X$.
The authors then propose to characterize a DNN model by the set of affinity scores of activation functions in the model under a given input distribution.
Experimental results suggest that the affinity scores are relatively low in transformer-based vision models, meaning that the activation functions are used in a more non-linear region compared to CNN models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* The proposed score presents an interesting insight in comparing the series of CNN models and transformer-based models. Experiments suggest that transformer-based models utilize the non-linearity of activation functions more efficiently, leading to the higher prediction performance.

Weaknesses:
* It is empirically shown that the proposed score has a low correlation with existing non-linearity metrics such as R^2, but it is unclear whether the existing metrics are insufficient to analyze the DNN models in the way proposed in this paper. I would like to see how the distribution in Fig. 3(C) changes when other metrics such as R^2 are used instead of the proposed $\rho_{aff}$.
* In my opinion, one would expect the nonlinearity score to behave symmetrically at $x=0$ for activation functions like ReLU, but the proposed affinity score seems to have a lower score at negative $x$, as shown in Fig.2 or Fig.6. Is there any reasonable explanation for such a behavior of the proposed score?

Limitations:
See weakness above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes empirical statistics about different DNN architectures in the hope to shed some light into why some architectures are better than others for some computer vision tasks. To do so, the study leverages common optimal transport results on DNN's internal representations, under some strong assumption about the distribution of those representations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper proposes to consider an interesting and useful question of going to the bottom of why some architectures are better than others as measured by some restricted downstream task.

Weaknesses:
- I do not agree with the following statement `Without non-linear activation functions,
84 most of DNNs, no matter how deep, reduce to a linear function unable to learn complex patterns.` as to me, models such as transformers with linear attention and linear MLP blocks have no actual nonlinearity but are higher order polynomial of the input, i.e., are not linear. Could the authors provide clarifications on that statement or did I misunderstand something?

- I also disagree with the following `Activation functions were also early identified [29, 30, 31, 32] as a key to making even a shallow
86 network capable of approximating any function, however complex it may be, to arbitrary precision.` since again, Fourier series for example can approximate any function as well. Hence DNN nonlinearities are certainly not the key ingredient to function approximation in general

- Many formal results such as Theorem 3.3 are well known and have been established for years (even decades) but no reference is provided which is misleading to the reader.

- Fig 2. is also misleading since the ""nonlinearity"" of any activation function depends on the range of the inputs. The only case that wouldn't be true is e.g. for ones with constant second derivatives, i.e., a linear activation function.... hence again that statement is highly misleading in presenting ReLU as inherently benefiting form that property compared to others

- the statement `No other metric extracted from the activation functions of the
260 considered networks exhibits a strong consistent correlation with the non-linearity signature.` is again an overstatement as the authors only compare with a few alternatives and theorem is provided to support such a statement

- the statement `We proposed the first sound approach to measure non-linearity of activation functions in neural
270 networks` is also incorrect, see e.g. 
  - https://jmlr.org/papers/v20/18-418.html
  - https://arxiv.org/pdf/2301.09554
  - https://arxiv.org/abs/1810.09274
  all the above works have been published in peer reviewed journals/conferences

Limitations:
In addition to my concerns expressed above, the study does not provide any actionable insights or understanding on the ""why"" of different architectures performing differently beyond the proposed statistical numbers. How could one use the provided analysis to better design model architectures or for model selection? 

Also, the paper does not provide any novel theoretical results. All the major theorems and results are already widely known within the community, yet they are presented as part of the contributions. With that in mind, the paper solely leverages existing OT tools, with some underlying simplifications on the DNN's data distribution, and report computed metrics. Hence the study falls below acceptance level in my opinion and would need a major rewriting + additional novel contributions to be worth acceptance.

The writing style is also filled with unsupported claims and highly misleading statements (see the **Weaknesses** examples).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel method for quantifying the non-linearity of activation functions in neural networks, termed the ""non-linearity signature."" Using an affinity score derived from optimal transport theory, it measures the non-linearity of individual activation functions. It defines the non-linearity signature as a comprehensive set of these scores across all functions in a deep neural network (DNN). The study compares these signatures across a range of popular DNN architectures in computer vision, revealing clear patterns in their evolution over the past decade, notably showing a trend towards decreasing non-linearity until the disruptive impact of vision transformers. It emphasizes the uniqueness of their measure, as it does not strongly correlate with other metrics across different architectures. The approach could potentially be applied to analyze the non-linearity of newer large language models (LLMs) and identify innovative neural architectures that optimize internal non-linear characteristics for enhanced performance, crucial in the era of costly experiments with large-scale model optimizations.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Novelty and importance. The paper introduces a theoretically grounded measure, the affinity score, for quantifying the non-linearity of activation functions using optimal transport theory, providing a robust framework for analysis. This is the first approach to approximately measure the non-linearity of DNNs, which is crucial for understanding their inner workings.
2. Solid theoretical and experimental validation.  The method is grounded in optimal transport theory, providing a rigorous theoretical foundation for the proposed non-linearity signature. This enhances the credibility and robustness of the findings. The experimental results demonstrate the practical utility of the non-linearity signature. It can predict DNN performance and meaningfully identify the family of approaches to which a given DNN belongs, making it a valuable tool for researchers and practitioners.
3. Clear Writing. The structure of the paper is well-organized, with a clear presentation of background knowledge, theoretical properties, experimental evaluations, and conclusions. This clarity aids in understanding the contributions and implications of the research. The paper's figures and tables are comprehensive, providing clear and precise information, and the writing maintains a coherent logical sequence.

Weaknesses:
1. The authors should discuss more activation functions. Currently, only ReLU, Tanh, and Sigmoid are included.  While these are among the most commonly used activation functions in neural networks, many other activation functions have been introduced and shown to be effective in various contexts, like GELU. Including a more comprehensive analysis of a diverse set of activation functions would enhance the robustness and applicability of their proposed method. 
2. There is currently some research on the nonlinearity of deep neural networks that should be compared and discussed.
3.  It would be beneficial to showcase examples from domains beyond computer vision. While the paper focuses on computer vision tasks, it may not address the non-linearity signature's applicability to other domains such as NLP, speech recognition, or reinforcement learning. The findings might be less generalizable if the proposed measure does not perform equally well across diverse types of tasks and data.

Limitations:
Yes, they have discussed the assumption of Theorem 3.3.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
lEDuaGGiCV;"REVIEW 
Summary:
In this paper the authors introduce LUCY, a new LLM based framework for converting text-to-SQL to query databases. Primarily this framework focusses on addressing user queries to databases that contain a large number of tables with complex relations between them.
The core idea of this approach is to decompose the query generation process into distinct stages. LLMs (GPT-4) are utilized for generative tasks such as identifying relevant tables and attributes and generating the SQL query. Meanwhile a deterministic constraint solver (OR-Tools) is employed to map relationships between these elements. In essence LUCY processes a user query through 3 phases namely MatchTables, GenerativeView and QueryView phases. In the MatchTables phase the goal is to identify the relevant tables and attributes. This is accomplished by iteratively prompting a Large Language Model (such as GPT-4) to identify relevant tables and attributes based on the user query and the database model, which includes the schema and an optional list of high-level design patterns. The database model is presented in a hierarchical manner and explored using a breadth-first search approach. Once the relevant relations and attributes are identified a schema graph is constructed and solved using a constrainst solver (i.e to identify the optimal path to join the tables) to build a view in the GenerativeView phase. A LLM is then prompted to generate a SQL query given the summary view and the user query in the QueryView phase. The authors further conduct experiments that demonstrate that the proposed technique achieves a better execution accuracy as compared to the existing state-of-the-art techniques on standard datasets(ACME, BIRD). Furthermore, they also introduce a show large improvements on a new benchmark dataset (Cloud Resources).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The literature review is comprehensive and the paper does a good job at clearly defining the problem to solve. 
2. The novelty of the proposed approach lies in the decomposition of tasks involved in generating SQL queries. By employing LLMs to handle specific subtasks, it effectively circumvents the need for LLMs to perform complex reasoning. A core distinguishing factor from prior research is the use of constraint solvers to identify the relevant paths for joining the identified tables. 
3. The authors also demonstrate that the proposed approach achieves a better execution accuracy than the existing SOTA on several benchmarks.

Weaknesses:
1. The paper ends abruptly without a clear and comprehensive conclusion. The paper presentation needs improvement in this regard.
2. The authors introduce a new benchmark for evaluation but do not offer sufficient details regarding it. A detailed overview of the queries and an analysis of why the existing SOTA techniques do not perform well on the same could be provided which could greatly inform future work.
3. The practical utility of the proposed technique seems to be limited as each user query requires multiple calls to be made to LLMs thereby entailing both increased latency and cost.
4. The error analysis is not very comprehensive and could be improved. For instance how does this technique fare when the names of entities in database schemas are not semantically meaningful or if there are conflicts in descriptions etc (as is often the case in real-world industrial databases).

Limitations:
1. As the technique leverages LLMs it seems to be heavily reliant on having semantically meaningful entity names /descriptions 
2. The proposed technique seems sensitive to hallucinations as it involves processing a query through multiple LLM phases. The errors in any of the earlier phases would result in it propagating to the next stage. For instance as the authors pointed out if the MatchTables phase produces an extra table this could in turn effect the end output.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of developing effective LLM-based assistants for querying SQL databases. In this context, users pose questions to a relational database in natural language, and the goal is to generate a SQL query that correctly answers the user's question when executed. The authors focus on overcoming a limitation of current text-to-SQL approaches: the difficulty LLMs face in handling databases with numerous tables and complex relationships, making it hard to determine the necessary table joins for the query.

To tackle this issue, the authors propose a workflow that begins with using the LLM to identify relevant tables and their attributes. In the second step, a constraint satisfaction solver (CSP) is employed to determine the necessary joins while adhering to database constraints. In the third step, a materialized view is created by joining the relevant tables. Finally, this view, combined with the user's question, is used to prompt the LLM to generate the final SQL query.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper tackles a very relevant practical problem, which is attracting significant attention in both academia and industry. The proposed workflow may add practical value.

Weaknesses:
The paper lacks depth, and the writing does not, in my opinion, meet the quality standards required for a venue like NeurIPS. Additionally, several potential limitations of the proposed workflow are not discussed.
	•	Accuracy of the answers is not the only important requirement in generating SQL from text. Database users also expect query generation to be time-efficient. The proposed workflow includes several computationally intensive steps: first, solving an NP-complete problem (CSP), and second, creating a potentially enormous materialized view by joining many tables. I would have expected a discussion on the computational limitations of this approach.
	•	The workflow lacks sufficient precision and clarity. For example, it is unclear whether the final query is expressed with respect to the materialized view as the only table or with respect to the original schema. Additionally, how lookup tables and various schema design patterns are identified in the input database is not well-explained. The authors claim that their approach guarantees the generated query respects database constraints, but this guarantee is not clearly defined. Algorithm 1 is underspecified; at this high level of detail, the algorithm seems redundant and could be subsumed by the text description. The exact SQL fragment covered by this approach is also unclear. While the limitations section mentions that queries requiring the union operator are not supported, it is unclear if other standard SQL constructs are also unsupported.
	•	While relevant related work is cited, the main body of the paper lacks a detailed discussion on the contribution in relation to recent approaches.
	•	The evaluation section is somewhat lacking. The tables are confusing, and it is unclear what each of the rows actually represents.

Limitations:
The limitations section mentions some but not all of the relevant limitations of this approach.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author proposes a new method, Lucy, designed to handle large databases with complex relationships between objects. Lucy operates through three steps: MatchTables, GenerateView, and QueryView. It first identifies relevant tables and attributes using LLMs, constructs a combined view with an automated reasoner, and generates the final SQL query. Lucy shifts complex reasoning from LLMs to a CSP solver, supporting various database design patterns. Experiments on ACME insurance, Cloud Resources, and the two BIRD databases show that Lucy outperforms other zero-shot text-to-SQL models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The proposed method offers a fresh perspective on tackling text-to-SQL research with a logical workflow.
- The paper is well-written and easy to follow.

Weaknesses:
- I am not convinced by the motivation of zero-shot text-to-SQL with the example of industrial databases having complex relationships. Text-to-SQL systems deployment in real-industry requires high performance. I doubt that people won't be using zero-shot models for real use applications. In KaggleDBQA, it also states ""we believe the zero-shot setting is overly-restrictive compared to how text-to-SQL systems are likely to be actually used in practice."" I would like to hear the authors' thoughts on this.
- The paper does not appear to be well-grounded in text-to-SQL research. For example, one way to handle complex relationships in text-to-SQL using LLMs is through schema linking. However, the paper does not mention this area of research and instead proposes MatchTables, seemingly ignoring the rich literature of text-to-SQL works. Other approaches include least-to-most prompting attempts in text-to-SQL for task decomposition and Natural SQL for intermediate representation (although it does not handle query nesting). Properly discussing these relevant methods of the proposed method will better situate the work.

Limitations:
The limitations of the work are well-stated.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Lucy, a framework for solving Text2SQL by LLMs, particularly for complex enterprise databases. Lucy leverages LLMs' understanding and reasoning capabilities to handle intricate database relationships and constraints. The framework operates in three phases: identifying relevant tables and attributes (MatchTables), constructing a view through constraint reasoning (GenerateView), and generating the final SQL query (QueryView). The empirical studies show Lucy achieves performance improvements on several zero-shot Text2SQL tasks.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Text2SQL is an essential problem in commercial scenarios.

Weaknesses:
The draft seems far from complete, so leave some high-level suggestions.
1. Make the title, abstract, and introduction more concrete. It is hard to tell the contribution or uniqueness of this work among other papers about Text2SQL by LLMs.
2. Survey related works and clearly state the contribution/novelty of the proposed method against others.
3. Define the terminologies or abbreviations before their first appearance.
4. Make the draft concise by removing unnecessary content. For example, the first challenge introduced in Motivation section is not relevant to this work.
5. The empirical studies could be more convincing by following others' evaluation protocols, such as BIRD.
6. Lack of comparison to other competitors.
7. The figures, tables, and their captions should be self-explanatory.

Limitations:
There is a discussion about the limitation, though the first limitation seems too broad and unnecessary.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ks0FrTSCnK;"REVIEW 
Summary:
The paper extends the problem setting of learning with noisy labels (LNL) to include open-set noise, where noisy labels may come from unknown categories, in contrast to the traditional focus on closed-set noise. The authors theoretically compare the impacts of open-set and closed-set noise and analyze detection mechanisms based on prediction entropy. They construct two open-set noisy datasets, CIFAR100-O and ImageNet-O, and introduce an open-set test set for the WebVision benchmark to validate their findings. Their results show that open-set noise exhibits distinct characteristics from closed-set noise. The paper emphasizes the need for comprehensive evaluation methods for models in the presence of open-set noise, calling for further research in this area.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The research problem is interesting. Compared with learning with closed-set noise, learning with open-set noise is under-explored. 
- The theoretical analysis seems to be solid.

Weaknesses:
- Some technical details are hard to follow. Writing needs to be polished. 
- The contribution from the algorithm perspective is not enough.

Limitations:
N/A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an approach to address the challenge of open-set noise in the context of learning from noisy labels. The authors propose a method that differentiates between 'easy' and 'hard' types of open-set noise, which is critical for improving the robustness and performance of learning models faced with noisy data. By integrating existing Learning with Noisy Labels (LNL) techniques with novel entropy-based noise detection mechanisms, the paper presents both theoretical insights and empirical validations of the proposed methods. The contributions are significant as they offer a refined perspective on handling different noise complexities, which can enhance the utility of machine learning models in real-world applications dealing with noisy labels.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Originality: The paper addresses the issue of open-set noise in learning from noisy labels with a novel approach, differentiating between 'easy' and 'hard' noise types. This nuanced consideration is original as it pushes the boundaries of how noise is typically treated in noisy label learning.

Quality: The theoretical explanations are thorough and complemented by robust empirical evidence that strengthens the methodological claims.

Clarity: The paper is well-structured, offering clear explanations of complex concepts, which aids in understanding the proposed methods and their implications.

Significance: The significance of this work is evident as it tackles a critical issue that can potentially enhance model robustness and performance in real-world scenarios where label noise is common.

Weaknesses:
Dependency on Specific Methods: The reliance on entropy-based techniques for noise distinction may not generalize across all scenarios or noise types.

Experimental Scope: The experiments primarily utilize synthetic datasets, which might not fully capture the complexity of real-world data applications.

Limitations:
Limited Experimental Scope: The experimental validation focuses predominantly on synthetic datasets like CIFAR100-O and ImageNet-O. While these are commonly used in the research community for benchmarking, the real-world implications of the findings might be limited without additional testing on more varied and real-world datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on open-set label noise problem. Authors first formally extending closed-set transition matrix to open-set transition matrix and define two noise ratios for open-set and closed-set separately. 
Then authors define error inflation rate as a measurement for noisy label impact and measure for two conditions, classifier fitted noisy distribution or memorized (overfit) noisy label. Later, authors propose a new type of open-set noise by exclusively transitioning outlier classes to a specific inlier class, and consider this as a ""hard"" open-set noise and traditional open-set noise as ""easy"" case. Authors further analysis two noise types on two classifier conditions and claim traditional entropy based open-set detection might only works on ""easy"" case. Experiments are performed on CIFAR-100, ImageNet and Webvision datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Authors formally define open-set noise with a similar symmetric/asymmetric setup as closed-set noise, and find out that it shows opposite trend with different classifier cases.

Weaknesses:
- The experiment parts lack of baselines. With a new type of noise proposed, previous baselines on easy open-set noise should be run to assess the performance gap and set up the benchmark.
- Figure 4 (a) and (b) have similar distribution, it is hard to draw conclusions from entropy dynamics.
- Supp E.1 results are confusing. ""X+EntSel"" should be a better strategy since it selects inlier clean samples. However, why the closed-set classification accuracy is always the worst? Table 1 Webvision result is similar as well. Why is the claim ""EntSel + SSR improves open-set detection performance"" valid? The Acc and AUC are both dropping after adding EntSel. Why SSR/DivideMix + EntSel is always the worst performance? Considering it is a combination of inlier and clean, shouldn't it be the best performing one? I assume this is still the normal accuracy and AUC, which is the higher the better.

Limitations:
Authors do not address any limitations in conclusion. A possible limitation might be related to approximation of fitted and memorized classifiers.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper refines the problem of learning with noisy labels (LNL) by addressing the often overlooked issue of open-set noise. It provides a comprehensive theoretical analysis comparing the impacts of open-set and closed-set noise, introduces novel datasets for empirical validation, and explores the effectiveness of entropy-based noise detection mechanisms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper offers a thorough theoretical analysis of the differences between open-set and closed-set noise, extending the current understanding of LNL.
- The exploration of entropy-based mechanisms for detecting open-set noise adds a practical tool for improving LNL methods.
- This paper is well-written and easy to understand.

Weaknesses:
- The author summarizes two types of open-set noise, i.e., the easy and the hard noise, which is very similar to the symmetric and asymmetric label noise from the perspective of the transition matrix. So does there exist the instance-dependent open-set noise? What is its form if exists?

- In Section 3.5, the author conducts analyses regarding entropy dynamics-based open-set detection, which belongs to the **Fitted case**. If adopting the vision language model (such as CLIP) to fine-tune and detect the open-set noise, is it aligned with the **Memorized case**? It would be better for the author to provide a real-world application for the memorized case.

- The author should clearly illustrate the construct method of closed-set in the experiment (Figure 3) for reproducibility.

Limitations:
See weaknesses

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
kOp0kiXZ3a;"REVIEW 
Summary:
The paper addresses challenges in model quantization for deep neural networks (DNNs), focusing on optimizing quantization-aware training (QAT) across multiple bit-widths with weight-sharing. To this end, this paper introduces a novel quantization method that exploits the highest integer precision to achieve nearly lossless bit-switching, reducing storage without relying on full precision. Key contributions include: (1) Adaptive Learning Rate Scaling: A technique that dynamically adjusts learning rates for different precisions to address competitive interference and inconsistent gradient issues during one-shot joint training. (2) Double Rounding: An extension for one-step rounding quantizer in fixed-precision quantization to improve accuracy. Experimental results on the ImageNet-1K dataset show that the proposed methods surpass state-of-the-art approaches in both multi-precision and mixed-precision scenarios, achieving higher efficiency and accuracy.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This submission is well-written, as well as with good figures in Sec.4.
- The authors conduct extensive experiments on multiple datasets and multiple networks.

Weaknesses:
- Some analysis is missing. For example, I'm wondering whether the second rounding leads to more quantization errors, as the first rounding is used to produce INT8 weights and second rounding is then performed to quantize lower bit-width, the twice quantization is possible to cause more clipping errors and rounding errors, some analysis could enhance the strength of proposed methods. 
- Some designs should be further clarified, e.g., why ALRS is applied only for the scaling factors? Intuitively, weights of small bit-width is induced large gradient variance by STE, and thus the weights of small bit-width should also benefit from using smaller LR. 
- Fig. 1 is a bit confusing, some colored arrows are not well explained. 
- This works essentially lies in the research of mixed-precision quantization, so I think it is better to compare more MPQ (e.g., HAQ, DNAS, LIMPQ, etc) research in the Sec.4. Moreover, some recent papers on multi bit-width quantization are missed on the , e.g., [1] (PTQ-based) and [2][3] (QAT-based), which could be included into the Related Work. 

[1] Xu, Ke, et al. ""PTMQ: Post-training Multi-Bit Quantization of Neural Networks."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 14. 2024.

[2] Tang, Chen, et al. ""Retraining-free model quantization via one-shot weight-coupling learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. 

[3] Zhong, Yunshan, et al. ""MultiQuant: A Novel Multi-Branch Topology Method for Arbitrary Bit-width Network Quantization."" arXiv preprint arXiv:2305.08117 (2023).

Limitations:
Please refer to the weaknesses. Overall, this paper currently needs more experiments and analysis to reveal some designs are reasonable.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a QAT scheme to jointly optimize a single model with different precisions. The authors apply their scheme on various CNN-based models on CIFAR-10 and ImageNet datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written
2. The ablation study is strong in my opinion and they evaluate various aspects of their scheme

Weaknesses:
1. I think the main limitation of the paper is the models and datasets. I believe that the study should be done on larger models (LLMs for example) as a architecture goal. For example, the authors show that they do not save a FP32 master copy of the model in their scheme. However, ResNet style models (or MobileNet) are easy to fit in even moderate GPUs and I don't think FP32 master copy is a big problem in that case (please correct me if I'm wrong).

2. I couldn't find a source-code to reproduce the results of the paper in my side.

Limitations:
yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses advanced methods in multi-bit model quantization. Specifically, this paper proposes a method for one-shot joint training of multiple precisions. To this end, the authors introduce a double-rounding quantizer that leverages the highest integer precision to achieve nearly lossless bit-switching while reducing storage requirements. Moreover, they also propose an Adaptive Learning Rate Scaling technique that adjusts learning rates dynamically for different precisions. Two proposed techniques mitigate the competitive interference between bit-widths caused by inconsistent gradients of different precisions during biased gradient estimation. They also extend their Double Rounding method to support one-shot mixed precision training and develop a Hessian-aware Bit-witdh sampling strategy. Experimental results on the ImageNet-1K classification task show that their methods outperform state-of-the-art one-shot joint QAT in both multi-precision and mixed-precision scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Eliminating the costs of retraining for mixed-precision quantization is a meaningful and challenging topic.

- The end-to-end experiments are sufficient, and the presentation is good.

Weaknesses:
- More uniquness analysis needed. The using of Hessian information seems a bit trivial, each layer's Hessian is just used to compare with the averaged Hessian trace. Firstly, as shown in recent zero-cost NAS research [1], the architectural proxies will be less effective as the training goes on, I'm not sure the Hessian information obtained on the initial full-precision model will remain useful as the quantization-aware training continues. Moreover, the sampling probability is modified with a simple ascending heuristic, which is not Hessian-aware. 

- Also applies here: the design of the double-rounding quantizer is similar to Bit-Mixer, Adabits, and ABN. Specifically, ABN also uses 

- ALRS needs further ablations. In ALRS, the authors use a fixed scaling ratio to bit-widths, e.g., 8-bit is 1, 6-bit is 0.1, and 4-bit is 0.01, the choice of these scaling factors still requires more ablation studies and discussions. 

- More comparisons needed. Since this paper adopts an ILP-based search algorithm to find optimal subnets, it is better to compare with these ILP-based mixed-precision quantization papers, e.g., [2] and [3]. 



[1] A Deeper Look at Zero-Cost Proxies for Lightweight NAS 
[2] Mixed-precision neural network quantization via learned layer-wise importance, ECCV 2022. 
[3] Hawq-v2: Hessian aware trace-weighted quantization of neural networks, NIPS 2020.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a bit-switching quantization method using Double Rounding, which applies rounding twice to achieve nearly lossless switching without storing a full-precision model. They also introduce Adaptive Learning Rate Scaling (ALRS) to adjust learning rates dynamically across precisions, ensuring consistent quantization updates. Additionally, they develop Hessian-Aware Stochastic Bit-switching (HASB) for one-shot mixed-precision training, optimizing bit-width distribution based on layer sensitivity, thus eliminating retraining stages.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. ALRS heuristic can help practitioners who wish to train mutli-precision model

2. Authors made extensive experiments on vision models and compare to previse methos

3. Most sections are well written

4. Code is given

Weaknesses:
**Novelty** is limited and I am not highly motivated that the problem is important.

1.	The main contribution is to not same 32bit weight and different quantization parameters but only the high bidwith using a pretty straightforward idea of double rounding during training
2.	The ALRS is based on observation and heuristic to fix it. It is nice and helps for when trying to use 2 bits as well. Yet, I am not sure it is important for methods that don’t use the double rounding.

**Motivation**

3.	Since we usually don’t switch models based on data I am not sure why this is important. Do we really have edge device that switch on a daily base model precision and thus need to store in small local memory the 32bit model? Can you elaborate why and where multi precision is really important.

4. No results on more recent models (LLMs)

Limitations:
The authors partially discuss limitation

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
k0qTnbQxzR;"REVIEW 
Summary:
The paper presents CogCoM, a novel approach to training large Vision-Language Models (VLMs) using a mechanism called Chain of Manipulations (CoM). This mechanism enables the model to solve visual problems step-by-step with evidence, inspired by human cognitive processes like marking and zooming into images. CogCoM integrates manipulations such as grounding, zooming, and OCR into the VLM architecture, allowing it to handle various visual problems without external tools. The model is trained using a robust data generation pipeline and evaluated across multiple benchmarks, demonstrating state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Advantages of the Paper

1. **Explainable Reasoning and Manipulation Mechanism**: CogCoM generates intermediate steps with evidence, making the reasoning process transparent and explainable, which is crucial for complex visual tasks. The model incorporates a flexible set of manipulations that can be adapted to various visual problems, improving its versatility and problem-solving capabilities.

2. **Data Generation Pipeline**: The paper introduces an efficient pipeline for generating high-quality training data, which is essential for training VLMs to perform detailed visual reasoning.

3. **Superior Performance**: CogCoM achieves superior results across multiple benchmarks, including detailed visual question answering and visual grounding, showcasing its effectiveness and robustness.

These advantages highlight the paper's contributions to advancing the capabilities of VLMs in solving detailed and complex visual problems through a novel, human-inspired approach.

Weaknesses:
Weakenss in Points

This paper is generally good but I can still spot the following issues.

1. **Design of Figures and Tables**: The figures in the paper are not well-designed. The first and second figures are repetitive in meaning, and the colors in the first figure are too light (consider adding black outlines to the boxes). The font size in the second figure is too small to be legible on smaller screens. Additionally, the captions for Table 2 and Table 3 are too close to the tables, violating the submission guidelines.

2. **Lack of Discussion on Related Work**: The paper lacks a discussion of existing related work. It should consider citing and comparing with at least other agentic LMMs such as LLAVA-Plus[1] to provide a comprehensive comparison and context.

[1] https://arxiv.org/abs/2311.05437

Limitations:
See above Weakness part.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the Chain of Manipulations (CoM) mechanism for data generation to enhance visual reasoning in VLMs. The authors developed a data generation pipeline, producing 70K high-quality samples, and created the CogCoM model. CogCoM achieves state-of-the-art results across nine benchmarks, demonstrating significant improvements in various visual tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The CoM introduces a new data generation mechanism that enables VLMs to perform step-by-step visual problem solving with supporting evidence.
2.A data generation pipeline is proposed, producing a dataset of 70K high-quality samples.
3.The trained model, CogCoM, achieved SOTA in nine benchmarks.
4.This paper is well-written and easy to understand.

Weaknesses:
1.During data generation, the process relies entirely on GPT-4 for prompting and existing models (GroundingDino, PaddleOCR) for generation. As mentioned in the appendix, inaccuracies in these current visual models can affect the quality of generated data and the model's reasoning capabilities. However, the system lacks validation or filtering mechanisms to enhance data quality.
2.To highlight the specific improvements brought by CoM, it would be helpful to provide results both with and without the incorporation of CoM data. This would clarify the impact of CoM, especially since CogCoM integrates a significant amount of additional data such as MultiInstruct and LLaVAR during the instruction tuning stage as shown in Table 1.
3.The CoM dataset includes 6K high-quality manually annotated math samples, but no test results for math problems are provided. Clarification is needed on whether the purpose of this math data is solely to enhance the model's reasoning capabilities.
4.The paper emphasizes that CogCoM is a model capable of multi-image multi-turn understanding, but no corresponding test results (qualitative or quantitative) are provided.
5.In the model section, some parameters are not specifically explained, such as the maximum turns the model can accept and the predefined threshold.
6.Typos error: Line 288 CogOM->CogCoM

Limitations:
see weakness

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Drawing inspiration from human cognition to solve visual problems through localizing, zooming, etc., this paper introduces a new framework called CogCom, which solves visual problems by automatically combining six types of basic manipulations. When facing a visual problem, CogCom can use reasoning to solve each step and employ basic tools to aid in the problem-solving process. To achieve this goal, CogCom constructed a data generation pipeline that leverages GPT4 to build the training data for   CogCom. The CogCom leads to performance gains compared to its baseline CogVLM on several benchmarks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The CogVLM makes gains based on CogVLM on several benchmarks. 
2. The pipeline that leverages GPT4 to construct manipulation pipelines for problem-solving is reasonable.

Weaknesses:
1. The VQA benchmarks reported in Table 1 are not very convincing. It would be beneficial to consider more modern and challenging benchmarks such as MMBench, MathVista, and SeedBench.
2. The comparison of baseline methods seems to be based on relatively outdated approaches. It might be more informative to compare them with more recent LVLMs like LLaVA-1.5, Monkey, and ShareGPT4V.
3. It would be helpful to discuss a closely related work ViperGPT [3] and V* [4]. ViperGPT shares an idea for solving visual problems via planning tool pathways. V* shares the idea of searching and zoom-in progressively.
4. The differences with some other related works should be discussed [5][6]. 

[1] Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models

[2] ShareGPT4V: Improving Large Multi-Modal Models with Better Captions

[3] ViperGPT: Visual Inference via Python Execution for Reasoning

[4] V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs

[5] CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding

[6] DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models

Limitations:
The limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
jwE1dgOox1;"REVIEW 
Summary:
Large scale topological descriptors of data are leveraged to compute point/node-level descriptors, which encode to which large scale topological feature each point belongs to. For this, a combination of applied algebraic topology and applied harmonic analysis is used. More specifically, large scale homological features are computed using persistent homology, then represented with harmonic cocyles, and then averaged locally to obtain a point-level descriptors. The problem of topological clustering (already introduced in the literature) is addressed, whose objective is to determine to which large scale topological feature a certain data point belongs to. A set of benchmark datasets are introduced for topological clustering. The pipeline is applied to these datasets as well as real world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Large scale topology of data is leveraged to assign point/node-level features to data. This gives concrete meaning to what it means for a data point to belong to a certain large scale topological feature.
- The method is based on well-established mathematical concepts.
- The concept of topological clustering is interesting and has potential.
- A suite of synthetic datasets is introduced.

Weaknesses:
Regarding unjustified claims:

- Existing approaches are undersold. Specifically, in the introduction it is said that ""none of these approaches is able to represent higher-order topological information"" and that ""such higher-order topological information is however invisible to standard tools of data analysis like PCA of k-means clustering"". However, cluster structure is topological structure. Does ""higher order"" mean homology in dimensions 1 and above?
- Remark 4.2 says that ""datasets with topological structure consist in a majority of cases of points sampled with noise from deformed n-spheres"". This seems like a really strong claim. Is there any evidence of this?

Regarding theory:

- Theorem 4.1 applies in a very restricted scenario. Moreover, I do not understand why the harmonic representative takes values in {0, -1, 1}. This seems very surprising since harmonic cycles/cocycles almost always take fractional values (in order to minimize energy). I did not understand the proof of this fact; specifically, why $g$ being a harmonic generator for the entire filtration range of $(b,1)$ implies this claim.

Regarding the methodology:

- The method, specifically line 225, seems to assume that a cycle with coefficients in $Z/3Z$ will also be a cycle when interpreting those coefficients (0,-1, or 1) as real numbers. However, this need not be the case. To see this it suffices to consider a simplicial complex given by a triangle with no interior. Thus, step 3 of Algorithm 1 (and the method more generally) seems to be heuristic.
- The setup up Table 1 is unclear to me. How can one compare TOPF, which produces feature vectors, with, say, DBSCAN, which produces a clustering?
- Figure 4 is hard to interpret. For example, how should one assess the effectiveness of the algorithm in Fig 4(a)?
- The methodology has many hyperparameters. Some choices, like delta=0.07 in line 241, seem arbitrary.

Limitations:
- The experimental evaluation is limited.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces TOPF, a topological feature extraction mechanism on point cloud data. The authors consider Vietoris-Rips/$\alpha$ filtrations over point clouds and compute the persistent homology. They propose a heuristic to select the “top” features from the barcodes. They consider the corresponding representatives for these features and project them onto the harmonic space of the simplices. These projected vectors are then normalized and used to construct a point-level feature vector. The authors use this framework for clustering. Towards this end, the authors introduce a topological point cloud clustering benchmark and report the experimental results on this benchmark.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors propose to use Hodge Laplacian and Hodge decomposition to compute feature vectors over points in point-cloud data, which is a novel idea.

Weaknesses:
1. I do not fully understand the “learning” the representation here, because the representation is not particularly being learnt. It is being computed by using the persistent homology of the point cloud. 

2. Experimental evaluation is limited to clustering. And even in clustering, it is primarily limited to shapes which are partially/fully topologically spherical.  

3. The robustness of the approach is due to the robustness of harmonic persistent homology known in the literature. 

4. The paper uses well-known notions in the TDA literature in the context of point-clouds, which amounts to an incremental progress in this direction. 

5. It would strengthen the paper if the authors include a small paragraph explaining why projecting onto the harmonic subspace solves the problems that exist in using the homology representatives directly. 

Minor: 

Page 2, Line 81: ‘Spaces in topology are “continuous”’. Continuity is a notion defined for functions on topological spaces and not for topological spaces themselves. Spaces are connected.

Limitations:
Yes, the authors have discussed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an approach to select and compute some point-level topological features for point cloud or general data set analysis.
The main ideas is to define a multi-scale simplicial complex representation, thus we can track how the homology modules change along the filtration and then select the homologies that persist for a long range of scales.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Topological features are usually not localized, the idea of being able to bring back the topological descriptor to the relevant points is quite novel and impactful.
- The approach is theoretically sound and well analyzed.
-The experimental evaluation is limited but convincing.

Weaknesses:
- the feature selection is very heuristic.
- The evaluation is only on point cloud clustering. Since we are evaluating effectiveness and robustness of localized features, feature/point correspondence problems would have been interesting.

Limitations:
The main limitation, i.e. the selection of the features, has been briefly addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel method for extracting per point topological features - TOPF. The method builds on previous results in topological data analysis which described a shape or a point cloud with a single global feature, by generating per-point topologically-aware features. The paper presents a quantitative evaluation and comparison of the proposed method with prior art on a new benchmark consisting of several synthetic examples, evaluates the robustness of the proposed method under noise, as well as presents qualitative examples of its performance on synthetic and real work data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
* The paper is well written and easy to follow. Prior art and the proposed algorithm description is detailed and comprehensive.
* To my understanding, the paper describes a novel method for per-point feature extraction based on topological information contained in a point cloud, and describes theoretical guarantees for its correctness on point clouds sampled from multiple n-spheres.
* The paper describes a new topological point clustering benchmark dataset consisting of seven synthetic point clouds with up to 5 labels, and evaluate the proposed and existing methods on this dataset showing that the proposed method outperforms existing methods in most cases.

Weaknesses:
* The paper lists common machine learning applications requiring point level features as a motivation for the proposed method. However, only quantitative experiments for point cloud clustering on a set of synthetic examples, and anecdotal evidence of performance on real world data, were presented. In order to fully understand the potential of the proposed approach to be applied beyond synthetic data, it would be beneficial to include additional evaluation, qualitative and quantitative, on real-world data and additional applications, e.g. as described in lines 304-307.
* Specifically, it would be interesting to see experiments on non-synthetic datasets with topological structure mentioned in line 266.
* Additionally, comparison with other well performing modern machine learning methods, such as graph neural networks for point cloud clustering, needs to be discussed, for completeness.

Limitations:
The authors adequately addressed the limitations and impact of the proposed approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
jKzLukkKZO;"REVIEW 
Summary:
The paper ""Learning to Control the Smoothness of GCN Features"" investigates the impact of activation functions, specifically ReLU and leaky ReLU, on the smoothness of node features in Graph Convolutional Networks (GCNs). It provides a geometric characterization of these effects, showing how altering the input's projection onto eigenspace M can control the smoothness of output features. The study introduces a Smoothness Control Term (SCT) to modulate the smoothness of node features, aiming to improve node classification tasks in both homophilic and heterophilic graphs. Experimental results validate the efficacy of SCT, demonstrating significant improvements in node classification accuracy for several GCN-style models .

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
## Originality
- The paper introduces a novel approach to control the smoothness of Graph Convolutional Networks (GCNs) features, which is a significant departure from traditional methods. It builds upon and extends the work of Oono & Suzuki and Cai & Wang by integrating geometric insights with the message-passing process in GCNs.

## Quality
- The paper provides a robust theoretical framework, including geometric characterizations and proofs, that underpin the proposed methods.
- Extensive experiments validate the theoretical claims, showing significant improvements in node classification accuracy. Detailed descriptions of the experimental setup, including datasets and hyperparameter tuning, enhance the reproducibility of the results.

## Clarity
- The paper is well-structured, with clear sections that logically flow from introduction to theoretical analysis, experimental validation, and conclusions.
- The paper provides a comprehensive review of related work, situating its contributions within the broader context of graph neural networks research.

## Significance
- The ability to control the smoothness of GCN features addresses a to me interesting and important challenge in graph neural networks, with potential applications in various domains such as social network analysis, biological networks, and recommendation systems.
- The proposed SCT shows improvements in real-world datasets.
- The insights gained from this work could inform future research on activation functions and feature smoothness in other types of neural networks.

Weaknesses:
## Weaknesses
- The removal of white space in the paper makes it hard to read. The authors should really try and make the paper easier to read visually by not condensing as much math in the main text as possible. Not only is this arguably in violation of the guidelines, but it also illustrates that the authors need to distinguish clearer what the main contributions are and which parts in the main text can go to an appendix.
- While the geometric insights provided are valuable, the complexity of the mathematical formulations is challenging for readers not well-versed in advanced geometry and spectral graph theory. Simplifying explanations or providing more intuitive examples could enhance accessibility. Moreover, I feel like the math could be made more intuitive by giving verbal explanations before the theorems. The cramming of the paper and not highlighting enough what the main contributions should be improved.
- Although the paper compares SCT with a few baseline models, it would benefit from a broader comparison with additional state-of-the-art methods in GCNs and GNNs to provide a more comprehensive evaluation of its effectiveness.
- The experiments are primarily conducted on benchmark datasets. Incorporating more real-world applications and diverse datasets would demonstrate the practical relevance and versatility of the proposed method.
- The drop in accuracy for deeper models (16 or 32 layers) is noted but not deeply analyzed. A more thorough investigation into the causes of this performance degradation, beyond mentioning vanishing gradients, could offer insights into potential improvements. What happens if you use techniques that combat eg oversmoothing?
- While the paper mentions computational efficiency, a more detailed discussion on the computational overhead introduced by SCT, including potential trade-offs between accuracy and efficiency, would be beneficial.

Limitations:
## Limitations

The authors have acknowledged several limitations of their work, including the over-smoothing issue in deep GCNs and the dependence on specific activation functions, but they could improve by providing more detailed discussions on computational efficiency and potential negative societal impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the challenge of balancing smooth and non-smooth features in graph convolutional networks (GCNs) for node classification. Building on previous work that highlighted the correlation between feature smoothness and classification accuracy, the authors propose a novel method to control the smoothness of node features through a geometric approach and an augmented message-passing process. Their strategy involves establishing a geometric relationship between input and output vectors of activation functions like ReLU and Leaky-ReLU, and integrating a learnable term in the graph convolutional layers to modulate feature smoothness. The paper provides an empirical study to showcase the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper offers a novel geometric insight into the effects of different activation functions, specifically ReLU and Leaky-ReLU, on smoothing in GCNs. Furthermore, it introduces an innovative method to enhance the message-passing framework for GCNs (and similar networks) to better control smoothing.

Weaknesses:
* The empirical results section is very weak and a major revision is needed in order to be able to judge the merit of the proposed method. The most significant weaknesses are:
    * The paper does not compare to established baselines from other competing methods. For instance, one could take a look at the tables in https://arxiv.org/abs/2202.04579 or https://arxiv.org/abs/2210.00513 and add the results to the comparison in this paper (i.e., for cornell, texas, wisconsin, squirrel, and so on). 
    * The paper reports different (mostly weaker) performance for the baseline models such as GCN and GCNII. Again, this should be fixed and established results can be taken from the tables in the two papers mentioned above.
    * The improvement using the proposed SCT is very marginal in many experiments, and in particular the performance still drops (or improves very little) for higher number of layers. It is somewhat expected that increasing the number of layers and solving oversmoothing does not help much in homophilic tasks, i.e., the tasks considered in Table 1. However, if the method works it should lead to significant improvements on heterophilic tasks, even for increasing number of layers. Therefore, please show the results for the datasets in Table 2 in the same way as you show them in Table 1, i.e., for increasing number of layers.
    * It is claimed in Table 1 that the drop in performance of GCN for increasing number of layers is due to vanishing gradients and not due to oversmoothing. This claim has to be justified empirically. 
    * Figure 4 in the appendix showing gradient norms is not meaningful. The y-axis should be displayed on a logarithmic scale. Vanishing gradients occur for gradients approaching zero (exponentially fast). It is not clear if this is the case here, since it is plotted in linear scale.
    
* The structure and readability of this paper should be improved. For instance, it would be helpful for readers who are not very familiar with the graph-learning field, to introduce the concept of GNNs and GCNs in the introduction. It would be advisable to move the technical aspects from section 1 to section 2. In fact, none of the definitions presented at the beginning of the introduction are needed anywhere else in the remainder of the introduction section. Thus, this could be moved to section 2.

Limitations:
* The method necessitates a pre-processing step to compute the eigenbasis in equation (5). This may not scale efficiently for very large graphs.
* The method does not show significant improvements. Notably, there are more effective models and methods available for mitigating oversmoothing, which have not been cited or empirically compared. Examples include https://arxiv.org/abs/2202.04579, https://arxiv.org/abs/2210.00513, https://arxiv.org/abs/2206.05437, https://arxiv.org/abs/2110.14446, https://arxiv.org/abs/2206.10991,
https://arxiv.org/abs/2006.11468.
* While the theoretical justification of the approach is intriguing, the insights are somewhat limited. For example, there are no provided estimates for choosing the parameter alpha, which instead has to be learned through gradient descent.
* The empirical results are not convincing.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies how GCN smoothes node features in terms of unnormalized and normalized smoothness. The results show that adjusting projection can alter the normalized smoothness to any desired level. Based on this, the paper proposes a new method SCT to let GCN learn node features with a desired smoothness to enhance node classification and verifies it effectiveness in practice.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	Understanding the effect of nonlinearities in GNNs is an important yet underexplored problem due to the complexity of nonlinearities. The paper offers a new perspective on it.

2.	Oversmoothing is a known issue, while it is also known that some amount of smoothness is desired for graph learning. How to find the ideal amount of smoothness among node features is an important but nontrivial problem.

3.	The proposed method SCT is principled and seems effective.

Weaknesses:
1.	While I can see that normalized smoothness has its own merit, this notion could be better motivated and connected to the literature. 

   a.	Given analysis in [27, 4] is asymptotic, I would not say that “over-smoothing – characterized by the distance of features to eigenspace M or the Dirichlet energy – is a misnomer”, as it is too strong of a claim to make. Those results essentially say that “very deep GCNs are bad due to oversmoothing” which has their limitations but can be well justified by the distance of features to eigenspace M or the Dirichlet energy.

   b.	My understanding is that normalized smoothness could be more connected to the non-asymptotic notion of oversmoothing studied in [32], which is defined based on the Bayes error of classification (the distance to the decision boundary) and hence taken the magnitude of features into account.

   c.	Based on the above, the argument presented in the paper can be strengthened in the following way: the motivation of normalized smoothness should be based on a discussion on how the magnitude (and hence normalized smoothness) is more related to a non-asymptotic notion of oversmoothing, which is directly related to the classification performance of finite-depth/shallow GNNs. Based on this, the results present in this paper is more practically relevant than the previous asymptotic result.

I would suggest the authors modify the relevant text in the introduction and analysis accordingly.


2.	The analysis only applies for GCNs, while whether the analysis or the proposed method can be extended to more complexed GNNs such as GATs or graph transformers is unclear. 

3.	The analysis only applies for ReLU and LeakyReLU, which reads a bit specific. I wonder if the results can be generalized to a general family of nonlinearities.

4.  For the experiments, there is a lack of baseline comparisons except the basic backbone architecture. For example, I wonder how it would compare to APPNP, which is proposed to balance the need to explore larger neighborhood and locality and the implicit goal is also to produce node features with the ""right"" amount of smoothness.

4.	Another presentation suggestion I have for the authors is that one should minimize the use of in-text math and bold or italic fonts for highlighting (such as line 167-173).  Math is hard to read in-text and when too many texts are highlighted, the paper becomes ever harder to read because everything seems to be emphasized and it kind of messes up with its original purpose.

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper first shows that in GCN, the output of ReLU or LeakyReLU lies on a sphere whose input is characterized by components parallel and perpendicular to $\mathcal{M}$, the space spanned by eigenvectors for the maximum eigenvalue of a graph. As a corollary, this paper shows that these activation functions do not increase the component of the feature vector perpendicular to $\mathcal{M}$. Furthermore, this paper defines the normalized smoothness and evaluates how its range varies with the activation functions. Based on this discussion, this paper proposes an SCT that learns the parallel components of the feature vectors. The proposed method is applied to GCN, GCNII, and EGNN, and verifies its effectiveness by applying it to node prediction tasks with various heterophily.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The theorems presented in the theoretical analysis (Propositions 3.2 and 3.3) enable a unified treatment of ReLU and Leaky ReLU.
- Numerical experiments show that the proposed method is effective for various data sets and models, showing the versatility of the proposed method.

Weaknesses:
- I need help understanding the explanation in Section 3.3. More specifically, it is difficult to understand that the *independence* of the inequality means that the upper bound of the inequality does not depend on the value of $\boldsymbol{Z}_{\mathcal{M}}$. I suggest writing it explicitly. 
- P7, L.265: It seems strange that although SCT changes its architecture depending on whether the underlying GNN is GCT or GCNII, it has the same name. I suggest naming SCT for GCT and SCT for GCNII differently.

Limitations:
The authors discuss in the conclusion section that the proposed method's limitation is that it assumes the model's oversmoothing. However, I do not think this is a limitation because if the model does not cause oversmoothing, there is no need to use the proposed method. Rather, I recommend evaluating whether SCT has bad effects when the model does not cause oversmoothing.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper deals with (Over-)smoothing is Graph Neural Networks. While it was previously known that GCN-type GNNs oversmooth, this paper reexamines the case for GCN-type architectures with Relu-type activation functions in terms of *normalized* smoothness. The authors show that the convergence behaviour of GCNs can be split into two parts, a ""smooth"" part and a ""non-smooth"" part and that by manipulating the smooth part, one can influence the normalized smoothness of the signal. From these theoretical insights, the authors propose a new system that uses a learnable parameter that modulates the smooth part, making the model able to learn the most beneficial normalized smoothness for the problem at hand.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper furthers the understanding of oversmoothing in GNNs
- The proposed approach is well-founded in theory

Weaknesses:
- The experiments are unconvincing. 
    - There is a slight improvement to be found in models with SCT, but this is not too surprising, as these models also have more parameters, and mostly improvements are quite slim.
    - You choose GCN, GCNII and EGNN, two of which have built-in skip connections, that are known to help with oversmoothing. This also coincides with the models that cope quite well with a larger number of layers. The vanilla GCN takes a huge hit in all benchmarks apart from ogbn-arxiv. So learning the normalized smoothness of features does not actually seem to help in the case of GCN. 
    - The other two models don't suffer from oversmoothing to begin with

Limitations:
The limitations are not well-discussed. 
The only limitation the authors claim for this work is, that: ""without this condition [that oversmoothing happens], SCT cannot ensure performance guarantees."" There are no performance guarantees given for SCT. This is the only limitation discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies how ReLU and Leaky ReLU affect the smoothness of node features in graph convolution layers. The authors demonstrate that adjusting the input projection onto eigenspace $\mathcal{M}$ of the node feature matrix can achieve any desired normalized smoothness. Additionally, they propose a Smoothness Control Term (SCT) to enhance node classification in Graph Convolutional Networks, validated on both homophilic and heterophilic graphs.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. From a geometric perspective, the authors prove that how ReLU and Leaky ReLU affect the smoothness of node features in graph convolution layers.
2. The experimental results validate the theory proposed by the authors.

Weaknesses:
Equation 5 implies that using SCT requires performing eigendecomposition. This paper avoids the high time complexity associated with eigendecomposition, especially when the number of nodes in a graph is very large.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
iRHxp1ibFj;"REVIEW 
Summary:
The paper introduces a novel image-level supervision method for semantic segmentation, utilizing approximate targets for the relative sizes of segments in training images. These targets, represented as categorical distributions for the expected average prediction over pixels, are integrated using a zero-avoiding variant of KL divergence as the training loss. This approach achieves quality comparable to full pixel-level supervision but is significantly less costly, requiring only rough estimates of the areas occupied by each class.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Using object size as a form of supervision is both innovative and interesting.
2. The proposed method is straightforward and easy to understand.

Weaknesses:
1. The title of the paper is misleading. It claims that approximate size targets are sufficient, but the work also uses image labels for supervision.
2. The most important comparison in Figure 1 is between 'Tag' and 'Size target,' as this validates the significance of using target size supervision. To clearly demonstrate that 'Size target' is superior to 'Tag' under identical conditions, it would be better to use the same architecture for both comparisons.
3. Labeling the size of objects can be challenging for humans and may introduce significant noise, especially for tiny objects. Although the authors demonstrate impressive accuracy with up to 8% size target errors, this remains a stringent annotation standard, particularly for small objects. For instance, as seen in Table 1, the mean relative error (mRE) often exceeds 10% during human annotation in the Pascal VOC dataset. Moreover, estimating target sizes in Pascal VOC is relatively easy since objects are typically large and centered. However, labeling images in more complex datasets, such as COCO, might result in a higher mRE.
4. In Table 1, the authors should also report the speed of tag annotation to highlight the cost of estimating target sizes.
5. The proposed method is straightforward and impressive for its end-to-end training, especially considering that existing weakly supervised semantic segmentation (WSSS) methods typically use CAM and two-step training. However, as shown in Table 2, while the proposed method achieves comparable accuracy to state-of-the-art WSSS methods, it relies on additional supervision and a high annotation standard (8% mRE). Moreover, Table 2 indicates that the accuracy with only tag supervision is close to that of fully supervised methods, suggesting that tag supervision alone may be sufficient for segmentation.

Limitations:
The authors do not discuss the limitations and broader impact of their method, which necessitates a dedicated discussion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new weakly supervised semantic segmentation task. This task uses pixel-level categorical distribution as the label in the training stage. KL divergence is used as the training loss. Experiments on three public segmentation datasets show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The proposed task is interesting. It provides the community another choice for segmentation with less annotation effort.

2.The proposed KL divergence loss is effective, demonstrated by experiments on three public datasets. It achieves performance comparable to methods using more expensive labels, like the box supervised one.

3.The proposed method is robust to size target error, which makes it more practical.

4.The writing is fluent and easy to follow.

Weaknesses:
1.Labeling effort on complex images. Images from PASCAL VOC (like Figure 1) are easy to annotate. It contains few classes and the background is generally clean. The density of target objects is low, and hence it’s also suitable for the proposed grid-based size target annotation way.

However, in practice, scenes are much more complex, with more classes, more crowded objects, and complex backgrounds. The authors are recommended to show the annotation effort on those images, like images from Cityscapes and ADE20K. I think when the scenes become more complex, the labeling effort will increase significantly. The labeling effort of size target will be much more than the tag way, since tagging will be less influenced in such cases.

2.Model performance on complex images. Similarly, it’s recommended to evaluate the model’s performance with the proposed loss on these complex datasets. This will give a more comprehensive understanding of the proposed method.

Limitations:
No negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper titled ""Approximate Size Targets Are Sufficient for Accurate Semantic Segmentation"" proposes a novel method of semantic segmentation that leverages approximate size targets instead of full pixel-level supervision. The method involves using categorical distributions to represent the expected average prediction over image pixels, utilizing the zero-avoiding variant of KL divergence as a training loss. The approach aims to reduce annotation costs while maintaining segmentation accuracy comparable to full supervision.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Originality: The use of approximate size targets as a form of weak supervision for semantic segmentation is novel and creative.
2. Quality: The experimental results are comprehensive and demonstrate the effectiveness of the proposed method across different datasets and segmentation architectures.
3. Significance: The approach has significant implications for reducing annotation costs in semantic segmentation tasks, making it highly relevant to practical applications.

Weaknesses:
1. Simplicity of Method：While the proposed method is innovative, it seems relatively simple. There might be opportunities to enhance its contributions with further development or by integrating additional techniques.
2. Limited Scope of Evaluation: While the paper evaluates the method on several datasets, it would benefit from a broader range of scenarios, including more diverse and complex images.

Limitations:
The authors have addressed the limitations related to annotation errors and have demonstrated the robustness of their method to these errors. However, it would be beneficial to discuss potential limitations in more detail, such as the scalability of the method to larger and more diverse datasets, and any assumptions made about the nature of the size target annotations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel image-level supervision method for semantic segmentation using approximate segment size targets. It utilizes categorical distributions for expected average predictions, reducing annotation cost and complexity. The authors propose a zero-avoiding KL divergence as a training loss, compatible with any segmentation architecture, and demonstrate significant robustness to size target errors, improving generalization. The method achieves state-of-the-art performance on multiple datasets with standard segmentation models like ResNet101. Additionally, it requires minimal extra information and no architectural changes, making it a practical and effective solution for weakly-supervised semantic segmentation in real-world applications.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper introduces a novel form of image-level supervision for semantic segmentation using approximate segment size targets. This approach is original in its use of categorical distributions for expected average predictions, providing a fresh perspective on weakly-supervised segmentation methods.

2. The quality of the research is high, with comprehensive experiments conducted on multiple datasets. The use of a zero-avoiding variant of KL divergence as a training loss is well-justified and demonstrates robustness to size target errors. The empirical results show that the method achieves state-of-the-art performance using standard segmentation models.

Weaknesses:
1. The paper claims robustness to size target errors but provides limited detailed analysis on this aspect. Including more experiments to quantify and analyze how different levels of size target errors impact performance would provide a clearer understanding of the method's robustness.

2. Lack of related work. The paper’s logical flow and organization need improvement. 

3. The paper lacks comprehensive comparisons with the latest models, such as ""SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation (AAAI24)"".

Limitations:
1. Fig and Figure are inconsistent in Line 24

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
hiwHaqFXGi;"REVIEW 
Summary:
The paper introduces DiGGR (Disentangled Generative Graph Representation Learning), a self-supervised learning framework that aims to guide graph mask modeling through disentangled latent factors to enhance the disentanglement of learned representations. Extensive experiments across 11 public datasets for node and graph classification tasks demonstrate the framework's effectiveness, significantly outperforming many existing self-supervised methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Innovative Approach: The DiGGR framework innovatively utilizes disentangled latent factors to guide graph mask modeling, a novel contribution in generative graph representation learning that significantly enhances the model's explainability and robustness.
Comprehensive Experiments: The paper conducts extensive experiments on multiple datasets and tasks, showing significant performance improvements over existing methods, thus providing strong empirical support for the proposed approach.

Weaknesses:
Complexity and Scalability: The framework appears computationally complex, which might limit its scalability to very large graphs or real-time applications. Unfortunately, this aspect is not extensively discussed in the paper.
Lack of Theoretical Analysis: While the empirical results are strong, the paper lacks a detailed theoretical analysis of why the disentanglement process improves performance, which could provide deeper insights into the method’s efficacy and limitations.

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a framework called DiGGR, aimed at improving the robustness and explainability of generative graph models by addressing the issue of entangled graph representations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper tells the story in an easy-to-read way, and the whole paper is quite easy to follow.
2. The problem of disentangled learning is a very popular yet important task.
3. The paper conducts comprehensive experiments to evaluate their method.

Weaknesses:
1. Lack of novelty. Graph disentangled learning is not a new task. There are tons of existing methods for disentangled representation learning, such as those maximizing KL divergence or minimizing mutual information between two sets of representations. A lot of related works such as [1], [2], [3] and [4] are not discussed. Also node factorization is not a new idea, such as node clustering in [3].

[1] Disentangled graph collaborative filtering. SIGIR 2020.
[2] Disentangled Graph Convolutional Networks. ICML 2019.
[3] Deep Generative Model for Periodic Graphs. NeurIPS 2022.
[4] Disentangled contrastive learning on graphs. NeurIPS 2021.

2. The motivation of the proposed method is not clear to me. For example, why should we use mask? Also, why the proposed method sticks to GAE, not VGAE or other types of GNN, such as GCN, GIN or GAT?

Limitations:
Yes. Limitations have been discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work proposes a disentangled generative self-supervised learning method for graphs. The authors introduce a latent factor learning module to capture the heterogeneous factors in the nodes. The proposed method factorizes the graph into factor-specific subgraphs, and jointly trains a disentangled Graph MAE applying distinct masks for each subgraph. Experimental results demonstrate that DiGGR outperforms traditional methods that treat the graph holistically, without accounting for its latent structure.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method first explores a factorization method for generative graph SSL. 
2. The authors provide extensive experimental results and analysis on both node and graph-level tasks to show the improved effectiveness, interpretability, and generalization by using the proposed method.

Weaknesses:
- The computation complexity of the proposed method is quite high. Could the author pride training time comparison to the baseline methods to help us get a sense of the real complexity?
- Could the author provide more insights on how to find an optimal factor number K according to the statistics of diverse datasets? This might be useful for real-world applications.

Limitations:
Yes, the authors discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a self-supervised learning framework DiGGR, aimed at enhancing the disentanglement of learned graph representations. The authors argue that existing generative graph models tend to overlook the entanglement of learned representations, leading to non-robust and non-explainable models. DiGGR addresses this by introducing a latent factor learning module and a disentangled graph masked autoencoder, allowing for factor-wise graph representations. The framework is tested on various benchmarks, demonstrating its effectiveness in outperforming previous self-supervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies an interesting research problem that is disentangled graph representation learning. This research problem is very hot recently.

2. The model design is easy to understand. The paper provides a detailed explanation of the proposed model.

3. The experiments demonstrate the effectiveness of the model. The performance improvement on some comparisons seems to be significant.

Weaknesses:
1. One of my concerns is from the novelty. I think the model design is a little similar to the works [1-2]. The authors should make more comprehensive discussions to show the differences between them.

2. The experiments ignore some recent or related contrastive baselines [1-4] for comparisons.  The improvements on some datasets seem to be not significant. 

3.  More large-scale benchmarks should also be considered, e.g., OGB. The experimental settings are not very clear for reproducing the results.

[1] Disentangled contrastive learning on graphs. NeurIPS 2021.

[2] Disentangled Graph Contrastive Learning With Independence Promotion. TKDE 2022.

[3] Augmentation-Free Graph Contrastive Learning of Invariant-Discriminative Representations. TNNLS 2023.

[4] MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning. AAAI 2023.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
hc9GkYw9kR;"REVIEW 
Summary:
This paper proposes a novel surrogate-assisted evolutionary algorithm named LORA-MOO, the core contribution is the introduce of ordinal-regression-based model  spherical coordinates approximation to SAEA and LORA-MOO can find a good trade-off between optimization efficiency and optimization results.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
This paper provides a novel perspective for modeling surrogates with high efficiency.

The experiments results seem good.

Weaknesses:
Motivation and contributions are limited. To the best of my knowledge, the main contrition of LORA-MOO is introducing ordinal-regression-based model for convergence and spherical coordinates for diversity. However, why do it and what is the connections between them? Besides, the manuscript includes a lot of informal expression. The proposed method is complex and effectiveness is limited.

Limitations:
See questions.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed the LORA-MOO framework, a surrogate-assisted MOO algorithm that learns surrogates from spherical coordinates. This includes an ordinal-regression-based surrogate for 10 convergence and M −1 regression-based surrogates for diversity.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The considered problem is pretty important.

Weaknesses:
Major:
1. Line 113-116, a bit too repetitive. 

2. Line 121, an initial dataset size of 11D-1, too specific.

3. Lines 121-134: the algorithms are described too specifically. It is more suitable for EC journals rather than NeurIPS.

4. Selection Criteria, too simple. 

5. What does LORA-MOO means?

6. Some HV-based MOBO methods are not compared as they are failed to solve many objectives. This argument is not accurate, please consider to run ""https://github.com/xzhang2523/libmoon/blob/main/libmoon/solver/mobo/run_dirhvego.py"", which supports more than ten objectives problems. 


Minor:
There are too many grammar errors in this paper. 
(1) Line 249, they are failed -> they failed. . 

(2) Line 270, HV use -> HV uses . 

(3) Line 175, consider using \max.

(4) line 21, are widely exist -> widely exist. 

(5) line 64, an non-pa .. -> a non-pa..

Limitations:
See weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a surrogate-assisted evolutionary many-objective optimization algorithm, named LORA-MOO. LORA-MOO is composed of a surrogate for ordinal modeling, which focuses on convergence, and m-1 surrogates for distribution modeling, which focus on diversity. Empirical study demonstrates the effectiveness of the proposed algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well-written and easy to follow. Although the proposed algorithm seems somewhat complicated, all the technical details are clearly presented, and the motivations behind them are explained. The empirical study is generally solid, with all the major parameters included in the ablation study, and most of the commonly used test instances are covered. The results show that LORA-MOO outperforms the baselines.

Weaknesses:
Although LORA-MOO obtained better indicator values than the baselines in synthetic problem benchmarks, it is difficult for me to find some fundamental differences between LORA-MOO and previous MOAs. Nor could I see what new insights this paper can provide for solving expensive MOPs. LORA-MOO models the convergence of solutions by a surrogate problem of the domination level, which is a common idea in MOO, adopted by many past methods such as NSGA-II. Such a surrogate is intuitive, but it is unclear to me why it could work better than the existing surrogates such as pairwise relation or function values, and why such a surrogate can be successfully modeled by a Gauss process. LORA-MOO uses m-1 surrogates to predict the spherical coordinate, but it seems identical to predicting function values. LORA-MOO also contains many other components, such as EA, PSO, non-dominated sorting, various clustering methods, and some subset selection mechanisms. These components have long been widely adopted by many MOAs, and there are many alternatives available as well. I agree that these components could usually make an algorithm perform better, but this paper does not adequately demonstrate any necessity for such a combination or any connections between these components. The ablation study appears to be a parameter-tuning experiment, presenting some results under different parameters. However, there is no ablation for the many components in the algorithm, so it is unclear what contributions these components actually make.

Expensive optimization problems are closely connected to real-world applications, and many real-world MOPs are indeed expensive problems. Therefore, I believe this is a very valuable research direction. However, the empirical study in this paper is mainly conducted on synthetic problems. DTLZ and WFG have undoubtedly driven the development of the MOO field, but they have also caused a significant number of researchers to focus narrowly on these synthetic test sets. As a result, there are now many algorithms that perform excellently on synthetic test sets but struggle to adapt to real-world problems. The authors have also conducted tests on NASBench, which is crucial for comprehensively demonstrating the algorithm's capabilities, but the results do not seem to be sufficiently convincing.

Limitations:
This paper does not summarize its limitations. I suggest the authors reconsider the limitations of this work and fully present them in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a surrogate assisted method for multi-objective optimization. The approach learns a surrogate function with the ordinal values as the regression labels. The ordinal values are generated using a iterative algorithm with the most dominated solutions having the highest ordinal values. The ordinal values are used to train a Kriging model used to select a point for observation using the convergence criterion. Another point is selected using the Kriging model trained on spherical coordinates via a diversity criterion. The approach has several parameters which are tuned via experimentation on real and benchmark datasets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The approach provides an innovative way for optimizing multi-objective functions by separating out the two objectives thus simplyfying the problem.
  1) The convergence objective designed to select the best solution wrt the ordinal values
  2) The diversity objective designed to improve the diversity of the Pareto optimal solutions
- It is experimentally shown that the method improves the IGD metric compared to several benchmark and real MOO problems.
- The ideas presented in the paper are well motivated and well presented.

Weaknesses:
- The approach presented in the paper is not sufficiently novel. Ordinal regression for multi-objective optimization has been studied before [1]. The differences with related prior work have not been discussed in detail.
- The proposed algorithm has many tunable parameters, and it is unclear how the parameters affect performance on real world problems when they have only been tuned on benchmark problems.
- The real world experiment on NAS shows improved regret eventually, but converges slower than other existing approaches. It is difficult to judge on the effectiveness of this approach based on a single experiment. Experiments on more real world optimization problems are necessary to make a conclusion.
- The paper is missing several notable MOO approaches from the Bayesian optimization community [2,3,4,5].

[1] Yu, Xunzhao, et al. ""Domination-based ordinal regression for expensive multi-objective optimization."" 2019 IEEE symposium series on computational intelligence (SSCI). IEEE, 2019.

[2] Tu, Ben, et al. ""Joint entropy search for multi-objective bayesian optimization."" Advances in Neural Information Processing Systems 35 (2022): 9922-9938.

[3] Zhang, Richard, and Daniel Golovin. ""Random hypervolume scalarizations for provable multi-objective black box optimization."" International conference on machine learning. PMLR, 2020.

[4] Paria, Biswajit, Kirthevasan Kandasamy, and Barnabás Póczos. ""A flexible framework for multi-objective bayesian optimization using random scalarizations."" Uncertainty in Artificial Intelligence. PMLR, 2020.

[5] Abdolshah, Majid, et al. ""Multi-objective Bayesian optimisation with preferences over objectives."" Advances in neural information processing systems 32 (2019).

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
hGhLd2ByoR;"REVIEW 
Summary:
This paper reveals novel pathologies in existing unsupervised methods aimed at discovering latent knowledge from large language model (LLM) activations. Instead of extracting knowledge, these methods tend to identify the most prominent features of the activations.

The paper theoretically demonstrates that arbitrary features (not just knowledge) can satisfy the consistency structure of a popular unsupervised knowledge-elicitation method, namely contrast-consistent search. Additionally, the authors conducted a series of experiments showing that current unsupervised methods for discovering latent knowledge are insufficient. While the paper proposes potential future solutions, it does not provide a definitive solution to the problem with existing unsupervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the paper is well-written, and its theoretical and analytical contributions may be useful. I am impressed about the extensive experiments.

Weaknesses:
More experiments on other LLMs are needed to further validate the claim.

It would be better to offer possible solutions to address the problems in existing unsupervised methods.

Limitations:
No solutions to address the problems in existing unsupervised methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a careful study on existing methods for discovering the latent knowledge from large language models (LLMs), especially Contrastive-Consistent Search (CCS). The authors prove that CCS might not actually discover the knowledge of LLMs, instead, it could fit any features that satisfy certain conditions. Through a series of experiments, the authors further demonstrate that CCS could be distracted by random words, irrelavant texts like the character's opinion, and remain sensitive to the choice of prompt. Finally, the authors propose some general principles for the future works about unsupervised LLM task discovery.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, the paper is well written and eazy to follow. The authors made interesting obervations about existing methods on knowledge discovery of LLMs. The theoretical analysis is well supported by the experiments. Sevaral guiding principles are also proposed for the future works. I think this paper would provide good information to the research community about unsupervised knowledge discovery of LLMs.

Weaknesses:
From my experience on unsupervised learning, I'd argue that the content of this paper *would not be sufficient to refute existing methods about unsupervised knowledge discovery (CCS)*. First of all, CCS is a method built on top of features from pretrained models. It'd definitely be sensitive to the features and thus also sensitive to the prompts, because features changes from different prompts (this could also be seen from the PCA visialization). Furthermore, as an unsupervised method, it'd be expected that the method might find multiple valid solutions, where only one of the solutions corresponds to the knowledge we are looking for. Taking the experiments from Section 4.2 as an example. The constructed dataset actually has two valid labels: the sentiment of the text and the sentiment of Alice. Depending on the optimization and the implicit bias of the algorithm, it could totally happen that an unsupervised method could found both valid labeling, or could only find one of them. I believe this is a common phenomenon shared by exsiting off-the-shelf unsupervised methods (like K-Means) cause they're searching for labels without supervision. From this perspective, I'd regard that this paper provides a method to construct ""adversarial datasets"" for CCS. However, it would not be a problem for CCS in practice.

Furthermore, the authors don’t provide solutions to this issue.

Also, I believe the mathematical notation in Section 3 could be simplified.

Minor issues: typo $c(x_i^+=1), c(x_i^+)=0$ in line 102

Limitations:
The authors have mentioned that this paper is focused on current methods and might not be directly applied to future works.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the failure modes of the method called ""constraint-consistent search (CCS)"" in knowledge discovery for language models. In particular, they showed: there is no unique identification on the minimizer of CCS, as there are a class of features achieves the optimal loss; demonstrated experimentally classic unsupervised methods detect features other than knowledge; discovered features are sensitive to prompt formats.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper points out a popular method's overlooked short-comings and presents both theoretical and experimental results to support that CCS may not be able to discover the true knowledge feature: 1. the observation on CCS loss is driven by xor operator rather than the feature is clever; 2. given the vast space of feasible features, CCS method is very sensitive to prompts and thus deserves more careful examination if to use CCS in practice.

Weaknesses:
The main weakness of the paper is its lack of novelty and potential impact to the field. The paper is more an analysis work on the application of a single method [1] proposed in 2023, which given the speed of ML innovation, it is hard to see long-term benefits of this criticism.  The general principles proposed in the discussion section (Section 6) are interesting and fit more into the line of proposing desiderata for the field - though in their current status, require more rigorous work. 

[1] C. Burns, H. Ye, D. Klein, and J. Steinhardt. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, 357 2023.

Limitations:
Yes.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
h8goI8uPXM;"REVIEW 
Summary:
The paper introduces decoupleQ, a novel method that decouples model parameters into integer and floating-point parts. This approach transforms the quantization problem into a mathematical constrained optimization problem, avoiding the limitations of traditional heuristic quantization methods. DecoupleQ achieves a significant improvement over existing methods in LLM., especially at extreme low bits (2-bit) and also release the W2A16 CUDA kernel.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. DecoupleQ eliminates the need for ad-hoc techniques to handle outliers and sensitive channels, focusing solely on optimizing model accuracy under extreme low-bit quantization.
2. DecoupleQ achieves a notable advancement over existing methods in LLM, particularly at extremely low bit. And the W2A16 CUDA kernel has been released.
3. DecoupleQ approach can be readily extended to supervised fine-tuning (SFT) to enhance model accuracy, or adapted for downstream sub-tasks.

Weaknesses:
1. Please correct me if I am wrong. It seems that decoupleQ combines several existing approaches. Specifically, it uses Adaround to get the integer part in ResNets and GPTQ to get the integer part in LLMs. Additionally, it integrates PTQ and QAT by applying PTQ to the integer part while using supervised training for the floating-point part.
2. Regarding your point from lines 58-61, I believe GPTQ clearly outlines how to calculate scale and zero point in their code. Moreover, GPTQ can be seen as a constrained optimization problem, where the constraints align with yours: each integer weight is confined within [$\alpha$, $\beta$], which is a default constraint in GPTQ.
3. Further experiments on LLMs are essential. For example, evaluating decoupleQ's performance in multi-task settings and within the LlaMa 3 family would provide valuable insights.
4. Could you provide more ablation studies in the second stage, such as experiments without training norm layers?
5. There is a typo in line 125. The first letter of 'decoupleQ' should be capitalized.

Limitations:
Please refer to the weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a linear and uniform quantization method, decoupleQ, which abandons the traditional heuristic quantization paradigm and decouples the model parameters into integer and floating-point parts, then transforming the quantization problem into integer and floating-point part. Experiments show decoupleQ achieves comparable acc as fp16/bf16 on 2-bit weight quantization setting.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Experiments show decoupleQ achieves comparable acc as fp16/bf16 on 2-bit weight quantization setting.

Weaknesses:
1. Experiments are based on W2A16, lower activation bitwidth(<=8bit) should be experimented.
2. The novelty is limited. The core idea of decoupleQ is similar to Normalization(Batch-Norm or Layer-Norm). The learnable floating part of decoupleQ equals to a learnable Normalization parameters.
3. More existing Quantization methods should be compared, such as, NWQ[1], PD-Quant[2]

[1] Leveraging Inter-Layer Dependency for Post -Training Quantization 
[2] PD-Quant: Post-Training Quantization based on Prediction Difference Metric.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents decoupleQ, a post-training quantization method that improves the accuracy of quantized models, particularly at very low bit-widths (2-bit). It achieves this by separating model parameters into integer and floating-point components and formulating the quantization process as a constrained optimization problem. This approach eliminates the need for traditional quantization techniques like outlier handling and focuses on optimizing the core objective.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The paper introduces a fresh perspective on quantization by abandoning traditional methods and reframing it as a constrained optimization problem.
2. decoupleQ demonstrates impressive results in 2-bit quantization, achieving accuracy comparable to higher precision formats like fp16/bf16 in large speech models.
3. The quantization process is linear and uniform, making it easier to implement in hardware compared to non-uniform methods.

Weaknesses:
1. The paper's writing lacks cohesion and clarity regarding its ultimate goal. The paper also has several spelling mistakes.
2. The authors claim to separate the model parameters into integers and floating-point components. However, as far as I understand, this practice is not a novel contribution but rather a common approach in quantization.
3. They address a portion of the optimization problem using GPTQ and another portion similar to BRECQ.
4. The authors acknowledge that their solution may not be optimal. 
5. The quantization process in decoupleQ can be more time-consuming than other methods.

Limitations:
Yes, however, it will be beneficial to divide the current Discussion section into separate Conclusion and Limitations sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel post-training quantization method to achieve 2-bit uniform quantization on large language and speech models. The proposed method decouples the quantized values into integer and floating-point parts, which are then optimized via a constrained optimization problem that can be solved with off-the-shelf solutions. The proposed method allows uniform quantization down to extreme bits.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper proposes a novel optimization-based method to conduct PTQ on large models. The proposed method is solid and unique from previous methods.
2. The proposed method achieves good performance with only uniform quantization, without special procedure for outliers etc., providing direct benefit to the runtime of the quantized model on general hardware.
3. The limitations and future directions are clearly discussed in the paper.

Weaknesses:
1. The distinction between the proposed decoupleQ and the traditional quantization methods are not clearly derived in Sec. 3.2. The statement that ""(s,z) lost the traditional meaning"" on line 138 is not clear. My understanding is that W, s, and z are now totally independent of the original weight w0 in the optimization process, as long as the final output error is minimized? I think adding a comparison with the optimization objective/procedure of the traditional quantization here will help.
2. The proposed method appears to be sensitive to the size of the calibration set, so that the calibration size reported in the experiments are much larger than that of the previous baselines. As it is understandable that the optimization process may require more data to avoid overfitting, it would be more fair if the baseline methods are also calibrated with the same dataset/training cost.
3. For the LLM experiments, only ppl is used as metric. However, the ppl has been shown to be an inaccurate metric to reflect the utility of the LLM after compression. More evaluations such as zero-shot performance on downstream tasks and the instruction following ability etc., as in SqueezeLLM and OmniQuant papers, would be helpful to see if the quantized model still retains the ability as the FP one.

Limitations:
The limitations and potential social impacts are adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
gsott1UXlq;"REVIEW 
Summary:
GATSM effectively captures temporal patterns and handles dynamic-length time series while preserving transparency, outperforming existing GAMs and matching the performance of black-box models like RNNs and Transformers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* This paper is easy to understand.
* GATSM can be understood as a linear representation with good transparency.
* Surprisingly, this method improves performance while providing better interpretability compared with black-box.

Weaknesses:
* It is not clear how multi-head attention works in Definition 3.1 to learn temporal patterns. My understanding is that the global feature interacts with the current feature in Eq.3, so why is it that the input to the attention is not $x$ but the transformed $\tilde_x$. And then, are temporal patterns captured from attention? 

* The addition of attention to NBM is under-motivated, so why not just replace $w^{nbm}$ to attention weight, so that you can learn one less set of parameters $w^{nbm}$.

* The experiment lacks DLinear [1], a strong baseline and providing interpretability. I think it's highly relevant.
1. Are Transformers Effective for Time Series Forecasting? AAAI 2023.

* Given the emphasis on interpretation or white-box modeling, qualitative experiments of the contributions/explanations need to be compared rather than visualization. If there is no ground-truth of the contribution, occlusion experiments in post-hoc methods [2, 3] can also be designed to explore the trade-off between performance and additive features.
2. Encoding time-series explanations through self-supervised model behavior consistency, NeurIPS 2023.
3. TimeX++: learning time-series explanations with information bottleneck, ICML 2024.

* In Table 7, why is the throughput of GATSM so much lower than NBM? Does NBM add up as features at the time level without sharing?

Limitations:
It's hard to deal with the higher-order interactions, e.g. GA^2M/GA^NM in time series, since time series often have long time points and the high complexity of gam-based techniques. In addition, the temporal-level causal interrelationships can be further explored.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GATSM(Generalized Additive Time Series Model), designed for handling multivariate time series data with a focus on transparency and interpretability. Using independent networks to learn feature representations and transparent temporal modules to learn cross-time step dynamics, GATSM effectively learns temporal patterns and maintains interpretability.  It achieves comparable results achieves comparable performance to black-box time series models on various datasets, and proves the transparent predictions with cases.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. A key strength of GATSM is its focus on transparency, providing clear insights into the decision-making process, which is crucial for applications in high-stakes domains like healthcare.
2. The paper presents a thorough evaluation across multiple datasets, including Energy, Rainfall, AirQuality, and several healthcare datasets, showcasing the model's robustness and generalization capabilities.
3. The authors provide extensive details about the experimental setup, including data splits, hyperparameters, and computational resources, facilitating reproducibility.

Weaknesses:
1.  The need for a large number of feature functions can limit scalability (even if it is reduced from TxM to B), particularly with high-dimensional data.
2. The dataset tasks encompass both 1-step forecasting and classification, each requiring distinct evaluation metrics. Presenting all results in a single table without proper clarification leads to confusion. 
3. The selected baseline black-box models are relatively simple. Consider including one or two state-of-the-art methods for a more comprehensive comparison.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a Generalized Additive Model for time series, combining feature embedding and attention layer. The proposed solution is evaluated on forecasting, binary and multiclass classification, over 8 datasets, against black box and transparent models. Global, local, time-focused and feature focused interpretability methods are provided.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Experimental evaluation is convincing: the presented model has state of the art performance. On the transparency side, the comparison between models allows the author to postulate on the presence of absence of interactions of covariates, and time-specific patterns. Evaluations of weights at the final linear layer give varied visualizations.

The reasoning behind the model construction and component is clear, and ablation experiment for each component were provided.

Paper is clear with no major problem in writing.

Weaknesses:
I am not sure if the work is original. The idea of using DNN first on the time axis without covariate interaction is not new, but wether there is a model similar to the proposed solution, I do not know. The review of previous works focuses on Generalized Additive Models, but a similar neural structure may have been presented without being positioned as a GAM.

One improvement to be done would be to add more clarity on figure captions. In its present form, it requires back and forth to the text to understand both what is plotted and what conclusions to draw from it.

Limitations:
Several limitations are identified: possible overfitting of the model due to overparameterization in NBM part, slow attention mechanisms that do not benefit from state of the art methods, and the fact that it was not evaluated for long sequences (and might not be suited for them).

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This work aims to build transparent models for the time series domain for better interpretability. Specifically, they proposed a work called Generalized Additive Time Series Model (GATSM) that consists of independent feature networks as well as a temporal attention module to learn temporal patterns. The corresponding model can be written into a scalar form to ensure interpretability/transparency. The authors applied their model to several datasets, and showed that GATSM can outperform existing generalized additive models. The model can also be used to interpret the features in original dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Important motivation. Building transparent models for time series is a critical task.
- The scalar representation of features seems clean (eq 11)

Weaknesses:
While this work is not of my direct expertise, I think the following contents have room for improvement:

1. Experimental results are weak.
- Black-box Time Series Models seem out-of-dated. The authors should consider better models such as TimesNet, PatchTST, FreqTransformer, or Informer for commonly used black-box models.
- Forecasting tasks use R2 score for evaluation, but an R2 score of 0.07 (or in general, below 0.5) seems very low. The authors should show some visualization examples to ensure the model is functioning.
- Figure 4/5 are not self-explainable, the authors should try to explain what is happening in those figures, and how the interpretability is quantified.
- The work could benefit from synthetic dataset, where casual relationships are manually crafted and thus can be evaluated.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
gBOQ0ACqoO;"REVIEW 
Summary:
This study reveals that modalities have varying impacts depending on depth, leading to the proposal of DH-Fusion. This method
dynamically adjusts feature weights using depth encoding, improving multi-modal 3D object detection. Results on nuScenes show DHFusion outperforms prior methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-presented. The structure is clear and easy to follow.
2. Comprehensive experiments on the nuScenes dataset are conducted to validate the effectiveness of the proposed DH-Fusion.

Weaknesses:
1. Lake of Novelty: The Depth Encoder in DH-Fusion is similar to the 3D Position Encoders in PETR (PETR: Position embedding transformation for multi-view 3d object detection). The Depth-Aware Global Feature Fusion (DGF) module and Depth-Aware Local Feature Fusion (DLF) module in DH-Fusion are analog to the Hierarchical Scene Fusion (HSF) module and Instance-Guided Fusion (IGF) module in IS-Fusion (IS-Fusion: Instance-scene collaborative fusion for multimodal 3d object detection). In conclusion, the contribution of this work seems like ""A+B,"" which is limited.
2. For the nuScenes test leaderboard, DH-Fusion achieved a Top 10 ranking only with 384x1056 image size and SwinTiny backbone.
Please provide results when using larger 900x1600 image size and ConvNeXtS backbone.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed a LiDAR-camera modality feature fusion method based on depth encoding for robust 3D object detection. Based on the observation that the LiDAR and camera modality information should have dynamic relative importance depending on the distance of object to be detected, the paper proposed a Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy which consists of a Depth-Aware Global Feature Fusion (DGF) module and a Depth-Aware Local Feature Fusion (DLF) module. Experiment on the public nuScenes and nuScenes-C dataset demonstrates that the proposed method is robust to various kinds of corruptions and achieves SOTA performance on 3D object detction.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The idea of depth-aware multimodality feature fusion for 3D object detection is reasonable, especially for the detection of distant objects.
2. The ablation study clearly demonstrates the effectiveness of the proposed DGF&DLF module when using BEVFusion as baseline
3. The presentation is clear and the ability of the proposed method on the detection of distant object in Figure 6 is impressive

Weaknesses:
1. How about the algorithm's performance on small object detection? small object could be normal-sized object at far distance or small-sized object in near distance, is it possible that the proposed depth-aware module hurts the detection performance of small-sized object in near distance? since according to Figure 5, LiDAR modality will have relatively larger weights at near distance, but it is in low resolution, so not good for small object detection.
2. Compare with SOTA, the achieved performance improvement is not that significant. as shown in table 1, the performance gap between the proposed method and IS-Fusion is small and IS-Fusion even achieves slightly better mAP, it is not clear whether the proposed method can achieve similar performance improvement as indicated in ablation study when using IS-Fusion as baseline.
3. In Figure 5(b), it would be good to add a color bar to indicate the magnitude corresponding to each color

Limitations:
There is no paragraph explaining the weakness of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel strategy for LiDAR-camera 3D object detection that emphasizes the importance of depth information in feature fusion processes. The authors argue that different modalities, such as LiDAR point clouds and RGB images, contribute variably at different depths, and this variation has been overlooked in previous works. The key contribution is the Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy that dynamically adjusts the weights of point cloud and image features based on depth encoding at both global and local levels. The DH-Fusion method surpasses previous state-of-the-art methods in terms of NDS on the nuScenes dataset and demonstrates robustness to various data corruptions. In general, the design is reasonable and performance is impressive.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is well-structured, with a clear abstract, introduction, methodology, experiments, and conclusion sections that logically flow from one to the next.
2.	The authors effectively communicate complex ideas through clear language and comprehensive illustrations, aiding the reader's understanding of the proposed method.
3.	The motivation of design is clear and experiments are extensive.
4.	The idea of depth encoding for dynamical fusion is interesting and reasonable.
5.	The performance is very impressive and the robustness makes the method more applicable to challenging scene.

Weaknesses:
The paper has no obvious weakness except they didn't do experiments on other datasets.
But I think the nuScenes is already large enough to demonstrate the general effectiveness.

Limitations:
There is no discussion of limitation in main text, but a justification is given in Checklist: using an attention-based approach to interact with the two modalities makes the detection results sensitive to modality loss.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces DH-Fusion, a novel Depth-Aware Hybrid Feature Fusion strategy for multimodal 3D object detection that leverages LiDAR and camera data. The key innovation lies in dynamically adjusting the weights of point cloud and RGB image features based on depth encoding at both global and local levels. The authors propose two modules: Depth-Aware Global Feature Fusion (DGF) and Depth-Aware Local Feature Fusion (DLF), which enhance feature integration and compensate for information loss during the transformation to Bird's-Eye-View (BEV) space. Experiments on the nuScenes dataset demonstrate that DH-Fusion surpasses state-of-the-art methods in terms of Novelty Detection Score (NDS) and is more robust to data corruptions, as evidenced by superior performance on the nuScenes-C dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper proposes a novel feature fusion strategy that adaptively adjusts the weights of LiDAR point cloud and RGB image features based on depth
2. The introduction of depth encoding at both global and local levels allows for more nuanced and context-aware feature integration, enhancing the detector's ability to understand the scene's depth structure.

Weaknesses:
1. The authors only present results on nuScenes dataset. The alogrithms should be also evaluated on other prevailing public dataset like KITTI.
2. The depth-aware fusion might be tailored to the specific characteristics of the training dataset, potentially leading to overfitting and reduced performance on diverse or unseen data.
3. While the paper includes ablation studies, a more extensive set of experiments that isolate the impact of different components of the system could provide deeper insights.

Limitations:
1.  While the method shows strong performance on the nuScenes dataset, its generalizability to other datasets or varied real-world conditions might require further investigation.
2. The paper does not provide a detailed discussion on the computational efficiency, which is crucial for practical applications, especially in terms of processing time and resource usage.
3 .The method assumes high-quality, synchronized data from LiDAR and camera sensors, which might not always be guaranteed in real-world scenarios.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
fzdFPqkAHD;"REVIEW 
Summary:
The paper presents ATS (Agent-To-Sim), a framework to enable agent behavior modeling from multiple casual video captures in indoor scenarios captured during long spans of time. The proposed pipeline consists in (1) 4D reconstruction of the scene geometry and observer and agent motion, and (2) controllable agent behavior learning and generation.

For the first stage, multi-video registration uses coarse-to-fine registration to globally align the cameras to a shared canonical space derived from DINOv2 per-frame features (initialized with a walkthrough clip of the environment) and then jointly optimizes the 3D structures while adjusting the cameras locally with novel featuremetric losses (which makes the optimization robust to changes of lighting and appearance and improves alignment accuracy) and standard photometric and regularization losses. With the proposed (annealed) swapping of latent per-video codes during optimization, missing information is shared across videos, while video-specific details are kept.

For the controllable agent behavior modeling, in order to generate plausible interactive behaviors, the generated behavior conditions on an encoding of the scene, observer, and past from the agent's egocentric perspective, which avoids overfitting to specific locations in the scene. Then, the ego-perception-conditioned generation of full body motion proceeds hierarchically via diffusion: Generated goals Z condition generated paths P, which finally condition generated body motions G.

The included experiments reflect the quality of the 4D reconstructions achieved by the proposal, the improvements in displacement errors compared to two baselines (as well as ablations of the proposed method), and a qualitative analysis of the effects of the behavior conditioning signals.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Great technical achievement to reconstruct agent behavior in indoor settings, exploiting the shared information across different videos captured at different times via robust alignment based on semantic features from foundational image models (DINOv2) and diffusion-based short-term hierarchical motion generation.
- Plausible long-horizon generation of agent motion for different bodies, conditioned on the environment, observer, and past trajectory.
- Despite the complexity of the system, the description is relatively brief and complete, whig, along with the rest of the paper, is excellently written.

Weaknesses:
- The paper focuses on environment-aware motion of agents in the presence of a (human) observer. Even if out of scope for this paper, it would be interesting to discuss more complex agent-environment interactions (see my questions below).
- I believe the current experiments use a small number of environments/scenes, which makes it hard to justify considering the system for larger-scale deployment, but I'll be happy to update my score if the authors correct me.

Limitations:
The authors reasonable address limitations and social impact in the appendices.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses using an iPhone's RGBD camera to collect several hours of videos within a room over a time span of one month. Through these multi-view videos, a 4D reconstruction of the room is generated. A collection of rigid bodies is used to simulate agents (such as cats, dogs, etc.) in the room. Utilizing goal-conditional path generation technology, users can ultimately control the movement of these agents by setting goals.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The video presented in this paper is very effective; it reconstructs 4D video from a single view and reconstructs a complete room from multiple views.
2. In addition to reconstruction, the paper also discusses how to control the movement of the agent through goal-condition path generation.
3. Intuitively, I think this is a good paper and may inspire researchers in the field of 4D reconstruction.

Weaknesses:
1. While I am not an expert in 4D reconstruction, I find the presentation of this paper rather unclear, particularly the methodology section, which is extremely difficult to understand. My confusion began around lines 126-127. What are the color and feature descriptors of the video? I later noticed that ψ is described as the DINOv2 [40] feature of the input image. So, is ψ a feature of an image? How to obtain it? The paper should clarify this. Additionally, what is X, and is it a point cloud obtained from a mobile phone? If so, how does the point cloud acquire its color in Equation 2?

2. I suggest using a table to explain each symbol in detail. If the explanation of a symbol requires context from the paper, ensure it is as understandable as possible. For technical terms, provide detailed explanations within the paper. A comprehensive symbol table in the appendix would significantly enhance the paper's clarity.

3. The paper lacks detailed quantitative experiments to demonstrate the effectiveness of the method.

Limitations:
The authors adequately addressed the limitations and potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents Agent-to-Sim, an approach to learn a 3D agent in a 3D environment from casual videos of the same agent captured over a long horizon. ATS first conducts 4D spatio-temporal reconstruction from the set of videos, including a deformable agent, the background scene, and a moving observer. This is done with a coarse-to-fine video registration method. Then, given the 4D reconstruction, ATS learns a hierarchical diffusion model over the agent's goal, path, and pose trajectories.  The overall approach is tested on a dataset of iPhone videos for over several types of agents and motion patterns.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- I am not a subject matter expert in this field. However, the paper was clear and well-written such that even a non-expert like myself can understand the proposed high-level approach. The attached supplementary materials give a great visual overview of the paper.
- The paper outlines several limitations of the proposed approach and future directions to address them. The limitations are meaningful and help the reader better understand the problem setting, modelling assumptions, and future directions.
- The paper tackles a challenging problem on the path towards building scalable and realistic simulators.

Weaknesses:
- Certain technical details are not clear for readers unfamiliar with the related literature. This limits understanding and reproducibility. See questions.
- Evaluation of the method seems limited and is mostly limited to qualitative comparisons. I suppose this is inevitable given that ATS tackles a new problem setting than related work. However, it does limit the reader's ability to evaluate the significance of this methodology.
- For behavior generation evaluation, I don't understand why certain baselines were selected. In particular, FaF seems like a detection + multi-agent motion forecasting paper for self-driving, so it's not immediately clear how it can be adapted to this setting.

Limitations:
The authors have adequately addressed limitations and potential social impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a method for learning interactive behaviors of various agents, including humans, cats, dogs and a bunny, by leveraging unstructured videos captured casually. The various videos are registered together in a common frame, offering a 4D reconstruction of the agent and the environment. Based on this reconstruction, the multi-modal distribution describing different agent behaviors is learned by using diffusion models and Control UNets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper addresses the very challenging problems of learning agent behaviors from a collection of unstructured videos captured over different sessions. To learn interactive behaviors, both the trajectories of the agent and the surrounding environment need to be reconstructed, as to have relevant context of the behavior. Additionally, the motion of the camera/observer need to be reconstructed as well, to allow the registration of the videos in a common frame. As the videos are collected over a potentially large period of time, change in the environment can occur, complicating the tasks of registration and reconstruction.

The idea of using ego-perception encoding for the learning and generation of plausible interactive behaviors is another strong point. After the agent and the environment are reconstructed, ego-perception encoding is learning perception codes of the scene, the observer and past trajectory, factors that condition the generation of the agent's body motion.

Behavior generation considers the generation of the goal and the conditioned generation of the path, taking into account the goal.

Weaknesses:
There are numerous models employed in the proposed framework. Due to the limited space available, few details are provided about their motivation and their implementation. This makes both understanding of the work and its reproducibility very challenging. 

A particular aspect which is not addressed in detail is the modeling of the agents, especially of animals like cats that are quite challenging due to their non-rigid nature. In particular, it is not clear how eq.2 is combined with eq.3, and why the same number of ""bones"" (b=25, L.137) is used for all agents. Also, the nature of G^b is not discussed in detail. 

Additionally, details on how NeRF-type reconstructions are combined with feature descriptors, and how this helps in handling layout changes is not discussed in detail.

More examples like the previous can be given for different aspects covered in the paper, like camera localization (eq.6), scene alignment (eq.7) and behavior learning (eq.10 and 11). Each of these aspects would certainly require more space for describing in detail the corresponding models and support the relative claims in the experimental evaluation. 

Regarding experimental evaluation in particular, only high-level results regarding the agent behavior prediction are provided, while it would be crucial to quantitatively assess the quality of 4D reconstruction and, importantly, to include a detailed ablative study.

Overall, although some very interesting ideas are proposed in this work, both for 4D reconstruction of agent behaviors and behavior learning and generation, I think that the paper is too densely packed without having enough space to describe the paper contributions in sufficient detail. In my view, even describing in detail one of the 4D reconstruction or agent behavior modeling parts alone would be challenging in the space available. This affects also the experimental evaluation, as not all claims are supported by the results.

### Minor comments
- L.35: ""Such systems do not scale well""
- Figure 1, caption: incomplete sentence ""conditioned different observer trajectories""
- L.88: ""whiling accounts""
- L.113: what ""longitudinal videos"" are?
- Figure 3, caption: what does ""low latency"" means in this context?
- L.215: ""we collect the a""

Limitations:
Limitations of the work are discussed in the Appendix.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
fTOw3BzcWs;"REVIEW 
Summary:
The paper introduces ExID, an offline reinforcement learning algorithm that enhances learning performance in limited data scenarios by combining domain knowledge in the form of simple decision trees with agent experience replay data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Domain Knowledge Utilization: ExID incorporates domain knowledge to guide decision-making in data-limited scenarios
* Teacher-Student Architecture: A teacher network, informed by domain knowledge, regularizes a student critic network to improve generalization.
* Regularization with Domain Knowledge: The algorithm uses a regularization term to align the critic's decisions with the teacher's advice for states covered by domain knowledge.

Weaknesses:
* Discrete Action Space Limitation: The algorithm is currently limited to discrete action spaces, necessitating future extensions for continuous action domains.
* Hyperparameter Tuning Challenge: The need for precise hyperparameter tuning complicates the deployment of ExID in scenarios where extensive optimization is impractical.
* The paper does not have enough strong experiment comparisions. The methods of the paper is related with offline RL methods, such as SCQ[1], ReDS[2], A2PR[3], CPED[4]. But it lacks the experiments comparisions with offlien RL methods. I think adding some SOTA baseline methods will improve your paper. It is not required that experimental comparisons must be given, but at least add some discussion with these methods to the paper.  

References：

[1] Shimizu, Yutaka, et al. ""Strategically Conservative Q-Learning."" arXiv preprint arXiv:2406.04534 (2024).

[2] Singh, Anikait, et al. ""ReDS: offline reinforcement learning with heteroskedastic datasets via support constraints."" Proceedings of the 37th International Conference on Neural Information Processing Systems. 2023.

[3] Liu, Tenglong, et al. ""Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning."" In International Conference on Machine Learning (ICML). PMLR, 2024.

[4] Zhang, Jing, et al. ""Constrained policy optimization with explicit behavior density for offline reinforcement learning."" Advances in Neural Information Processing Systems. 2023

Limitations:
* The paper only conducts experiments in several simulated environments and a real-world sales promotion dataset, which may not fully verify the effectiveness and applicability of the algorithm in more diverse and complex real-world scenarios.
* The performance of the ExID algorithm heavily relies on the quality of the domain knowledge. If the domain knowledge is incomplete, inaccurate, or biased, it may mislead the learning process and result in suboptimal policies. Moreover, obtaining high-quality domain knowledge can be challenging and time-consuming in practice.
* The proposed method mainly concentrates on discrete action spaces, and its performance and applicability in continuous action spaces are not clear. This limits the algorithm's utility in many real-world control tasks that involve continuous action spaces.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies offline RL when data is limited. The authors propose a domain knowledge-based regularization technique to learn from an initial tracker network and limited data buffer. The experiments verified the effectiveness of the proposal, which outperforms the classic RL baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is simple and technically reasonable.
2. The experimental results on the real sales promotion dataset show the proposal is a promising solution in real-world applications.

Weaknesses:
1. The technical novelty is limited. Despite the claimed use of expert knowledge, the method adopted by the paper is to directly train a policy from the knowledge, which assumes that the information provided by the domain knowledge is at the state-action level (a decision tree in this paper), which limits the feasibility of this method. Compared to the use of knowledge between latent concepts discussed in neuro-symbolic learning, I think it's more like traditional model distillation. 
2. In practice, limited offline data may come from domain knowledge-based strategies, such as human-designed rules, thus I have great concerns about whether these two can promote each other. Empirical studies on more real-world datasets or rigorous theoretical analysis will provide support to this issue and further improve this work.  
3. The introduction uses the sales task as an example, but the visualization is based on the Mountain Car dataset.
4. Definition 4.1 seems strange, why not directly define the offline dataset as a subset of the complete state spaces?
5. The $\eta$ in Proposition 4.2 is not well defined.

Limitations:
The authors have provided a discussion about the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel technique ExID, a domain knowledge-based regularization method, that adaptively refines initial domain knowledge to boost performance of offline reinforcement learning (RL) in limited-data scenarios. The key insight is leveraging a teacher policy, trained with domain knowledge, to guide the learning process of the offline-optimized RL agent (student policy). This mitigates the issue of erroneous actions in sparse samples and unobserved states by having the domain knowledge-induced teacher network to cover them. And the initial domain knowledge would be improved when the student policy reaches a better perform than the teacher policy.  Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase compared to traditional offline RL algorithms operating on limited data

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Originality: The paper's originality lies in its integration of domain knowledge into offline RL through a teacher policy network. This approach addresses performance degradation in limited-data settings, which is a novel and underexplored area. The introduction of the domain knowledge-based regularization technique and adaptive refinement of initial domain knowledge are particularly innovative.

2. Quality: The quality of the work is evidenced by the solid theoretical analysis and the thorough empirical evaluations conducted on multiple standard datasets, including OpenAI Gym environments (Mountain Car, Cart-Pole, Lunar Lander) and MiniGrid environments, as well as a real-world sales promotion dataset. The results consistently show that ExID outperforms existing offline RL algorithms in these settings.

3. Clarity: The paper is well-structured, with clear explanations of the problem, methodology, and results. The use of diagrams and tables helps understand the motivation of the problem (figure 1), the proposed method (figure 2), illustrate the effectiveness of ExID (Table 1-2). Each section logically follows from the previous one, making the overall argument easy to follow.

4. Significance: By tackling the challenge of limited data in offline RL, the paper makes a significant contribution to the field. The proposed approach has practical implications for various real-world applications where data is scarce and expert knowledge is available, such as in business, healthcare, and robotics.

Weaknesses:
1. Generalization to Continuous Domains: The paper is limited to discrete action spaces, which restricts its applicability to a broader range of RL problems involving continuous action spaces. This limitation is acknowledged by the authors.


2. Scalability: The scalability of ExID to more complex environments that requires a complex representation (e.g., a significant large tree) of domain knowledge is not thoroughly explored.  It would be beneficial to understand how the method performs in such settings and what challenges might arise because the challenging of updating the domain knowledge represented in a complex representation could hinder the learning process of the student policy in the proposed method ExID.

Limitations:
The authors acknowledge several limitations of their work, including the reliance on the quality of domain knowledge and the focus on discrete action spaces. While these limitations are well-addressed in the paper, it may be worth to consider a broad evaluation:
   * Conducting experiments on a wider variety of environments that have larger state and action spaces, would provide a more comprehensive evaluation of the method's applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
egGFHCFPiU;"REVIEW 
Summary:
This work creates a hybrid LLM and classic planning algorithm, by integrating a LLM into the GraphPlan algorithm. The GraphPlan is an algorithm that solves a relaxed planning problem (forward expansion), and then traverses the created graph to find a valid plan (backtracking). Both steps are expensive. In the hybrid approach, a LLM is prompted in the forward expansion to limit the exploration of states deemed irrelevant. In the backtracking phase, the LLM is used to sort actions to explore first. Experiments with corrupted domain files show that LLMs can better handle corruption than the GraphPlan algorithm.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- A very interesting novel idea of a hybrid planning approach with a fundamental classic planning algorithm.
- The paper provides an introduction to an interesting research area of classical planning (e.g., Figure 4).

Weaknesses:
- Multiple missing experiments and discussions severely undermine the results of the paper.
    - It is not clearly motivated why experiments with corrupted pddl domain files are interesting. This was introduced quite suddenly in the *results section* (lines 261-262) without enough details and without providing motivation.
    - The paper is missing important discussion and experiments about the trade-off between the hybrid approach and the classic GP algorithm. Experiments with valid pddl domain files are not included, which could have alleviate it.  
    - The effect of hyperparameters on the results, such as the number of iterations (N) in Algorithm 1, is not discussed.
    - The failure of LLMs4PLAN-GPT3.5 compared to the phenomenal success of LLMs4Plan-GPT4 is somewhat unexpected and undermines the results of the paper.
- Multiple details are missing regarding the experimental setups. (see questions below)
- The paper's writing needs to be improved. (see suggestions below)

Limitations:
The authors did not discuss limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates how large language models (LLMs) can be integrated into established planning frameworks, specifically graph-based planning. The authors propose a novel framework called LLMs4Plan, which incorporates LLMs at two critical stages of the planning process: action selection during graph expansion and candidate action set generation during backtracking. The framework is tested across various planning domains, demonstrating improved efficiency and effectiveness in planning tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper's approach of embedding LLMs into graph-based planning is innovative and contributes to the field of automated planning.
2. The technical implementation of LLMs4Plan is well-detailed, with descriptions of how LLMs are utilized in action selection and candidate set generation.
3. The effectiveness of the proposed framework is empirically validated across ten planning domains, showcasing its practical applicability.

Weaknesses:
1. The proposed integration of LLMs into planning frameworks in LLMs4Plan may be complex and difficult to scale.
2. Comparisons with more recent LLM integrated planning baselines is limited.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
There have been debates about the fundamental planning abilities of LLMs in planning tasks. To achieve more reliable performance, several recent works have embedded an LLM into a search framework (e.g., MCTS, BFS) and viewed LLMs as heuristics. Along this line, this work take a closer look at the roles LLMs can play in Planning Graph. It considers two tasks for LLMs: pruning actions and sorting actions (as heuristics).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written, with precise language and formalism.
- The experiment is conducted on over 10 domains, making it quite comprehensive.

Weaknesses:
1. My biggest concern with this work is that it restricts the use of LLMs to specific roles within a classical planning algorithm. There are many other roles LLMs can play in planning. For instance, see the recent LLM-modulo framework below. Instead of just filtering and ranking actions, LLMs have also been used to evaluate state values or rank plans (i.e., action sequences rather than individual actions).

    - Kambhampati, Subbarao, et al. ""Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks."" ICML 2024

2. The evaluation based on the number of nodes explored is partial. We should not ignore the time cost (e.g., latency of calling LLMs) + financial cost of using commercial LLMs. It could be very likely that, although LLM+Graph Planning expands fewer nodes, it may take a longer wall-clock time to give the final outputs. I understand that the evaluation could be tricky and it remains an open question for a while. However, the authors should at least make an attempt to address this.

3. In the abstract, this statement is inaccurate: “works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs.” There have been quite some paper embedding LLMs in off-the-shelf planning algos

    - Zhao, Zirui, Wee Sun Lee, and David Hsu. ""Large language models as commonsense knowledge for large-scale task planning."" NeurIPS 2023.
    - Yao, Shunyu, et al. ""Tree of thoughts: Deliberate problem solving with large language models."" NeurIPS 2023.


4. While the corrupted domain model experiment looks interesting, it is unclear what messages it tries to convey. Specifically, why would one run the algo on top of a corrupted domain model when there exists approaches that can leverage LLMs to help complete the domain model before starting the search?

    - Guan, Lin, et al. ""Leveraging pre-trained large language models to construct and utilize world models for model-based task planning."" NeurIPS 2023
    - Wong, Lionel, et al. ""Learning adaptive planning representations with natural language guidance."" ICLR 2024.


5. The step of LLM-based action pruning can make the search incomplete, since an LLM may keep ignoring the required action(s) -- in other word, there is no guarantee that the LLM can produce a goal-reaching plan. I notice the authors mention this at a later section (which should be moved to earlier part) that including pruning probabilities could address the problem. I don’t fully agree with this. Can the authors give more detail on how pruning probabilities could guarantee completeness?

6. In the prompt (fig. 3), only the proposition set at the current state is provided. Did the authors consider including the running history of actions (i.e., the partial plan)? Would this affect the overall performance?

7. Line 109: typo in “Algorithm ??”

8. Several works (mentioned earlier) already show that LLMs can be useful heuristics. Can the authors elaborate on the new insights this work provides?

-----
Overall, this study provides a thorough evaluation of LLMs within the Planning Graph algorithm. I appreciate the comprehensiveness of the experiments. However, I also have concerns over the scope of this study (i.e., restricting itself to a limited set of roles). I need to discuss with other reviewers and the authors before finalizing my recommendation.

Limitations:
See the Weakness section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to investigate integrating large language models (LLMs) into classical planning frameworks to enhance the planning effectiveness. The authors proposed a novel method named LLMs4Plan which integrates LLMs into action selection and mutual constraints solving within the graph-based planning framework. Evaluated across ten classic planning problems, this approach demonstrates improved success rates and reduced computational complexity compared to traditional methods. The study concludes that while LLMs alone are insufficient for planning, their integration into classical frameworks significantly boosts performance,.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper investigates an intriguing topic: the performance of LLMs in classical planning problems. While the impressive performance of LLMs in natural language processing and coding tasks is well-investigated, their efficacy in planning tasks remains largely unexplored. Understanding whether LLMs can replace classical planning algorithms is a significant and meaningful research question.
2. The paper conducts extensive experiments on ten classical planning problems, which enhances the credibility of its findings and conclusions. This comprehensive evaluation demonstrates the robustness of the proposed approach.
3. The paper reveals that LLMs still cannot surpass classical planning algorithms, thereby highlighting a valuable direction for future research. This insight encourages further investigation into how LLMs can be effectively integrated with traditional planning methods.

Weaknesses:
1. Although the authors point out that LLMs cannot outperform classical planning algorithms on their own and need to be integrated with classical methods to perform well, the paper lacks detailed insights on this integration. For example, specific strategies for integrating LLMs with the classic planning algorithms and the roles where LLMs excel within planning problems are not thoroughly discussed. The designed ""expandGraph"" and ""sortActions"" may not be the best practice manner. Future research directions to enhance the planning capabilities of LLMs should be more explicitly outlined.
2. The experiments are conducted in simulated planning domains, and the paper does not provide real-world applications or case studies to validate the practical utility of the approach. Including experimental results from more realistic scenarios would strengthen the paper.
3. While the method is effective for graph-based planning, its applicability to other planning frameworks or domains is not thoroughly investigated. A broader analysis could reveal the versatility of the proposed approach.
4. Typos: Algorithm ?? in Line 109.

Limitations:
See the Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
eYUbBKxABP;"REVIEW 
Summary:
The paper presents a formalization of fairness metrics intended to ease analysis of discrimination by automated decision making systems in the UK. While there is a relatively applied angle, the bulk of the contribution is intended to be a generic and re-targetable mathematical formalism.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper shows significant strength in its understanding of nuance with the way law works–something that is sorely missing from the vast majority of CS papers that attempt to handle legal concepts. I was very pleased overall by the mapping the authors performed between relevant legal concepts in the UK and their formal model of fairness. The bulk of the contribution here is in the modelling–which while it results in a simple formulation, should not be taken to undercut the value of the contribution.

Non-US legal contexts often get left out of the literature, even common law jurisdictions–yet they impact a significant number of people, and this work takes formalising fairness across that rubicon.

Weaknesses:
I do not have any major scientific critiques, though there were areas where the clarity of the paper could improve.

Lines 240-274 were written in harder to parse prose than the bulk of the rest of the paper. I had to reread that area multiple times.

The case study in Appendix A was actually very useful for understanding the authors' formalism and it is a shape that some of that context was not woven into the paper as concrete examples of how to understand the math.

The discussion on proxy discrimination never seemed to finish? I wasn't able to understand its meaning under UK law.


Missing a ref to Homer on L299.

All these are very minor issues. I'm substantially in favour of accepting this paper.

Limitations:
Ultimately, adherence to a formalism is *not* what courts generally take into account. While statistical analyses may be used to advance a given line of argument, the standards used are open-textured–and this is an inherent limitation of this line of work.
It also would have been good to see where this formalism sits under EU law (or representative EU-member law) or perhaps a discussion of how civil law jurisdictions handle these sorts of issues.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper maps existing literature and law on algorithmic fairness onto a decision-theoretic framework. It describes various desiderata (e.g. statistical parity) and legal restrictions (e.g., legitimate aims) in terms of expectations, distributions, estimation error, etc.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written and survey a large literature. It appears to state legal tests (particularly under U.K.) with care, while being careful not to overclaim about what its definitions actually establish.

Weaknesses:
n/a

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
- There is a gap between the definitions of fairness studied in the computer science literature, and the definitions of fairness operationalized by courts adjudicating discrimination claims. This limits the usefulness of the CS definitions.
- Amongst work attempting to reconcile legal and computational definitions of fairness, little has focused on anti-discrimination law outside the US.
- This paper makes four contributions in this context:
    - (1) It formalizes elements of anti-discrimination law into a decision-theoretic formalism
    - (2) If analyzes the legal role of the data-generation process
    - (3) It proposes conditional estimation parity as a legally-informed target
    - (4) It provides recommendations on creating SML models that minimize the risk of unlawful discrimination in automated decision-making

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The paper’s focus is interesting–the fairness literature is biased towards the US, and I imagine most fairness researchers would be unaware of subtle differences between UK and US anti-discrimination law.
- Because UK law is influential around the world, understanding how it regulates fairness in algorithmic systems has global importance.

Weaknesses:
- Much of the paper reads like a review of anti-discrimination law. This makes it difficult to parse out (1) what the technical contributions are, (2) why they’re novel, and (3) why they matter. 
- It’s extremely unclear what the technical payoff of the paper’s modeling choices are. The fairness field is overwhelmed with different definitions/frameworks. Why is the one proposed by the author’s meaningful over others? 
- It seems like an essential point to the paper’s argument is that prior work hasn’t studied UK anti-discrimination law. But if the paper wants to successfully extend that into an argument about modeling choices, I think it needs to explain why the existing definitions of fairness do not work for UK law.
- The recommendations provided are extremely general. Are these new or different from the many recommendations that already exist in the fairness/responsible AI literature?

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issues around existing fairness metrics and bias detection/mitigation methods not corresponding with legal notions of fairness, specifically under UK anti-discrimination law. The authors propose a theoretical framework for a data-generating process that aims to formalise the legitimacy of decisions and features in the data. Further, they propose a new metric ""conditional estimation parity"" which compares estimation errors for different protected groups.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well written and coherent. It translates potentially inaccessible legal scholarship and discussions clearly for a technical audience.
2. There is interesting discussion and the paper combines existing literature well. Although these discussions are not particularly novel, UK Equality Law in particular is rarely discussed and the investigations done here are useful to extend the literature for this niche.
3. The work addresses some big limitations in existing literature such as existing fairness metrics not aligning with legal notions of discrimination, particularly under non-US regulations, not considering context of what features are legitimate for an application or considering the estimation errors of decisions.

Weaknesses:
1. A lot of the paper is background or a collation of existing literature. The main contribution is the new conditional estimation metric metric but this metric relies on the true DGP and evaluating the estimation error which, as stated, can be complex in practice. This could make it difficult to use the metric in practice.
2. I understand it would be hard to use the metric for evaluating discrimination in existing datasets for the reasons specified above and also due to the inherent context-dependency of the metric (which is a benefit) but it could be useful to include some experimentation or results in a hypothetical scenario to show how it might be used in practice. As there are no results as such to comment on, it is difficult to assess it's significance.
3. The conclusions drawn such as ""Assess data legitimacy"" or ""Build an accurate model"", although justified with evidence in the paper, are not novel and are pretty standard, common-sense recommendations. 
4. Overall, the main novel contribution is the new metric but this is a small part of the paper. The rest of the paper is a nice collation and narrative of existing literature but I am not sure it significantly advances the field.

Other comments:
1. I can't see where SML terminology is introduced - I assume this means supervised machine learning?
2. In Section 1.4, DGPs are mentioned for the first time. It would be useful to have some more background to them before this - what exactly is a DGP? I do not believe it is ever explained.

Limitations:
The authors are honest about the strengths and weaknesses of their work (although some are hidden away and not pointed towards in the checklist). It would be useful to improve the discussion of limitations in Section 1.4 as it only mentions the limitation of applicability only in the UK.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a UK-and-European-law-based view of anti-discrimination law as it relates to fair machine learning and automated decision systems. It does a good job laying out the doctrine, arguing correctly that work in this area to-date is very centered on US legal concepts such as disparate treatment vs. disparate impact. Although I am willing to believe that there are subtle differences that drive important aspects of fair ML analysis, as the paper claims, I think the specifics of these differences could be made much clearer and need to be for the paper to have the impact it should.

Of particular note, the paper is very well situated in the surrounding literature. Although this contextualization should make the contributions more clearly offset from prior work, as presented I find the opposite: it is difficult to tell what is new as a contribution here. For example, while the contributions are clearly identified in 1.4, I think it would aid the paper if they appeared higher in the intro and were clearer about what is new and why it matters. The example in Appendix A could be used as a running example to show where new concepts are needed and what about existing work does not capture this different legal regime. In particular, after claiming that disparate treatment/disparate impact are distinct to direct & indirect discrimination, the definitions given from 105-114 seem to align tightly to the former. And while I'm not a lawyer, I don't believe that disparate impact claims require a showing of intent under US law either, so I found that distinction somewhat confusing.

On the technical level, the discussion of the true data generating process should really be contextualized in the literature on measurement and construct validity, specifically with respect to work by Jacobs & Wallach, which in particular encompasses the material in 2.3 on estimation parity (at least in part). Also, the causal analysis components of the discussion of data generation could cite more of the work of Kohler-Hausmann and also Hu (one paper from these authors is cited, but others are also relevant and speak more directly to causality and counterfactual fairness claims).

As a final observation, although the ML community talks in terms of ""fair"" outcomes, it is often conceptually clearer (and more in line with legal analysis) to use the same techniques as tools for identifying ""unfair"" activities or outcomes. Phrasing some of the claims this way may condense some arguments and tighten the presentation overall. Related to this, the discussion of these tools as part of an overall practical strategy for risk management is important and should receive more attention. For example, it would be good to discuss how the measures proposed would be used in real legal analysis of an example, such as in litigation or a regulatory proceeding.

I was also a bit confused about the analysis of constructed proxies for protected variables in 2.7. I understand that it's necessary to look beyond a formalistic view of whether a specific attribute is considered, but what happens if the proxy for a protected attribute is (say) the sum of two legitimate attributes? Why is it good enough to use only legitimate features? Also, at 393-394 it might be valuable to look at the recent paper on ""Less Discriminatory Algorithms"" and compare the approaches and outlooks.

Incredibly minor: 
* There is a missing period at 81.
* At 284-288, there is a latent call to questions of ecological validity which could be made more explicit

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* Generalizing beyond the US legal context is important and valuable and this paper does a good job explaining the UK and related legal systems' approach to anti-discrimination law.
* The paper is well written and well situated in existing literature

Weaknesses:
* Novelty is at times hard to identify. I think it's there, but the claims on what it covers should be clearer. In particular, the discussion of the decision-theoretic framing seems a bit under-attended even though it's potentially very useful.
* Some important concepts are missed, notably theories of measurement and construct validity/reliability are at least partially re-invented when they should just be treated as background.

Limitations:
I believe the limitations are expressed well.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
eM3Wzs6Unt;"REVIEW 
Summary:
The paper presents an off-policy hierarchical RL method, based on the HiT-MDP formulation of a Semi-MDP. The HiT-MDP formulation treats the option $o$ as an extension of the original state $s$ (which can be chosen by an extended action), and combines initialization-, termination- and option-policy in a single Markovian master policy $p(o\_{t}|s\_t,o\_{t-1})$. The policy in the extended state-action space, thus, decomposes into the high-level and low-level policies, $p(o\_{t}, a\_t | s\_t, o\_{t-1}) = p(o\_{t} | s\_t, o\_{t-1}) p(a\_t | s\_t, o\_{t})$, which can be trained using standard RL algorihms. 
Compared to the prior work, the paper makes the following contributions:
- Whereas previously PPO was used for reinforcement learning, the paper proposes to use SAC, resulting in improved sample efficiency
- The paper motivates the algorithm from a control-as-inference perspective

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The proposed method seems to be technically sound, and using off-policy agents for HiT-MDPs seems sensible. (Quality)

The provided code clarifies the implementation which helps reproducibility. (Quality)

The presentation is mostly clear. (Clarity)

Applying an off-policy agents to HiT-MDPs seems to be novel and effective (Origingality, Significance)

Weaknesses:
Originality
-----------
One of the main weaknesses of the submission is the limited novelty. Replacing the PPO agent of MOPG by a SAC agent seems to be straightforward, so this contribution is quite incremental. Indeed, the authors of HiT-MDP stated, that their ELBO ""can easily be extended to a SAC-like algorithm"" [35]. Furthermore, given that MaxEnt-RL was already derived from a control-as-inference perspective, deriving the special case of an HiT-MDP using this technique does not seem to be significant contribution either. I also don't see the value of this derivation that would justify devoting so much space on it; couldn't we just argue that we apply SAC to such particular form of an MDP?


Quality
---------
The experimental evaluation seems to be another weakness of the submission. While the method is evaluated on a reasonable number of MuJoCo environments, where it outperforms a reasonable number of baselines, the choice of baselines is not convincing because it looks like the method is only compared to on-policy algorithm. The submission claims that there method ""significantly outperforms existing on-policy and off-policy option variants"", but it is not clear to me to which off-policy baselines this claim refers to. It would be important to focus to flat and hierarchical off-policy methods in the experiments, such as [19], [50], [33] and Hao et. al (2023).  Furthermore, the choice of environments is not convincing, because it does not include more challenging long-horizon tasks that are typically used for evaluating HRL methods, such as Ant-Maze. While the performance on the standard locomotion environments is reasonable, the reported numbers don't seem to improve on the SOTA of flat-RL methods.   

The paper does not discuss the hyperparameter search although it states in the questionnary  that these details are provided in the main content and the appendix.

The paper argues that it did not perform any ablations due to limited computational resources. However, I don't find this argument very convincing, since the experiments are performed on simple vision-free locomotion tasks, that can be run on standard workstation, not even requiring any GPU. Ablations on the number of options would be very useful.



Clarity
---------
I found the background material on control-as-inference a bit confusing. In particular, line 106 which states states policy improvement constitues an M-Step of an EM algorithm that *maximizes* the KL towards $P(\tau|\mathcal{E})$. I don't think any practical algorithm involves such maximization, since the optimum would correspond to a delta distribution on the least-likely trajectory. (

Visually, the presentation is rather bad. Figures are not on the top, and in particular Fig. 2 seems to hide some text, since the sentence in line 271 ends with "", which"". Fig. 2 itself could be improved by increasing the plot sizes (there are some unnecessary white spaces) and by making the legend more readable.  


Significance
-----------------
While I think that the proposed combination of the HiT-MDP formulation and SAC is somewhat interesting, the submission does not provide a convincing argument for the method. When should I use it, instead of existing (hierarchical or flat) methods?

References
----------
* Hao, C., Weaver, C., Tang, C., Kawamoto, K., Tomizuka, M., & Zhan, W. (2023). Skill-critic: Refining learned skills for reinforcement learning. arXiv preprint arXiv:2306.08388.

Limitations:
The limitations are adequately discussed and I don't have any concerns regarding negative societal impact of the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Variational Markovian Option Critic (VMOC), an off-policy algorithm for hierarchical reinforcement learning. VMOC aims to address exploration inefficiency and update instability in existing methods. Key contributions include: 1. Use of variational inference for update stabilization 2. Low-cost option embeddings for improved scalability. The authors evaluate VMOC on Mujoco environments, comparing it to other on-policy and off-policy methods. They report improved performance in learning option sets for complex tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The paper is well-written, and the proposed method is theoretically justified.
2. The empirical evaluations show favorable results compared with existing methods.

Weaknesses:
1. Very similar ideas of the variational option framework have been proposed in [33] (off-policy) and [35] (on-policy). While [35] proposes an on-policy version, its off-policy version is also straightforward to deduce following [ref1]. The use of option embeddings is following [35].
2. The empirical evaluations are very limited; there is no ablative evaluation reported, which makes it hard to determine the contribution of the proposed method to the overall performance gain over various baselines.

References: 
[ref1] Levine, Sergey. ""Reinforcement learning and control as probabilistic inference: Tutorial and review."" arXiv preprint arXiv:1805.00909 (2018).

Limitations:
The empirical evaluations, especially ablation studies, are somewhat limited in scope.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the Variational Markovian Option Critic (VMOC) which combines variation policy iteration and the option critic. VMOC also modifies HiT-MDPs, where options are represented as latent embeddings rather than triples of (init states, policy, termination condition), to the off-policy setting. The paper performs comparisons to option-based methods and PPO on 10 Mujoco environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy-to-read. The figures clearly highlight the performance of the method. The translation from theory to the practical algorithm is well detailed.

2. The advantage in sample-efficiency over other option methods and PPO is clearly seen in Figure 2 across Mujoco environments. In fact, this gain looks to be in atleast two orders-of-magnitude (of fewer steps required by VMOC) which is amazing. The underlying MaxEnt objective in VMOC appears to be very useful with exploration in the high-dim mujoco envs.

Weaknesses:
1. It is not clear if this gain in sample-efficiency will transfer to discrete environments or is somehow applicable only in continuous envs. Perhaps the authors can perform comparisons on Atari or Procgen to demonstrate the same? It would be great if the authors could also discuss the changes in the algorithm in the discrete and continuous settings (perhaps such as the sampling of a_t from the replay buffer?)

2. It is unclear if all methods use the same number of options (e.g. the value used in VMOC appears to be 4). A clear ablation of various design choices like number of options would help demonstrate that VMOC is thoroughly better than the other option methods and is not brittle to hyperparameter choice. 

The analysis of the actual options learnt is also missing (this is for example seen in the option critic paper). This, alongside an analysis of the number of options, is crucial to understand if the method is actual learning composed actions that are further composable and generalizable or degenerating to something simple like learning the action primitives (although the latter would apply more to a discrete rather than continuous env).

3. Minor comment: The location of Theorem 1 in the preliminaries makes it unclear if it is a contribution of the authors or well-known statement. Perhaps the authors can clarify?

4. Another minor comment: It would be great if the authors could discuss other ways of combining options in the related work such as in [1] and [2].

[1] The Option Keyboard: Combining Skills in Reinforcement Learning, Barreto et al, NeurIPS 2019

[2] Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates, Dutta et al, CoRL 2022

Limitations:
The authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces e Variational Markovian Option Critic (VMOC), which learns actions and options simultatenously. They build upon the Hidden Temporal Markovian Decision Process (HiT-MDP) [1] to build a novel off-policy algorithm that utilizes entropy augmented rewards. Their method learns options’ embedding vectors (rather than conventional option tuples utilized in Semi-MDP [2]). They benchmark the learning performance of their method against several competitors on many classic control benchmark environments. 

*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.
2. Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2), 181-211.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- Extensive comparison against 8 competitor algorithms on 10 benchmark tasks
- A novel Soft Option Policy Iteration Theorem

Weaknesses:
The paper does offer a potentially interesting contribution to the wider research community. But it is held back by the lack of clarity and polish in writing. For example, two glaring signs of a hasty submission:
1. Sec 4 and Sec 5 are both titled experiments. Sec 4 is only 1 paragraph, and it essentially repeats the same information from the introductory paragraphs of Sec 5
2. In Sec. 5, line 271 just trails off without completion. I believe the authors moved around the images to correct for vertical space and accidentally hid the text.

While the experimental results focus on learning curves, where VMOC does well, they fail to provide other relevant evaluation metrics:
1. What do the learned options look like? A good evaluation could follow Fig. 5 and Fig. 6 from [1]
2. How many options are learned? Digging through the appendix, it says that they learned 4 option vectors. This leads to another question: how do they choose the number of options to learn?
3. The VMOC algorithm listed in the appendix only describes the gradient update process. No details about action sampling or other hyper-parameter tuning are described here
4. The environments used are challenging for model-free RL algorithms. That said, they may not be satisfactory for showcasing the potential of learned options. 


*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.

Limitations:
Much like Soft Actor-Critic, this work develops a novel Soft Option Critic style algorithm. I believe this line of work is very interesting and potentially impactful in the near future. However, their current draft is not well-written and hard to follow. Their experimental evaluation is also insufficient.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
UqvAFl0lkT;"REVIEW 
Summary:
This paper investigates using an emergent communication protocol as a auxiliary reward in navigation reinforcement learning setting, particularly those where exploration is a difficult (i.e., sparse reward settings).  The experiments show that certain emergent communication games can be effective in solving the RL problem.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
In conjunction with standard criteria, there are three characteristics that are particularly important for emergent communication research: reusability (how easily can another researcher use the products of this research), generalizability (how much do the findings of this research apply broadly to our knowledge of emergent communication), and directedness (does this research contribute concretely to particular questions in emergent communication research).

### Quality
- (minor) The experiment demonstrate that some of proposed approaches beat the baseline.
### Clarity
- Nothing of note.
### Reusability
- Nothing of note.
### Generalizability
- Nothing of note.
### Directedness
- (minor) Comparing emergent and natural language's utility for reinforcement learning is an important problem in emergent communication research.
- (minor) If emergent communication protocols are effective abstract representations of environment states, this could be useful to RL more generally.

Weaknesses:
### Quality
- See Clarity.
- (major) Is there a ""competitive baseline"" tested in this experiments; that is, the state-of-the-art, no-frills method that the proposed solutions would be competing against in the real world?  If there is, the comparison needs to be a clearer as to what exactly the advantage of using EReLELA is.
- (minor) The natural language baseline does not seem to actually be ""natural""; synthetic seems more accurate.  If the language procedurally generated, I do not think it can be considered natural.
### Clarity
- (major) I found this paper (esp. Section 3) very difficult to understand, even after rereading certain sections.  Overall, I do not have a concrete idea of what EReLELA is or why it is important.  For example, I understand that emergent communication protocol is supposed to abstracting observations and that the referential game is used as an ""Intrinsic Reward Generator"", but I do not understand how this is incorporated into the RL algorithm.  Furthermore, how is the referential game distinguished from more straightforward ways of generating auxiliary rewards?
### Reusability
- See Clarity.
### Generalizability
- (minor) How would the core findings of EReLELA apply more generally to other RL or emergent communication settings?
### Directedness
- Nothing of note.

Limitations:
N/A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work presents the idea of employing emergent languages (EL) abstractions combined with count-based approaches for exploration, to improve exploration in sparse reward reinforcement learning (RL) settings.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Overall, the major strength of this paper comes from its **novelty**. 

* The idea of emergent languages in RL has been scarcely explored and the idea presented is sensible and interesting.
* The Compactness Ambiguity Metric (CAM) definition is useful to compare how emergent languages compare to natural languages.
* The results presented in simple Minigrid environments show that the method has the potential to improve exploration capabilities.

Weaknesses:
The paper presents some weaknesses that make my opinion lean towards rejection.
* **Presentation**: from an aesthetic point of view there are things make the paper hard to read, such as the use of bright colours for text on a white background (see Experiments section) or wrapped Figures and equations that are too close to the main text (see captions of Fig.1 and 2)
* **Clarity**: some of the explanations provided are not completely clear. For instance, I struggle to understand what is happening in Figure 1 and the caption of the Figure (which is a long description with no sentence breaks) does not clarify enough. Similarly, for Figure 2, it is not clear what is happening, e.g. why some events are above or below the black line, and the Figure is not clearly explained in the text or caption.
* **Insights on the learned representation**: given that the use of emergent language abstractions is the main contribution, it would have been useful to get more insights into the representation being learned by the agent. While the authors present quantitative results in terms of performance or distance from natural language, it is not clear what are the properties of the emerging language, e.g. sentence length, number of unique utterances, etc. Assuming the RL community is one of the targeted audiences for the paper, it would be crucial to present some insights into this to ensure the contribution is clear.
* **Limited evaluation**: the evaluation is extensive in terms of ablations (also to be found in Appendix) but is quite limited in terms of environments and baselines tested. Differences from other related works, e.g. reference [51], should be at least described more in details, if running additional baselines is not feasible

Limitations:
Some limitations are presented at the end of the experiments section

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to leverage the Emergence Communication paradigm via the use of referential games to learn state abstractions for a Reinforcement Learning domain. The authors claim that using this approach, their proposed method is able to learn abstractions that boost exploration for an RL agent, and leads to performance that is comparable to Natural Language-based state abstractions.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1) The paper strongly motivates the problem highlighting the need of state-based abstractions for RL agents, the advantages and limitations of Natural Language-based abstractions, and how Emergent Language-based abstractions can avoid those limitations while achieving comparable results.
2) In my honest opinion, this paper greatly stands out for explaining the relevant literature and how this paper situates itself within the existing works. I thoroughly enjoyed reading the Introduction, and Section 2.1even more so! 
3) The method is well-explained, and experimental setup flows very logically making it intuitive to the readers the insights presented by the paper and the questions that arise.

Weaknesses:
Please see the questions section for more details.

Limitations:
The authors have discussed important limitations of the work, and also a well thought-out section on the broader impact of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
EReLELA investigates whether by asking the agent to learn and describe the environment through emergent language (EL) can help with hard exploration tasks, compared to using natural language (NL) description alone.

I personally find this angle interesting and refreshing -- and the connection to count-based exploration bonus is novel.

In theory, EL should work better than NL because by definition of pragmatics, through reference game (RG), the description from EL should be more compact and discriminative than NL. However, I appreciate the honesty of the authors that they point out there was no significant difference between EL and NL.

I thought back and forth about whether to accept or reject this paper. My current stance is that -- if the authors cannot substantially rewrite the experiment section and update their figures to make their conclusions very easy to understand, I don't think this paper meets the bar of acceptance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The direction is novel. The idea and execution are both solid. Ablations are great.
2. The first 4 pages of writing (intro, related work, background) are clear.
3. The experimental hypotheses are very clear (H1/H2/H3) and reasonable
4. Evaluation environments (KeyCorridor of MiniGrid) make sense and is commonly used.

Weaknesses:
1. I can only vaguely understand CAM. The way it's being described is still very confusing to me, and I already have some background on speaker-listener models. I recommend the authors considering rewriting with general audience in mind -- maybe present an algorithm box that shows how it's computed? Currently this section is interleaved with intuitions and actual procedure. Maybe separate them to some extent? (I see Appendix F/G is about agent architecture and RG. Would you guys consider condense the paper and move these two sections into main text?)
2. Sec 3.2 is very brief.
3. Experiment figures are labeled in a way that is beyond confusing. I would urge the authors to not use `Agnostic STGS-Lazlmpa-5-1 ELA+AccThresh=90+Distr=256+UnifDSS` as labels in their figure. Such label is fine for internal presentations/reports, but it is difficult for reviewers to quickly understand what the figure is saying.

My main concern of this paper is not about the content nor the experiment, just about the presentation.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
dQmEIwRw16;"REVIEW 
Summary:
This paper studies the loss function for soft class labels and entropy-based clustering. In particular, it introduces a new loss function called 'collision cross-entropy' as an alternative to Shannon's cross-entropy when class labels are represented by soft categorical distributions. The motivation for this new loss function is to handle ambiguous targets/labels in classification. The authors provide an EM algorithm for pseudo-label estimation and conduct experiments to demonstrate that this approach leads to improvements in classification accuracy when models are trained with soft, uncertain targets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The proposed collision cross-entropy may have advantages over Shannon's cross-entropy when handling soft labels in certain scenarios.
- Through experiments, the authors demonstrate that the proposed method achieves better robustness to label uncertainty, which is important for self-labeled clustering methods.

Weaknesses:
- [**Theory-1**] The main contribution of this paper is proposing the new loss function 'collision cross-entropy'. However, there is not much theoretical analysis about this loss function. From the current paper presentation, the Eqn. (9) can be interpreted as a modified version (or inspired by) Eqn. (6). For example, by minimizing the new objective for learning linear models, could this new loss lead to the right linear classification model?

- [**Theory-2**] For the EM algorithm, is there any convergence analysis for the EM algorithm proposed in this paper? 

- [**Experiments**] State-of-the-art for comparison. The methods for comparison in Table 1/2/3 are not very recent. It is possible that the previous methods still work well and be the state-of-the-art. However, I found some recent papers could achieve much better results, for example, the ACC on CIFAR10 of [DTC+2023] is 92%+, however the result in this paper is <84%.


[DTC+2023] Unsupervised Manifold Linearizing and Clustering. Tianjiao Ding, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, Benjamin D. Haeffele. ICCV 2023.

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Soft labels are often used to represent ambiguous/noisy/uncertain targets in classification, particularly in self-labelled clustering, where pseudo-labels are estimated together with model parameters.
The authors propose an alternative to Shannon cross-entropy for a loss term, called the collision probability. 
This term arises as a limiting case of a Renyi entropy, or as a probability that two random variables are equal.
The collision cross entropy admits several advantageous properties: it is robust to large deviations in the target data, it agrees with Shannon cross-entropy for one-hot labels, it is symmetric, and points that are labelled as uniform distribution have no contribution to training.
The authors provide an EM algorithm for pseudo-label estimation and show state of the art results.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- The paper is **very well written**, providing strong intuition and flowing prose. The intuition in Figure 1 is helpful, especially in showing that the proposed measure is robust to large target errors. 
- The main technical element appears to be an EM algorithm for solving the clustering problem obtained by using a collision cross-entropy in place of the Shannon entropy (which a swap in the arguments between equations (10) and (11)). This algorithm appears to be **technically sound**, and guarantees convergence of the subproblem in the M step. 
- The proposed term has **several nice properties** (as I mentioned earlier). Tt is robust to large deviations in the target data, it agrees with Shannon cross-entropy for one-hot labels, it is symmetric, and points that are labelled as uniform distribution have no contribution to training.

Weaknesses:
- My main concern is that **conceptually, the contributions are rather limited**. Generalisations of entropy are well-known, and as far as I understand (correct me if I am wrong), the main contribution is that authors use a different measure of entropy to Shannon entropy inside existing formulations. This leads the authors to investigate EM-algorithm and empirical performances, but as the paper is currently written (see further comments below), I cannot see whether these EM-algorithm and empirical performance benefits are actually real and beneficial. I also do not understand why this particular notion of entropy was used, compared with the other spectra of entropies. 
- An incomplete review of relevant generalized formulations of entropy is provided. This is not a weakness per se, however **perhaps the title in section 2.2 could be changed to something like Renyi Entropy**. Similarly tone down the discussion of generalised entropy measures throughout the paper. Alternatively, the authors might consider expanding their discussion and including more well-known entropy measures. For example, see section 8 and 11 of [1].
- The bold numbers in table 2 require clarification. The caption doesn't mention the number of trials (however the text mentions 6 trials). Compared with MIGD, excluding MNIST, due to the high variance in the trials, the results do not appear to be **statistically significant**. Perhaps the authors could consider running more trials and performing a significance test, and/or also bold relevant entries in MIGD. 
- As above for Table 3, 4 and 5. 
- It is **not clear how long the method takes to run** compared with competitors. Does the EM algorithm outperform a naive marginalisation of the log likelihood (using e.g. MC), both in terms of time and in terms of predictive performance? 
- Related to the above, is the reason for specialising on $\alpha \to 1$ because it allows for the EM algorithm? If you consider other values of $\alpha$, how do the results compare in terms of time and performance. Or is this setting intractable?


[1] Generalized Thermostatistics, Jan Naudts, 2011.


Minor:
- The text in the tables is too small to read without zooming in a lot.
- Recommend less active tense in the abstract: ""In case of soft labels y, Shannon’s CE teaches the model predictions σ to reproduce the uncertainty in each training example"" could be ""In case of soft labels y, Shannon’s CE results in model predictions σ which reproduce the uncertainty in each training example"".

Limitations:
The author checklist appears to be incomplete. The authors answer NA to ""Does the paper discuss the limitations of the work performed by the authors?"", without a justification. I do see a small discussion around local minima and numerical instability towards the end of section 4, but I think these could be further elaborated on.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the concept of collision cross-entropy (CCE) as an alternative to Shannon's cross-entropy (SCE) for self-labeling in the context of unsupervised and semi-supervised learning. The primary motivation is to address the limitations of SCE, especially its sensitivity to label noise and uncertainty. CCE aims to enhance robustness to such uncertainties by defining a probabilistic interpretation that encourages collision events between predicted and true distributions. The paper provides theoretical foundations, describes an EM algorithm for efficient optimization, and presents experimental results demonstrating the superior performance of CCE over SCE on the task of deep clustering.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality
-The paper introduces a novel loss function, the collision cross-entropy, which is well-motivated by the need to handle soft and uncertain labels in classification tasks, particularly in self-labeled clustering. The idea of maximizing the collision probability is distinct from the traditional approach of minimizing the (implicit) KL divergence between distributions.

Quality
-The paper provides a solid theoretical foundation for the collision cross-entropy, including its properties and relationship to other entropy measures. The derivation of an efficient EM algorithm for pseudo-label estimation further strengthens the paper's technical contribution.

Clarity 
- The paper is generally well-written and organized. The motivation, theoretical analysis, and experimental results are presented clearly. The authors provide sufficient details for an expert reader to understand and potentially reproduce the work.

Significance
- The proposed collision cross-entropy has the potential to be a valuable tool for handling soft and uncertain labels in various machine learning tasks.

Weaknesses:
Quality
- The superiority of CCE seems to hinge on making the model capture the same ""decisions"" as the target distribution, without forcing the model to capture the entirety of the distribution, as well as de-weighting target distributions which are not spiky. While the properties of the loss are clear, it is not self-evident to me that the properties *of the loss function* translate into necessarily *better properties for models*, both as a function for training a classification model directly or for clustering. 
- In addition, the experiments were conducted on fairly old architectures (VGG, ResNet) and small datasets. Often improvements on small datasets do not translate into improvements on larger-scale models. I would encourage the authors to examine for full imagenet dataset at the very least. This also open up the capability to look at various robustness / calibration properties of the models on the various corrupted forms of ImageNet. 

Clarity
- Certain sections, the task to which this method is applied and the desired model properties for the task could be more clearly explained. It took me a while to get my head around the deep clustering task which the authors are solving. 

Significance
- The impact of CCE on real-world applications beyond the presented datasets and tasks could be further elaborated. This notion that CCE is better for noisy pseudo labels immediately suggests to me examining it as a loss function for doing distillation / noisy teacher-student training of a model on a pseudo-labelled corpus of data, however, I didn't see any links to the area of distillation / teacher-student training within this paper.
- The significance would be bolstered by demonstrating CCE's performance on larger scale, more diverse and challenging datasets.

Limitations:
**Strengths:**
- The paper acknowledges the need for robustness to label noise and addresses this effectively through CCE.
- The paper briefly mentions the potential increase in privacy disclosure risk with larger synthetic datasets but does not elaborate on this limitation or discuss potential mitigation strategies. It would be beneficial to include a more detailed discussion of the privacy implications of the proposed method and any potential negative societal impacts.

**Weaknesses:**
- The discussion on limitations could be more explicit, particularly regarding any assumptions made and potential edge cases where CCE may not perform optimally.
- The experimental evaluation uses very old model architectures (VGG, ResNet) and small datasets (CIFAR-10, CIFAR-100, MNIST, STL-10) which feature images only as large as 96x96 pixels. I would be curious whether the advantages of this method translate to high-dimensional image data with more classes, and on more modern, transformer-based  architectures (eg: ViT) .
- Similarly, could the author's consider extending this approach to deal with pseudo-labelled sequence data, for language models or for translation models, for example?

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on the choice of the loss function in problems with soft distributions of the labels, in particular in the context of pseudo-labeling for unsupervised or self-supervised problems such as clustering. In sections 1-2 the paper gives a thorough review of existing practices and relevant theoretical research. In section 3 the paper proposes a new collision cross-entropy loss as a replacement of the standard Shannon loss, and discusses various aspects of this new loss. In section 4 the paper proposes a new EM algorithm for pseudo-label estimation in connection with the new loss. Finally, in Section 5 the new algorithm is experimentally compared with existing ones and is shown to outperform them.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is generally well-written. Sections 1-3 contain a thorough discussion of entropies and losses suitable for self-labeled clustering, with abundant references. The main point of the paper, the new collision cross-entropy loss, is well explained and motivated. The paper provides an experimental comparison of the proposed algorithm with alternatives and shows its significant advantage.

Weaknesses:
I'm confused by mixing the discussion of losses and EM algorithms in section 4. The bulk of the paper is focused exclusively on the advantages of the proposed new collision cross-entropy loss. The main claim in the abstract and introduction is that the proposed loss is better than the standard Shannon loss. The EM algorithm is mentioned only in the last line of abstract, as if in passing. However, the experimental comparison in section 5 obviously crucially depends on the EM algorithm proposed in section 4. How can we tell if the experimentally demonstrated advantage is due to the new loss or the EM algorithm? Since the main claim is about the superiority of the loss, why not just take any existing soft-labeled clustering algorithms and replace the standard Shannon loss by the proposed new loss? In my opinion, the lack of such a direct comparison substantially weakens the main claim of the paper. The advantage shown experimentally is good, but the conceptual takeaway may be misleading.

I found section 4 on the EM algorithm harder to read relative to the other sections (in fact, I'm not familiar with such algorithms and not even sure what EM stands for - apparently Expectation-Maximization, but this acronym is not explained in the paper). In constrast to the other sections, this one seems to assume familiarity of the reader with related algorithms. I didn't understand, for example, how equation (14) (E-step) was derived.

Another weakness I see is that the strongest results of the paper are largely experimental (not counting general arguments and auxiliary theoretical constructions in Section 4), but, as far as I understand, they are not easily verifiable since the code is not open-sourced.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cgb0Tn4uHy;"REVIEW 
Summary:
This paper introduces a method that supplements the traditional estimate of a class-dependent transition matrix, which is popular in label-noise learning. Traditional transition matrix methods are less effective for instance-dependent noise. To overcome the limitation, the proposed method adds a residual term such that it can extend the projection of a class-dependent T on label predictions to fit the true one as if we have an instance-dependent T. Theoretical analyses of the algorithm confirm its convergence and generalization properties under specific assumptions. Experimental results on various synthetic and real-world noisy datasets such as CIFAR-N and Clothing1M show the performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The performance is eye-catching.
2. The method is proposed with both theoretical analyses and experimental results.

Weaknesses:
1. The intuition of the proposed residual is not clear. For example, why a sparse structure is preferable in this problem? Why do u and v enable a sparse structure? Why is a Hadamard product employed? Why not simply use a vector u?
2. The theoretical part of the main paper is heavy but the outcome is not convincing. Specifically, there is a huge gap between Eq. (17) and Theorem 3.1.
3. The assumption in Eq. (7) is too strong.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem of learning with noisy labels. To handle the instance-dependent noise, the authors propose an extended model for transition matrix-based methods. Specifically, their model combines a class-dependent transition matrix with a sparse implicit regularization term. The authors provide a theoretical analysis of the proposed method. Experiments conducted on both synthetic and real-world noisy label datasets verify the effectiveness of their method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Theoretical analysis of the convergence and generalization are provided.
2. Experiments are conducted thoroughly, including experiments on synthetic and real-world datasets. The ablation study is also conducted.

Weaknesses:
1. The method proposed in this paper appears to be a straightforward combination of VolMinNet and SOP.
2. The experimental results for TMR are missing for the CIFAR-N, Clothing1M, and WebVision datasets.
3. An important baseline, CCR [1], which is the state-of-the-art among transition matrix-based methods, is absent.
4. The paper lacks an analysis of the estimation error of the transition matrix. It would be beneficial to compare the estimation errors of the transition matrix for TMR against those of other baselines.

**Reference**

[1] Cheng, De, et al. ""Class-dependent label-noise learning with cycle-consistency regularization."" *Advances in Neural Information Processing Systems* 35 (2022): 11104-11116.

Limitations:
I did not find that the authors have discussed the limitations and potential negative societal impact of their work. To improve the paper, the authors can provide a thorough analysis of the limitations of their method in an independent section. For example, scenarios where the method might not perform well can be included.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In learning from noisy labels, existing methods generally focus on class-dependent (but instance-independent) noise that can be modeled by a transition matrix $\mathbf{T}$. Some methods have also been proposed for instance-dependent noise (modeled by $\mathbf{T}(x)$). This work belongs to the latter. In particular, it proposes to implicitly model $\mathbf{T}(x)$ using an extended model based on the transition matrix $\mathbf{T}$ and a residual term $\mathbf{r}(x)$. Some theoretical properties (e.g. convergence and generalization) of the proposed algorithm (TMR) are analyzed under certain conditions. Experiments show that the proposed algorithm outperforms baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
**Originality**

The paper studies the challenging problem of instance-dependent label noise, which is less addressed in the literature compared to class-dependent noise. The proposed extended model for transition matrix, which is a combination of a transition matrix with residual terms, seems novel and effective. Related work is adequately cited.

**Quality**

The experiments are quite comprehensive. The paper compares the proposed method with multiple methods (including some state-of-the-art ones) on various datasets. The experimental results show that the proposed method outperforms all those baselines. Some theoretical properties (e.g. convergence and generalization) of the proposed algorithm are also analyzed under certain conditions.

**Clarity**

The description of the proposed method is clear. The experiment section is generally clearly written and well-organized.

**Significance**

The proposed method shows significant improvements compared with various baselines. Therefore, it has the potential to be adopted by other researchers and practitioners, advancing the state of the art in learning from noisy labels.

Weaknesses:
**Originality**

- In Lines 120-125, the residual term $\mathbf{r}(x)$ is introduced. However, it is not clear to me how novel it is compared to the previous work [57,25,30,31]. The authors should elaborate on this point.
- I can see why residual term $\mathbf{r}(x)$ might be useful, but why is it modeled as in the form in Line 124? The motivation should be explained.

**Quality**

- The convergence analysis seems very restrictive to me because it requires too many assumptions (Lines 171-175, Lines 183-186, and Appendix B.2).
- The generalization analysis (Theorem 3.2) is w.r.t. the training loss (surrogate loss) under the noisy distribution $\tilde{\mathbb D}$, but the test accuracy under the clean distribution $\mathbb D$ is what people really care about. Is it possible to prove any consistency guarantees?
- Knowledge of the ground truth $R_*$ is required to derive Theorem 3.2, but we do not know $R_*$ in practice.
- Section 3 is not clearly written, and I found it hard to follow and assess its correctness (see below).

**Clarity**

Section 3 is not clearly written, and I found it hard to follow and assess its correctness. Specifically:

- In Lines 173-174, is $R_{\ast}$ assumed to be $U_{\ast} \odot U_{\ast} - V_{\ast} \odot V_{\ast}$?
- In Lines 203-205, $\mathcal F$ is a set of loss functions. What is the exact meaning of ""about the data""? Why is $R$ not considered in $\mathcal F$? Is a fixed $R$ being used here?
- In Lines 206-207, what is the definition of $\epsilon$-cover?
- In Lines 207-208, what are the mathematical definitions of the ""average losses""?
- In Lines 210-213, it seems that here $R_{\ast}$ is fixed. Yet, it does not make sense to me because $R$ should depend on the transition matrix $T$ and the distributions $\mathbb D$ and $\tilde{\mathbb D}$. What is ""ground truth"" w.r.t. here?

**Significance**

The significance of the proposed method could be further enhanced through a more rigorous theoretical analysis (see above).

Limitations:
I did not see where the authors discussed the limitations of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In noisy label learning problem, noise is often characterized by confusion matrix. In contrast to instance-independent noise, this work considers a setting where confusion matrices could be different for different samples. Under this setting, the authors proposed to use a global confusion matrix shared by all instances and a residual term for each instance to account for the different between instance-dependent confusion matrix and the global confusion matrix. For learning, an MLE loss combined with an implicit sparsity regularizer is optimized.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The work is tackling a challenging yet important setting in noisy label learning. The proposed model is a natural and intuitive extension to instance-independent confusion matrix as it allows for a wider range of noise. The proposed algorithm (TMR) is simple to implement, and demonstrated to be effective under synthetic and real-data experiments.

Weaknesses:
- Motivation for the use of sparsity regularizer is not clear. The authors does not discuss much on why the vector $\textbf{r}$, or matrix $\textbf{R}$ in their model should be sparse. They did point out in page 3, line 117 that the difference when using the global transition matrix and the instance-dependent transition matrix should be small. However, that is not sufficient to promote sparsity, as any other $l$-p ($l>1$) norm could have promoted that goal.
- The use of implicit regularizer is also not clear. And more importantly, since the output of $\textbf{T}^T P(\textbf{Y} | X) + \textbf{r}(X)$ is a probability vector, $\textbf{r}$(X) has to satisfy certain constraints. This is not discussed nor specified anywhere in the paper. And hence it is questionable how the parameterization of $\textbf{r}(X)$ could produce valid probability vector $\textbf{T}^T P(\textbf{Y} | X) + \textbf{r}(X)$.
- The analysis might contain flaw. Equation (14) is incorrect: $\widetilde{\textbf{Y}}$ is a matrix composing of one-hot vectors while the RHS is a matrix composing of probability vectors. The two are not equal in general. This equation seems to be the key step to motivate the objective to be analyzed in (17), and also the key step in the proof of Theorem 3.1 (page 15, line 534). 
- The analysis is based on linear model which is not very realistic.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cVbd7uTvD8;"REVIEW 
Summary:
This paper (SC3D) proposes a single image-to-3D reconstruction method. It combines multi-view diffusion model and a 3D reconstruction model, and uses the 3D reconstruction results as self condition to improve the multi-view generation process. The motivation of the proposed method is to improve the geometric consistency previous single image reconstruction pipeline, namely first generate multi-view images then perform sparse view reconstruction. The core idea proposed in the paper, 3D-aware feedback, is reasonable and also appear in concurrent works IM-3D and VideoMV. Several ablation studies need to be included to prove that the proposed 3D-feedback (including RGB and coordinate maps) are improving the reconstruction quality. Authors also need to justify more about the contribution w.r.t. related work VideoMV. Furthermore, there is still space to improve the readability in the submission.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Major:
- The idea of using 3D reconstruction rendering as condition to improve the geometry consistency of multi-view diffusion models is reasonable.
- The experiments are comprehensive. The results demonstrate that the proposed feedback mechanism is solid in the multi-view reconstruction approach.

Weaknesses:
- Claim about key contributions: the 3D-feedback idea appears already in VideoMV. Since the VideoMV is already available on Arxiv in March, authors need to justify more clearly about the difference and contribution w.r.t. VideoMV.
- lacks generalization results: the method is evaluated on google scan objects, which is standard. However, i am curious to see if the approach generalizes to real world images.
- lacks one ablation: SVD+RGBs feedback, which is missing in Tab. 2 and Tab. 3.
- the readability of the Alg.1 and Alg.2 can be improved. Currently it is too specific and looks like python program. A more abstract algorithm is expected in a scientific paper.
- A typo: in line 213, after comma, ""we"" instead of ""We"" (wrong capitalized ""W"")

Limitations:
- No obvious limitations are found in the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for 3D asset generation conditioned on a single image. The approach follows the recent trend of a two-stage feed-forward model – first generating multi-view images and then using a sparse-view reconstructor to reconstruct the 3D object (specifically, LGM in this paper). This two-stage model has a significant drawback: the inconsistency of the multi-view generation model may result in an imperfect input for the reconstructor, thus causing quality degradation of the final generated 3D assets.

To address this issue, the authors propose adding a 3D-aware feedback mechanism to improve multi-view consistency and enhance the final reconstructed results. Specifically, a self-conditioned mechanism is introduced, where the output of the reconstruction model is fed into the diffusion model. This output is involved in the diffusion process, leading to better 3D consistency.

Overall, the method seems sound to me.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The problem definition and the motivation for the project are very clear.

(2) The paper is well-written and easy to understand.

(3) The method seems sound. By adding the rendering results of a reconstructor as input, which present strong multi-view consistency, the diffusion model is also capable of generating multi-view-consistent images.

(4) Table 3 appears reasonable and as expected.

(5) The appendix provides helpful details on training and network architectures, aiding in the reproduction of the results.

Weaknesses:
(1) Some related works lack citation and discussion:
(a) In ""Dmv3d: Denoising Multi-View Diffusion Using 3D Large Reconstruction Model"" [ICLR 2024], the paper uses a similar mechanism (though not entirely the same) by employing a 3D reconstructor as a multi-view image denoiser.
(b) “Carve3D: Improving Multi-View Reconstruction Consistency for Diffusion Models with RL Finetuning” [CVPR 2024] enhances multi-view consistency through RL fine-tuning.

(2) I encourage the authors to provide more visual results to help readers understand and appreciate the diffusion/reconstruction process. For example, could the authors provide some visual results of $\tilde{x}_0$ at different denoising steps?

(3) In the comparisons, although quantitative results are provided, could the authors include some qualitative (visual) comparisons to the baseline methods?

Other minor issues:

(1)	Line 118, The Plucker coordinate should be (d, o x d)

(2)	Line 206, We -> we

(3)	Line 225, meshe -> meshes

Limitations:
(1)	The method is limited to object-level reconstruction with a clean background. Though this is a common limitation in recent related works, I encourage the authors to explore this issue in future work.

(2)	As discussed in the paper, extracting high-quality surface geometry from the Gaussian model remains an open problem. This is an interesting topic for future research.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper observes that the current state-of-the-art image-to-3D generation models consist of two separate parts: generate multi-view images from a single image and run on top the 3D reconstruction. This process has no feedback loop, i.e. the reconstruction does not inform the image generation which in turn leads to a worse quality of reconstruction. They propose a method that builds in a feedback to loop back the feedback of the reconstruction into the diffusion process. They report superior 3D reconstruction quality over the usual two separate step method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The overall idea that drives the paper to provide a link between 3D reconstruction and diffusion at training and inference time is very powerful and novel, and has not been explored in existing text-to-3D papers. I think this is a significant asset of the paper in a crowded area.
2. The results presented seem to improve quite a bit over the existing state-of-the-art for the results shown.

Weaknesses:
1. The presentation of the paper is not clear. 
    - In Fig. 1 the paper is describing an iterative process. Fig. 1 has also no output, but suggests the 3D representation is the output. However, Fig. 2 suggests the (multi-view) images are the final output? Are the two decoders the same? In the paper you are referring to different models G and F. They are not mapped to the figure to get a better picture.
    - Lines 169-172: this is describing the training strategy. That should be moved to the part starting from line 180 where you are actually describing the training strategy.
    - Equation 1: c_skip is not explained
    - The paper has many typos. Especially in part 4, they appear in almost every paragraph.
    - Lines 227-228: This statement seems contradicting: “Directly employing a NeRF-based feed-forward model during the training process significantly reduces training speed due to the computational demands of volumetric rendering.”
    - Replacing the algorithm code with more concise pseudo code may make it much easier for more readers to understand.
2. Comparisons are not very comprehensive
    - None of the methods in Figure 1 are qualitatively compared. 
    - In Figure 3, it seems different views are compared in the first and second column
    - Minor: It would be also really helpful to introduce some visual cues into figure 9 to easier grasp the results.
3. Some claims are not justified
    - Is the section on augmenting the diffusion model with camera control in 3.1 a claimed contribution of the paper? The statement that: “This approach allows for more detailed and accurate 3D rendering, as pixel-specific embedding enhances the model’s ability to handle complex variations in depth and perspective across the video frames.” Is not justified at all, as other types of embeddings are not ablated.

Limitations:
Overall the paper does describe some limitations of the method, but it’s not clear if they are relevant. For example, is using the gaussian splatting method really a limitation in this case? I’d be interested to know how long this method takes (is it much slower than LGM baseline), how computationally intensive it is, or how sensitive it is to the initial generation by the multi-view diffusion model.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes SC3D for the single-image-to-3D generation, which integrates the diffusion-based multi-view generation and Gaussians-based 3D reconstruction through a self-conditioning mechanism. Specifically, during each denoising step, SC3D injects the rendered image and geometric map from the reconstruction model into the denoising process to enhance the multi-view consistency of the multi-view generated images. Experiments on GSO dataset demonstrates its superiority over existing methods mentioned in this paper.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. SC3D integrates multi-view image generation and 3D reconstruction into a single framework, ensuring similar data distribution between the two modules and thereby improving reconstruction quality during the reference process.

2. SC3D proposes a self-conditiond 3D-award feedback mechanism to bridge the multi-view image generation and 3D reconstruction, in which the rendered images and geometric map are injected in the multi-view generation network. Such design makes sense and could improve the consistency of the generated results from the multi-view generation network.

Weaknesses:
1. Lack detailed visual comparisons with baseline methods. The authors only compare SC3D with LGM but do not show results generated from other baselines, making the visual comparison results less convincing.

2. The paper suffers from poor organization. For example, Figure 4 and Figure 5 are not referenced anywhere in the text. The purpose of Figure 6 is confusing, as its caption suggests it shows results from another work, and it is difficult to discern differences among the three rows. Additionally, the paper's typesetting is of poor quality. There are many blank spaces in the text.

Limitations:
Please refer the weaknesses and questions above.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
