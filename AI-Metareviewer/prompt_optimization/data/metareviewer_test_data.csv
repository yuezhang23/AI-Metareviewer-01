id;text;label
yW3tlSwusb;"REVIEW 
Summary:
The paper explores computational aspects of implementing ERM in data-driven algorithm design. 

The paper contributes an efficient algorithm to ennumerate cells induced by a collection of hyperplanes. The paper then shows how to utilize this as a subprocedure to solve ERM problems for algorithm design, focusing on linkage-based clustering, sequence alignment, and two-part tariff pricing.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
One of the main interesting things I find about this paper is that the runtime of the ERM implementations is instance dependent, and specifically depends on the number of sum dual class loss function pieces. The paper comments that their runtime bounds imply improvements over prior work in the worst-case R but also can be faster for ""typical"" R.

The paper is well-written and easy to follow. The paper discusses relevant background and related work.

Weaknesses:
To what extent is the approach generalizable to other data-driven algorithm design problems? Is there a generic principle or a general characterization of the problems for which this approach can be utilized?

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In data-driven algorithm design, we are given a collection of problem instances sampled from an unknown distribution, and a family of algorithms for solving the problem, typically parameterized by a real-valued multivariate parameter. The goal is to find a setting of parameters such that the performance of the algorithm they parameterize is close to optimal among the family of algorithms, in expectation over the distribution of instances. Most prior work is firstly focused on the generalization aspect, i.e., on showing that a small number of samples suffices (in the sample complexity sense) for finding approximately optimal parameters for the distribution (these are the ERM parameters for the given sample of instance), and thus the family of algorithms is ""learnable"". It then (sometimes) proceeds to develop an efficient algorithm for finding those ERM parameters, based on the structure used to prove the generalization bound.

This paper focuses more systematically on the ERM efficiency aspect. To this end, it starts with a common theme in many prior works on data-driven algorithms, that had been abstracted out and formulated in a generalized form in Balcan et al. (STOC 2021): for any fixed problem instance, the function that maps a setting of parameters to utility (of invoking their associated algorithm on that instance) admits a simple piecewise structure. Say, there is a small number of ""simple"" boundary functions (say, linear thresholds) that induce a partition of the parameter space R^d such that the utility function restricted to each piece is ""simple"" (say, constant). This is helpful in bounding the VC dimension of the utility functions and thus proving generalization bounds, and also potentially for navigating the parameter space efficiently to find the ERM parameters.

The novelty in this paper is an attempt to give a more systematic recipe for the second part (navigating the piecewise structure for efficient ERM), with two main advantages -- (1) creating a unified framework that takes care of some parts of the ERM procedure in a general way, thus restricting the portion that needs to be figured out per each problem individually, and (2) obtaining algorithms whose running time depends on the actual number of pieces in the given instances (""output-sensitive"") rather than worst-case number of pieces. The per-problem part to figure out is a subroutine that, given a problem instance and parameter setting p, returns a list of candidate boundary functions for the piece that contains p, and this subroutine depends on the specific problem in question. The unified part of the framework uses this subroutine to search through the pieces in an ""output-sensitive"" running time.

__Post-rebuttal__: I appreciate the authors' elaborations on the technical content, and the conceptual aspect of the paper. I have raised my score to support acceptance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The strength of this paper is that the matter of efficient ERM in data-driven algorithms indeed merits its own systematic study rather than being left as an afterthought of the generalization bounds.

Weaknesses:
The main weakness is that the end result isn't very strong: the framework is restricted to linear boundary functions and (more disconcertingly) to a constant number of parameters, and for the most part does not yield improved running times in the worst case, but a different notion of efficiency (output sensitivity). It tends more to systematically organizing ideas that have appeared in the literature in one form or another and less to introducing new algorithmic insights or techniques. I also feel that the presentation and writing could be too opaque for a wide readership like that of NeurIPS.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the problem of learning optimal parameters for data-driven algorithm design. A characteristic of the problem is that the dual loss function, which measures the performance of an algorithm as a function of parameters, is discontinuous. Nevertheless, the dual loss is typically piecewise structured (constant, linear, etc.) with linear boundaries. Thus, roughly speaking, the problem of finding optimal parameters reduces to exploring polytopic cells partitioned by boundary hyperplanes. 

The main contribution is a cell enumeration algorithm that runs in output-polynomial time. The algorithm can be seen as a breadth-first search on a cell adjacency graph, where the enumeration of neighbors is done in an output-sensitive manner based on Clarkson's algorithm. The resulting output-sensitive complexity can be significantly better than the worst-case, as demonstrated in Example 1.

The authors then instantiate the ERM method based on the cell enumeration for linkage-based clustering and DP-based sequence alignment. The applications involve designing execution graphs, which originate from the execution tree of [BDL20]. Combining appropriate problem-specific execution graphs with the cell enumeration leads to the improved time complexity of ERM in several data-driven algorithm design problems, as in Table 1.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper addresses the important problem of optimizing algorithm parameters in data-driven algorithm design. 
2. The theoretical results given in Table 1 appear strong compared with previous ones. 
3. The output-sensitive cell enumeration might be of independent interest in the context of computational geometry.

Weaknesses:
1. The paper would have been more appealing if implementations of the proposed methods and experimental results were provided.
2. The paper is somewhat dense and it is not easy to follow the technical details.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yVzWlFhpRW;"REVIEW 
Summary:
Learn a state specific mask for actions. Rather than simply a state specific interval, extend the action mask to different convex set representations. Then, derive a policy gradient for each of these masking schemes.  The masking schemes are ray masks, hypercube transform mask and distributional masks. Applies action masking to seeker and quadrotor tasks and shows that this action masking improves performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed action masking covers a wide range of possible action mask definitions.

The derived policy gradients are relatively straightforward given the definitions of the action boundaries.

The derivations appear to be sound when applied empirically.

Weaknesses:
It is not clear how easy it is to recover the action masking criteria, especially under the more complex generator or distributional schemes, and it seems like this would be rare

The experiments are not particularly convincing because they all follow similar control tasks, but it also seems like these are the only tasks for which the action mask could be easily defined.

Limitations:
While noted in the limitations, deriving a method for identifying not only the policy gradient, but a policy from a learned value function is highly relevant for RL, and it is not clear how these restricted action spaces can be applied to Q value calculations, though it seems reasonable to assume it is possible.

Some suggestions in the main work of how the distributional or generator action space restriction could be defined as a function of the dynamics could be relevant since it seems like these functions have to be hand-engineered, and it is not obvious how to do that in domains where the dynamics are less well defined.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses challenges in RL with continuous action spaces, typically defined as interval sets. These spaces often lead to inefficient exploration due to irrelevant actions. The authors propose three continuous action masking methods to focus learning on relevant actions based on current state, improving predictability and suitability for safety-critical applications. They analyze the implications on policy gradients and evaluate performance using PPO across three control tasks. Results show higher final rewards and faster convergence compared to baseline methods without action masking.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality**
- The paper presents a unique perspective on action spaces by utilizing the relevance of action utility in tasks to improve performance. Conventional methods are limited to discrete domains (tasks) so applying their methods to continuous environments was interesting to see.

**Significance**
- The proposed approach has practical implications, especially in complex environments where distinguishing between relevant and irrelevant actions is crucial. Regardless of the coverage of baseline, their methods significantly outperform it establishing the state of the art performance.

Weaknesses:
**Reinforcement Learning with Policy Gradients (Section 2.1)**
- L84: ""r →: S × A"" appears incorrect.
  
**Continuous Action Masking (Section 3)**
- Assumption 1: Clarify the definition of action relevance.
  
**Ray Mask (Section 3.1)**
- L131: Need proof that g(a) is bijective.


**Generator Mask (Section 3.2)**
- Why is A(s) suddenly state-dependent? Provide motivation and further description.
- In Proposition 2's proof, the matrix multiplication seems infeasible due to mismatched dimensions (C is N x 1 and Ga results in P x 1).


**Experiment (Section 4)**
- Justify the rationale behind the design choices for action relevance in each environment.
- Compare the chosen action relevance approach to other relevant action settings.
  
**Results (Section 4.2)**
- Why compare to a standard PPO baseline and not to prior relevant works?
- Include qualitative results to validate the proposed methods.

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses methods for action masking in continuous action spaces to improve convergence stability and sample efficiency in reinforcement learning. The paper introduces three methods for action masking with convex relevant action sets, proves their convergence, and experimentally verifies their effects.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper is excellently written, defines a clear and well-motivated goal, and describes three intuitive and theoretically grounded methods to achieve that goal within well-defined and clearly stated limitations.

To my knowledge these approaches are novel (though the distributional mask I suspect has been used as a one-off solution in prior work as it is conceptually very simple), and their definition and analysis are nontrivial.

Weaknesses:
This paper is pretty solid overall, and I have few major complaints.

The one significant issue I see is that I think the distributional mask algorithm is off-policy by nature, meaning it's use with on-policy methods like PPO is biased and will cause performance loss or divergence. This may explain the observed underperformance of this masking method in two of the three experimental tasks, and while the algorithm can clearly converge in some cases it seems like a major issue with that particular mask in the context of PPO (off-policy algorithms could use it without issue, but those are left to future work here) that should be noted, or the mask omitted from this paper and left to future off-policy methods.

Beyond that, the experimental evaluation is relatively simple (though I think it is sufficient to validate these algorithms), and more challenging tasks would be useful to demonstrate the limitations of these masking methods. That said, the paper makes it clear that defining a suitable convex relevant action set is a manual process and can be challenging (this is okay as a limitation), so it is understandable why such stress tests are not performed. If there was a way to increase difficulty without major manual action set definition work it would strengthen the evaluation to include it.

I have a few other minor issues and questions noted below, but overall this is a paper that is clear in its goals and describes methods that achieve them, validated to a reasonable standard of theoretical and experimental evidence. There's more that could be done on this topic, but the contribution of this paper is significant on its own, so I'm inclined to recommend acceptance (particularly if something is done to address my concern about the distributional mask above).

Limitations:
This paper does an excellent job of making clear its limitations and scope.

I don't see any potential negative societal impacts from this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes mathematical formulations for continuous action masking in reinforcement learning, to incorporate domain-knowledge in the form of state-specific sets of relevant actions. It introduces 3 functional forms to extract relevant actions from the original action space, and consider its effect on the policy gradient. The policy gradient does not change much, and the paper shows that the forms compare similarly and better than learning an agent without any knowledge of the continuous action mask at all.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The problem of action masking in continuous action space is an underexplored one, but could have major impact in efficacy of agents and incorporating domain-specific knowledge.
- The proposed continuous action masking could potentially be useful for safety demarcations.
- The paper provides mathematical frameworks to formulate continuous action masking and also derive their (minimal) effect on the policy gradient.
- The paper is mostly well-written and explains the mathematical derivations quite well. Quick note that Section 2.1 and 2.2 could be made more integrated, currently they seem completely disconnected.

Weaknesses:
## 1. General applicability of this paper's ideas
Obtaining a state-specific relevant action set can be really hard. The paper, however, makes contradictory statements about this:
- L5: ""little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions.""
- L284-285: ""assume that an appropriate relevant action set can be obtained. Yet, obtaining this can be a major challenge in practice.""

From the experiments on the 3 environments, it already seems like defining the relevant action set requires a lot of domain knowledge about the state space features and the dynamics function.

As of now, there does not seem to be any way the ideas in this paper could be useful for any practical domain.
- Can the authors provide some concrete examples of how one can obtain such relevant action sets for problems of practical interest and scale?
- Can the authors provide any results on a commonly used continuous action space RL benchmark?

## 2. Gains not coming from the policy gradient, but only because of constraining the action space
The paper's proposed formulation is interesting because it uses continuous action masking as part of the learning policy and informs the policy gradient update about the continuous action mask. However, when we look at the resultant policy gradients for each mask in Eq. 10, Line173, and Line199, it seems that the policy gradient simply reduces to $\lambda_\theta log \pi_\theta(a | s)$ for all cases.

So, the effective change in implementation is just how the action to take in the environment is model: $a^r = g(a)$. But, this **doesn't utilize continuous action masking to improve the policy learning objective** in any meaningful way. Is my understanding correct in this?

Another observation that validates the claim that policy learning is not influenced much is seen from the results and L248-249. The initial rewards themselves are significantly higher, which means that the action mask just reduces the space of exploration of the agent so much that, as long as it takes a valid action, it would get a high reward.

## 3. Simpler baselines for continuous action masking
Continuing from the above point, if all that needs to be done is to compare different formulations of g(a), there is a much simpler alternative perspective:
- Action-Masking as part of environment: Simply, apply the action mask as part of the environment, without changing the PPO objective at all. So, there is an action-masking filter before executing an agent's action in the environment, and ignore if the action is invalid.
- Sampling-augmented action-masking: Keep sampling actions from $\pi_\theta$ until you find a valid action that can pass through the known continuous action-masking map.

The current PPO baseline is very weak, and does not utilize action-masking at all. It seems most of the learning in PPO is going into the effort of learning the action mask. To really justify this paper's proposed action masking schemes are useful, they must compare against other forms of naive action masking, including the two listed above. This perspective on considering the action mask as part of the environment is also much more generally applicable and does not require any change to the policy gradient update.

Limitations:
The authors discuss several limitations of the work, but the important ones of applicability and baselines need to be addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yVu5dnPlqA;"REVIEW 
Summary:
The paper proposes a 3-stage pipeline to harvest ex-large-scale instruction data from the pre-training web corpus to enhance LLM reasoning, which involves 1) recalling relevant documents, 2) extracting instruction-response pairs using LLM, and 3) refining the extracted pairs by completing the intermediate reasoning steps using LLM.

The paper

- proposes an effective pipeline to synthesize large-scale high-quality instruction data, especially for reasonable prompts and reliable answers;
- empirically validates the effectiveness of scaling up instruction data for reasoning tasks;
- builds `MAmmoTH2-Plus` models, achieving performance superior to or comparable with previous SotA on various reasoning datasets;
- provides a ex-large-scale instruction dataset for reasoning tasks, *WebInstruct*, as unique public data resource;
- conducts extensive ablation studies, providing many insights like:
  - SFT loss is better for LM loss (at least when evaluated on QA tasks);
  - refining extracted instruction pairs by completing the intermediate reasoning steps is significantly helpful;
  - using multiple LLMs to refine the instruction data is usually better than a single LLM;
  - “Education” data (exam-style) are usually better than “Forum” data (discussion-style) (at least when evaluated on QA tasks);
  - even benchmarks conventionally thought very relevant might conflict with each other (GSM & MATH in Table 5), implying limited generalization of LLMs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The scaling effect of instruction data is an important empirical question. The paper is the first to scale instruction data to 10M pairs, showing the feasibility and effectiveness of scaling up instruction data (for reasoning tasks).
- Synthesis of high-quality prompts and answers is important for further data augmentation but rather under-explored. The paper finds an effective method to synthesize reasonable prompt and relatively reliable answers by harvesting from web corpora.
- `MAmmoTH2-Plus` models achieve performance superior to or comparable with previous SotA on various reasoning datasets.
- Extensive experiments are conducted on various base models and especially diverse challenging reasoning benchmarks, instead of easy ones with limited scope (e.g. many benchmarks similar to GSM8K), convincingly validating the method's effectiveness.
- Many insightful and useful observations in ablation studies (as mentioned in the summary).
- The paper is generally well written to be clear and detailed.

Weaknesses:
- It might need further consideration about **whether training on *WebInstruct* is compatible with or necessary to be added to existing training pipelines to achieve the best final performance (for reasoning tasks)**. The paper achieves its best performances (`MAmmoTH2-Plus`) with a 2-stage instruction tuning on pre-trained models but doesn’t involve continual pre-training, which should be rather important for models’ reasoning abilities as proved by works like DeepSeek-Math. Pre-training and RL should be out of this work’s scope. But it would be better to further clarify the impacts of 1) continual pre-training, 2) training on *WebInstruct*, 3) final fine-tuning on additional instruction datasets and their combinations.
  - Table 7 shows the performance on reasoning benchmarks of applying 2/3/2+3 on Mistral-7B/Mixtral-8x7B. But **the comparison might be a little unfair**: the domains of the “Public Datasets” are wider than those of the *WebInstruct* with the code generation dataset *Code-Feedback*, but the benchmarks only involve mathematical and scientific reasoning in natural language, which might underestimate the performance of “Public Datasets”, considering the possible confliction between code generation and reasoning in natural language. It might be better to remove *Code-Feedback* from the “Public Datasets” to compare with *WebInstruct*.
  - **To consider 1) continual pre-training**, it is impossible to conduct by yourselves, but a possible workaround could be to make full use of the resources provided by DeepSeek-Math: DeepSeekMath-7B is continually pre-trained from DeepSeek-Coder-Base-v1.5. By comparing performances on reasoning benchmarks of applying 2/3/2+3 on DeepSeek-Coder-Base-v1.5/DeepSeekMath-7B and the two models themselves, a more comprehensive study on the impacts of these training stages can be done.
- Table 7 shows that, for strong Mixtral-8x7B, the gains of adding *WebInstruct* to “Public Datasets” is marginal, implying that **the effect of *WebInstruct* for strong base models might be limited**.

---

# After rebuttal and discussion

The authors resolved most concerns and validated that MAmmoTH2 can efficiently substitute continual pre-training in the standard SotA pipeline. The limitation is that MAmmoTH2 fails to combine with continual pre-training to effectively push forward the upper limit.

I decide to change my score to 8.

Limitations:
The limitations of this work are acceptable and the authors point out potential directions to address the limitations for future works.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces MAmmoTH2, a novel approach to instruction tuning for large language models (LLMs) by harvesting naturally existing instruction data from the web. The authors develop a three-step pipeline (recall, extract, refine) to collect 10 million high-quality instruction-response pairs without relying on costly human annotation or GPT-4 distillation. Fine-tuning LLMs with this dataset significantly improves performance on reasoning benchmarks. The MAmmoTH2-Plus model, further tuned on public instruction datasets, achieves state-of-the-art results on multiple benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Demonstrates a cost-effective way to collect large-scale, high-quality instruction data from the web.
- Significant performance gains on reasoning benchmarks, with MAmmoTH2 models outperforming existing models.
- Comprehensive evaluation across multiple benchmarks, showing robust improvements.

Weaknesses:
- The approach primarily combines existing methods (data recall, extraction, refinement) rather than introducing fundamentally new concepts or techniques.
- More explicit comparison with prior work is needed to highlight the unique contributions and differences of this approach.
- The quality and diversity of the collected data heavily depend on the web sources, which may introduce biases or inconsistencies.

Limitations:
The authors address some limitations of their approach, such as the dependency on web data quality and the challenges in maintaining the diversity and relevance of the instruction data. However, a more detailed discussion on potential biases introduced by web data and the ethical implications of using such data could strengthen the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes an approach to automatically harvest large-scale instruction data from pre-training corpora for reasoning tasks. The main steps include: (1) Recall: training a fastText model to recall relevant documents from the pre-training corpus, similar to DeepSeekMath; (2) Extract: using open-source models with few-shot prompting to extract question-answer pairs from the recalled documents; (3) Refine: prompting open-source models to remove noise, adjust formats, and complete the reasoning process for the extracted question-answer pairs.

Using this method, the authors harvested 10 million instruction data and trained MAmmoTH2 models. Without relying on closed-source models, MAmmoTH2 achieves excellent performance on various reasoning tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Method: The motivation is clear, and the idea of automatically extracting instruction data from web data is novel, simple, and scalable.
- Experiments: The experiments and evaluations are comprehensive and achieve good results.
- Well Written: The paper is very easy to understand.
- Reproducibility: The authors have open-sourced part of the corpus, models, and evaluation scripts to ensure the reproducibility of the results.

Weaknesses:
1. Effectiveness: I wonder if the WebInstruct approach can further improve the performance of state-of-the-art domain models. For example, DeepSeekMath achieved good results by only training on recalled documents and fine-tuning on high-quality data (MATH: DeepSeekMath-7B-Instruct 46.8% vs. MAmmoTH2-7B-Plus's 45.0%). Moreover, since the models have already been trained on SFT data, comparing only the few-shot performance is not comprehensive enough. I suggest also comparing the performance of the Plus version trained with high-quality ""additional instruction datasets"" for most of the experiments. Consider supplementing the following results:
   - Recall + Plus: Directly train on the 18M recalled documents and fine-tune a Plus version to verify if the ""extract + refine"" steps have significant benefits.
   - Recall + Extract + Plus: Directly train on the extracted QA (Fig.5, Extracted QA) with LM/SFT loss and fine-tune a Plus version to verify the benefits of the refine step.
   - In Fig.5, I also recommend reporting the performance after fine-tuning the Plus version for SFT loss vs. LM Loss.

2. Lack of method details:
  - For example, the code for the recall stage and the prompts used for extraction and refinement could be included in the repository or appendix.
  - In Sec. 5.1, I suggest explicitly defining the SFT Loss to help more readers understand it clearly. By ""SFT Loss"", I understand the authors mean ""masking the loss of instruction input"", right?

3. Scalability:
   - The effectiveness of WebInstruct constructed using small models is unknown for larger models; moreover, this approach is difficult to apply to models with hundreds of billions of parameters due to high inference costs.
   - During refinement, the model generate missing explanations. Have you observed and quantified the hallucination phenomenon? If present, such incorrect reasoning processes can negatively impact model training, such as increasing hallucination/bias, especially if the corpus is used for larger models.

4. Minor points:
   - Some citations are missing for baselines in Table 2, e.g., Gemma, Abel, and Rho-1.
   - How can the WebInstruct approach be extended to more general domains? What other issues need to be addressed?
   - A concurrent work, Jiuzhang3.0 [1], is quite similar in motivation and method. It would be better to discuss and compare with it. What are the advantages and issues of MAmmoTH2 compared to Jiuzhang3.0?
 
---

[1] Zhou, Kun, et al. ""JiuZhang3. 0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models."" arXiv preprint arXiv:2405.14365 (2024).

Limitations:
The authors have addressed most of the limitations. The limitations section can be further improved by referring to the weaknesses part.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a method to synthesize instruction tuning data at scale from the pretraining web corpus. The proposed method first recalls relevant documents from the corpus, and then extracts QA pairs, and finally refines the extracted QA pairs with an LLM. The synthesized instruction data proves to be helpful in enhancing the model’s reasoning abilities compared with instruction tuning data from other sources.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The proposed method is novel and effective.
2.	The authors conduct extensive experiments to demonstrate that it’s possible to synthesize tuning data from unsupervised text corpus to build strong LLMs that outperform models trained with data collected in existing paradigms.
3.	The paper is well-written and easy to follow. The code and data are released, which will serve as high-quality resources for research and building strong LLMs.

Weaknesses:
There lacks a discussion and comparison with a related work “Self-alignment with Instruction Backtranslation” (Li et al., ICLR'24) which also synthesizes instruction tuning data from unlabeled corpus.

Limitations:
The authors have discussed the limitations in Appendix H and societal impacts in Appendix I.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yUqUBGioBG;"REVIEW 
Summary:
This paper first investigates the effect of class distribution changes on comparative zero-sample learning by proposing and analysing a class distribution shifts parameter model, leading to the idea that loss minimisation leads to poor performance of representations over the class distribution shifts. Based on this finding, the authors utilise hierarchical sub-sampling and OOD environment balancing methods to obtain robust representations and address the poor performance caused by class distribution changes in zero-sample learning, and experimentally validate the effectiveness of the methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1- This paper studies the distribution bias problem caused by challenging unknown attributes in zero-shot learning and proposes an effective solution, which is important and innovative for solving the distribution bias problem in zero-shot learning.

2- The structure is clear. It enables the reader to quickly follow the research ideas and understand the content of each section.

3- Figures and tables are clear and accurate. The figures and tables in this paper are concise and clear, effectively support the ideas or conclusions, and enable the reader to grasp the critical information quickly.

4- Comparison and ablation studies are comprehensive. The authors demonstrated the superiority of their method through many experiments and analysed various factors.

Weaknesses:
1- Authors should describe their proposed soft-AUC trends in detail and analyse the penalty to help readers understand how they play a role.

2- In Experiment 1, the authors used the attribute blonde hair to shift the class distribution, but we know that some people may be hairless, so can the attribute gender be used to shift the class distribution?

3- The language of this paper needs to be scrutinized and improved. For example, redundant phrases such as ""with respect to"" (line 32) should be avoided. In addition, there are some grammatical errors that need to be improved, such as ""assumes"" in line 333 should be changed to ""assume"", and ""leverages"" in line 350 should be changed to ""average"".

4- WRITING DETAILS: Abbreviated nouns should be introduced the first time they appear, such as “OOD”.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a robust representation learning method that could assume the shift between seen classes and unseen classes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Good presentation and sound method.

Weaknesses:
Lack the experiments on the most popular benchmark of zero-shot learning [1] and comparison to some SOTAs, e.g. [2][3].

[1] Zero-shot learning-the good, the bad and the ugly[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
[2] Rebalanced zero-shot learning[J]. IEEE Transactions on Image Processing, 2023.
[3] Transzero: Attribute-guided transformer for zero-shot learning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Zero-shot learning classifiers face the challenge of distribution shifts, where the distribution of new classes differs significantly from that of the training data. In this paper, the authors introduce a novel algorithm to address this problem by creating robust representations through hierarchical sampling and environment balancing penalization. 

Experimental results also demonstrate a performance increase compared to the baseline ERM model on several real-world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to understand.
2. The paper proposes a new model that enables handling unknown attributes for distribution shifts and addresses new classes at test time.
3. The method is tested through both simulations and real-world experiments.

Weaknesses:
1. Some parameters need to be clearly defined, for example, $\rho_{tr}$, $\rho_{te}$, and $y_{uv}$ in Eq (4).
2. The proposed method creates multiple environments and computes penalties across them. What is the computational complexity? It's also beneficial to discuss the time complexity of Algorithm 1.
3. Figure 5 and Figure 6 do not straightforwardly show the performance.

Limitations:
The authors acknowledge the limitations in their paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper treats the problem of learning models for zero-shot open-world classification settings (open-world meaning previously unseen classes might appear at test time) that are robust to distribution shifts.

The proposed approach consists of two stages. In the first stage, synthetic environments $S_i$ are sampled from the training data following a hierarchical sampling approach, where first classes and then data pairs according to sampled classes are sampled. 
In the second stage, the model is updated to minimise a loss composed of standard ERM and the variance over environment AUC scores.

The benefits of the method are demonstrated on synthetic data, CelebA, and ETHEC (where also on the latter two a distribution shift is introduced synthetically).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed approach to generate synthetic environments through hierarchical sampling seems neat and novel (even though the idea of generating synthetic environments for learning robust models is not novel, see weaknesses)
- Adjusting the performance metric in the variance regularisation term for zero-shot verification, using AUC on embedding distances instead of loss (like in VaRex) seems like a nice way of avoiding performance plateaus and enables better performance in the conducted experiments
- In the experiments, the proposed method shows significant performance gains over ERM
- It is good to see a theoretical derivation of the necessary number of sampled environments to achieve a minimum number of examples from each class in at least one environment during the hierarchical sampling with a certain probability (Section 4.4). I believe this result should be made more prominent in form of a Proposition with a proof (in the appendix).

Weaknesses:
- Lack of baselines wrt generation of synthetic environments: The idea of generating synthetic environments to learn models that are robust to distribution shift is not new, and as such the proposed approach should have been compared to existing methods for this. For example, it would be interesting to see how the approach proposed by the authors compares to the approach of the 'Environment Inference for Invariant Learning' paper by Creager et al. (2021).
- The real data experiments are still semi-synthetic in the sense that the distribution shift is introduced synthetically (and is quite stark). I do understand that finding a dataset that has a stark enough distribution shift of one attribute inherently in it is hard or even impossible, and the synthetic shifts are good for highlighting the potential merits of the proposed approach. However, what is missing is a report on the performance of the proposed approach (in comparison to baselines) on the unshifted train and test sets of CelebA and ETHEC, to ensure that there is no performance trade-off. 

Minor:
- Figure 2 is never referred to in the text - as a result it is unclear what its purpose is.
- It would be helpful to refer to the result of Section 4.4 already in Section 4.1, when it is claimed that 'hierarchical sampling results in diverse mixtures of any unknown attribute' and that 'smaller subsets with $k < N_c$ classes are likely to exhibit distinct attribute distributions' Section 4.4. backs up these claims. 
- L 235 typo at the end of the line -> (10)
- Line 151: $h$ needs to be defined before referring to it in an equation, or immediately after the equation.

Limitations:
Limitations are briefly discussed in the Discussion.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yAa5l92TtQ;"REVIEW 
Summary:
This paper designs a novel hierarchical search algorithm (POETRY) for generating formal proofs with large language models step-by-step. In particular, POETRY will first search for proof steps with proof level 0 (these steps typically correspond to subgoals in the proof), and check the correctness of the level 0 proofs by assuming that all the subgoals can be proved. If and only if the level 0 proofs are correct, POETRY will recursively search for proofs to each of the proposed subgoals. Compared with the baseline best-first search methods with the same compute, POETRY significantly improves the pass@1 succ rate on both miniF2F valid and test set, as well as the PISA test set.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
-	The POETRY algorithm is neat and novel by mimicking how human write mathematical proofs hierarchically. 
-	This paper is well written and easy to follow.
-	The POETRY algorithm has potentials to be further improved by incorporating premise-selection techniques such as sledgehammer or Magnushammer.

Weaknesses:
-	From Table 1, it seems to me that the improvement from the search algorithm is less significant than the beam search. A drawback of the beam search method is that the algorithm becomes deterministic, meaning that generating more samples per theorem does not improve its performance. Since this paper only shows pass@1 results, it is unclear how the POETRY algorithm scales with more computing resources.

Limitations:
The authors adequately addressed the limitations and potential negative societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes POETRY, a method for formal theorem proving using language models by training the model to iteratively decompose the problem into sketches, recursively. The authors focus on Isabelle. At each step, POETRY takes a proof state and goal and predicts either a formal sketch (a proof using sorry at each step), or a ground proof step (e.g. 'by ...') that requires no recursion. These intermediate states are visited within best-first search, where the score of a node is given by the log-probability of all predictions made so far to get to that node. Intuitively, POETRY works by recursively generating lower level steps / sketches, until finding a complete proof, getting feedback from the formal environment at each step. To train the LM, the authors introduce a simple method to decompose existing Isabelle proofs from the AFP as if they had been generated by this recursive proof generation process. Experiments on minif2f show improvements on top of GPT-f and a version of Thor without Sledgehammer.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well motivated and tackles a timely topic, using a standard, hard benchmark (minif2f) for methods in this space. The writing is mostly clear (though see some notes below).

POETRY is a simple, sound and novel method to structure the proof generation process. It should be adaptable to other interactive theorem provers with some work. POETRY allows the prover model to get more intermediate feedback from the environment compared to methods that try to produce the whole proof at once. It also uses this feedback in a way that is complementary to proof repair methods (generally what the standard when we consider using intermediate feedback).

Weaknesses:
The choice of baselines (for Table 1) seems a bit convoluted. In particular, I don't really understand why use Thor without sledgehammer. The main point of Thor is to learn when to use hammers in proofs. Removing this makes the model much more similar to the GPT-f baseline.

As for the choice of pass@1, even though POETRY only makes a single prediction at the end, it gets feedback from Isabelle at each node in its tree. So that doesn't seem like a fair comparison either, if POETRY makes many intermediate predictions and calls Isabelle during its search, whereas GPT-f and Thor w/o sledgehammer seem to only produce and test a single prediction. It might be more fair to match the methods based on some other metric, like number of tokens generated, or number of calls to Isabelle (whichever seems to be the most significant bottleneck).

The question of ""Can POETRY find longer proof?"" is a bit ill posed as is. It would be possible for a method to find very long proofs that do not *need* to be long, and do better on this analysis without really being able to prove more complex theorems. What I think the authors are trying to show here is that POETRY can solve harder problems, estimating hardness by looking at proof length. For this, you might want to compare success rate based on the length of the ground truth proof: perhaps the baselines perform very poorly on theorems where the human proof is longer, whereas POETRY might have a better success rate. Another option is to show that either POETRY generates proofs of similar length to the ground truth proof (so, when POETRY generates long proofs, you'd estimate that the ground truth proof would also be long), or that it generates proofs of similar length to the baselines in cases where they all manage to prove a theorem. Any of these would help show that this result is not trivial.

Limitations:
Yes, adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a method called POETRY (proving theorems recursively) for constructing formal proofs in Isabelle/HOL. POETRY performs best-first search on proof sketches guided by a language model fine-tuned on proof sketches. POETRY outperforms other algorithms guided by language models that prove theorems step-by-step. POETRY also outperforms other methods that integrate automated theorem provers and language models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
While the idea of a proof sketch is not novel, the combination of the data curation process to enable the construction of proof sketches is. This takes a step towards generating conjectures which would be crucial to making progress on neural theorem proving.

Weaknesses:
1. It seems to me that the real reason for the success of POETRY is not the algorithm per say, but the data curation to construct proof sketches. In this vein, it would be instructive to have a before sorry and after having sorry to illustrate how the dataset is constructed. 
2. There should be more context explaining how to compare the Lean results against Isabelle/HOL. These are two different formal systems, with different proof methodologies.
3. More details on success cases and failure cases would help understanding the pros and cons of the approach taken in POETRY. For instance, are there certain kinds of problems that POETRY performs well on, e.g., geometry problems? How does POETRY perform when existentials need to be instantiated? Is it the case that POETRY can prove the same theorems as previous step-by-step approaches and can additionally prove more theorems that are longer, or do the approaches prove different short theorems?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces POETRY, a new method to prove theorems recursively. The key ideas are to use a modified best first search algorithm for the search part, and a *sorry* tactic for assumptions at the current level (to be proven later). The authors provide the intuition that this recursive structure allows POETRY to prove theorems in a top-down fashion similar to humans, getting into the details of proving a conjecture only if it is actually relevant to the best overall proof being explored. The authors conduct experiments with two standard benchmarks, showing notable improvements over baselines and SOTA search-based methods (but not LEGO-Prover etc. which rely on substantially larger general purpose LLMs).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is very well structured and clearly written. Intuitions, method details, connection to existing methods, limitations, and take away messages from experiments are all very well articulated.

The idea seems simple but is apparently novel (see 'weaknesses' below, related to this). 

The gains over several baselines are notable, of 5% or more (absolute).

I am assuming the authors will publicly release the code of their POETRY system for further research on this topic.

Weaknesses:
Not being very familiar with the area, I am surprised none of the existing SOTA methods use a similar recursive, top-down search of in theorem proving. I will have to defer to other, more knowledgeable reviewers for assessing novelty of the present work.

I did not fully follow why a *novel* recursive best-first search strategy is needed here. The description of this section (3.2) can probably use some clarification. E.g., why could one not account for the conjecture's level in the utility of the conjecture, and thus implicity enforce level-by-level proof search? On the same note, could the authors comment on the relationship between their proposed recursive best-first search and a combination of standard breadth-first search (i.e., staying within a level) and best-first search (i.e., preferring to explore the most promising node first)?

Just for completeness, it would have been good to know how well very large LLM based methods, such as LEGO-Prover, do on the considered benchmarks.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yUckuDjAE0;"REVIEW 
Summary:
This paper proposes to use input-convex neural networks to learn Bregman divergences as a means to distinguish semantically meaningful image corruptions from random noise perturbations. The approach is linked to classifier robustness by showing how the associated mirror descent algorithm can be used to perform adversarial training against image corruptions coming from a Bregman ball. Experiments on benchmark corruption datasets show that the proposed method outperforms prior learned similarity metrics in distinguishing corruption from noise and in adversarial training. The proposed method is also shown to generalize quite well to corruptions that it is not trained on.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is very well-written, easy to follow, and has great visualizations. The proposed method appears novel and performative, and is certainly of interest to the ML and robustness communities.

Weaknesses:
See questions below.

Limitations:
N/A.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present an approach to learn Bregman divergences that capture perceptual image similarities according to a given dataset.
Relying on two input-convex neural networks, they present a procedure that mimics mirror descent over the learned Bregman divergence.
The procedure is used to learn networks that are robust to image corruptions. Results on CIFAR-10-C subsets are presented.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The idea to (attempt to) do mirror descent over learned Bregman divergences in order to train networks robust to corruptions is novel and interesting.

Weaknesses:
While the idea in itself is interesting, I do not find neither the technical presentation, nor the provided results convincing.

Most of the motivation of the work derives from the use of Bergman divergences, which come with an associated mirror descent. However, in practice, I do not think the authors can be claiming to do mirror descent, because of the approximation of the inverse map, and because of the lack of a projection operator. Given the two above limitations, I do not think what the authors do would have convergence guarantees even in the convex case. Taking this into account, the stress on the mathematical motivation of the approach seems to be a bit fragile. Sometimes mathematical concepts are introduced without a clear purpose (for instance, the Legendre type, which is then not really necessary to justify their approximation of the inverse map). I would urge the authors to tone down these claims and refrain from saying they are doing mirror descent: I'd rather call it an approach ""inspired by mirror descent"".

Furthermore, the work assumes that a dataset describing the corruptions is available to learn the divergence: is this a reasonable assumption for datasets such as CIFAR-10-C, which are designed as benchmarks for OOD generalization?

Concerning the results: I do not think the proposed comparisons are fair. Both l2 PGD and RLAT use a threat model which is general and not targeted at specific perturbations. As such, they attain good performance over the entirety of the CIFAR-10-C corruptions. The authors, instead, focus on a small set of perturbations and show results for an algorithm that is explicitly aware of the perturbations the network need to be robust against.

Limitations:
The limitations paragraph is satisfactory.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a new method to learn Bregman divergences from raw, high-dimensional data. This method measures similarity between images in pixel space, and considers two images as similar even if one image is corrupted by real-world corruptions, such as blur, changes in contrast, or weather conditions such as fog. The method does this in-part by simultaneously considering real-world corruptions as close to the original image, while noisy perturbations as far from the original image, even when the $L^p$ distance considers noisy perturbations as close. The authors then define adversarial attacks by replacing the projected gradient descent with mirror descent using the learned Bregman divergence. Through adversarial training on this new learned Bregman divergence, they improve the state-of-the-art in robustness.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The authors clearly explain the pipeline of the algorithm and give great explanations for the choices they made (e.g. using equation (7) to approximate $\nabla \bar{\phi}$.)

- The authors make a good case for using Bregman divergences and learning the metric, and it seems like an interesting direction.

- The algorithm seems well-motivated at each step of the pipeline, and it looks like the authors took care to make sure each step follows theory. Figures 1, 2, and 3 are helpful in explaining the motivation.

Weaknesses:
- A big weakness is using only one dataset for comparison. Perhaps the authors could show more experiments on ImageNet-C, and/or the Berkeley-Adobe Perceptual Patch Similarity (BAPPS) that was introduced with one of the methods the authors compared against, LPIPS.
  - On the above note, there are a lot of parts to the algorithm, and it's unclear how hard one has to tune the algorithm to make sure each approximation lines up to get an overall, well-performing model. I would have wanted to see a stress test on the pipeline with larger images.
  - If the authors had comparisons on more datasets and of more difficulty than CIFAR10-C, then I would be inclined to raise the score to an accept.

- The proposed method takes longer to train than other standard adversarial training methods, as mentioned in the appendix.

EDIT: After considering the author responses and reading all the reviewers, I have raised my score from a 4 to a 5.

Limitations:
The authors appropriately acknowledge limitations where appropriate.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yURca4wi2L;"REVIEW 
Summary:
The study focuses on Video Atmospheric Turbulence Mitigation (ATM), which aims to restore videos that are affected by distortions caused by atmospheric turbulence. Specifically, the proposed ConVRT introduces a neural video representation that decouples spatial and temporal information, allowing targeted regularization of the network's temporal representation capability. Also, this paper integrates supervised and self-supervised learning, significantly improving the temporally consistent mitigation of ATM methods on diverse real-world data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The study introduces a novel framework called ConVRT for video ATM, which addresses the challenge of maintaining temporal consistency in turbulence mitigation. It proposes a neural video representation that decouples spatial and temporal information, allowing targeted regularization and effective mitigation of turbulence-induced temporal frequency variations. Furthermore, the study integrates supervised and self-supervised learning, improving the temporally consistent mitigation of ATM methods on diverse real-world data.

Weaknesses:
1.	As a neural representation method, ConVRT is designed to handle video clips with a limited number of frames, making it challenging to handle larger video sequences and more significant motions without compromising accuracy.
2.	The related work section lacks completeness. More test-time methods should be carefully reviewed. Also, no SOTA methods were involved in the experimental section for comparison, which is unacceptable.
3.	In addition to VRT, more SOTA transformer-based methods, as well as the recent-popular diffusion-based models, should be added for analysis.
4.	ConVRT employs test-time optimization, which can be computationally intensive. Therefore, model efficiency should be discussed.
5.	Some intermediate results should be given.

Limitations:
Please refer to the detailed comments!

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a method for improving the temporal consistency of turbulence-affected videos. The proposed method uses neural representations (MLP layers) to separately model the spatial and temporal deformations caused by air turbulence and is able to improve the temporal consistency of restoration results. It seems that the proposed method needs to be used in conjunction with another base turbulence restoration method. The prosed method (by combining with several other SOTA approaches) is evaluated on existing real turbulent video dataset, and a small dataset collected by the authors. Temporal consistency, especially for videos with moving objects, is apparently improved after applying the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well-written and easy to follow.
- The MLP-based network for mitigating spatial and temporal deformations is new and seems to be effective, especially on maintaining temporal consistency. 
- Qualitative evaluation is performed on videos with moving objects and have shown apparent improvement.
- A small real-world dataset was captured and used for testing, but details about the dataset are not clear.

Weaknesses:
- It seems that the method is an add-on approach for regularizing the temporal consistency of videos restored by another turbulence restoration methods. Since turbulent degradation is more complex than temporal inconsistency and spatial distortions (e.g., there might be blurriness and color aberration beyond the spatial and temporal deformations), being able to only handle these two types of artifacts seems quite limited for turbulence mitigation. 
- The quantitative evaluation results (Table 2) are confusing. This table shows metric scores for using the proposed method as a stand-alone (i.e., using the original turbulent video as input). However, the ""no base results"" are not demonstrated in any visual comparison figures (not even in supplementary materials). It would be useful to see visual comparison between ""no base results"" and others. What really confuses me about this table is that the metric scores of the original turbulent images (denoted as ""ori"") are even better than processed results in many cases (for example, its PSNR is higher than DATUM results for the HeatChamber dataset, and there many such cases). But according to visual results, most processed results have apparent improvement. Besides, after applying the proposed method, some metric scores become much worse (for example, the slice_tv scores for most datasets and method combinations). There should be some discussions explaining the metric results. 
- The paper missed some relevant prior works (see below). These works either use MLP for modeling turbulent distortions or use similar idea to enforce temporal consistency and they should be discussed and compared with the proposed method.
Li et al., ""Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation,"" ICCV 2021.
Thapa et al. ""Learning to Remove Refractive Distortions from Underwater Images,"" ICCV 2021.

Limitations:
Two limitations are discussed: 1. length of input video cannot be too long, and 2. processing time is sort of long (running time of the method is not reported). Turbulence strength and motion scale could be also discussed, if they are limiting factors.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduced ConVRT, a novel method for video atmospheric turbulence mitigation. 
This paper has a good structure and is well-written.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper proposed a new method to deal with turbulence mitigation.

Weaknesses:
1. limited real-world case visualization
2. limited proof of algorithm effectiveness
3. limited comparison with classic algorithm

Limitations:
Same with question

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an implicit neural representation (INR) framework for taking a pre-trained supervised video atmospheric turbulence mitigation (ATM) model and regularizing its output to be more temporally consistent. The main components are (1) an INR called the temporal deformation field; and (2) a subsequent INR called the spatial content field to output RGB intensity at a pixel in the video (at a certain time). These two INRs are trained on the output of a pre-trained ATM model, and regularized using a disparity loss (with MiDas pre-trained network) for temporal consistency, and a similarity loss for the content of the video. Experiments are conducted on real-world datasets with comparison to state-of-the-art ATM models recently proposed as well as some simulated ablation studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ Method can improve a variety of existing state-of-the-art ATM models, and the use of INRs with different feature representations used as inputs seems like an original contribution (at least to this application field, if not video representation in general)

+ The use of KLT tracker for visualizing the temporal variability across the frames is a good visualization and helps show the improvement of the method in a qualitative way

+ Supplemental videos on the website show the method stabilizing video in the presence of turbulence

+ Extensive quantification of the method shown in Table 2 to illustrate the effectiveness of ConVRT

Weaknesses:
- There is little detail about the spatial feature map M, temporal feature map N, canonical spatial feature map C. What does it mean to call these feature maps, and why are they chosen the way they are? For instance, why the Hadamard product for M and N, and not just learning a 3D feature map at that place instead directly? I also don't see how the C map is ""canonical"" to me in any obvious way (for instance, you could change the dimensions of Q1, Q2 and I don't see why that couldn't work in the method?). 

- The method seems to be focused primarily on fixing errors for supervised ATM methods. However, some of the classical approaches such as [Mao 2020] that utilize lucky frames, would they have this problem of temporal variability? I'm not necessarily asking for a comparison or new experiments, but it would be good to discuss if this problem primarily is for supervised methods. 

Reference: Zhiyuan Mao, Nicholas Chimitt, and Stanley H. Chan, ‘‘Image Reconstruction of Static and Dynamic Scenes through Anisoplanatic Turbulence’’, IEEE Transactions on Computational Imaging, vol. 6, pp. 1415-1428, Oct. 2020

- Table 3 is a very modest improvement . Does the Ltemp really help? A qualitative example would really help clear up that this L_temp is working (show a figure with and without Ltemp).

- How would the method handle issues such as camera shake (common in long-range videos that are shot with high optical zoom)? 

Minor suggestions:
- Line 187 - TurbNet, shouldn't it be whatever ATM method you are comparing with? 
- Line 205 - One shouldn't be capitalized
- Line 109 - unresolved citation
- Table 1 - more stylistic, but I don't think its necessary to put down the venue into the table. We shouldn't judge methods based on their venue, but on the content of the method itself, so having the citation alone is enough to let readers draw their own conclusions about the papers. I would remove this column from the table. 
- Table 3 - there is a typo in the PSNR_Img column where the lower number is bolded rather than the higher one

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yTTomSJsSW;"REVIEW 
Summary:
The paper introduces an alternative procedure for LLM alignment that does not fine-tune LLM weights, but instead learns a separate value function that is used to update hidden states. The value function is learned using a variation of temporal difference, then applied at inference time to modify hidden states by gradient ascent, maximizing the predicted state value. Authors evaluate their approach with multiple 7B LLMs on HH-RLHF data, comparing against both RLHF and training-free baselines. The paper also analyzes OOD generalization to HarmfulQA.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Authors propose an interesting approach to that can be used to alter LLM behavior in general
- When experimenting with HH-RLHF dataset, authors evaluate against multiple types of baselines and provide additional analysis that was interesting to read
- The paper is generally well-written and easy to follow
- Authors made the code available, in a (mostly) serviceable state

Weaknesses:
**1a. Motivation for the choice of baselines.**

In your work, you cite, among others, ARGS[26], DeAL [22], Value Augmented Sampling [21] that also learn value functions and use them to steer model outputs (in other ways), but, to the best of my knowledge, you do not compare against them as baselines, instead choosing a relatively older work on controlled decoding. While [21] may be dismissed as concurrent work, the other works appear to be a relevant alternative and it is not clear why they were not chosen as baselines.

If there is a reason why these works will, beyond reasonable doubt, fail at the task that you evaluate on, I would recommend that you explain this in the paper. If there is no such reason, the paper would benefit from comparing against them.

**1b. Motivation for the choice of models**

Your paper focuses on Llama, Vicuna and Falcon models, of the 7B variety. While these are indeed LLMs, the original Llama was released circa 1.5 years ago and since then, LLMs improved **significantly** across tasks.
Picking older LLMs appears counterintuitive, as their generally worse quality makes it harder to measure possible drawdowns introduced by LLM alignment.

If you have a reason for choosing these models, please explain why you focus on older LLMs as compared to, for example, Llama 3 8B (or 70B), Qwen2, Gemma or other models near the top of https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard . If there is no such reason, the paper would benefit from switching to more accurate models.

**2. Inference time exploration**

LLM use cases are often sensitive to inference throughput (tokens per second) and latency (time to first / next token).

To the best of my understanding, RE-Control introduces an iterative optimization step to each forward pass during autoregressive inference. Depending on the configuration, this may result in a significant slowdown, which may limit the practical significance of your approach.

I would argue that the work would benefit from analyzing this difference in speed in different settings (e.g. single-sequence vs batch inference, etc).

**3.  Main experiments are limited to one dataset and relatively small past generation LLMs, ranked by GPT-4**

This is definitely not a fault on authors' side, but the paper makes its main conclusions based on 7B models, using reward functions trained on a single dataset. This could result in accidental false conclusions if it turns out that, for instance, RE-Control harms the quality of stronger models or if it is somehow implicitly overfitting on on GPT4 opinions.

The standard way to minimize this risk is to diversify the experiments: try alternative alignment datasets (e.g. webgpt_comparisons, oasst1, etc), try larger models (llama-3 70B), introduce human rankings in some setups, etc. I understand that not all of these evaluations may be available to the authors, but for a NeurIPS publication, I would expect more variation in the experiments and, if there is a confounder that could not be eliminated (e.g. using GPT4 and no human eval), it should be stated among the limitations section.

Limitations:
The ""Limitations and future work"" appendix can be significantly improved. Currently, it focuses on future work and omits some limitations of the experiments, such as:
- using GPT-4 as the primary metric will make the results irreproducible once OpenAI cycles out GPT4, a closed-source model
- evaluating only on relatively weaker models (pre-previous gen, 7B) may miss some caveats or synergies from more capable LLMs
- using a single training dataset makes it possible that the proposed method is uniquely powerful in this one scenario but not others

The quality of the limitation section did not affect my score.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In their paper, the authors introduce RE-CONTROL, a novel approach designed to align Large Language Models (LLMs) through representation editing. They view LLMs as discrete-time stochastic dynamical systems and propose the insertion of control signals into the internal representations. This technique allows for precise manipulation of the model's outputs during test time token by token. The experiments show that this method increases the win rate on HH dataset and does not need significant inference time.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Viewing LLMs as a dynamical system and interpret the steering vector as a kind of controlling signal to align models is innovative.
- Make LLMs adjustable during the generation process, and the evaluation does not have to wait until the entire sentence is generated.
- They empirically show that their method outperform some test-time alignment methods and does not need significant inference time, which makes the method be more practical usable.

Weaknesses:
- Some parts of the paper are confusing, especially certain expressions. For example, they did not clarify some notations like a_t, V_{phi} etc.. The legend in figure 1 seems mismatched. And some figures are not mentioned in the paper.
- I think the performance of this method is highly depend on the value model. However, the paper does not discuss the reliability of the value model, which is crucial since it needs to assess the alignment effectiveness of the entire result based on each newly generated token and do so before the results are generated.
- The theoretical analysis and interpretation of their method is interesting, but lack rigor. e.g. the generated token (y_t) should be determined by logits (o_t), which is a part of state in the dynamic system. However, the paper interprets the generated token as kind of random variable or random noise (w_t).

Limitations:
Limitation are sufficiently discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper suggests editing language model features for alignment tasks. The authors first learn a value function of a language model from a human-preference dataset. They then increment feature representations in model layers to maximize test-time utility. Empirical evidence shows that this feature editing method surpasses both test-time and training-time alignment baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method, RE-CONTROL, is a useful middle ground between current training-time and test-time alignment methods:

- RE-CONTROL, unlike existing training-time methods, does not alter a language model’s parameters, reducing training costs. Instead, it learns a value function offline. 

- RE-CONTROL, unlike existing test-time methods, employs a learned value function to inject feature increments into features of language models.

The experiments are extensive in that they compared RE-CONTROL with both training-time and test-time alignment methods.

Weaknesses:
While the paper is technically well-executed, I believe it has three main limitations:  (i) the lack of compute--performance tradeoff analysis (ii) the lack of details in comparing RE-CONTROL with training-time alignment baselines. (iii) the limitation in application scope.

First, a compute-performance tradeoff analysis would clarify the behavior of RE-CONTROL. RE-CONTROL is more compute-intensive than other test-time decoding alternatives because it requires gradient ascent steps at decoding time (Section 4.4). These steps add up and can become quite intensive for generating long text. Therefore, comparing RE-CONTROL with test-time alignment alternatives while considering compute time would be informative. For instance, the authors could display the win rate of different test-time decoding methods on the y-axis and their wallclock time on the x-axis.

Second, I think the performance comparison between RE-CONTROL and training-time alignment methods in Section 6.1 seems very preliminary. There, the authors empirically show that the test-time alignment method RE-CONTROL *outperforms* training-time alignment methods like PPO, by concluding that

>We observe that RE-CONTROL achieves a higher GPT-4 win rate and average reward compared to both PPO and DPO. Furthermore, RE-CONTROL also outperforms these methods in terms of diversity and coherence.

I'm puzzled by how to interpret the results here. Should the take-home message here be ""Decoding-time RE-CONTROL is better than training-time PPO in alignment. Period."" or are there qualifications to this statement? I strongly suspect that some qualification is needed. To some extent, RE-CONTROL is a decoding-time approximation of PPO. Both methods use a learned value function to steer the model's behavior. At decoding time, RE-CONTROL does this in a more lossy (due to test-time gradient ascent) and shallower (because not all parameters are updated) way. Thus, with adequate training, I expected PPO to yield better results than RE-CONTROL. Note that this doesn't undermine RE-CONTROL's capability, as it is more lightweight than PPO. 


Thirdly, while RE-CONTROL is technically sound, its application scope seems narrow. To my understanding, RE-CONTROL is most appealing to users who are unwilling to train a language model offline, who are willing to train a value function offline, who aim to save computing power during training, and who don't mind using more compute during decoding. These intersections of users seem limiting. This raises the question: Is it better to simply use a similar compute budget for efficient alignment (e.g., LoRa) of the LM model using standard methods (DPO, PPO, etc.) and avoid ongoing compute costs during decoding?

Limitations:
See the ""Weaknesses"" section above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper ""Aligning Large Language Models with Representation Editing: A Control Perspective"" proposes a method for aligning large language models (LLMs) with human objectives through representation editing. Unlike fine-tuning, which is resource-intensive and unstable, or test-time alignment techniques like prompting that rely on the original model's capabilities, this method introduces external control signals into the hidden states of a pre-trained LLM. The method treats the LLM as a discrete-time stochastic dynamical system and applies control theory to train a value function on the hidden states, optimizing control signals at test time. The experiments show that this method, named RE-CONTROL, outperforms existing test-time alignment techniques and requires fewer resources compared to fine-tuning methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Innovative Approach: The use of control theory to introduce control signals into the hidden states of LLMs is novel and provides a new perspective on alignment.
Resource Efficiency: RE-CONTROL is less resource-intensive than traditional fine-tuning methods, making it more practical for large-scale applications.
Empirical Success: The experiments demonstrate that RE-CONTROL outperforms existing test-time alignment methods, showing strong generalization and alignment capabilities.
Flexibility: The method offers more flexibility than prompting or guided decoding as it perturbs the representation space dynamically during the generation process

Weaknesses:
Complexity: The method involves sophisticated control theory and optimization techniques, which might be challenging to implement and understand for practitioners without a strong background in these areas.
Dependency on Value Function: The success of the method heavily relies on the accuracy and training of the value function, which might introduce additional challenges in terms of training and performance.

Limitations:
Limited Scope: The paper primarily focuses on aligning LLMs for helpfulness and minimizing harmfulness. It might not address other important alignment objectives comprehensively.
Potential Overfitting: The reliance on a specific value function and control signals might lead to overfitting to the training data or specific tasks, limiting the method's generalizability.
Evaluation Metrics: The evaluation metrics, while comprehensive, might not capture all aspects of alignment, especially in diverse and dynamic real-world scenarios.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yS9xU6ANiA;"REVIEW 
Summary:
The manuscript introduces exogenous matching, an importance sampling method for efficient estimation of counterfactual expressions in various settings. This method transforms the variance minimization problem into a conditional distribution learning problem, allowing integration with existing modeling approaches. The authors validate their theoretical findings through experiments with different Structural Causal Models (SCMs), showing competetive performance in a range of counterfactual estimation tasks. They also examine the impact of structural prior knowledge and demonstrate the method's unbiased estimates and practical applicability in identifiable proxy SCMs.

Update: revising score upward following author rebuttal.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The topic is timely and important, as counterfactual estimation has become an increasingly popular subject in statistics and machine learning. The proposal builds on recent results in neural causal models, specifically with normalizing flows. The ability to incorporate prior knowledge in the form of Markov boundaries is especially welcome, since computing counterfactuals is often intractable without such constraints. The theoretical results appear sound (though I confess I did not go closely through the proofs) and the empirical results are compelling.

Weaknesses:
The manuscript is not always clear, probably because a great deal of material has been moved to the appendix to accommodate page count. The result is a somewhat disjointed text that would likely be better served by a journal publication than a conference paper. That said, I am generally supportive of this submission and would be willing to revise my score upward if my questions are adequately addressed (see below).

Limitations:
Limitations are adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an importance sampling method for efficient estimation of counterfactual expressions within general settings. It transforms the variance minimization problem into a conditional distribution learning issue, allowing integration with existing modeling approaches. The paper also explores the impact of incorporating structural prior knowledge, i.e. Markov boundaries, and applies the method to identifiable proxy SCMs, proving the unbiasedness of estimates and illustrating the method's practical applicability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper is well-structured, with many subsections and bullet points summarizing paragraphs.
2. The approach proposed in this paper has clear intuition and is easy to implement.

Weaknesses:
1. Contributions are not disentangled well. All three points involve experimental or empirical findings.
2. Some results of the ablation study are abnormal. First, the results show that the approach proposed is not robust. Under setting SIMPSON-NLIN and M, the inclusion of Markov Boundary Mask significantly improves ESP. However, under setting LARGEBD-NLIN and NAPKIN, the Markov Boundary Mask harms the performance, especially with backbone SOSPF. Second, the ESPs under setting LARGEBD-NLIN with backbone SOSPF are almost 0, even when $|s|=1$, which is hard to explain if including Markov Boundary Mask is effective. Third, the variance under setting LARGEBD-NLIN with backbone NICE is extremely large. 
3. Insufficient explanation or legend for figures, making it difficult for readers to understand. For example, in Figure 1, $\mathcal{M}$ with a subscripted hammer is not explained. In Figure 3, the legend does not indicate what different colors mean.
4. Hard to follow. Some terminologies need explanation or reference. For example, in line 234, *faithfulness* is not defined.

Limitations:
The authors adequately discussed the limitations in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents Exogenous Matching (EXOM), a new importance sampling method for estimating counterfactual probabilities in Structural Causal Models (SCMs). EXOM transforms variance minimization into a conditional distribution learning problem, providing an upper bound on counterfactual estimator variance as per Theorem 1. It outperforms existing methods across various SCM settings and integrates well with identifiable neural proxy SCMs for practical applications. By incorporating prior knowledge through Markov boundaries, EXOM further enhances performance, demonstrating its potential as an efficient tool for counterfactual estimation in diverse scenarios.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. EXOM provides a tractable and efficient approach for counterfactual estimation in general settings, including scenarios with discrete or continuous exogenous variables and various observations and interventions. This flexibility makes it applicable to a wide range of causal inference problems.
  
2. The method is built on solid theoretical grounds, with the authors deriving an optimizable variance upper bound for counterfactual estimators.
  
3. The authors incorporate structural prior knowledge, specifically Markov boundaries, into the neural networks used for parameter optimization. They empirically validate the effectiveness of this approach across various scenarios.
  
4. EXOM consistently outperforms other importance sampling methods in various SCM settings, as demonstrated by the experimental results. Its compatibility with identifiable neural proxy SCMs further enhances its practical applicability.

Weaknesses:
1. Theorem 1 relies on the assumption that the density ratio $q(\mathbf{u}|\mathbf{y}_ {\ast})/q(\mathbf{u}|\mathbf{y}_ {\ast}^\prime)\leq \kappa$ holds for all $\mathbf{u} \in \Omega_{\mathbf{u}}$ and $y_{\ast}$, $y_ {\ast} ^\prime \in \Omega_{\mathbf{Y}_{\ast}}$. This assumption may be overly stringent, as probability measures with infinite support sets might easily violate it. Could the authors elaborate on this assumption and provide examples of distributions that satisfy it?
  
2. In the Sampling and Optimization section, the distribution of the exogenous variable $\mathbf{U}$ is assumed to be known. However, in practical scenarios, $\mathbf{U}$ is often unknown, necessitating additional efforts to estimate $\mathbf{P_U}$ [A]​. Could the authors provide further clarification on this assumption and discuss potential methods for estimating $\mathbf{P_U}$​?
  
3. I understand the authors only consider models that provide identifiability results. However, it is encouraged to include neural proxy SCM methods based on VAE and DDPM as experimental baselines. While these may lack identifiability guarantees, comparing against them would further illustrate the superiority of the proposed method in relation to current state-of-the-art techniques.
  
4. While the method shows good performance on the tested SCMs, it's unclear how well it scales to larger, more complex causal models. The experiments are conducted on relatively small SCMs, and scalability to high-dimensional or densely connected causal graphs isn't thoroughly addressed.
  

[A] Ren, Shaogang, and Xiaoning Qian. ""Causal Bayesian Optimization via Exogenous Distribution Learning."" *arXiv preprint arXiv:2402.02277* (2024).

Limitations:
The authors discuss the limitations of their work in Section 6 and Section D.3. Notably, this work does not have a negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Based on the importance sampling methods, the authors propose an exogenous matching approach to estimate counterfactual probability in general settings. They derive the variance upper bound of counterfactual estimators and transform it into the conditional learning problem. They also employ the Markov boundaries information in the inference to improve the learning performances further. Extensive experiments validate the superiority and practicality of their method.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- This paper is clearly and well written.

- The authors give a theoretical analysis of their estimator, its log-variance upper bound in the general settings, and the counterfactual Markov boundary, and they also perform extensive experiments to demonstrate effectiveness in several cases: with two types of stochastic counterfactual processes, with three categories of fully specified SCMs, etc.

Weaknesses:
- This paper makes it clear in lines 140-144 about the assumptions needed for the proposed method. Regarding assumption ii), I think it is not mild, and I am wondering if the proposed method would be sensitive to the specified distribution $P_{\textbf{U}}$ of $\textbf{U}$. The authors might have performed such experiments, but it is not quite clear. 

- Are the Markov boundaries learned from the observational data via the d-separation, or they are given prior? The authors claimed that such Markov boundaries are structural prior knowledge in line 223, whereas they gave Theorem 3 to demonstrate how to obtain them. 

- It is suggested to offer the whole procedure or pseudo code of their proposed algorithm somewhere.

- In line 239, “augmentied” might be a typo error.

Limitations:
Not applicable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yRuJqoWoCs;"REVIEW 
Summary:
This paper introduces an SE(3)-equivariant multi-view depth estimation model based on the Perceiver IO framework. Specifically, each feature ray is treated as a token, and the feature vector of each ray is concatenated with an equivariant positional embedding. To achieve equivariance, the authors propose using spherical harmonics to encode the ray poses. Ray features are treated as type-0 (rotation-invariant) irreps. These equivariant ray encodings are processed through several equivariant self-attention layers and aggregated into global features and a canonical reference frame. The camera pose encoding is first inverse-transformed into this inferred canonical frame, resulting in an SE(3)-invariant query. A series of cross-attention layers between the encoded global features and the query features is then used to predict pixel colors. The authors demonstrate the effectiveness of the proposed approach on the ScanNet and DeMoN datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. To the best of the reviewers' knowledge, this is the first paper to address SE(3)-equivariant positional embedding for the transformer/PerceiverIO framework for multiview applications. While Fuchs et al [1]. and Liao et al [2,3]. have addressed SE(3)-equivariant attention for GNNs, their methods are more complex and computationally inefficient compared to the proposed approach.
2. The proposed method shows competitive benchmark results compared to state-of-the-art methods across multiple datasets. The ablation study convincingly demonstrates the significance of the equivariant embedding.
3. In the appendix, the authors put significant effort to make the concepts accessible to beginners,  including detailed visualizations for how the computations are done. This contrasts with typical papers on SE(3)-equivariance, which often include difficult equations that can be a barrier to entry for newcomers.




[1] Fuchs et al., “SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks,” NeurIPS`20

[2] Liao et al., “Equiformer: Equivariant graph attention transformer for 3d atomistic graphs,” ICLR’23

[3] Liao et al., “Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations,” ICLR’24

Weaknesses:
1. The authors introduced a new equivariant nonlinearity inspired by [4], but the motivation and benefits are not clearly demonstrated. What is the distinctive advantage of this new nonlinearity, compared to existing SE(3)-equivariant nonlinearities?

2. The number of parameters was not fixed during the ablation experiments regarding the maximum spherical harmonics degrees. A recent study [5] claimed that the reported increase in performance due to incorporating higher-type irreps in various works could actually be due to the increased number of parameters. It is essential to control the number of parameters to be similar between the ablated models and the proposed model.

[4] Deng et al., ""Vector Neurons: A General Framework for SO(3)-Equivariant Networks,” ICCV’21

[5] Wang et al., “Rethinking the Benefits of Steerable Features in 3D Equivariant Graph Neural Networks,” ICLR’24

Limitations:
1. Spherical harmonics can only encode the orientation of $t_i-\bar{t}$ and not the length $|t_i-\bar{t}|$. Therefore, typical SE(3)-equivariant networks address this by incorporating additional length encoding. However, in this paper, the distance information $|t_i-\bar{t}|$ is discarded.

2. Using irreps features inevitably introduces a band-limit. Increasing this band-limit is difficult because the feature dimension increases quadratically, which is also discussed by the authors.

3. The authors also mentioned that higher-degree spherical harmonics caused instability in training. However, this might be due to the choice of equivariant nonlinearities. Liao et al. [3] reported that certain nonlinearities cause instability in training.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a ray embedding representation with rotational and translational equivariance, integrating the existing Perceiver IO architecture to achieve robust multi-view implicit depth estimation. The paper first utilizes the mean shift and spherical harmonics to achieve translational equivariance, and then builds upon this to use spherical harmonics to achieve a rotationally equivariant representation, ultimately combining to obtain a three-dimensional transformation embedding with equivariance. By further designing equivariant encoders and decoders, the paper realizes robust estimation of depth from new perspectives. Experiments on the ScanNet and DeMoN datasets demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
-The motivation is clear, the algorithm design makes sense, and the experimental results are complete.

Weaknesses:
-Ablation study: Since the equivariance consists of two parts, namely translation and rotation, what would be the qualitative and quantitative impact of removing these two parts respectively?

Limitations:
see questions

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a SE(3) rotational and translational equivariant variation of Perceive IO for multi-view depth estimation with known camera poses. The authors first encode both the pixel-wise ray direction and the camera translation using spherical harmonics as the position encoding, and then to maintain equivariance under global transformations through the network's forward pass, the authors modify several components, including the linear projection, the latent array construction, and the output decoding. To demonstrate the effectiveness of the proposed method, the authors conducted experiments on several RGBD datasets, including ScanNet, SUN3D, TUM-RGBD, and Scene11, and achieved better performance than existing implicit multi-view depth estimations, such as DeFiNe, and multi-view stereo (MVS) models, such as DPSNet.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The authors introduce the problem well, explaining the importance of equivariance to the task of multi-view depth estimation effectively. They also provide a brief yet sufficient review of existing works, clearly positioning this work within the field.
- The authors have carefully designed several novel equivariant components:
  - A SE(3) equivariant positional encoding, where besides rotation, the authors smartly encode camera translation also using spherical harmonics.
  - An equivariant linear projection layer where the linear projection is applied to each group of features that corresponds to position embedding derived from the spherical harmonics of a specific order.
  - Equivariant latent array construction and the reversal of the rotation from the latent array before being cross-attended to the output queries.
  
   These designs, along with the adoption of existing equivariant components through the Perceive IO pipeline, ensure good performance and can be inspiring for other tasks that require equivariance.
- The experiments are sufficient and demonstrate the equivariance of the output and the overall accuracy.

Weaknesses:
The major weakness of this paper lies in its presentation and organization, which makes the paper difficult to read:

- Many important details from Sections 3.4 to 3.6 are placed in the appendix, making the main paper not self-contained. For instance, details in Appendices A.3 and E would be better suited in the main paper.

- Sections 3.4 to 3.6 are organized into fragmented components, where the holistic process of the Perceiver IO is missing. Specifically, the authors should introduce each modification in the order of the Perceiver IO pipeline. 

- The description of individual components are also confusing:
   - It is better to only briefly discuss components that are equivariant itself, such as attention, and discuss only how they made the input to the attention equivariant, such as the latent array in Section 3.5.1. Otherwise, it might be misleading to suggest that there are new equivariant attention modules themselves.
   - Why is only rotation sampled and encoded when constructing the latent array in Section 3.5.2 and Figure 4, while the inputs have the encoded camera translation?
   - Similarly, in Section 3.6, only reverse rotation is applied to the latents after several self-attention transformation blocks, while the translation is omitted.
   - Line 261-262: ""which allows us to leverage higher frequency information beyond the dimensional constraints of SPH."" The authors indicate that the Fourier encoding is not equivariant but use it for the output query, therefore, the authors should elaborate more on the insight behind this choice and provide sufficient proof to support this design.
  -  Many illustrations (Figures 8-11) in the appendix are confusing and do not help to clarify the equations.

Limitations:
The limitations have been sufficiently discussed in the Appendix. Q.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yRhrVaDOWE;"REVIEW 
Summary:
The paper presents an intuitive way to apply curriculum learning using diffusion based models to learn a goal distribution that can interpolate between the state-visitation distribution to states with high-value and high-intrinsic reward. As a result, the curriculum generates goals that lie at the edge of the states with non-zero occupancy, and higher value/ closeness to the target-goal.

The technical details are mostly complete and seem sound upon initial reading, I did not delve into the proofs/ derivations in the appendix. But the exposition of how we go from diffusion-models, to AIM, to visitation-count modelling, and to the newly proposed DiCuRL method, is mostly clear. 

Although there are multiple points of improvements, I think many practitioners will appreciate the author's work.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The technical details are quite clear upon initial reading, even if one is not familiar with AIM or diffusion models. I.e., the new method should be clear enough to reproduce from reading the paper.

Weaknesses:
### Major comments
 - The introduction dumps too much related work together to find the actual point and criticism that the authors want to make on the current state of the field.
 - Before reading the background/ technical details, motivation DiCuRL 1) is unclear how noising/ denoising is unique to helping exploration? Why can't another method (like a VAE, or GAN) do this through modelling the state-visitation distribution. Aren't we just choosing another, perhaps more powerful, generative method? **After reading the full paper:** I disagree that this is a sound motivation, any stochastic method over the state-visitation distribution could achieve this. I agree that modelling the state-visitation distribution is useful as it allows learning of goals that the agent has seen and can reach.  
 - 4.0 Line 222, it is not clear from the text what problem the authors are trying to solve through the graph construction and the optimization of the curiculum goal (Eq. 12). How is the 'optimal' curriculum goal even defined? Eq 12 of course shows the objective, but why do we need this? How is the graph even constructed (meaning the edges), is this fully-connected? Initial reading of this paragraph gives the impression of severe over-engineering of the goal-sampler.  
 - Figure 1 overlaps with table 1 and contains too many overlapping lines to draw a conclusion. This must be improved for presentation. Reduce the number of unnecesary baselines, show these in the appendix.
 - The results section spends most of its time speculating why the baselines perform in a certain way but does not focus on the authors' method. Line 281, states that there is a difference between OUTPACE and DiCuRL, however, neither method statistically significantly outperforms the other. Too much of the experimental setup is moved to the appendix.
 - It is unclear from figure 3 at what point during training this plot was made. Now the baseline methods look arbitrarily bad compared to the authors' method. It is color-coded, but maybe add a colorbar to figure 3 indicating the training episodes.

### Technical comments
 - 3.3 Slight confusion on the reward $r^\pi_\phi$, it's good to mention that you're actually learning $f(s)$ and using this to compute $r$.
 - 4.0 Explanation on the mixing parameter $\bar{\alpha}_k$ is omitted. Shortly state it in the main text.
 - 4.0 The definition of $g_d$ is too hidden. I infer from Alg.2 that this is supposed to represent the *true* goal distribution. 
 - Results, figure 1, table 2. Why plot the standard-deviations? Why not a non-parametric tolerance interval to get a sense of spread, or plot a confidence interval for the expected success-rate?


### Minor comments
 - Intro paragraph 1 should be split into separate paragraphs making distinct points. Not a lumpsum of information.
 - Intro paragraph 1, maybe make a distinction between hierarchical RL + curriculum RL for goal-generation. Even if HRL can implicitly generate curriculums, the motivation is often slightly different.
 - Direct reference to papers should be done with the author: 'Person et al., (year) showed ...', not '[1, 2] showed ...'. Or you could write, 'Other studies [1, 2, 3], investigated ...' or something similar.
- Intro paragraph 2 is not a paragraph but 1 sentence.
- Figure 3, since DiCuRL is mostly on par with OUTPACE this should be compared in the plot for comparing curriculum goals

Limitations:
The authors shortly discuss the limitations of their method, which I mostly agree with.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies curriculum reinforcement learning (RL) in the context of multi-goal RL, which aims to generate a series of goals with increasing difficulty to facilitate guiding learning policies. To this end, the paper proposes a framework that employs a conditional diffusion model that learns to generate a goal conditioned on the current state. The experiments in three maze navigation tasks show that the proposed method can reliably solve the tasks and perform them comparably to existing methods. This work studies a meaningful problem and proposes a reasonable framework. Yet, I am concerned with the limited domain (navigation) and tasks (maze) used for evaluation, the significance of the results, and the limited applicability beyond multi-goal RL, etc. Therefore, I am slightly leaning toward rejecting this paper, but I am willing to adjust my score if the rebuttal addresses my concern.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Motivation and intuition**
- The motivation for studying curriculum learning for multi-goal RL is convincing.
- Leveraging diffusion models to generate goals is reasonable.

**Clarity**
- The overall writing is clear. The authors utilize figures well to illustrate the ideas. 

**Related work**
- The authors provide comprehensive descriptions of existing works in curriculum RL.

**Experimental results**
- The experimental results show that the proposed method performs comparably to existing methods.

**Reproducibility**
- The code is provided, which helps understand the details of the proposed framework.

Weaknesses:
**Clarity**
- The first paragraph of the introduction is unnecessarily long, making it very difficult to follow.
- While the related work section describes several existing works in detail, it fails to differentiate these works from the proposed method exactly.

**Limited to goal-conditioned RL**
- The proposed method is limited to multi-goal RL, which requires a given goal. However, in many real-world applications, specifying a goal could be difficult or even impossible, making using the proposed method undoable. I feel it is entirely possible to extend the proposed method to the general RL setup, where only the current state is given. This will greatly increase the applicability of the proposed method.

**Evaluation is limited to the Maze navigation**
- The proposed method was only compared to existing methods in the Maze navigation tasks, where goals are represented as coordinates. It would be a lot more convincing if the evaluation was also conducted in other domains, such as robot arm manipulation, locomotion, and games. Additionally, evaluating in grid-world navigation tasks can add value to the paper by exploring discrete state and action spaces. 

**Significance of the results**
- According to Figure 1, I am not entirely convinced that the proposed method performs significantly better than the baselines. Also, the plotting scheme makes it difficult to interpret when many curves overlap.

**Related work**
- The related work section focuses on existing works in curriculum RL yet fails to discuss many works that use diffusion models for RL or imitation learning, including but not limited to
	- ""Learning Universal Policies via Text-Guided Video Generation""
	- ""Diffusion Policy: Visuomotor Policy Learning via Action Diffusion""
	- ""Learning to Act from Actionless Video through Dense Correspondences""
	- ""Goal-conditioned imitation learning using score-based diffusion policies""
	- ""Diffusion model-augmented behavioral cloning""
	- ""Imitating human behaviour with diffusion models""

**Algorithm 2**
- While Algorithm 2 is titled RL Training, Lines 15-21 are for evaluation/testing, which is a bit confusing.

**Minor errors**
- L282: It seems that a non-break newline is used here, which gives no space between this paragraph and the next paragraph starting from Line 283.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents a novel diffusion model-based curriculum learning approach, called DiCURL, for multi-goal reinforcement learning, namely goal-conditioned RL. The proposed conditional diffusion model leverages a Q-function and a learned reward function based on the Adversarial Intrinsic Motivation principle to incentivize goals that are reachable yet challenging to an RL agent. The paper evaluates DiCURL against state-of-the-art curriculum learning approaches in maze environments with differing maps. In PointUMaze and PointNMaze, DiCURL matches or slightly outperforms OUTPACE, which seems to be the best-performing method in these maze environments. In the most challenging map, PointSpiralMaze, DiCURL outperforms OUTPACE, while the rest of the methods fail to yield an optimal policy at the end of the training.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The related work section is extensive in terms of content and covers most of the recent advances in automatic curriculum learning for RL. The background and methodology sections are also detailed, and the problem setting and the proposed approach are explained clearly.

- The proposed curriculum learning approach is novel as it employs a conditional diffusion model. The idea of leveraging a Q-function and a learned intrinsic reward function to select achievable but challenging goals is intuitive, as well.

- Table 1 highlights the advantages of DiCURL, and the introduction section also supports this table.

- The curricula generated by DiCURL in Figures 2 and 3 (as well as the ones in the appendix) illustrate how DiCuRL yields optimal policies and outperforms some existing methods in evaluated environments.

Weaknesses:
- The introduction section should be improved in terms of writing. Content-wise, it is informative but also too dense. Some of the paragraphs are either too long or too short. Restructuring this section and making it more to the point would improve the readers' experience immensely. 

- OUTPACE is the second best-forming automatic curriculum learning method in the evaluated environments. However, the paper does not demonstrate the curricula generated by OUTPACE, unlike the curricula of GRADIENT and HGG in Figure 3, which do not perform as well.

- All environments (point maze domain in MuJoCo with different maps) in the empirical validation section have the same dynamics, low-dimensional state, and action spaces. Although DiCuRL's advantages seem apparent as the map gets more complex, the empirical validation is insufficient to conclude that DiCuRL can outperform state-of-the-art methods in various goal-conditioned domains.

- The roles of loss components related to the Q-function and AIM reward function sound intuitive, yet they are explained briefly. I suggest the authors run an ablation study to highlight their separate contributions.

Limitations:
I don't see any explicit limitations regarding the proposed approach and the problem setting of interest other than those discussed by the authors in the conclusion section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces DiCuRL, a novel approach that uses diffusion models to generate curriculum goals for reinforcement learning agents. The method trains a model to capture the distribution of visited states, focusing on those with higher Q-values and intrinsic motivation rewards (i.e., AIM rewards). This approach aims to generate goals at an appropriate difficulty level while guiding the curriculum closer to the desired final goal. DiCuRL employs the Minimum Cost Maximum Flow algorithm to solve a bipartite matching problem to select curriculum goals.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Strong empirical evaluation against competitors (Fig. 1)
- The paper is information-dense but reasonably well-written. It helps with the comprehension of the proposed ideas

Weaknesses:
- The approach is quite complicated and possibly unnecessarily so. I'd like to emphasize that I did not find any faults with the proposed method. It's just that I do not see how it will scale to more challenging, realistic environments.
- They missed citing a rich literature on exploration and curriculum RL. For example, see papers [1-5].
- The reward function for the Maze envs is not provided. Is this dense or sparse reward env? Note that, dense reward would not be a justifiable choice in this case.


*References*
1. Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., Wiele, T., Mnih, V., Heess, N., and Springenberg, J. T. (2018). Learning by playing solving sparse reward tasks from scratch. In International conference on machine learning, pages 4344–4353. PMLR.
2. Hertweck, T., Riedmiller, M., Bloesch, M., Springenberg, J. T., Siegel, N., Wulfmeier, M., Hafner, R., and Heess, N. (2020). Simple sensor intentions for exploration. arXiv preprint arXiv:2005.07541.
3. Nair, A. V., Pong, V., Dalal, M., Bahl, S., Lin, S., and Levine, S. (2018). Visual reinforcement learning with imagined goals. Advances in neural information processing systems, 31.
4. Korenkevych, D., Mahmood, A. R., Vasan, G., and Bergstra, J. (2019). Autoregressive policies for continuous control deep reinforcement learning. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages 2754–2762.
5. Narvekar, S., Peng, B., Leonetti, M., Sinapov, J., Taylor, M. E., & Stone, P. (2020). Curriculum learning for reinforcement learning domains: A framework and survey. Journal of Machine Learning Research, 21(181), 1-50.

Limitations:
Please suggest how this work can be extended to challenging environments with larger state-action spaces.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yRRCH1OsGW;"REVIEW 
Summary:
The paper suggests a flow-based generative framework on molecular trajectories, with various downstream tasks such as forward simulation and transition path sampling. Additionally, the model is trained in a transferable setting, across tetrapeptides.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Extensive experiments over various downstream tasks
2. Transferable settings for tetrapeptides

Weaknesses:
1. Experiment baselines

The baselines of experiments are mostly the Markov State Models. I think it would also be good if there were some comparison between other models, though I understand that many prior works targeted Alanine dipeptide not tetrapeptides.
- Forward simulation: ITO$^{[1]}$, Timewarp$^{[2]}$

- Interpolation (Transition path sampling): PIPS$^{[3]}$

2. (Minor) Necessity of additional tasks

The necessity of additional tasks relatively seems weak compared to tasks such as forward simulation, TPS, specially the inpainting design. Rather than additional tasks, ablations for stability might be a better? One can obviously see that scaling to long trajectories shows the stability against the time scale, and protein simulation shows the stability against space complexity.

**Minor typos, suggestions**

- Definition of S-MPNN only exists in the Appendix. It would great to point out that more details are in the appendix, in the first paragraph of section 4.4
- Figure 6 is not referenced in the main paper, only the appendix
- Figure 2F, reference of that blue indicates the side chains and orange indicates the backbones seems to be missing

[1] Implicit transfer operator learning: Multiple time-resolution surrogates for molecular dynamics, NIPS 2023

[2] Timewarp: transferable acceleration of molecular dynamics by learning time-coarsened dynamics, NIPS 2023

[3] Stochastic Optimal Control for Collective Variable Free Sampling of Molecular Transition Paths, NIPS 2023

Limitations:
1. Unconditional generation

As the authors have mentioned, unconditional generation is impossible since the model relies on key frames.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose MDGen -- a generative model to sample molecular dynamics trajectory conditioned on key frames. This is a direct application of video generation techniques to solve domain challenges in protein modeling. Specifically, SiT and flow matching models are used to sample SE(3)-invariant representation of all-atom protein representations. This work demonstrates the effectiveness of MDGen primarily on tetrapeptide systems, where the authors showcase four downstream application tasks including forward simulation, interpolation, upsampling, and dynamics-conditioned inpainting. 

In general, I find this manuscript well-written and easy to follow. The model performance looks reasonable on tetrapeptides, yet the results are proof-of-concept in nature and generalization to larger proteins remain challenging. However, it is one of the pioneering work in AI protein modeling to directly emulate MD simulation trajectories using data-driven approaches. To that end, I think it would be beneficial for this work to gain visibility across the research community to inspire future studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It is one of the pioneering work to adopt video generation techniques for MD trajectory generation. Although conceptually straightforward, good practices to generate time-coherent MD trajectories across different protein systems remain underexplored.
- The authors demonstrated a variety of downstream tasks using the same model architecture. The underlying modeling framework seems versatile and transferrable across different applications..
- Performance benchmark and analysis on tetrapeptides are comprehensive and provides insights to modeling these peptide systems.
- I think it is a good idea to model residue offsets relative to the key frames in order to bypass the need to learn sequence-to-structure mapping. MD simulations always start from a seed structure, so I do not think this is a key limitation as mentioned in L#310-312.

Weaknesses:
- Benchmark and evaluation results on tetrapeptides, although comprehensive, are proof-of-concept in nature. It may not be sufficient to demonstrate transferability to general protein systems.
- Performance on ATLAS (i.e., larger proteins instead of short peptides) does not seem promising. MDGen performance is worse than AlphaFlow in Table 4. I wonder if the main bottleneck is training data quality/availability, or model architecture?

Limitations:
See weaknesses and questions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new framework for generating trajectory of molecular geometries, ie, generative modeling for molecular dynamics. The paper proposes tokenization methods to tokenize the trajectory and learn flow models on the data. Experiments demonstrate the effectiveness of several tasks including forward sampling,  interpolation, and up sampling.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper tackles a new problem in molecular dynamics generation, which has not been explored in existing literature.

2. The paper is in good structure and easy to follow.

3. The paper provides a detailed analysis of several domain tasks on interested molecular structures, which demonstrate the critical usage in some scenarios.

Weaknesses:
1. Limited ML technical contribution, as all components exist in previous molecular generative models.

2. The experiment is comprehensive from a domain perspective. However, I feel the experiments lack some benchmarking comparison with state-of-the-art molecular generative models for related tasks. See my question below.

Limitations:
Limitation is nicely discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors proposed MDGen, a new framework that aims to model molecular dynamics trajectories via generative modeling techniques. By properly encoding the Protein MD trajectories according to the characteristics of key frames, MDGen adopts flow matching techniques (both continuous and discrete flow matching) to generatively model MD trajectories. As a unified framework, MDGen is able to perform diverse tasks including forward simulation, interpolation, upsampling and inpaiting. Extensive experiments are conducted to demonstrate the effectiveness of MDGen.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem this work aims to tackle is of great significance in scientific domains lie computational biology.
2. The formulation of molecular (protein) trajectories by using key frame references is reasonable and compact for reducing the modeling difficulties.
3. The experiments are comprehensive.
4. The paper is well-written and easy to follow.

Weaknesses:
1. Lack of discussion on related works. This work does not discuss related works on the same topic. Some works are mentioned in the Introduction section, but I still recommend that there should be an independent Related Works section for comprehensive discussion.  Here are also several works that are worth discussing: (1) EGNO, which uses neural operator learning approach to also model the trajectory dynamics of molecules; (2) DiffMD, which uses diffusion models to simulate molecular dynamics. The quality of this work should be further improved if the authors could carefully discuss the differences between MDGen and these works and the strengths of MDGen compared to these works.

2. Lack of ablation studies. MDGen is composed of several parts, including the design of the backbone model, the design choices of flow matching framework, and the adoption of Hyena architecture for efficiency consideration. In addition to the aimed tasks, it would further improve the quality of this work if the authors could conduct ablation studies on these aspects to help readers know what the influence of each part of MDGen is.

Limitations:
The authors carefully discuss the limitations of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a novel generative model for molecular dynamics (MD) trajectories called MDGEN. This model aims to serve as a flexible surrogate for MD simulations by generating entire trajectories conditioned on initial frames. It addresses tasks such as forward simulation, transition path sampling, trajectory upsampling, and dynamics-conditioned molecular design. The model is evaluated on tetrapeptide simulations and demonstrates its capability to generate reasonable ensembles of protein monomers.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty and Scope: The approach introduces a novel paradigm for surrogate modeling of MD, extending the capabilities of existing models to handle a variety of tasks that are not straightforward with current methods.
Generative Framework: The use of generative modeling for entire MD trajectories is a significant advancement, as it allows for a broader range of applications including forward and inverse problems.
Comprehensive Evaluation: The paper evaluates MDGEN on several tasks, demonstrating its effectiveness in forward simulation, interpolation, upsampling, and inpainting. The results show promising performance in terms of distributional similarity, dynamical content, and computational efficiency.
Technical Implementation: The detailed description of the tokenization process and the flow model architecture provides a clear understanding of how the model operates. The use of SE(3)-invariant tokens and the scalable interpolant transformer (SiT) backbone are well-motivated choices.

Weaknesses:
Complexity and Accessibility: The model’s complexity might pose challenges for reproducibility and accessibility for researchers who are not deeply familiar with both molecular dynamics and advanced generative modeling techniques.
Evaluation on Larger Systems: While the paper provides proof-of-concept evaluations on proteins, the primary focus remains on smaller tetrapeptides. The model's scalability and effectiveness on larger and more complex molecular systems need further exploration.
Dependence on Key Frames: The reliance on key frames for conditional generation limits the model’s ability to perform unconditional generation or inpainting of residue roto-translations, which could be a significant limitation in certain applications.
Computational Resources: The paper lacks detailed information on the computational resources required for training and inference, which is crucial for understanding the practical implications of using MDGEN in various research settings.

Limitations:
Reliance on Key Frames:

The model relies on key frames for parameterizing the roto-translations, which means it cannot perform unconditional generation or inpainting of residue roto-translations. This dependency might limit its applicability to scenarios where key frames are not easily obtainable or where full trajectories need to be generated from scratch.
Scalability to Larger Systems:

The architecture shows weaker performance on larger systems such as proteins compared to smaller peptides. This suggests that the current model and architecture might not be well-suited for handling the complex motions and larger size of protein structures without further modifications or enhancements.
Computational Resources:

While the paper mentions significant speedups compared to traditional MD simulations, the computational resources required for training the model (e.g., GPU hours) are not explicitly discussed. This information is crucial for understanding the practicality and scalability of the approach.

Generalization to Diverse Systems:

The current tokenization and modeling strategies are tailored to peptides and proteins. For more diverse molecular systems such as organic ligands, materials, or explicit solvent systems, alternative tokenization strategies might be necessary. This limits the immediate applicability of the model to a broader range of molecular simulations.
Limited Exploration of Additional Conditioning:

The paper primarily explores conditioning on initial frames and residue identities. Other types of conditioning, such as textual or experimental descriptors, are not explored but could open up further applications and improve the model's utility.
Data Availability and Quality:

The success of the model heavily depends on the availability of high-quality MD trajectory data. For many complex systems, obtaining such data can be challenging, which could limit the model's applicability and performance.
Evaluation Metrics:

While the paper uses several rigorous metrics for evaluation, the choice of metrics may not fully capture all aspects of the generated trajectories' quality. Additional metrics or more diverse evaluation criteria could provide a more comprehensive assessment.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce MDGen as a novel approach for modeling MD trajectories. They demonstrate the capabilities of this method in tasks such as interpolation, upsampling, and inpainting of small peptides. The accuracy as well as speed of the new approach compared to the ground truth baseline is quantitatively evaluated. Initial experiments toward upscaling to small proteins are shown.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The idea of MDGen is novel and very well presented in this manuscript. The results are convincing and interesting.

Weaknesses:
1. Parts of Sections 3.1 and 3.2 are very condensed and hard to follow. A more detailed description in the SI would be helpful, where the most important aspects of the cited work is also repeated.
2. The suitability of the chosen representation for longer amino acid chains is questionable. This is also mentioned in the manuscript, but nonetheless, proteins are mentioned many times (more than 30) in the manuscript, while almost all experiments are actually performed on very small peptides. It should be stated in a more prominent place that upscaling to proteins is not trivial.
3. The representation limits the model to learn MD trajectories of natural amino acids, as no all-atom representation is used directly. This should be made clearer in the manuscript.

Minor points: A lot of figures have no proper axis labels (e.g. Fig 3, 4, 5, 6). This should be fixed. The best models in Table 4 should be indicated in bold.

Limitations:
The approach is limited to peptides. Transfer to any other molecules is questionable due to a lack of suitable representation/tokenization.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yOe6ajdslI;"REVIEW 
Summary:
Due to a positive distribution shift, training and test distributions are not identical. However, existing AUC maximization methods don’t take it into account. To address this shift, this paper theoretically shows a new way to maximize the AUC on the test distribution by using positive and unlabeled data in the training distribution and unlabeled data in the test distribution. Finally, four real-world datasets validate the effectiveness of the proposed method.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
-	The proposed setting is novel and practical in AUC optimization. The distribution of negative data is generally stable but the distribution of positive data is more diverse or time-varying in medical diagnosis, intrusion detection, and visual inspection. 
-	The method presentation is easy to understand. This paper first introduces basic AUC fundamental knowledge. Then, it gives the problem setting of the proposed positive distribution shift. Based on this setting, the final expression is obtained through some intuitive and simple derivation. To be specific, the AUC maximization on the test distribution can be accomplished by using positive and unlabeled data in the training distribution and unlabeled data in the test distribution.

Weaknesses:
- The effect of the proposed methods on MINST and Fashion MINST datasets is not significant, which is inconsistent with those on the other datasets. The authors don’t give any explanation.
- The authors do not fully compare their method with the latest ones. For example, 
  - Positive-Unlabeled Learning with Label Distribution Alignment. (TPAMI 2023)
  - Dist-PU: Positive-Unlabeled Learning from a Label Distribution Perspective. (CVPR 2022)
  - Positive-unlabeled learning using random forests via recursive greedy risk minimization. (NeurIPS 2022)
- All theoretical derivations are only based on the sigmoid surrogate loss. As far as I know, square loss is also popular. Can the theoretical results extend to the other losses?
- There are some typos. For example,
  - In line 105, “However, these all methods assume that” should be “However, all these methods assume that”.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a method for AUC maximization in binary classification problems under positive distribution shift. They introduce their method, which is simple and easy to implement/understand, and then show it works well in some experiments.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well written and easy to understand;
- The paper proposes a well-motivated method and show how it can be easily implemented in practice;
- The experiments are convincing.

Weaknesses:
- The authors do not discuss how the classification threshold can be chosen in a practical situation under positive distribution shift.

Limitations:
The authors discuss limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers AUC maximization when the conditional probability distribution of the positive class changes in the test phase. To this end, the unbiased loss function is derived. The loss is approximated by positive and unlabeled data from training distribution, unlabeled data from test distribution, and class-prior of training distribution. In experiments, the proposed method outperformed the existing methods over the four benchmark datasets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This is the first study on AUC maximization for positive distribution shift.
- The proposed method outperformed the existing methods.
- The proposed method does not require the class-prior of the test distribution.

Weaknesses:
- Unlike the existing study [15, 42], the negative distribution is not considered.
- It lacks theoretical analyses of the proposed method.
- The extension of the proposed method is discussed but not evaluated in the experiments.

Limitations:
The limitations are discussed in Appendix A.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of maximizing the Area Under the Receiver Operating Characteristic Curve (AUC) in imbalanced binary classification problems where there is a positive distribution shift--this shift is where negative data remains constant, but positive data varies. A new method is proposed that utilizes labeled positive and unlabeled data from the training distribution, along with unlabeled data from the test distribution, to maximize the AUC effectively in the presence of such shifts.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper introduces a new loss function designed for AUC maximization under positive distribution shifts. Previous research has focused separately on AUC maximization and positive distribution shifts, but this study found the intersection of these two areas. The authors have successfully identified and explored this new research niche. The proposed loss function, derived from mathematical foundations, can be readily integrated into neural network training, offering a practical application for enhancing model performance. This paper is well-structured and clearly written, making it easy to follow.

Weaknesses:
Despite its strengths, this research primarily offers a simple proposal of a loss function, suggesting its contributions to the field might be limited. An expansion to include various metrics, such as F-1 and G-mean of TPR and TNR, which are also relevant for imbalanced data classification, could enrich this paper. Additionally, the experimental validation is somewhat restricted, utilizing only four datasets, all of which are image datasets. A more comprehensive evaluation using a broader range of datasets is necessary to fully assess the proposed loss function's effectiveness. Therefore, the reviewer believes that the contribution of this research may not be substantial enough for acceptance at a top-tier conference.

Limitations:
L1: An expansion to include various metrics, such as F-1 and G-mean of TPR and TNR, which are also relevant for imbalanced data classification, could enrich this paper.

L2: For extremely imbalanced cases, training difficulties are likely to arise. It is necessary to address how the proposed loss function can be effectively minimized in these scenarios.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yO5DVyCHZR;"REVIEW 
Summary:
This paper investigates the problem of universal online convex optimization to achieve problem dependent regret guarantees for different classes of convex functions (strongly convex, exp-concave, and convex) simultaneously. Problem/function/data dependent regret guarantees have become popular in literature to bridge stochastic and adversarial guarantees.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
S1) The paper is well written and easy to understand.

S2) The literature review is comprehensive and up to date.

S3) Simplicity of the incorporation of Bregman divergence is a plus.

Weaknesses:
W1) The contribution seems limited in that the improvement is only logarithmic for both efficiency and regret results.

W2) While the regret analysis is novel, algorithmic contribution is very limited, which leads me to believe this paper is more suitable to be a technical note.

Limitations:
No significant limitations. Necessary assumptions about the problem setting are properly addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies universal Online Convex Optimization (OCO) with gradient-variation-dependent regret bounds. That is, to design one single algorithm that is unaware of but is able to adapt to both two following groundtruth: 1) the type of curvatures: the loss functions could be convex, strongly convex, or exp-concave; 2) the curvature coefficient: exp-concavity $\alpha$ or strong convexity $\lambda$. As a result, the regret guarantee achieved by the algorithm scales with the cumulative gradient variation $V_T$ (which depends on the loss function sequence), rather than the time horizon $T$, as well as the corresponding curvature type of the underlying loss functions.

This paper proposes a new simple algorithm, that for the first time achieves optimal gradient-variation bounds for all three curvature types. Note that the gradient-variation bounds immediately imply small-loss (aka first-order) regret bounds as well as worst-case bounds. The #base learners is also improved from $(\log T)^2$ to $\log T$ due to the two-layer structure. The main result also finds broad applications including the SEA model and dynamic regret bounds.

Technique-side, the improvement comes from an alternative way to analyze the the empirical gradient variation w.r.t. surrogate losses and utilizing a negative Bregmen divergence term (due to linearization) to cancel other positive terms, which is often omitted in the analysis.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
I overall like such results. The authors present their observations and insights (from the regret analysis) in detail, leading to improved (and indeed optimal) regret bounds and even (conceptually) simpler algorithm design.

Weaknesses:
I didn’t spot any significant technical issues, and I'm just suggesting some minor “weakness”.

1. When the authors introduce the notion of $F_T$ and small-loss bound for the first time (around Eq. (1.2)), they may want to add that now the loss functions are non-negative (which I think should be necessary for all small-loss/first-order bounds?). Obviously, one can’t take squared root or logarithmic to a negative number.

2. In the application to dynamic regret, the problem setup is not clearly defined. What is the type of loss function? Is the strong-convexity/log-concavity known? It is particularly confusing since it’s right after the universal OCO setup.

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studied the problem of regret minimization of a set of functions $\{f_t\}_{t=1}^{T}$ over a compact and convex constraint set $\mathcal{X}$, i.e.,
$\sum{t=1}^{T}f_{t}(x_t) - \text{min}{x\in\mathcal{X}}\sum{t=1}^{T}f_{t}(x),$
where $x_t$ is the output of the proposed algorithm at round $1\leq t\leq T$.
The set of functions ${f_t}_{t=1}^{T}$ potentially satisfy certain curvature assumptions, e.g., strong convexity, convexity, or exp-concavity. In the paper, it is unknown which curvature assumption the function satisfies. The main goals of the paper are the following:

1. To construct a universal algorithm that adaptively acts on the curvature property of the function and achieves a proper regret bound.
2. For the case where the function is $\lambda$-strongly convex or $\alpha$-exp-concave, the algorithm should be adapted with respect to the curvature parameter, $\lambda$ to $\alpha$.
3. The algorithm should achieve a good problem-dependent regret bound: The goal of the paper is to attain a regret bound that depends on the following quantities:
$V_T = \sum_{t=1}^{T}\text{sup}_{x\in \mathcal{X}} \| \nabla f_{t}(x) - \nabla_f{t-1} (x) \|^2, \quad \text{and}\quad F_T = \text{min}{x\in\mathcal{X}}\sum{t=1}^{T}f_t(x).$

The proposed algorithm of the paper is a modification of the algorithm proposed by [1]. Similar to the approach introduced by [1], in Algorithm 1 (page 5) of the paper, the authors proposed $\emph{base learners}$ that are aggregated by a meta-algorithm, which outputs the final output of the algorithm at round $t$, $x_t$. The contribution of the paper mainly concerns the technical aspects that outperform the performance of [1] from the following points of view:

1.The paper improves the number of required base learners from $\log(T)^2$ (in [1]) to $\log(T)$.

2.This improvement of the algorithm outperforms the algorithm proposed by [1] up to a logarithmic factor for the situation where the loss functions $f_t$ are convex.

[1] Y.-H. Yan, P. Zhao, and Z.-H. Zhou. Universal online learning with gradual variations: A multi-layer online ensemble approach. In Advances in Neural Information Processing Systems 36 (NeurIPS), 2023.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper uses simple but interesting technique that contributes in tigher bounds for the case of convex losses. Inspired by [2] the authors used that exploit the imposed smoothness assumption of the loss function and Bregman divergence negative term from linearization of loss function, explained in Section 3.2.



[2] P. Joulani, A. Raj, A. Gyorgy, and C. Szepesvari. A simpler approach to accelerated optimization: iterative averaging meets optimism. In Proceedings of the 37th International Conference on Machine Learning (ICML), 2020.

Weaknesses:
The main weakness of the paper lies in its presentation. The content is too dense, and the last section on dynamic regret could be moved to the appendix. Some key parts of the paper are not well explained. For instance, it is unclear how the authors managed to outperform the number of required base learners in [1] by a logarithmic factor. Was this achieved through the application of a Bregman divergence negative term?

The contribution of the paper is limited to a simple technical improvement that enhances the achieved regret up to a logarithmic factor for convex functions.

The optimality of the result with respect to $V_T$ and $F_T$ has not been discussed by the authors.


[1] Y.-H. Yan, P. Zhao, and Z.-H. Zhou. Universal online learning with gradual variations: A multi-layer online ensemble approach. In Advances in Neural Information Processing Systems 36 (NeurIPS), 2023.

Limitations:
The authors adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study the regret minimization problem in online convex optimization without access to curvature information. They tackle the task of achieving problem-dependent optimal regret while requiring no prior knowledge of the function class (convex, exp-concave, or strongly convex). They propose an efficient-to-implement two-layer online ensemble structure that requires only one gradient query within each round. Their main technical novelty lies in providing a novel approach for gradient-variation bounds.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors tackle a very interesting problem in online convex optimization. The paper is well-written and the presentation makes it easy to follow. The main novelty lies in Sections 3.2 and 3.3, where they provide a new way of tackling gradient variations by utilizing the Bregman divergence term. They also make clever use of Proposition 1 in their analysis. The overall method utilizes techniques from several existing works and cleverly combines them to achieve an impressive bound on the regret.

Weaknesses:
The proposed approach seems reasonable to me. While I have not gone through the technical details very carefully, I seek one clarification on the proof of Theorem 1. In my opinion, the bottleneck of the proof is in showing the existence of an appropriate choice of $C_3$ and $C_4$ (page 17, line 594, 596). Can the authors comment if such a setting always exists? I would at least expect certain conditions like $\alpha_i^* > G^2/9L $ or $\lambda_i^* > 1/9L$ for results to hold.

Another small thing: I understand the authors ignore very small terms like $\log \log T$ from the order notation. It might be good to put a note in the introduction about it while presenting the result. I understand that it is there in Section 3.1 -- it might be good to move it earlier.

Limitations:
See above.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yMS7ansbr6;"REVIEW 
Summary:
This paper tackle deepfake detector problem with audio-visual data focusing on lipsync fake which generally a higher quality fake data. For that, this paper propose a dataset and a method. The dataset (AVLips) is formed using available datasets and 3 methods for the lipsync methods. The method (LipFD) extracts global and local features. For global features, transformer is utilized to get the context from all regions. For local regions, the video is cropped to different face areas: face + background, face, lips areas; and extract feature from each area. Weighting is used to select which cropped areas are more important.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
1. Contributions (dataset and method) in addressing lipsync based deepfake look sufficient.

2. Fine-grained features are considered. 

3. Analysis in real scenarios is interesting.

Weaknesses:
1. Ablation removing one or two out of the 3 branches for local feature extraction is missing. Figure 8 is just showing the important weight extracted from the overall framework doesn't show exactly how much the performance drop if the branches are not included.

2. Details are not clear (see Questions).

Limitations:
I can't find the limitations section. In the checklist, there should be limitation section however the reference to the section is broken.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on a new setting in Deepfake detection called lip-syncing fraud, which only contains fewer minor cues on the leap region. To tackle this issue, the authors provide a novel method called LipFD to obtain the features from both a global view and a regional view. Also, with the new AVLips dataset, this method shows a SOTA result compared to the recent methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This work provides a new setting on Deepfake called AVLips with a large number of high-quality samples.
2. The method mainly focuses on generating the features from the lips region which is novel.

Weaknesses:
Although the proposed method shows a good result, there are some confused expresses which may bring a hard understanding to readers:
1. For equation 3, what is $RA(\cdot)$ mean? What is $[F_G|\{F_R\}^i_j]$ means? There lack an explanation of these operations.
2. It will be better to have an ablation study on the selection of a vision transformer. Including the pretrain, the structure, etc.
3. It could be better to have more details about the dataset, including the number of samples, the visualization of samples with different methods, etc.

Limitations:
No, the author did not include. The authors should address limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The proposed work introduces a pioneering method for detecting lip-syncing forgery, an often overlooked threat in current research. By leveraging discrepancies between lip movements and audio signals, a dual-headed detection architecture significantly enhances detection accuracy. This work also contributes to the first large-scale audio-visual LipSync dataset, comprising nearly one hundred thousand samples, and conducts extensive experiments that demonstrate our method's efficacy. Results show up to 94% average accuracy in LipSync detection, with robust performance in real-world scenarios.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. this work proposes a new research problem -- lip forgery detection, which is meaningful and useful. A dataset for this research problem is also proposed.

2. The anonymous github makes this work very convincing.

3. The real-life applications shown in Fig. 6 is very impressive.

Weaknesses:
the proposed algoritm, LipFD does not have a strong techincal novelty in learning region and global features from the multi-modal input.

Limitations:
This method might be under-optimized solution for the facial forgery detection.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel method, LipFD, dedicated to detecting lip-syncing forgeries by exploiting temporal inconsistencies between lip movements and audio signals. This unique approach addresses a significant gap in existing DeepFake detection methods. Experimental results demonstrate that LipFD achieves high accuracy across multiple datasets, showcasing its effectiveness and robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper addresses a novel problem by focusing on specific DeepFake types that are challenging to detect with current DeepFake detection algorithms but perform quite well in state-of-the-art models.
- The paper is well-written and easy to follow. Experimental results indicate the effectiveness of the proposed method.
- The proposed dataset provides a solid foundation for further research in this field.

Weaknesses:
- The diversity of fake videos in the training set is limited, as it only includes three methods: MakeitTalk, Wav2Lip, and TalkLip. This limitation can lead to overfitting, as the classifier may easily learn the distinct patterns of these methods. For example, Wav2Lip produces blurry lip images and shows obvious artifacts when fusing lip and facial images. To demonstrate generalizability, testing on additional state-of-the-art generation methods is encouraged.
- While the method performs well on the proposed LipSync dataset, there is some variability in performance across different datasets like FF++ and DFDC. This indicates potential limitations in generalizability across diverse datasets, possibly due to the limited variety of fake videos in the training set. A robust model should be capable of detecting both LipSync and general DeepFake videos effectively.

Limitations:
No.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yKvHJJE9le;"REVIEW 
Summary:
The authors propose a time-varying extension of SAFEOPT to overcome the problems of time-varying rewards under time-varying safety constraints.

Under stationarity conditions, optimality guarantees are provided and the numerical simluation shows a (favorable) comparison to the SAFEOPT.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is very well written and easy to follow.

2. Based on related work, the problems of time-varying rewards under time-varying safety constraints are an open problem in literature, and his paper addresses that.

3. The paper provides formal safety guarantees for their TVSAFEOPT algorithm.

Weaknesses:
1. *Some delineation to related work seems rather vague and requires stronger justification.* An example for TVSBO: the time-variable and temporal aspect of the kernel can just as well be interpreted as context using existing results. Perhaps a table would help here to highlight key aspects. 

2. *Lack of real-world data experiments and comparison to related work.* To support the downsides of existing approaches, an empirical comparison to existing TVSBO approaches mentioned in the related work section would be needed.

3. The *empirical results could be more convincing* by adding a variety of initial safe sets and revised plots. The current plots/results are hard to parse. 

4. It would be beneficial if the *theoretical/technical challenge of extending safety to the time-varying case were more detailed*. This would streamline the presentation and help in assessing the impact of the contribution.

Limitations:
1. Practicality of the safety guarantee: requiring many Lipschitz constants for both for space and time while also requiring an RKHS norm bound.

2. Theoretical and empirical impact: Lack of comparisons to TVSBO approaches makes the implications of the contribution unclear both theoretically and empirically.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a safe Bayesian optimization algorithm TVSAFEOPT with a spatial-temporal kernel and time Lipschitz constants, which improves on SAFEOPT with time-varying reward and safety constraints. The optimality guarantee is proved for the stationary case and the safety guarantee for more general settings. The method is tested on a synthetic problem and gas compressors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The use of a spatio-temporal kernel in Bayesian optimization for time-varying safety constraints is novel.
2. A formal proof of safety and optimality guarantee under certain assumptions.

Weaknesses:
1. More discussion on how to make a tradeoff between optimality and safety is encouraged. 
2. Will this conservatism in safety become too large in high-dimensional problems?
2. The method to choose the proper initial safe set and kernel parameters is unclear.

Limitations:
The societal impact is discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the TVSAFEOPT algorithm, which is based on Gaussian processes with spatio-temporal kernels, designed specifically for optimizing time-varying rewards under time-varying safety constraints. The algorithm provides formal safety guarantees in a general time-varying setting, ensuring safety even when exploring non-stationary safe regions. It robustly subtracts safety margins to prevent unsafe decisions, adapting in real-time to changing environments. Furthermore, they provide optimality guarantees for locally stationary optimization problems, ensuring near-optimal solutions when the optimization problem becomes stationary.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
They provide formal safety guarantees in dynamic environments, ensuring safe decision-making even in non-stationary settings. 

Additionally, the algorithm offers optimality guarantees for stationary optimization problems, enhancing its reliability and performance

Extensive numerical simulations were provided to validate the proposed approach.

Weaknesses:
They extend the Safeopt algorithm from literature. However, it is clear on what are the additional contributions and difference between these two different approaches.

Limitations:
-

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yDo1ynArjj;"REVIEW 
Summary:
This work presents Diffusion Forcing, a new framework for probabilistic sequence modeling that combines diffusion models with Bayesian filtering. This framework builds on state of the art approaches to sequence modeling using diffusion models, but has several novel contributions.
First, it allows the model to use *independent* noise levels per element in the sequence, which is a key factor for the stability of autoregressive generation and conditional, guided generation.
Second, this work casts the proposed method for sequential decision making, by defining a new guidance technique that allows the generation of the next toke by guidance on the full distribution of future tokens.
Third, the authors go at great length in demonstrating empirically that their proposed framework is general and can be applied beyond text generation, as opposed to related work.
Diffusion forcing relies on simple ideas: noising is understood as (partial) masking, and it is cast for sequential data, giving rise to a causal variant of diffusion forcing. In practical terms, we have a dynamical system modeled with a simple RNN, in which hidden states follow the Markovian principle: the next hidden state depends on the previous hidden state and a current observation. Previous, next and current are to be intended as indexes in the sequence. Observations are obtained by running a diffusion model with independent noise levels per sequence index, and noisy observations can be used to transition to the next hidden state. A connection with Bayesian filtering is made clear in the paper. Then, we end up with an observation model (for state transitions) and a denoising model (for the diffusion of the observations).
The authors provide a sound development of the training procedure and objective, by showing that their training algorithm optimizes a weighted ELBO on the expected log-likelihood.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This work presents substantial improvement over the literature on the joint application of diffusion models and autoregressive models
* The proposed methodology is technically sound, and well supported by intuition, formal proofs and a wide range of experiments
* The experimental section expands over the literature by focusing on several domains including long-range video generation, planning, compositional generation and multivariate time series forecasting

Weaknesses:
* The intuition of the effects of noising on long-horizon generation (appendix B.2) is very similar to the ideas described in a related work AR-Diffusion [62]. This does not highlight the contribution of *independent* noise levels per sequence index
* Experiments do not compare (at least to the best of my understanding) Causal Diffusion Forcing to AR-Diffusion, which would be the natural competitor. Nevertheless, I understand that this would require considerable amount of adaptation work, since AR-Diffusion tackles language modeling mainly
* I liked Appendix B.6, but it is not referenced in the main, and I think this would be more helpful than the figures in sec 3.1

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce Diffusion Forcing (DF), a method for diffusion of sequential data where the noise level at each token can be different (“independent”). The authors show that DF provides more flexible steerability properties and more stable rollouts compared to full-sequence diffusion and teacher forcing. Experimentally, these enable stable video prediction along several timesteps, improved performance on planning tasks, and more robust robotic visuomotor control, relative to the respective relevant baselines.

The following is a more detailed summary.

### Overview of the method

Diffusion Forcing (DF) denoises a sequence of noisy tokens $x^k\_1, \cdots, x^k\_T$ at noise level $k$, starting at $k=K$ (maximum noise) and finishing at $k = 0$. A sequence of hidden states $z\_1, \cdots, z\_T$ is also maintained throughout. Importantly, different tokens can be denoised by different amounts at each denoising step.

The architecture has two main components: 
an encoder $p\_\theta(z\_t | z\_{t-1}, x^{k\_t}\_t, k\_t)$ mapping the previous hidden state $z\_{t-1}$, the current noisy token $x^{k\_t}\_t$ and the noise level $k$ to the new value of the current hidden state $z\_t$;
a denoiser $\epsilon\_\theta(z\_t, x^{k\_t}\_t, k\_t)$, which is used to denoise $x^{k\_t}\_t$.

At training time, the noise levels $(k\_t)\_{1 \leq t \leq T}$ are sampled independently, and the encoder and denoiser are trained jointly using the usual diffusion loss on the output of $\epsilon\_\theta$.

At inference time, the tokens are initialized with independent Gaussians. They are then denoised by first computing hidden states from left to right (via an RNN, in this case) using $p\_\theta$, and then by updating the values of the tokens using their current values and the hidden states.

The authors provide an ELBO interpretation for their loss function in the appendix.

### Features of Diffusion Forcing 

- The authors highlight the following features of DF:
- It supports classifier guidance, like ordinary diffusion;
- It allows for keeping the noise level higher for future tokens. This makes intuitive sense in an auto-regressive setting, where future tokens depend on past tokens.
- It supports a flexible planning horizon, as tokens are denoised sequentially.
- It supports a more flexible form of classifier guidance (or reward guidance): past tokens can be guided by rewards that depend on future tokens, due to DF’s autoregressive architecture.

When doing reward guidance, the authors propose drawing many samples of possible future trajectories, and averaging their rewards, rather than using a single sample as in ordinary classifier guidance. They term this approach Monte Carlo Tree Guidance (MCTG).

### Overview of experimental findings

- The authors evaluate Diffusion Forcing on video prediction, planning and robotics tasks. Their findings can be summarized as:
- In video prediction (datasets: Minecraft gameplay and DMLab), DF provides more stable rollouts than full-sequence diffusion and teacher forcing. In particular, DF’s rollouts do not diverge as the number of tokens increases.
- In planning (environment: Maze2d from D4RL), DF produces more more consistent trajectories, and executing the generated actions indeed produces a trajectory similar to that given by the generated states. - In addition, DF with MCTG significantly outperforms Diffuser on Maze2d environments.
- In robotics, DF is robust to missing or noisy observations and can perform imitation learning with memory (as it maintains a hidden state, rather than directly mapping observations to actions).
In the appendix, the authors provide additional experiments on compositionality and time series prediction.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The authors propose an original and performant method combining strengths of diffusion (steerability, robustness to noise, high-quality gradual sample generation) and auto-regressive sequence modelling (flexible horizons, temporal causality, memory in the case of RNNs).

1. In addition, the authors provide a theoretical justification of their loss function in terms of an evidence lower bound (ELBO).

1. The paper is written clearly, providing a clear motivation for the authors’ approach, contextualizing DF relative to existing work (especially Diffuser, AR-Diffusion and Diffusion Policy), and highlighting the main contributions of the method conceptually and experimentally.

1. Trajectory inconsistency is a major limitation of Diffuser, which I have contended with in my own research. Mitigating this limitation is an important enabler of bringing the strengths of diffusion to bear in sequential decision making.

1. Monte Carlo Tree Guidance can be seen as maximizing an empirical estimate of the expected future reward. From a policy optimization perspective, this seems more principled than doing gradient ascent on the realized cumulative reward of a given trajectory, as is done in full-sequence diffusion (e.g. Diffuser). As the authors explain in Appendix B.3, this technique relies on the architecture of DF to be effective.

1. The results on video prediction, available in an anonymized project website provided in the abstract, are particularly impressive in terms of stability and 3D consistency. This, together with results on planning and robotics, indicates DF might contribute to advances in diffusion world models; a research area of established relevance that has received significant attention recently.

Weaknesses:
1. Clarification on classifier guidance term $\nabla\_x \log c (x^{\textrm{new}}\_{1:H})$: If this term is to be understood as the gradient of $x \mapsto \log c(x)$ evaluated at $x^{\textrm{new}}\_{1:H}$, then the gradients of $c$ on future tokens would not flow to previous tokens, as the inputs $x$ are “frozen” before being fed into $\log c$. It seems that what the authors mean to say is that future tokens are treated as a differentiable function of  past tokens when computing the gradients. It would strengthen the exposition if the authors either clarify this point in the paper, or update the notation to avoid confusion, as the current notation might lead the reader to believe that the gradients from future tokens do not flow into past ones.

1. The naming of Monte Carlo Tree Guidance seems to misleadingly suggest a similarity with Monte Carlo Tree Search (MCTS). However, the method consists of sampling several future trajectories independently and averaging their guidance gradients, which seems quite divorced from MCTS, which involves actual search on a tree of states and actions and backpropagation of rewards through this tree. As such, I believe naming the technique Monte Carlo Guidance would be more appropriate.

1. High-dimensional control evaluation: Janner et al. (2022) evaluate Diffuser on high-dimensional control locomotion tasks from D4RL. It would be interesting to see an evaluation of Diffusion Forcing in this setting, in particular regarding the consistency between states and actions. I recall from my own experience that executing longer plans from Diffuser in these locomotion environments in an open-loop fashion (i.e. no re-planning) led to trajectories diverging from the generated states, as noted by the authors. It would be interesting to see whether this is addressed by Diffusion Forcing on these higher-dimensional environments.

1. The compositional generation environment referenced in Section 4.3 is very similar (if not identical) to the one used by Janner et al. (2022) in Figure 1b of their paper. I believe it is likely worth mentioning this in Section 4.3.

1. Minor formatting problems
    
    1. Line 186: $x^{K/2\_3}$ -> $x^{K/2}\_3$
        
    1. Table 1 caption: “Diffusion Forcingkeeps” -> “Diffusion Forcing keeps”; “Diffusion Forcingachieves” -> “Diffusion Forcing achieves”
        
    1. Line 495: “in full abstraction” -> “in full generality”
    Line 503: “likelihood for likelihood of all” -> “likelihood for all”
        
    1. Equation A.3: superscript $k\_2$ on the LHS should be $k\_s$
    Line 516: revise the bracketing of the expression involving $p\_\theta$.
        
    1. Line 522: “under uniform levels” -> “under uniformly sampled levels”
        
    1. Line 524: “in the sequel” -> “in the following section”
    Equation A.5: $s \leq T$ -> $1 \leq s \leq T$
        
    1. Line 592: specify range for $s$ on the first expectation
        
    1. Line 598: correct superscripts $t\_k$ to $k\_t$
        
    1. Line 608: revise bracketing of the numerator inside the $\ln$
        
    1. Line 616: In the last and penultimate lines, replace $\frac{\ln p(...)}{q(...)}$ by $\ln \frac{p(...)}{q(...)}$
        
    1. Line 628: “we” -> “we have”
        
    1. Line 631: correct superscript of $x\_t$ on the second line
        
    1. Line 634: expression with $p\_\theta$ broken between lines
        
    1. Line 635: capitalize Dirac
        
    1. Equation B.1: include \left[ \right] in the brackets
        
    1. Line 664: “we are” -> “we use”

Limitations:
The authors address relevant limitations in Section 5 and social impacts in the checklist.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to augment autoregressive models with diffusion. Specifically, rather than generating every token in one shot (one neural network evaluation), the paper proposes to gradually denoise the tokens following an autoregressive order. That is, every token is given a different noise level (lower for former tokens and higher for latter ones), and the tokens are jointly denoised to generate better samples. Compared to pure autoregressive prediction, diffusion forcing allows the model to refine the samples through the diffusion process. Compared to diffusion models, the proposed model is capable of variable-length generation and extrapolation.

The authors also demonstrate additional potential generation tasks that can be done by diffusion-forcing models such as guided autoregressive sampling. 

Empirical results demonstrate that diffusion forcing performs well on video prediction and various planning tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes an interesting combination of autoregressive models and diffusion models and demonstrates that the combination of both outperforms both individual models in terms of performance. Further, the diffusion-forcing paradigm offers many more applications that are otherwise impossible. For example, while doing variable-length generation, the model can leverage classifier-based/-free conditions. This provides much better flexibility to inference-demanding tasks such as planning and control.

The authors propose a training objective of diffusion forcing models based on noise prediction. The objective is proved to be a reweighted version of the evidence lower bound and thus is sound.

Diffusion forcing achieves much better performance compared to autoregressive models and diffusion models in long-horizon generation tasks.

Weaknesses:
A more detailed discussion of the noise schedule is desired to better understand the effectiveness of diffusion forcing. Is it necessary to use different noise schedules in different tasks to achieve good performance? Further, can we train the model with various/arbitrary noise schedules and at evaluation time find a good schedule? If any of these is possible it will greatly reduce the training complexity and extend diffusion forcing to more applications.

Theorem 3.1 states that the proposed objective is equivalent to a reweighting of the evidence lower bound. However, it is unclear how the noise schedule biases the reweighting since a very badly balanced ELBO can render the training process unstable.

How diffusion forcing balances efficiency and performance. In the extreme case where only one denoising step per token is allowed, diffusion forcing reduces to autoregressive generation. How much performance gain can we expect if we allow for more computation time?

Limitations:
As discussed by the authors, one of the main limitations is that diffusion forcing is only tested with RNN base models but not other autoregressive models such as autoregressive transformers.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Diffusion Forcing, a novel training paradigm for sequential generative modeling using diffusion models. Diffusion Forcing learns from sequential tokens with varying independent noise levels, enabling more flexible sampling strategies and general capabilities such as guidance. The experimental results demonstrate that Diffusion Forcing outperforms existing methods, including full sequence diffusion and teacher forcing, across various tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed Diffusion Forcing method is general and flexible, making it applicable to various tasks.
2. The paper provides a comprehensive discussion on the capabilities of Diffusion Forcing.
3. The experiments are well-designed and effectively demonstrate the proposed method's effectiveness.

Weaknesses:
1. **Writing clarity and organization.**  
   The writing style impacts readability, making the paper challenging to follow. It would benefit from a clearer organization. The paper primarily covers three points: (a) the proposed Diffusion Forcing (DF) method with independent noise levels and its theoretical analysis, (b) the capabilities of DF, including flexible sampling strategies, and (c) experimental results on various tasks. However, the current structure does not clearly present these points, particularly the DF method. Separating the design of DF and the intuitive explanation from the Bayesian filtering perspective, and listing the resulting capabilities in a separate section, would enhance clarity.

2. **Clarity of figures.**  
   The figures are not well-explained and are difficult to understand without referring to the text. For instance, Figure 1 omits latent states in the sampling process for both Diffusion Forcing and Teacher Forcing, which is confusing.

3. **Minor issues and typos.**  
   - Line 97: missing a "")""
   - Line 139: ""nevel"" should be ""level""
   - Line 186: ""$x^{K/2_3}$"" should be ""$x^{K/2}_3$""
   - Line 178, 184, etc.: paragraph titles are inconsistently formatted
   - Line 522: missing a ""(""

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yDjojeIWO9;"REVIEW 
Summary:
This work discusses an interesting security issue of deploying a model fine-tuned on a large foundational model in private downstream applications. It proposes a universal meta-initialized and gradient robust adversarial attack (UMI-GRAT) to break the powerful SAM and its various downstream models, without requiring prior knowledge of the specific downstream task and data distribution. The author explores the challenges associated with threating transfer-based adversarial attack without the task-related prior knowledge and provides the theoretical insights on the deviation in updating the adversarial perturbation when using the open-sourced model as the surrogate model. An extensive evaluation of UMI-GRAT's performance, transferability, and efficiency was conducted across five datasets and three different downstream tasks (medical image segmentation, shadow segmentation and camouflaged object segmentation), demonstrating the high effectiveness of the UMI-GRAT approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work discusses a critical adversarial issue of deploying large foundation model in real-world applications and for the first time considers a more challenging and practical scenario where the adversarial attacker breaks SAM and its downstream models in the absence of prior knowledge on the task and data distribution.
2. This work provides the in-depth analysis on the challenge of threating the transferable adversarial attack via open-sourced SAM and proposes the corresponding theoretical insights and solution.  
3. The work establishes a detailed experimental framework and the proposed UMI-GRAT shows superior performance on misleading various SAMs’ downstream models compared with previous methods, which serve as a preliminary exploration for future research.

Weaknesses:
1.	It’s recommended to give more comprehensive analysis of the UMI noise, including the size of the natural image dataset and the effect of various hyperparameters.
2.	There are more metrics such as $E_\phi$, $F_\beta^\omega$ in the camouflaged object detection task. It would be beneficial if the author could provide further data pertaining to these evaluation metrics to enrich the analysis.

Limitations:
The experimental results are all based on SAMs and their downstream models. It will provide more valuable insights if expanding the scope of analysis to assess whether this adversarial threat also applies to other large foundation models.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors present a new approach for adversarial attacks on Segment Anything Model (SAM)-based downstream models, addressing the challenge of attacking without prior knowledge of the downstream task or data distribution. Their key contribution is a universal meta initialization-based algorithm that exposes inherent vulnerabilities in the foundation model. The authors also introduce a gradient robust loss, which simulates uncertainty through gradient-based noise augmentation. This loss is derived from a theoretical formulation of adversarial update deviation between the open-sourced SAM and its fine-tuned downstream models. The authors provide an analytical demonstration of how their proposed method enhances attack transferability. The effectiveness of their approach is thoroughly validated through comprehensive experiments.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Originality: This is the first work to explore the feasibility of adversarially attacking various downstream models fine-tuned from the Segment Anything Model (SAM). The introduction of a universal meta initialization-based algorithm to uncover intrinsic vulnerabilities in foundation models is both effective and efficient. Additionally, the formulation of adversarial update deviation and the proposal of a gradient robust loss that simulates uncertainty with gradient-based noise augmentation further enhance the transferability of adversarial examples.

Quality and Clarity: The writing is generally clear but has room for improvement. The methodology and results are well-structured, though some technical sections could benefit from additional clarification.

Significance: This work is highly significant given the increasing prevalence of foundation models like SAM. The proposed methods for enhancing attack transferability have important implications for AI system security and could influence future directions in both offensive and defensive strategies in adversarial machine learning for SAM.

Weaknesses:
1 - My major concern is related to the novelty of the proposed approach. Although I agree that this is the first work in the context of SAMs, the main components, such as downstream agnostic adversarial examples and meta learning-based fast initialization, have already been proposed in the literature.

2 - The authors, in line 45, briefly highlight downstream agnostic examples in just one line. They should clarify in the related work section how their work is different from references 55 and 56 of the main paper, beyond just applying it to SAM. Similarly, another related work that the authors missed is [1] (given below), in which the generated adversarial examples are agnostic to downstream tasks.

3 - Similarly, the authors did not mention any work related to meta-learning-based adversarial examples in the paper. There are multiple works that use meta-learning to craft universal adversarial examples, such as [1, 2] below. The authors use these meta-learning-based methods for initialization of adversarial examples, but this has already been explored in [3] below. The authors should mention these meta-learning-based approaches in their paper and discuss how their method is different from these approaches, beyond just the application to SAMs.

4 - It is not clear to me when the authors claim in line 8 that they are attacking ""without accessing the downstream task."" What is the task here? Is it not the segmentation task? In [1], their task-agnostic adversarial examples are effective against classification, detection, and segmentation. Since the downstream task here is segmentation-based, is it not obvious what the task is? Please clarify this.

5 - The authors should include some specific aspects of SAM to make their attack more unique. Currently, they are utilizing the SAM image encoder, which, in my opinion, is not much different from the previous works listed below.

6 - For experiments, why have the authors compared their method with intermediate-level feature-based approaches? They should also compare it with different downstream agnostic adversarial approaches as listed below.

7 - In Equation 8, how did the authors choose the threshold lambda? 

[1] A Self-supervised Approach for Adversarial Robustness (CVPR-2020)

[2] Learning to Generate Image Source-Agnostic Universal Adversarial Perturbations (IJCAI22)

[3] Meta Adversarial Perturbations (AAI2022-Workshop)

[4] Adversarial Initialization with Universal Adversarial Perturbation: A New Approach to Fast Adversarial Training

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an adversarial attack against fine-tuned derivatives to a publicly available foundation model, such as the Segment Anything Model (SAM). In the proposed threat model, attackers can potentially manipulate these downstream models even without knowing the specific task or data they are used for. Under this threat model, proposes a new attack method called UMI-GRAT (Universal Meta-initialized and Gradient Robust Adversarial Attack). Through a bi-level optimization procedure, this method leverages the information from the open-source SAM to create adversarial examples that can fool mislead the original SAM and its fine-tuned versions. Finally, this paper demonstrates the effectiveness of the proposed UMI-GRAT attack against SAM through extensive experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is motivated by real-world safety concerns for fine-tuning a public foundation model on private domain-specific datasets.
2. The figures and tables are well-polished and generally reflect the overall message of the paper.
3. The proposed UMI-GRAT attack method is unique and backed by theoretical analysis.

Weaknesses:
1. The effectiveness of the proposed attack is only demonstrated by attacking the SAM model. However, more experiment settings (e.g. against pretrained MAE models) are warranted to demonstrate the generalizability of the proposed attack.

Limitations:
The authors acknowledge the limitations of this work in the appendix. They candidly acknowledge the limitations in evaluations as the proposed attack is only evaluated against SAM. I appreciate the authors openly acknowledging this limitation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the vulnerability of Segment Anything Model (SAM) and its downstream models to transferable adversarial attacks. The authors propose a novel attack method called Universal Meta-Initialized and Gradient Robust Adversarial attack (UMI-GRAT) that leverages the open-sourced SAM to generate adversarial examples effective against fine-tuned downstream models, even without access to the downstream task or dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper tackles a practical and challenging problem of attacking downstream models fine-tuned from a publicly available foundation model without knowledge of the downstream task or data.
2. The proposed UMI-GRAT method is well-motivated and technically sound. The authors provide theoretical insights into the gradient deviation problem and propose a robust solution using gradient noise augmentation.
3. The paper presents extensive experiments demonstrating the effectiveness of UMI-GRAT in attacking SAM and its downstream models

Weaknesses:
See Questions.

Limitations:
Limitations are discussed in the Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose an adversarial attack method that can contaminate downstream tasks from the perspective of adversarial transferability. They address the problem that SAM models do not have similar optimisation routes after fine-tuning for different downstream tasks by designing universal meta initialization. In this paper, the authors address the problem that SAM models do not have similar optimisation routes after fine-tuning for different downstream tasks by designing UMI noise. The authors introduce the idea of meta-learning to allow their algorithm to quickly adapt to different situations, i.e., downstream tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The theoretical part of this paper is detailed, the experiments are sufficient. The comparison with other methods shows the sophistication of their approach.



2. The attacks proposed in this paper are novel. It contributes to the topic of attacking downstream tasks of large models. A discussion on adversarial transferability is introduced under this topic.

Weaknesses:
1. The readability of the Methodology section of this article is somewhat poor. The authors define the problem to be solved through the form of propositions. Similarly, if the authors could summarise the formulas as Theorem and put the proof process (both formulas and reasoning) specifically in the supplementary material, it would make the article more coherent.



2. The randomness of the experimental results is unknown. I understand that due to the larger computational effort, it is not practical to report error lines on all major experiments. But it would be better for the authors to report a set of randomness on a smaller dataset and simpler settings, which will influence the reviewers' opinion of the results of this method.

Limitations:
The authors correctly list the implications of their work for the use of large models such as SAM in downstream tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yCh1z6Dcto;"REVIEW 
Summary:
**Context**. The focus of the present paper is on-device fine-tuning (gradient computation and weight update **starting from a pre-trained model**) under limited memory budget. One way to cut the memory cost of storing the computational graph for gradient computation by standard backprop is the Memory Efficient Zeroth Order (MeZO) optimizer [Malladi et al, 2023], whereby a directional gradient is computed via weight perturbation (a.k.a SPSA): computing the loss $L$ difference yielded by two forward passes with weights differing by $\epsilon u$ estimates $\nabla L \cdot u$. Since it is a purely forward procedure, it obviates the need to cache activations to execute a backward pass.

**Core contribution**. The present paper proposes a quantized version of MeZO where weight perturbation, gradient computation and weight update are carried out on quantized quantities. The proposed algorithm, coined QZO-FF (Alg. 1), is tested against a variety of fine-tuning tasks (few-shot learning, cross-domain and in-domain adaptation), modalities (image and audio data), and architectures (convolutional, attention-based, recurrent), with several variants being explored (with fp8 / fp32 activations) and benchmarked against standard backprop. The efficiency of QZO-FF, both in terms of resulting performance and memory usage, is demonstrated.

**Paper outline**. More precisely:
- Section 2 provides background knowledge on memory-efficient backprop (2.1), forward-mode differentiation (2.2) and quantized training (2.3).
- Section 3.1 and 3.2 formalizes further ""forward gradients"" (3.1)  and the SPSA / weight perturbation procedure to estimate them (3.2). A hardware-friendly extension of SPSA coined as ""Sign-m-SPSA"", which estimates $\text{sign}(\nabla L \cdot u) u$, , is introduced along with the resulting SGD update (3.2).
- Section 3.3 presents the core algorithmic contribution by combining SPSA / weight perturbation and weight quantization (Alg. 1). More precisely, weights and perturbation are statically, symmetrically quantized (e.g. their range are estimated and set once, before fine-tuning), with one scale for each ($\Delta_w$ and $\Delta_q$). Therefore: i) $\Delta_w$ and $\Delta_q$ are fixed, with weights and perturbations quantized with 16 and 8 bits respectively, ii) the integer part of the perturbed weights is accumulated in 32 bit, iii) the dequantized perturbed weights is quantized-dequantized back into 16 bits using the same $\Delta_w$ scale (Eq. 6). The Sign-m-SPSA gradient estimator is applied and quantized-dequantized using the perturbation scale ($\Delta_z$, Eq. 7) . Finally, the weight update itself is quantized, such that it happens in the quantized integer part and is rescaled by $\Delta_w$ (Eq. 8). Alg. 1 summarizes the procedure in the case where the number of perturbed directions at use is 1 ($m=1$).
- Section 3.4 presents several algorithmic ""enhancements"" of the QZO-FF algorithm to improve the optimization procedure itself or its memory footprint.
- Section 4 presents experimental results. First, few-shot learning is considered (4.1) on visual and audio data. Here, ""FF"" refers for short to ""QZO-FF"". A quantized version of FF, where 8 bits activations are used, is also tested. On vision, three architectures are tested (ResNet12, 18 and ViT tiny) on 5 different standard few-short learning datasets. Two scenarii are considered: full fine-tuning and linear probing. It is shown overall that FF always yields better performance than the zero-shot baseline and within 5%, accuracy-wise, to the BP baseline on 26/30 experiments, and that the ViT backbone yields the least degradation. On audio, a similar experiment is done with two architectures (CRNN, AST) on two audio datasets. On 11/16 experiments, FF accuracy is 5% off the BP baseline. Then, a cross-domain adaptation task (4.2) is considered, where the different algorithmic enhancements previously introduced (e.g. quantized FF, gradient averaging, ""sharpness aware"" scheme...) are tested. Most importantly, it is observed that quantizing weights to 8 bits jeopardize the FF algorithm. Finally, sector, 4.3 presents in-domain OOD adaptation using the same fine-tuning schemes (LP, D-VPT) with three levels of corruptions of the CIFAR-10 dataset as OOD datasets. In this setting, FF achieves comparable performance with BP.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The problem tackled is highly relevant to on-device training, pragmatic and builds upon recent work [Malladi et al, 2023].
- The proposed algorithm is sound and well-explained.
- There are a lot of experimental settings, data modalities and architectures being explored.
- The proposed technique is effective in providing a learning signal, effectively training models and yielding relatively good performance compared to the BP baseline.

Weaknesses:
- It is unclear what is kept in full precision in the proposed procedure (see my questions below).
- On a related note, it is also unclear that the proposed algorithm enhancements don't offset the advantages of manipulating statically quantized quantities (see my questions below). 
- The experiments aren't all sufficiently well explained, neither in the main nor in the appendix, which is frustrating because there is a lot of work done there and we fail to deeply understand the proposed setups. I would even say that there are almost too many different experimental setups. Under constrained time budget to write the paper, I would have prioritized a lesser number of better detailed experiments rather than a lot of them left unsufficiently explained.
- **There aren't any error bars in any table and figures**, although the authors ticked in their checklist that they reported error bars and provided appropriate information about the statistical significance of their experiments (L. 520). For lack of this, it is very hard to draw any clear conclusion in terms of comparison between the different algorithms at use, e.g. is there a statistically significant gap here, or are these two results within error bar? We don't know.
- I don't understand what the 2D plot of the loss landscape really brings here in terms of insights.

Limitations:
See weaknesses and questions above.

If I had detailed description of each of the experimental setups tackled, a precise knowledge of what exactly is kept in full precision, how some of the algorithmic enhancements really work and error bars on all figures, I would be prone to increasing my score. I really want to encourage the authors to do so because I do believe that the core of this work is of interest.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the feasibility of on-device training using fixed-point forward gradients. The authors propose methods including sign-m-SPSA, Momentum Guided Sampling, Sharpness-aware Perturbation, Sparse Update, and Kernel-wise Normalization to reduce memory footprint and accuracy gaps and conduct experiments across various deep learning tasks in vision and audio domains. Key contributions of this paper include formulating forward gradients in the quantized space, demonstrating the feasibility of on-device training, and visualizing the neural loss landscape during training. The study shows that training with fixed-point forward gradients might be a practical approach for model customization on edge devices.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
++ This paper proposes an improved method for forward gradients, called Quantized Zeroth-order Forward Gradient (QZO-FF), which enables forward gradients training using quantization. 

++ QZO-FF is quantized and does not require backpropagation, thereby reducing memory overhead and eliminating the need for processors to have training capabilities. However, I doubt this because even though forward gradients do not require backpropagation, they still need to update weights and possibly save momentum and they need to perform additional quantization for $z$ in QZO-FF. Therefore, we may need some hardware adaption to assist feed-forward training.

++ The experiments across various benchmarks show that there is only a slight degradation in accuracy while the memory cost is reduced.

Weaknesses:
1. Some results are missing in the experiment. For example, (1) the memory cost of (BP, LP, fp16) is not measured. I think the memory cost of LP is important because it seems that the reduction of memory cost mainly comes from LP instead of FF and Quant in Figure 3 and Figure 4, and I think the claim that ""this number is further reduced to only 0.28MB"" and ""the saving increases to 8.1× when sparse update and fixed-point are enabled"" in Appendix B is totally misleading and unfair. (2) The accuracy of (BP, LP, quant) is not measured so there is no baseline for (FF, LP, Quant). (3) The accuracy of (FF, FT, Quant) and (BP, FT, Quant) is not measured. (BP, FT, Quant) should be some BP fixed-point training methods like Quantization-Aware Scaling (QAS) mentioned in related work.

2. Lack of ablation studies. The effects of techniques proposed in Section 3.4 are not well-studied. (1) There is no ablation study for Section 4.1. (2) The effect of sharpness-aware and kernel-wise normalization is not measured separately in Section 4.2. (3) I want to know __which__ of these techniques work in __what__ experiment settings. I believe that, as a new algorithm with many enhancement techniques, the authors should inform the readers about which parts of the algorithm are useful under which circumstances.

3. The model size (100K - 80M) is somewhat small compared to the concept of ""pretrained models"". How does the proposed method perform for larger models and how does the model size affect the effectiveness of the method?

Limitations:
The limitations are discussed well by the authors.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors investigate fixed-point forward gradients for quantized training. They conduct experiments across various deep learning tasks in vision and audio to assess if this method yields competitive models while conserving memory and computational resources.
They introduce algorithm enhancements to reduce memory usage and accuracy gaps compared to backpropagation, using fixed-point precision for forward gradients during training or adaptation.
Their findings demonstrate the feasibility of on-device training with fixed-point forward gradients across diverse model architectures (e.g., CNN, RNN, ViT-based) and parameter sizes (100K to 80M), offering practical solutions for model adaptation on edge devices.
The authors also visualize neural loss landscapes and training trajectories, providing insights into the dynamics of training with forward gradients for efficient on-device model adaptation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1 .They understand quantization and tried not to leave anything float

2. Experimenting with SAM and ZO is nice 

3. The paper is well written

Weaknesses:
1. Sadly no experiments on LLMs on which most fine tuning is done today

2. Marginal novelty: generally they just added quantization to ZO-FF – is that enough?

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a quantization approach for fine-tuning pretrained data to new local data on resource-constrained devices. In particular, the weights perturbation, gradients estimation, and weights updates are quantized to either 8-bit or 16-bit. This quantization approach is combined with Momentum Guided Sampling, Sharpness-aware Perturbation, Sparse Update, and Kernel-wise Normalization to enhance fine-tuning performance. The proposed approaches are evaluated on various AI benchmarks. The results of this study indicate that quantized forward gradients are a good candidate for a fine-tuning approach that can be deployed on edge devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1- The paper is well-written and well-organized.

2- The quantized approach is evaluated on a variety of tasks that show the generalizability of the new approach.

3- The Sign-m-SPSA-SGD approach is interesting and novel.

Weaknesses:
1- The author are recommended to discuss the accuracy degradation of quantized forward gradients compared to the backpropagation algorithm. In some cases, the accuracy degradation is high (more than 5%). A comparison of performance versus hardware complexity (FLOPs or another metric) is recommended, as seen in [1].

2- Evaluating the efficacy of quantized forward gradients on fine-tuning LLM models such as LLaMA-3 is recommended. 

[1] Carmichael, Zachariah, et al. ""Performance-efficiency trade-off of low-precision numerical formats in deep neural networks."" Proceedings of the conference for next generation arithmetic 2019. 2019.

Limitations:
The author addresses the limitations of this study by mentioning the initialization requirements for forward gradients and 16-bit weight quantization.

1- It would be suggested to discuss various initialization approaches that might be used instead of a pretrained network.
2- It would be suggested to discuss other numerical formats, such as 8-bit floating point or Posit, to solve the 16-bit weight quantization problem.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yBrxziByeG;"REVIEW 
Summary:
A new paradigm of multi-modal image fusion named Text-DiFuse is introduced, based on the diffusion model. The paradigm embeds a mechanism for aggregating feature-level multi-modal image information into the diffusion process of degrading multi-modal images, addressing the optimization gap between ""degradation removal"" and ""multi-modal information fusion"". Additionally, a zero-shot model is introduced to modulate the fusion strategy based on user-input target text, enhancing the saliency of the target of interest. The conducted experiments suggest significant improvements in both human visual perception and advanced computer vision tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1)	Embedding the mechanism of aggregating feature-level information into multiple diffusion processes to fuse multi-modal information is interesting. It is foreseeable that this diffusion paradigm produces fused images with better fidelity compared to methods based on likelihood-constrained diffusion models. 
2)	The coupled approach effectively resolves the issue of compound degradation in the process of multi-modal fusion, as evidenced by experimental results demonstrating significant advantages over the sequential approach.
3)	The authors emphasize the importance of foreground targets in advanced visual tasks and propose enhancing target saliency through zero-shot assisted re-modulation. This approach diverges from traditional uniform fusion rules, demonstrating effectiveness.
4)	This approach shows strong applicability. It demonstrates superior performance in multiple tasks including infrared and visible image fusion, medical image fusion, and polarization image fusion.

Weaknesses:
1)	After the diffusion model is effectively trained, the sampling process can follow different step intervals. The information fusion in this method is integrated into the diffusion process, but the article does not seem to specify the sampling interval at which the results are obtained. Also, this article does not discuss the impact of the sampling interval on the fusion performance.
2)	The presentation is slightly unclear. For example, from Equation 2 to Equation 6, both the features and the images carry the condition N that represents the degradation. Why does equation 7 no longer include N? Why can it be considered that the degradation has been removed at this point?
3)	In Table 2 and Figure 4, some existing image restoration methods are cascaded in front of the fusion method to promote fairness in comparison, such as low-light enhancement (CLIP-LIT), denoising (SDAP), and white balance (AWB) algorithms. 
Please explain the choice of the order in which they are connected in series, i.e. why low light enhancement first, then denoising, and finally white balance. 
4)	Modulating the salience of targets of interest in the fusion process through language is novel. Intuitively, I think the improvement in semantic properties brought about by this modulation is widespread. Currently, the effectiveness of language modulation has only been verified in the semantic segmentation scenario. It is recommended to provide an evaluation in the object detection scenario to further verify its role.

Limitations:
The authors have analyzed the limitations and potential negative impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work focuses on the topic of multi-modal image fusion. Two innovations enhance the performance of the fusion. One is the clever integration of information fusion into the diffusion process. This coupling way enables the fusion function to resist degradation. The other is the introduction of a text-based fusion remodulation strategy. This changes the limitation of previous fusion methods that could only use fixed mappings, allowing for the dynamic adjustment of the fused image based on specific requirements. This remodulation also enhances semantic attributes, improving the scores of the semantic segmentation task.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Integrating information fusion into the diffusion process is novel. Especially, each sampling step triggers an information fusion, which enhances the sufficiency of information fusion. This coupling can ensure the robustness of information fusion, addressing challenges such as low light, noise, and color cast. 
2. The introduction of multi-modal large models is interesting, particularly the ability to remodulate fused images using textual commands. This capability could potentially facilitate the flexible deployment of the proposed method across different application requirements. The demonstration of enhanced semantic attributes and improved semantic segmentation performance is good. 
3. Overall, the experiments are relatively sufficient. The comparative experiments include both baseline comparisons and pre-enhancement comparisons, which are important for ensuring fairness. 
4. The code is provided, which helps in reproducing the performance.

Weaknesses:
1. On page 5, line 174, the source data used for fusion contains degradation, [{Xb,Y}|N]. My question is, in Equations (9) and (10), where do the clean {Xb,Y} used to guide the fusion come from? Is there a multi-modal dataset that contains paired degraded and clean data? The paper seems to lack an explanation for this. 
2. The forward process of the diffusion model involves T steps of noise addition, while the reverse process consists of T steps of iterative sampling. Is the Z0 obtained in equation (8) a hypothetical Z0 derived from the diffusion relation at each sampling, or is it the Z0 after completing the full T steps of sampling? This determines the object of the constraints in the loss functions (9) and (10). It would be better to provide a detailed discussion on this. 
3. Only after the T steps of sampling can the data without degradation be obtained. So why can Z_{t-1}^b in equation (7) be considered free from degradation N? 
4. It's understandable that using textual modulation to control the desired targets of interest can enhance semantic attributes. My question is whether these enhanced semantic attributes can be generalized. In other words, can it also be effective in other high-level visual tasks besides semantic segmentation? 
5. Typo: The Zt on the left side of equation (8) seems to have a missing superscript b.

Limitations:
Yes, there are discussions about the limitations and potential negative societal impacts.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper addresses two primary challenges in multimodal image fusion: the mixed degradation of modalities and the insufficient salience of target objects. It proposes two methods to tackle these challenges: feature-level fusion diffusion and the re-modulation of fusion rules in target areas using a zero-shot segmentation model. They implement adequate experiments for evaluation, and the results demonstrate this method's advanced performance across various aspects, including the visual and semantic.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
+ The mixed degradation of modalities and the insufficient salience of target objects are two interesting problems in multimodal image fusion. This paper’s discussion and solution of these two problems may promote the usability of fusion methods in real scenarios. 
+ The information fusion at the feature level is integrated into the diffusion process, which effectively realizes the degradation removal.
+ The customized object highlighting strategy based on the zero-shot segmentation model is flexible. In particular, its gain in semantic attributes will increase the usability of the fused image in downstream tasks.
+ This paper conducts lots of comparative experiments and ablation studies on the overall method. 
+ The narrative of this paper is comprehensive and clear. For me, it's easy to follow.

Weaknesses:
- This paper mentioned that the diffusion model is pre-trained to enable the denoising network to have the degradation removal function. However, details about the construction of the data used to train the diffusion model are missing. They need to describe this process to make the overall approach clearer.
- This paper focuses on multimodal image fusion, being reflected in the title. In the main text, the proposed method is evaluated in two scenarios: infrared and visible image fusion and medical image fusion. In the supplementary materials, they further provide experiments on polarization image fusion. I am curious whether the applicable scenarios of the proposed method can be further expanded, such as the typical fusion of near-infrared and visible bands.
- The experiments on polarization image fusion only provide visual results, and it would be better to add a quantitative evaluation.
- I noticed that the proposed method separates the chrominance component and the brightness component, and then performs de-degradation on them separately. An explanation of why this operation is needed should be given. Perhaps an ablation experiment could more intuitively show the effect of this operation.
- There are some minor typos, such as potential misspellings of dataset names in Tables 1 and 2. In addition, there seems to be a lack of underline on AG's second place.

Limitations:
Limitations and broader impacts have been included.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes an interactive framework that can exploit the intrinsic connection between image restoration and multi-modal image fusion.
The authors embed information fusion within the diffusion process and address the ""composite degradation challenge"" i.e., multi-modal information integration with
effective information restoration from degradation like colour casts, noise, and improper lighting. Particularly, first, independent conditional diffusion models are applied
to each modality with compound degradation -- the degradation removal priors are embedded into the encoder-decoder network. A fusion control module (FCM) sits in
the multi-step diffusion process to manage the integration of multi-modal features and remove degradation during T-step sampling. Next, to interactively enhance
focus on objects of interest during diffusion fusion, the authors designed a text-controlled fusion re-modulation strategy that incorporates a text and a zero-shot OWL-ViT to
identify the objects of interest. In other words, this step performs a secondary modulation with the built-in prior to enhance saliency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- It is interesting to see the effect of combining image restoration and multi-modal image fusion in a single framework.
 - The proposed method is well-motivated and the authors provide a clear explanation of the method.
 - The Text-controlled fusion re-modulation strategy could be useful in many applications.
 - The authors provide the code in the supplementary material (although I have only dry run the code and not tested it).
 - Extensive experiments are conducted to validate the proposed method.
 - The authors provide ablation studies to show the effectiveness of each component of the proposed method.

Weaknesses:
For now, I have minor concerns and mostly questions (as listed in the next section).
 - The authors should add a brief discussion on the competitors in supplementary material. For example, differences between TarDAL, DeFusion, LRRNet, DDFM, and MRFS.
 - Typo in Eq. 2: $\Theta_{t}^{X^{B}}$ should be $\Theta_{t}^{X^{b}}$.
 - Improve the caption of Figure 2. I had to read the entire paper to understand the figure (it should be self-explanatory).
 - Not much of a weakness, but the authors could improve the clarity of the paper if they added the tensor dimension of each variable in Figure 2.

Limitations:
The authors discussed limitation section in supplementary A.4 -- Particularly, Table S1 shows number of parameters and runtime. This is highly appreciated.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yBHbeSpwYS;"REVIEW 
Summary:
This paper proposes a simple yet effective method based to address the issue of contextual bias for multi-label image recognition. It utilizes the casual intervention theory to pursue causal label correlations and suppress spurious label correlations. It utilizes the k-means to model the confounders, and employs the cross-attention mechanism to achieve the causal intervention. Experimental results demonstrate the efficacy of this approach.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is well-written and easy to understand.

This method seems easy to implement.

The approach achieves good results.

The problem is interesting in multi-label recognition tasks.

Weaknesses:
Why the k-means algorithm is used to build confounders, the author should give further explanation.

In the paper, the number of cluster centers is only calculated to 100, and what will happen if it continues to increase?

Regarding inference time, how many forward passes does the method require?

In L191, how to obtain P(c) from the data?

Limitations:
The authors have addressed the limitation of this method. This method is dataset-dependent, and it can not determine the specific semantics of confounders, resulting in limited interpretability. Besides, it only considers the causal intervention, and does not consider causal reasoning.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a novel approach to addressing label correlations in multi-label image recognition by using causal intervention. The method involves decoupling features, modeling confounders, and implementing causal interventions to capture useful contextual information while suppressing spurious label correlations. This approach is highly innovative and has significant potential applications.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. **Innovative Approach:**
   The paper introduces a novel method that applies causal intervention to model label correlations in multi-label image recognition. This innovative approach addresses the challenge of spurious label correlations and captures useful contextual information, which is a significant advancement in the field.

2. **Comprehensive Methodology:**
   The proposed framework integrates several complementary techniques, including feature decoupling with a Transformer decoder, confounder modeling through clustering, and causal intervention using cross-attention mechanisms. This comprehensive methodology enhances the robustness and accuracy of multi-label image recognition models.

3. **Thorough Experimental Validation:**
   The paper conducts extensive experiments across multiple datasets, demonstrating the effectiveness of the proposed method. The results consistently show improvements over existing approaches, particularly in scenarios with contextual biases, underscoring the practical value of the method.

Weaknesses:
1. **Lack of Hyperparameter Analysis:**
   The paper does not provide a detailed analysis of the hyperparameters involved in the proposed method, such as the number of clusters for confounders or the parameters of the cross-attention module. A sensitivity analysis of these hyperparameters would be beneficial to understand their impact on model performance and to guide practitioners in tuning the model effectively.

2. **Insufficient Discussion on Method Limitations:**
   The paper lacks a thorough discussion on the limitations of the proposed method. It would be valuable to include an analysis of scenarios where the method might not perform well, such as when the selection of confounders is inaccurate or when the causal relationships between labels are weak. Addressing these limitations can provide a more balanced view of the method's applicability and robustness.

3. **Limited Ablation Studies:**
   Although the paper includes some ablation studies, the number and depth of these experiments are not comprehensive enough. More detailed ablation studies are needed to analyze the independent contribution of each module (e.g., feature decoupling, confounder modeling, and causal intervention) to the overall performance. This would help in understanding the importance and effectiveness of each component of the proposed method.

Limitations:
The effectiveness of the proposed method relies heavily on accurately modeling the confounders. If the selection of confounders is not precise or representative of the underlying data distribution, the causal intervention may not effectively distinguish between useful contextual information and spurious correlations. This could potentially limit the method's performance in scenarios where confounder selection is challenging.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a causal intervention mechanism for multi-label image classification, where causal label correlations are pursued and spurious label correlations are suppressed. To achieve this, the authors frame a pipeline consisting of a branch for decoupling label-specific features and a branch for summarizing causal label correlations. The results from both branches are combined for final predictions on image labels. Comparative experiments and ablation studies demonstrate the effectiveness of the proposed causal intervention mechanism.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper is generally well written with clear motivation and objectives.
- Causal intervention is technically novel and well motivated in terms of multi-label image classification. 
- Experimental results are impressive, outperforming sub-optimal methods by a considerable margin. Ablation studies are aslo well designed to showcase the contribution.

Weaknesses:
- Line 175: 'Correaltions' -> 'Correlations'.
- Line 237: 'Transformer encoder' -> 'Transformer decoder'.
- $f_{fc}$ in Eq.6 and $f_{fc}$ in Eq.11 should be different if their parameters are not shared.
- In Figure 4, in the causal label correlation branch, the confounder features are added into label-specific features. However, the outputs are not seen to be used in subsequent steps, and it seems that only the label-specific features are utilized for causal intervention. The diagram of this module needs to be improved.
- More experimental evidence should be provided to verify the effectiveness of the confounder modeling. For example: using random vectors to replace cluster centers as confounders. Only the feature visualization and ablation study on clustering center number are unconvincing.
- Although this paper is well motivated, the modeling process, especially Equation 11, is confusing.

Limitations:
See Weakness and Questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yAAQWBMGiT;"REVIEW 
Summary:
This paper addresses the problem of data selection for finetuning large pre-trained models. The key contributions are:

1. A theoretical analysis of data selection for finetuning that reveals a variance-bias tradeoff in high dimensions.
2. A provable result showing that gradient sketching can efficiently find a low-dimensional subspace that preserves fast-rate generalization.
3. A practical two-stage algorithm called Sketchy Moment Matching (SkMM) that uses gradient sketching to explore the parameter space and moment matching to exploit the low-dimensional structure.
4. Empirical validation on synthetic and real datasets demonstrating the effectiveness of the approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a rigorous generalization analysis for data selection in both low and high-dimensional settings. The proofs are detailed and appear sound.
2. The proposed SkMM method is simple to implement and scalable to large models/datasets. Experiments on both synthetic and real data demonstrate the effectiveness of the approach.

Weaknesses:
1. Some of the theoretical results rely on assumptions (e.g., low intrinsic dimensionality) that may not always hold in practice. More discussion of the implications when these assumptions are violated would be valuable.
2. The method introduces new hyperparameters (e.g., sketching dimension, moment matching strength) without much guidance on how to set them optimally.

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study the task of data selection. They extend the classical variance reduction to the high dimensional case and provide a variance-bias tradeoff analysis. Based on the theoretical results, they propose sketchy moment matching, which first utilizes gradient sketchy to form a low-dimensional space and then uses moment matching to reduce the variance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposal is a reasonable improvement over the baselines which often only consider bias or variance reduction. The theoretical analysis is also a decent contribution of the paper.

Weaknesses:
The experiment focuses on linear probing, which already limits the scope of the evaluation. Furthermore, even under this limited scope, the setting does not seem to be challenging. For the synthetic setup, the sample count is 2000 while the rank is 2500, so it seems not to be a very high-dimension setup (the rank is not so much larger than the sample count). Also, the cluster count seems to be low for both tasks, 8 for synthetic, while the number of class is 10 for Cifar-10.

Limitations:
Yes, the authors discuss the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper concerns the data selection problem: given a collection of $N$ embeddings of dimension $r$ for $r\gg N$, the goal is to pick a subset $S$ of points of size $n$ so that one could run any downstream algorithm on $S$ with a regularization term, so that the empirical risk is small even on the entire finetuning set. Assuming the model is $y=\phi(X) \theta_*+z$ where $\phi: \mathbb{R}^d\rightarrow \mathbb{R}^r$ and $z$ is an i.i.d. noise vector with zero mean and bounded variance, then there exists a subspace that one could project onto and decompose the empirical risk as a bias and a variance term. Further, under the assumption that the second moment matrix has low intrinsic dimension, then one could find a good subspace via gradient sketching: draw a JL matrix $\Gamma\in \mathbb{R}^{r\times m}$ for $m\ll r$, then as long as one has $\Gamma^\top \Sigma^{\phi} \Gamma \preceq c_S \cdot \Gamma^\top \Sigma^{\phi}_S \Gamma$, then the error could be decomposed into a bias, variance and a sketching error term. A sketching gradient, moment-matching algorithm is proposed, involves applying sketching to the gradient, form the Jacobian and solve a quadratic relaxation. Experiments are performed on both synthetic datasets and CIFAR10.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The main theoretical contribution is that for over-parametrized setting where $r\gg n$, one could provably show the existence of a subspace that one could project onto and perform data selection on that subspace. Moreover, if the second moment in addition has low intrinsic dimension, then one could use standard dimensionality reduction techniques (in $\ell_2$ norm) to sketch the high-dimensional gradient. In the sketchy moment-matching algorithm proposed in the paper, the authors first sketch the gradient then use uniform sampling  to construct $S$.

Weaknesses:
The core results of this paper are not technically very novel and surprising, the algorithm could be interpreted as a generalization of the leverage score sampling via JL trick due to Spielman and Srivastava, STOC'08. The analysis largely draws inspirations from the over-parametrization literature, which makes sense as finetuning is essentially training in an over-parametrized setting. Another point that is a bit unsatisfactory is the sketchy moment-matching algorithm utilizes quadratic relaxation to solve the program efficiently with projected gradient descent, but all analysis is based upon *not solving the quadratic programs*. The authors should try to provide some theoretical justifications of sketchy moment-matching, as that's one of the key contributions of this paper.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of data selection in the over-parametrized fine-tuning regime, i.e. when the number of fine-tuning parameters $r$ is larger than the amount $N$ of available examples. We want to subsample $n\ll N$ examples that form a representative set to train on, and hopefully achieve quality as close as possible to fine-tuning on the whole set.

The idea is to compute the gradients $G\in \mathbb{R}^{N\times r}$ of all examples wrt the fine-tuning params and then select a subsample $S\subseteq [N]$ such that the Gram matrix of the gradients is approximated: $c\cdot \Sigma_S := c \cdot G^\top I_S G \approx G^\top G := \Sigma$. However, this is not possible to achieve since the model is over-parameterized. Fortunately, if the spectral approximation holds on a low-dimensional subspace of the parameter space, this is good enough, so the authors project the gradients on a random low-dimensional space. The proof goes through under the assumption that the singular values of the gradient matrix are well-concentrated on a small enough (<10%) support.

The experimental results include fine-tuning on a synthetic linear task, as well as fine-tuning a vision transformer on CIFAR-10 image classification.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors study the data selection for fine-tuning problem from first principles
- The writing is overall good and math looks sound, even though I didn't check details.
- The experimental results look promising since SkMM beats a variety of algorithms including leverage scores.
- The idea of spectral approximation on a subspace of the parameter space is interesting.

Weaknesses:
- Important details on the experimental setup are missing or unclear. Specifically, what is the optimization process after the data is subsampled? For the image classification experiments, what is being fine-tuned, is it all the ViT parameters? For how many epochs? 
- The algorithm requires computing the gradients of all samples, which can be computationally expensive. Besides, if we are computing all gradients, why can't we just train one epoch on all datapoints? Why is data selection useful in this case?
- The literature review could be expanded, including relevant papers such as BADGE [1], Coreset-based sensitivity sampling [2].
- In the experimental results, the authors should also compare with margin sampling (in addition to entropy sampling), as well as uniform sampling for the image classification task.
- Computing the moment-matching subset in Algorithm 3.1 seems overly complicated, see questions

[1]: Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds

[2]: Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
y9zIRxshzj;"REVIEW 
Summary:
This paper introduces a new causal model in which individual events of the cause variable trigger events of the effect variable with dynamic delays. The authors propose a cause-effect matching approach to learn a fully directed acyclic graph, named the CASCADE algorithm. The algorithm performs a topological search on observational data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper presents a comprehensive theory and algorithm, and conducts extensive experiments, particularly with real data, to validate the effectiveness of the proposed method. The analysis and the algorithm are presented in a logical way.

Weaknesses:
The proposed method is the direct matching between a cause event and an effect event, which precludes modeling a single event causing multiple other events, as well as multiple events jointly causing a single effect event.  This limits the applicability of the algorithm.

Limitations:
The authors discuss in detail the limitations and applicability of their algorithm.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The article employs the Algorithmic Markov Condition alongside Kolmogorov
complexity for causal discovery from event sequences. It focuses on a specific scenario in
which the sequence of events is divided into source and effect variables. The principal
contribution of this study is its innovative application of Pearl's causality model with
combination of AMC method, in contrast to the traditional Granger causality approach,
enabling the identification of both instantaneous and delayed effects.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Originality: The author employs Pearl's model of causality, diverging from
traditional Granger causality, to innovatively incorporate instantaneous effects
into the analysis of sequential events for causal relationship discovery.
2. Quality: The article is with good quality and honest about its strength and
limitation on their work.
3. Clarity: The article presents its algorithm with well-defined logic and
substantiated proofs.
4. Significance: The article offers an innovative approach to integrating
instantaneous effects into the causal discovery of sequential events, proposing a
potential method to enhance causal discovery techniques under such conditions.
However, it imposes strict limitations on the scenarios involving event sequences.

Weaknesses:
1. Significance: As mentioned in the limitation section by the author, strict
assumptions like direct matching between a cause event and an effect event leads
to challenges and possible violations in practical application, and it lacks
flexibility.
2. Section 3.3, which discusses the connection to Hawkes Processes, might be better
placed in an appendix or in a section dedicated to comparing different
methodologies. Its current placement in the theoretical part of the paper is
somewhat abrupt, especially since there is no direct focus on these processes in
your model.
3. The experimentation section lacks depth. It would be beneficial to evaluate and
report on the robustness of your model when its assumptions are challenged
during real-world applications.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In their work, the authors are concerned with recovering causal relations, where cause and corresponding effects occur in varying temporal distances. The authors leverage information theoretic formulations and properties of the algorithmic Markov condition to recover the causal graph via minimum description length principled. To this end, the authors present the 'CASCADE' algorithm, which recovers the topological ordering of the causal structure and proof identifiability results. The algorithm is evaluated on multiple synthetic data setups to examine the algorithm's performance under different varying noise, event type, and collider settings. Lastly, the algorithm is tested on a banking and daily activity data set to demonstrate robust performance on real-world data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well-written and introduces the problem setup and formalisms intuitively. The authors consider the challenging problem of modeling causal event sequences. The information-theoretic treatise and causal modeling of the event-generating process via minimum description length encodings are well described and follow common notation from related work. While I am not an expert on the topic of time series event causality, relevant related work seems to be sufficiently discussed and compared to.

The overall intuition on all proofs is well described. To the best of my knowledge, proofs of theorems 1, 3 and 4 seem to be correct. (Please see minor comments on Thm. 2 below). The presented CASCADE algorithm seems to be sound and its robustness is evaluated via multiple real-world and synthetic experiments, varying the noise and number of event types.

Weaknesses:
While the authors present strong theoretical identifiability results, these guarantees are tied to a restrictive set of assumptions (faithfulness, sufficiency, low noise) and hold only for a specific type of event process (single excitation, no suppressing effects). While the authors state all assumptions explicitly, the paper could be improved by discussing the possible implications and reasonability of real-world applications.


Proof of Theorem 2 (Sec. A.2; second line of l. 496): As all other terms seem to be taken over from the line above, it is unclear to me where the canceled term on the left side of the inequality is coming from. (Since all terms are positive, I believe the transformation to be still correct.) Furthermore, it is not obvious to me how the equation following l.497 and the noise ratio of $n_{i,j}/n_j$ leads to the desired result. The paper could be improved by providing a more detailed explanation of this step.


The experiments seem to demonstrate consistently better results compared to related algorithms. However, from the experimental description in B.1, it seems that the experiment on the especially challenging identification of colliders --due to unclear parent assignment-- only considers a setting with a single collider. The authors might want to demonstrate algorithm performance for settings where multiple colliers exist, to better examine the algorithm's robustness regarding unclear EM assignments.


Minor:
* It would be helpful to mention the definition of H() in Sec. A.1 as the entropy, which is only mentioned afterward in A.2.
* Typos in the Proof of Thm. 2 (sec. A.2 l.490): ""dealys"", ""ofset""; and the Conclusion (l.340) ""discovers"" -> ""discover"".
* In Sec. 4.1 l.201 text and formula disagree on the complexity: ""[...] leading to an overall quadratic complexity $O(p^3)$"".

Limitations:
Limitations with regard to the applicability of the algorithm are discussed. Assumptions required for identifiability of the considered causal models are stated explicitly but might be hard to check in real-world settings. The work might be improved by discussing societal impacts from applying the algorithm under possible assumption violations in real-world settings.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a method for identifying causal relationships in event sequences. The authors presents a causal model that handles both instantaneous and delayed effects, contrasting it with existing methods like Granger causality. This algorithm is evaluated on both synthetic and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The theoretical foundation based on the AMC and MDL principle is provided.

2. The proposed CASCADE algorithm is evaluated through extensive experiments.

3. The paper is well-organized, with clear explanations of the proposed model, theoretical underpinnings, and algorithmic steps. The use of illustrative examples and detailed proofs enhances understanding.

Weaknesses:
1. The paper acknowledges assumptions such as the direct matching between cause and effect events and the focus on excitatory effects. However, it could provide more discussion on the impact of these assumptions and potential ways to address them.

2. Scalability and computational complexity: The paper demonstrates the algorithm's performance on datasets with a moderate number of variables and events. An evaluation of its scalability to very large datasets, which are common in real-world applications, is less emphasized. The computational complexity of the algorithm, particularly for large datasets with many event types, is a concern. The quadratic complexity in the number of event types may limit its applicability to very large-scale problems.

3. Parameter sensitivity is not provided: How sensitive is the CASCADE algorithm to the choice of parameters for the delay distribution and cause probability?

Limitations:
The authors discussed the limitation in the Conclusion section. The identifiability of instantaneous effects relies on the strengths of the trigger and noise probabilities, which may be challenging to estimate accurately in practice.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
y9sHKrdnRt;"REVIEW 
Summary:
The paper introduces MC-DiT, a training paradigm for Diffusion Transformers (DiT) in the field of generative diffusion models for image generation.  By utilizing the proposed clean-to-clean mask-reconstruction approach, the model can better leverage contextual information at different noise variances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper provides a perspective on the limitations of noisy-to-noisy masked reconstruction, supported by theoretical insight and empirical analysis.
- The method is overall reasonable.
- The performance seems good.

Weaknesses:
- Will the additional two branches of DiT decoders increase the training overhead compared with other baseline methods? How about the training cost of each iteration compared with baselines?
- Comparing with MDT-XL / 2-, the improvements of MC-DiT-XL / 2-G seem to be marginal. 
- How is the natural information measured in Fig. 1?
- Will the code be released?

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper observes that reconstructing masked noisy patches from unmasked noisy patches harms contextual information extraction during the training of DiT and then proposes a novel training paradigm named MC-DiT with clean-to-clean mask-reconstruction. Two EMA branches of DiT decoders are designed to avoid model collapse.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The manuscript adequately puts forth a number of propositions and commendably supports these with ample evidence and rigorous demonstrations, fostering a robust intellectual foundation for their arguments.
2. The authors' perspective on applying a noisy-to-noisy mask reconstruction approach is convincingly articulated.

Weaknesses:
1. The presentation of generated images for visualization is rather limited in quantity, necessitating an expansion to adequately illustrate the diversity and quality of the results. It is suggested to present generated results with the resolution of $512\times 512$. This paper only provides visual results in Figure 5 with the resolution of $256\times 256$ and it also claims superiority on $512\times 512$ image generation.
2. Lack of experiment details about training time, inference time and memory usage.

Limitations:
They provide accurate limitations in the conclusion section.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces MC-DiT, a novel training paradigm for Diffusion Transformers (DiT) in image generation. It addresses the limitations of current masked-reconstruction strategies, which fail to effectively extract **contextual information** due to noisy-to-noisy reconstruction. MC-DiT employs clean-to-clean reconstruction, allowing for better contextual information utilization during diffusion denoising. The authors also design dual decoder branches to prevent model collapse. Theoretical and empirical analyses validate their approach, and experiments on the ImageNet dataset show that MC-DiT achieves state-of-the-art performance in both unconditional and conditional image generation tasks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1.The introduction of the MC-DiT paradigm, which utilizes clean-to-clean mask-reconstruction, represents a novel approach that addresses the limitations of existing methods in extracting contextual information.

2.The authors provide a thorough theoretical and empirical analysis, particularly focusing on mutual information, which strengthens the validity of their claims.

3. The proposed MC-DiT achieves superior results in both unconditional and conditional image generation tasks, as demonstrated by the state-of-the-art FID scores on the ImageNet dataset.

Weaknesses:
1. The paper primarily focuses on image generation using the ImageNet dataset. It remains to be seen how well the approach generalizes to other domains or datasets with different characteristics.

2. The authors should clearly elaborate on the differences between MC-DiT and other masked diffusion transformers (such as MaskGiT,SD-DiT, and MaskDiT).

Limitations:
The authors acknowledge that the training and inference speeds need improvement.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel training paradigm for Diffusion Transformers (DiT) in the context of generative diffusion models for image generation. The authors propose MC-DiT, which focuses on enhancing contextual information extraction by reconstructing clean unmasked patches from clean masked patches, as opposed to the traditional noisy-to-noisy reconstruction. The method employs two complementary branches of DiT decoders to balance the use of noisy and clean patches, preventing model collapse.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.	The paper presents a new insight into the use of clean-to-clean reconstruction for learning contextual information in masked diffusion models, which is a significant departure from traditional noisy-to-noisy reconstruction methods.
2.	The authors provide a theoretical analysis of mutual information between unmasked and masked patches, demonstrating the limitations of existing methods and the benefits of their proposed approach.
3.	The introduction of two complementary DiT decoder branches to prevent model collapse is a thoughtful addition that addresses a common issue in such models.
4.	The paper reports state-of-the-art results in terms of FID scores and IS scores, indicating that the proposed MC-DiT is highly competitive with existing methods.

Weaknesses:
1.	The proposed MC-DiT model may be more complex than necessary, which could potentially hinder its adoption and implementation in practical applications.
2.	The paper acknowledges that the training and inference speed of MC-DiT needs to be improved, which suggests that the current approach may have efficiency issues. The authors should provide specific comparisons to demonstrate that these efficiency sacrifices are worth the performance gains.
3.	The paper could benefit from a more detailed comparative analysis with other state-of-the-art methods, including feature visualization, to better understand the advantages of MC-DiT.
4.	Can the author explain whether this specific context information is pixel-wise information or semantic information? And their role in the overall framework?
5.	Ablation experiments can be further supplemented and improved. For example, the hyperparameters in Tab.5 can be further observed to have an impact. The current scaling still has some ambiguity.

Limitations:
The authors have mentioned the limitations in this paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors reveal the issues of Diffusion transformers of having semantic inconsistency as they fail to learn the contextual information. Based on their theoretical analysis, they proposed a novel training paradigm to fully learn contextual information with clean-to-clean mask reconstruction. The paper is well organised and written.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors have a comprehensive understanding of issues and the state-of-the-art. In terms of originality and quality, the work is technically sound in general. The analysis and written are clear in general.

Weaknesses:
Please see the list of questions for improvement and clarification on some of the aspects.

Limitations:
The authors mentioned that the training speed and inference speed still need to be improved and a possible future mitigation on the issue.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a training strategy for diffusion transformers that fully learns contextual information by introducing clean to clean mask reconstruction during training, and designs complementary DiT decoder branches as well as corresponding supervisory losses to avoid the problem of model collapse, giving theoretical and experimental validation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	Sufficient theoretical analysis
2.	The overall writing of the paper is logically clear

Weaknesses:
1.	There are errors in the description of parts of the paper, e.g., x1 in lines 107 and 109 of the introductory section of Masked AutoEncoders is described as masked and unmasked, respectively.
2.	Visualization of experimental results is indeed missing, and only quantitative experimental results exist in the body of the paper.
3.	Using the training strategy in the paper, although it can improve the results, it is not possible to conclude the size of the contribution of the training strategy to the final experimental results, as parameter tuning is still required in the testing phase.

Limitations:
The methodology proposed in the paper needs to be fine-tuned to the task during the inference phase, otherwise good results may not always occur. Further, it can be found that similar strategies that utilize unknown information in the inference phase for training can have application limitations, which are not conducive to extending and applying the strategy to tasks that do not have access to potentially clear images and information.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper critiques previous masked-reconstruction strategies in DiT training for their poor contextual information extraction, attributing this to noisy-to-noisy reconstruction. The authors theoretically and empirically validate that this approach limits mutual information between unmasked and masked patches. To address this, they propose a new training paradigm, MC-DiT, which uses clean-to-clean mask-reconstruction combined with diffusion denoising at varying noise levels. To prevent model collapse, they design two complementary DiT decoder branches to balance the reliance on noisy and clean patches. Model collapse would happen in this context due to excessive reliance on clean patches for reconstruction, leading to insufficient utilization of noisy patches and imbalanced training. Extensive experiments on the ImageNet dataset show that MC-DiT achieves state-of-the-art performance in both unconditional and conditional image generation, with faster convergence.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper motivates the need for their research in their introduction and is an interesting idea.
- The paper adds to the mathematical discussion surrounding image generation using diffusion using well understood mutual information metric prevalent in other areas of computer vision.
- Presents experimental evaluation, with section on reproducible details and supplementary materials.

Weaknesses:
- While reading the article, there are many questions that arise which effect the reading experience of the article.
- The main weakness of the paper is that at many occasions claims are made which are intuitive, but they are attributed to be implied from an equation / proposition which do not (at least not immediately) show the claim to be true. Look at questions for more details.
- Some experiment details are unclear (in questions).
- Table 1 is a bit difficult to read here with the number of methods and it is not obvious how the horizontal lines are drawn, i.e. what makes them different from other quadrants. I think there is enough space for a column or two to add a bit more detail instead of adding them all to the name of the method.
- Figure 3 (a) is used to showcase speed of convergence. However, I think the distinction between convergence and a convergence to lower loss should be made. All 3 lines more or less flatten at the same time, you could actually argue the red and orange line are flattening faster. I agree the blue line is lower, but that does not mean it has converged faster, only converged to a lower loss. This also leads to a second point, a lower loss here does not necessarily mean a more performant model. As you notice in your own experiments, you require fine-tuning to make the output desirable. Therefore, I disagree that the model converges faster on the whole.

Limitations:
Questions listed above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
y9huwsnGRJ;"REVIEW 
Summary:
The paper introduces **LeapAD**, an interesting paradigm for autonomous driving inspired by human cognitive processes, addressing the limitations of prevailing data-driven methods in complex scenarios. LeapAD incorporates a dual-process decision-making module consisting of an Analytic Process (System-II) for logical reasoning and experience accumulation, and a Heuristic Process (System-I) for quick, empirical decision-making based on the learned knowledge from System-II. By emulating human attention to focus on critical objects, LeapAD simplifies environmental interpretation and mitigates decision-making complexities. The system is tested in the CARLA simulator, demonstrating superior performance over camera-only methods with less labeled data. The Heuristic Process shows continuous improvement through a reflection mechanism and a growing memory bank, indicating the effectiveness of the dual-process approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents several notable strengths across dimensions of originality, quality, clarity, and significance: 

  

### Originality 

1. **Dual-Process Decision-Making**: The combination of an Analytic Process (System-II) and a Heuristic Process (System-I) emulates human cognitive functions, offering a biologically inspired framework for autonomous driving. 

  

### Quality 

I think the quality is good. 

1. **Continuous Learning**: The reflection mechanism and growing memory bank enable continuous learning and improvement, showcasing the adaptability of the proposed system. 

  

### Clarity 

The paper is well-written, with clear and concise explanations of complex concepts. The dual-process framework and its components are described in detail, making the methodology accessible to a broad audience. 

  

### Significance 

**Advancing the Field**: By introducing a dual-process decision-making framework, the paper opens avenues for research in autonomous driving and artificial intelligence, potentially influencing future developments in the field.

Weaknesses:
While the paper presents some interesting contributions, there are areas where improvements could be made: 

  

### Methodological Concerns 

  

While I appreciate the design of the Analytic Process and Heuristic Process, does the paper clearly distinguish between the two? My understanding is that the Analytic Process uses LLMs, while the Heuristic Process uses a lightweight language model. Why can it be called the Heuristic Process? It would be better to clearly state why can it be called the Heuristic Process and the Analytic Process. 

 

### Experimental Limitations 

  

1. **Quantitative Metrics**: 
The paper's experimental results are primarily based on the CARLA simulator, lacking real-world experiments. CARLA scenarios are still too simple. It would be better to report results that can comprehensively evaluate the performance of LeapAD, such as using the real-world dataset nuScenes. 

  

### Clarity and Presentation 

  

1. **Technical Details**: 
This paper is based on Qwen VLM. It is not clear whether the performance improvement is due to this Qwen VLM or the two-system design. It would be better to include more ablation studies to explore the influence of VLMs, such as LLaVa. 

  

By addressing these weaknesses, the authors can provide a more thorough and robust evaluation of LeapAD.

Limitations:
Yes, the limitations are discussed in Section 6.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents LeapAD, a dual-process closed-loop autonomous driving system.

LeapAD first uses a VLM to analyze the scene by selecting and locating critical objects in the scene, and then it uses a dual-process learning approach to learn driving behaviors.

The dual-process learning system contains an Analytical Process and a Heuristic Process. The Analytical Process is strong but expensive to run. It is used to summarize the driving experience into the Memory Bank. The Heuristic Process is more lightweight and is used to generate controls to control the vehicle. The Heuristic Process is trained with data in the Memory Bank.

The Analytical Process can also reflect from collision events in previous simulation runs. It will analyze the cause of the collisions and save the knowledge in the Memory Bank.

The authors evaluated the LeapAD method in closed-loop simulation with the CARLA simulator. They used the Qwen models as the VLMs and GPT-4 for the Analytical Process.

The evaluation result shows that LeapAD surpasses the performance of the other camera-only models on the CARLA Town05 benchmark.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* The dual-process idea is neat and thought-provoking. It equips the autonomous driving system with the ability to learn from past experiences.

* The method achieves stronger performance than state-of-the-art methods in CARLA closed-loop simulation.

* This paper is well-written and provides sufficient details for reproducing their approach.

Weaknesses:
* The performance improvement is not very significant compared to the baseline.

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a paradigm to design an annotation-efficient end-to-end autonomous driving system that harnesses the power and generalizability of open-source LLM models. It proves that critical frame/instance selection are critical to a decision-making module training. This method is evaluated by closed-loop testing in CARLA and achieves the SOTA performance among camera-based methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The core idea is straightforward.
2. Achieves the SOTA result.
3. Complete adequate ablation studies to support its claim.

Weaknesses:
1. No quantitative benchmark on its VLM module on simulation and the real world. Only some samples are listed in the paper.
2. The paper only presents an overall benchmark on the system but no failure case analysis.
3. The result relies on the foundation model performance and the paper does not show a way to fill the gap between the simulation and the real world, which limits its impact.

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper ""LeapAD"" introduces a new approach to autonomous driving that addresses key challenges in adaptability and interpretability. It draws inspiration from human cognition to enhance decision-making processes in complex environments.
The system incorporates two complementary processes:
- Analytic Process: Provides thorough analysis and reasoning, accumulating driving experience through logical reasoning.
- Heuristic Process: Employs swift, empirical processing and learns from the Analytic Process through supervised fine-tuning. This dual-process setup enhances adaptability and performance.

Closed loop testing in the CARLA simulator demonstrates that LeapAD outperforms methods relying solely on camera input. The Heuristic Process can inherit knowledge from an Analytic Process powered by GPT-4, leading to continuous performance improvements as the memory bank expands.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The paper is generally well-written and but easy to follow. Good motivation for the model design in the introduction. 
2.	I like the problem setup: how can we design AV systems that continually learn from its mistakes. 
3.	The experimental results seem to support the authors' claims.

Weaknesses:
I overall liked the idea of closed-loop autonomous driving approach that could emulate the critical attention mechanisms required for smooth driving environment in safety critical scenarios. The notion of heuristic and analytical processes for executing actions in robotics seems a novel approach.
However, my primary concern lies in the setup of data and models for generating scene descriptions into text to identify critical objects. Operating within the text domain, which requires subsequent interpretation and tokenization by the analytical and heuristic modules, seems less efficient than using a direct vectorized representation. For instance, representing an object with parameters such as {v = 0.2 m/s, s = 3m, class = Car} is likely more efficient and robust than the text output ""The car is 3 m away from the ego vehicle and is moving at 0.2 m/s."" This textual method could lead to inefficiencies, especially in scenarios with multiple dynamic actors.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
y929esCZNJ;"REVIEW 
Summary:
The paper introduces MomentumSMoE, a novel integration of heavy-ball momentum into Sparse Mixture of Experts (SMoE) to enhance stability and robustness. It establishes a connection between SMoE and gradient descent on multi-objective optimization problems.
The paper demonstrates theoretical and empirical improvements of MomentumSMoE over standard SMoE across various tasks. The method is universally applicable to many SMoE models, including V-MoE and GLaM, with minimal additional computational cost.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
To the best of my knowledge, attempting to accelerate the fixed point iteration in SMoE is an original idea.

It seems like there is comprehensive empirical evidence for the method, but I am not an expert on metrics for the SMoE, and will have to rely on other reviews to be confident in this strength.

The paper is fairly clear, with well-organized sections and figures.

Weaknesses:
My largest negative for this paper is the largely unfounded connection between the SMoE and gradient descent.  If the authors had made a connection to accelerating fixed-point iterations in general, I would want to accept this paper.  Essentially, the authors are assuming that $\nabla_x f$ has strictly real eigenvalues when they should just work with truly, potentially complex, eigenvalues, ex., using tools as in Azizian et. al.  For example, when performing this analysis, various other acceleration schemes are often better, like negative momentum (Gidel et. al.) or complex momentum (Lorraine et. al.).  I would be curious to see some empirical investigation (or theoretical) or what the eigenvalues of $\nabla_x f$ are – ex., as in Figure 7 of https://arxiv.org/pdf/2102.08431 -- to validate any theoretical claims about what acceleration schemes should be used.

But, of course, the spectrum is only known in small-scale problems, leading to the second weakness, which is that some of the methods – ex., RobustSMoE – seem to rely on knowing the spectrum to set various parameters, which we won’t have access in real settings.  Th

The theoretical results are also largely just reproductions of known theoretical results for momentum once you assume that the update from the SMoE is a gradient. This makes them not much of a contribution from my point of view other than leveraging existing tools. I think these results could be easily substituted for analogous techniques from Azizian.

Azizian, Waïss, et al. ""Accelerating smooth games by manipulating spectral shapes."" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.
Lorraine, Jonathan P., et al. ""Complex momentum for optimization in games."" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.
Gidel, Gauthier, et al. ""Negative momentum for improved game dynamics."" The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.

Limitations:
The limitations are discussed briefly, but a delineated section elaborating on all the limitations would be valuable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a variant of sparse mixture of experts, MomentumSMoE, by incorporating momentum into the traditional sparse mixture of experts framework. The authors provide both theoretical proofs and empirical evidence demonstrating that MomentumSMoE offers greater stability and robustness compared to the standard sparse mixture of experts. Experiments on language modeling and object recognition tasks are conducted to verify the effectiveness of the proposal.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of integrating momentum into sparse mixture of experts  is interesting. 
2. Both the theoretical proof and extensive empirical results are provided to demonstrate that the proposed MomentumSMoE is more stable and robust than SmoE; the experimental results are appealing.
3. The code is provided.

Weaknesses:
The pseudocode may be provided to better illustrate the implementation of the proposal.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel approach to enhancing the robustness and stability of Sparse Mixture of Experts (SMoE) models. Inspired by the analogy of gradient descent and SMoE, the authors develop a family of models by incorporating momentum into the training process. The key idea is that training SMoE is a multi-objective optimization problem where the monument-based gradient descent method is more stable and robust than the vanilla one. They proposed the AdamSMoE and Robust MomentumSMoE, which demonstrate improved performance across a variety of tasks, including language modeling and object recognition.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The integration of momentum into SMoE is a non-trivial innovation that addresses instability and inefficiency issues in existing models.

(2) The paper provides convincing empirical evidence showing the effectiveness of MomentumSMoE across multiple benchmarks.

(3) The proposed method's compatibility with other momentum-based optimizers, like Adam, suggests it can be broadly applied to various SMoE architectures.

Weaknesses:
(1) Formulating SMoE as a multi-objective optimization problem is doubtful to me. Every expert network is continually changing during the model training, which makes each objective nonstatic, which violates the basic assumption of multi-objective optimization, whose objectives should be very clear and stable. 

(2) It is unconvincing to use ||f(x)|| as the key metrics to measure the efficacy of SMoE or MoE. This confuses me a lot. Please explain why the output norm represents the goodness/badness of the model.

(3) There are some grammar issues. Please use `` instead of "" in the paper (line 665).

(4) There is no sufficient discussion of computation overhead. Training efficiency is a critical issue for current foundation model training. Does computation significantly increase by applying momentum over the SMoE? Keeping an additional copy weight (p in Fig 1) would take additional memory and may decrease the throughput.

I'd like to hear a more insightful discussion regarding all the points above from the authors.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the instability problem of training SMoE models. By establishing a relationship between SMoE and multi-objective optimization, the authors integrate momentum into SMoE and propose MomentumSMoE. Experimental results show that MomentumSMoE is more stable than SMoE during training.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper tackles a critical issue in the training of SMoE models.

2. The proposed method is generalizable and can be applied to various SMoE models such as V-MoE and GLaM.

3. Experimental results demonstrate that this method is more stable than SMoE during the training process.

Weaknesses:
1. This method has little effect on models with few layers.

2. The largest models for evaluation only have  388M parameters, which are much smaller than mainstream MoE LLMs.

3. From a theoretical standpoint, developing a framework to explain the enhanced robustness of MomentumSMoE would be interesting.

Limitations:
The authors have adequately discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xqc8yyhScL;"REVIEW 
Summary:
This paper investigates the effectiveness of Large Language Models (LLMs) in solving Programming-by-Example (PBE) tasks. Evaluations are conducted on three classic PBE domains including lists and strings, as well as a graphics programming domain. The findings suggest that while pretrained LLMs are not inherently effective for PBE, fine-tuning significantly enhances their performance on in-distribution tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Thorough evaluation and detailed analysis.

- Clear cases and illustrations.

- Addressing the challenge of small datasets for fine-tuning LLMs.

Weaknesses:
- In the experiments, there are no LLM competitors in the graphics domain. Any reasons?

- Why are only FlashFill and LambdaBeam compared in the experiments of Figure 6?

- The adaptation method used to improve out-of-distribution performance exposes the model to the test set content beforehand. Especially in string tasks, directly selecting the adaptation seed program from all test cases may be unfair. 

- The examples used in the experiments are relatively weak and do not closely resemble real-world programming tasks.

- If the adaptation's seed program is not provided, even after fine-tuning, the out-of-distribution generalization ability of LLMs still appear to be quite weak.

Typos:
in abs: potentially increasingly the flexibility -> potentially increasing the flexibility

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on the classical task of Programming By Example (PBE): given some (input,output) pairs, the goal is to generate a program that ""fits"" these examples (producing the outputs when given the inputs), and also generalizes well to new inputs.
The paper evaluates mostly 7B and also 33B LLMs on three PBE tasks. The paper finds that finetuning these LLMs on these tasks further boosts their accuracy.
The paper also investigates out-of-distribution (OOD) generalization, and finds that OOD can be improved using a semi-supervised approach, where the model is given (input,output) pairs from the new domain (but not the desired program); then the LLM samples potential programs that solve the (input,output) pairs; if the program is correct (which can be validated) - it is added to the training set and the LLM is trained / finetuned again iteratively.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. The paper is very clear and easy to follow, and it contains many examples that are visualized nicely.
1. The paper connects modern LLMs with the classical problem of Programming By Example (PBE)

Weaknesses:
1. Undefined, non-scientific, message - the title of the paper is ""Is Programming by Example solved by LLMs?"". This title leads the paper (""We investigate here the extent to which large language models pretrained on source code can solve PBE""), but I think that it's an undefined question. What does ""solve"" mean? By construction, and according to the ""no free lunch"" theorem, PBE can never be ""solved"". So ""solving"" PBE just depends on the difficulty of the questions. Even if we could define ""solve PBE"", how would you measure it? Is 90% considered ""solved""? Is 80% ""solved""? This problem is further expressed in L214: ""absolute performance in LOGO remains poor"" - 16% accuracy is not ""poor"" when you do not compare it to anything. Any accuracy number below 100% is considered as ""unsolved"" as any other number, and 100% is not possible on a hard enough dataset (because of ""no free lunch"").

1. Novelty - this is mostly an evaluation paper, that does not introduce any new approach or technique. Further, from the empirical evaluation, the answer to the question ""Is Programming by Example solved by LLMs?"" is, as expected, is ""somewhat, but not quite"": nothing in the empirical results was surprising or unusual: (a) finetuning LLMs on task-specific data works well; (b) semi-supervision on OOD data helps; (c) using Python as the output programming language works much better than the DSLs of classical work, because modern LLMs were trained on much more Python data than niche DSLs.

1. The OOD claim is a bit weak, because only in the relevant section it is said that ""assuming we have access to problems drawn from the testing distribution"" (without their labels, but these labels can be sampled and validated).

1. The paper compares its approach (a finetuned LLM) to classic symbolic, DSL-based (non-learning / non-neural) approaches several times throughout the paper, and speaks in favor of the LLM-based approach. This comparison to classic approaches is a bit of a strawman, since it is quite obvious that 33B LLMs are much more powerful than Flashfill (which is a paper from 2011) (Table 1).
The paper also mentions that:
>We also find that the resulting system can cover a broader scope of problems than classic symbolic methods, owing to the use of a Turing-complete language, which, at least theoretically, allows learning any computable function.

And I think that such claims completely miss the point: the reason that LLMs are better than classic symbolic methods is **not** the use Turing-complete languages. LLMs would have been better than classic symbolic methods even if the classic symbolic DSLs were turing-complete as well. The reason is that LLMs were trained on trillions of Python tokens.

6. Another trivial claim: in Section 4.2, the authors find that ""posterior description length is more predictive than program size and prior description length"". Simplifying the paper's claim, without using words from probability, basically says: the perplexity of the desired output sequence is predictive of its accuracy on downstream tasks. I think that this claim is quite trivial, and is very common in practice in LLM training: measuring perplexity on a validation set is usually closely correlated with success on downstream tasks.  Isn't this posterior iexactly what the model was *trained* to predict?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper performs a relatively thorough study on using LLM for example-guided program synthesis tasks. The results presented in the paper suggest that LLMs make strong progress toward solving the typical suite of example-guided synthesis tasks, potentially increasingly the flexibility and applicability of PBE systems.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The PBE problem is interesting and well-motivated. Major papers in the field are well cited and referenced
- Extensive amount of traditional datasets are being evaluated
- The insights derived from experiments are somewhat valuable

Weaknesses:
- CoT and other simple prompting methods are not evaluated
- While there is an extensive amount of experiments and comparisons, we find that the outcome is relatively predictable.
- While the writing is generally okay and easy to understand, multiple typos and mistakes found in the writing (also mentioned in questions). Please consider fixing them.
- The LOGO visual examples are converted to an ASCII grid of characters (Fig. 8b). This might not be the most intuitive representation. Details about the transformation is not shown, such as how each number (0-9) is derived, the resolution of the ASCII grid, etc. With this design, it does not make sense for a non-fine-tuned LLM to solve the task. But technically you could still fine-tune GPT-3.5 with these inputs, but I guess it is okay to not include this experiment.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates whether the long-studied programming by example task is ""solved"" by large language models with Turing-complete languages like python.
Their evaluation is on three domains: lists, strings, and LOGO/Turtle graphics.
They evaluate three LLM-based approaches, including a self-instruct-like fine-tuning approach that tunes LLMs on synthetic labeled data, and an adaption approach assuming access to problems (not solutions) from the testing distribution.
Compared to several symbolic, neurosymbolic, and LLM baselines, the proposed approaches perform better.
The analysis of the correlation between different aspects of the target program indicates that the fine-tuned model is beyond blind guess-and-check.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The experiments are comprehensive, and the analysis of different predictors of model performance is helpful in understanding the extent to which LLMs solve PBE.
2. The proposed methods make use of the fact that PBEs problems can be accurately synthesized using model-generated inputs and programs. The experiment results show that they are effective in solving in-domain problems and adapting out-of-distribution ones at test time.
3. This paper answers some interesting questions regarding the role of LLMs for PBE and points out what researchers might work on in the future.

Weaknesses:
Contamination. As the authors acknowledged on Line 148, the problems could be in LLMs' pertaining data. I wonder if the authors have an idea of how much of a role such potential contamination plays in LLMs' superior performance. Is there anyway to rule out or minimize the impact of that confounder?

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
y8Rm4VNRPH;"REVIEW 
Summary:
This paper proposes the Delta Rule method to construct the state updates for Linear Attention. Furthermore, the paper introduces a chunk-wise training approach, allowing the computational cost of training to grow subquadratically with the text length. Experimentally, the paper validates the effectiveness of the model architecture using three synthetic benchmarks: MQAR, MAD, and RegBench. Additionally, the paper uses Common Sense Reasoning and Retrieval tasks in LLM pre-training to verify the model's performance in real-world tasks. The model has been validated at scales ranging from 340M to 1.3B parameters. Furthermore, this paper explores the possibility of combining the Delta Rule with Sliding Window Attention and Global Attention, demonstrating the positive impact of the hybrid architecture on model performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Solid work. The paper provides a good derivation, offering a more general method for state updates in Linear Models.
2. The experiments are comprehensive and effectively demonstrate the validity of the model architecture.

Weaknesses:
1. Have you conducted experiments on long context? For example, measuring extrapolation and scenarios akin to ""looking for a needle in a haystack""? As a linear model, I would like you to further discuss its capability to generalize to long context.
2. The algorithmic speed of Delta Net increases linearly, but it seems to be slower than GLA. Can you analyze the factors contributing to this?
3. Could you further explain the insights of the Delta Net updates? I understand there are algorithmic differences compared to GLA operators, but what unique benefits do they bring? Is there any theoretical analysis?

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel algorithm for the efficient training of DeltaNet Linear Transformers. DeltaNet enhances contextual associative recall using a delta rule-like update but was previously limited by inefficient parallelization in its training algorithm. The work described in this paper presents a hardware-efficient algorithm that leverages the memory-efficient WY representation for computing products of Householder matrices, enabling the scaling of DeltaNet similar to other linear Transformer models. The authors trained a 1.3B parameter model on 100B tokens and found that it outperforms strong linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces a novel hardware-efficient algorithm for training DeltaNet Linear Transformers, leveraging the WY representation of Householder matrices, which effectively addresses the parallelization limitations of previous algorithms.
- Through large-scale experiments, the authors demonstrate that DeltaNet significantly outperforms existing models like Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks.
- The new algorithm enables the scaling of DeltaNet to larger datasets and parameter sizes, which is crucial for large language models.

Weaknesses:
The algorithms presented in this paper are satisfactory in terms of efficiency and performance.

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a hardware-efficient algorithm for training linear transformers with a delta update (DeltaNet; SMS21). This architecture has an attention formulation that prevents the direct application of chunk-wise parallel algorithms for computing its output. To address this issue, the authors introduce a re-parameterization of DeltaNet as a matrix-valued RNN whose recurrence is given by a generalized Householder transformation. This enables the use of WY representation which is memory efficient and eliminates the need to materialize the hidden state matrices. Experiments on synthetic benchmarks and language modeling tasks shows competitive performance compared to strong baselines (Mamba, GLA) and faster speed than the original Deltanet implementation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well motivated and situated with respect to prior work. It provides sufficient background for linear transformers, demonstrates great scholarship in crediting prior work, and has a clear exposition of the proposed idea. In addition, it presents an informative overview that compares the formulations of recent linear transformers that highlights their differences.  
- Proposes an efficient algorithm for training linear transformers with the delta update which is a competitive variant. The re-parameterization is non-obvious and leverages WY representation for Householder matrices in a novel way. Previously, this architecture could not be easily scaled to larger models and datasets with a recurrent formulation. In addition, it introduces two competitive hybrid methods based on DeltaNet that leverage local and global full attention. 
 - Demonstrates the effectiveness of the proposed approach on two synthetic benchmarks and eleven language modeling and understanding tasks compared to strong baselines such as Mamba and GLA. The results are consistent, have a good coverage, and are important for the researchers working on efficient transformers. 
- The experiments are thorough and have convincing settings, namely all the variants are trained from scratch with the same configurations, there are ablations to justify the design choices, and the experimental reporting is very detailed.

Weaknesses:
- W1. In terms of scale, the model explores two different architectures of increasing size up to 1.3B parameters. Even though this size is considerable, it is still relatively small compared to the LLMs that are widely used such as Llama, Mistral (7B+ size). There is always the question of whether the quality is maintained with further model increase.
- W2. The improved results compared to Mamba and GLA make use of additional architectural components: convolution and local/global attention, without them the results are comparable to the other models.

Limitations:
Yes, they have.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
y8P633E5HQ;"REVIEW 
Summary:
The authors introduce spectral GNNs that are equivariant to functional symmetries. Specifically, they introduce node-level, graph-level and pooling non-linear spectral filters and show that these are able to outperform standard convolutional GNNs on (semi-supervised) node classification and graph classification tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The experimental results are compelling.
- To the best of my understanding, the theory is sound
- The idea being proposed is novel and worth being investigated
- The paper is clearly written, even though it doesn't seem to be very accessible to readers unfamiliar with graphs signals processing

Weaknesses:
- While the authors did a good job in trying to introduce all the relevant concepts, the paper is quite dense with mathematical details and notions that will likely be unfamiliar to many GNN researchers and may therefore hinder the accessibility of the manuscript.

Limitations:
The limitations are clearly discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issue of inadequate modeling of graph equivariance in existing spectral GNNs due to nonlinear operations. The authors investigate the concept of domain translation in graph space as functional translations, drawing from the convolutional operations defined on images. Based on a series of in-depth analyses, they propose nonlinear spectral filters (NLSFs) that are fully equivariant to graph functional shifts and demonstrate universal approximation capabilities.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The research problem is highly valuable, and the ideas presented are novel.
2. The theoretical analysis is rigorous, thoroughly supporting the arguments and solutions proposed in the paper. It reflects the authors' deep understanding and insight in the field.
2. Notable experimental improvements in classification tasks.

Weaknesses:
1. The paper is missing important spectral GNN models such as [1,2,3,4].
2. There is a lack of discussion on equivariant GNNs, such as [5,6,7,8,9,10,11,12]. While the focus is on spectral GNNs, it is also essential to discuss works in the spatial GNN domain, especially given that the paper’s title starts with ""Equivariant Machine Learning on Graphs."" Positioning your work within the broader GNN field can further elucidate the significance and unique advantages of your contributions.
3. The experiments are somewhat lacking in comprehensiveness. While you have demonstrated the effectiveness of your proposed model in common node and graph classification tasks, your core contribution is enhancing equivariant representation learning in spectral GNNs. I suggest adding experiments that specifically show how the improved performance is due to enhanced equivariant learning, such as graph isomorphism tests.

[1] How powerful are spectral graph neural networks

[2] Bernnet: Learning arbitrary graph spectral filters via bernstein approximation

[3] Specformer: Spectral graph neural networks meet transformers

[4] Graph neural networks with learnable and optimal polynomial bases

[5] Universal Invariant and Equivariant Graph Neural Networks

[6] E(n) Equivariant Graph Neural Networks

[7] On the Generalization of Equivariant Graph Neural Networks

[8] Expressive Power of Invariant and Equivariant Graph Neural Networks

[9] Approximately Equivariant Graph Networks

[10] Equivariant Polynomials for Graph Neural Networks

[11] Graph Neural Networks for Learning Equivariant Representations of Neural Networks

[12] Sign and Basis Invariant Networks for Spectral Graph Representation Learning

Limitations:
yes, the authors discuss the limitations in Section 6

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a spectral GNN called non-linear spectral filters (NLSF), which aims to enhance GNNs with nonlinear functions. Since general GNNs with nonlinear functions do not commute with unitary operators, this paper defines Graph Functional Shifts, which is a set of unitary matrices commuting with a normal graph shift operator (GSO). It then formulates two functions for spectral index and filter bank, respectively, and concatenates these two functions as graph attention. In the experiment section, NLSF is compared with GAT, SAGE, and other spectral-like GNNs. In the node-classification task, att-Node-level NLSF shows outstanding performance among these models. In the graph classification task, att-Graph-level NLSF achieves comparable results with these models, and att-Pooling-NLSF performs better than other models in the graph classification task.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. NLSFs have a solid mathematical foundation and proof, especially on Universal Approximation and graph expressivity.
2. The experimental results validate the effectiveness of the theory.

Weaknesses:
1. The method proposed in this paper requires the use of eigenvalues, hence it necessitates eigen decomposition of the GSO. The time complexity of eigendecomposition is relatively high, especially for very large graphs.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose nonlinear spectral filters (NLSFs) that achieve full equivariance to graph functional shifts, demonstrating that these filters have universal approximation properties. These NLSFs are designed based on transferable spectral domain, potentially improving GNN performance in node and graph classification tasks across diverse graph structures.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1- The paper is well-written and self-contained, offering clear, didactic insights. The experiments provide valuable conclusions that future practitioners will find useful. However, a synthesis of the information could further enhance readability and understanding for the reader.

2- The use of the nonlinear spectral filters for graphs to achieve full equivariance to graph functional shifts may be a promising avenue to explore.

Weaknesses:
Despite these merits, I have the following concerns about the paper.

1- While the paper presents a compelling method with potential applications in graph analysis, one significant limitation is its scalability, particularly concerning large-scale graphs. The reliance on specific spectral properties, such as the leading eigenvectors, may not only limit the method's capacity to capture diverse graph dynamics but also result in computational inefficiencies when applied to extensive graph datasets.

2- The datasets used in the paper predominantly consist of mid-sized, homophilic graphs, which may not fully represent the diverse range of real-world applications, particularly in contexts involving heterophilic graphs.

3- The efficiency of the proposed models in terms of computation and resource utilization is not adequately discussed.

Limitations:
Also, the efficiency of the proposed models in terms of computation and resource utilization is not adequately addressed. For practical applications, especially in resource-constrained environments, understanding the computational overhead is essential.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper tackles the task of Network design for graph neural networks. The suggested approach is based on spectral properties of graphs. So far in the literature spectral methods were limited in assuming that the graph domain is fixed. To address this, a relaxed version of symmetry is proposed based on band-limited projections. In addition, a nonlinear spectral filter design is suggested, suggesting node-level, graph-level, and pooling operations. The method is evaluated on several graph learning tasks, demonstrating improvement in generalization over existing spectral methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper makes a valuable contribution to the literature on Graph Neural Networks (GNNs), particularly by addressing the challenge of transferability in spectral methods, which is highlighted as a significant issue.

claims are supported by theoretical analysis.

The paper is self-contained, providing both background information and a short overview on spectral graph learning.

Weaknesses:
Writing Quality: Some sections of the manuscript could benefit from revision. For instance, reordering the paragraphs in the introduction could improve readability. Specifically, mentioning what was missing from previous works earlier rather than at the end would help.

More examples can be found in the method section: i) The discussion on problem the activation functions is missing some details, e.g., what rho is exactly? ii) The paper states that ""It is important to note that functional shifts, in general, are not induced from node permutations. In stead, functional shifts are related to the notion of functional maps..."". This sentence is too vague. Consider adding more details to make it clearer.

No qualitative results are provided. Is it possible to visualize learned features as in the illustration in figure 2? Is it possible to design a toy experiment showcasing the suggested notion of relaxed symmetry, for which the suggested network design generalizes adequately?

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
y8HUXkwAOg;"REVIEW 
Summary:
This paper handles the problem of selecting all the minimal-size subsets of multivariate time series variables such that the past leads to an optimal predictive model for the forecast of a given target variable, which is essentially a time series feature selection problem. Past algorithms have worked to select a single such subset. The proposed algorithm is relatively efficient, in that it does not take as much longer than finding a single subset as one would think, but leading to more insight and better ""Markov blankets.""

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper handles an important problem in a clever way and is explained quite clearly.
2. The experimental results are convincing and actually include running time, which is often omitted.
3. The theoretical results look correct, although admittedly I did not comb through the proofs in great detail.

Weaknesses:
1. The proposed algorithm was only compared against GroupLasso and not against any other among the related work mentioned in the paper.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the problem of feature selection when forecasting multivariate time series. They propose a novel algorithm called ChronoEpilogi based on identifying a Markov boundary of the time series variables. They experimentally and theoretically validate the findings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A significant problem to tackle, 
2. Good formalization of the problem,
3. The paper is generally well-written.

Weaknesses:
1. Not all limitations are discussed: for instance, the model assumes that the selected set of variables remains fixed over time. When we deploy time series models, it is important that the method should work with different train and test time segments. However, since the set of variables is selected, generally, it might not be applicable to other train/test splits and thus might cause issues in real-world use.

2. In my view, the experiments could be more comprehensive. It would be beneficial to consider other forecasting models, baselines, and datasets to provide a more robust evaluation of the model's performance. 

3. Prior work discussion is incomplete: for instance, signature paths and feature selection should be discussed and compared to the method. Some specific examples include
    - Cross-correlation analysis,
    - Signature transforms https://arxiv.org/abs/1603.03788

Limitations:
In weaknesses, I’ve already commented that some crucial assumptions have not been discussed. These assumptions are critical to the framework, in my opinion.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a scalable algorithm called ChronoEpilogi that aims to select multiple subsets (Markov Boundaries) of time series (TS) features in order to better understand the underlying data generation process and to provide better explanations of downstream forecasting tasks. Through extensive experiments, the authors show that time series forecasting models perform better when fed with these subsets of TS features (individually) than when fed with all TS features.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-  Originality: Although the problem addressed in this paper is not new and the proposed solution is based on the combination of existing methods/models, the originality lies in the fact that, unlike previous work, the proposed algorithms provide a compact representation of mutually equivalent variables for multivariate time series (MTS). In addition, the redundant and irreplaceable variables in MTS can be relatively easy to identify.

- Quality: The document is well structured and the assertions are fairly well supported.

- Clarity: Overall, the document is well written and pleasant to read. However, the reviewer suggests improving definition 2. What does V stand for?

- Significance: The reviewer believes the proposed algorithm can be used as an alternative solution to provide explainability in diverse and sensitive fields such as medicine and autonomous vehicles.  Indeed, in these fields, when it comes to forecasting tasks, identifying the  the time series  features that influence the decision is as important as model accuracy.

Weaknesses:
- The experiment is not conducted with multivariate time series that have a high rate of missing values. This is a crucial aspect that the study should have taken into account, as missing values are inherent in time series and may affect causal inference.;

- The authors simply identify subsets of time series variables without providing concrete explanations that could have strengthened their claim. For example, the authors should have taken a few subsets (Markov Boundaries)  in any task and shown how they are actually relevant to the target;

- The reviewer understands the usefulness of greedy heuristics to speed up the algorithm. However, this heuristic has the disadvantage of providing suboptimal results. Although the authors demonstrate its effectiveness, it would be interesting in future work to test it on additional datasets covering different domains.

Limitations:
- The authors have identified the limitations of their work and plan to address them in their future work;

- On line 176 it should be forward instead of backward;

-  The reviewer suggests improving definition 2, which is the core definition of the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors presents **ChronoEpilogi**, an algorithm for multiple feature selection in multivariate time-series (TS) forecasting. This approach aims to identify all minimal-size subsets of TS variables (Markov Boundaries) that optimally predict a given target variable's future. The key contributions are:

1. **Theory Development**: Introduces the problem of multiple time-series variable selection (MTVS) and the concepts of informational equivalence and interchangeability in TS data.
2. **Algorithm Design**: Proposes ChronoEpilogi, which identifies all Markov Boundaries under broad, non-parametric assumptions.
3. **Experiments and Results**: Demonstrates ChronoEpilogi's scalability to hundreds of TS variables and its effectiveness in reducing the number of variables while maintaining or improving forecasting performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Some of the key strengths of the paper are:

1. The paper proposes a novel theoretical foundation for multiple feature selection in TS data including concepts like informational equivalence and interchangeability. Combining these concepts the authors have been able to propose an empirical method to detect Markov Boundaries
2. Furthermore, the proposed algorithm ChronoEpilogi is shown to handle large datasets effectively, making it suitable for real-world applications with numerous TS variables. This scalability is crucial to real world usage of the algorithm
3. Another key contribution for the paper is that ChronoEpilogi aims to identify all minimal-size subsets, offering multiple valid forecasting models and insights.

Weaknesses:
While being a very interesting paper, there are some avenues for improvement: 
1. While the authors discuss the scalability, the algorithm's computational complexity seems to be high for very large datasets, potentially limiting its practicality.
2. Some of at the assumptions that the algorithm relies on such as Compositionality and Interchangeability may not hold in all real-world scenarios, potentially affecting its generalizability. The authors should consider discussing the limitations in their papers and how well the assumptions hold in practice
3.The authors have provided thorough experimentations. However, to justify the practicality of the algorithm, it would be interesting to report additional validation on more diverse and complex real-world datasets

Limitations:
See weakness above

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the problem of finding all minimal subsets of variables for optimal prediction of time series data, coining the term ""Markov Boundaries"" for those minimal subsets constituting Markov Blankets for the target time series variables in question. 
The paper then proposes novel algorithms for this problem,FBE and FE, and prove the soundness and completeness of FBE (FE is a faster approximate algorithm) and empirically evaluate their performance. 
The experiments are conducted using both synthetic data (with ground truth causal structure) and real world data, and compare the performance of the proposed algorithms against baselines of Group Lasso (GL) and No variable selection, with respect metrics including predictive accuracy, accuracy of causal structure learning (for synthetic data), computation time and solution size. 
The results presented validate a number of claims about the proposed methods, the notable ones being that they are more accurate at uncovering the ground truth causal structure on synthetic data and FE roughly comparable to GL on real world data sets in terms of accuracy and computation time, sometimes significantly out-performing it in terms of solution size. 
The problem formulating is apparently novel and interesting, and the proposed methods are also novel and theoretically sound (and complete). The empirical results show that they are at least competitive to the standard baselines. 
This work would add some valuable knowledge and insights to the community with interest in causal modeling and interpretable learning in time series data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The problem formulation is novel and interesting and well motivated practically. 
The proposed solution is novel and sound and complete.
The empirical evaluation is reasonable.

Weaknesses:
The performance of the proposed methods against the baseline of Group Lasso on real world data sets is not exactly compelling. 
More clarify on the relative advantage of the proposed method(s) would be valuable. 
The optimal algorithm, FBE, is not evaluated on real world data sets, which I assume is due to computational complexity. It would be beneficial to know if any evaluation (even if partial) could be performed on FBE on the real world data.

Limitations:
The authors do mention that the claims are limited to the scope of their empirical evaluation which could be enhanced. 
It would be additionally desirable to address the question of quantifying the statistical confidence of the outputs of the algorithms.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
y7oxY5pq4j;"REVIEW 
Summary:
This paper addresses inverse rendering in high-illumination scenes with strong shadows where past methods bake shadows and highlights into estimation results. This paper proposes to use ACES tone mapping and makes it scene-dependent for inverse rendering in high-illumination scenes. This paper also proposes to directly estimate the visibility of each spherical Gaussian of direct illumination instead of a visibility field, which enables an accurate representation of shadows at the edge. The experimental results on the synthetic and real-world datasets show that the proposed method can estimate accurate albedos, surface roughness, and illumination without artifacts in the high-illumination scenes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ This paper proposes a novel regularized visibility estimation that enables an accurate representation of shadows at the edge.
+ Experimental results show that the proposed method successfully estimates BRDF and illumination while existing methods suffer from artifacts. They also indicate the effectiveness of ACES tone mapping compared with log tone mapping methods.

Weaknesses:
- It is unclear why the ACES tone mapping, of which usage is the key contribution, enables the robust inverse rendering of high-illumination scenes with strong shadows.
- The proposed method loses the albedo of the detailed texture due to smoothness loss in Eq. 10. For example, Bear in Fig. 7 and truck in Fig. 4.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for the inverse rendering of high-illumination and highly reflective scenes. There are two training phases, in the first phase, it trains by Neus, to get geometry and compute visibility by octrees. In the second phase, it decomposes lighting as SGs and material by MLPs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Direct and indirect lighting, visibilities are presented by SGs, which is compact. 
From the results, shadows and specularities are decomposed well. 
Tone mapping is used, as NeRF in the Dark, which improves results for high-illumination scenes.

Weaknesses:
Figure 2 gives comparisons with and without smooth loss, however, the one w/o smooth loss is better, while the loss may over-smooth the details. 
Figure 11 shows results where the lighting consists of the original colors.

Limitations:
See above.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces RobIR, an inverse rendering approach that can better tackle “high-illumination” scenes. RobIR first leverages the existing neural field model (NeuS) to represent 3D geometry information including normal, visibility, and indirect illumination. It then utilizes these geometry priors to decompose rendering attributes of the scene through the approximated rendering equation with spherical Gaussian. This work introduces an optimizable ACES tone-mapping and regularized visibility estimation model to better handle HDR color and shadow occlusions, respectively. Their experiment demonstrates some impressive results on shadow disentanglement.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work has some further thoughts on color tone mapping for the typical multi-view inverse rendering. The proposed optimizable ACES tone mapping looks very effective in improving inverse rendering results.
2. The proposed visibility representation (RVE) also plays an important role in the final results. RVE with its neural net accumulates and denoises Monte Carlo samples to achieve more accurate and stable visibility evaluation. Their efforts to refine the visibility should be appreciated.
3. I am glad the authors clearly point out they use the original NeRF rendering of Hotdog and Lego, instead of the NeRFactor’s.

Weaknesses:
1. This paper still follows a commonly used multi-stage inverse rendering strategy with neural fields. The geometry representation is based on NeuS; the rendering formulation (SG rendering, visibility, and indirect illumination) is mainly based on InvRender; The proposed optimizable ACES tone and REV are more like incremental improvements over InvRender. The key rendering formulation and optimization remain the same as the prior works. Therefore, the novelty of this work is moderate.
2. The description of RVE in Sec. 3.4 is not very clear.  It seems that MLP $Q(x, \tau)$ directly outputs N visibility ratios, thus $\eta(x)$ in Eq. 12 should also be an N-dim vector instead of a scalar value.
3. The proposed method is quite time-consuming. Training time even without NeuS is around 5 hours.
4. The proposed regularization terms may hurt the high-frequency details in real-world scenes (Fig. 7).
5. This method is limited to dielectric materials, without the consideration of metallic and glossy objects, as already pointed out by the authors.
6. The paper should include some inverse rendering methods with differentiable path tracing, as these methods can explicitly handle visibility, for example, NvDiffRecMC, Mitsuba, etc.
7. Minor errors:
    * L275 accuurate -> accurate
    * Figure 8: Hotdog label is wrong.

Limitations:
The limitations are adequately discussed in the last paragraph of the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces RobIR, an inverse rendering approach designed to handle strong or directional illumination scenes with strong shadows and specular reflections. 
The proposed method aims to decouple environment lighting and object materials, with the goal of producing high-quality albedo without baked shadow.

Building on top of prior inverse rendering methods such as InvRender, RobIR introduces two components that further boost the reconstruction quality: (1) ACES tone mapping with an optimizable gamma parameter to better capture the image formation process; (2) regularization for visibility estimation. 

RobIR demonstrates better performance over existing methods in quantitative and qualitative evaluations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The model design choices are valid and sensible. 
- The experiments and ablation study are thorough. 
- The paper is well written and easy to follow.

Weaknesses:
[W1] The benefit of ACES tone-mapping is a bit surprising. Despite a more accurate formulation for the image formation process, the task of inverse rendering is inherently still an ill-posed problem. It’s a surprising conclusion that a tone-mapping formulation can robustly and significantly benefit shadow removal. 

With many other regularization terms entangled, it’s a bit hard to evaluate the correctness of this specific component. 
I’d hope to know more details in the following aspects: 


[W1.1] Missing visualization of the tone-mapping curve. Despite an important contribution, the estimation results of the tone-mapping curve are missing. What does the default ACES tonemapping look like, and what does the final tonemapping look like with the optimizable gamma? 

In the revised version, the curves should be qualitatively visualized and included in main paper. From the existing experiments (such as the PSNR metrics), the audience cannot intuitively understand how well the tonemapping is estimated. 

[W1.2] Missing evaluation of the tone-mapping curve. 

As many datasets do not have GT tonemapping, it’s unclear how accurately the tone-mapping approximate the GT tonemapping. 

One way to evaluate is to add additional tone adjustment to the input dataset. Assume with the original dataset images $\{ I \}$ and the method originally reconstructs a tonemapping curve $f$. Given a new tone adjustment function, e.g. $g(x) = x^\kappa$, the adjusted dataset images become $\{g(I)\}$. Re-running the method can get a newly reconstructed tonemapping curve $f_\kappa $. The consistency between $g \circ f$  and $f_\kappa$ can indicate how well the model can approximate the additional introduced tone adjustment function. $\kappa$ can be set to values like 0.5 or 2. 

[W1.3] The evaluation metric on the Albedo is flawed. As albedo estimation/optimization often involve an unknown scale, PSNR alone is not a proper evaluation for Albedo. Check out [1] for more analysis and more appropriate scale-invariant metrics.

[W1.4] Most of the results are from synthetic datasets, where the GT tonemapping could potentially be close to ACES. For real-world results in Fig.7, the albedo estimation looks strongly regularized and over-smoothed. 

[W1.5] Are the radiance values (before tonemapping) and indirect illumination in HDR? If so, what is the activation function? 

[W2] The proposed method involves a complicated training pipeline (two stages with each stage have its own loss scheduling), and the novelty is relatively limited. 


**References** 

[1] Grosse et al., Ground truth dataset and baseline evaluations for intrinsic image algorithms, ICCV 2009.

Limitations:
The paper discussed limitations in the end of paper. It’ll be beneficial to include failure cases in Appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
y6qhVtFG77;"REVIEW 
Summary:
This paper introduces NeuroBOLT, a transformer-based model. NeuroBOLT utilizes multi-dimensional representation learning across temporal, spatial, and spectral domains to translate raw EEG data into comprehensive fMRI activity signals across the entire brain. Experimental results showcase NeuroBOLT's ability to effectively reconstruct resting-state fMRI signals across primary sensory, high-level cognitive areas, and deep subcortical regions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper tackles one of the most challenging and competitive topics in neuroscience.
2. The motivation behind the paper is quite clear, and the experimental section is logically sound.
3. The figures and tables in the article are clear and well-organized, making it highly readable.

Weaknesses:
1. The method abbreviation and the title are not closely related. It is unclear where 'BOLT' comes from in the title, and even after reading the abstract, it remains confusing.
2. In fact, there has been a lot of work on fMRI-EEG in recent years, especially in 2023 and 2024, but the author's related work lacks a significant amount of relevant literature.
3. In the abstract and introduction, the author's description of the method is inconsistent with the organization in the methodology section, resulting in a need for improved readability.
4. The writing of the article needs to be further standardized. For example, 'FMRI data' is sometimes written with a capital 'F' and other times as 'fMRI data'.
5. Although the layout and presentation of the tables are aesthetically pleasing, the font size is too small, making them difficult to read even when enlarged.
6. The paper does not provide code or data to support the reproducibility of results. 
7. This paper lacks details on the parameter selection for the baseline methods. Although the authors state, 'The baseline models are from [44] and [16], where we choose the models with the best downstream classification task performance,' the datasets and tasks in references [16] and [44] are not entirely consistent with those in this paper. Therefore, the authors should specify the exact process of parameter selection.
8. The equations and symbols in the article are not very standardized. The authors should provide notation to help readers understand.
9. The authors should conduct statistical tests to validate the significance of their methods.
10. For readers in the NeurIPS community, the theoretical contribution of this paper appears to be weak.
11. I don't quite understand what the author means by the third point of contribution: 'Successful resting-state fMRI reconstruction To our knowledge, this is the first study to successfully reconstruct the resting-state fMRI signal from raw EEG data, with only 26 electrodes.' What is the significance of 26 electrodes?
12. Equation 2 does not appear to be a complete equation.

Limitations:
Limitations are discussed in the section of Discussion and Conclusion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The manuscript proposes an EGG-to-fMRI synthesis model. The framework implements a transformer architecture and uses a multi-channel feature combination expanded across the temporal axis. To evaluate the proposed model, EGG and fMRI data from 22 participants were recorded while they were in the resting state with eyes closed.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The manuscript addresses an interesting problem and can open opportunities for multimodal neuroimaging analysis. Overall, this line of investigation is little explored, therefore, the present manuscript is novel and of interest to the community.

The present manuscript is quite complete, (1) the model and rationale behind are sound; (2) a dataset is collected which allows a faithful evaluation of the proposed translation (from EEG to fMRI); (3) it's rather easy to read and follow the manuscript, (4) the reported results are promising.

Weaknesses:
The biggest weakness is that the framework and the dataset are only addressing the resting state. While this is an important baseline to investigate, it would have been great to explore the fidelity of the proposed framework when participants are presented with some stimuli.

It is unclear whether the source code and dataset will be released publicly.

The stability of the results is fully guaranteed given no statistical analyses are performed.

Limitations:
The manuscript sufficiently discusses its current limitations. I think the limitations section should tap into the particular scenario that the model has been evaluated on (resting state) and whether the results will be generalisable to other scenarios remains to be shown.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors present a deep learning architecture for inferring functional magnetic resonance imaging (fMRI) signal from electroencephalography (EEG) data. The proposed model, named NeuroBOLT, utilizes transformer backbones to provide spatial, temporal, and frequency-based features from the EEG are utilized for reconstruction. The authors demonstrate the performance of their architecture on a small (N=22) data set using a propriatary data set of simultaneously measured EEG-fMRI.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
fMRI reconstruction from simultaneous EEG is a fascinating topic, and a difficult problem to tackle. The approach taken by the authors in this work is novel for the task at hand, i.e. using a multi-scale spectral feature embedding. Although the decision to use multi-scale spectral embeddings is not new in MRI analysis, as far as I could find the approach has not been utilized for this particular problem and the authors address novel problems for their application to simultaneous EEG-fMRI data in a deep learning architecture. At best this paper is a novel methodological tweak applied with state of the art architectures to see improvements over other deep learning baselines.

The breadth of the experiments attempted by the authors is promising; however, see my discussion below for more of a discussion of the limitations of the experiments performed. 

The authors also perform an ablation study to explore how the inclusion of Multi-Scale Spectral features improves model performance, thus demonstrating the benefit of combining the multi-scale spectral features with the spatiotemporal. This is well appreciated.

Weaknesses:
The major weaknesses of this work come down to weaknesses in the empirical evaluation. I am afraid that in its current state, the evaluation does not lead to a convincing demonstration of this method for fMRI reconstruciton, and the claims in the introduction about novelty coming from the application to multiple brain regions and resting-state fMRI seem somewhat overemphasized. Currently this brings it to a full reject as the paper is otherwise sound but the limitations in the evaluation are significant enough to bring it well below the threshold, and cannot be easily addressed in the rebuttle I believe. 

First, I will highlight the lack of reported standard deviations or error bars in any of the results. No standard deviations are provided in tables 1 or 2, or in any of the figures providing results. In the checklist, the authors state ""Error bars are not reported at the current stage because it would be too computationally expensive to compute over all the brain regions and for all participants,also there is limited space in the paper to put all the statistics. But we could always add this information if reviewers think it’s important to know.""

While I appreciate the authors' acknowledgement of this exclusion, I do think error bars are absolutely necessary to demonstrate the efficacy of the proposed method. The demonstrated improvements are often quite small (e.g. improvement from 0.540 to 0.588 in table 1), and it is not clear if the purported improvements can be explained away from model noise. I could not find any information about controlling model initialization or seeds as well to ensure that random initializations played a less significant role between experiments even with the same architecture on different regions. I absolutely think error bars are necessary for this work, and the reasoning provided by the authors is not mitigated elsewhere or behind a more significant barriers other than training and evaluation time. Additionally, the authors could have mentioned this omission in the limitation section of their main paper since I had to go to the checklist to be sure the authors were aware of the issue.

Second, in the abstract the authors highlight the ability to ""generalize to other brain areas"" and ""other conditions (such as resting state)"".  Unless I am missing something, I cannot find any experiments by the authors that address these particular gaps. The authors do provide inter- and intra-subject predictions which is interesting; however, their model is still only trained on individual ROIs, and they don't include any experiments demonstrating transfer learning between models trained on other regions, and they do not include any experiments studying other tasks BEYOND resting-state fMRI. Thus, the paper falls into the same limitation as past works which were only focused on task, just in the other direction. This work would have been much more compelling if they could demonstrate a model which trained well both on task and rest related data, or even better, which could reconstruct task-related data despite only being trained on resting-state fMRI. The acknowledgement of the limitations in the literature is thus misleading as the proposed method still suffers from these same limitations. The choice to only demonstrate the results for several ROIs highlights this limitation - it would be okay if the authors did not seem to imply elsewhere that their model gets around the single-ROI training approach from past methods. 

Clearly, the N in this study is quite small. This is to be expected as simultaneous EEG-fMRI is still quite rare as a sequence to collect; however, the authors seem to gloss over all of the myriad issues which will come with training their data over such a small data set. I am not penalizing this work for the small N in and of itself, but as I cannot find any mention of common obstacles such as overfitting, bias towards particular kinds of reconstruction errors, and other limitations that would inevitably arise. I am extremely surprised I could find no mention of pretraining anywhere in this work, which I almost imagine would be hugely necessary for these kinds of studies with very small data sets. Again, it's not necessarily a limitation in and of itself to not do these things, with how the paper currently reads these seem to be touted as benefits of the new approach which are not backed up by evidence.

Limitations:
The authors do provide some discussion of the limitations of their work; however, as I have noted above, there are some limitations which are ommitted from the main body of text which at least should have been acknowledged in this section.

The authors state they have IRB approval in section 4.1 of their paper. I see no reason for additional review.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
y6JotynERr;"REVIEW 
Summary:
The authors aim to develop a knowledge distillation method that addresses the challenges posed by heterogeneous device prototypes in federated learning. By capturing the knowledge transfer among device prototypes, the proposed TAKFL tries to preserve each device's unique contribution and prevent knowledge dilution during the learning procedure. The method incorporates a self-regularization technique to address issues of the noisy and unsupervised ensemble distillation. Evaluation of some CV and NLP tasks shows the performance of model accuracy and scalability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors focus on knowledge distillation in heterogeneous settings, which is meaningful to real-world tasks. The overall presentation is clear and easy to understand. The proposed TAKFL shows some primary theoretical analysis of the learning efficiency. The evaluation provides a quantitative analysis to compare model accuracy and scalability with previous methods.

Weaknesses:
While the paper presents a clear presentation and evaluation, there are a few aspects to be strengthened. 

1. The concept of ""task arithmetic"" in the context of federated learning is vague. It would be beneficial to explain how this concept enhances the design of the federated learning process.   
2. While the knowledge distillation process is described, it appears to be largely based on existing methodologies. It would be interesting to explore any novel design elements introduced in TAKFL. Additionally, what is the theoretical convergence order of the proposed method?  
3. Although TAKFL demonstrates higher model accuracy in performance comparisons, the baselines used, such as FedAvg and FedDF, seem outdated. Incorporating more recent baselines could provide a more rigorous evaluation of TAKFL's effectiveness.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper focus on a problem that traditional federated learning methods fail to effectively handle scenarios where devices have widely varying capabilities. It improve existing Knowledge Distillation (KD) methods that are inadequate in these heterogeneous environments. Experimental results show the validity of their proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. TAKFL treats knowledge transfer from each device prototype’s ensembles as separate tasks and distills them independently.
2. The paper is well written with comprehensive experiments

Weaknesses:
1. Assumption on Weight Disentanglement Property: The theorem 2 reply on the assumption of the weight disentanglement property  (line 679-681 in appendix)  is  too strong. In practice, achieving weight disentanglement is challenging. Studies [1,2] demonstrate that there are interferences among task vectors, making disentanglement difficult. [3] achieves disentanglement using Neural Tangent Kernel (NTK) and shows without disentanglement the performance is dropped. Consequently, asserting the disentanglement property is problematic, thereby limiting the theoretical impact. 

2. Similar to weight conflicts when doing merge, averge logits also have conflicts, the paper only considers vanilla average logit as the KD loss. However, there are studies to resolve the issue, like [4,5] have proposed better ensemble KD loss designs, more studies online. Therefore, incorporating these methods for comparison is important.

3. The computation cost of the method is high as the number of distillation process is O(M^2) which exponentially increases with the prototype number.

4. The changes compared to [6] are minor. Initially, I believed this method could be simple and efficient. However, upon reviewing the weight disentanglement property, I have concerns about its practical validity, which limits the novelty of the approach.

5. A minor issue is only apply to data with prototype label, thus may limited its impact
 
[1] TIES-Merging: Resolving Interference When Merging Models

[2]  Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch

[3] Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models

[4] Adaptive Multi-Teacher Multi-level Knowledge Distillation

[5] Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space

[6] Ensemble distillation for robust model fusion in federated learning.

Limitations:
please refer to Weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel framework called TAKFL, which addresses the challenge of transferring knowledge in federated learning across heterogeneous devices, ranging from small IoT devices to large workstations. TAKFL uniquely handles the knowledge distillation by treating the transfer from each device prototype as a separate task, allowing for tailored integration of knowledge to optimize performance. The approach is validated theoretically and through extensive experiments, demonstrating superior results over existing methods across both computer vision and natural language processing tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) Practical Problem and Innovative Approach
The authors address a significant, real-world challenge in federated learning—knowledge transfer across heterogeneous devices. Their novel framework, TAKFL, innovatively treats each device's knowledge transfer as a separate task, allowing for customized integration. This tailored approach is both practical and theoretically sound, making it a substantial contribution to the field.

(2) Clarity and Organization
The paper is well-structured, facilitating easy understanding of complex concepts and methods. The clear presentation enhances the accessibility of the content, making it easier for readers to grasp the significance of the proposed solution and its impact on the field.

(3) Strong Experimental and Theoretical Support
The authors back their claims with extensive experimental results across multiple tasks and datasets, demonstrating the effectiveness of TAKFL in diverse scenarios. Moreover, the inclusion of theoretical analysis adds depth to the validation, reinforcing the reliability and scalability of their approach. This combination of empirical and theoretical evidence strongly supports the paper's contributions and conclusions.

Weaknesses:
(1) [main concern] Strong Dependence on Hyperparameters
The TAKFL framework introduced in the article significantly relies on the setting of hyperparameters, especially during the Task Arithmetic Knowledge Integration process, where the weights for different task vectors are set as hyperparameters and adjusted on a validation set. This might limit the method's generalizability across different real-world applications. To enhance the practicality and robustness of the method, it is recommended that the authors explore more automated hyperparameter optimization strategies to reduce the need for manual tuning and improve the adaptability of the model.

(2) [main concern] Strong Assumptions in the Selection of Public Datasets
The experimental design involves the use of public datasets, such as CIFAR-100 and ImageNet-100, for knowledge distillation. This choice seems to be based on two key assumptions: that the public datasets must exhibit high diversity and that the training data distribution can be approximately considered a subset of the public dataset. These assumptions may not always hold in practical applications, so it is advisable for the authors to thoroughly investigate the actual impact of these choices on model performance in future work. Additional experiments could validate the effectiveness of these assumptions, and considerations of these potential limitations should be explicitly stated in the manuscript.

(3) [minor concern] Quantification of Data Heterogeneity and Hyperparameter Selection
The authors utilize a Dirichlet distribution to quantify Data Heterogeneity, setting $Dir(\alpha)$ at 0.3 and 0.1 to simulate varying degrees of data heterogeneity. However, there is insufficient explanation for the choice of these specific values. To enhance the transparency and reproducibility of the research, it is recommended that the authors provide a detailed rationale behind these parameter choices, based on logic and references to previous studies. Moreover, to give readers a more intuitive understanding of the data distribution differences under different $\alpha$ settings, descriptive statistics or visualizations, such as the distribution of samples across categories, would be helpful.

Limitations:
N.A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduced a KD-based framework (TAKFL) to address the dilution and diversity issues in heterogeneous FL knowledge transfer learning. The TAKFL distills knowledge from prototypes of varying sizes and incorporates a self-regularization to mitigate noise simultaneously, then integrates these separately distilled knowledge by task arithmetic. Empirical evaluations across various CV and NLP datasets demonstrate the framework's effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-organized and easy to follow.
2. The paper novelty introduced a theoretical model to illustrate the efficacy of knowledge distillation in heterogeneous FL.
3. The paper proposed a new framework, TAKFL, for considering varying sizes of prototypes with different contributed information, and experiments on different CV and NLP datasets show its effectiveness.

Weaknesses:
1. Some baselines are lacking. For example, FedProto [1], which also employs prototypes within device heterogeneous FL, should be included for a more comprehensive comparison. 
2. It seems that the proposed method incurs higher time and storage costs, as it requires the independent learning of multiple student models compared to the vanilla methods. The paper should provide an efficiency analysis that compares the proposed method with existing baselines, highlighting both time and storage metrics.
3. It would be better to provide a visualization study for a better understanding of the effectiveness of transfer learning from different prototypes.

[1] Tan, Yue, et al. ""Fedproto: Federated prototype learning across heterogeneous clients."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 8. 2022.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
y2fAmldTIf;"REVIEW 
Summary:
This paper proposes a data pruning algorithm for the training of Homomorphic Encryption (HE)-based neural networks. The authors introduce an HE-friendly importance score and client-aided masking to prune samples in the dataset. The authors further propose ciphertext-wise pruning to merge ciphertexts with empty slots, thereby reducing computational costs during training. Finally, the paper presents empirical studies to validate the effectiveness of the proposed data pruning method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The main advantages can be listed as follows:

1.	The paper provides a new data pruning method for data encrypted by HE scheme. The authors propose the HEL2N score, which substitutes the $\ell_2$-norm in the EL2N score with $\ell_1$-norm, and the client will select important samples based on the score computed by server.
2.	The paper proposes the ciphertext-wise pruning, enabling the server to merge ciphertexts with empty slots with the communication of the client.
3.	The paper conducted experiments on five datasets and compares the proposed method with the HETAL method to demonstrate its effectiveness.

Weaknesses:
Despite many strengths, there are some issues with the paper as follows:

1.	The submission requires further revisions for clarity and consistency. At line 43 “methds” should be “methods”; At line 74 and Figure 1 “CIFAR10” should be “CIFAR-10” as in Section 4; At line 326 “Table2” should be “Table 2”; At line 340 “Figure 4(4)” should be “Figure 4(a)”; Figure 1 and 4 should have sub-captions denoting which subgraph is (a) or (b).
2.	The computation costs associated with data pruning raise concerns. As given by Eqn. (1), computing the HEL2N score involves multiple gradient computations for each sample, which is computationally intensive. Moreover, ciphertext-wise pruning seems to require a large number of rotations, which is also a very slow HE operation. If the data pruning process is time-consuming, it may negate the benefits, making it more efficient to train directly without pruning.
3.	The novelty of the paper is questionable. The HEL2N score primarily modifies the $\ell_2$-norm in the EL2N score to an $\ell_1$-norm, and directly computing the square of EL2N score seems to be faster than HEL2N score. The trick of masking is also common place in HE literature. Moreover, one advantage of HE-based method is that they do not require any communications between server and client. The requirement for client-server communication in client-aided masking and ciphertext-wise pruning could diminish the significance of the proposed method.

Limitations:
The authors addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the scenario where the client encrypts the model and dataset with homomorphic encryption and outsources them to the server for training. It accelerates the training process through dynamic data pruning. This paper makes the following three contributions: 
First, this paper is the first to use dynamic data pruning to accelerate model training in homomorphic encryption (HE) scenarios. Second, because using the plaintext data pruning method in the HE scenario incurs significant overhead, this paper proposes an HE-friendly method for evaluating the importance of data samples. Lastly, because of the high cost of sorting in HE, this paper proposes that the client undertake this part of the computation. Additionally, since a single SIMD ciphertext can contain multiple data samples, pruning may not reduce the number of ciphertexts, even though the samples within each ciphertext become more sparse. To address this issue, the paper proposes to combine several sparse ciphertexts to reduce HE computation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) This paper is the first to apply dynamic data pruning to accelerate model training in HE scenarios.
2) It introduces a HE-friendly important score to make data pruning more efficient.
3) This paper uses ciphertext-wise pruning to reduce the number of ciphertexts while keeping detailed information.

Weaknesses:
1) The HE-friendly score needs more explanation. In this paper, the score is directly introduced without any theoretic proof of its effectiveness. 
2) The work is a bit incremental. Applying data pruning in the HE scenario doesn't seem very challenging, and there is no significant difference between data pruning in plaintext and ciphertext.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
1. The paper introduces a Homomorphic Encryption (HE)-based confidential training framework that enhances training efficiency through encrypted data pruning.
2. The paper proposes HE-Friendly Score (HEFS), an enhancement over the existing EL2N score, to efficiently assess the importance of encrypted data samples.
3. Due to the high complexity of sorting scores and calculating the pruning mask on the server, the paper introduces a method that generates data pruning masks with the assistance of the client, enabling the server to perform pruning.
4. The paper proposes a method for pruning at the ciphertext level to reduce sparsity in the encrypted data, thereby accelerating the training process.
5. The performance of HEFS, CAM, and CWP is evaluated on diverse datasets such as MNIST, CIFAR-10, Face Mask Detection, DermaMNIST, and SNIPS. The results are compared with the previous method, HETAL (ICML2023), to demonstrate improvements in training speed and accuracy.
6. The experimental results indicate that the proposed methods can accelerate confidential training by up to 16 times with minimal loss in accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper tackles the novel problem of accelerating confidential training through encrypted data pruning, a topic that appears to have not been previously explored in existing research.
2. The methodologies and experimental procedures are clearly explained, ensuring the reproducibility of the results by providing their code (although I have not tested the code yet).
3. Considering that HE training is a very challenging subject due to the computational complexity of operations on homomorphically encrypted data, it is noteworthy that the authors have implemented detailed techniques such as pruning within the homomorphic encryption framework for the first time. However, it is crucial that the design carefully considers both security and performance limitations.

Weaknesses:
1. There are concerns regarding the privacy threat setting in this paper. The focus is solely on the importance of the client's data privacy, without explicitly considering the server's model privacy. In other words, the server's model is assumed to be publicly available information. This assumption is reflected in the final step, where the client recovers the weights of the trained model and sends them back to the server.
2. If the server's model is publicly available, it would be more efficient for the client to process the data in plaintext after receiving the pretrained model. If the scenario involves a massive pretrained model, such as a large language model (LLM), which individual clients cannot train, then training such an LLM in an encrypted state would require at least 1000 times more computation on the server due to the difference in computational overhead between homomorphic encryption and plaintext operations. This level of computation may be unmanageable, and the final decryption of the model by the client would also be infeasible due to the enormous size (100 trillion parameters) of the model.
3. If the primary concern is the client's privacy, it would be significantly more efficient to have the client train on a pretrained model in plaintext, use federated learning, or adopt other methods rather than struggling with encrypted training on the server, as proposed in this paper. The bottom line is that if the server's model privacy is not considered in the security threat model, it is questionable whether this approach is practical or appropriate.
4. The HE.cmp operation does not yield a precise 1 or 0; rather, it produces a fractional value when the two compared values are similar. During the sorting process in Algorithm 1, swapping based on HE.cmp might introduce noise if the score differences are not substantial. This could affect performance. To avoid this, HE.cmp would need a high-degree approximation. The paper does not provide sufficient information on HE.cmp, and additional explanation would be beneficial.
5. Due to the overhead of homomorphic encryption operations, involving the client in the training process because of the complexity of sorting diminishes the method's utility. While the paper does not consider the server's model privacy, allowing the client to access intermediate values during training does not pose an additional security risk. However, in scenarios where the server's model privacy is a concern, this method could enable the client to gain critical information about the model. Increasing the client's role in plaintext processing during training could significantly reduce the server's burden and enhance overall performance. In extreme cases, the client could potentially handle the entire training process in plaintext. The paper needs to explain why the client should only assist with sorting.

Limitations:
They addressed it adequately.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method for pruning data in a utility-preserving way under homomorphic encryption, evaluating the method to demonstrate that the savings from training on pruned data outweighs the costs of encrypted data pruning computations. The methods for determining how relevant data items are to improving training performance are similar in spirit to those in the active/few-shot learning literature, but the paper does not explicitly draw this parallel.

Although the paper offers a concrete threat model related to ""private training"" (where model training is outsourced to a third party), several aspects of the threat model seem not to achieve the stated goal of limiting the outsourcing party/vendor's ability to learn useful facts about the data. As the current state-of-the-art is to bind third-party infrastructure providers contractually, I'd like to see a map from how the approach in this paper, in a threat model that is weak, could be strengthened to a threat model that would obviate these purely contractual limitations. For example, there are standard constructions to move from honest-but-curious models to models where the third party is more adversarial (although some reliance must always be assumed in the case of outsourced computation). More difficult are the problems of separating data flows from inferences about the content of training data through various sorts of indirect leakage (training time, ciphertext size, dependence of the presence/absence of the allowed ""early stop"" signal, etc). Although this problem is difficult in general, I suspect there are ways to organize the threat model here so that it can make stronger claims around solving them. At a minimum the threat model should declare indirect data flow out of scope while recognizing that it can leak data items to the outsourcing partner.

A few structural aspects of the paper confuse an otherwise solid presentation: a core assumption in the setup of the model in which the protocol is used is that the outsourcing partner will compare the sorted data items (under encryption) to a threshold importance score determined by the utility loss of pruning, but how this threshold is set/computed is left open (an experiment uses an ex vacuo value of 0.9 for this parameter, but why is not explained even in this concrete context!); although it is clear that the goal is only to outsource training, it is not clear that an organization without the infrastructure capability for training will have infrastructure capability for things like serving - this should be declared out of scope or unpacked/discussed a bit; approximations made to simplify computation under encryption, such as the replacement of $\ell_2$ with $\ell_1$ norm at 204-205, are not directly evaluated or justified; in general, the grouping of samples into batch ciphertexts is assumed but not explained - the paper should explain its necessity and the benefits it provides vs. the simple solution of putting each sample in its own ciphertext.

Although the evaluation is valuable in supporting the core claims of the paper, there are some structural issues there as well: although the experiments characterize the tradeoff between utility and pruning ratio, this tradeoff is very different for the two example datasets. How general ought a tradeoff curve for this be? How data dependent might it be? Relatedly, might pruning affect performance for different classes differentially, especially in situations where class balance is poor? Much of the ""fairness"" literature focuses on ways that aggregate analysis breaks down when distinct classes might or ought to be treated differently by the model. Does this affect the analysis? Could it in some cases?

Last, I observe that 4MB of communication overhead for a tiny database (CIFAR-10, 43750 samples) is manageable but there is no discussion of scaling here. Are the proposed applications small like this? If not, at what point is the overhead too much? Does scale cause this to break down? I here recognize that the paper inherits the inherent inefficiency of FHE constructions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The research question is well motivated and the solution a useful tool to making private outsourced training a more realistic option. I am not well versed enough in the FHE training literature to evaluate the novelty of this specific approach, but the core idea is sound.
* Evaluations justify the theoretical claims nicely, even where improvements are available.
* The overall argumentation is strong, even if some details are never defined or explained as noted in the summary.

Weaknesses:
* The summary notes several places where definitions can be sharpened or details can be re-ordered to improve the presentation.
* There are a handful of places where some copyediting would improve the presentation, although the high-level argument structure is in general strong.
* The key question of how the pruning ratio is determined must be explained, since it is an input to most of the provided private training algorithms and also a key determinant of the model in which the protocol is meant to be used.

Limitations:
As noted in the summary, there are places where limitations could be more clearly expressed, for example with regard to indirect data leakage, with regard to the determination of the critical pruning ratio parameter, and also with regard to aggregated analysis and generality of the pruning/utility tradeoff.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xojbzSYIVS;"REVIEW 
Summary:
The paper presents a framework that integrates large language models (LLMs) into sequential recommendation systems (SRS) to tackle the long-tail challenges. The framework includes dual-view modeling, which combines semantic embeddings from LLMs with collaborative signals, and a retrieval-augmented self-distillation method to enhance user preference representation. The authors validate their approach through extensive experiments on three real-world datasets, demonstrating significant improvements over existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1)	The dual-view modeling and retrieval-augmented self-distillation methods are novel contributions that enhance the performance of SRS.
2)	Utilizing LLMs to derive semantic embeddings for items and users adds a new dimension to the traditional collaborative filtering methods.
3)	The extensive experimental evaluation, including comparisons with multiple baselines and ablation studies, strengthens the validity of the findings.
4)	The paper provides comprehensive details on the methodology, including mathematical formulations and algorithmic steps, facilitating reproducibility.

Weaknesses:
1) There is a risk that the semantic embeddings might overfit to the training data, especially if the textual descriptions are not diverse enough.
2) The performance of the framework might be sensitive to the choice of hyper-parameters, which is not extensively explored in the paper.

Limitations:
The authors have addressed several limitations, but there is room for more in-depth discussion on potential biases introduced by semantic embeddings and the sensitive to the choice of hyper-parameters.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel framework designed to address the long-tail challenges in sequential recommendation systems (SRS). By leveraging semantic embeddings from large language models (LLMs) and combining them with collaborative signals, the authors propose a dual-view modeling framework and a retrieval-augmented self-distillation method. This approach aims to enhance recommendations for both long-tail users and items without adding significant inference load. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed framework.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper successfully integrates LLMs with SRS to address long-tail challenges, a novel approach that leverages the semantic understanding of LLMs while maintaining low inference costs.
2.	The dual-view modeling framework effectively combines semantic and collaborative signals, providing a comprehensive enhancement for SRS.
3.	This method innovatively uses interactions from similar users to enhance user preference representation, addressing the long-tail user challenge.
4.	The proposed framework is model-agnostic and can be adapted to any sequential recommendation model, making it highly applicable in real-world scenarios.

Weaknesses:
1.	The proposed dual-view and self-distillation methods add layers of complexity to the SRS, which may pose challenges in practical implementation.
2.	The framework assumes a certain level of similarity in user interactions, which might not hold true for highly diverse user bases.
3.	Impact on Popular Items: While the focus is on long-tail items and users, the potential impact on recommendations for popular items is not thoroughly explored.

Limitations:
Discussing potential negative societal impacts, such as reinforcing biases in recommendations, would be beneficial.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the challenges in sequential recommender systems (SRS), particularly the long-tail user and long-tail item issues, which complicate user experience and seller benefits in real-world applications. The authors propose the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR) to mitigate these challenges. The LLM-ESR framework leverages semantic embeddings derived from large language models (LLMs) to enhance SRS without increasing inference load. To tackle the long-tail item problem, the framework employs a dual-view modeling approach that integrates semantics from LLMs with collaborative signals from traditional SRS. For the long-tail user issue, a retrieval augmented self-distillation method is introduced to improve user preference representation by utilizing more informative interactions from similar users.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The work includes extensive experiments, testing multiple aspects of the model's capabilities.
    
- The approach is quite new. Recommender systems based on LLMs are a promising direction.

Weaknesses:
- The paper does not sufficiently and deeply discuss existing work, making the motivation and core idea of the paper seem less convincing, and the innovation of the paper is also insufficient.
    
- The baselines used in the experiments are limited.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wlLjYl0Gi6;"REVIEW 
Summary:
This paper proposes a learning-based rank predictor for scheduling LLM inference to reduce Head-of-Line (HoL) blocking issues, which significantly outperforms state-of-the-art LLM serving systems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper addresses an important question in LLM serving.
2. This paper is easy to follow with a good presentation.
3. The evaluation results are comprehensive and solid.

Weaknesses:
1. One potential issue with preemptive scheduling for LLM inference is the accumulated unused KV cache. How do you handle them when the GPU reaches the maximum memory limit? 

2. How much does the ranking model (OPT) size affect the prediction and throughput performance? For example, what if I use a smaller auxiliary model (OPT-125M) for a larger LLM (LLaMA-70B)?

3. How much is the performance gap between the ranking-based method and Oracle? It would be better if the authors could add such results to provide a performance upper bound.

Limitations:
Please see the weaknesses above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes an approach for optimizing scheduling in LLM  serving by learning a generated token length ranking model. The authors demonstrate that understanding the relative order of generation lengths can effectively guide the scheduling process, specifically through the use of SJF/ SRTF scheduling strategies.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written, making the methodology and results clear and easy to understand.
2. The experiments are well-designed and convincingly demonstrate the benefits of the proposed approach.
3. The proposed method has shown practical improvements when integrated with current serving techniques.

Weaknesses:
1. While the approach is effective, it builds upon existing work that has already identified the benefits of SJF/SRTF scheduling for LLMs[1][2]. The novelty is somewhat limited to the application of ranking loss instead of classification loss.
2. If we directly predict the token length, it could potentially offer advantages such as improved memory allocation and cache strategy adjustments, which are also crucial for optimizing LLM serving. In contrast, using relative order may not provide these benefits.
3. The paper lacks a thorough discussion of some related work,  such as [1][2]

[1] Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction

[2] Power-aware Deep Learning Model Serving with µ-Serve

Limitations:
see weakness above.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper reveals the Head-of-Line (HOL) blocking problems caused by the first-come-first-serve (FCFS) scheduling strategy in LLM services. To alleviate these problems, the authors train an OPT model to generate scores for evaluating the relative text length of given prompts. Based on these scores, the authors develop a novel scheduler for LLM inference and serving. Experimental results demonstrate the effectiveness of the proposed method, significantly outperforming the baseline method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method is efficient and effective. Training a small language model (i.e., a 125M OPT model) is cheap, and the resulting latency gains are substantial.
2. This paper is novel. Unlike traditional methods that predict the real generation length, predicting the relative ordering between request lengths is sufficient for ranking.

Weaknesses:
1. Since the request queue Q is re-ranked after each batch of data is scored, the ranking scheduler may be sensitive to the batch size.

Limitations:
See weaknesses and Questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the inefficiencies in scheduling LLM inference requests, which often use a first-come-first-serve (FCFS) strategy, leading to Head-Of-Line (HOL) blocking and reduced throughput. The authors propose a novel scheduling method based on predicting the relative ranks of output lengths in a batch of requests, rather than attempting to predict exact generation lengths. This prediction helps in approximating the shortest-job-first (SJF) schedule, which is known to minimize average latency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper employs a straightforward but effective scheduling algorithm that approximates the shortest job first (SJF) strategies. This approach effectively reduces response latency and improves throughput. The authors have tackled the challenge of accurately approximating SJF. The empirical results demonstrate significant improvements in both latency and throughput, highlighting the effectiveness of their approach. The paper introduces interesting metrics to determine the relative range of output lengths. 

The paper addresses a crucial issue in LLM workload scheduling. By focusing on reducing response latency and enhancing throughput, it tackles a significant problem that is highly relevant to the efficiency and performance of LLM servingsystems.

Weaknesses:
- The current scheduling approach only considers output length. Would you also consider other dimensions, such as prompt length? Longer prompt lengths can consume more memory and increase token latency, impacting overall response latency and throughput. Additionally, would you consider implementing preemptive scheduling to correct any mispredictions dynamically?

- Your predictor is trained using 10k traces from ShareGPT and LM-SYS. However, these traces are primarily from GPT-4 and other models. Have you considered that different models Llama3 might behave differently, with varying verbosity and output lengths even for the same prompts? If the predictor cannot be reused across different models, you might need to account for the overhead of retraining the model to maintain accuracy.

- You should discuss Andes [1], which also propose a request scheduling strategy to improve quality of experience. 
[1] Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services 

- SJF scheduling inherently risks starving requests with longer response length, as these jobs can be indefinitely delayed. How do you address this issue to ensure that longer requests are also processed in a timely manner?

Limitations:
See weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
y10avdRFNK;"REVIEW 
Summary:
This paper studies the problem of learning a diffusion process from samples. It proposes a new scheme based on learning the ""causes mismatch"" of the process, rather than the ""effects mismatch"" as in previous works. The new method is significantly more efficient than the schemes from prior works, and works well in practice.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written, and the scheme proposed seems to work well in practice on the examples it was tested on. The loss is intuitive, and resembles the score-matching loss from diffusion models, but is the analogous version for arbitrary diffusion processes. Overall, this seems like a paper that people at NeurIPS would be interested in.

Weaknesses:
I am not familiar enough with the literature, but it seems surprising to me that this scheme has never been proposed before. In particular, the loss is exactly the score-matching in the case of diffusion models, and there are works [1], [2] that have proposed a similar loss for arbitrary diffusion processes. 

[1]: https://arxiv.org/abs/2208.09392
[2]: https://arxiv.org/abs/2209.05442

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers learning diffusion dynamics from observational data of populations over time, identified as learning the energy functional in Equation 3.  Past research has confronted this inverse problem via complex bilevel optimization, limited to potential energies.  This paper proposes an alternative model JKOnet* that can work with potential, internal, and interaction energies, efficiently minimizes a quadratic loss instead of a complex bilevel optimization, has much lower computational complexity, and out-performs baselines in simulations.  A variant for linearly parameterized functionals has a closed form solution.  The paper's new method reconsiders the JKO scheme using first-order optimality conditions, resulting in decompose the problem into first computing optimal transport plans between adjacent populations and then optimizing a loss for fixed plans.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- Inferring diffusion dynamics from observational data is a difficult and significant problem for which this paper appears to provide a solid contribution.  The paper substantially improves upon JKOnet in terms of multiple directions: better performance (Figure 3), simpler optimization objective (Equation 11), better scalability and efficiency (e.g. Table 1, Section 4.2), and improved generality (Table 1, Section 4.3).  These dimensions are analyzed in experiments across a range of different energy functionals, where the gains are shown in log-scale displaying orders of magnitude improvement.  The paper makes a convincing argument for using JKOnet* over JKOnet.
- The methodology appears quite strong, well-motivated, and original, with solid intuition given by the authors throughout the paper.

Weaknesses:
Minor weaknesses:
- While the results are strong, occasionally the language feels too imprecise.  For example, ""runs at lightspeed"" seems inaccurate compared to ""runs very efficiently"".  The authors also mention that they rely upon weeks-old advancements in optimization in the abstract which seems unneeded.
-  The paper is generally very well-written except for the introduction which could use editing.  It introduces a lot of terminology and details from past research.  Similarly, Figure 1 is referenced multiple times including in the introduction but it was hard to understand until after reading Section 3. 
- The construction of the optimal transport plans does not seem to be included in the computational complexity comparisons.  While this is computed once for JKOnet*, it is additional expense over JKOnet.

Limitations:
Limitations are adequately addressed in Section 5

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study diffusion processes from the perspective of Wasserstein gradient flows. Based on the recent fixed-point characterisation for Wasserstein proximal operator methods, they introduce Jordan-Kinderlehrer-Otto (JKO) type methods for learning potential and interaction energies that govern the diffusion process. Such methods are assuming that a sample of the population distribution at each time step is at hand (not necessearily obtained by tracking individual particles) implying important applications across various fields. While theoretical novelties are present (w.r.t. paper [26] that lies in the foundation of this work), the main contribution is the overall methodology for learning diffusion processes.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Paper is, besides minor issues reported bellow, excellently written - very clear, precise and intuitive with well balances technical details between main text and the appendix. Existing ideas are neatly combined to obtain significant improvements of the JKO-type methods and extensive empirical evaluation is presented. The proofs seem correct and well-written.

Weaknesses:
While I do not find important weaknesses, I feel that next several small issues can be addressed to further improve readability:

1. When addressing content presented in the appendix it would be good to refer to the section, e.g. see Figure 6 in Appendix A.

2. It would be good to say what $\rho_t$ is in Example 2.1

3. While Table 3.1 reports per-epoch complexity for all the methods, it would be important to note that JKOnet$^*$ have additional computational complexity for solving $T$ OT problems of size $N$ in $d$-dimensions. Detailed remark on the initial computational complexity, depending of the algorithm used, should be reported.  

4. In Section 4 it would be helpful to introduce the problems, that is to better explain the task of each experiment and the role of functionals ($V(x)$ ?!)  appearing in Appendix F.  Maybe giving an example on Styblinski-Tang functional appearing in Figures 2, 3 and 4, and then referring to other ones by their names and/or reference equations.

Limitations:
Limitations are addressed adequately.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces JKOnet*, a new method for learning diffusion processes from data. It uses first-order optimality conditions of the JKO scheme instead of complex bilevel optimization. JKOnet* can recover potential, interaction, and internal energy components of diffusion processes. The authors provide theoretical analysis and experiments showing JKOnet* outperforms baselines in accuracy, speed, and ability to handle high-dimensional data. They also derive a closed-form solution for linearly parameterized functionals. JKOnet* offers improved computational efficiency and representational capacity compared to existing approaches for modeling diffusion dynamics from population data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Develops JKOnet*, a method using first-order optimality conditions of the JKO scheme to learn diffusion processes, avoiding bilevel optimization and improving computational efficiency.
- Provides theoretical analysis and proofs for JKOnet*, including a closed-form solution for linearly parameterized functionals, backed by comprehensive experiments across various test functions.
- Demonstrates improved performance in terms of Wasserstein error and computation time compared to existing methods like JKOnet, especially in high-dimensional settings.
- Enables recovery of potential, interaction, and internal energy components of diffusion processes, expanding the model's applicability to more complex systems and improving interpretability.

Weaknesses:
- The experimental evaluation is limited to synthetic datasets. Real-world data applications would strengthen the practical relevance of the method.
- While the paper discusses limitations, it does not thoroughly explore potential failure cases or boundary conditions where JKOnet* might underperform.
- The paper does not provide a comprehensive comparison with other recent approaches in learning diffusion processes beyond JKOnet, which could provide broader context for the method's improvements.

Limitations:
The author discusses limitations in section 5

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xzCuBjHQbS;"REVIEW 
Summary:
The authors derive a novel gradient descent step schedule from a Bayesian point of view, establishing a connection between Bayesian optimization and classical optimization. The theory gives support to some commonly chosen step schedules and is validated on MNIST dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well written, with clearly explained and carefully chosen notations. The figures are very pretty. It's a pleasure to read.
2. The paper has a good motivation. Worse-case theory, in general, can mislead people. Average-case studies are desired. The disparity between Bayesian optimization and classical optimization is quite obvious, and one can imagine there can be many optimization algorithms with mixed characteristics of both genres. The direction the paper explored is promising.
3. The research is very detailed and solid. The authors give sound proofs to their theorems and organise the results in a clear manner. The experiments are very extensive and well displayed.

Weaknesses:
1. The so-called ""average case study"" is not fully justified. The expectation of $J(w_n)$ is not in general equal to the expectation of $J(\theta)$ with $\theta$ fixed and then replaced by $w_n$. This is because $w_n$ is by itself a random variable. More concretely, suppose that $J$ is sampled randomly from $\mathcal{N}(\mu, C)$ with $\mu$ being a constant, say $\mu_0$. Then the expectation of $J(w_0)$ would be $\mu_0$ but the expectation of $J(w_n)$ for $n$ large would be much smaller than $\mu_0$. The method in this paper can only be thought of as average case study in the initial stage of optimization. The authors mention ""forgetful"" but I believe the problem is more serious than it looks. The authors also mention ""risk-affine"", but I don't necessarily agree with it. The claim ""Since RFD is defined as the minimizer of an average instead of an upper bound – making it more risk affine"" feels weak, because I don't think it's well justified yet that RFD is the minimizer of an average.
2. Incomplete story and lack of depth. Overall, there are lots of results but none of them are highlighted enough to be a gem. On the theory side, it's not clear whether there is any nontrivial key technical contribution in the proofs. It's not obvious that the derivation of the step schedule from a Bayesian viewpoint involves more than straightforward calculation. It needs more to stand as a strong theoretical paper. Furthermore, it would be better if there was a clear table presenting a convergence rate comparison of this new method and classical ones. On the empirical side, only MNIST is not enough, although the authors did a lot of experiments on MNIST. So as a new methodology paper, we need stronger empirical evidence. It's understood that the authors are studying a very hard problem, but excuses cannot serve as strengths of the paper.

Limitations:
Yes. Limitations are adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The current paper studies random function descent, draws connection between RFD and SGD, and derives an adaptive step size scheduler. More specifically,  the authors study minimizing a stochastic first Taylor approximation of random functions, which has similar form of gradient descent when the random function is Gaussian process. This connection also hints a step size scheduler for standard GD method. The authors then explore this step size scheduling scheme and study its asymptotic performance, which helps explain some recent step size scheduling tricks such as gradient clipping and warmup. Finally, the authors propose a practical way to evaluate necessary statistics required for the newly found step size scheduler with current ML mini-batch loss. The authors show simulation results for MNIST data to exemplify the effectiveness of the drawn step size scheduler.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper is very well-written, theoretically sound, and the findings seem new and pretty insightful, thus I feel it makes good contribution to research on optimizer learning rate scheduling. 

The main topic is about random function descent, and the minimization of stochastic first Taylor approximation of random function results in a gradient-type method (when Gaussian random function is considered) is surprising and impressive. 

The writing is well-organized, with all terms being properly defined and all theorems (Theorem 2.3, 4.2, 5.2, 6.2) well-formulated and capture core ideas. Theorems that are more representative is presented in main text for better digestion with more complete/general versions listed in Appendix. Theorems and Definitions are followed by simple and efficient explanations (i.e., discussion after Definition 2.1, Definition 2.2, around Theorem 2.3, and many others). Plots and tables are provided and are clean and easy to interpret.

The math is clean, sound, and rigorous, with very complete proofs (i.e., D.1.1 and D.1.2). Extensions are well-explored (Section E) and more general cases are discussed (Section E.3 for example). From first derivation of step size (Theorem 4.2), to its asymptotic version A-RFD (Definition 5.2) and its stochastic version S-RFD (Theorem 6.2), all are interesting and important findings.

Practicality of the proposed method has been considered. Though the proposed step size scheduler looks complicated, the authors figure out ways to evaluate necessary statistics required to put the step size scheduler into use (Section 6), and the effectiveness of proposed method applied to current ML tasks is also exemplified by examples (Section 7).

The research topic is valuable. Learning rate scheduling has been an open research area for a long time in optimization field. Currently in machine learning/deep learning research, a great deal of pressure comes from comparing with baseline methods which involves arduous hyperparameter tuning, among which learning rate is often the core. Thus studying learning rate scheduling is of great importance and this paper provides a novel connection between RFD and GD (with also extended comparison to Adam in Section E.1) which is very encouraging. Moreover, classical convergence result for optimization algorithms are mainly with worst case bound, RFD is instead for average case performance, the authors try hard and derive partial result for convergence (Corollary 5.3), and we expect there would be more study of difference between worst case performance and average case performance.

Weaknesses:
Though I appreciate the presentation quality, theoretical soundness, and novelty of the work. The main drawback of current paper boils down to three parts: lacking comparison with prior work, potential concerns with the practicality and effectivenss of the proposed method, and the (relatively) strict assumption of the theory.

1. The current paper doesn't involve literature review section, though it draws connection to prior work dispersedly, no systematic review has been intended. I currently make my evaluation of the novelty of the work based on my own (might be poor) understanding. I feel adding a related work section is desirable and then a more fair evaluation of value of current work can be made.

2. Still about prior work but for baseline method comparison. The simulation results (mainly Figure 3) only compares the proposed method with SGD/Adam with tuned fixed learning rates. More recent work such as D-Adaptation [1] also studies tuning-free learning rate scheduler for SGD/Adam, from not RFD perspective but more classical optimization angle, hasn't be mentioned/compared against. Moreover, the experiment in current paper seems much simpler and less thorough than the setting considered in D-Adaptation. 

3. With respect to practicality, though the authors provide empirical ways to evaluate covariance in mini-batch training, the recipe still looks a bit complex, i.e., one should go evaluate $C$ and $C'$ from the observation first. Unlike current adaptive gradient method such as Adam/AdamW, or even D-Adaptation, which only depends on some statistics involving current/past gradient/function values. Moreover, since RFD is measuring average case performance, it's more risk-affine and tends to predict larger learning rate, which may be harmful for convergence in some cases.

4. Despite that I feel minimizing stochastic Taylor approximation of random function is interesting and worth exploring, the derived GD-type algorithm is for Gaussian random function (Theorem 4.2), though the authors mention this assumption was also used in [2], it might be desirable to more demonstrate to which extent one should expect this assumption to be close to real settings.


[1] Learning-Rate-Free Learning by D-Adaptation (Aaron Defazio and Konstantin Mishchenko).

[2] Yann N Dauphin et al. “Identifying and Attacking the Saddle Point Problem in High Dimensional Non-Convex Optimization”.

Limitations:
Limitations have been discussed in Section 8.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Many machine learning model have parameters that are optimized by some form of gradient descent. Given a parameters $\omega$ in a space $\Omega$ and a loss function $\textbf{J}: \Omega \to \mathbb{R}$, typical gradient descent proceeds by picking a starting point $\omega_0$ and iteratively taking steps in the direction of steepest descent

$$
\omega_{n+1} = \omega_n - h \nabla J(\omega_n) = \omega_n - \eta \frac{\nabla J(\omega_n)}{||\nabla J(\omega_n)||}
$$
where $h$ is the learning rate, or similarly $\eta = $ is the step size. The learning rate/step size is an exogenous pre-determined user hyper parameter.

This paper proposes a method to automatically determine the steps size parameter. (I may have misunderstood and I welcome any correction by the authors) this method, called Random Function Descent (RFD), take a point $\omega$, computes the function value and gradient $J(\omega)$, $\nabla J(\omega)$, which is then used to fit a Gaussian process model. The GP model has a constant prior mean mean and a stationary, isotropic kernel, and by fitting one data point and it's gradient vector, the constant prior mean is updated to a still mostly constant surface however with a single local deformation at $\omega$ resulting in a peak in the uphill direction from $\omega$ and a trough on the direct opposite downhill side, . the RFD method jumps straight to the bottom of the trough, mathematically
$$
\omega_{n+1} = \text{arg min}_{\omega'} \mathbb{E}[J(\omega') | J(\omega), \nabla J(\omega') ]
$$
where the expectation is the posterior mean of the GP having been fit to the one data point. As the GP kernel is isotropic, there is no prior bias in any direction and the direction of the trough is exactly the direction of the gradient, consistent with normal gradient descent.

The paper considers many of the technical and theoretical hurdles and provides solutions in each case. Finally experiments with MNIST are provided. 

I somewhat struggled with the paper and have set my confidence score to low accordingly.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- tuning the baselines in the numerical experiments

Weaknesses:
Unfortunately for me, I struggled to understand much of the paper, I believe this could be partially be due to writing style, I have tried to keep my technical and writing comments separate

I apologize if my understanding is incorrect, and look forward to the authors response to correct any such errors.

- all the parameter updates are using euclidean distance in parameter space. In contrast, Natural gradient descent makes parameter updates that have equal distance in output distribution space. In practice, I believe an approximation is implemented by using inverse squared gradients for each parameter similar to ADAM/RMSprop that use root mean squared gradients. Obviously, 

- the numerical experiments seem a little lacking, RFD doesn't appear to show a significant improvement on Figures 3, 6, 7. MNIST and FashinMNIST are very small and perhaps too easy, any optimizer will ""max out"" any model pretty quickly I assume.


The below points are my personal subjective comments on the writing.
- I am a little reluctant to agree that this paper has much to do with Bayesian optimisation as suggested by the abstract and introduction. RFD fits a GP model to a single data point and  only uses the posterior mean, it is the same as kernel ridge regression.
- I felt the terminology of ""stochastic Taylor Expansion"" was rather unhelpful and somewhat counterproductive. In my mind, zeroth/first/second order Taylor expansion refer to constant/linear/quadratic local polynomial approximations to a function, however the given function approximations are non-linear (lemma 4.12) this description unfortunately rather mis-directed my thoughts.
- L69: as above, ""it naturally incorporates covariance based trust"" assumes a lot of context that has not been introduced in the paper at this point, upon first reading I was rather lost, upon second reading it makes sense but felt out of place.
- (there are many topics and details covered the main paper, would it be possible to focus on a few big ideas?)
- Table 1, Figure 2, what is the scale ""s"", I assume the length scale in the covariance $C()$ function?  This appears not to be introduced in the paper.
- L62, should the final term of the equation be $\frac{L}{2}||\omega - \Theta||^2$?

Limitations:
- the assumption of isometric in parameter space

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
### Summary

The paper ""Random Function Descent"" explores the limitations of classical worst-case optimization theory in explaining the success of optimization in machine learning and selecting appropriate step sizes. It establishes a connection between Bayesian Optimization and classical optimization through a ""stochastic Taylor approximation,"" rediscovering gradient descent. This rediscovery introduces a new step size schedule called Random Function Descent (RFD), which is scale-invariant. The analysis provides a theoretical foundation for common step size heuristics such as gradient clipping and gradual learning rate warmup. The paper also proposes a statistical procedure for estimating the RFD step size schedule and validates this theory with a case study on the MNIST dataset.

In the introduction, the paper emphasizes the importance of cost function minimization in machine learning, typically performed using gradient-based methods that require step sizes chosen by established heuristics. The paper aims to enhance the theoretical understanding of these heuristics and proposes RFD as a new algorithm based on this deeper insight. The authors highlight that classical optimization theory, which relies on \(L\)-smoothness, provides conservative learning rates unsuitable for average cases, necessitating the reliance on step size heuristics in machine learning.

The authors bridge the gap between Bayesian Optimization (BO) and gradient-based methods by introducing a stochastic Taylor approximation based on a forgetful BO posterior. This results in the RFD optimization method, which combines the properties of gradient descent with scale invariance and a complete step size schedule derived from BO. The contributions include proving the scale invariance of RFD, discussing common distributional assumptions in BO, establishing the connection between RFD and gradient descent, and investigating the step size schedule suggested by RFD.

The paper further develops a non-parametric variance estimation method robust to covariance kernel choices and extends RFD to mini-batch losses. The case study on the MNIST dataset demonstrates the practical application and effectiveness of the proposed RFD algorithm compared to traditional methods like Adam and stochastic gradient descent (SGD). The discussion includes limitations and potential extensions of the proposed method, emphasizing the need for new mathematical theory to address the risk-affine nature of RFD and its larger step sizes.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
### Strengths

1. **Innovative Approach**: The paper introduces a novel connection between Bayesian Optimization and gradient descent through the stochastic Taylor approximation, leading to the development of Random Function Descent (RFD). This approach provides a new perspective on step size selection and optimization in machine learning.
   
2. **Theoretical Foundation**: The analysis of RFD step sizes offers a solid theoretical foundation for commonly used heuristics such as gradient clipping and learning rate warmup. This bridges the gap between empirical practices and theoretical understanding.
   
3. **Scale Invariance**: RFD's scale invariance is a significant advantage, making it robust to different scales of input parameters and cost functions. This property is stronger than the affine invariance offered by the Newton method.
   
4. **Practical Validation**: The statistical procedure for estimating the RFD step size schedule and its validation on the MNIST dataset demonstrate the practical applicability and effectiveness of the proposed method. The case study shows that RFD can outperform traditional optimization methods like Adam and SGD.
   
5. **Comprehensive Analysis**: The paper provides a thorough investigation of the step size schedule suggested by RFD, including explicit formulas, asymptotic behavior, and explanations for gradient clipping and learning rate warmup. This comprehensive analysis enhances the understanding of RFD's behavior and potential benefits.

Weaknesses:
### Weaknesses

1. **Complexity and Accessibility**: The theoretical development and mathematical derivations in the paper are complex, which might limit the accessibility and understanding for practitioners who are not well-versed in advanced optimization theory and Bayesian methods.
   
2. **Assumptions and Simplifications**: The paper relies on certain assumptions, such as isotropic Gaussian random functions, which might not hold in all practical scenarios. The need for these assumptions could limit the generalizability of the proposed method.
   
3. **Risk-Affine Nature**: RFD's risk-affine nature, resulting in comparatively larger step sizes, might lead to instability in certain cases. The paper acknowledges this limitation and suggests that further work is needed to address this issue and develop new mathematical theories for convergence guarantees.
   
4. **Empirical Validation Scope**: While the MNIST case study is a valuable demonstration, the empirical validation is limited to a single dataset and a specific neural network architecture. Additional experiments on diverse datasets and models would strengthen the evidence for RFD's effectiveness.
   
5. **Variance Estimation Procedure**: The non-parametric variance estimation method, while robust, involves a bootstrapping procedure that could be computationally intensive. This might pose challenges for large-scale applications and require further optimization for practical use.

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xymhWyiZOp;"REVIEW 
Summary:
This paper identifies a major problem with anchored training, that the performance of anchored training does not increase with increasing reference set size, and proposes a simple regularization approach to overcome this problem. This approach is evaluated on OOD generalization, calibration and anomaly rejection, and task adaptation, and various facets of anchored training are analyzed.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper makes the interesting finding that the performance of anchored training does not increase with increasing reference set size, and that this problem is not alleviated by more sophisticated inference strategies. The paper also proposes a simple reference-masking regularization technique to help alleviate this problem. The experiments show the effectiveness of the proposed approach, and there is also analysis of how the method interacts with data augmentation and noisy labels. An ablation study of the $\alpha$ parameter is also performed. Training recipes are also provided, making the paper easier to reproduce.

Weaknesses:
One weakness is that the reference set selection strategy and reference set sizes are not explained for the experiments. 

The impact/novelty is a bit limited because of the lack of comparisons to non-anchored training works.

Minor points: in the tables, decreases in performance could be colored in a color other than pink. Figure 1 could be improved with error bars. One highlighting was missed in Table 3. The abbreviation LP is not defined.

Limitations:
The limitations of this work are discussed by the authors at the end of the paper. Negative societal impact is probably not a concern for this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors propose a new strategy to train anchoring-based models, significantly improving performance, training efficiency, and model generalization compared to previous approaches. The key to the method is the added masking strategy that allows the model to better profit from anchoring-based training. The authors demonstrate that modifications only in inference (using several samples or searching for the best references) or the number of used references do not improve model performance, while the application of the masking procedure significantly improves it, as shown on various image classification datasets, specifically CIFAR-10, CIFAR-100, and ImageNet, using different architectures (both CNN and attention-based). The experiments demonstrate the effectiveness of the proposed method and the significant benefit of using it for improved generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is clearly written and easy to follow. The idea is intuitive and easy to grasp. The related work section provides an adequate discussion of existing approaches to anchoring-based training. The analysis narrative, with the presented drawbacks of existing methods, is very clear and easy to understand.

* The idea of masking the reference input argument is very clear and logical. The intuition behind why the problem could occur: 1) argument size grows combinatorially, and therefore 2) the model could learn to ignore the reference argument; seems correct, which is further clearly supported by the experiments.

* The authors provided an extensive evaluation of their approach, spanning different datasets and architectures, which provides a solid grounding to support the proposed method.

Weaknesses:
* It seems that the evaluation could benefit from an additional comparison with other existing state-of-the-art OOD/uncertainty methods to better represent the quality of the results (not just in comparison with former anchoring-based approaches, but overall).

* From the perspective of the experimental evaluation, I would be curious to see evidence that the behavior demonstrated in the paper would hold in other domains, such as texts, graphs, more complicated vision tasks (e.g. segmentation), not limiting to image classification task.

Limitations:
No

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a thorough discussion on the use of anchoring for training vision models. In particular, the paper tackles 1) the problem of reference diversity when training with anchoring to explain how superior generalization can be achieved 2) addresses the problem of spurious correlations learnt between the residual and 3) how different inference-time strategies can enable greater out-of-support generalization. Overall, this comprehensive study of anchoring provides useful guidelines for how anchoring should be applied to extract maximum performance. The paper empirically confirms this via the proposed anchoring scheme outperforming prior work noticeably.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1) Clarity: The paper is very clearly written and easy to follow. Readers unfamiliar with the literature like myself are able to understand what anchoring is, how it can be useful for (out of support) generalization and how current methods fail to apply anchoring in the most effective way. 

2) Thoroughness of Evaluation: The paper conducts thorough ablations on several components of the anchoring pipeline. Reference diversity, reference masking, inference procedure etc. More

Weaknesses:
No obvious weaknesses.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors analyze the effect of anchored training through a series of small experiments and find that, contrary to claims in prior works, increasing the size of the reference set is not beneficial and that this shortcoming cannot be mitigated through existing inference strategies. The authors provide a simple yet efficient fix by randomly masking out the reference during training, and forcing the model to make high entropy predictions in those cases. This solution does not incur any training overhead, and the authors demonstrate in extensive experiments that the fix is applicable to different models and datasets, yields improvements for OOD performance over various distribution shifts, and improves calibration and anomaly resilience.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper is very well written and structured and is overall easy to follow. The initial experiments highlight the studied problem well.
2. The authors showcase an important limitation to existing anchoring techniques that was unknown to the community.
3. The proposed solution is simple and is demonstrated to consistently improve performance across models and datasets.
4. The experiment section is extensive and covers both OOD performance as well as safety-relevant metrics. The results convincingly demonstrate the effectiveness of the proposed method.

Weaknesses:
The paper is very well written, I don't see any major weaknesses that would prevent an accept.

Minor weakness: The optimal $\alpha$ is determined when using the entire dataset as a reference set. However, as is clear from the motivation, risk of spurious shortcuts is larger with a smaller reference set. Wouldn't this imply that the optimal $\alpha$ would be larger for smaller reference sets? How should this value be chosen in practice and for datasets larger than ImageNet-1k?

Limitations:
Limitations were sufficiently addressed, especially the empirical nature of the work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xxY8d4rnSb;"REVIEW 
Summary:
This paper presents a method to estimate 3D human keypoints from a sequence of monocular 2D keypoints observations. It builds upon an existing sequence-to-sequence architecture (MixSTE), with a different output parameterization exploiting a kinematic skeletton prior, and different training losses. Lengths of the skeletton bones are predicted for the whole sequence to ensure consistency across frames (and maybe also left/right symmetry of the skeletton), and five 3D pose hypotheses with associated scores are predicted for each frame, parameterized as a list of 3D relative orientation for each bone with respect to its parent in the kinematic tree.

The authors develop theoretical arguments regarding the benefits of enforcing such structural priors in the predictions, and illustrate with a toy example the interest of having multiple predictions in case of ambiguous multimodal output. They validate their approach on Human3.6M and MPI-INF-3DHP datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The motivation for exploiting bone lengths constraints is well expressed, with a clear and detailed discussion provided in Section 4. The discussion of experimental and ablation results is insightful and shows – in a setting dependent on an oracle – benefits of the proposed approach.

Weaknesses:
The idea of enforcing body priors (constant bone length here) is not novel and has actually been heavily exploited in a whole line of work relying on more advanced parametric models such as SMPL [100]. This line of work would deserve being considered in the paper, as it encompass approaches suitable for 2D-to-3D sequence lifting such as e.g. [101].

The authors present a pose space consisting in 3D coordinates of joints linked by some rigid segments. Based on this definition, a natural pose parameterization would consist in the 3D direction of each segment, yet the authors chose to overparameterize poses by using relative 3D bone orientation instead. I understand that such choice can have practical benefits in term of biomechanical constraints and additional supervision signal when ground truth data is available, but such choice should be properly motivated, discussed and ablated in the paper.


The authors describe two ways of aggregating results L247 but do not state which one they use for MPI-INF-3DHP, and they only report oracle results on Human3.6M and for the ablations.

In my understanding, pose hypotheses are selected independently for each frame and there are no temporal terms in the training objectives or aggregation method. Since the proposed approach deals with temporal sequences, it would be worth evaluating the temporal consistency of the predictions, through qualitative video examples and quantitatively e.g. using joint acceleration metrics. Having multiple hypotheses for each frame brings combinatorial questions worth discussing in my opinion.

References:
- [100] Loper at al., “SMPL: A Skinned Multi-Person Linear Model”, at SIGGRAPH Asia 2015.
- [101] Baradel et al., “PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling”, in TPAMI 2022.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a MCL-based framework for multi-hypothesis 3D human pose estimation. This framework predicts skeletal parameters so that the predicted 3D poses in a sequence are constrained to one smooth manifold. To prove the superiority of such a framework, the paper presents detailed theoretical analysis on the drawback of unconstrained single-hypothesis HPE and why MPJPE alone is not enough for pose evaluation. The experiments show the proposed framework is capable of keeping the consistency of predicted poses and achieving state-of-the-art MPJPE in the meantime.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* Simple and reasonable manifold representation. The proposed framework keeps the predicted human pose on the target manifold by representing the human pose with bone lengths and orientations, and the 3D pose is a direct inference from forward kinematics. The manifold is represented by the kinematics itself.
  
* Inspiring theoretical analysis on basic problems in 3D HPE. The paper arrives at some theoretical conclusions (line178-183), along with detailed proofs. They can provide some refreshing ideas on the innate drawbacks of traditional loss functions and MPJPE metrics.
  
* Good performance under both MPJPE and consistency measures, as validated in Table 2 and 3.

Weaknesses:
* Theoretical analysis on the advantage of multi-hypothesis methods over single-hypothesis ones could be added. Specifically, why a **constrained multi-hypothesis** method performs better than an **unconstrained single-hypothesis** method in MPJPE? Though this is already validated by the experiments, I personally believe it would make the paper more solid if the authors could make this analysis.

Minor problem:
* In Fig.4 (C) and (D), it is not quite clear how the estimations (crosses and triangles) correspond with the inputs (black dots). There might be some unexpected shifts, as the projections of the predicitons do not strictly align with the inputs (like in B).

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a new method to estimate 3D human pose from 2D observations (lifting). To ensure the body symmetry and temporal consistency, the authors disentangle human skeleton to two parts: temporally consistency bone scales and temporally variable bone rotations. The authors use fancy formulas to prove that, minimizing MSE loss could not gurantee manifold consistency. The quantitative and qualitative results on Human3.6m and MPI-INF-3DHP datasets show the superiority of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The evalution results in this paper is quite impressive, especially the newly proposed consistency metric. Figure 1 clearly shows the superiority of the proposed method. 

2. The authors try to prove the theoretical optimal of the proposed method, which is worth encouraging.

Weaknesses:
I am not an expert in manifold theory, therefore my questions only relate to human pose estimation. 

1. How to constrain the rotation space during training? 

2. The pose lifting method is quite similar to Anatomy3D (bone length + rotations). Can I view this paper an multi-hypothesis extension of Anatomy3D? Why? 

3. Previous paper ""POSE-NDF: MODELING HUMAN POSE MANIFOLDS WITH NEURAL DISTANCE FIELDS"" is similar to this paper in concepts. SMPL naturally guarantees bone length symmetry, and the learnable parameters (rotations and shape parameters) are similar to this paper in its functionality. It would be better to cite it. 

4. Suppose that, there is a virtual dataset, all 2D human joints are rendered (projected) from strictly symmetric 3D joints, then, could learning the lifting function on this virtual dataset using MSE loss guarantee the results all lie on manifold? 

5. (An optional question) The ground truth 3D joints of Human3.6M datasets come from the marker tracking on body surface, which naturally could not guarantee skeleton length consistency. Why learning symmetric bones yields better results (both Anatomy3D and the proposed methods)?

Limitations:
The authors addressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper propose ManiPose, a manifold-constrained multi-hypothesis model for 3D human pose lifting. The authors provide empirical and experimental evidence to show that joint position regression leads to inconsistent skeleton lengths. And they propose to predict globally consistent pose scale and individual joint rotations per frame (rather than joint positions) to constrain the predictions to the pose manifold. Empirical results demonstrates that the proposed ManiPose framework improves the pose consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper provides valuable theoretical analysis to support their arguments and provides intuitive toy examples to illustrate the ambiguity in pose lifting.
* The paper conducts extensive experiments on H36M and MPI-INF-3DHP datasets.

Weaknesses:
* The paper uses a multi-head design to predict multiple hypotheses. This design loses the flexibility of sampling different numbers of hypotheses and limits the maximum number of hypotheses to a small number. This often results in limited hypothesis diversity. In the experimental section, the authors do not provide numerical of visual measurements of hypothesis diversity.
* According to the comparison in Table 4, the manifold constraint proposed in this paper sacrifices MPJPE to improve pose consistency, serving as a trade-off approach between accuracy and consistency. Although the consistency is improved, it lags behind the traditional position regression or manifold regularization in accuracy, and does not bring essential improvement (improve both in accuracy and consistency) compared with these two methods.
* Missing comparison with two recent multi-hypothesis methods. [1] GFPose: Learning 3D Human Pose Prior with Gradient Fields. [2] DiffPose: Toward More Reliable 3D Pose Estimation.

Limitations:
As the authors discussed in the Limitations Section, they used forward kinematics to obtain joint positions, which can lead to error accumulation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xvYI7TCiU6;"REVIEW 
Summary:
The authors study MARL in heterogeneous settings, where agents are not allowed to share their parameters, and make use of the sequential updating scheme under the CTDE schema. They propose a method which exploits the preceding information to improve exploration and heterogeneity sequentially. This method is equipped with a mutual policy divergence maximization framework, which utilizes the discrepancies between episodes to enhance exploration and between agents to heterogenize agents. Interestingly, the authors propose the conditional Cauchy-Schwarz divergence to provide entropy-guided exploration incentives.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The problem of exploration in settings with heterogeneous agents is important in MARL and not well-explored in literature. 

- The paper is the first to study the effectiveness of policy divergence maximization in the sequential updating schema, upon which important related work has been built.

- The paper proposed the conditional Cauchy-Schwarz (CS) divergence as an alternative to the popular KL-divergence in MARL. Such an alternation may be interesting to the broader RL community. Interestingly, unlike KL-divergence which can expload for small values of the denominator, CS divergence has a provable good lower bound ($-\log(n)$) only dependent on the number of finite actions.

- The proposed method displays good performance, in comparison to strong SOTA methods (including MAPPO, HAPPO), on benchmarks with heterogeneous agents.

- The proposed framework is simple and easy-to-implement.

- The paper is generally well-written and easy-to-follow.

Weaknesses:
- The improvement over the baselines (standard KL-divergence, entropy term, no incentive) does not seem to be quite consistent in the ablation study, due to (a) high variance in the results of the no incentive, and (b) very close improvement over the KL divergence baseline in terms of best episodic reward in 2 out 3 tasks.

- Since the CS divergence is new in MARL and RL, a table containing the running times of the evaluated algorithms is missing. How costly is the CS divergence?

Limitations:
The authors provide limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel training objective where it encourages the policies to diverge from each other and from the previous policy under heterogeneous multi-agent tasks based on sequential recently proposed sequential policy update. It utilizes CS divergence for calculation of ""distance"" between policies for tractable and stable optimization compared to KL divergence. The evaluation is done in high-dimensional multi-agent mujoco and and bi-dexhands environments, outperforming existing state-of-the-art sequential algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written and easy to understand; Fig. 1 is very informative.
- The problem of exploration under agent heterogeneity is an important problem in multi-agent learning
- The proposed method is sound and is backed by theory

Weaknesses:
- The evaluation is hard to judge whether the proposed method is actually performs better than the baselines, this is a deal breaker. I suggest the authors also incorporate aggregate quantities from https://agarwl.github.io/rliable/

I'm willing to increase the score if the authors show that the improvement is statistically significant

Limitations:
Minor comments
- line 36, wrong citation format
- line 193 and line 217, Ep. 5 should be Eq. 4

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper is situated in the problem setting of heterogeneous cooperative agents, under the sequential update framework. The paper introduces the novel MADPO algorithm, in which agents maximize the Cauchy Schwarz divergence between agents and between episodes of data gathered by the same agent, to improve exploration. Empirical validation is performed on the Multi-Agent Mujoco and Bi-DexHands benchmark suites, demonstrating that the MADPO outperforms baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the paper is clear, succinct, and the main idea is clear and easy to understand. The format, and figures are good, with all expected components included. The idea of maximizing the inter/intra agent divergences is intuitively appealing. Further, the authors address the pitfalls of naively maximizing intra-agent divergences by adopting the Cauchy Schwarz divergence. It's especially nice that maximizing the CS divergence implies maximizing the policy entropy as well. Experiments are done on a large number of tasks, with comparisons against expected baselines and parameter sensitivity analyses all present.

Weaknesses:
1. The motivation of the paper is not altogether clear to me. The paper seems to suggest that exploration is more challenging in the sequential update setting, necessitating devoted algorithms. Why would this be the case? 
2. In many of the presented domains, the improvement of MADPO over the next best method is not very large. Sometimes, confidence intervals of MADPO overlap those of the next best method. Can the authors provide statistical significance tests for the main results in Figures 2 and 3, comparing MADPO to the next best method?
3. Some minor suggestions:
- Please check your paper carefully for typos, as there are quite a few: 
    - Line 89: ""connecting link dimension curse""? Not sure what this is
    - No period after Figure 4
    - Trust interval -> confidence interval 
    - Lacking 'and' at line 174
    - Line 204: conditoned -> conditioned
    - Line 216: extra ""of"" 
- Please be sure to state the number of trials in the main text. It is mentioned in the Neurips checklist, but I could not find it  in  the main text
- Please make the colors of the methods the same for both domains (i.e. pick 1 color for MADPO and be consistent with it)

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel multi-agent reinforcement learning (MARL) method called Multi-Agent Divergence Policy Optimization (MADPO), which enhances exploration and heterogeneity through a mutual policy divergence maximization framework. MADPO leverages a sequential updating scheme and quantifies discrepancies between episodes and agents, termed intra-agent divergence and inter-agent divergence, respectively. To address the instability and lack of directionality in traditional divergence measurements, the paper proposes using conditional Cauchy-Schwarz divergence to provide entropy-guided exploration incentives. Experiments demonstrate that the proposed method outperforms state-of-the-art sequential updating approaches in two challenging multi-agent tasks with various heterogeneous scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. **Innovation**: The paper introduces MADPO, a novel MARL method that enhances agent exploration and heterogeneity through mutual policy divergence maximization. 
   
2. **Theoretical Foundation**: The use of conditional Cauchy-Schwarz divergence to address instability and directionality in traditional divergence measurements is a contribution.
   
3. **Experimental Validation**: The experiments conducted on two challenging multi-agent tasks with different heterogeneous scenarios convincingly demonstrate the effectiveness and superiority of MADPO in enhancing exploration and heterogeneity.

Weaknesses:
1.  The paper lacks analysis and comparison with relevant literature on sequential decision-making, such as:
   - Liu J, Zhong Y, Hu S, et al. Maximum Entropy Heterogeneous-Agent Reinforcement Learning[C]//The Twelfth International Conference on Learning Representations.  (This paper extends SAC to heterogeneous sequential decision-making scenarios, and the relationship between this work and the current paper remains unclear.)

2. It is unclear whether the intrinsic reward method proposed in this paper can ensure that the resulting trained policies are consistent with the original policies.

Limitations:
The authors have addressed the limitations of their work and discussed potential negative societal impacts in accordance with the guidelines.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xvVeSZoVJO;"REVIEW 
Summary:
In this paper, the authors proposed an essential problems: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? The authors presented a robust camera-insensitivity collaborative perception with a novel dynamic feature-based 3d neural modeling mechanism to address the issue. Moreover, to verify the effectiveness of the model, the authors also provided a new large-scale dataset, OPV2V-N for this field. The experiments result showcase the model’s robustness in proposed dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Strength:
1.	The paper presents an interesting viewpoint that is to recover noisy camera perceptual information from other agents’ views by modeling the collaborative neural rendering field representation, in which the model is divided into two stages: a time-invariant static background and time-varying dynamic foreground.s
2.	The paper develops a new dataset to fill the gap of the lack of a comprehensive collaborative perception dataset that accounts for different camera noise scenarios.
3.	The paper is well-organized and interesting to read.

Weaknesses:
1.	From my perspective, the paper lacks the theory analysis for the proposed method. Moreover, the authors fail to introduce the motivation of each sub-module in the presented model. For example, can the authors showcase the motivation of using Nerf for the static and dynamic fields, are there any dominant advantages of nerf, compared to other 3d reconstruction methods in this method?
2.	It is necessary to give more rigorous mathematic analysis of equations in this paper. Furthermore, the authors are required to introduce the details of each networks, including the training parameters, learning rate, weight values in eq. 12.

Limitations:
The current work focuses on addressing the camera-insensitivity problem in collaborative perception. It is evident that accurate reconstruction can compensate for the negative impact of noisy camera features on collaborative perception.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? Therefore, RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism is introduced. To validate the new method, the authors also provide a new dataset: OPV2V-N. RCDN serves as baseline here. Ablation Study shows for 5 models (F-cooper, Att-Fuse, Disco-Net, V2VNet, CoBEVT a significant improvement over their baselines, w/o RCDN.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper builds up on three pillars: single perception, collaborative perception and neural rendering. The base idea is novel to the best of my knowledge. The problem formulation is clear and well sounded, easy to follow. The System architecture is strong. The authors also focus on the differentiation between static and dynamic scenarios, especially for the neural fields both based on the BEV volume feature space. This differention is very important, not very often in detail discussed. The ablation study especially table 5.1 shows very accurate an increase of performance for different tasks static (lanes, free space) and dynamic perception. The experimentsl part introduces a new dataset, which is necessray for the investigation.

Weaknesses:
The overall system architecture sounds good. However, there are some open points for me, the impact of section 4.3 and 4.4, i.e. the neural fields part, seems open in terms of clarification. Example: What is difference between sf w , sbw in equation (7)?
The experimental section is a bit too short. I feel its not finished yet. However, there is limited space. The overall approach is not usable for realtime.

Limitations:
The most relevant limitation is the missibg real-time applicability.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents RCDN, a method to aggregate multi-sensor perception signals in dynamic environment.
The key idea, is to improve the aggregated multi-agent feature with the multi-view rendering loss.
At its core, RCDN gathers input streams at varying timesteps of multiple agents. The gathered images are fused into Birds Eye view (BEV) then further decoded into volume. 
The volumetric features are learned into static scene and dynamic scene components with NGP based representation.
Overall procedure is supervised with rendering loss, (cyclic) optical flow consistency.


The method is evaluated on new dataset, OPV2V-N, which is an updated version of OPV2V, with additional masking and optical flow. 
The results show that RCDN helps BEV segmentation with various backbones, compared to the model used without RCDN.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
1: poor

Strengths:
The main benefit of the RCDN, is that it is fairly easy to apply into different existing feature backbones, as it is the post-processing step built on top of BEV features. 
Experimentally, the usage of RCDN significantly improves the segmentations which implies that the features are better aligned throughout the noisy signals. 
This makes the work to be a great off-line data augmentation / preparation pipeline for generating BEV segmentation features. 
The paper additionally proposes OPV2V-N dataset, which may be somewhat valuable addition to the community.

Aside from technical perspective, the paper is easy to follow and well-written.

Weaknesses:
The paper's main weaknesses are two folds. 
1. The paper does not evaluate on tasks other than BEV segmentation. 
While I believe that the pixel-aligned features from NGP would give benefits over various vision tasks, the paper only demonstrates on smaller domain of work which undermines its actual potential. It would have been more interesting to compare how it impacts in different downstreaming tasks, such as detection / tracking.

2. Technical contribution seems to lack novelty. 
The paper is a mix of two known-to-work solutions; BEV feature decoding for segmentation (used with various baselines in the experiments), and NGP (or radiance field based) multi-view pixel / density alignment through rendering loss. Usage of rendering loss to improve segmentation map is well-investigated in different literatures in the NeRF community (e.g, semantic-nerf).

Limitations:
No concerning limitations are found.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposed Bird Eye View (BEV) semantic segmentation pipeline from collaborative perception, robust to motion blur, sensor noise, occlusion and even failure. The proposed a pipeline that adapts neural rendering techniques to overcome the noise/malfunction in camera capture and occlusion. With the proposed method combined with prior methods, performances on OPV2V-N (the proposed BEV semantic segmentation dataset) are improved.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper proposed to apply neural rendering concept for ‘robust’ collaborative-perception BEV segmentation. It is natural way of thinking to overcome noise/malfunction in the caption system but the way the paper adapts neural rendering to BEV segmentation is novel. And, the performance is verified with OPV2V-N dataset.

Weaknesses:
Evaluation is only performed with OPV2V-N dataset which may result in overfitting. More evaluation with different dataset is required. The author may need to compare methodologies on other dataset although the existing dataset do not have noise. The author also may add random noise to the prior dataset and run experiments.

The manuscript was uneasy to read and understand. The paper should re-written. The comments below are without understanding supplemental materials fully.
- The way proposed algorithm is combined with prior method is unclear. The reviewer guessed that the MCP module can be replaced with prior methods, but it is not stated.
- Many abbreviations are not explained sufficiently and terminologies the author defined are ambiguous and may be incorrect. 
- MCP is short for the multi-agents collaborative perception process but the paper did not explain MCP module in details with no reference
- BEV, no full name, no reference.
- “Camera-insensitivity” can be understood terminologies related to camera sensor sensitivity (how much the camera sensor accept photon…).
- Robust Camera-Insensitivity: Robust == Camera-sensitivity? The latter one may be redundant
- Line 6. introduce a new robust camera-insensitivity problem: cam be replaced “introduce BEV segmentation when the camera capture are unreliable (or noisy)?” Should be more concrete without ambiguous words
- Line19 “Ported to” mean?
- There are more unclear sentences.

Limitations:
.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces RCDN, a novel method for robust camera-insensitivity collaborative perception. This method aims to overcome challenges associated with noisy, obscured, or failed camera perspectives by using dynamic feature-based 3D neural modeling. RCDN constructs collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. The proposed system consists of two collaborative field phases: a time-invariant static background field and a time-varying dynamic field. To validate RCDN, a new dataset called OPV2V-N was created. The paper demonstrates that RCDN improves the robustness of baseline methods in extreme camera-insensitivity settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
*Innovative Problem Addressing*: The paper tackles a significant real-world problem of camera insensitivity in multi-agent collaborative perception, which is crucial for autonomous systems.

*Novel Methodology*: The introduction of dynamic feature-based 3D neural modeling and the construction of collaborative neural rendering field representations are innovative approaches.

*Comprehensive Dataset*: The creation of the OPV2V-N dataset, which includes various camera failure scenarios, provides a robust platform for testing and validating the proposed method.

*Performance Improvement*: The extensive experiments and quantitative evaluations show significant improvements in robustness and performance over baseline methods.

*Detailed Evaluation*: The paper includes both quantitative and qualitative evaluations, along with ablation studies, which thoroughly demonstrate the effectiveness of RCDN.

Weaknesses:
*Complexity and Computation*: The proposed method involves complex modeling and multiple steps. The author should provide the latency.

Generalizability: The performance of RCDN is primarily validated on the OPV2V-N dataset, which may limit the generalizability of the results to other datasets or real-world scenarios.


*Failure Cases*: It would be nice if the authors provide failure cases, which is important.

Limitations:
Please see weakness and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xoc4QOvbDs;"REVIEW 
Summary:
This paper studies multi-view clustering and seeks to investigate the view cooperation issue. The authors consider DMVC as an unsupervised cooperative game and regard each view as a participant. Compared with the existing methods, this consideration is new and interesting. Based on the novel idea, the authors proposed SCE-MVC, a novel shapley-based cooperation enhancing multi-view clustering method. The paper is well-organized. The experiments are convincing.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper proposes a new point also an interesting point for multi-view clustering tasks, i.e., considering the multi-view collaboration as a cooperative game. 

2. The experiments are sufficient and convincing. The authors validate the method from many aspects. The proposed SCE-MVC obtains much better performance on six diverse datasets.

Weaknesses:
1. Figure 2 is confusing. The specific structure of View Cooperation Enhancing Module is not clearly presented.

2. There are many formulas and symbols. It is suggested to add a notation table.

3. Although the authors try to explain model (1), it is still difficult to understand Shapley Value from the model. In addition, many variables are not clearly explained. The authors should present more information about the model and explain all variables used in this model, such as S_i, {i}, s\{i}， etc.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author introduces a Shapley-based cooperation enhancement framework aimed at fostering collaboration among different views. The SCE-MVC method incorporates cooperative game theory, considering each view as a participant in the model and assessing their contributions using the Shapley Value.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Viewing each view as an individual player within game theory represents a fresh perspective in multi-view clustering. Also, enhancing clustering performance through balancing view contribution is both well-founded and innovative.

Weaknesses:
1. Using the SCE module in an alignment-based framework only provides a marginal improvement to the model. Does this imply that the SCE module is ineffective in the alignment-based framework? 

2. The view contributions of alignment-based method is much balanced than view contributions of joint methods. Does this imply that the alignment-based method is much better than the joint method? It's not reasonable since the clustering performance of alignment-based methods may not necessarily be better than that of joint methods.

3. Is the complexity of computing Shapley values truly O(n!)? When dealing with a larger number of views, can this evaluation framework still be utilized for computation?

4. Are the loss functions L in Eqs (15) and (16) on page 6 the same? If so, there is a problem of inconsistent dependent variables. In addition, $D_ij$ in Eq. (9) is a scalar and should not be bolded.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The study centers on improving task performance via deep multi-view clustering (DMVC) and fostering cooperation among different views. Specifically, the study evaluates view contributions, emphasizing the significance of strengthening cooperation among views.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Considering multi-view tasks from a collaborative standpoint represents a novel approach, with the paper's motivation being notably fresh. Moreover, the paper elucidates potential contribution imbalances in the joint method and addresses them through the SCE method, thereby enhancing cooperation among views.

Weaknesses:
When dealing with datasets comprising more than two views, such as three views, how can one assess whether the contribution of the views has become more evenly distributed after employing SCE? While the paper visually presents the contributions of the views, could a quantitative method be provided for this evaluation?

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This research merges game theory with multi-view clustering by introducing the Shapley-based Cooperation Enhancing (SCE) approach. It features a module to systematically evaluate each view's contribution. The approach promotes view cooperation by adjusting the training convergence rate of view parameters based on their contributions. Extensive experiments on various datasets demonstrate the method's effectiveness when applied to different MVC frameworks.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1) The paper integrates the Shapley value from game theory into DMVC, allowing for precise assessment of each view's contribution.
2) Theoretical analysis is thorough, with clear and intuitive figures.
3) The manuscript is well-organized and clearly written.

Weaknesses:
The article categorizes DMVC into alignment-based and joint methods. What criteria were used for this classification? Furthermore, only one DMJC method is used as a representative for joint methods.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper firstly considered DMVC as an unsupervised cooperative game where each view can be regarded as a participant. Then, the authors introduced the shapley value and propose a novel MVC framework termed Shapley-based Cooperation Enhancing Multi-view Clustering (SCE-MVC), which evaluates view cooperation with game theory. In summary, this paper was well written with obvious superiority.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
-- A MVC framework was designed that utilizeD game theory and Shapley values to evaluate and elevate inter-view cooperation. 
-- The experiments were sufficient, and the analysis of the experimental results was adequate.

Weaknesses:
-- In this paper, why utilize $\phi_i$ to measure the contribution of views instead of the view weight $w_i$? The article's explanation on this is not clear enough, and there is a lack of experiments to demonstrate the relationship between $\phi_i$ and $w_i$.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
vt2qkE1Oax;"REVIEW 
Summary:
The authors propose a loss function that seeks to group the trajectories into low-rank matrices where the motion of object points can be approximately explained as a linear combination of other point tracks. Experiments on the synthetic MOVi-F variant of
the Kubric dataset  and the real datasets DAVIS 2016, SegTrackv2 and FBMS show that the proposed method outperforms single-sequence methods, single-stage end-to-end methods and multi-stage methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1) The authors address key issues in the field and the contribution is original even if somewhat incremental.
2) The proposed method is detailed and reproducible.
3) Experiments are relatively well conducted on synthetic and real datasets showing the superiority of the proposed method.

Weaknesses:
About the presentation, please clearly state a name/acronym to the proposed method and replace ""ours"" by it in the comparison tables.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a method for training a segmentation network using long-term point trajectories as a supervisory signal to enhance optical flow. It proposes a novel loss function aimed at grouping these trajectories into low-rank matrices, allowing the motion of object points to be approximately represented as a linear combination of other point tracks. The proposed approach surpasses previous methods in motion-based segmentation, demonstrating the value of long-term motion and the effectiveness of the new formulation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The introduction describes the problem in more detail when introducing the issue.
2. The structure of the article is good.
3. The experimental results of the method proposed in this paper show a significant improvement.

Weaknesses:
1. The main contribution of this paper is the proposal of two losses, but the loss seems to be effective in the experiments of other segmentation methods.
2. The contribution of the paper in Subspace Clustering is not described clearly.
3. The resolution of Fig 3 is relatively low.
4. There is a lack of comparison in terms of inference speed.

Limitations:
The authors have acknowledged some limitations of their work. I suggest the authors could further describe the limitations in the generalizability of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel loss function that allows training image object segmentation
models based on object motion in videos. Motivated by recent work on self-supervised
learning of segmentation using optical flow, the authors propose to use longer point
trajectories as additional self-supervision signal. Related to subspace clustering, the
proposed loss function encourages to predict segments whose trajectories can be well
explained by few basis trajectories. The predicted segments are merged into a binary
segmentation mask and evaluated on standard, real-world segmentation benchmarks.
Previous methods based only on optical flow are consistently outperformed, demonstrating
the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Unsupervised learning of segmentation is an important problem. Several recent methods
  approached this task using optical flow as a self-supervision signal, extending this
  line of research to trajectories is a well-motivated idea.
- The mathematical motivation of the loss is very well explained. Without having a deep
  mathematical background, I could follow the derivation of the loss function without
  issues.
- Standard benchmarks and modelling components are used for evaluation, which makes it
  easy to compare the proposed method to previous approaches.

Weaknesses:
1. It is not described clearly enough what kind of segmentation task is targeted. From
   the introduction and method section it seems to me that multi-object segmentation is
   adressed, only at the very end of the method section it is mentioned that the
   predicted segments are merged into a binary segmentation in some cases.
   - To my understanding the task is multi-object segmentation for MOVi and binary
     segmentation for all other datasets. This should be clearly stated in the
     experiment section.
   - It should be stated in the introduction and method section more clearly that the
     main task is binary segmentation.

2. The proposed method is not compared to models that do not use optical flow for
   self-supervision. It would be interesting to see how the proposed method compares to
   other self-supervised segmentation approaches. For example
   - CutLER ([Wang et al. 2023](https://arxiv.org/abs/2301.11320)) and VideoCutLER ([Wang et al. 2023](https://arxiv.org/abs/2308.14710))
   - DINOSAUR ([Seitzer et al. 2023](https://www.amazon.science/publications/bridging-the-gap-to-real-world-object-centric-learning)) and VideoSAUR ([Zadaianchuk et al. 2023](https://proceedings.neurips.cc/paper_files/paper/2023/hash/c1fdec0d7ea1affa15bd09dd0fd3af05-Abstract-Conference.html))
   
    The masks predicted by these models could be merged to obtain a binary segmentation
   in the same way as for the proposed method.

Limitations:
The authors address limitations of their work in a dedicated paragraph. Their
discussion is brief but adequate in my view.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a model to process long-term motion and short-term motion simultaneously to achieve motion-based segmentation. Specifically, motivated by subspace clustering, this work proposes a loss function that enables training a neural network to learn motion grouping from both optical flows and point trajectories. It outperforms the previous method in the unsupervised video segmentation task. The qualitative comparison also shows obvious improvement, giving a clearer boundary.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation and method explanation seems to be clear. The paper writing is easy to follow.
2. Using a simple sample to introduce the low-rank intuition is convincing and reasonable. Based on this core idea, other smoothing losses and regular loss from optical flow make learning more effective.
3. Experiments show the strength of the proposed strategy. A comprehensive ablation study has been performed to illustrate the impact of each factor.

Weaknesses:
1. As mentioned in the limitation, the paper's principle assumes that the object is rigid. However, the task that this paper works on not only includes rigid objects -- it's a general video segmentation task. Then it seems that the low-rank theory can not extend to a general setting. And why not consider local rigid like ARAP loss? (SpatialTracker)
2. Do not give some corner cases or failure cases, especially for non-rigid objects. I hope to see some corner cases like multiple objects, where they behave similarly in the short term but different in the long term. Then it can better demonstrate the motivation of the paper.

Limitations:
As mentioned in the weakness, the principle the paper proposed is reasonable, but seems like it does not fully support the motivation and the ultimate goal of the task. More analysis and experiments are needed to show the right practice when applying the proposed to real-world videos.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tackles video object segmentation by incorporating into the loss function not only instantaneous optical flow information but also long term pixel tracking information.  Raft was used for optical flow and CoTracker was used for long term pixel tracking in the experiments.  The experiments show a marginal improvement in performance when combining the two information sources in the loss function.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper flows quite well, it addresses that video object segmentation is the problem space, the focus is on loss function, Figure 2, the layout appears clear as well.  There are a handful of datasets and comparing methods used in the experiments.

Weaknesses:
Table 2 where the experimental results are presented lists a collection of methods categorized into different groupings.  Perhaps these groupings and methods could be better discussed in the lit review, it appears that the categories in the lit review do not correlate nicely and I do not know the difference of these methods unless I look at the references and read the papers myself.
The improvement is incremental.  IT is expected that there would be some improvement however what cases do we actually get the improvement in, a bit of more depth in the analysis would make this a better paper.
I assume that the camera is static?, correct? if not, perhaps making this clearer would help.
I have no idea how long the long term point trajectories were, perhaps analyzing this would help.  Also depending on the trajectories, were there occlusions or other interesting factors that contribute to the loss function would be interesting.

Limitations:
I am not sure that the examples actually illustrate that motion segmentation is necessary for these cases.  I would focus on cases where appearance information is not enough.
Can this system deal with a moving camera or does the camera have to be static?
How well does the system work under occlusion?
Different motions of the object of interest will results in different performance, perhaps diving into this analysis would be informative.
Both sources of info, optical flow and long term pixel tracking info are based on 2D info, the projection of 3D info.  This has limitations.  The paper should have explored different object movements.  It does state that non rigid objects when dealing with multiple objects is an issue however an in depth exploration for a single non rigid object would be informative.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xvTMc9Ovx3;"REVIEW 
Summary:
1.This paper this contributes a new large-scale dataset named Traffic Object Importance (TOI) to addresses the problem of on-road object importance estimation, which utilizes video sequences captured from the driver’s perspective as the input.
2.The author also proposes a model that integrates multi-fold top-down guidance with the bottom-up feature.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1.This paper describes in great detail the specialized methodology and the structure of the models.

2.The scarcity of large-scale publicly available datasets hinder the development of on-road object importance estimation.

3. This paper considers the effect of traffic rule on object importance and successfully models this abstract concept by proposing an adaptive object-lane interaction mechanism.

Weaknesses:
1.In page 3 the author mentions that the traffic rule is crucial for object importance and focus on the traffic line rules , but the influence of traffic rules is varied, such as signalization. Therefore, in page 4 of table 1, the author is able to provide statistics on the scenario categories of TOI dataset and the traffic rule constraints within the dataset in experiment. 

2.In page 6, the author uses three common intention behaviors in driving to reflect the driver intention (i.e., turning left, going straight, and turning right). Since the video clip length is set at 16 frames, it is important to clarify if each of the three intentions corresponds to individual frames with the 16-frame clip cut during the training and testing phases, or if multiple intentions are present within the 16 frames. The authors should further elaborate and provide the proportion of each intention in the dataset.

3.Insufficient evaluation of indicators in the experimental section. The author may add another evaluation indicator.

4.The section three can include a schematic diagram of the annotation process for the dataset.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper collects a new large-scale dataset and proposes a novel method that integrates multi-fold top-down guidance with the bottom feature to address the problem of on-road object importance estimation. Specifically, the dataset is almost three times larger than the current publicly dataset for on-road object importance. In addition, this paper considers an adaptive mechanism for object-lane interaction, effectively modeling the impact of traffic rules on object importance. Experiments on several benchmarks validate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper makes several key contributions and demonstrates strengths for on-road object importance estimation

(1) This paper introduces a novel, extensive dataset, set to be released to the public, which is nearly three times the size of the current largest public dataset. 

(2) The method is well-motivated and straightforward. It estimates the importance of objects on the road, integrating various top-down guidance factors with bottom-up features, marking the first of its kind.

(3) The proposed method addresses the pivotal role of traffic rules in estimating object importance, an aspect previously overlooked by existing methods. It successfully encapsulates this concept through an innovative, adaptive mechanism for object-lane interaction.

Weaknesses:
This paper has also two weaknesses: 

(1) The paper does not provide a detailed discussion on the computational efficiency of the proposed method, which is crucial for real driving scenarios. Moreover, it is recommended to compare the model parameters and latency with other methods.

(2) Another concern lies in the practicality of the method. This method and the proposed dataset are both for single-camera scenarios, but in real autonomous driving scenarios, surrounding view is a more widely used type and a safer option. Will the proposed method also work well in the surrounding view?

Limitations:
The proposed method only considers the effect of three types of driver intentions on object importance estimation, which is not sufficient for complex driving scenarios. I carefully checked the paper and found no potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel dataset for on-road object importance estimation. More data about which objects are important for self-driving is included and is promised to be released. Moreover, a novel method that integrates driven intention, semantic context, and traffic rule is devised to tackle the related problem. The paper is well-written.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
A new dataset is introduced with rich data and labels. The presented method is novel and shown to be effective for the studied problem. Details about the dataset and the method are comprehensive and technically sound. Results are also promising.

Weaknesses:
Some of the concepts lack sufficient details to explain. See questions below.

Limitations:
The authors have mentioned limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work addresses the issue of estimating the importance of on-road objects using video sequences from a driver’s perspective, a critical task for enhancing driving safety. The authors introduce the Traffic Object Importance (TOI) dataset, which is significantly larger and more diverse than existing datasets, and propose a novel model that integrates multi-fold top-down guidance factors—driver intention, semantic context, and traffic rules—with bottom-up features for more accurate importance estimation. Experimental results demonstrate that the proposed model significantly outperforms state-of-the-art methods in on-road object importance estimation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The introduction of the Traffic Object Importance (TOI) dataset, which is significantly larger and more diverse than existing datasets, provides a robust foundation for training and evaluating models in on-road object importance estimation, thereby addressing a major limitation in the field.

2.  The proposed model effectively integrates multi-fold top-down guidance factors—driver intention, semantic context, and traffic rules—with bottom-up features, which showed good performance for the TOI task.

Weaknesses:
1. Lack of description of the annotation details. 
How many annotators are involved in the annotation procedure? It would be good if the authors can provide some annotation procedure samples regarding the double-checking annotation mechanism and the triple-discussing annotation mechanism.

2. It seems this annotation will be varied according to different traffic rules. Since KITTI is collected in Germany, the annotators should be familiar to germany traffic rules. However the authors did not mention this information in their submission, thereby the label quality is doubtful.

3. The authors are encouraged to build up the first benchmark based on the proposed dataset by using various existing object detection methods, e.g., Yolo, with the proposed head or simpler head. It is interesting to see how the existing object detectors work on this new task.

4. More statistics of the dataset are encouraged to be given, e.g., the number of important object of different categories, etc.

Limitations:
yes the authors mentioned it in the appendix

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xutrKezbPF;"REVIEW 
Summary:
Some existing methods alleviate the capacity gap between the teacher and student by setting up Teacher Assistants (TAs), introducing a large number of additional parameters and computational costs. Based on this, this paper proposes to train multiple RDM modules and connect multiple independent classification heads to generate branches with different performance to simulate the TA model. The authors think that this hierarchical model of extracting teacher knowledge can help alleviate the capacity gap between the teacher and student.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* Assistants and teachers sharing shallow modules are more efficient in terms of parameter quantity compared to multiple independent Assistants models.
* Although a large number of fully connected layers have been introduced, the proposed method hardly introduces any additional training overhead.

Weaknesses:
* I noticed that there is a significant difference in baseline performance between Table 1 and the original text, and Table 3 only uses a single Assistant for TAKD and DGKD, while the author's proposed method uses three RDM headers, which is not a fair comparison.
* There are significant differences in the value of R across different datasets, R=1 for CIFAR but R=10^4 for ImageNet. This means that the selection of hyperparameters on unfamiliar datasets is challenging, and the parameter tuning process may introduce multiple computational costs, which limits the versatility of the method.
* I noticed that the method proposed by the author introduces and trains at least $3N$ additional layers of MLP, and the forward process and loss calculation cost in distillation is also increased several times. However, the training cost is even the same as the method NormKD based solely on Logits Distillation without additional modules (Figure 1 (b)). Can you present the results with specific numerical values? What is the key to introducing so many parameters without introducing additional computational overhead?

Limitations:
The authors have discussed a limitation in Conclusion.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Inspired by Shannon’s rate-distortion theory, this paper proposes two modules, namely the Rate-Distortion Module and the Information Bottleneck Module, to construct intermediate representations for knowledge distillation. Extensive experiments on various datasets demonstrate the effectiveness of this method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-presented and easy to understand.
2. This method not only works for traditional CNN networks but also performs well on modern CLIP models.
3. Extensive experiments demonstrate the effectiveness of this method.

Weaknesses:
1. The author's motivation and explanation for TA distillation are not very convincing. In my view, the RDM and IBM proposed in this work can be interpreted as two adapters connected to the teacher and the student respectively for distillation, and the principle is similar to the FT method.
2. In Table 2, 9 and 10, most of the compared methods were published in 2022 or before. The authors are encouraged to compare your method with recent state-of-the-art methods such as MLKD[1], CTKD[2], and LSKD[3].
3. In Eq. 5, I am a little confused about the author's formula representation. Generally speaking, the left side of the comma is the network to be trained, and the right side is the learning target. But the author seems to have it reversed here.

References:  
[1]. Multi-Level Logit Distillation. CVPR 23.  
[2]. Curriculum Temperature for Knowledge Distillation. AAAI 23.  
[3]. Logit Standardization in Knowledge Distillation. CVPR 24.

Limitations:
Please refer to weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new distillation method, CIFD, designed based on *Shannon’s Rate-Distortion theory* and  **Information Bottleneck Principle (IBP)*. CIFD contains Rate-Distortion Modules (RDM) for the teacher to substitute heavy Teacher Assistant (TA) and Information BottelNeck Module (IBM) for the student to mimic the features from several RDMs. Experiments demonstrate the effectiveness of the method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper is organized well.
2. The experiments on CLIPs are good, verifying the broader effectiveness of the method.

Weaknesses:
My main concerns are from three aspects: **i) the story of the paper; ii) the reason why CIFD works; iii) insufficient experiments and comparisons.** Some concerns are mixed among the three aspects. And I will list them one by one.

1. **Insufficient experiments on verifying the basic settings of the paper.**
The story starts with *""When the teacher model is significantly larger than the student, previous works that utilize TAs induce high training costs.""* I would believe the basic settings of this work as the teacher-student network pairs are in large parameter scale differences. From this point of view, the paper should contain more systematic experiments to verify the efficacy under this setting. Specifically, CIFD should be compared with previous methods on ImageNet with teacher-student network pairs with large different parameter scales, not just traditional ResNet-34 -> ResNet-18 and ResNet-50 -> MobileNet-V1.

2. **The trade-off between the story and the empirical solutions.** In my opinion, the paper is a little bit overdecorated and overclaimed. The author proposes many concepts, such as *Shannon’s Rate-Distortion theory* and  *Information Bottleneck Principle (IBP)*, and claims **""This is the first application of Shannon’s Rate-Distortion theory to aid knowledge distillation""**. I don't mean that the aforementioned statement is misleading.  But, if we go deeper into the design, the reason why the method works may come from the **noise-adding and noise-removing process**. Many previous works have verified that the above process could benefit the learning process in computer vision, like MIM and diffusion models, which have been empirical solutions.   In KD, there also exist distillation methods following MIM and diffusion models, like MGD and DiffKD. From this point of view, the authors should not claim ***""the first""*** only, but make a deeper analysis of related methods and make detailed comparisons. ***I strongly encourage the authors to make a good balance between the story and the verified empirical solutions.*** Even though it seems not as novel as this version, it would provide the readers with more useful knowledge and insights.

3. **The design may alter the network architecture of the student.** It seems that the IBM module would also be included in the validation stage. If my judgment is true, the added module (though lightweight) would also benefit the performance. Under such circumstances, the comparisons with previous methods, especially for light-weight models, are unfair.

Limitations:
Limitations are included in the main paper and the Appendix

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xtK3gZjQDC;"REVIEW 
Summary:
The paper analyzes decision support systems based on prediction set algorithms. The authors show that: (i) the usage of conformal prediction techniques is generally sub-optimal in terms of accuracy; (ii) the problem of finding the optimal prediction sets under human assistance is NP-hard. Moreover, they provide (iii) a greedy algorithm that is guaranteed to find prediction sets that are better than those provided by conformal predictors. Experimental evaluation on synthetic and real data show the effectiveness of the considered approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main strengths of the paper are:

1. the actual paper contribution is well framed;
2. the theoretical analysis is sound;
3. the proposed algorithm improves over existing approaches.

Weaknesses:
I think this work is a good paper, without major weaknesses, as it provides solid theoretical insights.
The concerns I have are mainly due to typos/details missing. I will point out here these and a few remarks that might be considered for the final version of the paper.

1. It seems to me that Table 2 and Figure 3 are missing the BRUTE FORCE baseline.
2. regarding the style of the paper, I found lines 135-146 very dense. Maybe providing a more concrete example (e.g., what could 1,2,3 represent?) might help the reader getting through it.
3. In Algorihm 1, I think adding a comment to the pseudo-code (from lines 4 to 13) could be useful
4. regarding the limitation section (evaluation) a useful reference might be [Stutz et al., 2023], where the authors evaluate the possibility that human experts might not be approximating the true probability distribution 
5. the experimental analysis (on real data) could be enriched with other popular Learning-to-Defer datasets, such as Cifar10H or hatespeech.

[Stutz et al., 2023] - Stutz, David, Abhijit Guha Roy, Tatiana Matejovicova, Patricia Strachan, Ali Taylan Cemgil, and Arnaud Doucet. ""Conformal prediction under ambiguous ground truth."" Transactions on Machine Learning Research (2023).

Limitations:
The paper adequately discussed the limitations and societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors first show that conformal prediction sets may not lead to human decision optimality.  The authors then introduce a greedy algorithm to generate candidate prediction sets that improve human decisions regarding the accuracy metric.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors find the sub-optimality of conformal prediction sets on providing candidates for human decisions. Thereby, they propose a novel method to produce prediction sets that helps to improve human prediction.

Weaknesses:
* The presentation somewhere is unclear: 
  * Line 86. Please break the sentence properly.	
  * Line 40/43/48: It is unclear for readers when the authors mention “optimal” multiple times but delay its explicit definition later.
  * Line 197: It is confusing when the authors refer to the role of $a$. What is the value of $a$?

* The authors claim they propose an efficient algorithm. However, I am not sure which part is efficient. Are there any numerical metrics, e.g., running time, supporting this contribution? Additionally, how should we understand this restriction of “for a large class of non-conformity scores and expert models” in line 51?

* Line 90:  But you also miss the possibility outside the prediction set, especially when the prediction set is not that good. I think the authors need to discuss the exploitation-exploration dilemma.

* The authors use the scores related to softmax and APS. Other papers propose alternative scores like RAPS and SAPS. I think they should be included.

Limitations:
Please see the above sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper shows the conformal prediction set may not be the optimal set recommendation to humans if humans follow certain choice models. The authors then propose a greedy algorithm by modeling $P(y|x)$ and the choice model of humans assuming it follows MNL model. Authors compare the proposed method against the standard conformal prediction set under synthetic human experts and the proposed method has a slightly better performance compared to traditional conformal sets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors consider conformal prediction in the human-in-the-loop setting, which is an important problem. The first part of the paper shows the conformal prediction set may not be the best recommendation set for humans, which is easy to understand since most conformal sets arrange the set in a ranked order and we can play with the human choice models to create an example that conformal sets may not be the best recommendation set.

Weaknesses:
The problem setting is not realistic: The authors do not allow humans to select outside the conformal prediction set. However, in the setups of most empirical successes of human-AI collaboration with conformal prediction, this is allowed. Similarly, if the authors do not allow humans to select outside the conformal prediction set, humans' value is greatly reduced and the optimal thing to do may be just to use fully automated AI prediction and in all the toy examples the authors provided, kicking humans out of the loop is the optimal system (humans only make things worse). 

The theoretical analysis seems useless: I think the theoretical analysis is useless for two reasons: 1) while identifying the optimal set is NP-hard, in practice the metric we care about is $\mathbb{E} g(S|x)$, not identifying the optimal set. If an algorithm can get a good rate of convergence for this regret, then this problem is not hopeless, so I think authors need to show for all conformal prediction algorithms, what is the regret lower bound for $\mathbb{E} g(S|x)$; 2) while I can see that sometimes the label set can be large. In practice, the theoretical results may not be a big issue for many problems since most problems have small label set (binary or three classes). This negative results may not seem that severe as the authors presented in the paper. 

The solution is disconnected and not useful in human-AI collaboration: 1) The proposed solution does not enjoy the distributionally-free guarantee, which is the main reason why people use conformal prediction. I would expect authors to provide a conformal prediction algorithm that is human-centered, rather than directly switch lanes to traditional prediction methods. 2) The proposed solution requires $P(y|x)$ and the true human choice model, which is too strong to be realistic. If I know $P(y|x)$, why should I involve humans in the loop anymore (recall that authors can restrict humans only select from prediction set so humans are not necessary in the system). The optimal strategy would be directly use $P(y|x)$ to select actions. 

Baselines: For human-AI collaboration tasks, I expect to see the proposed solution is better than human working alone or AI working alone. The authors should compare with AI only baseline using $P(y|x)$. Based on the toy example and my current understanding of the paper, the proposed solution cannot beat AI only baseline.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to construct optimal prediction sets under which experts can achieve the highest accuracy. The authors claim that human experts cannot attain maximum accuracy with the prediction sets generated by conformal predictors. To address this issue, the paper proposes an efficient greedy algorithm based on maximum marginal gain to find prediction sets that outperform those generated by conformal predictors. The paper offers two main theoretical contributions: the first proves that finding the optimal prediction set is an NP-hard problem, while the second demonstrates that the proposed method enables experts to achieve higher accuracy than conformal predictors. Empirical results further validate the effectiveness of the proposed approach.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-motivated and easy to follow.
    
2. The authors provide a theoretical analysis for their motivation and offer a theoretical guarantee for the superior performance of the proposed greedy algorithm.
    
3. The paper presents an extensive set of experiments, including both synthetic and real data.

Weaknesses:
1. Further validation on more realistic datasets, such as ImageNet and CIFAR100, could strengthen the main points of the paper.
    
2. The experiments lack comparison with other classical score functions, such as Regularized Adaptive Prediction Sets.

Limitations:
They are adequately discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xse8QMGnyM;"REVIEW 
Summary:
Existing data in 3D human pose estimation are typically collected indoors with human actors. To address this scalability issue, the authors propose to synthesize 3D human pose data via an Osteo-kinematic model and introduce biochemical constraints for better physical plausibility. Additionally, to deal with the inherent ambiguity in single-view depth estimation, the authors introduce Binary Depth Coordinates to explicitly model the relative spatial relation between adjacent joints. Extensive experiments verify the effectiveness of the proposed approach.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Leveraging biomechanical prior knowledge to synthesize physically plausible human data is $\textbf{intuitive}$ and $\textbf{interesting}$.
2. Comprehensive experiments verify the effectiveness of the proposed data augmentation approach (BPG) and Binary Depth Coordinates (BDC). Specifically, BDC can be applied to different methods, e.g., image-based and lifting-based, showing superior generalization ability.

Weaknesses:
1. $\textbf{Repeated text}$: The first paragraph of Sec.2 appears to be a copy-paste from the abstract, which is highly discouraged.
2. $\textbf{Requirement of camera intrinsics}$: While BDC shows notable performance gains to baselines, solving depth requires camera intrinsics (principal point and focal length), typically not required by current 3D HPE methods. This requirement may introduce additional constraints for in-the-wild inference.

Limitations:
The authors have included the limitations in Sec.6. However, the requirement for camera intrinsics may also be considered a limitation and discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces two components aimed at addressing challenges in 3D human pose estimation, specifically in terms of scalability and generalization. The authors propose a Biomechanical Pose Generator (BPG), which incorporates biomechanical principles to generate plausible new 3D poses. They also introduce Binary Depth Coordinates (BDC), a component designed to mitigate the depth ambiguity encountered when lifting a 2D pose to 3D. The paper includes ablation studies to demonstrate the impact of each component, and compares these new approaches to existing pose augmentation methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper’s focus on addressing the challenge of limited datasets and enhancing the generalizability of the method is interesting and to the best of my knowledge the idea of biomechanical pose generator which does not rely on a source dataset is novel. Also, the authors’ attention to the depth ambiguity in 3D pose estimation from a single image adds a value to the field. The authors have conducted comprehensive experiments and ablation studies, which provide valuable insights into the effectiveness of the proposed components. The inclusion of cross-dataset evaluation is crucial, as it allows for a robust assessment of the Biomechanical Pose Generator (BPG) component’s effectiveness.

Weaknesses:
1- The paper is generally well-written, but some parts could be clearer. Including a figure to illustrate the entire system could significantly help reader comprehension. For example, a diagram showing the VPose (or any baseline) architecture and the integration of the BDC component might be more effective than a text-only description. Additionally, including some implementation details about the BDC component in the main paper could improve the flow of information.

2- There are some ambiguities in the experiment section that need clarification. When referring to the “source-dataset”, it would be helpful to specify whether this refers to the Human 3.6M dataset or the newly synthesized poses. Similarly, when discussing evaluations on 3DHP and 3DPW, it would be beneficial to mention the specific subset used, such as the test set.

3- There appears to be some confusion between Table 1 and the results in Figure 4 (left). While Table 1 shows improvements in the Human 3.6M results when adding new poses generated from BPG, Figure 4 (left) indicates that adding more data increases the MPJPE error (without integrating BDC). This seems contradictory and could benefit from further explanation.

4- Typos: There are a couple of typographical errors that need correction. On Line 177, (xi) is repeated twice instead of yi. On Line 287, BDC should be corrected to BPG.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a 3D human pose estimation framework that incorporates data augmentation and depth ordering information. The main contributions are two-fold: First, the proposed Biomechanical Pose Generator (BPG) generates plausible body poses based on kinematic constraints, which is used for data augmentation. Second, the Binary Depth Coordinates (BDC) disambiguate the projective depth of each joint by classifying whether the joints are positioned towards or away from the camera. The proposed framework achieved state-of-the-art performance in single-frame 3D human pose estimation settings.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The proposed method achieves state-of-the-art results in various 3D HPE datasets.
- The effect of data augmentation is validated in cross-domain learning settings.

Weaknesses:
My major concern lies on the novelty of the contribution.

- There are numerous research papers that regularize 3D human pose based on kinematic constraints. The authors did not clarify the distinctiveness of BPG from these conventional works, except for stating that BPG achieved better performance. An analysis showing how the proposed BPG generates more plausible poses compared to previous augmentation methods is required, either by displaying the generated poses or by showing qualitative estimation results.
- The concept of BDC is similar to [1] which learns ordinal depth information. The authors should cite the paper and discuss the difference.

The paper also contains	ambiguously explained parts or lacks details about their methods. Please refer to Questions section.

[1] G. Pavlakos et al., ""Ordinal depth supervision for 3d human pose estimation"", CVPR 2018

Limitations:
Suggestions
- In Fig. 4, it should be clearly stated what * means. It would be better to use GFpose+BDC instead of GFpose*.
- More detailed explanation about how $T$ in Eq (1) and ${d}_{m,n}$ in Eq (2) are formulated would clarify the methods.

Typos
- Line 185, by the projection from -> by back-projecting a ray from
- Line 212, duplicated sentences.
- Line 115, to peed -> of

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper address the task of 3D Human Pose Estimation from monocular RGB. The authors make two main contributions: The Biomechanical Pose Generator (BPG) and the Binary Depth Coordinates (BDC). BPG is a 3D human pose generator that leverages the ""Normal Range of Motion"" (NROM) that is used in the medical field to describe standard biomechanical limitations. With it, BPG is capable of generating biomechanically sound 3D human poses by randomly sampling joint angles and bones that lie within a certain ratio to each other.
BDC is a coordinate system that decompose a 3D pose into constituents. Specifically, it decomposes it into the 2D coordinate, bone length, a binary depth parameter indicating the closeness to the image plane as well as the 3D coordinates of the parent joint. This decomposition, so the authors claim, allows models to better deal with depth ambiguity. 
Experimental results demonstrate that the proposed approach achieves better performance over the compared related work on a variety of datasets (cf. Tbl 1-4). Ablative studies demonstrate that BDC helps keep performance steady even in the face of larger depth ambiguity (Tbl. 5) and that related work can benefit as well from switching to the proposed coordinates (Tbl 6.)

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors properly motivate and evaluate their approach. Depth ambiguity in monocular RGB is a challenging problem to address. I particularly liked Tbl. 5 that demonstrated that BDC is capable of handling even larger depth ambiguities.
- The paper was easy to digest and understand.
- One of the main strength of this paper is that BDC can be combined with other related work, yielding improvements (Tbl. 6)

Weaknesses:
- My biggest concern about the paper is that BDC is very similar conceptually to ""Hand Pose Estimation via Latent 2.5D Heatmap
Regression"", Iqbal et al., ECCV'18. Yet there is no mention of the paper, let alone any comparisons. The mentioned paper also addresses with depth ambiguity by decomposing the 3D pose into 2D pose and a root-relative depth vector. Addressing the differences, performing comparisons with this approach would better contextualize as well as strengthen the contribution of the paper.
- BPG shows to improve performance by improving the 2D to 3D lifting component. Yet, it's contribution is rather sparse, as it essentially amount to performing forward kinematics on bounded joint angle and bone lengths. It does not take into consideration statistics on poses. Certain poses are more common, due to them corresponding to actual human movement patterns (such as walking) that are affected by gravity. Randomly sampling poses without taking such statistics into consideration may generate a range of synthetic poses that are unrealistic, leading to non-optimal improvements.

Limitations:
The authors address limitations of their methods, such as not taking temporal dynamics into account.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xrbgXJomJp;"REVIEW 
Summary:
This paper extends the IQ-Learn method to cooperative multi-agent settings. The main insight is to use mixing networks to enable centralized training via decentralized Q functions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is quite relevant to NeurIPS and it is indeed important to extend IQ-Learn (or similar inverse learning algorithms) to multi-agent systems.

Weaknesses:
- The major concern that I have is that, if my understanding is correct, the paper assumes access to the global state information. This is not realistic. In real application, this will never be the case. So the algorithm does not seem useful in practice.
- Typo: In line 62, it should be ""generalization"" instead of ""generation"",
- In line 72, \citet should be used instead of \cite or \citep so that the author names will become a part of the sentence.
- In line 162, \eqref should be used instead of \ref so that the parenthesis will appear around the equation number.
- The architecture figure is in page 7. It would significantly increase the readability if it came earlier. 
- By the time the reader reads line 191, the IGC principle is still undefined. This makes reading very difficult.
- The same thing is true at line 203, too.
- Typo: In line 241, it should be ""makes"" instead of ""make"".
- Typo: In line 242, it should be ""yields"" instead of ""yield"".

Limitations:
The paper does not discuss the broader impacts. I disagree that there is no potential societal impact. I invite the authors to think about the applications their algorithm may have and then consider how their algorithm would affect those applications (both positively and negatively).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the problem of extending a single-agent imitation learning algorithm, inverse soft-Q learning (IQ-learn, Garg et al. Neurips 21) to the multi-agent cooperative setting. The proposed algorithm, MIFQ, leverages the ideas of mixing networks and the individual-global-max (IGM) principle, to perform the extension. Experimental evaluations of MIFQ are conducted on SMAC-v2, MPE, and Gold Miner, and demonstrate that MIFQ improves over baselines across various domains and with varying numbers of demonstrations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses the challenge of generalizing a key imitation learning (IL) algorithm from single-agent to multi-agent settings, offering a novel approach with MIFQ. The problem is clearly specified and represents an important contribution to the MARL literature. 

The empirical results are robust:
- MIFQ outperforms most baselines with various demonstrations.
- Extensive experiments across multiple domains and tasks confirm MIFQ's superior performance.
- Comprehensive comparisons with baselines (BC, independent IQ learning, alternative Q-factorization methods, etc.) highlight MIFQ's advantages.

Weaknesses:
1. Some aspects of the method do not seem fully justified to me: 
    - The authors claim in lines 143-148 that a shortcoming of the IQ learn method is that the objective depends on the centralized state and joint action. However, Section 5.4 of the IQ Learn paper presents a state-only objective (independent from the actions). I wonder if the authors could discuss whether a simple state-only extension of IQ Learn, where critic depends on the centralized state as usual, but the actor depends on the observations, would be sufficient to sidestep many of the concerns addressed by IQ Learn? 
    - The authors also claim in Section 4.1.2 that the straightforward Independent Inverse Q-learning is not a satisfactory solution because the method ""…has limitations in addressing the interdependence between agents and the global information available during the training process."". Can the authors more explicitly discuss what the shortcomings of an independent version of IQ-learn is not satisfactory? Does it suffer from convergence problems? 
					
2. The current experimental analysis is somewhat shallow, and essentially amounts to a description of the plots. The authors could improve the analysis of MIFQ by considering the following additional questions: 
    - The original IQ learn paper plots the rewards to validate that their method recovers the ground truth reward. Can the same be done here? 
    - Why does MIFQ perform worse than BC on MPE, particularly the reference and spread tasks?
3. There are some issues with how the experimental results have been reported. 
    - What is the number of trials for each of the results? Please include this in the main paper. 
    - The caption of Figure 2 is missing key information to understand the figure. What is the number of demonstrations used to train each of the methods? What does the shaded region mean? Based on the std devs reported in the Appendix, I assume it is the standard deviation; please see the note below and instead compute 95% confidence intervals. 
    - No measurements of uncertainty are provided in Table 2, and standard deviations are provided only in the Appendix. Standard deviations reflect the underlying variance in models learned by the algorithm, rather than providing a measure of statistical significance. Please also compute 95% confidence intervals to enable readers to judge the statistical significance of the gaps in mean test returns -- ideally, bootstrapped confidence intervals. See this paper for a reference on best practices:  https://arxiv.org/abs/2304.01315
3. There are also some minor clarity issues: 
    - IGC is used in line 192, but is only explained in the following Section 4.2.2
    - Definition 4.2 - this definition is not specific enough to be useful. It handwaves  by only requiring that the joint policy be 'equivalent' to the collection of individual optimal policies. Equivalent in what sense?

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel algorithm, Multi-agent Inverse Factorized Q-learning (MIFQ), for cooperative multi-agent imitation learning (IL). It extends the inverse soft-Q learning framework to multi-agent settings by introducing a mixing network architecture for centralized training with decentralized execution. This enables learning local and joint value functions effectively. The authors conducted extensive experiments across multiple challenging environments, demonstrating that their approach outperforms existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-  The introduction of a multi-agent extension of inverse soft-Q learning using factorized networks is a significant and novel contribution to the field of IL.  
- This paper is well-written and organized, and provides a sound theoretical analysis.
- The empirical results across three different environments, including a complex version of the StarCraft multi-agent challenge, are impressive. The proposed method outperforms existing baselines.

Weaknesses:
As someone who is not an expert in the field of imitation learning, I perceive no significant weaknesses in this paper from my perspective.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the imitation problem in cooperative Multi-Agent Reinforcement Learning (MARL). It extends inverse soft-Q learning to the multi-agent domain by leveraging value factorizations under the Centralized Training with Decentralized Execution (CTDE) paradigm. Experimental results demonstrate the effectiveness of the proposed approach across several environments.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The study of imitation learning in MARL is a valuable and relevant research problem, and the paper provides promising solutions.
- The experimental results are robust and convincingly support the proposed method's effectiveness.

Weaknesses:
- The paper's organization could be improved. The current structure alternates between theory and architecture without a clear flow.

- The similarity between IGC and IGO[1] requires further clarification.

- The objective function (6) introduces sub-optimality compared to the original objective (3) due to the restriction that $Q^{tot}$ and $V^{tot}$ must be monotonic. Additionally, since $Q^{tot}$ and $V^{tot}$ use different mixing networks, the relationship between them violates Equation (2). This indicates that Equation (6) does not represent the same objective as Equation (3), even without considering the sub-optimality introduced by factorization. These issues need further theoretical exploration and discussion.

- Although the experimental results are promising, the superior performance seems to stem from the QMIX algorithm's advantage over other MARL algorithms. An important missing baseline is the soft actor-critic version of IQ-Learn, which uses a centralized Q function with decentralized critics and does not seem to violate the original objective.

[1] Zhang, et al., FOP: Factorizing Optimal Joint Policy of Maximum-Entropy Multi-Agent Reinforcement Learning, ICML 2021.

Limitations:
Limitations are discussed in conclusion section.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xqrlhsbcwN;"REVIEW 
Summary:
The paper proposes a novel training framework for regression tasks called the Approximated Orthogonal Projection Unit (AOPU), optimized using truncated natural gradients. The authors utilize the Rank Rate (RR) of the augmented data covariance matrix as a metric. They demonstrate that their method offers more stable training than existing architectures and optimizers, which is crucial for industrial applications requiring online training during production. Additionally, the authors provide a comprehensive analysis of their setup's convergence.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Detailed introduction on the background and intuition.
2. The method is very simple.
3. A thorough theoretical analysis of the method was provided.

Weaknesses:
1. Poorly arranged paper; conclusions are at the end of the appendix.
2. In the introduction, the authors claim that their methods improve interpretability, but they do not explain later why that matters. Also, many existing works explain the behavior at a neuron level; it is not clear why one has to track the parameter itself.
3. Experimental qualities are not good; no hyperparameter search is mentioned in the paper, which is essential when the authors claim that their method improves training stability.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the Approximated Orthogonal Projection Unit, the basis for a new neural network, designed to enhance the stability and interpretability of regression models, particularly in industrial soft sensor applications. The primary aim is to address the need for stable and immediate optimization in online settings, where traditional NN training techniques fall short. The paper introduces the theoretical background and demonstrates the effectiveness on two tasks, while also introducing ablations and comparisons to several other techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method appears novel and straightforward.
- The paper provides a solid theoretical foundation.
- The paper imrpoves interpretability of the neural network's behavior and training dynamics by differentiatiating between trackable and untrackable parameters, enhancing the interpretability.
- The authors demonstrate superior performance of AOPU in experiments with two chemical process datasets, showcasing its practical effectiveness in achieving stable convergence compared to existing models.
- Practical Relevance: Tailors the AOPU framework specifically for industrial soft sensor applications, addressing the need for immediate optimization and stability in online settings.
- Limitations, such as numerical stability issues during matrix inversion in the training process, are discussed.

Weaknesses:
- Code not published. The justification provided is somewhat questionable, since easy reproducibility should also enable the authors to provide code (possibly mirrored from the code implemented at the company).
- While the page limit is formally met, the authors make extensive use of the Appendix, including core elements of the paper. The Conclusion and Limitations, for example, are in the appendix.
- There is no mention of thorough hyperparametertuning and its results.

Limitations:
The limitations have been addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a new model for soft sensor tasks, the Approximated Orthogonal Projection Unit (AOPU), to enhance the stability and interpretability of regression networks. AOPU incorporates trackable and dual parameters, which are treated differently during the inference and training processes.  AOPU truncates the gradient backpropagation at dual parameters, optimizes the trackable parameters updates, and enhances the robustness of training. The paper provides theoretical proof that AOPU is an approximation of both MVE and Natural Gradient Descent (NGD).Experimental results on two chemical process datasets demonstrat that AOPU outperforms other models in achieving stable convergence.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method is novel and has a strong theoretical basis. The authors provide detailed proofs of theorems in the appendix.
2. If the contents in the appendix are considered, this paper analyses the proposed AOPU from many aspects, and provide sufficient experimental results and ablation study to validate the advantage of AOPU.

Weaknesses:
1. Due to the limitation of paper length, the contents in the formal paper is incomplete. Many important content like quantitative analysis and ablation study are put in the appendix. The formal contents also lacks a conclusion section. For the quality of publishing, I suggest submitting the paper to other platforms like a IEEE Transaction, where the paper length can be longer.

2. The proposed method are not incorporated into DNN structures, therefore its expressive power is limited in more complicated tasks. Considering the requirements of industrial soft sensor tasks, this is not a critical flaw, but it still hinders AOPU from challenging AI applications.

Limitations:
The limitations of AOPU is discussed in the paper, specifically, the need for an understanding of the RR distribution to guide the selection of hyperparameters. I do not see potential negative societal impact of the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xpRUi8amtC;"REVIEW 
Summary:
This paper proposes SDSGG, a novel open vocabulary scene graph generation(OVSGG) algorithm that leverages the reasoning capability of a LLM to better determine the relations between objects in the scene. It achieves this goal by first prompting a LLM with multiple persona prompts to expand a simple relational predicative to a list of detailed visual descriptions, which are subsequently used to augment the classification process.  It also introduces a novel mutual visual adapter, which better captures the interaction between subjects and objects.  Experiments show that these proposed designs are effective.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Incorporating a LLM to augment the predicate labels for scene graph generation is a novel idea.  This paper provides meaningful insight to future works in this area.
2. The experiment results (table 1-2) are strong, significantly outperforming previous methods.
3. The authors conducted extensive ablation studies on various design elements.

Weaknesses:
1. Prompting the LLM is a key element of the method, however some crucial details are missing. For example, how are the prompts constructed? While the author provided the prompt in Appendix Fig 5, it is unclear how the  ""{scene content to be discussed}"" is generated. The author did show some examples throughout the paper, but they are not sufficient for the reader to understand the underlying process. In particular, in L167, the author showed example #1 ""Imagine there is an animal that is eating, "". In Fig 1c, there is example #2 ""Assuming that the scene has a man riding a horse.""  These two descriptions have two different granularity, as one only includes the generic concept of ""an animal that is eating"" while the other has specific class names ""man"" and ""horse"". The authors should clearly describe what information is included into the prompt, and discuss the scalability and cost of generating such prompts. I suppose if the prompts are like example #1, they can be generated offline based on predicative label sets. However, if the prompts are like example #2, they need to be generated for every possible triple of (subject, predicative, object) over the label space, or be generated online over possible objects in a scene. It is unclear which is the case.

2. Additional discussions and experiments are required to justify some of the design choices. For example,

  2.1 in eq 8, the loss of descriptions marked by possible coexistence is to make the prediction ""close to those of CLIP."" (L255). If this is the case, why not directly use CLIP results for these possible coexistence descriptions at inference time (eq 2)?

  2.2 some discussion is needed on if CLIP is good at classifying the generated descriptions. What are the nature of these descriptions and do they fit well with CLIP's pretraining pipeline (i.e. object-level image caption)? As a concrete example, can CLIP properly distinguish descriptions involving counting, such as ""with four legs"", and ""with two legs"", mentioned in the examples?

 2.3 what happens if we discard ""possible coexistence"" descriptions and only use definite coexistence and contradiction? Table8 shows that it is ideal to have a low weight for ""possible coexistence""  loss. What happens if we set the weight to 0 and remove it at inference pipeline?

Limitations:
The authors discussed limitations and societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to solve the open-vocabulary scene graph generation problem. Previous methods mainly adopt scene-agnostic prompts as text classifiers. The authors argue that using the fixed text classifiers not only struggles to model visual relations with high variance, but also falls short in adapting to distinct contexts. Therefore, the authors propose the scene-specific description based OVSGG framework. They employ an LLM and ask it to play different roles. Besides, they design the mutual visual adapter to encode visual features. Extensive experiments show that the proposed method significantly outperforms top-leading methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The motivation and idea of this paper are innovative and interesting. Simply applying LLM to SGG cannot effectively reason the relationships. The authors consider employing the context and introducing multiple roles of LLM, which is shown to be effective for solving the OVSGG problem.

Besides, the experiments are convincing. Plenty of ablation studies are provided.

Weaknesses:
My main concern is Computational Complexity: The proposed framework involves multiple stages, including generating descriptions, renormalizing them, and applying mutual visual adapters. This multi-step process could be computationally intensive, making it less practical for real-time applications or scenarios with limited computational resources.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper starts by discussing methods for Open-vocabulary Scene Graph Generation (OVSGG) based on the CLIP model, highlighting the issue that current OVSGG methods do not differentiate between various scenes, which limits their effectiveness. The authors introduce SDSGG, a scene-specific description-based OVSGG framework that improves both the textual and visual parts, enhancing the model's open-vocabulary relationship prediction capabilities.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The novelty of this paper lies in its analysis of the issues present in current OVSGG methods, leading to the conclusion that differentiating between scenes is necessary to enhance the performance of OVSGG. The proposed Scene-specific Descriptions are particularly insightful.
2. The paper validates its findings on two datasets, VG and GQA, with experimental results showing significant performance improvements over previous state-of-the-art methods.

Weaknesses:
1. The description in Sec3.1, Scene-specific Text Classifiers, of the paper is somewhat confusing. This confusion arises primarily because the text section includes multiple different naming conventions and several distinct modules. It is recommended that this section be rewritten to make it easier for readers to understand. Additionally, the terminology used in this section is inconsistent with that in lines 64~77, leading to comprehension difficulties.
2. For the OVSGG method, it is suggested to also train the model on a full set of relations and compare its performance with conventional SGG methods to ensure that it achieves good performance under standard settings.
3. Is the model robust to different base/novel splits? It is recommended to train and test the model on different base/novel dataset divisions to assess its robustness.
4. It is advised to train and test the model on the PSG dataset as well.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xoCFd1WKpf;"REVIEW 
Summary:
The authors propose a method based on lexical representation for Visual-Language Alignment (VLA). The method relies on aligning two strong unimodal models, namely DINOv2 for the visual modality and Llama 2 for the text modality. Each backbone is fine-tuned with a few adapters or additional layers. The two modalities use separate codebooks mapping to a joint vocabulary. The authors also propose an overuse penalty to limit the excessive activation of irrelevant tokens. Finally, the authors introduce the PatchDis metric to measure patch-level alignment. Evaluation on zero-shot cross-modal retrieval datasets shows state-of-the-art performance of the method with the compared baselines. Additional experiments on the patch-level representation and sparsity showing the effectiveness of the method are also reported.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The authors proposed an effective and interpretable Lexical Representation approach for Visual-Language Alignment
- The proposed method is described clearly
- The experimental results show state-of-the-art performance in comparison to the baseline selected

Weaknesses:
- The vocabulary is based on the Llama tokenizer which, as stated in the limitations, may split words into meaningless sub-word tokens and may also lack longer relevant words.
- The latent baselines for zero-shot cross-modal retrieval do not include recent methods such as BEiT-3 [Wang, Wenhui, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal et al. ""Image as a foreign language: Beit pretraining for vision and vision-language tasks."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19175-19186. 2023.]
- One main difference with the compared methods could be the use of the DINOv2 visual backbone and the Llama 2 textual backbone, it is possible the proposed method benefits from these strong backbones. All methods' visual and text backbones (and their potential pretraining) should be discussed in detail to enable the readers to properly judge the merit of the proposed method

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes LexVLA, a more interpretable VLA framework that learns a unified lexical representation for both modalities without complex design.
LexVLA uses DINOv2 as the visual model and Llama 2 as the language model, proposing an overuse penalty to avoid false discoveries.
LexVLA outperforms baselines on cross-modal retrieval benchmarks, even when fine-tuned on a modest dataset.
Extensive experiments were conducted to analyze LexVLA's performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is easy to follow.
2. The framework does not require complex design or training configurations, making it more accessible and efficient.
3. LexVLA outperforms baselines on cross-modal retrieval benchmarks, even when compared to models trained on larger datasets.
4. Ablation demonstrates the decision choice and effectiveness of proposed components.

Weaknesses:
1. I can't quite get the novelty of this work. The lexical representation mentioned in the paper is somehow a way to select important information and then map it to the code book. However, the codebook strategy was explored [1]. Especially the visual part, where does the concept of Lexical come in? Can the author elaborate more on this?
2. In Table 1, the improvement is pretty limited in the bottom block compared to using CLIP in the last and first blocks. It makes readers question whether the performance was gained by the DINOv2 representation.
3. The alignment was tested on only one task, it will be more interesting to test on other multimodal tasks such as zeroshot classification, or even grounding since it has DINOv2 representation.


[1] Duan, Jiali, et al. ""Multi-modal alignment using representation codebook."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents LexVLA, a vision language alignment method integrating a pretrained vision model and a pretrained language model. To retain the original capabilities of pretrained single-modal models, it adopts a unified lexical representation with unique codebooks. Moreover, the vision model is tuned with a projector, and the text model is tuned with LoRA. A metric for patch-level alignment is proposed to evaluate interpretability. Experiments are conducted on retrieval benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The content is rich. An architecture, an objective, and a metric are proposed.
- Inserting lightweight components to tune vision and language models to learn lexical representation while refraining from original capability degradation is intuitive. 
- The LexVLA can be applied to various architectures.
- Experiments are conducted on multiple benchmarks.

Weaknesses:
- Even though a new metric is proposed, the effectiveness of its reflection on interpretability is not verified quantitatively or qualitatively.

Limitations:
While multiple technical contributions have been made in this paper, some of the components lack rigorous verification.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xnmm1jThkv;"REVIEW 
Summary:
This paper studies structure learning problem for additive noise model (ANM) in both linear and nonlinear settings. It proposes a hybrid constraint based approach to learn the DAG by leveraging the local ancestral relationships. The algorithm consists of ordering search and edge discovery these two steps. Correctness is shown and simulation is conducted to compare with other approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Though ANM is shown to be identifiable for a long time, e.g. by RESIT, the high computational complexity and hardness in nonparametric regression and CI tests stand as roadblock. The finer analysis and exploitation of local structure in the proposal show potential to tackle this task efficiently;
- The introducing of the proposed method is well-written and easy to follow for researchers working in relevant area.

Weaknesses:
- The main contribution of this work is the exploitation of local structure to reduce the number of nonparametric regression and CI tests. However, despite of the quick discussion below thm 3.7 and 4.5, there is no explicit and formal statement on these to emphasize the contribution, and also comparison with others, e.g. RESIT. 
- The experiments are preliminary. More setups should be considered to demonstrate the superiority of proposal: e.g. different graph types like scale-free graphs, different number of edges, different noise, recovery criterion like F1 for linear setting, more benchmarks like CAM, GSGES, etc. 
- See Questions.

Limitations:
No limitation is discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors present a causal discovery method by firstly determining the order of the causal variables, then determining the existence of edges between any two variables. The experimental results demonstrate the superiority of the proposed method compared to relevant methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
I thank the authors for their detailed clarifications, which address most of my concerns. I increase my score to 5.

------------------------------------

Despite the theoretical results simple, the idea and the method are interesting and somewhat novel. 

The theoretical results seem sensible.

Weaknesses:
1. Lack of necessary discussions: I think there are some similar idea in the literature, such as [1], where they maintain the order of the variables. What is the advantage of this method on [1]? The proposed method should be compared to [1] as well. 
[1] L. Solus, Y. Wang, C. Uhler, and L. Matejovicova. Consistency guarantees for permutation based causal inference algorithms. ArXiv preprint arXiv: 1702.03530 (2017)

2. Lemma 4.1 is confusing. In the condition, it is required that $x_i$ is one of the parents of $x_j$. Why is it possible that $x_i$ and $x_j$ are not causally related?

typo: 
Line 202: the

Limitations:
No.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents theoretical results about extensions of the partial order induced by a causal DAG and uses these results to propose new constraint-based algorithms for ANMs.

**Edit**: increased rating from 3 to 5, soundness from 1 to 2, and contribution from 2 to 3.

**Edit 2**: increase rating from 5 to 6, after the authors fixed $A_{top}$ calculation; solid paper, but I think the impact of a hybrid causal discovery method like this is limited

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- Takes a simple idea (which seems original but also somewhat obvious from an order theory perspective) and turns it (creatively, originally) into causal discovery algorithms with contisency guarantees, broad applicability (ANMs), and good identifiability (specific DAG instead of MEC)
- Very clearly written, as far as grammar, organization, motivation (but importantly, not mathematical notation)
- Based on the theoretical results, the algorithms have potential to be very significant to the field of causal discovery

Weaknesses:
1. The main (and fatal) weakness is the claims of strong performance in the abstract combined with the inadequate experimental results:
    1. the abstract makes a claim of ""achieving greater accuracy than current methods"", but then the limited experiments only compare on simulated data (rather than real) to a few closely related algorithms (as opposed to a selection of classic or state-of-the-art methods, such as PC or GRaSP) in settings the authors have already explained are challenging for existing algorithms (very sparse DAGs, rather than a range of sparsities), and even then the proposed algorithm doesn't seem to do especially well. It also seems the NHTS algorithm is missing from the experiments.
2. A smaller but nonetheless important weekness is notation that contradicts mathematical conventions, making the writing unecessarily difficult:
    1. consulting introductory texts on partial orders and order theory would help clear up some of the confusion. For example, a topological sort is conventionally a linear extension of a partial order, making the introduced terms ""linear topological sort"" and ""hierarchical topological sort"" confusing. Replacing the former introduced term with just ""topological sort"", ""linear order"", or ""total order"", and replacing the latter introduced term with something that more clearly indicates it is 'between' a partial order and a linear order (i.e., it extends the partial order, but not completely into linear order), would be more natural/conventional and easier to understand.
    2. the authors seem to use $\mapsto$ to indicate the domain and image of the ordering functions, but $\mapsto$ conventionally denotes how a specific element in the domain is mapped to a specific element of the image, hindering precise and easy comprehension.
    3. other notation in Definition 2.1, such as inconsistent/unexplained indexing of $\pi$ make the definition harder to understand/not rigorous
    4.  it's unclear what the difference between $x_j \dashrightarrow x_i$ (called a directed path) and $x_j \dashrightarrow \ldots \dashrightarrow x_k$ (called a front door path) is.

Limitations:
The authors have adequately described what assumptions their methods require (and hence to which settings the results are limited).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper mainly focuses on proposing efficient search algorithms for finding the hierarchical sort ordering (linear topological sort) of variables. As mentioned in Section 5, finding such hierarchical orders can significantly improve the efficiency of causal discovery of edges, making the algorithm tractable (traditional algorithms such as PC are exponential). The paper studies two cases: linear (LiNGAMs) and non-parametric, where a complete algorithm based only on path analysis is developed for the linear case, and a combination of path analysis and layer-wise search is developed for the non-parametric case. Both algorithms improve the discovery of hierarchical order.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well structured and clearly written. The theoretical contributions, including the causal path analysis and corresponding algorithms, are interesting and also important in practice as can be told from the analysis of computational complexity. All results are properly formulated as definitions and theorems and proofs are included in the appendix. Experiments are also conducted and their results are discussed in depth in Section 6. In general, I enjoyed reading it.

Weaknesses:
- In general, I suggest adding more examples to demonstrate the procedure of algorithms, probably for NHTS (Algorithm 2) so that we can see a clear cut between the two stages (root-identification and layer identification).
- While the authors touched a bit at the beginning of Section 4, non-experts may benefit more if the paper could include additional details about the difference between the linear and non-linear cases (especially how they affect conditional independencies if any). 
- For definition 2.1, it will be great to provide a hierarchical topological sort that cannot be trivially converted to a linear topological sort; that is, we cannot simply add more layers to a hierarchical sort to obtain a linear topological sort.
- Lemma 4.1 is a little confusing to me: if $x_i$ is a parent of $x_j$, how are PP1 and PP4 possible? $x_i$ must be a direct cause of $x_j$, right? Also, when you say ""$x_i$ and $x_j$"" are not causally related, does it mean that there is no directed edge from $x_i$ to $x_j$ or no directed path? Does ""active path"" mean any unblocked dependency path (backdoor or frontdoor)?

Limitations:
OK

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xjyU6zmZD7;"REVIEW 
Summary:
The paper trains SNNs using surrogate gradient learning. In order to mitigate the gradient vanishing problem, the paper proposed the Shortcut Back-propagation method and utilizes an evolutionary algorithm framework to balance the training of shallow and deep layers. The effectiveness of the proposed method is demonstrated through many experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1)	The shortcut backpropagation method and the evolutionary training method are novel. 
2)	This paper can well handle the gradient vanishing problem.
3)	The paper is well-written.
4)	The paper shows the effectiveness of the proposed methods through many experiments.

Weaknesses:
1)	The author should add more mathematical proof to demonstrate that the mentioned residual structure in SNN is not very effective? The introduction of shortcut branches might add complexity to the network architecture, which could affect the interpretability of the model.
2)	Some recent SOTA works should be compared with too.  The authors can also compare with paper [1][2] which obtains really good results by MS-ResNet-18 backbone with 1 or 6 timesteps on large imageNet datasets.


[1]Yao M, Zhao G, Zhang H, et al. Attention spiking neural networks[J]. IEEE transactions on pattern analysis and machine intelligence, 2023.

[2] Qiu X, Zhu R J, Chou Y, et al. Gated attention coding for training high-performance and efficient spiking neural networks[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(1): 601-610.

Limitations:
I find no limitation about the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a simple method to mitigate the gradient vanishing problem in the training of SNNs. This method introduces some early classification heads (including a pooling layer and a fully connected layer) to the SNN. Because the gradients from the early classification heads pass fewer surrogate gradients, this method aids the SNN in addressing the gradient vanishing problem. The authors also suggest an evolutionary training framework that changes the loss function to gradually adjust how important early classification head outputs are during the training phase. The proposed methods are only alive in the training phase and will not affect the inference phase of SNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This proposed method partially alleviates the gradient vanishing problem in the training of SNN with surrogate gradients. Furthermore, the method has demonstrated excellent performance across multiple datasets. The Short-BP method can be easily integrated into the SNN training process without introducing excessive computational overhead. Furthermore, the evolutionary training framework effectively mitigates the short-BP problem, which may make the network pay more attention to early classification heads than the final SNN output. The writing in this paper is clear and concise.

Weaknesses:
1. In this paper, the author only demonstrates a change in gradient distribution in the first layer. Presenting the changes in the men and variance of the absolute gradients for each layer would provide a more direct proof of their argument.
2. The author should provide a more detailed mathematical proof to explain why the use of surrogate gradients in deep SNN would lead to gradient vanishing, as well as why direct use of residual learning will not address the problem.
3. The author has not demonstrated their method on much deeper network architectures where the gradient vanishing problem is more severe.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes shortcut connections between layers to mitigate the gradient vanishing problem in SNNs. Additionally, the authors present a way to phase out the shortcut connections over training so that inference can be done without these additional connections. The experiments show that this method improves training performance in several image classification tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.The idea is small, but interesting and effective enough.

2.The performance improvement over the existing SNN methods is noticeable.

3.The paper is well-written.

Weaknesses:
1.The proposed method will increase the training time.

2.In the experimental section, some newer methods should be compared with this method.

3.Figure 2 lacks horizontal and vertical coordinates, and the readability and comprehensibility of the picture need to be improved.

Limitations:
None.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xjXYgdFM5M;"REVIEW 
Summary:
This paper addresses the challenges associated with the decline in performance of LLMs after undergoing knowledge editing. The study identifies the primary factors contributing to performance degradation from both data and model perspectives. By constructing a Multi-Question Dataset (MQD) and analyzing the impact of editing objectives, token length, and diversity, the paper finds that perplexity associated with editing objectives significantly affects model performance. From the model perspective, a strong correlation was observed between the L1 norm of parameter layers and editing accuracy. The paper proposes a novel method called Dump for sequence (D4C), which effectively manages the parameter growth and improves model performance post-editing.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Innovative Methodological Approach: The study introduces a new method, D4C, which addresses the explosive growth in parameter norms and optimizes model performance post-editing. This approach is both innovative and practical for managing edited models.
- Comprehensive Data Analysis: The construction of the Multi-Question Dataset and detailed analysis of how different types of data affect model performance provide valuable insights into the mechanics of model editing.
- Clear Identification of Problems and Solutions: The paper clearly identifies specific problems associated with knowledge editing in LLMs, such as catastrophic forgetting and performance bottlenecks, and provides targeted solutions to these issues.
- Empirical Validation: The experiments conducted in this paper offer empirical evidence supporting the proposed methods, enhancing the credibility and applicability of the findings.

Weaknesses:
- Generalizability of Findings: The study focuses on specific scenarios and datasets, which may limit the generalizability of the findings across different types of LLMs or editing tasks.
- Potential Overfitting to Edited Scenarios: There is a risk that the model may become overly optimized for the edited scenarios, potentially affecting its performance on unedited or unrelated tasks.
- Complexity of Implementation: The proposed D4C method, while effective, may be complex to implement and integrate into existing systems due to its sophisticated handling of parameter layers.
- Unsuitable Citation Format: The citations in this paper are in the format of “XXX et al. [YEAR]”, which are not suitable enough, and had better change into the format of [1], [2], [3], ……

Limitations:
- Dependency on Specific Data Characteristics: The effectiveness of the proposed solutions may depend heavily on the characteristics of the data used for training and testing, which might not be consistent across different domains or applications.
- Evaluation Metrics: While the paper introduces new evaluation methods, the reliance on perplexity (PPL) and L1 norm metrics might not completely capture all aspects of model performance and health, especially in nuanced or context-dependent scenarios.
- Limited Experimentation: The experiments (Section 5) in this paper are too limited to demonstrate the conclusion. 
- Scope of Editing Objectives: The study might not fully capture the impact of highly diverse or complex editing objectives that could be encountered in real-world scenarios.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Recent research has shown varying degrees of decline in model performance following small changes made by certain model editing methods. This paper is the first to comprehensively analyze the reasons behind such performance declines. Through extensive experiments, it identifies two main factors: data and model. For data-specific factors, the paper finds that perplexity and token length significantly influence performance. For model-specific factors, the L1 norm of the edited layer is identified as a key influence. Building upon these insights, the paper proposes a method named Dump for sequence (D4C), which significantly improves model performance.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-motivated: Exploring the reasons behind and impact of small changes made by model editing techniques on the performance of unedited samples is of great significance.
- The analysis of the data-specific and model-specific factors is supported with diverse datasets and comprehensive experiments. The model-specific analysis, in particular, is evaluated rigorously, addressing the forgetting issue that prior works often overlooked

- The observation of the influence of editing on the model norm is intriguing. High-norm parameters can be sensitive to noise and numerically unstable. It would be beneficial if the authors could also provide an L2-norm plot for comparison.

- The experimental results are impressive, demonstrating significant improvements and validating the effectiveness of the proposed method.

Weaknesses:
- My main concern with the data-specific analysis is whether the conclusion is about correlation or causation. Many variables can be changed about the input data. Plotting a single Figure 3 might be insufficient to justify that perplexity and token length are the main reasons for the decline in model performance after editing.

- Unfortunately, the constructed dataset is not open-sourced. 

- Recent research [1] has shown that model editing methods (e.g. ROME, MEMIT) are not good at handling multi-hop questions, how would D4C perform in such more challenging scenarios?

- Some theoretical analysis can be conducted to demonstrate that D4C does not lead to an increase in norms.

[1] Mquake: Assessing knowledge editing in language models via multi-hop questions. EMNLP 2023

Limitations:
The limitations are discussed in the paper.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates the reasons behind performance decline in sequential model editing approaches that selectively update parameters based on both data and model factors. To address the issues causing this decline, the authors propose a method to save editing history, thereby transforming sequential editing into batch editing with minimal computational overhead.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Extensive experimentation is conducted to empirically demonstrate how factors such as dataset characteristics, editing objectives, and model-specific properties affect performance in sequential model editing.

A simple matrix storage solution is introduced, which enables the conversion of sequential editing into batch editing.

Weaknesses:
The study is restricted to two closely related editing approaches.

Experimentation is limited in demonstrating the efficacy of the D4C method. Different datasets and a larger number of edits for a more thorough evaluation are needed.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the reasons and solutions for the decline in model performance of model editing.  The authors conduct experiments from two perspectives: data and model. Specifically, to clarify the impact of data on the performance of edited models, the authors first evaluate how editing different types of data affects model performance. Then, the authors construct a Multi-Question Dataset (MQD) and identified that the performance of  the edited models is primarily influenced by the diversity of the editing objectives  and the length of the tokens. Secondly, the authors explore the factors that affect model  performance from a model perspective. Experiments revealed a strong correlation between the L1 norm of the edited model layers and the editing accuracy, and  identified an editing quantity bottleneck. To enhance the performance of edited  models, the authors propose a Dump for sequence (D4C) method that effectively improves  the performance of edited models and overcomes the previous editing bottleneck issue. This method allows for multiple effective edits with minimal impact on  model performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper investigates the impact of data on the performance of edited models. Evaluations are conducted across multiple tasks, revealing that the editing objective is the primary factor influencing model performance.

The authors found that the decline in edited model performance is correlated with the explosive growth of the L1 norm of parameter layers during the editing process.

This paper proposes a caching sequence edit method that leverages O(1) space complexity to retain past knowledge and regulate the explosive growth of the parameter layer norm.

Weaknesses:
The writing of this paper should be improved. There is no overview of this paper, which makes it hard to follow the details of Section 3 and 4.

The motivation of the proposed method  is not clear.

There are many typos such as line 182.

There are many missing references such as: 

Knowledge Editing for Large Language Models: A Survey

Stable Knowledge Editing in Large Language Models

A Comprehensive Study of Knowledge Editing for Large Language Models

Editing Large Language Models: Problems, Methods, and Opportunities

Limitations:
Yes

The limitations are on page ten. I am unsure if this counts as exceeding the page limit.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xgiurUq0ss;"REVIEW 
Summary:
This paper proposes DDK, a knowledge distillation (KD) framework that distills large language models (LMs) into small LMs. Unlike previous KD methods, DDK dynamically adjusts the domain weights during distillation. Experiments show that DDK outperforms other KD baselines across various tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well written and the method is easy to follow.
2. The experiments show that DDK outperforms other KD baselines on various tasks.

Weaknesses:
The extra computation introduced by KKD should be considered. It seems KKD requires the inference of a large LM during the training of the small LM. When the teacher model is much larger than the student model (QWen-1.5 14B v.s. QWen-1.5 1.8B), the inference cost of the teacher model would be even larger than training the student model. Therefore, it is more reasonable to compare the performance of the distilled model and the baselines given the same FLOPs.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new framework called Dynamic Domain Knowledge Distillation (DDK) to enhance the efficiency of knowledge distillation for large language models (LLMs). Unlike traditional methods that overlook domain performance differences between student and teacher models, DDK dynamically adjusts the distillation dataset composition based on these differences, ensuring a more stable and effective knowledge transfer. This approach addresses the issue of excessive focus on domains with minimal performance gaps and enhances overall model performance. Extensive evaluations demonstrate that DDK significantly outperforms existing knowledge distillation methods and continuously pretrained baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed dynamic dataloader for KD is technically sound. 
- Numerical experiments well validate the efficacy of the method.

Weaknesses:
- Dynamic dataloader requires knowing the training data distribution and category beforehand. 
- Missing references. Similar ideas have been explored in pruning LLMs, such as ShearedLLaMA, LoRAShear to recover the knowledge . The paper needs to discuss with them in the related work section due to the closed relation between pruning and KD. 

Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning

LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The work introduces a novel framework for knowledge distillation (KD) for LLMs. The key innovation of DDK is its dynamic adjustment of the distillation dataset composition based on domain performance differences between the teacher and student models. The paper presents extensive evaluations demonstrating that DDK significantly improves performance in various KD settings, outperforming both continuous training baselines and existing KD methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.  The authors provide extensive empirical evidence demonstrating the effectiveness of DDK in improving the performance of student models across various benchmarks.
2.  As the computational and storage demands of LLMs are significant barriers to their widespread deployment, KD is a promising solution.  The proposed KD method is simple and easy to follow.

Weaknesses:
1. Discuss the difference between DDK and the Dynamic Batch Loading proposed by Sheared LLaMA[1], which is also 
 proposed to adjust domain proportions for dynamically training smaller models. They also identify discrepancies in loss between smaller and larger models across various domains, and accordingly, they sample more data from domains where the discrepancy is more pronounced. While they concentrate on structural pruning, it is akin to the DDK. Consequently, I perceive the novelty of DDK as being somewhat limited.
2. The results of Qwen 1.5 in Table 1 are not significantly convincing. The MMLU/HumanEval of Qwen 1.5 1.8B in the Qwen official blog are  46.8/20.1 while the authors' report is 44.5/11.9. In addition, compared to the official results, we can see that the DDK fails to improve the model of the students on MMLU. The authors need to check this and provide **more robust results of baselines**.

[1] SHEARED LLAMA: ACCELERATING LANGUAGE MODEL PRE-TRAINING VIA STRUCTURED PRUNING. Xia et al., 2023

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposed a KD strategy for LLMs. Specifically, with assess to the domain-specific performance of both the teacher and student LLMs, DDK uses domain knowledge guided sampling to dynamically update the data mixture. In addition the paper also conducts a statistical analysis of the domain distribution of the datasets involved. The training process is relatively straightforward and easy to generalize. The experimental results also show that DDK's training method improves the performance average of different data sets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A complete training algorithm is designed, and the process is explained clearly. The process of the DDK algorithm is easy to extend to the training process of other models.
2. The authors conducted a comprehensive knowledge distillation experiment on two large model families and a comprehensive ablation study.

Weaknesses:
1. Although the method proposed in this paper is easy to understand and effective, I doubt that the method in this paper is limited to LLMs. In other words, this paper does not mention (or needs to explain) how previous researchers (before LLMs) performed domain-enhanced distillation for domain-biased datasets, and why these previous methods cannot be applied to the distillation of LLMs to achieve similar results. The advantages and novelty of this paper's domain sampling method over previous work that may be transferable to LLMs need further explanation.
2. In the experimental part, there is a lack of key comparison between DDK and other methods that focus on similar domain sampling. The baseline actually involves the work that focuses on domain in KD (cited as [60], etc.), but the subsequent analysis only compares the total average score of DDK and these works, which seems to lack comparison and analysis of similar works. As far as I know, other baselines are more general KDs, and do not focus on domain information.
It is certainly worth noting that DDK performs better than baselines such as MiniLLM, but I think what can better illustrate the effectiveness and novelty of this paper is the comparison with similar domain data sampling, including experimental analysis.
3. In the experimental section, you can add experiments on the dataset and the scale property of the teacher model. This is a possible suggestion.

Limitations:
The method proposed in this paper is very complete and solves the problem well, but there are perhaps two points to note:
1. The distillation method designed in this paper does not seem to be necessarily limited to LLMs. One of the main difficulties of KD on LLM may be that the distribution difference between the teacher and student models is too large, but the key points that this paper focuses on and solves seem to be orthogonal to this point. So it also leads to a similar question: why previous similar methods that focus on domain sampling cannot be migrated to LLMs, and what are the advantages and novelties of this paper's design.
2. Following point 1, in the experiment, what are the specific advantages of this method over the predecessors in the domain problem (not just the overall average, of which result shows DDK outperformed others). This may be what I am very curious about after reading the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xgP5ynlZWf;"REVIEW 
Summary:
For real-world images corrupted by multiple simultaneous degradations, this paper first analyzes the limitations of using all-in-one restoration models and various task-specific models. The authors then introduce RestoreAgent, which automatically identifies the types of degradation in a degraded image, determines the sequence of restoration tasks, and selects suitable models from the model pool. RestoreAgent presents an automated restoration pipeline that requires only an input image and a general human instruction, without any prior knowledge of the involved degradation tasks or manually predefined task sequences.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper comprehensively analyzes the challenges and limitations of employing all-in-one models and multiple task-specific expert models with fixed or random task sequences, as well as fixed or random models for each task.
2. The authors evaluate various configurations of RestoreAgent using diverse objective image quality metrics (PSNR, SSIM, LPIPS, DISTS, and their combinations), all of which outperform the human expert model on the corresponding metric. 
3. RestoreAgent exhibits the scalability by extending to new tasks and models with minimal computational resource. 
4. The presentation, including writing, analysis, and visualization, is clear and easy to follow.

Weaknesses:
1. Incomplete descriptions about data construction. 

- Authors randomly select up to four types of degradation from a degradation set (noise, blur, JPEG, rain, haze, and low-light) to construct paired training data. According to data synthesis strategies in [1,2], JPEG compression is typically performed after noise and blur, and in the final order. Is the degradation order of JPEG compression in this paper the same? If not, the authors should discuss the reasonableness of random sampling.

- What are the components of 23k paired data? One degraded image for each high-quality image or many degraded versions for each high-quality image? 

- What is the configuration in ablation studies about training data amount? Simultaneously scaling up low & high-quality images or synthesizing more low-quality images for each high-quality image? If it’s the former, will increasing the number of degraded images while keeping the number of high-quality images unchanged improve performance?

2. Inference time for input images with diverse resolution.
- The authors are suggested to report the running time for input images of various resolutions. This should include the total time, the running time for the RestoreAgent, and the running time for the subsequent restoration models. The reviewer is curious whether the agent's response time exceeds that of the restoration models when processing high-resolution images, such as those with 4K resolution.

3. Scalability for new tasks and models. 
- Section 4.5 demonstrates that the proposed RestoreAgent can extend to new tasks and models in just half an hour, surpassing human expert-level performance on the new task. However, it is unclear whether adaptation to the new task results in performance degradation on prior tasks, similar to the catastrophic forgetting problem in continual learning. The authors are encouraged to report the performance of the fine-tuned model on the previous tasks to address this concern.

[1] Wang X, Xie L, Dong C, et al. Real-esrgan: Training real-world blind super-resolution with pure synthetic data[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 1905-1914.

[2] Zhang K, Liang J, Van Gool L, et al. Designing a practical degradation model for deep blind image super-resolution[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 4791-4800.

Limitations:
The manuscript includes the checklist guidelines.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new pipeline to address multiple degradation, like noise, blur and low light. Besides, a RestoreAgent with multimodal large language models is introduced to assess the type and extent of degradations in the input images and perform dynamic restorations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and well-organised. 
2. The whole pipeline seems to be novel and reasonable. 
3. The method achieves SOTA performance on several benchmarks and different degradation tasks.

Weaknesses:
The overall motivation of this paper is commendable, but I have a few concerns:

1. The author mentions that RestoreAgent can autonomously assess the type and extent of degradation in input images and perform restoration. This strategy is interesting. However, I am wondering how the order of different enhancement techniques is defined. For example, if the input has noise and rain streaks, how is the order of dehazing and denoising techniques determined? Will this affect performance?

2. In contrast to other image enhancement techniques, the proposed RestoreAgent should first find a suitable restoration task and then select the most appropriate model to enhance the quality of the input. Therefore, I am concerned whether this process will increase the inference time. The authors should provide some computational analysis.

3. The enhancement capabilities of this work rely heavily on existing enhancement frameworks. If existing frameworks cannot work well in some cases, such as extreme noise effects, I guess the proposed RestoreAgent may also fail. Is this true? If so, I suggest the authors mention this in the limitations section.

4. The explanation of ""ranking"" and ""balanced"" in Table 1 is still unclear. The authors should clarify the definitions of these terms.

5. It would be better to show more visual comparisons of the RestoreAgent.

Limitations:
Limitations are not mentioned in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an image restoration pipeline designed to handle various degradation types and levels by leveraging MLLM’s capabilities to select the appropriate model and determine the execution order. It begins with an analysis of why execution order and utilizing multiple models for different degradation levels are crucial for restoring complexly degraded images. The paper then constructs an instruction dataset and fine-tunes the MLLM. Experimental results demonstrate the effectiveness of the proposed restoration pipeline.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1.
This work presents a compelling analysis of complex image restoration. This insight is valuable given that degraded images in real-world scenarios often involve multiple types of degradation.
2.
This approach leverages the strengths of different models for handling specific noise levels, thereby eliminating the trade-off between generalization and performance.
3.
This paper formally defines the problem of handling multiple degradations and model selection in image restoration.
4.
Extensive experiments demonstrate superiority of such pipeline in processing degraded images with multiple degradations.

Weaknesses:
1.
In the introduction, it would be helpful to explain how the Multi-Level Learning Model (MLLM) excels at understanding different types and levels of image degradation. This will show why MLLM is well-suited for handling complex combinations of image degradation. Providing this clarity will make the benefits of using MLLM for image restoration more evident.
2.
When incorporating a new type of degradation, the cost extends beyond merely training the MLLM. Please also discuss the process of constructing training data for the newly added degradation and how it integrates with previously trained data.
3.
In lines 211-212, please clarify what the mean and standard deviation are calculated over. The subscript ""i"" is already used for degradation type and it might be clearer to use another character.

Limitations:
The authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces RestoreAgent, an innovative image restoration system that leverages multimodal large language models to autonomously handle images with multiple types of degradation. The system addresses limitations of existing all-in-one models and fixed task sequences by dynamically adapting to each image's specific degradations. RestoreAgent can identify degradation types, determine appropriate restoration tasks, optimize the execution sequence, select the most suitable models, and execute the restoration process autonomously. The authors present a method for constructing training data and demonstrate that RestoreAgent outperforms existing methods and human experts in handling complex image degradations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper represents a innovation and a good contribution in image restoration and potentially opens up a new research direction for this area.
2. The motivation is strong. The authors effectively demonstrate the importance of task execution order and model selection in multi-task scenarios. The designed system adeptly addresses these issues.
3. Experimental results indicate that RestoreAgent's decision-making capabilities in handling complex degradations surpass those of human experts. This kind of pipeline also surpass all-in-one models.
4. The paper is generally well written and clear to understand.

Weaknesses:
1. The paper constructs a training dataset for training the multimodal large language model and a testing dataset as a benchmark for evaluating performance across multiple tasks. More details and explanations regarding the construction methods of these datasets would be beneficial.
2. Table 1 presents performance rankings using both ordinal and percentage forms. The definitions and explanations for these ranking forms are somewhat lacking, which might require readers to spend extra time understanding them. Clearer explanations would facilitate better comprehension.
3. The proposed Autonomous Restoration Agent represents a novel paradigm that is likely to encounter numerous new challenges. Beyond the issues already mentioned in the paper, the authors could consider discussing additional limitations and future research directions for this paradigm. This would help future researchers better follow and improve upon this work.

Limitations:
See Weaknesses

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xeviQPXTMU;"REVIEW 
Summary:
This paper investigated the problem of watermarking the Federated Graph Learning (FGL) models. This paper proposed the first backdoor-based FGL watermarking framework, called FedGMark. Specifically, to tackle the issues of ineffectiveness and vulnerability of existing methods, FedGMark designed two modules respectively. One is a Customized Watermark Generator (CWG). CWG aimed to generate the watermarked trigger samples (graphs) using each client's secret key. The other is the Robust Model Loader (RML). RML guaranteed that the watermarked models were certifiably robust against layer perturbation attacks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The first attempt to watermark federated graph learning models.
- The watermarked models are certifiably robust against attacks.
- Experiments on various datasets and models validate the effectiveness of FedGMark.

Weaknesses:
My major concerns are as follows.
1. Unclear threat model: The threat model and the problem formulation of this paper is unclear. What's the capability of the adversary and the defender? And more importantly, who is the adversary to steal the FGL model? This paper proposed to watermark the FGL model from the client side, which means the clients should be trustworthy. Is the central server an adversary in this paper? To my best knowledge, the typical threat model of various attacks in FL (e.g., backdoor attacks or Byzantine attacks) assumes that some of the clients may be malicious. The author should add a section on the threat model or problem formulation and clarify why they make these assumptions. This may be helpful to better understand the problem the authors tried to solve.
2. Privacy concern: I also worry that utilizing FedGMark may raise privacy concerns. In Section 3.4, the watermarked client needs to use a subset of its training graphs as the watermarked graphs. However, in FL, the client's graphs are privacy-sensitive, and using them to verify ownership may lead to privacy leakage. This is contrary to the original purpose (preserve privacy) of FL.
3. Missing experiments on the robustness against backdoor defense: This paper considers three different watermark removal attacks. However, since FedGMark utilizes backdoor-based watermarking methods, it is important to validate whether FedGMark is robust against backdoor defenses.
4. Missing introduction to ownership verification: This paper lacks an important section to introduce the ownership verification procedure of FedGMark.

Limitations:
This paper does not include a discussion of the limitations. However, I think there is a strong assumption that the clients need to be trustworthy in FedGMark. A discussion on this assumption is necessitated.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript introduces FedGMark, a backdoor-based watermarking method specifically designed to protect Federated Graph Learning (FedGL) models from illegal copying and model theft. They claim that the proposed FedGMark is the first method to safeguard the intellectual property of FedGL models, offering certified robustness against watermark removal attacks, leveraging unique graph structures and client information to create customized and diverse watermarks. Experiments demonstrate its effectiveness and robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper introduces FedGMark to address the overlooked vulnerability of FedGL model ownership and identifies three main challenges in current watermarking techniques: inapplicability to graph data, vulnerability to removal attacks, and lack of formal guarantees. The proposed method, including CWG and RML, is clear and intuitive, and the authors have provided comprehensive experiments to support their approach.

Weaknesses:
1.	I strongly recommend setting a ""Threat Model"" subsection to clarify the potential security threats to FedGL. In my opinion, since the authors consider watermark removal attacks like distillation and finetuning, FedGL operates under a white-box setting.
2.	The paper assumes attackers know the internal information of the target watermarked model, enabling distillation, finetuning, and layer-perturbation attacks. However, I find the white-box setting narrow and trivial. The authors should consider black-box attacks, which are more challenging and meaningful. Many studies on black-box attacks can be found.
3.	In watermarking-related literature, robustness and fidelity are more frequently used terms than watermark accuracy and task accuracy.
4.	In the ""Inapplicable or Ineffective"" item, the authors state, ""For instance, they require input data to have the same size, while graphs can have varying sizes,"" which is not entirely accurate. For example, some Wavelet and DCT-based watermarking methods can be scalable.

Limitations:
Please refer to Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem of protecting model ownership in the emerging domain of Federated Graph Learning (FedGL) by proposing FedGMark, a backdoor-based watermarking technique. The authors argue that existing watermarking approaches are either inapplicable to graph data or exhibit weaknesses in terms of robustness against removal attacks and lack of formal guarantees. FedGMark aims to overcome these limitations by leveraging graph structure and client information to learn customized watermarks, employing a novel graph learning (GL) architecture that enhances robustness, and providing certified robustness guarantees against layer-perturbation attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper clearly outlines the limitations of existing watermarking techniques and presents a well-motivated approach to address them. The design of FedGMark, with its CWG and RML modules, is tailored to the specific challenges of watermarking in FedGL.
-  FedGMark demonstrates promising empirical performance in terms of both main task accuracy and watermark accuracy. It outperforms the baseline approach (random graph-based watermarking) significantly, especially under watermark removal attacks.
- The paper provides theoretical guarantees for the robustness of FedGMark against layer-perturbation attacks, a unique and valuable contribution in the watermarking literature.

Weaknesses:
1. The reliance on pre-defined private keys for watermark generation may not be practical in all scenarios, and alternative key management methods should be explored.
2. The assumption of limited attacker knowledge about the watermarked model may not hold in practice. Evaluating FedGMark against more knowledgeable adversaries would provide a more realistic assessment.
3. The focus on FedAvg for model aggregation limits the exploration of other aggregation methods and their impact on watermark robustness.

Limitations:
1. FedGMark's evaluation focuses solely on FedAvg for aggregating client models. The impact of alternative aggregation methods (e.g., those prioritizing clients based on data quality or model performance) on both watermark robustness and overall FedGL model performance remains unexplored.
2. The paper acknowledges the increased computational cost of using more submodels (S) in RML but doesn't fully analyze the scalability of FedGMark. Further investigation is needed to understand how performance scales with different numbers of clients.
3. FedGMark relies heavily on structural modifications of the graph as the watermark.  The effectiveness and robustness of alternative trigger designs, such as feature-based triggers, hybrid triggers, or combinations of different trigger types, have not been explored. 
4. The paper lacks specific details about the hyperparameters used for training the GL models on the client-side. The impact of client training dynamics, particularly the choice of learning rate and the number of local epochs, on the watermarking performance and robustness of FedGMark remains unclear and requires further investigation.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work studies watermarking for federated graph learning (FGL) to protect the ownership of participants. It proposes a customized watermark generator for local clients that can capture the local graph structure and private client information, and a robust model loader consisting of multiple GL submodels and a majority-voting-based ensemble classifier, which can defend against the proposed layer-perturbation attack.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work claims to be the first to study watermarking for FGL models.

2. The method can leverage local graph and client information to generate customized watermarks.

3. The paper introduces a layer-perturbation attack to further demonstrate the certifiably robustness of the proposed backdoor-based watermarking for FGL.

4. The work is well-motivated with preliminary studies.

Weaknesses:
1. The concept of ownership in FGL can be confusing and is not well-defined in this paper. For example, can every client claim ownership of the federated trained model? Since the watermarks from different clients are different, can any single client claim entire ownership? Additionally, for clients who participate in the FL but do not have watermarks, how can they claim ownership?

2. The motivation for using local customized watermarks is not clear. The following problems arise: (1) It is unclear how to conduct ownership verification. Should it use the global watermark or the local watermarks? (2) If using a global watermark, what is the necessity of employing customized watermarks, or what is the adequate way to aggregate the global watermark from customized watermarks? If using local watermarks, how can the customized watermarks be used across clients?

3. The method requires specific GL models (to be split to multiple submodels), which can be hard to adapt to existing FGL methods, especially for advanced FGL methods.

4. The motivation for incorporating submodels for GL is missing. Why is this design necessary?

5. (1) What does “layer indexes” for splitting GL models mean? From section 3.3, it is not clear how the submodels are split and how the split submodels are decoupled from each other regarding cascaded structures. (2) Additionally, structural information can be important for graph learning. How would discarding such structural information impact in this setting?

6. The global model is obtained by simply averaging uploaded clients’ models (not weighted by data size, or applying proxy terms for regularization). Can this method address the potential heterogeneity issue when local watermarks are highly disparate from each other?

7. The proposed method can introduce efficiency issues, as it significantly increases the number of parameters and computation time.

Limitations:
Please see Weaknesses above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xeXRhTUmcf;"REVIEW 
Summary:
This paper introduces a new method for Out-of-Distribution detection based on the concepts of Lens Depth and Fermat distance. This method is used to see whether a sample has a similar representation in the penultimate layer of a Neural Network as the samples in the training data. The method is subjected to various tests of Out-of-Distribution detection and is shown to be on-par or exceeding alternative methods. However, the proposed method does not intrude on the training process of the model, and therefore cannot have a negative impact on the classification performance. Alternative methods assume a Gaussian Distribution in the hidden representation, but the use of (a modification of) Lens Depth allows estimating the “similarity” of the sample without assuming a certain distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The application of Femat Distance and Lens Depth introduces mathematical concepts that are not common knowledge and not obvious to a Machine Learning audience. The application of these methods in OoD detection is new (originality)
-  Previous literature is well cited, and the mathematical concepts are clearly and intuitively introduced, with clearly stated relevance (clarity)
The claims made follow naturally from the evidence and are not overstated. The evaluation is in line with common practice in the field of OoD detection (quality)
- The paper is well written and consistently builds a clear argumentation (clarity)
- Mathematical concepts are introduced with both formalism, and an intuitive explanation (clarity).
- The proposed method is competitive with other methods, and is minimally invasive to the training process. This could be helpful when then training process is outside of the control, for example for large pre-trained models (significance)

Weaknesses:
- Small claims are not entirely accurate. Line 4 says there are “no assumptions” about the form of the distribution, but there are only minimal assumptions (see question 3). Line 262 claims that the proposed measure is a good measure of “uncertainty estimation”, but it’s only evaluated for OoD detection, so it may be wildly over/underconfident and behave poorly on aleatoric uncertainty. Line 323 conjects that OoD detection may ensure fairness, but I see no reason why. Line 5 claims that the proposed method is applicable to any classification model, but the performance is only tested for Neural Networks (quality/clarity)
- The explanation of Lens Depth may be made more intuitive with a visualisation to support Lines 94-99 (clarity)
- Presented results are not substantially better than previous methods. Authors argue that the main benefit is that the proposed method is minimally invasive to the training process, but the authors do not make a strong case on why this is necessary (significance)

Limitations:
The authors claim that their method works on all classification models, and without any assumptions on the distribution of the data. However, this is missing evidence. Authors only demonstrate effectiveness in Neural Networks on Computer Vision data. While it is true that the method may be applied to other models and other data, more research is needed to establish its effectiveness there. Other limitations are demonstrated and addressed. The positive conclusions are appropriately based on the findings and are not over-optimistic. 

The authors discuss the high computational cost and demonstrate methods to make it more efficient, but it’s not clear what the remaining computational cost is.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a non-parametric approach to out-of-distribution (OOD) detection. Given a trained neural network classifier, it is proposed to combine the Lens Depth (LD) with the Fermat distance (in an improved form) to capture the geometry and density of the data in feature space. Without assuming any prior distribution, the paper classifies OOD samples for toys and small scale benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The combination of the Lens Depth with the sample Fermat distance for the out-of-distribution problem is a solid and interesting contribution. 
- The paper is well written and easy to follow. In general, the approach is clearly described.
- The results on small scale experiments are convincing. 
- The approach presented does not include the training process of the model.

Weaknesses:
- An extension of the related work to include papers on OOD would be necessary for the content of the paper. 
- An additional evaluation metric would be helpful, e.g. FPR-95, ECE. This point should be addressed. 
- A large-scale evaluation, e.g. ImageNet, is also missing. This is the main limitation of the paper.

Limitations:
The paper has a broader impact statement to discuss the idea of robust decision making.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new method for OOD detection/scoring based on the lens depth and Fermat distance, arguing that it has advantages over prior methods by being non-parametric, non-invasive, (almost) tuning-parameter-free, and quite effective in adapting to the unknown structure of the data to identify OOD points.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Subject matter is important
2. I found the paper really easy and fun to read.
3. 4.2 is a nice, simple, and practical modification—very natural and clearly successful!
4. Both the Lens Depth and Fermat Distance are nice, intuitive notions, and it is natural and fun to think about their combination!
5. I raise a number of conceptual issues below, but at the end of the day the demonstration of the method on standard data sets, comparing it to state-of-the-art methods, is fairly compelling, hence my high score.

Weaknesses:
1. LD is interesting and intuitive but what happens when the data falls into two disjoint clusters? Then won’t LD (with basically any distance I can think of, including Fermat distance) consider points in between those two clusters to be extremely central, despite the fact that, since they lie in neither distribution, they could reasonably be considered very OOD? Related: it seems the FD is infinite (whenever \beta>0) between two points separated by a region of zero density, suggesting that the sample version will be highly unstable in this setting, as it is should not converge at all but instead diverge to infinity. I see this is addressed in 4.4 by computing sample FD separately per cluster, but how were the clusters computed? Clustering is no trivial task, and given that things go wrong without clustering, I imagine S(x) in eq (4.2) depends rather heavily on the clustering. This (seems to me important) aspect of the proposed method seems underexplored/underexplained in the paper.
2. How does the convergence of the sample FD to the population FD depend on dimension? It’s a bit hard to believe it doesn’t suffer from some sort of curse of dimensionality, since it depends on a density and density estimation very much suffers from the curse of dimensionality. It seems many of the nice demonstrations of it in this paper occur in 2 dimensions (with the data lying nearly on a set of dimension 1), which doesn’t seem very representative of NN feature spaces.
3. Claim of “no trainable parameter” in the abstract is rather misleading, given the need for choosing both \alpha (ok there is a case made that maybe this isn’t too important) and the clustering.
4. Lit review is well-organized, but very focused on methods for NN OOD detection. The paper makes a big deal out of the method being non-intrusive, but another way of saying this is just that the proposed method is a way of scoring a point being OOD with respect to a distribution, which is a problem that, in general, has nothing to do with NNs or their feature representations. Surely there is a large body of work on outlier detection in statistics that could be considered in a similar light to this method, where one takes an off-the-shelf outlier detection method’s score and just applies it to the data transformed to be in the feature space of the NN? That is essentially what this paper is doing (though for a novel method, and I am not questioning its novelty). I just wonder what other existing methods are out there that could be doing something similar, even if they haven’t been explicitly applied to NNs.
5. Section 4.5 and Appendix E: choices II and III seem like they would rather seriously break the connection between the estimated LD and the true LD, since the k-means clustering will in general (and in typical circumstances) have clusters with very different numbers of points in them, so by reducing to the cluster centers (or center+’s), you are representing very different numbers of points with different centers. Another way to say it is that the density of the n points via methods II and III is quite different from that of the original N points (or via method I), and hence using them to compute the LD will be quite different in nature from using method I or the original N points. I would expect these methods (II and III) to not even have any kind of consistency property to the true LD of the original points, given their change in the density. 
6. I appreciated the authors’ honesty in reporting LL ratio results as being better than their method (of course, it comes with a more complex process), but it seems worth noting that it is substantially better. Since all the AUROC scores are close to 1, it is natural to look at. 1-AUROC (so smaller is better), in which case the LL ratio gets 0.006 and LD gets 0.029, almost 5x higher. I don’t think the authors were misleading in presenting these results, but I found the two sentences (lines 252-254) highlighting the challenges associated with the LL ratio to be a bit vague, and the results might be more convincing if those challenges were made more explicit (possibly in an appendix if there isn’t room in the main paper).
7. I don’t find Fig 5.2 very convincing, since the monotonicity here is a pretty weak property and no comparison is made with other methods—my guess would be that many methods satisfy monotonicity. Is that not the case?

Limitations:
I guess some of my points listed under “weaknesses” could be interpreted as limitations, and I would like to see them better addressed/discussed. If they are (even if the authors don’t change their method at all), that would raise my score.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors address the problem of out of distribution detection in supervised learning with particular focus on neural networks models. The developed method worj in some feature (embedding) space by measuring the statistical depth of the query point with respect to some reference set of points. The particular implementation combines lens depth function with Fermat distance. The authors validate the proposed approach in a series of experiments on simulated and real-world data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is very well-written and easy to follow.
* The considered problem is relevant for practice as there is a significant demand in efficient and non-intrusive methods for uncertainty quantification. 
* The proposed approach is solid with all the steps being properly motivated.
* The authors did a significant effort to do a comprehensive literature review, experimental evaluation and analysis, though all the steps were not fully successful (see Weaknesses and Questions below).

[After rebuttal comment] I appreciate the answer by the authors and increase my score to 6. My main concerns were addressed.

Weaknesses:
* While usage of statistical depth functions and distribution/manifold related distances looks logical, it is not clear why the particular choices of Lens Depth and Fermat distance were made.

* The baselines considered are not comprehensive enough and some of the baselines are not interpreted correctly by the authors of the present paper. In particular:
a. Non-Gaussianity of embedding distribution was directly considered in [1] aiming to improve over GDA. I think that is worth comparing with this method as the present paper target the same issue though with the completely different approach.
b. I believe that the authors incorrectly say that the difference between papers [2] and [3] is only in usage of spectral normalization. In my opinion, even more important is that [2] uses Mahalanobis distance as uncertainty measure while [3] considers the density of Gaussian mixture instead.

* The experiments are done with relatively simple datasets like CIFAR-10 for in-distribution data and SVHN/CIFAR-100/TinyImageNet as OOD. With the proposed approach being relatively lightweight, it is not clear why not to consider CIFAR-100/ImageNet as in-distribution with corresponding OOD choices (like ImageNet-R or ImageNet-O as OOD for ImageNet).

References 
[1] Kotelevskii, Nikita, et al. Nonparametric uncertainty quantification for single deterministic neural network. Advances in Neural Information Processing Systems 35 (2022): 36308-36323.
[2] K. Lee, K. Lee, H. Lee, and J. Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.
[3] J. Mukhoti, 362 A. Kirsch, J. van Amersfoort, P. H. Torr, and Y. Gal. Deep deterministic uncertainty: A new simple baseline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24384–24394, 2023.

Limitations:
Limitations are adequately addressed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xcqSOfHt4g;"REVIEW 
Summary:
This paper proposes a new framework for masked diffusion models for generative modeling of discrete data. Masked diffusion models offer an alternative to autoregressive models for discrete data but have faced challenges due to complex formulations and unclear relationships between different approaches. This paper presents a simplified and generalized framework to address these issues, enhancing the performance and training of masked diffusion models.

The key contributions includes:

1. Simplification of Model Formulation: The paper establishes properties for the forward process and its time reversal using elementary arguments, provides a simple expression for the Evidence Lower Bound (ELBO), demonstrating it as a weighted integral over time of cross-entropy losses, and shows invariance properties similar to continuous space diffusions.

2. Re-derivation of Training Objectives: The paper demonstrates how various previously proposed discrete diffusion training objectives can be derived from the ELBO objective by altering parameterization, relaxing constraints, or modifying loss weighting.

3. Performance Improvements: The paper demonstrates state-of-the-art likelihood and zero-shot transfer results on text and image tasks using the proposed ELBO objective.

4. Generalized Masked Diffusion Model: The paper proposes a generalized masked diffusion model that allows state-dependent masking schedules, further improving predictive performance on test likelihoods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper makes a notable contribution to the field of generative modeling for discrete data by introducing a simplified and generalized framework for masked diffusion models.
2. The quality of the paper is reflected in the thoroughness of its methodology and the robustness of its experimental validation.
3. The paper is well-written and clearly structured, making it accessible to both experts and those new to the field.
4. The significance of the paper lies in its potential to impact a wide range of applications in generative modeling for discrete data.

Weaknesses:
1. While the paper provides a robust theoretical foundation, there could be more emphasis on practical applicability. The paper could benefit from additional practical guidelines for implementing the proposed framework, such as more detailed pseudocode and specific implementation challenges.
2. The experimental results presented are strong, but the range of tasks and datasets could be expanded, such as VQ-Diffusion [1] for token-based text-to-image.
3. I am unfamiliar with diffusion models for text generation. For image generation, the paper has reported likelihood results, missing some other common metrics, such as FID and IS.

[1] Vector Quantized Diffusion Model for Text-to-Image Synthesis

Limitations:
The authors have adequately described the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper simplifies the mathematical formula for the absorbing state diffusion process. By doing so, the authors derive a continuous-time ELBO for masked diffusion models. Their method, MD4, achieves better perplexity scores than SEDD on text8 and zero-shot perplexity on numerous datasets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
Simplifies the complex mathematical formulations for the absorbing state diffusion for D3PM.

Weaknesses:
Weaknesses:

1. Weak empirical results
    1. The zero shot numbers for D3PM in Table 1 look fishy.  There are only 2 differences between Md4 and Absorbing State D3PM:
        1. Mathematical simplification. In the discrete case (Eqn. 6), even though MD4 features a Simplified functional form for the ELBO, it shouldn't give it any performance benefits in terms of perplexity since it is mathematically equivalent to D3PM.
        2. The improvement in ELBO could be because of the continuous time formulation. However, VDM [1]  has shown that for gaussian diffusion, improvement from discrete (T=1000) to continuous time (T = $\infty$) barely improves the likelihood by less than 1%. For this reason, I request the authors to perform eval on an already trained model and report the  perplexity numbers on text8 or OWT using Eqn (6) with T=100, 1000, 10000. If the numbers reported for D3PM in Table 1 are indeed correct, and if the entire improvement is coming from the continuous time formulation, then the discrete time MD4 should get a number that's comparable to D3PM's zero shot ppl numbers. 
        
        Questions: How did they retrain D3PM? Did they use the same transformer backbone as MD4? Did they use the same model size and data pre-processing scheme? Did they use  uniform state or absorbing state diffusion process? The authors need to clarify this.
        
    2. CIFAR10 Experiments. The AR baselines use old transformer models hence the comparison isn't quite fair. Current SOTA diffusion models on Imagenet 32 achieve a NLL of 2.55 [2] which is far better than the absorbing state diffusion models. So, I'm unsure about the takeaway from Table 3. In the conclusion section, the authors claim that ""… on text and image data, the resulting masked diffusions outperform existing discrete and continuous diffusion models …"" which is factually incorrect given that their method largely underperforms against gaussian diffusion [1, 2].
2. Limited evaluation of GenMD4. The authors mention that GenMD4 performs poorly on zero-shot tasks. I request the authors to quantify this poor performance by providing 
    1. Validation ppl numbers on OWT
    2. zero-shot ppl numbers.

[1] Kingma, D., Salimans, T., Poole, B. and Ho, J., 2021. Variational diffusion models. *Advances in neural information processing systems*, *34*, pp.21696-21707.

[2] Sahoo, S., Gokaslan, A., Sa, C., Kuleshov, V., 2024.  Diffusion Models With Learned Adaptive Noise. arXiv:2406.07524

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a streamlined and generalized framework for masked diffusion models, addressing the complexities and inefficiencies of existing models, including those based on Score Entropy Discrete Diffusion (SEDD). It introduces a continuous-time variational objective for masked diffusion models, simplifying the evidence lower bound (ELBO) to a weighted integral of cross-entropy losses. Additionally, the paper presents state-dependent masking schedules, enhancing the flexibility and performance of these models. The proposed methods demonstrate state-of-the-art results in text and image tasks, significantly improving likelihood and zero-shot transfer performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper offers a novel theoretical formulation of the continuous-time variational objective for masked diffusion models, simplifying the training process and ensuring consistency between forward and reverse processes.
- The introduction of state-dependent masking schedules provides a more adaptable approach, catering to the specific characteristics of the data and improving model performance.
- The proposed methods achieve state-of-the-art performance in both text and image generative tasks, significantly enhancing likelihood and zero-shot transfer capabilities.
- By reducing the ELBO to a weighted integral of cross-entropy losses, the paper makes the training and understanding of masked diffusion models more accessible and potentially more stable.
- The paper includes comprehensive experimental validation on various datasets, demonstrating the robustness and superiority of the proposed methods.

Weaknesses:
-  Despite the theoretical simplifications, the practical implementation of state-dependent masking schedules can still be complex and computationally demanding. Specifically, obtaining the starting x_T is challenging, and since the sampling process lacks stochasticity, sampling cannot be done from the completely masked state.
- The state-dependent models have a tendency to overfit to dataset statistics, which can limit their effectiveness in zero-shot transfer tasks.
- While the paper demonstrates superior performance, a more detailed comparative analysis with other state-of-the-art methods, particularly regarding computational efficiency and training times, would provide a clearer picture of the advantages.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Summary: This paper introduces a framework for masked diffusions that consolidates previous research on the topic and organizes it into a cohesive structure. The authors also present a generalized model within this framework, which enables the use of state-dependent masking schedules and optimization of scheduler parameters.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The GenMD4 framework offers a valuable approach to optimize the forward process. In earlier studies, forward processes were typically manually designed and set within the model. However, GenMD4 adjusts the forward distribution to align with the estimated distribution, thereby improving the forward process. This innovation may serve as a source of inspiration for developing more effective forward processes.

2. This paper summarizes previous formulations of masked diffusion models and establishes the connections between them.

Weaknesses:
1. In line 90. The handling of $p(x _0|x _{t(1)})$ could be enhanced. Assuming $p(x _0|x _{t(1)}) \propto q(x _{t(1)} | x _0)$ is equivalent to assuming that $q(x _0)$ is uniformly distributed. In reality, it should be treated the same as other $p(x _s|x _t)$.
2. In line 114. When discussing multidimensional data, it is not straightforward to assume that the backward process factorizes across tokens. This is because the distribution $p(x _0)$ does not factorize across tokens. Achieving factorization necessitates a small time step dt, which may not be easily observable. Additionally, in the previous single-token scenario, dt dose not need to be small, indicating that one step is sufficient to model the distribution $p(x _0 | x _1)$. This aspect is crucial for multidimensional data and should be emphasized in a fundamental paper like this.
3. In append F. The presence of a non-zero $\alpha _1$ may result in the ""medium brightness problem"" [1]. However, there is no singularity when $\alpha _1$ is zero if log-SNR is not introduced, and the time interval can be extended to [0, 1].
4. In append G2. When applied to masked diffusion, $R_{kj}$ is zero when $ j \ne k$ and $j \ne m$. Given that $R_{kk} + R_{km} = 0$, $\tilde{q}$ can only take on one value (m), resulting in no additional variance.
5. In image experiments, MD4 employs masked noise, while $\tau$LDR uses Gaussian noise. We recommend conducting experiments with the same noise scheduler to demonstrate conclusively that MD4 is superior. If the goal of this paper is solely to establish that masked noise outperforms Gaussian noise, we recommend explicitly stating this claim. Additionally, we advise detailing the sampling method, as variations in methodology can influence the quality of generated samples.

[1] Common Diffusion Noise Schedules and Sample Steps are Flawed, Lin et al., 2024

Limitations:
The method is only applied to masked diffusions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xcF2VbyZts;"REVIEW 
Summary:
Paper proposes a pipeline method of orchestration pre-trained foundation models to solve the social relationship classification problem. It uses vision models to extract information in text about the scene in the form of caption. Relevant information, i.e. age, gender, general description, of individual persons and objects are also extracted in text form via instance segmentation + masking + captioning. The generated text are then further converted to Social Story with a LLM. With the novel prompt engineering method, GSPO, another LLM will then generate the social relationship from the Social Story.

Experimental results on the challenging benchmarks, PIPA and PISC, indicates its strong performance with zero-shot setup. Extensive ablation studies were also done to evaluate the contributions of the various components. In particular, it clearly showed the merits of the ""Social Story"" design.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Paper proposed a novel method to solve the challenging social relationship classification problem. The proposed method cleverly combine several state-of-the-art foundation models in a logical, intuitive, and yet non-obvious design to achieve state-of-the-arts experimental results.

Weaknesses:
1. Besides the clever design of the pipeline, the direct technical contributions is slightly on the weaker side as there is no obvious technical breakthrough. The proposed GSPO appears to be the main new technique introduced. However, I am not an expert in this area and will defer to other reviewers on its technical novelty and merits.

2. (minor) The use of the generic semantic segmentation model (SAM) may not be the optimal choice. There are much stronger Human Instance Segmentation methods which can replace the paper's custom SAM method. Such methods are specifically trained on person dataset to handle various challenging scenarios unique to human segmentation, e.g. heavy occulsion, human-like objects (e.g. maniquinn).

Ling, E., Huang, D., & Hur, M. (2022). Humans need not label more humans: Occlusion copy & paste for occluded human instance segmentation. BMVC.

Limitations:
(minor) There may be some unintended negative consequences of automatic classification of social relationship.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces SocialGPT, a modular framework for social relation reasoning that integrates the perception capabilities of Vision Foundation Models (VFMs) with the reasoning capabilities of Large Language Models (LLMs). To optimize prompts, the authors propose GSPO, a segment-based optimization algorithm for automated prompt tuning. Extensive empirical results validate the effectiveness of SocialGPT both quantitatively and qualitatively. GSPO consistently enhances SocialGPT's performance across various LLMs, and case studies demonstrate the framework's generalizability and interpretability.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-organized, with a logical flow and clear explanations of each step.
- The proposed SocialGPT framework innovatively combines perception from VFMs with reasoning from LLMs, achieving competitive zero-shot performance and offering potential explanations for its reasoning process.
- Extensive experiments, ablation studies, and case studies comprehensively evaluate the framework's effectiveness.

Weaknesses:
Section 3.2 mentions that using precise coordinates can pose challenges for LLM numerical reasoning. However, it appears in Figure 3 that the objects' positional relations in the social story are inferred from numeric coordinates provided in the dense captions with symbols. Does this coordinate-based inference lead to similar numerical reasoning challenges? Additionally, how are relative positional relations conveyed here using referral symbols?

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a framework called SocialGPT for social relation reasoning, which combines vision foundation models and large language models. A greeedy segment prompt optimization methods is also proposed to prompt LLM. Experimental results show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
---The paper is well organized and written. 

---The idea of combining VFMs and LLMs is reasonable.

Weaknesses:
--- The paradigm of using VLMs for perceiving and LLMs for reasoning is currently a common solution for multimodal tasks. The main difference of this paper seems to be the use of a generated social story as the representation of visual content. As stated by the authors, LLMs perform best when working with human-readable natural language and often struggle with arithmetic reasoning tasks, which is why they design an additional process to generate social stories. However, the generation of social stories is also done by LLMs, which also suffer from the above difficulties. 

--- The authors propose a candidate set consisting of alternative prompts for each segment and select the best-performing prompt from their combination. The final prompt is obtained by selection rather than generation, which limits the upper bound of the performance on the manually collected candidate set. 

--- The function of SAM is to distinguish individuals in the image and obtain their coordinates. However, in the social story generation phase, the LLM (Large Language Model) discards the coordinates, retaining only the semantic information and losing the positional information. Conducting social relationship reasoning purely based on semantics may be insufficient. For example, in Figure 2, the social relationship is identified as a sibling relationship (brother and sister), but there are two boys in the image, both fitting the given description of ""stands out in his vibrant red and green striped pajamas,"" making it unclear which individual P1 refers to.

Limitations:
Please see the weakness and limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This manuscript introduces SocialGPT, a modular framework designed to enhance social relation reasoning by combining Vision Foundation Models (VFMs) and Large Language Models (LLMs). SocialGPT utilizes VFMs to convert image content into a textual social story, followed by LLMs performing text-based reasoning. The paper further introduces the Greedy Segment Prompt Optimization (GSPO) algorithm to optimize prompts for LLMs, addressing the challenges of long prompt optimization. The proposed method achieves competitive zero-shot results on social relation recognition tasks and offers interpretable answers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The GSPO algorithm provides an efficient method for optimizing long prompts, significantly improving the performance of LLMs in social relation reasoning tasks.
- SocialGPT achieves competitive zero-shot results on PIPA and PISC datasets, demonstrating the effectiveness of the proposed approach without additional model training.
- By leveraging LLMs for reasoning, SocialGPT can generate language-based explanations for its decisions, enhancing the interpretability of the results.

Weaknesses:
- The approach involves substantial computational resources for both the perception and reasoning phases, potentially limiting accessibility and scalability for some users.
- The experiments, while promising, are primarily conducted on two datasets. Further testing on a broader range of datasets and tasks would strengthen the generalizability of the findings.
- The method assumes that the visual context provided by VFMs is sufficiently detailed and accurate, which might not always hold true in diverse real-world scenarios.
- The compatibility of the proposed method seems to be limited; Table 5 implies that LLaMA2-based SocialGPT performs very poorly compared to Vicuna. The proposed framework may work only for specific types of models.

Limitations:
See the weakness section and question above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xbuaSTqAEz;"REVIEW 
Summary:
This paper incorporates a multi-model subspace proxy learning (Multi-Sub) to design a novel end-to-end multiple clustering approach and utilizes the synergistic capabilities of CLIP and GPT-4 to align textual prompts expressing user preferences with corresponding visual representations. The main contributions of Multi-Sub can be summarized as follows:
1. Capturing user’s clustering interest: Existing works struggle to adapt to diverse user-specific needs in data grouping. To overcome this limitation, Multi-Sub explicitly captures a user’s clustering interest by learning the desired clustering proxy under a user’s interest and aligning textual interest with corresponding visual features.
2. Simultaneous optimized framework: The previous methods separated the representation learning and clustering stages. Different from them, Multi-Sub obtains both the desired representations and clustering simultaneously, which significantly improve the clustering performance and efficiency.
3. Extensive experimental validation: Extensive experiments on all public multiple clustering tasks demonstrate that Multi-Sub outperform other methods. Moreover, a series of ablation studies further verify the effectiveness of Multi-Sub.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. In real world, data may have multiple aspects that they can be grouped into different clusters. However, existing methods solely consider a single partition. So, it is meaningful to propose an effective algorithm to overcome this problem.
2. The authors leveraged large language models (LLMs), including GPT-4 and CLIP, to align image and textual representations in the same subspace. Then, multi-modal subspace proxy learning is introduced to allow for the customized representation of data in terms specific to the user’s interests.
3. Experimental results on public datasets show that the Multi-Sub method has a significant improvement, indicating the effectiveness of the propose method.

Weaknesses:
1. To change the two-stage learning approach of previous works, Multi-Sub aims to learn representation and clustering simultaneously. However, Multi-Sub employs a two-phase iterative approach to align and cluster images in training process, including (1) Phase I: Learning and Alignment; (2) Phase II: Clustering and Optimization. I wonder if this is another form of two-stage task.
2. The description of Clustering Loss is not very clear in Section 3.4, how to determine that samples belong to the same class? By pseudo-label? Where did the pseudo-label come from?
3. In this paper, the authors introduced large language models (LLMs) to learn representations and bridge the gap of textual and image features. But does the direct use of a pre-trained large language model introduce a priori information about the category, which can lead to unsupervised scenarios being corrupted?

Limitations:
The authors have discussed social impacts and limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an innovative approach for addressing the limitations of existing multiple clustering methods. By leveraging the synergistic capabilities of CLIP and GPT-4, Multi-Sub aligns textual prompts with visual representations to cater to diverse user-specific clustering needs. This method introduces a novel multi-modal subspace proxy learning framework, which automatically generates proxy words from large language models to represent data in terms specific to user interests. The experimental results demonstrate that Multi-Sub consistently outperforms existing baselines across various datasets. Overall, I believe this paper makes a substantial contribution to the field of deep clustering and holds significant practical application value.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper offers several notable strengths that contribute to its overall impact and significance in the field of multiple clustering: 
1.	The integration of CLIP and GPT-4 for multi-modal subspace proxy learning is novel and effectively addresses the limitations of traditional multiple clustering methods.
2.	Multi-Sub excels in capturing and responding to diverse user interests, providing tailored clustering results without requiring extensive manual interpretation. Moreover, the performance gains come at a low cost and seem relatively easy to achieve. 
3.	The writing is clear and easy to follow. The figures are well-drawn, allowing for a quick understanding of the research motivation and methodological design.
4.	Extensive experiments on a wide range of publicly available datasets demonstrate the robustness and generalizability of the proposed method.

Weaknesses:
Despite its strengths, there are some areas where the paper could be improved to enhance its clarity and applicability:
1. Although the paper mentions the hyperparameters used, a more detailed analysis and discussion on the sensitivity of the method to these parameters would be beneficial.
2. Given the method's iterative nature and the use of large models, there is a risk of overfitting, especially on smaller datasets. I am curious whether regularization techniques were used to address this issue?
3. Table 3 compares the impact of different text encoders on performance. Clearly, there are significant performance differences when using different encoders, and the authors have indeed analyzed this issue. However, I believe the reasons behind this phenomenon could be explored in depth. Intuitively, given that the input text is quite simple, the overall performance should not be particularly sensitive to the choice of text encoder.

Limitations:
The limitations are discussed in the paper by the authors. There is no potential negative societal impact

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper is about Multiple Clustering, which is an interesting topic. The authors propose a novel end-to-end multiple clustering approach that incorporates a multi-modal subspace proxy learning framework. The paper is well written and well organized. However, there are several concerns in the current version of the paper that addressing them will increase the quality of this paper.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1 The authors' idea of using large models to aid clustering is novel.
2 The paper is clearly structured and easy to understand.
3 The paper has sufficient experiments to support its point of view.

Weaknesses:
1 The authors point out that different clustering results can be given for different customization needs of users. Then it will bring several associations (not necessarily accurate): a. What should be done if the user's demand is exactly opposite to the potential clustering distribution? b. The experiments do give different clustering results for different demand types, if the user proposes a new type of demand, can the model also adaptively adjust?
2 Figure 2 is well drawn but could be further improved, some icons and fonts need to be adjusted.
3 The authors point out that their model is capable of outputting clustering results directly, and then there should be a corresponding formula to represent this. In addition, it is hoped that the authors will discuss further why, if it is not a difficult task to output clustering results directly, few previous methods have done so.
4 Authors should add details about the dataset, such as data size, feature types, etc.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an end-to-end multi-clustering method that integrates a multimodal subspace proxy learning framework. It combines text prompts expressing user preferences with corresponding visual representations to achieve clustering based on user interests.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The clustering task, driven by user interests, aligns better with user preferences and is more applicable to real-world scenarios.
2.The experimental results are promising, and the methodology is clear and logical.

Weaknesses:
1.The contributions of the paper are not very clear. At first glance, it appears to merely combine CLIP and GPT, lacking innovative architecture.
2.The baseline methods chosen for comparison are neither cited nor introduced.
3.Section 5 discusses only limitations, lacking a discussion on broader impacts.

Limitations:
The authors only address the limitations of their work and do not discuss broader impacts.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xavWvnJTST;"REVIEW 
Summary:
The authors explore the relationship between feedback control and learning with recurrent neural networks (RNN). Specifically, they enforce a control signal onto a RNN that is used to generate a trajectory for a outreaching task, and then propose to use local learning rules on the neurons in the RNN. They show that with feedback control the network can adapt faster to perturbations, of the task and show that the local (in time) gradients are better aligned with the global ones.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The claims are all very reasonable and well illustrated. I this is the first time such feedback-based learning used in proper control settings, which is surprising given that it is based on control theory.

Weaknesses:
Main problem:
My main concern is that I the task chosen consists on bringing a system to a desired static target, so it is possible that there is no ""out of equilibrium dynamics"", rather the learning simply consists on bringing the ""arm"" to the required target and it just so happens that the shortest trajectory aligns with the velocity profile. While it could be that the trajectory is indeed learned (and with some implicit or explicit regularization it should be the case), the current task is not conclusive. If the point is to really learn a trajectory, the authors should have picked a task where the trajectory is a bit more complex than going to equilibrium. Maybe a limit cycle? Otherwise the work might be a minor modification of Meulemans et al.
Also, I fail to see the ""biological circuits"". If we are talking about recurrent neural networks, this is fine, but usually when we talk about circuits in biology we would refer to cell types (and this has a lot of constraints). In fact the authors themselves state that they are agnostic to the biological implementation, which is hardly in compatible with the title. I would replace it by recurrent neural networks.

Other issues:
- The key findings are not clear in the introduction. The term ""inference learning"" is only used there and in one of the figure, but it is not clearly defined. If the authors mean that feedback control can train an RNN then this has already been shown. For the second finding, ""increased accuracy of approximate local learning rules"" it would be better stated as increased accuracy WITH local learning rules (or something similar). For the third, the second order gradient is not really injected (this would suggest that the gradient is imposed on purpose); rather, the feedback control is implicitly related to second order optimization methods.
- Line 142: it seems natural that if the network is perturbed from its trajectory the feedback would be stronger to compensate for the perturbation. I don't see why this is ""suggested"". Also, the sentence is badly written ""suggest that the during task... activity is increasingly by feedback"").
- LInes 164 and 165. The authors say that "" using a local learning rule without feedback control show an increasing performance gap compared to those trained with feedback control and BPTT"". The sentence could be interpreted as if the network is trained with feedback control AND BPTT (combined). A better wording would replace AND by OR 
- In 3.4 it is a bit hard to follow. It seems as if the authors are using an eligibility trace to train the RNN through BPTT. But this intermediate step might not be real BPTT as it is commonly used. 


Literature issues:
- The work of Meulemans et al 2022b is credited with alleviating the temporal storage required by BPTT. While they did do that (and it is a good paper), I think that they based the memory load decrease on previous work (Bai et al., Deep Equilibrium models 2019), which if memory serves does use BP. The logic of my comment is that by training the equlibrium point of the network one can avoid the memory load, regardless of the training method.
- The connection between feedback-based learning and second order optimization has been is very closely related to Meulemans, et al. ""A theoretical framework for target propagation"" 2020. That paper mentions target propagation, but it is very similar to feedback based learning (as the authors probably can infer).
- This is a personal opinion, the authors do not need to take it into consideration: The biological plausibility claims seem to rely on the locality of the learning rules. While it's a requirement that learning rules should not break the laws of physics (or in this case basic biological knowledge), learning rules should at least have some basis on biology, which I am missing here. A brief mention of why would one think that the learning rules are close to biological ones would be welcome. My guess for this feedback-based work would be something with a temporal component such as  temporal hebbian rules (ex: Aceituno et al. ""Learning cortical hierarchies with temporal Hebbian updates."" 2020)

Limitations:
They did mention some limitations, but the key issue of whether this is general motor control (or shaping attractors) is not addressed.
Also, if the paper is about how feedback control guides credit assignment on biological circuits, being agnostic to the biological circuit is a problem, rather than a strength. To make a biological statement there should be some non trivial biological predictions, or some mention of what exactly does this bring to neuroscience that wasn't already known.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Feedback controllers are ubiquitous in neuroscience but their functions are not fully understood. This paper studies how feedback control interplays with biologically plausible online learning on a standard motor control task. The authors show that:
- feedback control enables to adapt to task variations without any plasticity, by approximating the gradient of the loss with respect to the hidden states.
- it makes tractable approximations to RTRL more reliable by shrinking the recurrent Jacobian eigenvalues.
- it incorporates some second-order information in the weight updates, leading to faster learning.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper studies an important understudied question, that is the interplay between feedback and learning.
The paper is overall well-written and is easy to read. The message is clearly delivered.
The experiments are carefully designed and well executed, and support the claims of the paper.

Overall, the paper will be an insightful read to the community.

Weaknesses:
While the experiments are overall well executed, there are a few points that should be improved to make the paper's claims more robust:
- In the appendix, it is written that the learning rate is taken to be constant. To make claims about e.g. learning speed, the optimizer, in particular its learning rate, has to be tuned.
- Figure 5b: it is not clear from this experiment that RFLO-c contains some second-order information. The alignment with the 2nd-order gradient result is not convincing as the estimated gradient is more aligned to the first-order gradient than to the second-order one. This experiment needs to be improved for it to support its claim. The BPTT-c baseline that I mention below may be a good starting point for further analysis as the gradient of a ""controlled loss"" (which is not the case for RFLO).
- A BPTT-c/RTRL-c baseline would be an interesting add to disambiguate between the role of feedback control and approximate gradient estimation through RFLO. This baseline would include feedback control in the recurrent neural network dynamics and optimize for the MSE loss at the output. This would be useful in e.g. Fig3b and Fig5b.

Limitations:
The paper is theoretical and its limitations have been properly addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Recent work has shown that feedback signals can be critical to rapid adaptation in control tasks, and may explain how biological intelligence can make rapid adjustments when solving such tasks. This paper studies how feedback control achieves this. To do so, the authors train an RNN enhanced with feedback control on a common control task, and study how the feedback signal lead the network to achieve more rapid adjustments when perturbations are introduced. The 3 main findings are that the feedback signals align well with the optimal global gradient of the error, that they help the network better weigh current information (vs. less relevant past information) during perturbations, and that they indirectly inject second-order information to the RNN.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- This work focuses on improving the theoretical understanding of an important method. Given that our understanding of many deep learning methods are woefully inadequate, such work is critically important for the field's development.
- The method and the results are clearly presented, the figures are excellent, and the writing is easy to follow.

Weaknesses:
I am not familiar with feedback control and motor tasks; hence, I ask the AC to please take this into consideration to appropriately weigh my review. My remarks on the methods could be wrong or trivial. That said, I'll do my best to provide feedback.

- Several sections of the paper seem to just present results from previous work, including section 3.1 and the entirety of the methods section. This makes the contributions of this paper seem rather thin.

- I may be missing something, but some of the results seem minimally surprising. For example, in section 3.2, the authors state ""...the feedback contribution to the overall network output increases during perturbation."" But how could it not increase during perturbation? Isn't the network explicitly trained to use the feedback information to make corrections during perturbation? The same goes for the alignment between the feedback signal and the optimal global gradient, and the indirect introduction of second-order information-- is it not by design that the network use feedback to make corrections, and thus the larger the correction needed (i.e. the larger the optimal gradient) the larger the feedback signal? And is it not by design that second-order information gets introduced via the recurrent connections that enables the network to ""save"" information from previous timesteps in the hidden state?

- The authors claim that feedback control guides credit assignment in biological circuits, but uses BPTT during the pretraining phase of the RNN, which they acknowledge is not biologically plausible. 
It seems to me that backprop is still doing much of the heavy lifting in terms of solving credit assignment, thus I'm not sure this claim is sufficiently justifiable. A more defensible claim given the current results may be that feedback control may guide motor adaptation in biological circuits.
Similarly, some parts of the intro and abstract strongly suggest that the presented method would perform credit assignment without suffering from the biological implausibilities of backpropagation (e.g. the abstract sets up the problem as ""backpropagation is known to perform accurate credit assignment of error, how a similarly powerful process can be realized within the constraints of biological circuits remains largely unclear""), yet the actual method relies heavily on backpropagation.

- The experiments are performed on a single task, using a small single layer RNN with 400 hidden units, and therefore it's unclear whether the findings would scale to other tasks and larger architectures. Given that the primary goal of this paper is to improve understanding of an existing learning algorithm, and most of the analysis are performed via empirical testing, I believe it's important for the authors to demonstrate that their conclusions are robust over a wider range of tasks and hyperparameters/architectures.

Limitations:
I'd like to see an expanded discussion in the limitations section regarding the remaining aspects of the feedback-enhanced RNN that remain biologically implausible. Particularly, the usage of BPTT in a work that aims to explain how biological credit assignment is performed is quite troubling for me, given its significant biological implausibility. Ideally, I think the authors should show that their results hold on a network trained using a biologically-plausible learning rule enhanced with feedback control.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the effect of feedback control on motor learning in recurrent neural networks, finding that feedback control improves learning performance and better aligns with the true gradient w.r.t. the task.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Alignment with the true gradient is an interesting result and helps explain why feedback works
- The authors study alignment from different perspectives (e.g. step-wise/full gradients, Newton method)
- The task the authors consider is widely used in monkey experiments, therefore it should be possible to adapt the conclusions to real data or use them to guide new experiments

Weaknesses:
- The training setup is rather limited; it would be interesting to see training done for other tasks and architectures (or RNN sizes).
- The paper might benefit from some theoretical analysis of why the feedback signal alings with the true gradient, although it’s not clear if that can be easily done.

Limitations:
The authors have addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xaqPAkJnAS;"REVIEW 
Summary:
- This paper presents an information theory approach to obtain a single graph fused from a multiplex graph, which preserves 
   - sufficient task-relevant information 
   - while removing task-irrelevant noise. 
- A learnable graph augmentation strategy is also developed. 
   - The learned graph and representation can be applied to different types of tasks. 
- The effectiveness is supported by extensive experimental results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is well-motivated. 
   - The authors find that each graph contains much unique task-relevant information, which is ignored by mainstream contrastive learning-based methods.
- This paper develops multiple graphs non-redundancy principle, which lays the foundation for multiplex graph data process. 
   - Two random and generative graph augmentation strategies are accordingly built to capture view-unique task information.
- The experimental results are promising. 
   - The framework demonstrates a clear advantage over existing methods, including advanced supervised approaches, highlighting its potential for broad application.
- This paper provides the code and all experimental settings for reproducing the results.

Weaknesses:
- The difference between the existing non-redundancy principle and multiplex graph non-redundancy is unclear. Please clarify it.
- The proposed InfoMGF-LA runs out-of-memory on MAG data. The reason should be given.
- It is possible that the proposed method cannot handle real-world large-scale graph. It should be addressed in the future and discussed in the conclusion part.
- The difference between the proposed method and DGM is unclear.

Limitations:
Some imitations are addressed in $\S5$.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces InfoMGF (Information-aware Unsupervised Multiplex Graph Fusion), a novel framework aimed at addressing the issue of graph structure reliability in Multiplex Graphs. The primary goal is to refine graph structures to eliminate noise and maximize task-relevant information. Theoretical analysis and comprehensive experimental results validate its effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	Originality: The paper addresses a critical gap in Unsupervised Multiplex Graph Learning (UMGL) by focusing on the reliability of graph structures, which is often overlooked in existing research.
2.	Quality: The proposed InfoMGF framework effectively refines graph structures to eliminate noise and maximizes both view-shared and view-unique task-relevant information. Theoretical analyses provided in the paper validate the effectiveness of InfoMGF in capturing task-relevant information and improving graph fusion. Extensive experiments demonstrate that InfoMGF outperforms various baselines and even sophisticated supervised approaches in different downstream tasks.
3.	Clarity: The paper is generally clearly written and well organized.

Weaknesses:
1.	Scalability: The framework involves several steps. Though the paper provides the complexity analysis in Appendix for each step, it is still unclear what is the overall complexity.
2.	Reproducibility: The authors share the code for reproducibility. However, I didn’t see the datasets.
3.	Accuracy: The authors should check for the few grammatical and spelling errors that occur in the text.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces InfoMGF, an innovative framework for Unsupervised Multiplex Graph Learning (UMGL) that addresses the often-overlooked issue of graph structure reliability. InfoMGF refines graph structures by removing task-irrelevant noise and maximizing task-relevant information through mutual information maximization. Extensive experiments demonstrate its superior performance over various baselines and even some supervised methods, validating its effectiveness in enhancing node representation learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- New Problem Formulation: The paper pioneers the investigation of graph structure reliability in multiplex graphs, which is a significant advancement in the field. Multiplex graphs enrich the representation of real-world systems and its analysis is very difficult inherently.
- Theoretical Analysis: The several theorems are quite interesting and provide a solid foundation for the proposed method. In particular, Theorem 3 proves the necessity of fusing multiplex graphs.
- Extensive Evaluation: The framework is thoroughly tested against various state-of-the-art methods on both node clustering and classification tasks, showcasing its robustness and effectiveness across different tasks. The comparison methods are representative and new.

Weaknesses:
- Robustness: Fig.4 shows that the proposed method is very robust to structure noise. However, more analysis is needed. Both InfoMGF and SUBLIME are structure learning methods. Compared to InfoMGF，Why does the performance of SUBLIME degrade rapidly in the case of edge deletions?
- Clarity: The paper develops two algorithms in this paper: InfoMGF-RA and InfoMGF-LA. However, it is a little confusion that what is the difference in their objective functions.

Limitations:
The paper discusses the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a novel approach to improve Unsupervised Multiplex Graph Learning by refining graph structures to eliminate noise and maximize relevant information. The method utilizes mutual information maximization to integrate multiple graph views effectively. Theoretical validation and comprehensive experiments show that the proposed method outperforms existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	Multiplex graph provides an efficient representation of complex systems. This paper focuses on non-redundancy issue, which is a new perspective and opens up a new avenue for future research.
2.	The proposed method adopts an unsupervised and generalized approach. Its performance surpasses several supervised approaches, underscoring its potential for practical applications. 
3.	The framework’s performance is validated through comprehensive experiments and compared with more than 20 methods.
4.	Visualization is also a strong point of this paper. The figures of node correlation, heatmaps of the subgraph, and unique relevant edge ratio are very illustrative.

Weaknesses:
1.	According to Table 1 and 2, it seems that the proposed method improves more on clustering than classification.
2.	Overall, this paper is well-organized. However, the writing could be improved in terms of tone and words.
3.	There are too many notations, which are confusing.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xabStWAUtr;"REVIEW 
Summary:
This paper distinguishes two forms of knowledge learning in the model: 
1. co-occurrence statistics: from modeling the co-occurrence of entities in the text.
2. factual associations: from modeling entity relations established through implicit associations.

They synthesize two datasets where knowledge is represented in the above two ways. They show that models that learn factual associations can generalize better than models that learn co-occurrence statistics. They also show that models that learn from factual associations can utilize the knowledge better for reasoning.

They further study where the knowledge of these two different representations is stored in the model. They show that co-occurrence statistics are stored in the middle layer, while factual associations are stored in the lower layers. Accordingly, they propose to reset the middle layer while training the model. They show that this approach makes models generalize and do reasoning better.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The identification of the two forms of knowledge learning shed valuable insight on how models generalize from learned from the training data.
2. They create a dataset and an associated experiment, which can be used for further studies in the same direction.
3. They study where the knowledge is stored in the model. According to the findings, they propose a simple but effective approach to improve the models’ generalization ability and utilization of the knowledge for reasoning.
4. Their experiment is comprehensive. They utilize a benchmark dataset MQuAKE-T and they include fine-tuning only the lower layers as a baseline.

Because studying how language models acquire knowledge from training data is crucial for developing a better training paradigm and I found this paper solid and well-presented, I highly recommend this paper.

Weaknesses:
1. They only experiment with MQuAKE-T where each sentence encodes a piece of knowledge (subject-relation-object). The authors could experiment with some more realistic settings where a single sentence contains more than one piece of knowledge.
2. It would be interesting to see how model scaling affects the behavior. The authors could experiment with models of different sizes in the same model family.

Limitations:
Yes, it's addressed in the last section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies how language models acquires factual knowledge during finetuning. It shows that narrative input tends to teach a model co-occurrence between entities, while referencing input teaches more about factual association. Models that learn factual association generalizes better to various question answering tasks than models that learn co-occurrence, especially for multi-hop reasoning tasks. By resetting different layers to the pretrained weights in models, the authors show that co-occurrence is mostly learned by the middle layers, while factual association is mostly learned by the lower layers. Based on this observation, the authors propose to reset the upper 2/3 layers to learn factual association when finetuning models on narrative input.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper studies how factual knowledge is learned by language models training on pure textual data, which is novel to my knowledge. The authors delivered clear lessons based on synthetic data and per-layer parameter ablation, and provided two solid solutions for real-world reasoning datasets. These lessons are important to the community of language models and reasoning.
- The paper is well structured and very easy to read. There are no typos and grammar errors.

Weaknesses:
- The analysis of this paper is limited to triplets, which do not represent all kinds of knowledge in reasoning tasks. Can you extend the conclusions to more general knowledge forms?
- The authors do not provide enough insights why narrative input tends to teach co-occurrence statistics. The only insight I can find in the paper is that co-occurrence statistics can be learned faster (Line 245-247). I would suggest the authors discussing this more in Section 3.

Limitations:
Yes. The authors have thoroughly discussed limitations of their analysis in the conclusion section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work investigates the deficiencies of pretrained language models in learning factual knowledge, highlighting that these models tend to learn word co-occurrence statistics rather than true factual associations. The authors find that language models, when dealing with explicit relationships, are prone to merely memorize word co-occurrences and perform poorly on tasks that require reasoning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This work shows that language models tend to learn word co-occurrence statistics instead of true factual associations. This finding is important for improving the knowledge learning of language models.
* The authors propose two methods to improve the learning of factual associations. First, by using text with implicit rather than explicit factual associations, they force the model to learn these associations. Second, by actively forgetting the learned co-occurrence statistics, they allow the model to better learn and retain factual associations.
* The proposed strategies significantly improve the model's performance in multi-hop reasoning tasks on both synthetic and real-world datasets, proving their effectiveness.

Weaknesses:
* The generalization across different domains. This work synthesizes Country-City-Animal data, which is somewhat limited.
* Reasoning or memory? The purpose of implicit training is to force the model to understand factual associations through indirect connections, thereby enhancing its reasoning abilities. This approach will help the model perform better on complex, multi-step reasoning questions rather than simple memory tasks because of their training pattern. While, it can’t directly prove that referencing method can bring better memory than Co-occurrence. Moreover, for simple QA tasks, the Referencing method performs worse than the Narrative method. Different test tasks should be designed to verify knowledge retention. For instance, adding more noise and interference during simple QA tests to evaluate the robustness of memory. Design memory retrieval tasks that do not require complex reasoning to ensure that the tests only assess the model's ability to recall facts.
* Although it mentions that co-occurrence statistics and factual associations are parameterized in different layers of the Transformer model, it lacks a deep explanation of the specific mechanisms and reasons behind these phenomena.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the learning of factual knowledge in pretrained language models, distinguishing between knowledge represented as word co-occurrence statistics and true factual associations. The authors find that language models tend to learn co-occurrence statistics, which do not generalize well to reasoning tasks, while factual associations, which generalize better, can be harder to learn. They propose two strategies to improve the learning of factual associations: training on text with implicit associations and using a method called active forgetting to discard learned co-occurrence statistics. Their experiments on synthetic and real-world datasets demonstrate that these strategies significantly enhance the models' ability to generalize factual knowledge in various reasoning scenarios. The paper includes a thorough layer-wise analysis of knowledge parameterization in transformer models finding different localization for co-occurence statistics vs factual knowledge in model weights.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I think the strengths of this paper are in the following contribtions 

- Identification of Knowledge Representations: The paper clearly distinguishes between two forms of knowledge representation in language models: co-occurrence statistics and true factual associations. This distinction is crucial for understanding the limitations of current models. Additionally, the detailed analysis of how co-occurrence statistics and factual associations are parameterized across different layers of transformer models provides valuable insights into the internal workings of pretrained models.

- Empirical Validation: The authors conduct comprehensive experiments using synthetic and real-world datasets to validate their claims. They show that models trained on implicit associations generalize better to reasoning tasks than those trained on explicit co-occurrence.

- Novel Training Strategies: They propose a training strategies to improve factual learning are innovative. Training on text with implicit associations and a method of actively forgetting learned co-occurrence statistics to unblock factual learning.

- Public Release of Resources: Finally, the release of the synthetic corpus and code to reproduce their reulsts can facilitate further research and experimentation in this domain.

Weaknesses:
I did not find any major weaknesses in this paper.

The main ones, which are mentioned by the authors when addressing current limitations of their work are the following:

- Synthetic data split: how are you splitting your synthetic data? Are you evaluating on an unseen subset for both synthetic as well as natural dataset? I understood you are testing on unseen data for natural dataset and I am unsure if that's also the case for the synthetic dataset. Please clarify. This is the reason why I am, at the moment, giving a score of 6 for what would otherwise be a clear 7.

- Overhead in Data Preparation: Converting general text to forms with implicit associations for real-word data may require significant effort and sophisticated rewriting techniques, potentially limiting practical applicability.

- Limited Scope of Text Variations: The paper only considers two forms of text (narrative and implicit association). There is a need to explore more diverse textual representations to validate the findings comprehensively.

- Focus on a single type of reasoning: While the claims that learning implicit knowledge improve performance on complex reasoning tasks, the paper focuses on a specific type of reasoning. Other type of reasoning like logical or mathematical should be validated. Additionally, it is unclear whether the proposed finetuning method and data harm existing model performance on standard LLM benchmark. It would a nice addition to show whether the method in the paper do not conflict with existing model knowledge in other domains.
 
- Evaluation information: Taken from the appendix ""For evaluation on question answering tasks, we report 5-shot exact match accuracy unless otherwise specified."" Please add this in the main body of the paper and mention why you use this metric instead of others like F1 for QA tasks. Is it because all your tasks require a single word as gold label? Is this true also for the real-world dataset in table 3 (MQuAKE-T and 2WikiMultiHopQA)? Please add this info together with your generation parameters used at inference time (number of generated tokens/sampling parameters etc.)

- 
---

Minor

- Missing reference: De Cao et al. Editing Factual Knowledge in Language Models, EMNLP 2021. This is an important reference when discussing model editing since it was among the first contribution in this area.

- line 200 the reference to Appendix 3.3 is wrong

----

### Final Recommendation

Overall, I think the claims are backed by well-presented empirical evidence and I vote for the inclusion of this paper to NeurIPS.

### Update post rebuttal

I increase my score from 6 to 7

Limitations:
Yes in the limitations section after the conclusion on page 9

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vpEq2bzsS0;"REVIEW 
Summary:
The paper introduces a novel framework called MoTE. This framework addresses the trade-off between zero-shot generalization and close-set performance in video recognition tasks by tuning a mixture of temporal experts. The key contributions include:

- Introducing Weight Merging Regularization to balance generalization and specialization.
- Proposing temporal feature modulation to improve generalization during inference.
- Demonstrating state-of-the-art or competitive results on various video datasets such as Kinetics-400, Kinetics-600, UCF-101, and HMDB-51.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The introduction of Weight Merging Regularization and temporal feature modulation provides a novel approach to balancing generalization and specialization in video recognition.
- The experimental results are thorough, demonstrating the effectiveness of the proposed methods on multiple datasets.

Weaknesses:
- The framework's text space is confined to video category names, which limits the richness of textual representations. Expanding the semantic space using large-scale generative models could enhance performance.
- The method currently explores limited forms of additional parameters. Extending the approach to other forms could improve generality and versatility.
- While results on certain benchmarks are promising, the model's performance on more diverse and challenging datasets needs further validation.
- The additional complexity from Weight Merging Regularization and other components can slightly increase training time, which may be a barrier for real-time applications.
- Extensive fine-tuning required for different tasks can be computationally expensive and time-consuming.

Limitations:
The following work is recommended for citation & discussion:

Oh, C., Lim, H., Kim, M., Han, D., Yun, S., Choo, J., Hauptmann, A., Cheng, Z.-Q., & Song, K. (2023). Towards calibrated robust fine-tuning of vision-language models. In NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models.

Tu, S., Dai, Q., Wu, Z., Cheng, Z.-Q., Hu, H., & Jiang, Y.-G. (2023). Implicit temporal modeling with learnable alignment for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 19936-19947).

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issue of Video-Language Models (VLMs), such as CLIP, experiencing reduced generalization performance to unseen categories when learning domain-specific knowledge for video understanding tasks. The authors propose the MoTE framework, which introduces temporal experts and employs a Mixture of Experts (MoE) approach to effectively learn domain-specific knowledge for videos. Additionally, a soft stochastic routing policy is utilized to further enhance the learning efficiency of the experts. To guarantee the discrepancy in knowledge learned by different experts while maintaining a flat loss landscape, the paper incorporates weight merging regularization, which improves the generalization performance of the learned features. Moreover, the paper presents a temporal feature modulation method that leverages the semantic relevance confidence of proxy text features to modulate features.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper introduces the Mixture of Experts (MoE) approach in zero-shot video classification tasks based on Video-Language Models (VLMs). By utilizing weight merging regularization and other methods, the approach ensures effective learning of domain-specific knowledge in videos while maintaining strong model generalization.

2. The study effectively combines temporal modeling of visual content with the MoE approach. During downstream task adaptation, it leverages multi-perspective data bias learning to avoid overfitting, thus enhancing the learning effectiveness of domain-specific knowledge in videos.

3. The paper analyzes model generalization from the perspective of loss landscape flatness. By improving the flatness, weight merging regularization enhances the generalization performance of the learned features.

Weaknesses:
1. There is ambiguity in the use of certain symbols within the paper. For example, the symbol L is used to represent both the loss function of CLIP and the number of layers in the Transformer introduced in MoTE. This issue is particularly evident in Equations (4) and (7). The paper should consider adjusting the usage of these symbols to avoid confusion.

2. There seems to be a problem with the calculation in Equation (5). The notation ""exp"" typically represents the exponential function of e, but this is not clearly explained. According to the equation, the probability of selecting an expert increases with i, which seems to contradict the intended randomness of stochastic. This requires clarification or correction.

3. In the Introduction and Section 3.4, the paper emphasizes the plug-and-play characteristic of the modulation module. However, the subsequent experiments only demonstrate the improvement in model performance without introducing additional training parameters (Play). They do not showcase the flexibility and usability of the module regardless of the upper model structure (Plug). Therefore, it would be beneficial to add experiments validating the plug-and-play effect or adjust the relevant descriptions in the paper.

Limitations:
The paper does not adequately explain the connection of data bias view and MoE in Section 3.2. For reading-friendly, there should be additional descriptions of the relationship between experts and data bias views, and how the MoE approach leverages multiple data bias views to improve model performance.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces MoTE (Mixture-of-Temporal-Experts) to improve the generalization and specialization capabilities of visual-language models (VLMs) when adapting to video tasks. MoTE addresses two main questions: how to enhance the generalization of additional parameters during fine-tuning, and how to balance generalization and specialization in a unified model. The approach uses multiple feedforward network (FFN) experts in each Transformer layer to capture various data bias views, improving generalization. A routing algorithm based on multinomial distribution maximizes knowledge diversity among experts, while Weight Merging Regularization effectively combines generalized and specialized knowledge in the final model.

To further improve generalization at test time, MoTE incorporates a Temporal Feature Modulation module. Notably, the approach maintains the same computational cost and final structure as conventional methods. The paper contributes to the field by offering a new perspective on enhancing parameter generalization and balancing it with specialization in the context of adapting VLMs to video tasks. Extensive experiments demonstrate that MoTE achieves an optimal trade-off between zero-shot and close-set performance, with thorough ablation studies showing the scalability and effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The manuscript is well-written and easy to follow.

- It is interesting to observe that the introduction of a mixture of experts can enhance the balance between acquiring generalizable knowledge and learning video-specific features. The motivation is intuitive, and the extensive experiments effectively validate the method’s efficacy.

- The design of weight merging regularization and temporal feature modulation harmonizes the pursuit of the two learning objectives. The temporal feature modulation is particularly noteworthy, as it takes into account the categorical relationships between the training and test sets to inform the integration of features.

Weaknesses:
- The primary motivation for this study stems from two objectives: (1) mitigating the catastrophic forgetting that emerges with the integration of trainable parameters, and (2) striking a balance between generalizable knowledge and video-specific learning within one single model. However, these objectives bear considerable resemblance to the work presented in the paper FROSTER (ICLR 2024), which has not been discussed by the authors. While I acknowledge that the current paper and FROSTER employ distinct methodologies to address these issues, their close relevance necessitates a thorough discussion and a direct performance comparison.

- According to the description in the paper, the baseline model utilizes a clip encoder equipped with several temporal transformer layers. This leads me to question whether the model can be effectively integrated with alternative network architectures, such as adapter-based networks, X-CLIP, and ST-adapter, particularly given their noted efficiency in training.

- I would also request that the authors provide details regarding the additional computational and training time costs associated with implementing their method in conjunction with the baseline model.

- I believe it would be beneficial to delve deeper into the specific types of actions that each expert excels at recognizing. Providing a more detailed analysis in this area would enhance our comprehension of the distinct roles played by various experts, as well as the unique temporal knowledge they contribute in comparison to one another.

[1] FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition. ICLR 2024.

Limitations:
The authors have not sufficiently addressed the limitations of their methodology, as it has been applied exclusively to a specific type of adapted network without demonstrating broader applicability. It would be advantageous to see an exploration of the method’s versatility across different network architectures.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To preserve the generalization ability of the model trained on general visual-language model (VLM) with task-specific data, while boost the performance on specific task, this paper propose a new framework and training strategy to learn a unified model with specific performance and generalization ability. Three techniques are introduced. Mixture temporal experts to avoid overfitting on the task-specific data. A weight merging regularization to enlarge the loss flat region such that optimization on generalization ability will not introduce perturbation that drops the close-set performance. A temporal feature modulation to reuse the feature of VLM model when the target category label is not fitted during task-specific finetuning. The proposed method is evaluated on four benchmark datasets. K400 for close-set finetuning and UCF-101, HMDB-51and K600 for zero-shot evaluation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	To train a model with both task-specific performance and zero-shot generalization ability is a interesting topic, and it is less explored in the community. 
2.	The proposed method achieves competitive performance compared with the similar methods.
3.	Balancing between the zero-shot and the task-specific ability is always hard to handle. Considering the wide application of general VLM, this method bears practical value in the industry.

Weaknesses:
1.	The experimental setting may hide the weakness of the proposed method. The method is only trained on K400 and evaluated its zero-shot ability on UCF-101, HMDB-51and K600. Considering K400 is already a large-scale dataset, the MoTE may still have good performance on UCF-101 and HMDB-51. Besides, K600 is an extension of K400, therefore they may have similar data distribution. It would be great to also finetune the model on small-scale dataset and evaluated generalization ability on large-scale dataset, for example, train the model on UCF-101 and evaluate it on K400.
2.	A simple solution to handle the zero-shot / task-specific balancing issue is to use a finetuned model such as Text4Vis for specific task and to use its temporally mean-pooled clip feature when facing out-of-distribution task. This baseline is missing in the comparison. If the performance of this baseline is acceptable, is it really necessary to train a unified model with such much cost?

Limitations:
The limitation has been discussed in the suplemental material.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xZxXNhndXU;"REVIEW 
Summary:
The paper presents a novel 3D scene representation for novel view synthesis (nvs) in dynamic urban environments where, in particular, under heterogeneous imaging environments. The proposed representation relies on existing ingredients: 3D Gaussian Splatting, learned static/dynamic object instances, and  a global scene graph.

The resulting system yields very strong results on a series of public autonomous driving benchmarks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
### + Readability.
Overall, in its current state, the paper's readability is relatively good. The main ideas, concepts, are mostly well discussed, conveyed, and articulated, throughout the paper.

### + Practical usefullness of the considered problem.

### + Structure, and Organization of the Contents.
The presentation is mostly on point and each dedicated section of the paper is properly balanced. The use of text real-estate is fair.

### + Relative simplicity of the conceptual contribution.

### + The amount of implementation details is very good.

### + The reported performance.

### + Implementation details for reproducibility: excellent.

Weaknesses:
### - (1) Positioning of the conceptual contribution vs. the competitive landscape.

In particular, the proposed method looks very much like a revisit of Panoptic Radiance Fields [49] by replacing the NeRF component byt 3D Gaussian splats. 

While this is perfectly fine, this merits a targeted, transparent discussion in the main paper to help the reader understand the whereabouts of how the proposed contribution relates (or not) with such pieces of litterature.

### - (2) How much does it cost?

Missing piece of information regarding the resource usage, memory footprint, typical timings etc to better understand the downsides of using the provided method.

### - (3) (To a lesser extent) Certain contents in the paper are unclear.

Figure 4: what is happening? Adding color annotations or boxes would definitely help.

Limitations:
The authors provide one dedicated paragraph that reasonably addresses such considerations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to perform view synthesis for dynamic urban scenes. This paper adopts 3DGS as scene geometry and uses neural fields to model the dynamic appearance of urban scenes. The neural scene graph is introduced to handle the movement of dynamic objects, and a deformation field is used to handle local articulated motions. Experiments show that the proposed approach outperforms baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The presented pipeline well handles the dynamic appearance of urban scenes.
2. The experiments are sufficient and validate the effectiveness of the proposed approach.
3. The idea of combining neural fields with 3DGS is sound and effective.

Weaknesses:
1. The method presented in the paper takes 0.17 seconds to render an image at a resolution of 1550x2048, which is significantly slower than conventional 3DGS. Is the trade-off of such a significant sacrifice in rendering speed for quality improvement justified? Does the author have any solutions to address this issue?
2. The paper needs to evaluate the extent to which neural fields impact the rendering speed of 3DGS.
3. The pipeline figure of the paper should be clearer. The connections between the various modules are not easily discernible from the figure and its caption. For instance, it is not clearly depicted how the latent codes obtained from the scene configuration are inputted into the neural fields. Then, how are neural fields combined with 3DGS to represent static scenes and dynamic objects? The figure only shows simple association arrows. However, these modules are not merely input-output relationships. There are some combination operations between them.
4. The paper uses neural fields to represent appearance, which reduces the memory footprint but may also significantly impact rendering speed. Has the paper considered how to address this issue?
5. In Figure 2 of the paper, regarding the neural fields section, the symbols for static opacity correction and dynamic deformation are inconsistent with the descriptions in Section 3.2 of the paper. This is quite confusing.
6. I am curious whether the combination of neural fields with 3DGS could make the optimization of 3DGS unstable?
7. The non-rigid objects mentioned in the paper refer to cars, right? Or other objects? I did not see how the paper describes the modeling of cars. Although the paper mentions the use of scene graphs for modeling, I did not see how dynamic cars are represented using scene graphs. Does the paper treat dynamic cars as non-rigid objects directly? In this case, how can the large range of movement of dynamic cars be handled?

Limitations:
The limitations from the introduction of neural fields should be discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a hybrid neural scene representation for dynamic urban driving scene modelling. The method utilizes 3D Gaussians as an efficient geometric scaffold and neural fields to represent appearance, thereby reducing memory. To account for transient scene geometry variations caused by weather, seasons, and other factors, the authors introduce an opacity attenuation field that modulates the scene geometry. For modeling dynamic actors in the scene, an object-centric representation is used, with a non-rigid deformation in the canonical space to animate objects such as pedestrians. Experiments demonstrate that the proposed method achieves state-of-the-art performance while rendering faster than previous methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* The paper is well-written and easy to follow.
* The decomposed representation of appearance significantly reduces memory usage.
* It models transient scene appearance and geometry, as well as non-rigid objects like pedestrians.
* The evaluation and ablation study are comprehensive.
* The paper demonstrates visually superior results compared to baselines such as SUDS and ML-NSG.

Weaknesses:
* The rendering of the proposed scene representation requires query appearance from the neural fields, it is unclear whether this will impact rendering speed compared to spherical harmonics representation.
* This paper lacks a comparison with recent neural field baselines such as UniSim and NeuRAD for urban driving scenes. Additionally, there is no comparison of the speed to 3D Gaussian baselines.
* How to control the non-rigid objects in the scene? e.g., animating the pedestrians given a sequence of human poses.
* Is it feasible to render other sensor modalities in autonomous driving, such as LiDAR?

Limitations:
The authors discussed the limitations of modeling other sensor phenomena such as rolling shutter effects, motion, and more complex camera models. They also discussed the broader societal impacts of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper works on novel view synthesis (NVS) for large-scale, dynamic urban scenes. This paper proposes a neural scene representation called 4DGF, which uses 3D Gaussians as an efficient geometry scaffold while relying on neural fields as a compact and flexible appearance model. The proposed method integrates scene dynamics via a scene graph at global scale while modeling articulated motions on a local level via deformations. The method significantly outperforms baselines in terms of speed and rendering quality on three benchmarks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The idea of combining Gaussian Splatting and neural fields to model geometry and appearance, respectively, is very interesting. This makes a lot of sense considering the efficiency and the advantages of each of the two representation. This is definitely a more scalable approach to large-scale scenes compared to prior work.

2. Extensive experiments have been conducted to validate the proposed method, this includes comparing with recent baselines on three benchmarks and the ablation studies that carefully examine each component. Moreover, the rendering quality improvement and the speedup is very significant on all three datasets.

3. The paper is very well-written and easy to follow. Implementation details are sufficiently discussed for reproducibility.

Weaknesses:
1. I appreciate the authors' including a video in the submission. I found sometimes there's a large foggy region near the camera (e.g., the regions on the right during the 5-6th second), do the authors have any explanations on that? Is it caused by any limitations discussed in Sec. 5?

2. I understand that this paper mainly focuses on large dynamic scenes. I am curious how this hybrid representation performs on 3D statics scenes (e.g., the benchmarks that the original 3DGS have been tested on). This seems to be a more straightforward way to see the effect of using neural fields instead to model appearance.

Limitations:
The limitations seem to have been sufficiently discussed.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xZKXGvLB0c;"REVIEW 
Summary:
The paper explores the potential differences in predictor merging when approached from causal versus anti-causal directions. The results from MAXENT and CMAXENT indicate that in the causal direction, the solution converges to logistic regression, whereas in the anti-causal direction, it converges to Linear Discriminant Analysis (LDA). The study also examines how the decision boundaries of these two solutions vary when only partial bivariate distributions are observed, highlighting implications for Semi-Supervised Learning (SSL) and Out-Of-Variable (OOV) generalization.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper investigates the differences that arise in predictor merging from causal and anti-causal perspectives. It demonstrates through MAXENT and CMAXENT that the causal direction results in logistic regression, while the anti-causal direction leads to Linear Discriminant Analysis (LDA). Additionally, the paper analyzes how the decision boundaries of these two methods change when only some bivariate distributions are observed, discussing the implications for Semi-Supervised Learning (SSL) and Out-Of-Variable (OOV) generalization.

Weaknesses:
1. **Small scale dataset**: The main weakness is the small scale of the data and models studied in the paper. I believe the challenge of reducing computational cost with mixture-of-expert models is more relevant to larger models. The authors however only presented results on small bivariate distributions. Experiment results with larger models are appreciated. If experiments with larger models are not feasible, I hope authors can discuss potential limitations of the study under those larger-scale/multivariate scenarios. Do you expect the findings in Eqn 1&2 change in larger-scale/multivariate setups?

2. **Lacks comparison**: This paper lacks sufficient comparisons with other papers. Can you explain what are the differences and advantages of the proposed method, compared to the pi-Tuning method proposed in [1] (Section 3.2 and Section 3.3)?

3. **Contributions are obvious from the observations given in MAXENT and CMAXENT**: The overall paper seems like a consolidation of few previous papers. 

4. **Non-causal**: The paper has studied the causal and anti-causal setups but from the signal processing point of view pleas study the non-causal settings.  

5. **Results under the availability of Noise and biases**: The paper has shown result in a toy example which is good for overall pipeline understanding, but not sufficient to understand it from ML perspective. For example: what if the random variable leverages some amount of noise and have biases that imposes skewness in the distribution? 


References:
[1] Wu, Chengyue, et al. ""pi-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation."" International Conference on Machine Learning. 2023.

Limitations:
The major limitation of the paper is the experimentations that assumes the data distribution to be gaussian. The paper should include non-causal results and its interactions with causal and anticausal models.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors give a treatment of the mixture of experts problems using the idea of maxent; they use this as a tool to discuss how to merge causal and anti-causal inferences on the same data, in part as a way to assess the quality of the data being analyzed.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The discussion of the differences and merging of causal and anti-causal analyses was strong and appreciated.

Weaknesses:
The framing of the paper, I felt, missed a lot of literature and possible approaches to the issue being addressed. That is, the paper is framed as a discussion of merging of expert models (which can be an important problem), and maxent is proposed as a method for doing this. But the merging of experts problem is itself framed as a problem of inferring causal graphs where each expert has access to only part of the data. The problem of overlapping dataset has been extensively studied, but no reference is made to that literature, or to any ideas that literature has proposed which may compete with the maxent proposal here. More recently (actually not that recently) the discussion has taken a turn into discussions of privacy, for contexts, e.g., where different hospitals have access to their own dataset but may want collaborate on building causal model without risking making available by inference the identities of their respective patients--i.e., the so-called ""federated learning"" problem. This has been extensively studied also in the literature to date with many proposals given for how to address it. I think this paper could benefit from a literature review of this sort to place the proposed ideas in context, with comparisons made to alternative methods, or at least reasons not to compare to particular methods that make sense.

Also, the paper is mainly theoretical but could have benefitted from discussion of an empirical or simulation example.

Limitations:
This is a theoretical paper, entirely, so does not benefit from worked examples, so it is difficult to judge impact. I did not see any discussion of societal impact. I also did not see any discussion of promises of software or data to help users assess reproducibility of any results (as empirical results are not assessed).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem of learning a mixture of experts (predictors) where individual predictors have been learned with different causal constraints. It studies different asymmetries that arise when we merge different predictors using the Causal Maximum Entropy (CMAXENT) objective. It goes on to show that different data-generating processes lead CMAXNET to reduce to different objectives under some restrictive setting. Next, they show how the learnt predictors will have different decision boundaries under different data moment restrictions.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to follow. 
2.  The contribution of this paper, however, restricted to a simple setup, is novel. The author shows that under different assumptions on the data-generating process, the CMAXENT objective will yield different predictors and establish necessary and sufficient conditions under which the predictors are different.

Weaknesses:
1. The connection with the OOV generalization literature is not discussed properly. In particular, it would be interesting to see how this paper's observation relates to the paper ""On causal and anti-causative learning"" (ICML 2012) and the guarantees they have for generalization to the distribution shift.

2. In the introduction and abstract, the authors mention that they study the problem of merging the two predictors, i.e., one predictor trained to assume an anti-causal data generating process (DGP) and another assuming causal. Next, in Sections 3 and 4, the authors show the closed form of each predictor separately under different DGPs. However, little is said about the final ""combined"" predictor and its generalization properties. See Question 1 for more.

Limitations:
Yes, they are added to the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the differences and properties that emerge when one uses causal, anticausal features for prediction.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**S1.** This work makes several interesting observations of causal and anticausal predictors under their parametric assumptions.

**S2.** This work suggests some potential considerations for practitioners dealing with feature sets that contain both types of information.

Weaknesses:
**W1.** The primary weakness of this work is that the connections are underexplored empirically and in more complicated settings, e.g., higher dimensions and discrete data.

**W2.** While I do not have an issue with the simplifications you have made to make the connections clear, the lack of more general results combined with a lack of real-world datasets that exhibit properties resembling the observations from your analysis limit the impact of this work is insufficient for the venue.

**W3.** Some of the observations merely confirm properties already known, e.g., the asymmetries on causal and anticausal directions [1-2].

[1] Schölkopf, Bernhard, et al. ""On causal and anticausal learning."" arXiv preprint arXiv:1206.6471 (2012).

[2] Janzing, D., and B. Schölkopf. ""Causal inference using the algorithmic Markov condition. to appear in IEEE Transactions on Information Theory."" See also http://arxiv. org/abs/0804.3678 (2008).

Limitations:
The setting considered is too theoretically and empirically simple to be convincing about real-world tasks.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xXRnUU7xTL;"REVIEW 
Summary:
- The paper introduces SelfCodeAlign, a fully transparent and permissive self-alignment pipeline for code generation in LLMs without relying on extensive human annotations or distillation from larger models. SelfCodeAlign generates instruction-response pairs from seed snippets, evaluates responses with test cases, and fine-tunes models based on successful executions. The approach shows superior performance over state-of-the-art methods, including GPT-3.5-Turbo-based distillation, particularly in HumanEval+ benchmark. The pipeline demonstrates effectiveness across various model sizes, emphasizing the benefits of self-generated data over teacher models with smaller performance gaps. 
- Overall, I feel that SelfCodeAlign is a very easy workflow to follow and I see much potential for such pipelines that do not depend on distillation or human annotations. I recommend an accept.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
## Originality
- The paper adequately cites related work, clearly identifying gaps such as the lack of transparency in existing methods, which is a key motivation for their work. 

## Quality
- The submission is technically sound with both quantitative and qualitative analysis.
- The authors provide detailed experimental results, demonstrating significant performance improvements over baselines.
- The inclusion of both large-scale and small-scale model evaluations further strengthens the quality of the research.
- In terms of ethical considerations, they have considered all terms of use as well as the data in code snippets.

## Clarity
- Well organized paper, except for appendix.

## Significance
- The results are highly significant, as SelfCodeAlign achieves performance improvements, notably surpassing models that are an order of magnitude larger in size. This work addresses the challenge of instruction tuning without human annotations or distillation, offering a scalable and transparent solution that advances the state of the art in code generation.

Weaknesses:
## Originality
- Perhaps similar to this paper [Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models](https://arxiv.org/pdf/2312.06585) ? Even if it is different, I think that this should also be part of your baseline comparison as well.

## Quality
- The qualitative examples provided in the appendix are excessively long, which may overwhelm the reader and obscure the main differences and contributions of the methodology. It would be beneficial to reduce the number of examples or to shorten them, focusing on highlighting the key differences and improvements over baseline methods. Additionally, the examples are presented in black and white with no descriptions or annotations, making it difficult to discern their significance. Providing clearer, annotated examples with concise explanations would enhance the readability and impact of this section.
- I do not see any weaknesses discussed in this work, for example, in what scenario do you think does this methodology not work? Why is the score still not perfect? (or for eg, below 80% accuracy)

Limitations:
Limitations are stated. However, I think the authors miss one important point – the reliance on the seed snippets. The performance of the model is based on what it was finetuned on, so if the seed snippets are not sufficiently diverse or representative of the target tasks, then it might have resulted in a large drop in accuracy.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors proposed SelfCodeAlign that finetunes the model based on the filtered data generated by the same model itself. The authors conduct experiments to show that SelfCodeAlign outperforms most open-sourced models that were finetuned on public code dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The code generation problem is important and the results (compared to models trained on public dataset) are promising.

Weaknesses:
Compare to models that are distilled/trained on non-disclosed data, the performance of SelfCodeAlign is not as competitive. The presentation can be improved, see **questions**.

Limitations:
Limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces SelfCodeAlign, an entirely transparent and permissive pipeline designed for self-aligning code large language models without the need for human annotations or distillation. By applying SelfCodeAlign to CodeQwen1.5-7B, the authors generated a dataset containing 74k instruction-response pairs. They then fine-tuned CodeQwen1.5-7B using this dataset, resulting in SelfCodeAlign-CQ-7B, which demonstrates robust performance on the HumanEval+ benchmark.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The performance is satisfactory: SelfCodeAlign-CQ-7B achieves a pass@1 score of 67.1 on HumanEval+, outperforming larger models like CodeLlama-70B-Instruct (65.2), which is a significant achievement.
2. The process is auto-mated: This paper introduces a novel self-alignment pipeline including concept extraction from seed code, task generation, multiple response generation, and execution validation. This approach is independent of human annotations or large model distillation, making it easy to be applied.
3. Scalability: Experiments demonstrate the method's applicability to models ranging from 3B to 33B parameters, showing good scalability across different model sizes.

Weaknesses:
1. Lack of Diversity in Generated Tasks: While the method aims to produce a variety of coding tasks, it is unclear how this diversity is achieved or measured. There is a risk that the generated tasks may be biased towards certain types of coding problems, which could limit the model's ability to generalize effectively.
2. Overreliance on Self-Generated Tests: The method relies heavily on tests generated by the model itself to validate responses. This self-validation approach could result in a feedback loop where the model learns to create tests that are easy to pass, rather than generating truly challenging or comprehensive tests. The paper does not address how this potential issue is mitigated.

Limitations:
Refer to the weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a pipeline for generating synthetic instruction tuning data. The method consists of the following steps: 1. data filtering is applied to seed coding data to select high quality examples; 2. base LLM is used to generate a set of coding concept and category based on the seed data; 3. base LLM is used to generate coding instruction, response and test; 4. generated examples are selected based on the code execution result.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. the paper focuses on using base model to generate synthetic data to self-improve, which is an interesting and useful angle for synthetic data generation
2. the method is evaluated on several different coding LLM benchmarks which shows the effectiveness of the method
3. there are also ablation experiments verifying the contribution of specific design choices in the framework.

Weaknesses:
While using base model to self-improve is an interesting and useful direction, synthetic data generation could be improved by using a stronger LLM than the base model. It is not clear from the paper whether the proposed framework would be effective compared to previous methods if we use a stronger LLM to synthesize the data. The synthetic data generation could also be potentially improved by having multiple rounds of data generation process.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xW6ga9i4eA;"REVIEW 
Summary:
The authors address a key issue in personalized federated learning, which enables clients with heterogeneous model structures to participate in federated learning with consideration of effectiveness and efficiency. This method is based on model assembly and reassembly, in which the blocks and layers can be treated as modules. After that, the server selects the personalized models and assigns them to the clients. The received models will be used as the teacher to guide the local update. The authors run extensive experiments to demonstrate the effectiveness of their algorithm.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-organized and clearly motivated. Its logical structure and presentation aid comprehension, while the clear and accessible framework and figures enhance readability. Experiments, discussions, or analyses robustly support each claim.

2. The focus on controllability renders the algorithm more applicable in real-world scenarios, allowing for greater human involvement in the model generation process. The authors effectively demonstrate the utility of their design through experimental results.

3. The authors have performed extensive experiments, including principal studies on image datasets, ablation studies, hyperparameter evaluations, and thorough discussions. These efforts confirm the validity of the techniques and provide deep insights into the paper's contributions.

Weaknesses:
1. Based on the algorithm itself, it includes the reassembly, assembly, matching, and other operations. The reviewer may be concerned about the computational burden compared with the one without any controllability. 

2. How to select the anchor block and why needs to be stated clearly.

3. According to the experiment results, the reviewer is wondering about how this approach can be used with the public data with/without labels and the possible reason why it is robust to the public data with or without labels.

Limitations:
No negative social impact to the reviewer’s best knowledge. The study of K should be put in the main content as that is an important part of the algorithm.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a controllable model reassembly approach to enable heterogeneous model cooperation in federated learning. The designed CMSR algorithm provides the control of the space to save the computational cost.  Furthermore, the approach also achieves model personalization for each local client. They test the proposed approach on benchmark datasets and compare with other baselines under different settings.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1, This paper targets one of the challenges in federated learning, which is the model heterogeneity. To the best knowledge, most existing related works are based on knowledge distillation.  This work presents a controllable approach to conduct block assembly and reassembly from local models to achieve heterogeneous model cooperation and model personalization. The idea itself is interesting and practical.
2, They take efficiency, generalization, and personalization into consideration. They provide comprehensive analysis and provide detailed discussion under various settings, which support their statement soundly. 
3, Their presentation and logic are both easy to follow and understand. The framework, experiment results, and discussion are clearly presented.

Weaknesses:
Weakness
1, In their approach, the authors employ K-means clustering. The reviewer is curious about how the value of K is selected and how this selection influences the results.

2, One of the main contributions compared to pFedHR is the enhanced controllability. I am interested in understanding the nature of this controllability, specifically the extent to which the generated models can be controlled.

3, The paper focuses solely on image classification. Adhering to the review guidelines, the reviewer is not requesting additional experiments, but the reviewer is interested in exploring whether the existing methodology could be applicable to other tasks.

Limitations:
There are no potential negative societal impacts of the work. There are two limitations as follows:
(1)	This approach still raises extra computational cost at the server side. 
(2)	That would be great if this approach can be extended to other tasks and other domains.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a `pFedClub` method for personalized federated learning that enables controllable heterogeneous model aggregation, addressing limitations of existing approaches such as lack of personalization, privacy concerns, and uncontrolled model size growth.

Extensive experiments conducted on three benchmark datasets using various CNN-based model structures validate the effectiveness of the proposed method under both IID and non-IID settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- They conduct extensive experiment including the discussion about the hyparameter $K$ to validate the controllability of the proposed method and computational efficiency on the server.

Weaknesses:
1. The writing and structure of the paper need improvement, particularly in the ""Order-constrained Block Search"" paragraph. The concept of order is unclear, especially the meaning of $q < u$ in line 177. It's not evident whether this refers to a similarity score or another metric. The author should provide a clearer explanation of this constraint.
2. In equation (1) on line 141, the meaning of 'CKA' is not defined. The authors should explain what CKA stands for and how it's calculated. Additionally, it's unclear whether this computation occurs on the server. If clients must transmit input $x_{m,i}^t$ and output to the server, this raises privacy concerns that should be addressed.
3. The paper doesn't specify whether the features $x_{m,i}^t$ and $x_{n,j}^t$ in equation (1) have the same dimensions. This should be clarified to ensure a proper understanding of the similarity calculation.
4. The sampling process for the Anchor Block selection is ambiguous. The probability distribution over all models for this selection is not clearly defined.

Overall, the authors should formulate the proposed method more rigorously, using well-defined notations and providing clear explanations for each step of the algorithm. This would significantly improve the paper's readability and reproducibility.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses heterogeneous model aggregation in federated learning. To this end, the authors introduce pFedClub, which aims to generate personalized models for federated clients while ensuring that the models remain within size constraints. Specifically, pFedClub consists of three main steps: first, it decomposes models into multiple blocks and clusters them using the K-means algorithm; second, it replaces original blocks with others from the same clusters to create a set of candidate models; third, it selects the optimal personalized model for each client using a public dataset and an initial model transferred to the server. Extensive experiments illustrate its significant improvement over existing methods in this field.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The work is well motivated and explores an interesting problem in federated learning. 
2. The presentation of this paper is clear, and the authors comprehensively and intuitively describe the proposed pFedClub. 
3. The paper conducts sufficient experiments and compares the proposed method with previous works. The numerical results demonstrate the superiority of pFedClub.

Weaknesses:
1. The proposed work requires a public dataset, which is unsuitable in federated learning due to the privacy concerns. Is this work applicable to a public dataset different from the training data distribution? For example, the clients collaboratively train a model for CIFAR-10, while the server holds a public dataset from ImageNet. 
2. Although the proposed work achieves remarkable under convolutional neural networks, it is unclear how pFedClub performs under transformers. Is the proposed work suitable for a setting where clients hold three different sizes of LLM, i.e., LLaMA-7B, LLaMA-13B, and LLaMA-70B?

Limitations:
See **Weaknesses**

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xUoNgR1Byy;"REVIEW 
Summary:
This submission tries to tackle one big question in the field of interpreting the data-driven preference learnt by RLHF in human language. The technical path this submission took is to train probe on SAE features to distinguish between good and bad RLHF features.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
+ The attempt to interpret what happens during RLHF training is a good direction to pursue.
+ Releasing the SAE direction and training code could be an excellent news to the community.

Weaknesses:
+ Unclear why have to probe on top of SAE feature. SAE greatly increase the dimensions of the features, leading to overfitting---you can find a separating plane for whatever classification task in this high-D space. Lacking comparison to normal probing. 
+ Considering the problem from a dynamical perspective can be fruitful. Noted that the authors did ablate the features and observe a performance drop on preference dataset. But it's also interesting to see the progress of RLHF training, how it warps the features spaces, even the SAE features' relative importances.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The goal of this paper is to predict where patterns in LLM activations learned from RLHF diverge from the human preferences used for the RLHF training. 
Given a base model and an RLHF tuned version of it, the method involves first identifying the 5 layers with highest parameter difference according to an L2 norm. Then two auto-encoders are trained over the activations from these layers. The encoder and decoder weights of the autoencoder are tied, and the output from these is preferred for studying the activations as they are expected to be more sparse, condensed and interpretable than the raw activations.
At inference time, for each input, the activations from the high divergence layers are computed, passed through the autoencoder and then aggregated. Given a pair of contrasting inputs, a linear probe is trained to predict activation deltas using the above aggregated autoencoder output as input. The output of the probe is meant to be a predicted feedback signal that can be compared to the ground truth fine tuning feedback. For sentiment analysis, a strong correlation is observed with the Pythia-160m model but this is weaker for Pythia-70m and GPT-Neo-125m.  

For another validation the probes, GPT-4 is used to generate explanations of the features in the decoder weights of the autoencoders that get activated when the predicted feedback is positive. GPT4 is then prompted to predict whether or not these are relevant to the fine tuning task, based on a language description of the task. It is found that a feature identified by GPT-4 as relevant to the fine-tuning task is between twice and thrice as likely to be correlated with predicted positive feedback.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is quite accessible for a reader whose area of focus is not interpretability.

Weaknesses:
As a reviewer not particularly experienced with work on interpretability, the takeaways of this paper are somewhat unclear. For example, if we finetuned a new model on one of the datasets used in this paper and trained probes in a similar way from its activations, what would that tell us about the the difference between the base and RLHF versions of that model? Alternately, is the goal to discover information about a model where the base and RLHF-tuned versions are available but the data is not, and hence we do not know what factors might have influenced the preference annotations that guided the annotation. 

I did not fully understand how the activation deltas are calculated. While most of the paper is fairly readable to a reviewer with a different area of focus, this aspect could be improved.

Limitations:
The paper has discussed limitations but not broader impact of their method.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose an approach for measuring and interpreting the divergence between learned feedback patterns (LFPs, or simply the model's activation patterns) and the feedback reward distribution of the preference training dataset. To do so, they identify layers whose activations have moved the most during RLHF training and input these layers' activations into a sparse auto-encoder (SAE) that is trained to provide sparse representations of the LLM's activations. Then, they train probes to predict the feedback signal (e.g. reward, sentiment label) from the SAE's outputs. They use these probes both to measure the divergence of the LFPs from the actual feedback signals and to interpret which features are most important for the LFPs.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The authors ask an interesting question of whether we can measure and interpret the difference between a trained model's activation patterns and the preference distribution it has been trained on. The interpretability aspect of this question is interesting, since it can help us better understand what exactly the model has learned (or not learned) from its training dataset.
- The authors provide a good explanation of why sparse auto-encoders are being used for this task (rather than interpreting the raw model activations), as well as the limitations thereof.

Weaknesses:
- The effectiveness of this probing method seems to rely on many key assumptions being true, such as (i) sparse autoencoder outputs being more interpretable than the original model's outputs, (ii) sparse autoencoder output representations being faithful to the original model's representations, (iii) the probes being accurate, and (iv) GPT-4 being accurate/faithful when giving descriptions of each feature. There is very little experimental evidence provided for confirming that any of these assumptions are true, and these claims are difficult to test in the first place.
   - In fact, the authors mention that a likely reason for the low correlation between the probe's predictions and the VADER lexicon (for some models) is ""the complexity of the probe's task...a linear regression model is unlikely to recover such granular rewards accurately from just the activations"" (L265-266). Although they do find a high correlation for one model, the insufficiency of this probe implies that it is not effective for accurately measuring the divergence between the model's activation patterns and the feedback label distribution. If the correlation is low, we cannot tell whether that is the probe's failure, or if the model has not  acquired strong LFPs, or some combination of the two. Since this probing technique is a central contribution of the paper, I would expect stronger probes and more rigorous evaluation of the effectiveness of the probes.
   - How can one ensure that GPT-4's interpretations of the features are accurate or faithful?
- Table 5 purports to check whether the predicted LFP-related features were actually important and useful to the LLM, but the numbers before and after ablation are often very close together (or identical, in the case of GPT-Neo-125m). It would be helpful to report confidence intervals or standard errors to check whether these differences are significant. But as it currently stands, this table's results does not seem to strongly support the claim that the predicted LFP-related features are indeed relevant to/critical for LFPs.

- Lack of clarity in explaining methods:
    - Much of the writing about the methods is unclear, contradictory, or omits many details. For instance, the explanation of the logistic regression probe in L233-234 says ""we label the concatenated activations as positive or negative based on the averaged activation deltas for each token over the entire input sequence, and train a logistic regression model to classify the activations,"" which would suggest that this probe's inputs are the activations. But L493 (in the appendix) says ""...we give a positive or negative label to concatenated autoencoder outputs based on the sign of their activation delta. We then train a logistic regression model to predict the labels from the concatenated autoencoder outputs,"" which suggests that the inputs are actually the autoencoder outputs, not the original model's activations. Which is it?
   - In Section 3.4, how is GPT-4 prompted to provide the explanations?
   - Given how confusing and verbose the methodology is, I would encourage the authors to write out some of the procedures in equation form, rather than long paragraphs of text.

Limitations:
The limitations section was well-written and thorough, and covered many of the concerns I had myself. An additional limitation is that this method is computationally expensive and requires both training another model and running inference on a sufficiently powerful LLM (e.g. GPT-4) to interpret the features. In this paper, most of the results were for smaller models (under 1B params), and it is unclear whether this method would be scalable to larger models.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates how large language models (LLMs) learn preferences from human feedback during fine-tuning using reinforcement learning (RLHF). The authors introduce the concept of Learned Feedback Patterns (LFPs) to describe activation patterns in LLMs that align with human feedback. They aim to measure the accuracy of these patterns in capturing human preferences by training probes on condensed representations of LLM activations. The probes predict the implicit feedback signal in these activations and compare it to true feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The introduction of LFPs provides a new perspective on understanding how LLMs learn from human feedback. This concept helps in quantifying and interpreting the alignment between LLM activations and human preferences.

- The authors validate their probes by comparing neural features correlated with positive feedback against GPT-4’s descriptions of relevant features. This cross-validation strengthens the reliability of their findings.

- The use of synthetic datasets to elicit specific activation patterns in LLMs adds to the reproducibility and robustness of the study. These datasets are also made publicly available for further research.

Weaknesses:
- The study primarily focuses on a few specific models (e.g., Pythia-70m, GPT-Neo-125m) and tasks (sentiment generation, toxicity), which might limit the generalizability of the findings across different LLMs and applications. More recently released models are of more value for studying RLHF patterns and verify that the method can be generalized. The patterns are easy to extract because that the used data are quite obvious to encode and decode. 

- While the probes show significant accuracy for certain tasks, the paper notes weaker correlations for more granular reward predictions, suggesting that the approach might struggle with highly detailed feedback signals. The issue of feature superposition in dense, high-dimensional activation spaces poses a challenge to fully interpreting the learned features. Although sparse autoencoders mitigate this to some extent, the problem remains a significant obstacle.

- The validation process relies on GPT-4’s ability to describe neural features, which introduces a dependency on another model’s interpretability. This could introduce biases or inaccuracies if GPT-4’s descriptions are not perfectly reliable.

- The paper acknowledges that while their method identifies features involved in feedback signals, it does not provide a mechanistic explanation of how these features interact or influence the expected feedback signal. This limits the depth of interpretability.

Limitations:
discussed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xUjBZR6b1T;"REVIEW 
Summary:
ReVideo presents a novel view of video editing by modifying content with input trajectory to create new content. It designs a three-stage strategy to wrestle out the problem of ignoring motion control when direct training. The main contribution of this work relies on the new task of editing motion via user-specified trajectory while keeping the original video movement. The editing results are superior and photorealistic.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The first video editing work on creating new motion and content.
2. Good writing; the paper is easy to follow, and the motivation and three-stage training strategy on decoupling content and motion control is reasonable. The proposed SAFM learned a dynamic fusion weight at different timesteps.
3. The editing results are photorealistic and adhere to the original motion or follow user-specified trajectory with no artifacts.

Weaknesses:
1. The author did not provide the method or explanation of how ReVideo edits the first frame, making the total editing pipeline not end-to-end for users.
2. Part of the original video motion, like mouth movement in the Zuckerberg->robot (head6) and tail movement in dog->lion, is not kept in the edited video.
3. I would like to know how the drag-based editing method handles non-rigid motion, such as the rotation of car tires from a side view. In examples like sea2 and sea2_2, where a shark and a dinosaur are added, the limbs of the animal seem unable to move, making the video look unrealistic. However, in soccer and some human-centric examples, the legs of dogs and people can move normally. Therefore, I would like the authors to add an example of a vehicle moving on the road from a side view, including the movement of the wheels, to address my concerns. This may be a limitation of the drag-based method.
3. There is no quantitative comparison of the ablation study; I understand that the image results in Fig 7 are clear, but only one video qualitative ablation is not reasonable. 
4. There are no qualitative video comparisons with other methods in the supp or project page, but only Fig 6, and the automatic metrics are worse than pika even though I understand the clip scores are not accurate, which can not reflect temporal consistency accurately. I suggest the author supply the comparison video between Revideo and other methods in the rebuttal phase.
5. The training cost of three stages: even though Revideo makes great progress in creating new motion, training cost like GPU costs, time costs, memory costs and so on, is still a problem since users prefer to edit a video in a zero-shot manner when using a pretrained video generation model and the compared methods like AnyV2V is training-free.

Limitations:
no significant limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a video editing method that enables precise localized adjustments to content and motion within specific areas of a video. It introduces a three-stage training strategy and a spatiotemporal adaptive fusion module to integrate edits across frames and locations effectively. This method allows for complex editing tasks such as changing content while maintaining motion, adding new motion to static content, and simultaneously modifying both elements.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper introduces a novel challenge of editing both content and motion in specific video areas and combines techniques from diffusion models and video editing to achieve nuanced control.
- The three-stage training strategy enhances the robustness and effectiveness of the edits, supported by experimental validation that demonstrates superior performance compared to existing methods.
- The paper is well-organized and clearly explains complex concepts, including the innovative spatiotemporal adaptive fusion module and detailed training strategy.

Weaknesses:
- The decoupling training could cause some artifacts. Although the paper demonstrates these artifacts could mostly be alleviated by deblocking training. I can still see some blocky/unnatural results in the result videos.
- The training is quite complicated and separated into three stages. I feel the training strategy could 'overfit' this particular video dataset.
- This method is more like a direct combination of video diffusion and ControlNet.
- More detailed implementation specifics, particularly regarding parameter settings and the architecture of the spatiotemporal adaptive fusion module, are needed.
- The method's computational demands and potential scalability issues are not adequately addressed. For example, what kind of GPU does one need to perform training and testing? 
- The paper focuses heavily on technical aspects with less consideration of user interaction.

Limitations:
Quality limited by SVD.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents ReVideo, a new approach for precise local video editing of both content and motion. It introduces a coarse-to-fine training strategy to progressively decouple content and motion control, and a spatiotemporal adaptive fusion module to integrate them effectively. Experiments show ReVideo can modify local video content, customize motion trajectories, or change both simultaneously, and extend to multi-region editing.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This appears to be the first attempt at exploring local editing of both content and motion in videos using diffusion models. Being able to modify content and motion trajectories in specific regions is a novel capability compared to prior work.
- The proposed three-stage coarse-to-fine training strategy to progressively decouple content and motion control is an interesting technical approach to deal with the core challenge.
- The spatiotemporal adaptive fusion module is another novel component to integrate the content and motion conditions across sampling - steps and spatial locations.
- Extending the approach to allow multi-area editing without requiring specific training demonstrates flexibility.
- Most of the visual and quantitative results show improvements over prior methods 

Overall, this paper addresses a timely and important topic with significant potential benefits for the community. Despite some weaknesses, the reviewer recommends acceptance, considering this is a relatively new area and the paper presents promising results. The score may be adjusted based on the quality of the rebuttal.

Weaknesses:
## Practicality of the Editing Workflow

The current editing interface requires users to specify both a target content image and a set of motion trajectories. While this allows for fine-grained control, it may not be the most intuitive or efficient workflow for common editing tasks. Consider the scenario of object removal - the user would need to carefully craft a content image with the object removed and ensure that the remaining motion trajectories are consistent. An alternative approach could be to directly specify the regions to remove and have the model infer the appropriate content and motion changes automatically. The paper would benefit from a more detailed discussion of the practical trade-offs and usability considerations of the proposed editing framework.

## Limited Motion Control
While the method allows for editing the motion of individual objects, it assumes that the overall scene motion (camera movement, background motion) remains fixed. This limits the applicability of the approach in scenarios where the goal is to modify the global motion patterns (e.g. stabilizing shaky footage, changing the camera viewpoint).

## Precise Placement and Key Contributions of this Paper

While the individual technical components (e.g. coarse-to-fine training, adaptive fusion) are well-motivated, it's worth considering whether similar strategies have been explored in related domains. For instance, progressive training to handle multi-factor variation has been used in GANs, and spatially-adaptive normalization is common in style transfer. Drawing more connections to such related work would clarify the novelty of the specific adaptations made here.

## Content-Motion Entanglement

- The key technical contribution of the paper is the decoupling of content and motion information through a coarse-to-fine training strategy. However, it's not clear if this decoupling is complete or if there are still some residual entanglements between the two factors. For instance, the edited content may still contain some motion information that could interfere with the specified motion trajectories, leading to artifacts or inconsistencies. A more thorough analysis of the content-motion separation and its impact on the editing quality would be informative.

- Is decoupling content and motion the only way to address the issue - could a joint representation learning approach work instead? Acknowledging alternate strategies would help justify the chosen approach.

- **Figure 4 is not very intuitive. It would benefit from additional justification, theoretical analysis, and insights into why such a simple composition from two videos is effective.** This is a key concern.

## Multi-area Editing 
- The extension to multi-area editing is a nice addition, but the paper could go further in characterizing the challenges involved. Are there issues with preserving global coherence across multiple edited regions? How does the method scale with the number of regions? Providing such details would give a more complete picture of the capability.

## Clarity and Reproducibility
- Implementation details: There are some missing specifics that could hamper reproducibility. For instance:

   - How exactly are the editing regions defined during training - what is the procedure for randomly sampling them?
   - What metrics are used for the ""threshold filtering"" of motion trajectories and how were the thresholds chosen?
   - Are there any data augmentation, regularization or optimization tricks used during training?

## Evaluation Metrics

The quantitative evaluation relies primarily on low-level metrics like PSNR and LPIPS, which may not fully capture the perceptual quality and coherence of the edited videos. Additional metrics could provide a more comprehensive assessment:

- Metrics that specifically measure the consistency of the edited regions with the target content and motion (e.g. using an object detector or tracker).
- Metrics that evaluate the temporal stability and smoothness of the edited videos (e.g. some metrics that are used in video inpainting tasks, Please refer to [this repo](https://github.com/MichiganCOG/video-inpainting-evaluation) for details).
- Human evaluations of the overall realism, coherence, and faithfulness to the editing inputs (e.g. through user studies).



## Robustness Evaluation and Ablation Studies

While the paper does include ablations for a few key components (e.g. SAFM, training stages), there are other design choices that are not fully explored. For instance:

   - How important is the choice of motion representation (trajectory vs. alternatives)? Testing with different motion inputs would reveal the sensitivity to this factor.
   - What is the impact of the trajectory sampling strategy and hyperparameters? Varying the number and selection of trajectories could provide insight into the robustness.
   - How does the performance vary with the size and shape of the editing regions? A systematic evaluation across different region properties would be informative.
   - Only the end-to-end video editing pipelines are compared, but not the individual technical components. For instance, how does SAFM compare to simpler fusion schemes used in prior work?
  - Input noise and perturbations (e.g. in the content image or motion trajectories)

## Dataset Complexity 

-  While the approach achieves good results on the chosen datasets, it's unclear how well it would generalize to more complex video content (e.g. with dynamic backgrounds, scene changes, occlusions etc.). Discussing the potential failure modes and current limitations would help scope the contribution appropriately.

- The examples shown in the paper are largely limited to simple object-level edits in relatively constrained scenarios (e.g. clean backgrounds, single objects). It's unclear how well the method would perform on more challenging videos with complex scenes, multiple objects, occlusions, camera motion, etc. Testing on a wider range of video complexity would help establish the generality of the approach.

## Editing Scenarios
The paper demonstrates a few key editing applications (e.g. object addition/removal, motion editing), but there are other important scenarios that are not explored, such as: performing semantic-level edits (e.g. changing the action or interaction between objects).
Showcasing the method's performance across a fuller range of editing tasks would demonstrate its versatility.

## Open Source
Will the code for training and inference be released?

Limitations:
Please refer to the weakness section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xSziO6gQgG;"REVIEW 
Summary:
This paper studies the implicit bias of the gradient descent on the Next-Token Prediction (LTP) problem in linear models. They first formulate this NTP problem as minimizing the cross-entropy (CE) loss over distinct contexts, each tied with a sparse conditional probability over the token space. They then provide the necessary conditions for the CE loss to reach the entropy lower bound, i.e., the NTP-compatible condition and the NTP-separable condition. Then, they prove one sufficient condition for those two conditions is oevrparameterization, i.e., the dimension of the embedding space d is larger than the number of distinct contexts in the dataset. Assuming both compatible and separable conditions, they then prove the directional convergence of the minimizer of the CE loss within a certain range and the directional convergence of the GD iterate towards the direction of the solution of an NTP-SVM.

In general, I think this paper delves into a good and important problem: the optimization path and implicit bias of NTP mechanism. The authors provided a good formulation, and the proof is solid.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. They investigate an interesting and important problem: the optimization path and the implicit bias of NTP.

2. Their formulation of NTP into the CE minimization over distinct contexts is novel.

3. They provide rigorous theoretical results and the proofs are solid, to my knowledge.

Weaknesses:
1. The main issue of this paper is that, for the NTP-compatible and separable conditions to hold, one needs d > m. Does this overparametrization condition usually hold in practice or not? To my knowledge, in practice, the embedding dimension d is much smaller than the number of training data. Since m is not the number of training data and can be much smaller than that, it is not clear to me whether this assumption is possible in practice.

2. There are some paragraphs that are not very clearly written. For example, in lines 154-157, why does equation 4 constrain W^p w.r.t. this subspace? Why is the solution W* unique, assuming equation 4 has a solution? I think those can be expressed as lemmas to make them clearer. In line 148, the authors claim that (3a) holds if and only if the data satisfies the NTP-compatible condition. The 'if' direction is trivial, but the other direction needs a more rigorous proof.

Limitations:
See above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the implicit bias of optimization in next token prediction tasks by analyzing the structure of the decoding matrix at infinite time. The paper introduces two novel conditions under which the loss reaches its minimum theoretical value and demonstrates that if these conditions hold (which can be, for example, the case when the model is overparameterized), then after GD training, the decoding matrix will converge (in direction) to a matrix reminiscent of the maximum-margin matrix in ""standard"" classification.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work studies a timely topic (next token prediction) and approaches it from a learning theoretic perspective (implicit bias of optimization), which has proven to be very fruitful in ""standard"" classification. The assumption of sparse contexts is clever and should be of wider applicability. The results are novel and analogous to similar results that were proven for ""standard"" classification. Furthermore, the presentation is comprehensive, with many pointers to related work, which help contextualize this paper's contributions.

Weaknesses:
A weakness, which the authors do acknowledge in their work, that prevented me from giving a higher score is that there is no clear connection between the structure of the weights and generalization, as there exists in ""standard""/one-hot classification. As a result, it is unclear how much insight can be derived from the current result. I would appreciate the authors' thoughts on this.

Minor: The text is too dense in places, with the authors trying to include more details than what the space permits. I would suggest moving some of the discussion in Sections 6 and 7 to the Appendix to facilitate a smoother flow.

Limitations:
The authors thoroughly discuss the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective, the central challenge being to discern the ""implicit bias"" of the optimizer towards particular solutions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well written, and the notation is very clear.

- The paper provides a a very interesting starting point for studying the solutions found by gradient descent in NTP settings

Weaknesses:
While the paper provides a a very interesting starting point for studying the solutions found by gradient descent in NTP settings, it's not very clear whether margin maximization practically corresponds to any meaningful takeaway in language modeling.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study investigates the structural properties of solutions chosen by gradient-based optimizers for next-token prediction (NTP), framing NTP as cross-entropy minimization across various contexts with sparse conditional probability distributions over a finite vocabulary. It focuses on the optimization bias of gradient descent (GD), characterizing how GD selects parameters that equate the logits’ differences of supported tokens to their log-odds.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This study enables deriving the data-entropy lower bound in NTP for understanding the optimization and generalization properties of NTP models.

Weaknesses:
The study's focus on linear models  analyzing CE loss for NTP may limit its novelty and applicability, making its contributions to the field appear unclear compared to existing research.

Limitations:
The limitations of this study include its reliance on the simplicity of the analyzed model, unclear distinctions and advantages over existing research, and its omission of key aspects such as the properties of attention mechanisms.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
x7pjdDod6Z;"REVIEW 
Summary:
This paper introduces MeshFormer, a sparse-view reconstruction model designed to generate high-quality 3D textured meshes from sparse RGB images and their corresponding normal maps. By leveraging voxel representation, 3D inductive biases, SDF loss, and normal information, the model shows comparable inference performance to concurrent methods, while the entire training process can be completed using only 8 GPUs within a week (concurrent methods typically require around 100 GPUs). Experimental results demonstrate the effectiveness of the design.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors provided a detailed explanation of the motivations behind the model designs (including the introduction of voxel representation, the introduction of 3D full (or sparse) convolution, and so on) and demonstrated the reasonableness of these choices.

2. Compared to baseline methods, this model is simpler to train and demonstrates better qualitative and quantitative results. 

3. The ablation study demonstrates the effectiveness of normal input, SDF supervision, geometry enhancement, and other methods proposed in the paper.

Weaknesses:
1. Although the authors provide detailed textual descriptions in the method section, it would be better if more mathematical symbols and equations were used, which could explain the entire pipeline more clearly and unambiguously.

2. For reproducibility, the authors should provide more implementation details, including a more detailed model architecture, the values of hyperparameters (e.g., \lambda in the loss function), and other relevant information.

3. The authors don’t report the comparison of inference time and memory usage between the proposed model and the baseline models.

Limitations:
Yes, the authors addressed limitations, potential negative societal impact, and mitigation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a high-quality feed-forward 3D object reconstruction method from sparse view RGB images. It uses an explicit voxel structure for better geometric inductive bias, auxiliary inputs such as 2D diffusion generated normal images and SDF representation for better geometric details, and an end-to-end trainable pipeline that eliminates the need for multi-stage refinement. The method gives high quality reconstruction results, especially in terms of fine-grained and smooth geometry.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Although the network architecture and 3D representations are more complicated than previous methods, they are end-to-end differentiable and alleviate the training burden of multi-stage refinement.
2. The idea of using 2D diffusion generated normal images as input to the reconstruction pipeline is interesting and insightful.
3. It is more computationally efficient to train (Line 73).
4. The qualitative results are impressive, especially the mesh normals.

Weaknesses:
1. In original LRM the only supervision signal needed is RGB images. The proposed method, however, needs access to the full 3D shape for supervising the occupancy. It is fine for hand-made 3D assets but might poses some difficulty when trying to scale to real datasets.

Limitations:
1. It requires 2D diffusion models to generate auxiliary inputs, which can drastically slow down the reconstruction speed.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this work, the authors propose a sparse view reconstruction model that utilizes a set of images (with camera poses) and corresponding normal maps to produce a reconstructed textured mesh. The primary contribution lies in adopting voxel-based 3D representation and employing a network architecture that integrates both 3D convolution and attention layers. Moreover, direct geometry supervision (SDF loss) is applied during the training process, alongside rendering-based losses. Experimental results demonstrate that the generated 3D shapes achieve state-of-the-art performance when compared to existing works on the single-view to 3D task.

However, as highlighted in the weakness section, there are potential misclaims regarding the technical contributions. It is highly recommended to revise the manuscript to cite and discuss these related works. Despite this, I am currently inclined towards accepting the paper and would be happy to champion it if the aforementioned issues are addressed in the final version.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The writing is clear and easy to follow.
- The combination of SDF loss and rendering losses appears novel for training a feed-forward based network. Additionally, the ablation study in Table 3(b) clearly indicates that SDF supervision is crucial for achieving good geometry, as evidenced by the significant CD difference between (b) and (g).
- Although [33] has explored using normal maps for the reconstruction task, it seems new to employ normal maps as inputs and supervision for a feed-forward reconstruction network.
- Experimental results demonstrate state-of-the-art performance over existing baselines, as shown in Table 1 and Figure 3. Furthermore, it is illustrated that existing methods cannot achieve similar performance given the same computational resources (Table 2).
- The ablation study confirms that various components are essential for the final performance, including considering normal input and SDF supervision.

Weaknesses:
Possibly Misclaimed Technical Novelties:

However, the current manuscript may contain several misclaims regarding its technical novelties.

One claimed novelty is the adoption of a 3D voxel representation. However, the use of 3D voxel-like volumes in reconstruction is not a new idea and has been well-explored in various works, including:

A. Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process, CVPR 2023

B. SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation, CVPR 2023

C. Locally Attentional SDF Diffusion for Controllable 3D Shape Generation, SIGGRAPH 2023

D. One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion, CVPR 2024

E. Make-A-Shape: a Ten-Million-scale 3D Shape Model, ICML 2024

Additionally, the use of convolution + transformer layers to process grid input seems to be standard procedure in 2D generation tasks, as seen in:

Diffusion Models Beat GANs on Image Synthesis, NeurIPS 2021

Similar architectures have also been widely adopted in some of the aforementioned 3D reconstruction works, such as [A, C, D, E].

Regarding image conditioning, the cross-attention with image patch features is also well-explored in various works mentioned above, such as [C, D, E].

Limitations:
The main limitation is well described in Section 5.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes an improved framework for feed-forward reconstruction models. The authors advocate a number of improvements over the initial design of Large Reconstruction Model, including model architecture and training schemes. Experiments show that the method reconstructs better geometry and texture on Google Scanned Objects and OmniObject3D datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is focused on ablating different components for feed-forward sparse-view reconstruction, and in-depth analyses are provided for each design choice. Although there are no complicated new method proposed, such analysis bring value for understanding how and why each component works.
- The proposed method is evaluated on (preprocessed) real-world multi-view datasets, showing improvements over baselines on all metrics. Extensive ablative analyses are also provided to better understand the behaviors of the proposed method.

Weaknesses:
- Since this is more of an analysis paper, it would be good if the authors could also document the other components that were tried/ablated but did not see significant differences.
- Since training resources was discussed and compared, it would be nice if there could be an analysis on the mesh generation/reconstruction quality over training time.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this work, the authors propose MeshFormer, a sparse-view reconstruction model that explicitly leverages 3D native structure, input guidance, and training supervision. They leverage 3D sparse voxels as their representation and combine transformers with 3D (sparse) convolutions to inject 3D prior. Additionally, they propose to take the corresponding normal maps together with sparse-view RGBs as input and also generate them as output, which could be used for geometry enhancement. Extensive experiments show that MeshFormer can be trained efficiently and outperforms state-of-the-art methods in terms of generating high-quality textured meshes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- MeshFormer is able to generate high-quality textured meshes with fine-grained geometric details.

- The authors find that using normal images together with RGB images greatly helps in predicting geometric details. Additionally, the model outputs a normal map, which can be used for geometry enhancement.

- The proposed method explicitly leverages 3D native structure, input guidance, and training supervision, resulting in faster convergence speed and better geometric details.

Weaknesses:
- Pixel-based 2D methods (e.g., LGM) can preserve thin details, while 3D-based methods often smooth these details. How do you justify that? For example, in Figure 3 Column 4, the loose thread of the toy is captured by LGM, while MeshFormer ignores it.

- The proposed name ""VoxelFormer"" seems improper to me. It seems more like a 3D UNet with a deep bottleneck composed of multiple transformer layers.

- The projection-aware cross-attention layer projects 3D voxels onto the m views to interpolate m RGB and normal features. However, in the object case, one 3D voxel usually only corresponds to one view (due to occlusion). This cross-attention is projection-aware but not truly 3D-aware. Have you tried some occlusion-aware attention in your sparse model? Since you already have the coarse structure of the object, it could be used to filter out unneeded features.

- According to Table 3 (d), you mention ""we replace the cross-attention with simple average pooling and observe a significant performance drop."" Could you also try max-pooling? Additionally, do you concatenate the 3D feature voxel at every level of the network, as done in One-2-3-45++?

Limitations:
The authors already include limitations and broader impact in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xSU27DgWEr;"REVIEW 
Summary:
This study addresses the gap in the theory and algorithms of unsupervised domain adaptation based on f-divergence proposed by Acuna et al. 2021. Specifically, while the theory uses absolute values, the algorithms do not, and this issue is resolved by introducing a single scaling factor. The newly proposed f-DD generalization bound is derived based on Rademacher complexity, and tighter bounds are obtained using the localization technique.

As a specific domain adaptation algorithm, an adversarial type algorithm is proposed, yielding favorable results in benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This study bridges the gap between theory and practice in existing UDA methods based on f-divergence, advancing the foundational research in domain adaptation. Furthermore, the derivation of sharper bounds using the recently introduced localization technique for DA is highly commendable as a contribution to the theoretical framework of DA.
The authors validate their theoretical contributions with empirical results, showing superior performance on popular benchmarks.

Weaknesses:
While the empirical validation is strong, it is limited to specific benchmarks. Broader validation across diverse datasets and tasks would strengthen the findings. It is nice to present some insight into what kind of dataset the proposed f-DD works well (and why), and also into what kind of dataset it does not work well (and why).

Limitations:
Not applicable.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the learning theory aspect of the domain adaptation problem, where the key is to bound the estimation errors between expectations over shifting distributions. Specifically, this work improves the recently developed $f$-divergence-based generalization analysis, where the main results ensure a tighter generalization upper bound and the consistency between theory and method. For finite sample setting, a sharp bound is provided to accelerate the asymptotic rate. Numerical simulation is conducted to demonstrate the superiority of the theory-guided method over the existing discrepancy-based framework.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ The motivation is clear, i.e., improving the $f$-divergence-based bound and bridging the gap between method and theory, and the presentation is easy to follow.
+ The technical part is generally sound and the justifications are sufficient.
+ The experiment results are superior compared with recently developed generalization bounds.

Weaknesses:
+ Some notations are inconsistent in theoretical analysis.
+ The proposed algorithm needs further justifications.
+ The experiment comparison could be improved.

Limitations:
The limitations are discussed in the checklist, and there seems no potential negative societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to develop an improved version of f-divergence-based unsupervised domain adaptation (UDA) learning theory. In particular, the authors introduce a novel f-divergence-based domain discrepancy measure (f-DD) by combining the two existing concepts, which are f-divergence and domain discrepancy. Based on that f-DD measure, the paper next provides a generalization bound on the target domain, which is shown to be sharper than the existing related bound. The experimental results consistently demonstrate that f-DD outperforms the original f-DAL in three popular UDA benchmarks, with the best performance achieved by Jeffereys-DD.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written and easy to follow. The idea of introducing f-divergence-based UDA, targeting a better risk-bound on the target domain is novel and interesting. All the main statements of the paper are theoretically supported, though I did not have enough time to verify all of those propositions/theorems carefully. 

The experimental results consistently demonstrate that f-DD outperforms the original f-DAL in three popular UDA benchmarks, with the best performance achieved by Jeffereys-DD.

Weaknesses:
The novelty of the paper is quite limited since the f-divergence-based domain discrepancy measure (f-DD) is proposed by combining the two existing concepts, which are f-divergence and domain discrepancy. 

In Theorem 5.2, the authors claim that the application of the localization technique gives a fast-rate generalization, they do not provide a concrete evidence. Could the author give some explanations/clarifications for that. 

Moreover, the experimental part of the paper seems not be very convincing since it only provides experiments with quite small datasets (Office31, Office-Home, MNIST & USPS) and simple model (e.g., Lenet). It raises the concern about capability of f-DD in more complicated settings with large datasets and backbone network.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper improves the theoretical foundations of UDA proposed by previous work, named f-DD. By removing the absolute value function and incorporating a scaling parameter, f-DD yields novel target error and sample complexity bounds, allowing us to recover previous KL-based results and bridging the gap between algorithms and theory presented in Acuna et al. Leveraging a localization technique, this paper also develops a fast-rate generalization bound. Empirical results demonstrate the superior performance of f-DD-based domain learning algorithms over previous works in popular UDA benchmarks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
1) This paper holds significant theoretical significance in the field of UDA (Unsupervised Domain Adaptation);
2) The proof of the theorem is very solid; 
3) The experiments are also sufficient.

Weaknesses:
1) The readability of the paper is poor. It is almost entirely composed of definitions, remarks, lemmas and theorems, lacking a figure to introduce the motivation of this paper and explain why the improved framework is effective. 2) It is difficult to reproduce the results, as the training objective (5) is very abstract and unclear how to implement it experimentally. 3) This paper requires a substantial foundation of reading other papers in order to be understood.

Limitations:
Please refer to the weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, new expected risk analysis based on f-divergence is provided for the unsupervised domain adaptation problem. Although there are prior researches on expected risk analysis based on f-divergence, several issues have been pointed out, such as the fact that the variational representation of f-divergence used in these studies does not recover the Donsker-Varadhan representation of KL-divergence, and the use of the absolute value of the variational representation as a measure of domain discrepancy. 
In this paper, to address these issues, the authors adopt an alternative variational representation of f-divergence and, based on this, provide an upper bound evaluation of the expected risk in the target domain, namely ``target risk $\le$ source risk + marginal domain discrepancy + joint error''. Additionally, a sample approximation version of the derived upper bound is also provided, allowing it to be estimated from the data  (excluding the joint error part, as in conventional bounds).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper clearly discusses what are difficulties  with the conventional DA theory using f-divergence and explains how it is solved by the proposed approach. Especially, this paper provides a solid theoretical foundation, with detailed assumptions and rigorous proofs that are well-documented in the appendix. 

- Previous expected risk bounds in UDA have often been given by relatively simple inequality evaluations, following the formulation given by Ben-David et al. In contrast, a similar upper bound evaluation using the f-DD proposed in this paper requires an inequality evaluation for ``change of measure"" (as given in Lemma 4.1), and it can be seen that this is not an incremental extension of the conventional DA theory.

Weaknesses:
- I don't think there is enough information needed when trying to calculate the derived upper boundary from the sample. For example, $t_0$ in Lemma 4.2 and the construction of the Rashomon set in Sec 5 should be discussed in more detail.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xRdpCOdghl;"REVIEW 
Summary:
The paper suggests a new sampling method for the labeled set of semi-supervised learning. This sampling method, termed RDSS, selects a set of examples that is both representative of the data, and diverse. The paper shows that using such a sampling function improves both freematch and flexmatch, and compares it against other sampling methods, and methods from AL and SSAL.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea of the paper is good, and is well supported by theory. The experimental setup does convince me that the suggested method is better than random sampling when picking the labeled set of SSL. However, a better comparison to previous works is required, see the Weaknesses section.

Clarity: The paper is clearly written, the idea is well presented and intuitive, and the paper is easy to read and follow.

Weaknesses:
Some of the claims made by paper already appeared in previous art. Specifically, [1] showed that ""traditional"" AL methods do not pick bad labeled sets for SSL when compared to random sampling. [2] showed that when the labeled set is particularly small, instead of traditional AL techniques, one should focus on labeling examples that are more typical and diverse, showing that such a method can drastically improve both AL and sampling techniques for SSL. [3] presented sampling strategy, showing that picking examples that are representative and diverse examples for the labeled set of  SSL improves it by a big margin in low-budget scenarios.

The proposed manuscript does not reference or compare to any of these works. This affects both the novelty, significance and quality of the proposed method: the novelty is somewhat more limited, as many of the ideas overlap with existing works. The significance of this work is impacted, as while the problem at hand is important, it is unclear if the presented ideas pose significant advancement over the existing methods, and the quality is diminished, as a lot of comparisons are missing in the experimental setup.

Specifically, any low-budget strategy could be potentially applied to SSL as well, so those methods should be compared against as well. See for example [4], [5].

Additionally, the vice-versa argument should also hold -- if AL methods can be applied in this case, this method can be used as a method for picking labeled examples for active learning purposes and should be tested as such, as the literature in AL is much broader than the literature of picking the labeled set in SSL, which can provide a much wider context for the given work.

In addition, the framing of the paper is a bit unclear to me. I think the paper could benefit from explaining use cases in which one has the option to pick in advance the labeled set for SSL, which is not already covered by AL use cases.

-------

[1] Mittal, Sudhanshu, et al. Parting with illusions about deep active learning. (2019).

[2] Hacohen, Guy et al. Active learning on a budget: Opposite strategies suit high and low budgets. (2022).

[3] Yehuda, Ofer, et al. ""Active learning through a covering lens."" (2022).

[4] Mahmood, Rafid, et al. ""Low budget active learning via wasserstein distance: An integer programming approach."" (2021).

[5] Wen, Ziting, et al. ""NTKCPL: Active Learning on Top of Self-Supervised Model by Estimating True Coverage."" (2023).

Limitations:
not relevant

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a Representative and Diverse Sample Selection approach (RDSS) that utilizes a modified Frank-Wolfe algorithm to minimize a novel α-Maximum Mean Discrepancy (α-MMD) criterion, aiming to select a representative and diverse subset from unlabeled data for annotation. Experimental results demonstrate that RDSS consistently improves the performance of several popular semi-supervised learning frameworks and outperforms state-of-the-art sample selection methods used in Active Learning (AL) and Semi-Supervised Active Learning (SSAL), even under constrained annotation budgets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.This paper is in Well-written, logically organized, and smoothly expressed.
2. The presented results demonstrate the effectiveness of the proposed approach.

Weaknesses:
1. The author conducted tests on two baseline methods(FlexMatch [58] and Freematch [50]), but neither of them represents the current state-of-the-art.
2. Some details of the experiments are unclear, such as in Table 3.

Limitations:
as Weaknesses

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new sample selection method, RDSS, for the SSL task. RDSS considers both the representativeness and diversity of the selected sample and achieves state-of-the-art performance. This is achieved by the proposed α-MMD criterion and an efficient optimization algorithm GKHR.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. RDSS considers both representativeness and diversity of samples, which is a convincing strategy, and the experimental results also demonstrate the effectiveness of this motivation.
2. Sufficient theoretical analysis and experimental comparisons are conducted to demonstrate the effectiveness of the proposed method.

Weaknesses:
I would like to see images of the actual selected samples and visualizations of the feature distribution to demonstrate that RDSS indeed balances the representativeness and diversity.

Limitations:
Please refer to the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Choice of the labeled set in the semi supervised learning is critical for the final performance of the model. This problem can also be looked as AL with SSL, or single shot AL with SSL (in other words similar to experimental design). This works provides a way to select the seed set which is representative, as well as diverse. The problem is reduced to minimizing MMD and similarity score of the selected examples. The paper finally proposes a greedy algorithm, and compare the proposed method against various subset selection baselines, and AL.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I like the motivation of the problem and a neat theoretical derivation of the objective, and the provided theoretical analysis. Paper was also easy to follow and experiments are compelling.

Weaknesses:
- From a purely combinatorial point of view, I think that the final objective is supermodular in nature. Given the vast literature on submodular/supermodular functions, is it not possible to get an algorithm purely from that standpoint? If so, how different would it be from the proposed one? 

- Can one derive things such as leverage scores to detect the outlier-ness of a given point (or any other score)? If so, then couldn't one use something such as diversity - outlier score (or add a score that models likelihood) , with diversity such as Facility location function, and optimize the final objective using greedy? 

- In experiments I believe one of the strong baselines such as facility location function is missing. Facility Location has a rich history and have been used in several instances in Active Learning ([1, 2, 3, 4]). I believe authors can add a small discussion on FL and add that baseline. Furthermore, other diversity based approaches have also been considered in the past [5]

- Now a days a lot of focus is also for doing finetuning of existing CLIP models [3]. I'd appreciate one experiment on fine-tuning the CLIP models using the proposed method. 


References
- [1] Submodularity in machine learning and artificial intelligence
- [2] An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models
- [3] LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning
- [4] Deep Submodular Peripteral networks
- [5] GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning

Limitations:
Refer to the weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xRQxan3WkM;"REVIEW 
Summary:
This paper studies the implicit bias of the Adam optimizer for logistic regression on linearly separable data. The authors prove that Adam converges to the linear classifier with the maximum $\ell_\infty$-margin. This result contrasts with the classical results on (stochastic) gradient descent (with or without momentum), which converge to the maximum $\ell_2$-margin solution.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The authors theoretically study a popular yet not well-understood optimization method, Adam, in the context of a well-studied classical problem: logistic regression on linearly separable data. This offers a solid and insightful contribution to understanding Adam. In particular, distinguishing Adam from (S)GD with/without momentum on this classical problem is a very interesting result.
- The technical contributions are also of independent interest, as they prove the results for Adam without relying on the stability constant (which is closer to practice) and use mild assumptions.
- The paper is well-written and easy to follow. The proof sketch provides a clear and comprehensive overview of the proof of the main theorem.

Weaknesses:
There are no major concerns about this paper. Below are minor comments and some areas for improvement:
- The paper does not provide an intuition behind why Adam achieves the maximum $\ell_\infty$-margin solution, in contrast to GD which achieves the maximum $\ell_2$-margin solution. It would be great if the authors could offer insights on how the $\ell_\infty$-margin arises instead of the $\ell_2$-margin, for example, through a warm-up analysis with SignGD ($\beta_1=\beta_2=0$) or RMSProp ($\beta_1=0$). One way to provide an intuition is as follows: Gunasekar et al. (2018) proved that steepest descent converges to the max-margin solution, implying that SignGD (steepest descent w.r.t. $\ell_\infty$-norm) converges to the maximum $\ell_\infty$-margin solution. Since SignGD is known to be a good proxy for Adam, this may offer an insight into why Adam converges to the maximum $\ell_\infty$-margin solution.
- The authors claim that the bounds in Corollary 4.7 are derived under worst-case scenarios and argue that this is why, in practice, we often observe margins converging faster than the bounds in the corollary. However, this statement lacks supporting evidence. The paper should prove that the rate of convergence is tight. Otherwise, the observed faster convergence of margins in experiments might simply indicate that the bound is not tight enough.
- Some sentences, including those in the abstract, use the term ""convergence"" unclearly. For example, in the abstract, ""this convergence occurs within polynomial time"" does not indicate the objective (the normalized $\ell_\infty$-margin in this case) of convergence. This could be confused with other notions of convergence, such as convergence in direction (i.e., $\frac{w_t}{\lVert w_t \rVert} \to \frac{w^*}{\lVert w^* \rVert}$).
- (page 6, line 183) According to the paper, the normalized $\ell_2$-margin converges at a speed of $O(\log \log t / \log t)$ when using GD. However, this should be corrected to $O(1 / \log t)$. According to Soudry et al. (2018), the normalized weight vector converges to the maximum $\ell_2$-margin vector ""in direction"" with a convergence rate of $O(\log \log t / \log t)$, i.e., $\lVert \frac{w_t}{\lVert w_t \rVert} - \frac{w^*}{\lVert w^* \rVert}\rVert = O(\log \log t / \log t)$. However, the normalized $\ell_2$-margin converges at the speed of $O(1/\log t)$, i.e., $|\min \frac{\langle w_t, y_t \cdot x_t \rangle}{\lVert w_t \rVert} - \frac{\langle w^*, y_t \cdot x_t \rangle}{\lVert w^* \rVert} | = O(1/\log t)$.
- (page 1, line 25) Typo: reply on -> rely on

---
[Gunasekar et al. 2018] Characterizing Implicit Bias in Terms of Optimization Geometry, ICML 2018.

[Soudry et al. 2018] The Implicit Bias of Gradient Descent on Separable Data, JMLR 2018.

Limitations:
The paper discusses its limitations and future directions, including the extension of the results to homogeneous neural networks and the analysis of stochastic Adam instead of full-batch Adam. I think both directions are promising avenues for future research.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The main focus of this paper is on the implicit bias of Adam for a single layer linear model which performs binary classification on separable data. In particular, assuming a zero stability constant $\epsilon$, this paper reveals that Adam finds the solution that achieves maximum-$\ell_\infty$-margin and characterizes the convergence rate for different classes of learning rate. This implicit bias is different from the $\ell_2$-norm minimization solution obtained by previous work which does not assume $\epsilon = 0$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is clearly written and well-organized. It is easy and clear to follow the argument and motivation of this paper, e.g., the proof sketch makes it easy to follow the way how the theoretical conclusion is developed. In addition, to me, the introduction of the related works are comprehensive and clear. It also clearly summarizes the difference between this paper and related works.
- The settings and results of this paper are new compared to previous works, i.e., previous works showed an $\ell_2$-norm solution implicit bias of Adam on separable data while this paper reveals an $\ell_{\infty}$-norm implicit bias when the stability constant $\epsilon$ is zero.

Weaknesses:
Despite the novelty of the theoretical claims, I still have several concerns, which I will discuss in the following.

1. Removing the stability constant $\epsilon$ makes the approach of this paper fails to characterize the influence of it, which, though being small, still has non-negligible effect, e.g., [1] observed that Adam with an $\epsilon$ that is too small does not even converge in certain circumstances. Treating $\epsilon$ as 0 seems a bit rough to me. 

    In addition, [2] showed that Adam minimizes the interpolation norm of gradients that depends on magnitudes of various hyper parameters including the stability constant $\epsilon$ (although [2] did not specify the types of loss functions and model architectures). [1] claimed that Adam with nonzero $\epsilon$ converges to $\ell_2$-norm solution, which is also verified by extensive experiments. As a comparison, this paper showed that both Adam with $\epsilon=0$ and with a non-negligible $\epsilon$ do not converge to the aforementioned solutions (line 210). In this sense, it seems that the conclusion reached by this paper contradicts with those derived by [1, 2]. Therefore, in my view, it would be better to start with a non-zero $\epsilon$ and let the case with $\epsilon=0$ be a special case to better capture the effect of the $\epsilon$ on the implicit bias.

2. This paper only considers a simple setting: the model is only a one-layer linear model and there is no stochastic sampling noise which is typically necessary in practice. As a comparison, authors of [1] have already studied Adam on separable data for homogeneous models, which can cover the single layer model of the current work as a special case. Thus excluding the stochastic sampling noise in the current work is kind of unsatisfying to me since the model is already a simple one. In addition, I think that the authors of the current work should at least repeat the experiments conducted in [1] (such as those for homogeneous neural networks) to further support their theoretical claims, especially considering that the authors claimed in line 210 that their results are more accurate than those of [1].

**Reference**

[1] Wang et al. The implicit bias for adaptive optimization algorithms on homogeneous neural networks.

[2] Cattaneo et al. On the Implicit Bias of Adam.

Limitations:
I do not find a separate limitation section in the main part. In my view, removing the stability constant is a bit rough. This makes the approach presented in this paper fail to capture how the implicit bias of Adam changes for different values of stability constant.  

The societal impact is not applicable to this work as it focuses on theoretical parts of Adam.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper examines the implicit bias of the Adam optimizer in the context of linear logistic regression, demonstrating that it converges to the maximum $\ell_\infty$-margin solution under certain mild conditions. The authors note that omitting the stability constant in Adam updates results in a different implicit bias than gradient descent, with or without momentum, which converges to the maximum $\ell_2$-margin solution. They also explore various decreasing learning rates, showing that Adam's margin converges at a polynomial rate, which is faster than that of gradient descent. Additionally, they provide numerical experiments that support their findings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Understanding why Adam performs better than GD in several settings is an important problem and this work takes an important step towards this by showing that Adam has a different implicit bias than GD in the linear logistic regression setting.

- Overall, the paper is well-written and easy to follow. The proof sketch in Section 6 is explained well.

Weaknesses:
- The paper does not present results for a fixed learning rate and only considers a set of decreasing learning rates.

    - The discussion in lines 50-52 and after Corollary 4.7, comparing the rates of Adam and GD, should also comment on the convergence rates for GD with adaptive learning rates (e.g., normalized GD) which have been shown to converge faster (see [1] and related work) than GD.

    - (Minor) In Assumption 4.3, ‘non-increasing’ should be ‘decreasing’ or ‘diminishing’.

- The results in prior work on implicit bias of GD are global (hold for any initialization), whereas the results in this paper require an assumption on the initialization (Ass. 4.2). Based on the discussion following this assumption, it might be better to state an assumption on the data and then show that the condition on the initialization holds as a Lemma.

- The paper does not comment on how optimal the obtained rates in Corollary 4.7 are.

**References:**

[1] Wang et al., Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling, 2023.

Limitations:
There is no potential negative impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the author studies the implicit bias of Adam optimizer for a single layer neural network on separable data. The author's work suggests that, compared to the implicit bias of gradient descent which is the max $ \ell_2 $ margin solution, Adam solution converges to the maximum $ \ell_\infty $ margin solution. For this work, authors take both exponential and logistic loss and find that the convergence speed is on a polynomial order. 

In order to confirm the results, the authors perform experiments on synthetic datasets for binary classification tasks and confirm Adam’s convergence to the $ \ell_\infty $ margin comparatively.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work is novel (to the best of my knowledge) and interesting as the study of implicit bias of Adam could have further implications in characterizing the difference in optimization behavior of Adam vs SGD in practical scenarios. The assumptions of the work have been clearly presented and seem reasonable. With regard to the $ \epsilon $, while theoretical results are not provided, the authors include convincing experimental illustrations to convince me of the assumption. I also appreciate the well written proof sketch which helps convey the ideas

Weaknesses:
At the moment, I have some concerns with the paper which are more fit to be discussed as questions.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xQWJBeK5rh;"REVIEW 
Summary:
The paper introduces the SICSM framework, integrating Selective State Space Models (SSMs) with Generative Flow Networks (GFNs) to tackle challenges in dynamical systems characterized by irregularly sampled trajectories and partial observations. SICSM leverages the adaptive temporal modeling capabilities of SSMs to learn input-dependent transition functions, enhancing structural inference accuracy. It aggregates diverse temporal dependencies and channels them into a GFN to approximate the posterior distribution of the system’s structure. Extensive evaluations across multiple datasets demonstrate SICSM's good performance in accurately inferring complex interactions in partially observed systems.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The integration of Selective SSMs with GFNs is a novel approach that addresses significant challenges in structural inference for dynamical systems. The adaptive mechanisms for handling irregular sampling and partial observations are particularly innovative.
- The research is thorough and well-documented, with extensive evaluations across a variety of datasets. The methodological rigor and comprehensive experimental validation enhance the reliability of the findings.
- The paper is well-organized and clearly written, with detailed explanations of the methodologies and experimental setups. Figures and diagrams effectively illustrate the concepts and results.
- The proposed SICSM framework has broad applicability in scientific discovery and system diagnostics across multiple disciplines. Its ability to handle real-world complexities such as irregular sampling and partial observations makes it a valuable tool for researchers.

Weaknesses:
- The implementation of SICSM is computationally intensive, requiring significant resources and expertise. This complexity may limit its accessibility and widespread adoption.

Limitations:
The authors have adequately addressed the limitations of their work, including the challenges posed by irregular sampling and partial observations. They propose future research directions to explore dynamic systems with mutable structural elements, indicating a proactive approach to potential limitations. The discussion on incorporating prior knowledge and adapting to different hop distances further strengthens the framework’s applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to combine State Space Models and Generative Flow Networks to perform structural inference in an irregular time series context. The proposed method is evaluated on a series of different tasks where it performs well, and compared to a number of baselines. The method's robustness to short time series and missing observations is evaluated.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proposes an interesting architecture and solves problems that have the potential to be very relevant in real world contexts, such as biological time series. The empirical evaluation is fairly thorough about testing on many different tasks.

Weaknesses:
My main concerns for the paper are its low novelty and its low number of ablations, which make it hard to understand how specific pieces contribute to the performance of the method.

Generally I'm uncomfortable with the way many things are presented in the paper, it's not always clear what's a novel contribution and what's not. I encourage the authors to be clear and exercise an abundance of caution. 

/!\\ In my humble opinion this paper uncomfortably downplays its similarity to DAG-GFN [14] and JSP-GFN [15] in several places, and I'm not even an author of these papers. This is especially concerning considering that in many instances JSP-GFN is the closest performing baseline to the proposed method.

Limitations:
Adequate.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors consider the problem of structure learning of dynamical systems from irregularly sampled trajectories and partially observed systems. They propose Structural Inference with Conjoined State Space Models (SICSM), a method based on selective state space models (SSMs) and generative flow network (GFNs). The central idea of this work is to use a SSM for modelling the behaviour of dynamical systems while using a GFN to learn the interacting graph structure between the variables of the system. The authors evaluate their proposed approach on a comprehensive set of datasets for various tasks and compare against a numerous baselines.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors present a method that addresses a challenging problem in the domain structure learning of dynamical systems -- i.e. learning system structure from irregularly sampled trajectories and partially observed systems. The use of SSMs to approximate system dynamics while using GFNs to learn the graph structure of the system is unique and novel approach to this problem. The authors provide a comprehensive evaluation of their method over variety of systems for irregularly sampled trajectories and partially observed systems, demonstrating SICSM consistently outperforms counterpart approaches.

Weaknesses:
- The method has 3 key components: state space model, embedding residual blocks, and a GFN to approximate the graph structure of the system. It is not entirely clear how these individual components interact and the explicit need for the GFN (see questions below). 
- The authors consider a comprehensive set of datasets and baselines, but only one evaluation metrics (AUROC). For example, some other metrics to consider for this task are: structural hamming distance (SHD), F1-score, area under the precision-recall curve (AUPRC). Only considering one evaluation metrics makes it difficult to assess the robustness of the approach.
- Another method that seems relevant to this work which address an similar problems is CUTS (Cheng et al. 2023). It appears that majority of the baselines considered in this work are or not necessarily methods explicitly tailored to handle irregular time-series. Including a method like CUTS in this evaluation may be important to create a fairer comparison of SICSM. 

References:
Cheng, Yuxiao, et al. ""Cuts: Neural causal discovery from irregular time-series data."" International Conference on Learning Representations (2023).

Limitations:
The authors discuss limitations and broader impacts in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Processes of scientific interest which are representable as graphs, in biology, chemistry, material sciences, mechanics, are an important application for machine learning. Nodes often represent physical objects, some of which influence each other. Nodes exhibit a set of features which can be observed over time. Prior knowledge about the process stems from a mechanistic understanding and can often be represented as the presence or absence of edges between nodes. Node feature observations may be irregularly spaced through time; not all nodes may be observed with every observation. 
This paper develops a statistical model for this application with support for irregularly sampled and partial observations of node features, as well as prior knowledge incorporation. Prior knowledge is restricted to the indication of presence, but not absence, of edges. Partially observable nodes are assumed to be from a static node set throughout all observations (i.e., nodes are either always observable or always unobservable). Observations are not assume to contain a timestamp indication (as in mobile phone accelerometer readings, which may be irregularly sampled but whose timestamp is read at input).
The model's architecture is relatively sophisticated and is based on generative flow networks to represent and learn the structural aspects of the graph, and state space models to represent the evolution of node features over time. 
The paper presents experiments on 16 datasets stemming from 4 physical models, and compares to 7 other models, showing superiority in scenarios where observations are irregularly spaced or nodes partially observable.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper takes an established problem class (graph systems) with its known challenges (irregular sampling, partial observations), which is not original. However it goes to great lengths to make use of two strong methods, GFN and SSM, with a resulting combination that seems reasonable, strong and of useful application.
The paper is generally clear, notations are coherent and legible, several diagrams support the explanation. To improve the writing, a running example might help bridge the abstractions (node, edge, state...) to physical reality, illuminating and motivating the implementation. The same goes to comment on the connection between the model and the applied datasets (some of this is covered in Annex C, with the exception of C.5 which leaves the physical counterparts of modelled data undescribed).

Weaknesses:
Experimental validation is moderately convincing. Baseline implementations seem strong, with care taken to recover implementations of competing methods, as documented in Annex D. However, all datasets are synthetic. The only real dataset, PEMS, presented in Annex C.5, with results in Annex E.2. In addition, experimental validation seems unconcerned with performance outside the specific cases of partial observations or irregular sampling -- reducing the paper's claim to ""this model is better for these two scenarios only"".
There seem to be a duplication in the presentation of datasets (both in sec5.1, between the paragraphs starting l.279 and l.290, and again between Annex C.1 and C.2 vs C.4) -- this is confusing. Also, sec3.3 seems to be internally redundant with duplicated points (e.g. l.151 vs eq3, and l.148 vs l.157, which again is confusing. Numerous sentences have incorrect English syntax which obscures their meaning

Limitations:
* Checklist point 4 and 5: implementation link is claimed to be provided here and in Annex l.863, but I don't see it. It should be provided in the main paper since the supplementary can't be assumed to be reviewed.
* Limitations: a few more assumptions on the usage scenario should be spelt out, as mentioned in this review
* Checklist point 7: It is certainly possible to report error bars on plots through shading, provided they are not as tiny as you make them here. In addition, error bars could be reported, without lengthening the main paper, in the Annexes -- but they aren't, with the only exception of Table 3 on an experiment which is not reported in the main paper.
* Checklist point 6: experimental settings are not as detailed as that they would allow reproduction. Several details are missing for this, e.g. batch size, data splits.
* Checklist point 12: the claim, and requirement of the checklist that assets have their license mentioned is not complied with regarding either datasets or existing code for competing methods. Despite the claim, this point mostly is not complied with.
* Checklist point 13: does this imply the code is not intended to be released as an asset?

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xOCAURlVM9;"REVIEW 
Summary:
This paper presents a novel 3D object retrieval method. First, to facilitate this task, the authors build 3 datasets for training and evaluation, which may significantly benefit the community. Then the paper propose the Isomorphic Assembly Embedding (IAE) and the Structured Fuzzy Reconstruction (SFR) modules, which are designed to generate assembly embeddings with geometric-semantic consistency and overcome the distribution skew of unseen categories. Besides, HIConv is proposed to capture high-order correlations within and among objects. Extensive experiments show that the method achieves sota performance.

Soundness:
3: good

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
1. This paper builds 3 datasets for the task, which may facilitate future research. 
2. The paper proposes several novel modules to capture the part-level and inter-object features for object retrieval.
3. The task itself is important in shape understanding.

Weaknesses:
1. No visualization results.
2. The presentation is hard to understand. There are quite some complex equations, like Eq 2 and Eq 4. Please briefly explain what they mean and how they work.
3. In Fig. 1, it shows that intra-object features are extracted before inter-category features. But in Fig. 2, I only see Inter-object features? It's hard for me to match them up.
4. I still don't understand the input. So you need dense point cloud with ground truth 3D part segmentation as input, right? If the segmenation is not perfect, will the method collapse? if the point cloud undergoes SE(3)-transformation, will the method collapse? Can this method handle partial point cloud input, like the point cloud back-projected from depth map?

Limitations:
The authors didn't discuss the limitations

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The manuscript introduces a framework (HAFR) for addressing the challenge of open-set 3D object retrieval. The authors propose a bottom-up approach focusing on part assembly, leveraging both geometric and semantic information of object parts to enhance retrieval performance across categories, including those unseen during training.

The HAFR framework consists of two main modules: Isomorphic Assembly Embedding (IAE) and Structured Fuzzy Reconstruction (SFR). The IAE module utilizes Hypergraph Isomorphism Convolution (HIConv) and assembly auto-encoders to generate embeddings with geometric-semantic consistency. The SFR module tackles distribution skew in open-set retrieval by constructing a leveraged hypergraph based on local and global correlations and employs a memory bank for fuzzy-aware reconstruction.

The authors have created three datasets, OP-SHNP, OP-INTRA, and OP-COSEG, to benchmark their approach. Extensive experiments demonstrate the superiority of HAFR over current state-of-the-art methods in open-set 3D object retrieval tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a method for open-set 3D object retrieval that cleverly integrates part-level information using hypergraphs, which is a unique and promising direction in the field. The HAFR framework is well-thought-out, with clearly defined modules (IAE and SFR) that address different aspects of the retrieval task, from assembly isomorphism to distribution skew mitigation.
- The construction of three new datasets with part-level annotations provides a valuable resource for the research community and supports the validation of the proposed method.
- The methodology is clearly described, and the algorithms are well-structured, making it relatively easy for readers to follow the technical contributions.
- The paper is well-written and easy to follow.

Weaknesses:
- The paper does not address scenarios with varying numbers of parts per object. Expanding the framework to handle flexibility in the number of parts could improve its applicability.
- The manuscript could benefit from a discussion on the computational complexity and efficiency of the proposed methods, especially when scaling to larger datasets or higher-dimensional part features.
- Why not evaluate on the PartNet(https://partnet.cs.stanford.edu/)?
- Although the paper claims state-of-the-art performance, they do not achieve the best (SDML is the best on OP-COSEG for NDCG metric), what is the reason?
- Some implementation details, such as network architecture specifics and hyperparameter settings, could be better elaborated to ensure reproducibility.
- The paper mentions that data and code will be made available upon acceptance, which is good practice. - However, providing this information upfront or during the review process could enhance transparency and reproducibility. For the three datasets, the detailed construction is missing and encourages the authors to publicize the data, facilitating the community.
- The limitations and failure cases should be discussed comprehensively.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to utilize the part-assembly representation method to mitigate the distribution skew of unseen categories, enhancing the generalization performance for open-set 3D object retrieval. Compared to previous methods, this paper benefits from part-level representation learning rather than object-level representation, obtaining in a good generalization on unseen categories. To utilize the part-level representation, this paper introduces Isomorphic Assembly Embedding (IAE) and the Structured Fuzzy Reconstruction (SFR) modules. The former can generate the assembly embedding isomorphically for each object, and the latter is used for generating the fuzzy representation thus overcoming the distribution skew of unseen categories.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The problem is well-motivated and the solution seems working well. The results are good. The paper also contributes three 3D point cloud datasets with multiple part annotations for benchmarking. Extensive experiments on the three benchmarks demonstrate the superiority of the proposed method over current state-of-the-art 3D object retrieval methods.

Weaknesses:
1. The datasets OP-INTRA and OP-COSEG mentioned in the paper may have limitations in category diversity, number of parts, and dataset size, which may affect the generalization ability of the model.
2. The framework comprises many sub-architectures, such as the HIConv layer, multiple auto-encoders, fuzzy embeddings, and memory bank, it seems to be relatively complex. However, this paper does not explicitly discuss the computational efficiency of the model, including training and inference time, and computational cost.
3. Though the paper proposes a solution to the open set problem, the datasets are all virtual. Its generalization ability to unseen categories in real-world applications still needs further verification.
4. The ablation studies show the effect of the HIConv layer. However, only comparisons with MLP and GIN are performed, but no comparisons with other neural layers such as KAN, nor is the number of HIConv layers ablated.
5. The experiments are only conducted on the proposed datasets. The generalization ability of the model on a wider data distribution requires more verification. It would be better to add some experiments on previous public datasets or datasets without open-set settings to demonstrate generalization capabilities.

Limitations:
The paper should add discussions on limitations and possibly show some failure cases.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method for finding similar samples from a set of 3D objects given query objects in an open setting, where objects can belong to both already seen and new categories. This method is based on considering 3D objects as hypergraphs consisting of individual geometric and semantic parts of objects. The hypergraph is used to form Isomorphic Assembly Embedding. The second part of the proposed HAFR framework is the Structured Fuzzy Representation module that constructs a hypergraph based on local certainty and global uncertainty correlation to enable transfer from seen to unseen categories. The authors propose a new layer, HIConv, which improves the quality of the generated representation. The authors demonstrate the effectiveness of their approach on three datasets that they constructed for this task.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The idea that one can understand the whole object shape from its parts sounds interesting and reasonable.
- The description of Isomorphic Assembly Embedding and Structured Fuzzy Reconstruction is formal and rather clear.
- The authors conduct extensive ablation studies of their method.

Weaknesses:
- Based on the provided experiments, it is unclear if HAFR can generalize well to an unseen domain. Are the results in Table 2 provided for the same suite of model weights?
- The literature review does not include existing methods for open-set 3d object retrieval and recent methods for closed-set 3d object retrieval.
- When comparing with other methods, the authors use their own modification of existing multimodal methods. A comparison with modern methods for open-set 3d object retrieval, such as [1], is necessary to demonstrate the effectiveness of this particular method of object representation.
- The method's description lacks an explanation of how the resulting fuzzy embeddings are used to find similar objects. Additionally, the description contains undefined concepts like isomorphism loss and integration function. If these concepts are not introduced by the authors, please include references to articles where they are defined.

[1] Zhou, J., Wang, J., Ma, B., Liu, Y. S., Huang, T., & Wang, X. (2023). Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773.

Limitations:
The authors discuss limitations in the conclusion regarding the use of the assembly fuzzy representation for a varying number of object parts. In my opinion, another limitation is the need to segment the point cloud into parts to use this method.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a framework for open-set 3D object retrieval, called the Hypergraph-Based Assembly Fuzzy Representation (HAFR) framework. This model leverages an Isomorphic Assembly Embedding (IAE) to integrate geometric and semantic consistency. Furthermore, a Structured Fuzzy Reconstruction (SFR) is used to overcome the distribution skew of unseen categories. On three point cloud datasets constructed by the authors, this model outperforms the state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The motivation for this work is well-established.
- The idea of using hypergraph structures to achieve high-order correlations both within and between objects is novel.
- Sufficient quantitative and qualitative comparisons verify the effectiveness of the proposed model.

Weaknesses:
- In structured fuzzy reconstruction, the value of k in the k-nearest neighbors seems to determine the global uncertainty hyperedge. However, the paper lacks explanation or experiments to clarify the selection of k value.

- While HGM2R [1] employs a multimodal approach, the IAE component appears to be similar to the Multi-Modal 3D Object Embedding in HGM2R. What are the differences and unique contributions of IAE compared to the embedding technique used in HGM2R?

-In Table 2, although HGM2R also utilizes hypergraphs, it shows only slight improvements over previous methods in most metrics. For example, the mAP scores on three datasets are only about 0.1 higher. However, the method proposed in this paper demonstrates a significant improvement over HGM2R on the OP-COSEG dataset, with an increase of nearly 0.6. How can this result be explained?
[1] Hypergraph-Based Multi-Modal Representation for Open-Set 3D Object Retrieval. TPAMI 2023.

Limitations:
The authors discusseds the limitations in the conclusion section. But I did not find the societal impacts mentioned in the conclusion section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xO9GHdmK76;"REVIEW 
Summary:
This work proposes a novel approach for enhancing neural network performance by scaling feature interaction spaces to infinite dimensions using kernel methods. Recent advancements have introduced feature interaction spaces, but these are often limited to finite dimensions, primarily through element-wise multiplications. To overcome these limitations, the authors propose InfiNet, a model architecture leveraging the Radial Basis Function (RBF) kernel to enable infinite-dimensional feature interactions. Finally, the authors provide several empirical results on standard vision tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This work provides an interesting generalization of feature-feature interactions via kernels. For the best of my knowledge, this is a novel idea that appears to perform well in practice. However, I am not overly familiar with the current state of the field of deep learning for computer vision. It further provides several larger-scale experiments and interesting ablations.

Weaknesses:
* there is no theoretical justification that increasing the dimension of the feature-feature interaction space will lead to better generalization. The paper does a good job analysing this question with ablations. However, this remains an open theoretical question.

* I understand that the motivation for this work comes from applications in computer vision. However, since a major focus in this paper is on comparing the proposed approach to self attention, it would be interesting to not only test this method on images, but also on language. 

* the method is reported to have lower FLOPs on average than competing methods. Why is that? Is that a major drawback of this method?

* performance improvement on ImageNet is only marginally. In many cases the proposed method even performs worse than competing methods.

* paragraph starting in line 148: this is on over-claim and has to be removed or rigorously proved. It is not clear how a higher order of $k$ implies better generalization or training. Unless shown in this paper or referenced from another paper, this has to be removed.

Minor: 

* line 28: more context for formulating self attention that way has to be provided. It is explained in more detail only at the end of section 3. 

* caption of figure 2: there is '?'. Moreover, a description of the presented images should be included. What is shown in Figure 2 on the right hand side? This is only explained in the main text,not the caption. This needs to be changed.

* figure 2, first image on the left: hard to read -- text overlaps with drawing.

Limitations:
* The paper provides empirical results only on vision tasks. However, a major selling point of this paper is generalizing approaches like self attention in terms of feature-feature interactions. Therefore, comparisons with transformers on language tasks should be performed. 

* No theoretical analysis is provided proving that the proposed method leads to better generalization.

* The method appears to have on average lower FLOPs than competing methods, while at the same time only marginally outperforming (or even performing worse than) competing methods on imageNet.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies placing a kernel function inside of a neural network architecture to facilitate interaction of features/dimensional expansion. They consider deep convolutional networks with parallel pathway features $x$ and $x'$ and a kernel function computed with both pathways' features as inputs $k(x,x')$. Standard kernel mathematics is used to explain feature expansion. The main novel results are empirical performance of these ""InfiNet"" architectures, which are shown to perform well in a number of computer vision tests.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea of unifying different orders of interaction embodied in various neural network architectures, including Transformers is appealing and probably important. The accuracy of the InfiNet experiments is impressive, with a moderate reduction in FLOPs. The paper is easy to read and well-organized, although suggestions are given for how it could be improved.

Weaknesses:
My main concerns with the paper are a lack of context for the approach as well as missing important explanations. I also think a good amount of the math that's included could be considered ""filler"" material that could go into the appendix, since it doesn't represent new results. (I am referring to sections 4.1 and 4.2, most of which can be found in most textbooks which cover kernel methods.)

* Notation which is commonly used in the paper $\oplus$, $\otimes$, * is not explained. You should _explicitly_ define it somewhere, at least in the appendix (and refer people there). In particular, people may be confused by * for elementwise/Hadamard multiplication, since in convnet literature this is often the convolution operator. You call this the ""Star Operation"" in line 124, but I think it is just elementwise multiplication.
* The authors seem to have missed the vast literature on the connections between random features, neural networks at init, and kernel methods. (CKNs are mentioned but without any discussion of the topics I mention here.) In particular, one way that you could approximate the InfiNet architecture would be to take the two feature streams and pass them each into the same wide, random network/layer and compute the dot product of features at the next level. That would only approximate the kernel function in the InfiNet architecture, and is likely less efficient, but it provides a way to perform dimensionality expansion with a more traditional layer. The authors should discuss these connections.
* Different order of interactions have been studied in random feature and kernel settings already. In random features, interaction order is connected to the sparsity of weights, see e.g. https://arxiv.org/abs/2103.03191 and https://arxiv.org/abs/1909.02603. In kernels, this were referred to as additive kernels https://arxiv.org/abs/1602.00287, also studied in multiple kernel learning https://arxiv.org/abs/0809.1493 (these are just some examples among a larger literature).
* The authors do not seem to want to release their code. They have said ""Yes"" on Question 5, stating that the code and data are open, but there is no link or indication in the text that the code is available or will be when the paper is published. That seems deceptive.

Limitations:
I would strongly prefer that the limitations be included in the main text during the discussion. With movement of some of the standard math, there would be space.

The authors only consider the squared exponential kernel with bandwidth parameter equal to 1. Other kernels might work better. In particular, the effective dimensionality of the RKHS (related to the kernel decay rates) would be higher with a ""less smooth"" kernel like the exponential/Laplace kernel.

The results are likely not reproducible unless the authors release their code.

The results are also limited only to supervised vision tasks, rather than other modalities or unsupervised settings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a new architecture for computer vision applications that models high-order interactions between features. The architecture is similar to an attention block, but introduces an RBF Kernel layer that captures interactions of order higher than two. The resulting method has strong empirical performance across image classification tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of the paper is very interesitng and novel.
- The empirical results show promising performance across involve multiple tasks against sophisticated methods

Weaknesses:
- The presentation of the method seems overly complex in some places. For example, providing a clearer explanation of each new layer (perhaps in pseudocode) would help. While the Infiniblock definition is clear, the reader needs to go back to the previous section to understand the input/output shaped of the RBF layer, which takes work, and can be made simpler. Making clearer the intuition behind high-order interactions would be helpful as well. Showing examples of what the model learns would be helpful to make things concrete.
- The empirical performance is reasonably similar to those of previous methods, hence the empirical improvement is not that large.

Limitations:
I do not see any ethical and societal implications of the work that need to be discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper shifts the focus from traditional neural network design, which emphasizes feature representation space scaling, to feature interaction space scaling. It introduces a new model architecture, InfiNet, that enables feature interaction within an infinite-dimensional space using the RBF kernel, leading to state-of-the-art results. The paper also discusses the limitations of current models in capturing low-order interactions and proposes the use of classic kernel methods to engage features in an infinite-dimensional space.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The idea of the paper is simple, novel and well exposed. 

- The paper introduces InfiNet, a model architecture that leverages infinite-dimensional feature interactions using RBF kernels, which enhances model performance of traditional models.

- InfiNet achieves new state-of-the-art performance in various tasks, demonstrating the effectiveness of infinite-dimensional interactions.

- The paper includes extensive experiments on datasets like ImageNet and MS COCO, showing the scalability and efficiency of InfiNet.

Weaknesses:
- the paper builds on the simple use of kernel methods. The novelty of the methods is minimal, in the end it is an RBF kernel.

- the performance improvement of Infinet over other models is mostly marginal and no errors have been displayed.

- the paper doesn't really have theoretical novelty

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xNncVKbwwS;"REVIEW 
Summary:
This paper introduces methods for constrained OCO
which automatically achieve the optimal rate without knowing
in advance whether the losses are convex, strongly convex,
exp-concave, or smooth, while using only 1 projection per round.
This is notable because the standard approach proceeds by combining
several expert algorithms with a meta algorithm; in constrained settings
these expert algorithms require implementing a potentially expensive
projection. This work avoids projecting each of the expert algorithm
iterates leveraging the constrained-to-unconstrained reduction of Cutkosky 2020.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper addresses a clear and real problem that has been left unaddressed
by the majority of literature on this topic. The approach is a pretty straight-forward
modification of existing reductions, but uses them in a new and unexpected way.

Weaknesses:
The main weakness is that the paper feels poorly factored. There is
a very large number of back references to previous equations, and the paper would be
very hard to read in print. To actually follow the math, it's almost necessary to
read the paper with a pdf viewer which can display pop-up previews when hovering over links.
I think this would be remedied by better factoring the results into lemmas and propositions.

As noted, the approach is a fairly straight-forward modification of the results from
Cutkosky \& Orabona (2018) and  Cutkosky (2020), and essentially boils down to
not dropping negative terms in the analysis, and then exposing matching terms
in the regret decomposition. I think this is fine overall; these considerations
are missing from the literature, and this is a fitting
place for them to enter the literature.

Limitations:
Yes, the paper points out in the conclusion that the bounded domain / gradient assumption is
a significant limitation that they hope to address in future work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of online convex optimization with unknown smoothness properties of the loss functions, which can be convex, strongly convex, or exp-concave. The authors propose an algorithm that achieves regret bounds of order $\sqrt{T}$, $\log T$, and $d \log T$ respectively, while requiring only a single projection step per round on the original domain $\mathcal{X}$. Such projections can indeed be computationally expensive. Additionally, the authors present regret bounds with improvment for small losses.

Most algorithms that achieve similar adaptive regret upper bounds rely on meta-algorithms that combine experts (running ONS or OGD with surrogate losses), inspired by the MetaGrad algorithm. Typically, these algorithms necessitate $\log(T)$ projection steps per round (one per expert), which can be computationally burdensome. Mhammedi et al. (2019) reduced this projection cost to $O(1)$ but at the expense of a $d \log T$ regret for strongly convex losses. To overcome this, the authors introduce new surrogate losses based on a black-box reduction technique by Cutkosky et al. (2018), which simplifies the constrained optimization problem on $\mathcal{X}$ to another domain, such as the Euclidean ball, where projections are easier.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-written and offers valuable insights into the use of surrogate losses to adapt to strong convexity or exp-concavity. It may serve as a comprehensive entry point into the extensive literature on universal OCO algorithms.
- Despite combining various existing techniques, the results are non-trivial and required solving technical challenges, especially for the strongly convex case. The authors introduce novel negative terms in the analysis to achieve their results.
- Experiments included in the appendix demonstrate that the computational improvements can be significant.

Weaknesses:
- The theoretical improvements may appear incremental, appealing to a niche audience interested. The improvement being only in the specific case of strongly convex $\log T$ regret with $O(1)$ projection steps. The primary high-level ideas in the algorithm and analysis are based on prior work.
- The paper still relies on a meta-aggregation procedure, which, although theoretically effective, is not particularly elegant and maintains a per-round complexity of order $O(\log T)$. Achieving $O(1)$ complexity per round seems however highly challenging.
- The convex rate is actually $O(\sqrt{T \log\log T})$, not $O(\sqrt{T})$ as stated in the results.

Limitations:
The authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies universal OCO algorithms with fewer projections. Previous work either use $O(\log T)$ projections per round, or have a sub-optimal dependence on $d$ for strongly-convex loss. This work designs a new surrogate loss to achieve tight regret for Lipschitz convex/exp-concave/strongly-convex losses simultaneously, with only 1 projection per round.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The technical contributions are solid: this paper makes a strict improvement over previous results.

The paper is very well-written, clearly introducing the challenges and the main ideas. Details of the analysis and algorithm are nicely explained.

Weaknesses:
The contribution seems somewhat incremental to me. The only improvement is a $d$ factor for strongly-convex loss. Such result is nice to know but I'm not sure how significant such it is. In addition, the technical novelty isn't significant either.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
x9eFgahVBI;"REVIEW 
Summary:
The paper basically presents three theoretical analyses related to ICL. The section 2 shows that we can use CBOW to do the (country)-(capital) kind of ICL. The section 3 shows that positional embeddings, multiple layers in autoregressive LM, and blocked noise structures are important for ICL. The section 4 shows that ICL could fail when there are systematic and consistent mismatches between the training sequence and testing sequence.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
I think this paper is easy to follow and most explanations are clear (One minor suggestion: it would be more clear to also illustrate the correct answer of each prompt and provide some brief explanations such as the prompt in section 3 tries to repeat the first letter of a word). I choose fair in the presentation rating because I feel that the paper oversells its contributions in the title and abstract.

All the claims are supported by both strong theoretical conclusions and empirical simulations. The theoretical contributions are novel to me but I am not a theoretical researcher. Since the situations/preconditions of the claims are extremely simple, I think its significancy is not high for practitioners, but the contributions might be significant for theoretical researchers and might inspire the follow-up work.

Weaknesses:
I think the main weakness of this paper is the mismatch between scope it seems to cover and its actual scope. The title and abstract suggests that this paper tries to study why the ICL works well given the unstructured training data in practice, but what the paper actually did is thoroughly studying 3 toy situations. 

I understand that we often have to simplify the situations in order to get strong theoretical conclusions. I also think that, at least to me, it is difficult to derive those theoretical results in such simplified situations and all the toy situations are relevant to the ICL. Nevertheless, I think these situations are not very representative to most of the practical ICL settings. After all, most ICL is beyond just relying the co-occurrence statistics of the sentences like CBOW, finding the first letter of the word, and repeating some words in the context. 

I understand that nowadays, one paper often needs to oversell its scope in the title and abstract to get attentions. Hence, although I suggest that the authors can revise the main storyline to reduce the overselling, I am fine with the current title and abstract if this paper is accepted at the end. I am also not a researcher who studies theory, so I do not know how significant or novel these theoretical results are. Therefore, I would like to let other researchers who have better theoretical background to rate this paper.

Limitations:
The limitation discussion in the appendix K is fair.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the emergence of in-context learning (ICL) in both CBOW (Mikolov et al,. 2013) and Transformer models. The focus is on simple synthetic settings that can be studied both theoretically and through synthetic experiments with small models. The paper identifies co-occurence as a key ingredient for ICL to emerge in CBOW. Then the paper considers how positional information is critical for a Transformer (or any) model to identify certain order-dependent patterns. Finally, the paper presents two sythetic scenarios involving repetition in which ICL fails with a simple model.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper begins by identifying synthetic scenarios in which co-occurrence within a sentence is sufficient for a continous bag-of-worlds (CBOW) model to be able to perform ICL. The paper proves two theorems identifying when co-occurrence statistics are sufficient to ensure that CBOW could perform ICL. 
- The paper then proves that positional information is required to perform certain sythetic tasks related to the ordering of tokens.
- Finally, the paper identifies two sythetic settings in which one might expect ICL to work.
- Sythetic experiments support each of the above claims.

Weaknesses:
- The paper states that ""ICL is achievable by only modeling co-occurrence information using [CBOW]"". However, this seems to miss the generality with which the term ICL is used. That is, ICL is commonly used for generation tasks such as full-sentence machine translation (not just the simple token-level translation examples in this paper). So to say that ""ICL is achievable"" seems like a misuse of the terminology. Without a more careful definition of ICL, this statement is invalid.
- After showing that Llama 2 is unable to use ICL to translate the word English words ""soon"" and ""main"" to Indonesian, the paper claims that ""these models should be equally likely to produce the correct answer for any given [word], irrespective of its relevance to the in-context examples. However, our experiment demonstrates that this is not the case"". This is a huge leap for a poorly designed experiment. Llama 2 was trained on 98.08% English data. The amount of Indonesian language data may have been miniscule. As such, co-occurence may offer an explanation for the result, but adjacency might be equally informative. To speak of co-occurrence without any discussion of adjacency seems a bit odd here. This same issue appears later in the paper's claim ""This suggests ICL may arise from co-occurrence information"", whereas a claim that it is informed by co-occurrence might be more apt.
- It is not clear to this reader why one would expect the setting in Section 4.1 to succeed via ICL in the first place. For example, we also wouldn't expect these settings to suceeed if they were presented to a supervised learner either because of the mismatch between the training examples and the prompt example.
- The paper relegates the entire 2.5 page related work section to the appendix. It would be better to include more in the main paper; at present only lines 25-32 in the Intro address prior work making it difficult to position this paper early on.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the emergence of ICL using a synthetic setting. Particularly, it focuses on the importance of concurrence statistics to ICL, and shows that under some simplified conditions, a CBOW-styled model is proven to complete the correct completion for an ICL example. The paper additionally proves the importance of position encodings in the studied setting, showing that when the ICL task is inherently task dependent, position encodings is necessary for good performance.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper studies an important problem. The approach---reconstructing LM behavior in much ""shallower"" models---is intriguing and can be applied to additional problems concerning LMs. The technical claims are well presented and the paper is overall very readable.

Weaknesses:
The main weakness is that the paper studies a very synthetic setting. I understand some simplification are needed for the derivation of theoretical results, and this is OK. But, for example, it would be interesting to try deriving results on cases where the input consists of valid grammatical sentences, rather than a concatenation of tuples. If that is not possible, the paper should clearly state the disparity between ""real"" ICL setting and this setting. While LMs can be presented with tuples in inference time, they are usually not trained on such tuples, but rather on free form language.

Limitations:
See above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the emergence of ICL from training on unstructured data. It explores two types of ICL tasks: the first involves input-output pairings that frequently co-occur within sentences, and the second comprises recognizable patterns that do not commonly co-occur. The authors demonstrate that the first task can be addressed by modeling co-occurrence information, and highlight the importance of positional information and blocked noise structures through the second task. Additionally, the paper discusses scenarios where ICL fails. Both theoretical and experimental evidence are provided in the paper.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- It enhances understanding of how the structure of training data influences the emergence of ICL capabilities
- the paper provides a mix of theoretical proofs and empirical validations to support its claims

Weaknesses:
- There is a lack of experiment details in the paper, such as the number of training sentences used, the frequency of each input-output pair's repetitions within training sentences, and the methodology for generating training and evaluation data.
- The scope of the experiments is limited, using small datasets and simplistic model architectures. Moreover, there is an absence of real-world data.
- There is uncertainty about whether the findings would scale well to complex real-world data, larger models and higher embedding dimensions.

Limitations:
Limitations are discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xNlQjS0dtO;"REVIEW 
Summary:
This paper proposes a mitigation strategy called ""pure tuning, safe testing"" to mitigate harmful finetuning issues for LLMs. The strategy is very simple, basically to use a safety system prompt for inference and do finetuning without such a prompt. The core philosophy is that harmful knowledge in the finetuning stage is learned without a safety prompt, but in inference time the the added safety prompt is used and therefore harmful knowledge will not be activated.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The studied problem -- harmful finetuning for LLMs by itself is important and has raised widespread public interest among the community,  and this paper is one of the early batches of papers to propose a timely analysis and mitigation strategy for the problem.

2. Comprehensive evaluation is conducted to show the effectiveness of the method. 

3. The paper is well-written, and the proposed strategy is simple enough to understand, which I think may raise the common interest among the community.

Weaknesses:
1. The core issue of PTST is that:  given that the system prompt has changed between finetuning/testing, it does not make sense to me that why the helpfulness is not degraded while the harmfulness is lowered. Both benign helpful knowledge/harmful knowledge are learned with the finetuning system prompt, changing the template in the inference time will simultaneously lower helpfulness/harmfulness in my view.  However, this is not the case in Table 2 (a) and Table 3(a), which indicates that changing the template will not always lower helpfulness (sometimes even increase helpfulness, e.g., CA->CL). I conjecture the reason is that the length of CL prompt is longer, which elicits better helpfulness performance. An explanation for this phenomenon will be appreciated. 


2. The observation in Section 4 that mixing safety data can reduce ASR is available in Vlguard Zong et al. [2024]. I understand that this is a concurrent finding, but it would be nice if the authors could mention and discuss this in Section 4. 

3. The experimental results are not intuitive enough. Particularly,  I think it is not ideal to use so many prompts (e.g., TV, TA,CV,CA,CL) for comparison. When I am reading  Table 2, I am confused about which one is a safety prompt and which one is not a safety prompt, and therefore, I cannot immediately get the intuition shown by the results.  

4. The literature review seems to be comprehensive, but there are a few related works missing. Since (Qi et al, 2024), there are a few mitigation solutions proposed to address the same challenges. I would appreciate it if the authors could appropriately cite and discuss these literature:

------------------Before NeurIPS review cycle------------------

[1] Fine-tuning can cripple your foundation model; preserving features may be the solution https://openreview.net/forum?id=VQ7Q6qdp0P (ICLR2024 template) 

[2] Immunization against harmful fine-tuning attacks https://arxiv.org/pdf/2402.16382 （ICLR2024 workshop template）

------------------concurrent------------------

[3] Representation noising effectively prevents harmful fine-tuning on LLMs   https://arxiv.org/pdf/2405.14577 （NeurIPS2024 template）

[4] Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning  httpsImmunization against harmful fine-tuning attacks  https://arxiv.org/abs/2405.18641   （NeurIPS2024 template）

[5] No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks   https://arxiv.org/pdf/2405.16229 （NeurIPS2024 template）

[6] Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models  https://arxiv.org/pdf/2405.16833v1 （NeurIPS2024 template）

[7] A safety realignment framework via subspace-oriented model fusion for large language models https://arxiv.org/pdf/2405.09055 （Elsivier Journal template, first available May, 2024）

[8] Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models https://arxiv.org/abs/2405.17374 （NeurIPS2024 template）

I am aware that some of the listed work is concurrent work (e.g., con-current submissions to NeurIPS 2024). However, it is encouraged to also cite and discuss them, because that will be beneficial for the development of the research field (but the authors should at least cite those existing works that appeared before the NeurIPS2024 review cycle).

5. Baselines for comparison are lacking.  As there are already a few mitigation strategies for harmful finetuning issues, I suggest the authors add one or two baselines, e.g., Vaccine[Huang et al., 2024] for comparison.

Limitations:
The authors have discussed the limitations, but I suggest the authors discuss the potential impact that the helpfulness will be lower with changing the template (see the first weakness), although this is not that apparent per the authors' experimental results.  

Overall, I believe that the idea of changing the template in finetuning/testing should reduce the risk of harmful finetuning, but of course, should come with the potentially negative impact that downgrades the finetune performance. I am willing to increase the score, as long as my detailed questions are answered and the limitation is clearly discussed in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper shows that the prompt templates used during fine-tuning and inference play a crucial role in safety alignment. Then, the authors propose to fine-tune models without a safety prompt, but include it at test time (user inference), which is counter to intuition. The authors demonstrate their method in the following experiments: when using the same prompts during training on GSM8K and testing, attack success rate increases for a Llama-2-Chat model (the authors considered 5 different prompts). The authors also show the same trend across models GPT-3.5 Turbo, Mistral-7B-Instruct-v0.2, and Llama-2-7b-chat, and across datasets ChatDoctor and OpenOrca.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This is a paper that points out a new direction in safety alignment for fine-tuning language models. The paper is written very clearly, with a novel method and supporting experimental results.

Weaknesses:
Improvements to this paper can be made from the following aspects: (1) there still seems to be noise in the experiment results, although that does not take away the novelty in proposing the PTST approach, (2) there should be more discussion about implications of PTST

Limitations:
The authors have discussed limitations in the conclusion section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses a critical issue, i,e., LLMs' loss of safety after being fine-tuned. The authors pay their attention to the prompt templates used during fine-tuning and testing, which leads to the main observation that fine-tuning with the valina template and testing with the safe template yields the best robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) Understanding the effect of fine-tuning on the LLM safety through prompt templates is novel.

(2) The PTST strategy shows promising performance gains when compared with the common strategy where a template is consistently used.

(3) The authors conducted experiments on several templates, models, and datasets.

Weaknesses:
(1) The authors leave the understanding of the PTST strategy to future work and very limited discussion on the underlying mechanism of PTST can be found. Although it might be hard to develop a rigorous theory explaining the strategy, I still feel it necessary for the authors to at least propose some hypotheses and try to verify them with concrete experiments. 

(2) There are cases when the helpfulness of models is notably decreased if we adopt the PTST rule, such as (TV, CL) and (TA, CL) for Llama-7B. 

(3) Some lightweight defenses such as Self-Reminder [1] and ICD [2] can be incorporated into the (CL, CL) training scheme, which will serve as good baselines for PTST. Comparison with safeguarding algorithms can help readers better understand the significance of PTST.

[1] Defending ChatGPT against jailbreak attack via self-reminders; Xie et.al; Nature

[2] https://arxiv.org/abs/2310.06387

Limitations:
Please see weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper discusses the issue of maintaining model consistency after fine-tuning large language models (LLMs). The research team, through extensive experiments, found that the prompt templates used during fine-tuning and inference play a crucial role in maintaining model safety. The paper proposes the ""Pure Tuning, Safe Testing"" (PTST) principle, which involves not using safety prompts during fine-tuning but incorporating them during testing to significantly reduce the occurrence of unsafe behaviors.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Through extensive experiments, it is demonstrated that prompt templates are crucial for maintaining safety during both training and testing.
2. The PTST approach is proposed, which improves safety performance.

Weaknesses:
1.Why fine-tune on math datasets (gsm8k, Orca-Math) to verify the model's safety? How does the performance compare when fine-tuned on safety-specific datasets, such as Anthropic/hh-rlhf?\
2.The experiments on PTST are insufficient, as they do not adequately compare the effectiveness of the approach with current alignment algorithms such as PPO, DPO, KTO, among others.\
3.This paper proposes the PTST algorithm, but it is a training technique and lacks a certain level of innovation.

Limitations:
1.Is the PTST algorithm still effective with an increasing amount of data or the introduction of mixed datasets?\
2.Lacks comparison with other methods for improving LLM safety.(Aligner,DPO...)

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xNZEjFe0mh;"REVIEW 
Summary:
This work introduces three algorithms for communication-efficient Federated Group Distributionally Robust Optimization. The effectiveness of the proposed algorithms are verified through both theoretical and experimental results.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1) This work studies an important problem of federated group distributionally robust optimization.
2) The theoretical results show the advantages of the proposed algorithms.

Weaknesses:
1) This work proposes three algorithms, including FGDRO-CVaR, FGDRO-KL, and FGDRO-KL-Adam. There lacks a comparison between these algorithms. For example, what are the connections and differences between these algorithms?
2) The analysis for FGDRO-CVaR assumes the loss function to be rho-weakly convex, which is missing from the main context.

Limitations:
See weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of reducing communication costs and sample complexity in Federated Group Distributionally Robust Optimization (FGDRO). The authors present the FGDRO-CVaR algorithm and the FGDRO-KL algorithm to address different constraints. Subsequently, they conduct extensive experiments across various real-world tasks, including NLP and CV tasks. The corresponding empirical results confirm the effectiveness of their proposed methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The exploration of reducing communication costs for federated group DRO is a rarely-studied topic within the FL community.

2. The theoretical convergence analysis for the proposed algorithms is somewhat solid.

3. The authors conduct comprehensive experiments to validate the effectiveness of the devised algorithms.

Weaknesses:
1. The contributions and novelties of this paper are unclear. It appears that the authors have directly combined existing federated adaptive algorithms with pre-existing federated group DRO methods in this paper.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents three methods for Federated Learning Group Distributionally Robust Optimization: (i) one tailored to reduce the CVaR which optimizes the top K-losses, (ii) another one tailored to tackle the KL divergence, and finally (iii) one that uses Adam locally. The paper is well written and the ideas are presented. To the best of my knowledge, the proofs are correct. My main concerns are regarding the relevance and importance of the subject, the lack of experiments, and the lack of empirical studies on communication efficiency.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
[S1] The paper is well-written, and the ideas are presented. 

[S2] The theoretical results are correct, to the best of my knowledge.

Weaknesses:
[W1] The relevance of the subject is not entirely addressed. See [Q1]

[W2] The experiment section is limited. In particular, the paper does not present any intuition on the problems they are solving. They do not consider the number of samples per server for example. I believe the authors should include a class imbalance problem [AN AGNOSTIC APPROACH TO FEDERATED LEARNING WITH CLASS IMBALANCE - Shen et al, ICLR 22].

[W3] Communication efficiency is not properly addressed by the authors. The authors show the number of communication rounds required, but they do not take into account how much is communicated. The authors claim that this method is more efficient in terms of communication, and they show it theoretically, but in the experiment section, there is no evidence of communication efficiency. I suggest the authors reveal the communication cost associated with each method, measured in the amount of data shared between servers. 

[W4] Privacy is an important subject of Federated Learning, but in this paper, there is no analysis of the privacy aspect. Can the authors elaborate on the privacy aspect of this work? 

[W5] Federated learning is a technique used to train on a set of machines. The idea is that the number of machines that participate is large. It appears to me that the largest number of servers is 17. This seems to me insufficient for a distributed learning problem.

Limitations:
See weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to improve the efficiency of existing federated group distributionally robust optimization (FGDRO) when considering two specific types of regularization, condition value at risk and KL divergence. To address the first type of problem, the authors propose FGDRO-CVaR that reduces the sample complexity and communication costs simultaneously. For KL conditions, the proposed FGDRO-KL reduces the sample complexity while retaining the same communication costs. Moreover, the authors integrate the notion of Adam into FGDRO-KL, yielding FGDRO-KL-Adam and achieving better convergence speed.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper is well-written, though some background information is missing.
2. The problem is well-motivated. The sample and communication efficiency is a pivotal problem in federated learning, though the benefits are not fully analyzed in the experiments.
3. The proposed method is grounded and improves over prior baselines.

Weaknesses:
1. The background can be more thoroughly explained. The authors are encouraged to provide additional context to address the following questions, which will greatly enhance the paper's completeness. Why is federated group distributionally robust optimization (FGDRO) an important problem or technique? What are the sources of the additional communication costs? Why is it necessary to consider two different types of regularization? Are these types of regularization relevant to different applications?

2. My major concerns lie in the experiments and their settings. 

    - **Data Splits.** While FGDRO's main advantage appears to be its ability to address non-IID optimization, the experimental setup concerning data splits lacks clarity. An analysis of the non-IID levels, such as those derived from different Dirichlet-distributed data splits with varying $\lambda$ values, is missing. Including more representative baselines, such as SCAFFOLD and FedProx, which are also designed for non-IID optimization, could further enhance the analyses.

    - **Performance.** The proposed method performs similarly to the baselines in most experiments. For example, in Tables 2 and 3, apart from the Adam variant, the proposed method is comparable to the baselines. This would be acceptable if the proposed method demonstrated improved efficiency; however, relevant analyses on this aspect are absent from the experiments.

    - **Communication or Sample Complexity Analysis.** An empirical analysis comparing complexity versus utility would be beneficial and highlight the advantages of the proposed method. For instance, the experiment in Figure 1 can be extended to a comparison among different baselines.

Limitations:
Yes, the authors have discussed the limitations in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xM5m7J6Lbl;"REVIEW 
Summary:
This paper defines social markov decision processes (SMDPs) as an MDP generalization incorporating a population of individuals with distinct utility profiles aggregated by a social welfare function. It provides a novel quantitative definition of alignment in this context, then leverages this definition to characterize probably approximately aligned policies and safe policies, prove the conditions under which they exist, and relate them to the accuracy of the reward model.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper is well written, and the background is particularly clear.
2. The definitions and theoretical results are thorough and rigorous. This paper precisely relates the probability of aligned behavior to the world model accuracy, which I believe is valuable.
3. This paper acknowledges that realistic inaccuracy in the world model could cause intolerable uncertainty in the PAA policy, and shows a more practical approach (safeguarding a black-box policy).

Weaknesses:
Even the more practical approach of safeguarding a black-box policy may have severe limitations. I believe the paper would be strengthened by a discussion of the feasibility of this -- in particular, what is computational complexity of computing $\mathcal{A}_{safe}$ for a SMDP?

Typo: On line 277, I believe ""expansive"" should be ""expensive"".

Limitations:
This paper includes an excellent discussion of the limitations of these results, including the theoretical conditions under which PAA and safe policies will be unreliable. The paper also discusses further practical and philosophical limitations in Section 5.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper applies ideas from the Probably Approximately Correct framework to agent alignment. The paper defines a new idea of a policy which is Probably Approximately Aligned and explores the existence of such policies under certain assumptions of social welfare and models of the world. The authors show that probably approximately aligned (and approximately aligned) policies exist when there is a sufficiently accurate world model. However, to compute this policy is quite expensive. Thus, the authors also develop the idea of a safe policy which can be derived using a PAA policy and seems to be a policy that will probably not result in a catastrophically bad state.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper appears to be a very reasonable application of a well established form of analysis into a novel domain.

The main idea of providing bounds for the quality of an agent's policy is very important and will likely be the focus of much work in the near future. This is quite useful work and appears to me as the potential basis for work that can eventually have significant beneficial impact on the world.

The paper is generally well written and the motivation is clear. In places the math is a little dense but it seems to be as approachable as it can be for this sort of analysis. I do certainly appreciate that you've put a moderate amount of the work into the actual paper rather than stuffing all the important stuff into the appendix.

Weaknesses:
Not a weakness, but my disclaimer: I was not able to thoroughly review every detail of the math due to time constraints so my understanding of the paper is limited.

The primary (and minor) issue I see with the paper is that it is quite abstract and doesn't give a clear idea of how close this is to being useful. While obviously difficult to fit into a conference paper, an experimental section may give some intuition for details such as how accurate a world model really needs to be, how beneficial PAA/safe policies are, etc.

It seems that Sec 3.2 is constructive in a sense and provides a PAA policy. Some further commentary on the practicality of this policy (is it entirely impractical to use it for synthetic experiments, or simply impractical in any useful setting/world model?) would help to contextualize the paper.

Limitations:
Limitations are well stated.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper aims to define alignment quantitatively and ensure AI agents' actions are predictable and safe. The paper start by outlines the basics of utility and social choice theory, focusing on quantifying social satisfaction and the conditions under which it is measurable. Next, the paper defines probably approximately aligned (PAA) and approximately aligned (AA) policies and provides a modified sparse sampling algorithm to achieve these policies under certain conditions. The paper also presents the idea of ""safe policies"" and a method to ensure AI actions are verifiably safe for society.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Originality: This paper introduces a novel, quantitative definition of alignment in social decision-making contexts, drawing from utility and social choice theory.

- Quality: The paper primarily focuses on theoretical contributions rather than empirical experiments. It is well-structured.

- Clarity: The paper provides detailed mathematical derivations and proofs to support the existence of PAA and safe policies. It includes extensive references and context, including foundational works in utility theory, social choice, AI safety, and reinforcement learning, emphasizing the interdisciplinary nature of aligning AI with human values.

- Significance: This work has a significant impact. While primarily theoretical, the work aims to provide a foundation for developing AI systems that could be safely used in critical applications like social governance, policy-making, or resource allocation.

Weaknesses:
The safeguarding method is described in a general context, with limited discussion of its applicability to specific real-world problems. Consider adding examples of real-world applications where the safeguarding method could be particularly beneficial. For instance, discuss its application in autonomous vehicle systems, healthcare decision-making, or financial trading algorithms.

Limitations:
The authors discuss various limitations of their approach, including computational complexity for large state spaces and strong assumptions about the availability and accuracy of information. The paper also highlights challenges in building reliable world models and the philosophical questions surrounding the informational basis of utilities.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the potential for AI agents to safely make critical decisions, such as those in a government setting, by examining the concept of alignment. It introduces Probably Approximately Aligned (PAA) policies, which are policies that are nearly optimal in aligning with social welfare objectives. The authors draw from utility and social choice theories to provide a quantitative definition of alignment and propose methods to ensure AI actions are verifiably safe for society. They also discuss the practical challenges in implementing such policies and suggest future directions for research in this area. The focus is on developing a theoretical framework that could eventually be applied to AI governance and decision-making processes.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The authors draw from utility and social choice theories to provide a quantitative definition of alignment and propose methods to ensure AI actions are verifiably safe for society.

Weaknesses:
I think the problem is not well presented.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xL7Ve14AHA;"REVIEW 
Summary:
This article proposes an optimization algorithm RAMDA for training structured neural networks, which combines a number of optimization techniques including dual averaging, momentum, and coordinate-wise preconditioners. Similar to the existing RMDA algorithm, RAMDA also has the capacity to identify the local manifold structure of the solution. The author(s) provide theoretical analyses to justify the convergence property of RAMDA, and develop an inexact subproblem solver as required by RAMDA.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed RAMDA algorithm extends the existing RMDA algorithm by adding a coordinate-wise preconditioner, and its theoretical analysis seems to be novel.

Weaknesses:
I think one major weakness of the current manuscript is the **correctness** of some theoretical results presented in the article.

1. Theorem 1 suggests that the regularizer function $\psi$ can be nonconvex. However, as the RAMDA algorithm heavily relies on the proximal operator of $\psi$, how do you define the proximal operator when $\psi$ is nonconvex? For example, equation (6) is used to define the new iterate $W^t$, but when $\psi$ is nonconvex, it is likely that the ""argmin"" is a set and is not uniquely defined.

2. Taking a closer look at the proof of Theorem 1, I feel that the author(s) may have a misunderstanding of an existing theorem. In Appendix B, equation (11) is obtained by citing Theorem 10.15 of [1]. However, Theorem 10.15 of [1] applies to functions of the form $F(x)=f(x)+\psi(x)$, where $f$ is smooth and nonconvex, but $\psi$ is convex. In other words, the non-convexity only applies to the smooth part, not the regularizer.

3. If the findings above are valid, then the author(s) may need a thorough examination of the technical proofs to see if there is any error.

4. If we assume $\psi$ is convex, then there should be an Nesterov-accelerated version of Algorithm 2 that converges in $O(\varepsilon_t^{-1/2})$ iterations, which is faster than the rate given in Theorem 1.

[1] Beck, A. (2017). First-order methods in optimization. Society for Industrial and Applied Mathematics.


===============================================================

Edit: during the rebuttal the author(s) seem to have addressed the concerns above.

Limitations:
The authors have discussed the limitations of the proposed algorithm in Section 5 and Appendix C.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper develops regularized adaptive momentum dual averaging (RAMDA) for structured neural networks. The method uses the preconditioning matrix to accelerate the convergence of a regularized momentum dual averaging (RMDA) method at the price of requiring the local solver (e.g. standard proximal gradient methods) to solve the subproblem. By the preconditioning matrix inspired by the AdaGrad stepsizes in Eq. (2), RAMDA outperforms RMDA and other existing gradient-based methods for solving structured neural networks in various learning applications.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Theoretical results suggest the convergence towards the solution to the subproblem when the proximal gradient methods are used as the local solvers, and almost surely convergence of RAMDA that derives from the manifold theory under the standard $L$-smoothness assumption on the objective functions $f$.   

2. Empirical results illustrate the superior performance of RAMDA over RMDA and other existing gradient-based methods for various neural network tasks. Clear criteria, e.g. for solving the subproblems, are clearly stated in the numerical experiments.

Weaknesses:
1. I think there is an error in Eq. (3) where it should be the square root $\sqrt{\cdot}$ in the diagonal operator for $P^t$. This is because $P^t$ uses $U^t$ that is computed from the element-wise multiplicative product of the gradient $G^t$. Is $P^t$ inspired by the AdaGrad stepsizes? If so, then adding the justifications on using $P^t$ in RAMDA is worthwhile to better distinct RAMDA from RMDA. 
2. In the experiments, can you comment on the impact of the different $\epsilon_t$ on the training performance of RAMDA? Because I believe that using $\epsilon_t$ a bit higher than $10^{-8}$ set in your experiments RAMDA might achieve far lower training time than other methods while keeping still comparable perplexity to solve Transformer-XL with WikiText-103 in Table 4, or Tacotron2 with LJSpeech in Table 5.

Limitations:
The limitations of current theoretical results are clearly and fairly discussed after Theorem 2.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
#### Summary
The paper introduces the Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. RAMDA addresses the challenge of solving the subproblem involved in the regularized adaptive methods, which typically lacks a closed-form solution. The paper presents an inexactness condition that retains convergence guarantees and proposes an efficient subproblem solver. The algorithm leverages manifold identification theory to ensure that the iterates of RAMDA attain the ideal structure induced by the regularizer at convergence. Extensive experiments demonstrate the effectiveness of RAMDA in various tasks, including computer vision, language modeling, and speech synthesis.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
#### Strengths
1. **Novel Algorithm**: RAMDA combines adaptive momentum dual averaging with efficient inexact subproblem solving, providing a practical and theoretically sound method for training structured neural networks.
2. **Theoretical Guarantees**: The paper provides strong theoretical support, including convergence guarantees and structure identification, ensuring the algorithm's robustness.
3. **Practical Efficiency**: The proposed inexact subproblem solver is efficient, making RAMDA feasible for large-scale applications.
4. **Empirical Validation**: Extensive experiments across multiple domains demonstrate the superior performance of RAMDA in terms of both prediction accuracy and structured sparsity.

Weaknesses:
#### Weaknesses
1. **Computational Complexity**: The computational complexity of the proposed subproblem solver, especially for high-dimensional data, needs more detailed discussion.
2. **Generality**: While the paper focuses on specific types of structured neural networks, extending the methodology to other models and regularizers would enhance its generality.
3. **Comparative Analysis**: More detailed comparisons with other state-of-the-art methods, beyond the provided benchmarks, would strengthen the empirical validation.
4. **Implementation Details**: Practical guidelines for implementing RAMDA, including parameter tuning and handling different data distributions, are somewhat lacking.

Limitations:
no

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xImeJtdUiw;"REVIEW 
Summary:
The paper introduces a novel multi-modal model, IsoFormer, designed to integrate DNA, RNA, and protein sequences for predicting RNA transcript isoform expression across different tissues. It utilizes pre-trained modality-specific encoders to generate embeddings that are then combined using a sophisticated aggregation method. The model demonstrates significant improvements in prediction accuracy compared to single-modality approaches.

Contriburion:

1. Developed the first general-purpose multi-modal model integrating DNA, RNA, and protein sequences.

2. Demonstrated successful application of transfer learning from modality-specific encoders.

3. Provided a new robust framework for advancing the prediction of RNA transcript isoform expression.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Innovative Integration of Modalities: The paper presents the first attempt to integrate three biological sequence modalities (DNA, RNA, and proteins) in a unified model, providing a comprehensive approach reflective of natural biological processes.

2. Effective Transfer Learning: IsoFormer effectively leverages pre-trained encoders to enhance its predictive power, benefiting from both intra-modal and inter-modal transfer learning.

3. Robust Evaluation: Experiments demonstrate the model's capability, outperforming existing methods in predicting transcript isoform expression, which is a challenging task due to its multi-modal nature.

Weaknesses:
1. Complexity and Computation: The model's complexity and the computational demands might limit its accessibility and use, particularly in environments with restricted resources.

2. More Comprehensive Evaluation for PLM's representation learning capability would make this paper better.

Limitations:
1. Data Requirements: The effectiveness of the model is contingent on the availability of comprehensive and high-quality multi-modal datasets.

2. Generalizability: While promising, the results are primarily validated on specific types of gene expression data, and its performance across broader biological applications remains to be fully assessed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework for the multi-modality pretrain model according to the Central dogma of biology. The method encode DNA, protein and RNA at the same time. The proposed method can transfer knowledge from the encoders pretraining and modalities.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is well-organized and easy to follow
The authors have proved that the multi-modality of single cell data can help model predictions.

Weaknesses:
Lack of experiments. 1. More ablation studies should be conducted about removing different modalities of the model in Table 2 ( e.g. we observe only RNA can achieve a high performance, what about protein+RNA? ).

 2. More dataset details should be included. The split of training/validation/test sets is not clear. If the authors do the experiments on the same dataset, they should split the dataset according to the tissues to validate the transferability of the proposed method.

Limitations:
The biological system is more complex. The proposed method only include the direct map from DNA to RNA. However, in real world, RNA can effect the expression of DNAs. More details should be discuss in the future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper models isoform relative abundance across tissues with a multimodal approach based on 3 pretrained encoders for DNA, RNA, and AA sequences.  DNA encoder uses a sequence centered on the gene’s TSS, RNA encoder uses the known isoform sequence from RNAseq and the protein encoder uses corresponding AA sequence.  They perform multiple ablations on the utility of having all 3 separate encoders and, given separate encoders, how to aggregate them into a single isoform specific embedding/prediction, and look at attention layers of RNA module to find biologically meaningful regions of attention.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Isoform level analysis using 3 separate pretrained encoders for DNA, RNA, and AA sequences is a good strategy.  The authors provide useful ablations on the utility of the multi modal approach and on modern strategies for combining those into a single embedding.  Looking for biolgoically meaningful interpretations of attention layers is useful.

Weaknesses:
I don’t think the authors can claim this is the first attempt to combine DNA, RNA, and AA modalities with techniques from NLP.   See the recent Evo work here https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2 .  While they evaluate their performance against Enformer, that’s a large part of their own model.  So the evaluations have an intramural feel to them.  It’d be interesting to see how their strategy compares to other multi modal models such as Evo, and more RNA centric work like Borzoi, which looks at a more fine grained look of variant effects on the DNA to RNA relationship.  Looking at average isoform abundance across individuals is all well and good, but GTEx also has individual genomes, and genomic variation across individuals will also of course affect splicing patterns and which isoforms come from what individuals.

Limitations:
The authors should be more explicit about the limitation of using reference genome and known isoform sequences and how this kind of sweeps splicing as a function of dna sequence under the rug.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
xDrKZOZEOc;"REVIEW 
Summary:
The paper introduces Optimization Consistency Models (OptCM) as a novel method for solving combinatorial optimization (CO) problems efficiently. Traditional diffusion models, although powerful, are computationally intensive due to their iterative denoising processes. OptCM overcomes this limitation by learning direct mappings from noise levels to optimal solutions, enabling rapid single-step solution generation. The contributions of this paper are three-fold. First, OptCM reduces the computational overhead significantly by enabling fast, single-step solution generation while maintaining high solution quality. Second, This protocol ensures that samples from different generative trajectories converge consistently to the optimal solution. Thrid, Introduced at the test stage, this method enhances solution exploration and quality during inference.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
OptCM significantly reduces the computational overhead by enabling fast, single-step solution generation, compared to the multiple steps required by traditional diffusion models. This efficiency allows for rapid inference, making it practical for real-time and large-scale applications.

Despite the reduced computational steps, OptCM maintains high solution quality, often outperforming state-of-the-art methods that require more extensive processing. The optimization consistency training ensures that the generated solutions are close to the optimal solution.

The optimization consistency training protocol is a novel approach that minimizes the differences among samples from varying generative trajectories, ensuring robust and consistent solution generation. This method enhances the model's ability to generalize across different problem instances.

The introduction of a consistency-based gradient search during the test stage allows for further exploration and refinement of the solution space, improving the final solution quality. This approach bridges the gap between training and inference, making the model more adaptable to new instances.

Weaknesses:
Overall, the paper's strengths lie in its innovative approach to reducing computational complexity while maintaining high solution quality, its robust and versatile model design, and its impressive performance on benchmark tasks. However, there are some weaknesses in this paper. Some weaknesses listed below might be found to be too general and don't require the authors to address them now.

The advanced techniques used in OptCM, such as optimization consistency training and gradient-based search, may be challenging to implement and require a deep understanding of the underlying principles.

The model's performance is closely tied to the quality and diversity of the problem instances used during training. If the training set does not adequately represent the test instances, the model's effectiveness might be reduced.

While the paper demonstrates the superiority of OptCM over state-of-the-art neural solvers, it could provide a more detailed comparison with traditional, non-neural methods in terms of both performance and computational efficiency.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances CO DM-based neural solvers under the setting where labeled training graphs are available by considering Consistency Models and gradient search (which was adopted from T2T).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1- The paper is in general well-written and technically sound.

2- The use of CMs to accelerate the sampling procedure.

Weaknesses:
See Questions.

Limitations:
See Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents Optimization Consistency Models (OptCM) for solving combinatorial optimization (CO) problems efficiently. By leveraging the consistency model, OptCM maps varying noise levels to optimal solutions in a single step, significantly reducing computational overhead. This approach is validated through extensive experiments on the Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS), demonstrating significant superior efficiency compared with state-of-the-art diffusion-based models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper introduces a consistency model to improve the efficiency of diffusion-based combinatorial optimization solvers.
2. Extensive experiments on TSP and MIS show that OptCM can outperform existing methods.

Weaknesses:
1. This work is mainly incremental, based on previous works DIFUSCO [1] and T2T [2].
2. Larger-size TSPs, such as those with 100000 nodes, should be tested against state-of-the-art learning-based methods [3].
3. Despite significantly improving solving efficiency, the proposed method is limited in addressing constrained COPs (e.g., CVRP and more complex COPs) and requires optimal solutions as labels.

[1] DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization, NeurIPS, 2023.

[2] T2T: From Distribution Learning in Training to Gradient Search in Testing for Combinatorial Optimization, NeurIPS, 2023.

[3] GLOP: Learning Global Partition and Local Construction for Solving Large-scale Routing Problems in Real-time, AAAI, 2024.

Limitations:
The authors have discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduced a new algorithm for solving some classic combinatorial optimization problems. The method falls into the category of learn-based generative solvers. More specifically, it is a direct extension of the DIFUSCO [1] and T2t [2] solver, which are diffusion-based generative solvers. The improvement is mostly done through improving on the sampling step of the two aforementioned works with consistency models (CM) [3], a recent notable regime that enables drastic reduction of the number of function evaluations (NFE, or sampling steps) of vanilla diffusion models. The novelty lies in extending CM into discrete regime and combining it with consistency-based gradient search, which is necessary for combinatorial optimization problem. Empirical evaluations show the effectiveness of this new solver, where it achieves competitive objective value in a much shorter time, compared to various baselines. 


[1] Z. Sun and Y. Yang, “DIFUSCO: Graph-based diffusion solvers for combinatorial optimization, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. 

[2] Y. Li, J. Guo, R. Wang, and J. Yan, “T2t: From distribution learning in training to gradient search in testing for combinatorial optimization,” in Advances in Neural Information Processing Systems, 2023.

[3]  Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, “Consistency models,” arXiv preprint arXiv:2303.01469, 2023

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Overall I think the method is novel. The extension of consistency training framework into diffusion-based CO solvers is not trivial, and this work is trying to solve a well-motivated problem. The empirical evaluations are quite convincing compared to diffusion-based CO solvers (of course, one expects so as consistency models inference time is much faster than diffusion generative models). 

2. The paper is overall well-written, and the authors seem to have done quite thorough literature reviews to gather sufficient baselines to compare to the performance of their work to.

Weaknesses:
1. It is necessary to elaborate on why the authors wrote ""... note F1 is exactly the (implicit) the objective of the diffusion and consistency models..."" at line 259. In other words we need to see why (should be a lemma with proof) we have the quantity $F_1$ in equation (6) is the equivalence of the loss in eq (4).

2. It is unclear how the overall training paradigm takes place in practice, including the gradient search part. The authors should include an algorithm box on the training of their OptCM framework. Algorithm 1 on Multistep Consistency Sampling is almost the same as in the original Consistency Models paper, so I suggest the authors move them to the Appendix. If I'm not mistaken, the training of consistency models is quite tricky, for example, it is crucial to design a suitable training time discretization schedule for CM to work well. Could the authors elaborate on this for their problem? 

 I'm willing to re-evaluate my score if the authors can answer these two points.

Limitations:
Consistency training introduced additional training overhead; I think the authors did not discuss this in Appendix D.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xCUXJqQySD;"REVIEW 
Summary:
This paper proposes a novel method for creating patient-specific digital twins using non-invasive patient health data. The authors introduce a physics-informed self-supervised learning (SSL) algorithm that pretrains a neural network on learning a differentiable simulator of the cardiac process. Then, another model is trained to reconstruct physiological measurements from non-invasive data while being constrained by physical equations learned during pretraining. The method is applied to identify digital twins of cardiac hemodynamics using echocardiogram videos, showing good results in unsupervised disease detection and in-silico clinical trials.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The method uses non-invasive data, avoiding time-consuming and complicated patient interventions.
* The model accuracy is enhanced by including a physics-based model during training.
* The authors demonstrate the method's utility in modeling complex physiological processes like cardiac pressure-volume loops with open-sourced datasets.

Weaknesses:
* Simplifications/assumptions in the Windkessel and LVAD models might not fully capture the complexity of the heart dynamics.
* The results might be sensitive to low quality non-intrusive data, which can affect the global accuracy of the method.
* It is not clear the demographic diversity of the echocardiography dataset. Thus, the model might not generalize well across all the segments of the population.

Limitations:
Limitations were addressed appropriately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel methodology for identifying patient-specific digital twins using noninvasive medical imaging, particularly focusing on cardiac hemodynamics. By leveraging a physics-informed self-supervised learning approach, the research addresses the challenge of modeling digital twins without invasive data collection. The process involves pretraining a neural network to simulate physiological processes and then fine-tuning it to reconstruct physiological measurements from noninvasive modalities, constrained by the learned physical equations. This framework allows for the simulation of patient-specific physiological parameters and the potential for conducting in-silico clinical trials and unsupervised disease detection.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper introduces a cutting-edge method combining physics-informed neural networks with self-supervised learning to tackle the inverse problem of estimating physiological parameters from noninvasive imaging data.

2. By utilizing noninvasive data, the proposed method significantly reduces the need for invasive procedures, enhancing patient safety and comfort, and potentially broadening the applicability of digital twin technology in routine clinical practice.

3. The methodology's ability to simulate detailed physiological conditions and interventions opens up vast possibilities for its application in personalized medicine, including unsupervised disease detection and in-silico clinical trials, which can significantly accelerate the development of therapeutic strategies.

Weaknesses:
1. Insufficient performance comparison. The paper only compares PINN and Neural ODE methods. There are many variations of PINNs methods which outperform the original one. The paper should choose more solid baselines.

2. No ablation study. The paper proposed a two-stage training strategy, but it didn't show why it is necessary.

Limitations:
The proposed method requires a pretty strong assumption of the PDEs dynamics. I am a little bit worried about the training efficiency of this method. Can it be used as a real diagnostic tool? Additionally, I am intrigued by the potential of this method to be applicable across a broader range of medical disciplines.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
I have read this manuscript during ICML review. It looks the same so I copied my previous review.

The authors presented a method to infer the physical parameters θ of physiological process (heart pumping blood) from noninvasive observation y (the echo image). The mapping from y to θ cannot be directly learned due to the lack of paired data. Instead, they find that an intermediate variable x_bar (the EF) can be annotated by experts (x_bar=g(y)) which can also be calculated based on θ (x_bar=m(M(θ))). Thus the observable pair (y, x_bar) provides the supervision for learning θ = F_inv(y), through the relation x_bar=m(M(F_inv(y))) where m is rule-based and M is a solution to ODE. They first train a surrogate network to approximate M(θ) through synthetic simulation, followed by learning the parameter of F_inv on observations {(y, x_bar)}.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is clearly written and easy to read with clear symbols.
2. The idea of introducing a physical model helps better characterise the physiological system and provide a learning method.
3. The surrogate model overrides the cost of solving ODE and make it differentiable during learning.
4. The inference of physical parameters helps to perform virtual experiments.

Weaknesses:
1.The authors compared their methods in predicting EF from echocardiogram with supervised 3DCNN but did not outperform them in MAE. It should be noted that the EF is calculated on ED and ES segmentation and supervised segmentation network is expected to perform even better.
2.Lack of validation. The authors performed validation by comparing EF derived from the physical parameters to the observed EF. But this does not guarantee the correctness of their physical parameters. In fact, arbitrary intermediate variables can be defined and could also lead to the comparable prediction of EF.
3.Training the surrogate model of x=M(θ) need to generated synthetic samples while this is domain-dependant. Whether the learned mapping fits extreme or shifted situations is unknown, lacking of uncertainty analsysis.

Limitations:
My biggest concern is that the method lacks ground-truth validation, at least some samples.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a method to identify parameters for digital twin models of patients using non-invasive health data, eliminating the need for invasive procedures. This method focuses on scenarios like cardiac hemodynamics, where traditionally invasive measurements (e.g., through catheterization) can be predicted using non-invasive data (e.g., echocardiograms). The novelty of the method is to solve the associated inverse problem specifically for that patient, so that personalized predictions can be performed.

The proposed method uses a two-step SSL approach that structurally resembles pretraining and finetuning in SSL. First,  a neural network is pretrained on synthetic data to learn the forward dynamics of a physics-based model. Then the pretrained model is then used to train another network on actual non-invasive patient data to predict physical parameters.
Application to Cardiac Hemodynamics:

The paper illustrates how to apply the above method for cardiac hemodynamics using echocardiography. This allows the prediction of patient-specific pressure-volume (PV) loops using non-invasive echocardiogram videos.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is overall well-written and it identifies a real problem with potential high impact: the design of personalised medical twins to avoid invasive procedures

Weaknesses:
The authors focus only on a very specific medical use-case, they don't try to generalize their method to more cases. In the introduction the authors illustrate this as a generic approach, therefore I was surprised to not find an attempt to support their vision with more application examples. Without the demonstration that this method can have a broader use I don't think this paper is suitable for presentation at this conference. Also, the authors don't run convincing ablations to demonstrate that their approach is sounding.

Limitations:
The authors mention the limited clinical validation. This is certainly one limitation to take in account. I would consider also the lack of generalization to different clinical procedures.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
xCIbVuXwPM;"REVIEW 
Summary:
This paper studies surrogate loss design and the trade-off between surrogate consistency and loss dimension. The contributions are three-fold: (1) the characterization of the hallucination region, where the decoded prediction from the surrogate loss minimizer gives a class with no target probability mass, indicating a completely ""irrational"" prediction; (2) the construction of the (""weakly but reasonably"" inconsistent) calibrated surrogate and link under the low-noise setup; (3) the decomposition of property eliciation into multiple elicitation problems with low dimensions.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
+ **Very well-motivated problem**: Consistent/calibrated surrogate losses, the main topic of this paper, are sometimes too restrictive because the (in)consistency analysis hinges on ""far-fetched"" distributions---as argued by the authors, and as can be seen in some counterexamples to show the inconsistency of the well-known Crammer-Singer hinge loss. If we can remove such ""pathological"" situations from the entire distribution space, we would have a nicer characterization of loss functions. This is the central motivation of this paper.
+ **Interesting instances to show calibrated surrogates under the low-noise condition**: This paper mainly investigates two situations to convince the benefits of the relaxed notion of calibration: the unit cube and permutahedron. Both make sense with reasonably practical scenarios and provide the first attempt to show the benefits of incorporating the knowledge of the noise level in loss function design.

Weaknesses:
+ **The result with the unit cube may need $d \\geq n$**: At first sight, the statement of Corollary 7 (the calibrated link design for the unit cube) does not have any restriction on $\\alpha$ (say, any dependency on the embedding $d$), unlike Corollary 8 (the calibrated link design for the permutahedron). When I look at its proof, I suspect we need the condition $d \\geq n$ for this because the proof leverages that ""$P\_\\alpha^y$ is a strict subset of the orthant that contains $v\_y$"" at l.500. Otherwise, it is strange because we can choose an arbitrarily small embedding dimension $d$.
+ **The trade-off could be seen only for the unit cube case**: The main contribution of this paper is to showcase the trade-off between the consistency and the embedding dimension, as suggested by the title. However, we may not see a clear trade-off in the unit-cube case. This is also related to the above point: Once we have $d \\geq n$, there may not be a clear trade-off for $\\alpha$ and $d$. Given this, we are not very sure how universally we can observe the trade-off across different elicitation problems.

Nevertheless, I don't think these points are significant enough to undermine the contributions of this paper. I expect the authors to address them, which leads to the presentation of the contributions in a more fair/precise manner.

Limitations:
This work has discussed the limitations. Specifically, there is computational hardness to break down an elicitation problem into multiple low-dimensional problems in Section 5.

This work does not have potential negative societal impacts because the contributions of this paper are relevant only to general machine learning theory.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method called polytope embedding, which embeds multiclass predictions onto real numbers. The paper studies the properties of this embedding, like hallucination and calibration. Further, with low-noise assumptions, the authors showed more calibration results for their embedding in some cases like embedding into the unit cube.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The topic of this paper on how to design a consistent surrogate loss seems interesting. The results proved in the paper also relate to topics that people care about like hallucination and calibration.

Weaknesses:
Even though each result seems interesting, the structure of this paper is not very clear to me. I am open to different arguments but I think it is hard for readers to understand what the authors are trying to show with this paper.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper examines the trade-off between consistency and dimensionality in multi-class classification. It has been known that the lower bound on the dimension for consistent surrogate losses under any distribution is $n - 1$, wheren $n$ is the dimension of the input space. The authors propose the notion of partial consistency, which permits the establishment of surrogate losses at much lower dimensions than the input dimension. This method allows for the consistency of lower-dimensional surrogate losses under a low-noise condition and the construction of dimensionally reduced consistent surrogate losses across multiple problem instances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is the first study to explore the control of the trade-off between consistency and dimension in multi-class classification.

2. The paper is well-written and clear, even though it is theoretically dense.

Weaknesses:
1. The paper demonstrates the existence of distributions under which consistency holds for low-dimensional surrogates but does not offer much guidance on how to identify such distributions for a given surrogate loss.

2. The paper restricts its study and analysis to asymptotic guarantees.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the problem of constructing consistent multiclass surrogate losses for the 0-1 loss while reducing the dimension of the scoring function is studied. The concept of partial consistency, which can be dated back to the study of multiclass SVM, is used as a crucial part of this work. It is first revealed that any losses with scoring function’s dimension less than #class number-1 will lead to severe misclassification. Then low noise assumption is used to enable the trade off between (partial) consistency and the scoring function’s dimension number. An attempt of recovering full consistency with several scoring functions of lower dimensions is also made.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. A clear trade off between the strictness of low-noise assumption and scoring function’s dimension number is quantified.

2. The analysis of hallucinations is also enlightening, which provides an interesting insight of why some well-suited models (with some losses) can make wrong predictions even with no real-world evidence.

3. The clear presentation and layout of the results make them easy for readers to understand.

Weaknesses:
While the dimension of predictor is reduced and thus the computational cost of training can be promisingly reduced, the computational cost of the inference stage may increase compared with that of the traditional #class number-dimensional scoring functions. Is there any remedy for this problem?

Limitations:
Please see the weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
x7usmidzxj;"REVIEW 
Summary:
In this paper, the authors analyze the convergence of Adam under milder noise conditions (affline variance) and milder smoothness conditions (both $L$-smoothness and $(L_0,L_q)$-smoothness) and propose a $O(\text{polylog}(T)/\sqrt T)$ convergence rate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper analyses the convergence of Adam under milder smoothness conditions compared to the previous work. The result is relatively solid and convincing. The writing structure is also relatively clear.

Weaknesses:
1. As the author claimed in their paper, they did not provide numerical experiments in this paper. While this paper is a theoretical paper focusing on the convergence analysis of Adam, some simple numerical experiments aligning with the results will make it more convincing.
2. This paper exhibits a slight lack of novelty. Since after checking out the proof details, I found that the crucial techniques were almost proposed by the previous related works.  However, this weakness is trivial, especially for a theoretical paper, and as I claimed in the Strength part, the result of this paper is solid.

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper provides two probabilistic convergence rates for Adam with generalized affine variance noise under smoothness and generalized smooth condition, respectively, which achieves comparable results to many prior results.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Please see the above Summary.

Weaknesses:
1. I suggest that authors should provide detailed formulas for some notations including $\textbf{g}_t, g(\textbf{x}), \nabla f(\textbf{x})$. Is $\nabla f(\textbf{x})$ the gradient with the form of expectation.

2. In line 118, the reference [10] is cited repeatedly.

3. Section 5 is used to discuss the most related works and make comparisons with the main results in this paper. However, authors only discuss the most related works without any comparison with their main results. 

4. As mentioned in 1., if $\nabla f(\textbf{x})$ is the gradient with the form of expectation, the two results (Theorems 3.1 and 4.1) in this paper are not fully high probability since $\frac{1}{T} \sum_{t=1}^T ||\nabla f(x_t)||^2$ is equivalent to $\frac{1}{T} \sum_{t=1}^T ||E_{z_i}[\nabla f(x_t,z_i)]||^2$ ($z_i$ is the training data defined by me) which is smaller than $\frac{1}{T} \sum_{t=1}^T E_{z_i}[||\nabla f(x_t,z_i)||^2]$. In other words, the results with the form $\frac{1}{T} \sum_{t=1}^T E_{z_i}[||\nabla f(x_t,z_i)||^2]$ can directly derive the corresponding results with the form $\frac{1}{T} \sum_{t=1}^T ||E_{z_i}[\nabla f(x_t,z_i)]||^2$, to say nothing of the one with additional high probability. Therefore, from my perspective, high probability is not an advantage for this paper, but rather weakens $\frac{1}{T} \sum_{t=1}^T ||E_{z_i}[\nabla f(x_t,z_i)]||^2$.

Limitations:
The contribution of this paper is a bit weak. Although authors weaken some assumptions, such as noise and smoothness, the form of their results is also weakened. Therefore, I don’t ensure the weakened assumptions are their advantages.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the high-probability convergence of Adam in the non-convex setting under relaxed assumptions. The authors consider a general noise condition that governs affine, sub-Gaussian, and bounded noise conditions. They also consider a generalized smoothness condition motivated by language model experiments. Under these assumptions, they obtain a convergence rate of $\text{poly}(\log T/\delta)/T$, where $T$ is the number of iterations and $\delta$ is the confidence level.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Their result look novel and significant. They have shown the high-probability convergence of Adam under relaxed conditions than all previous papers.
2. The proofs look correct. 
3. The paper is well-written and results are clearly presented.

Weaknesses:
1. One major concern is, by choosing $\beta_2=1-1/T$, does the author essentially reduce Adam to SGD with momentum, as this makes $v_t$ almost a constant? Btw, I think for [18] and [23] in Table 1, $\beta_1$ should be $1-1/\sqrt{T}$. Please also check other rows more carefully.

I will increase the score if this concern is addressed.

Limitations:
See weaknesses and questions above

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wm9JZq7RCe;"REVIEW 
Summary:
This paper presents a study on tokenization by investigating the behavior of transformers on simple data generating processes . It shows that, in the absence of any tokenization, transformers trained on $k$th-order Markov processes predict characters according to a unigram model, which is quite problematic given how poor unigram models are at modeling Markovian data. Paradoxically, they observe that, even the simplest unigram model learnt by transformers *with the appropriate tokenization* is able to model the probability of sequences sampled from a $k$th-order Markov process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written, with empirical observation intermingled with theory, which I quite liked. The theory is also accompanied by a lot if intuition, insight and interpretation, which really helps drive the point home.

Weaknesses:
- In section 3.2, the authors chose to focus on developing guarantees for a newly developed tokenizer, which, to my knowledge, is seldom used. It would've been maybe of greater use to the community to also, or instead, establish these guarantees for the more commonly-used tokenizers, such as BPE.

- I appreciate that this is mostly a theoretical study of tokenizers, and while the observations put forward are valuable, I found myself wondering what practical takeaways this paper presents to improve current tokenizers. That is something I would love the authors to comment on.

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors show that tokenization is a fundamental property of transformer-based models, in the sense that without it, it is hard (if not impossible) to achieve low cross-entropy loss on next-word prediction. They show that tokenization helps breaking the unigram barrier (i.e., the best loss a unigram model can achieve) and give a theoretical characterization of the information tokenizers provide in terms of statistics on token distribution.

In particular:

Section 2.1 shows how models without tokenization cannot achieve the optimal cross-entropy loss, while when equipped with a tokenizer they break the unigram barrier.

Section 3 studies tokenizers that assign all possible substrings (up to length r) as tokens in the dictionary and shows their theoretical optimality in learning processes ruled by k-Markov chains. A consequence is that unigram models can also do that, in the limit.

Of course, this comes at the expense of the model's efficiency and potential attacks that one can run on an exponential number of tokens (i.e., the surface attack grows very large).

Finally, the authors show that tokenizers can trade off the vocabulary size while maintaining low cross-entropy (i.e., they can behave like an optimal model).

Finally, they extend the theoretical framework to LZW tokenizers.

Experiments are conducted on tokenized vs. non-tokenized models on {k=1}-Markov models and then on some real datasets to show that tokenizers trade-off complexity and efficiency in learning an optimal representation of the characters (and their frequency) in the training distribution.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The article studies an important problem, and I think there is value in the paper.
To the best of my knowledge, comparing BPE to non-tokenized models is new, and the figures give some interesting insights (e.g., Figure 2).
Your paper contains much theoretical work, contributing to its quality.
The results in the limit for unigram and BPE/LZW models are noticeable (Section 3 and Eq. 2).

In general, the results seem solid and are also interesting for linguists and NLP researchers. BPE and other tokenization methods find a trade-off between unigram models, as per Eq. 2, and the complexity of the resulting vocabulary (and model).

Weaknesses:
One of the main weaknesses of this work is how it is presented. 
Maybe it's me, but I found it quite hard to read. See questions.

Another concern is how theoretical results apply to real-world datasets. See questions, but Fig. 5 seems to mitigate the impact of your theoretical results.
In fact, for the vocabulary that grows larger, all the models have a similar value of cross entropy (for around ~50K tokens).

The article seems rushed, as there are many typos (I just listed some).
- Line 150 “the a”
- Line 173, “it make since” --> “sense”
- Line 175, eq. and many others --> Eq. (it’s not wrong per-se, but you capitalize Fig, Example, etc.)
- The Notation paragraph shouldn’t go with related works but should be in the next section.
- Notation in 2 is a bit sloppy (this is a personal suggestion): you can use D() and E() for the decoder/encoder (and enclose them with \mathcal).

Limitations:
Please see previous sections.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper offers theoretical insights into the importance of tokenization in language models. Tokenization is ostensibly the artifact that makes training LMs not an end-to-end procedure. This design choice introduces biases, as it is not optimized for exactly the same criterion as the full model. Yet training without a tokenization step almost always leads to worse language models. This paper attempts to provide reasons based in probability theory for this phenomenon. The authors first explore a toy setting, in which transformer models are tasked with predicting distributions from kth order Markov processes. They offer a theoretical explanation for why the error of models is capped at that of a unigram model and how tokenization alleviates this issue. They then show that tokenization schemes with certain properties can achieve the optimal cross-entropy loss. The work offers some basic experimental results confirming their insights.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Tokenization is a core part NLP pipelines yet it still needs to be better understood from a theoretical perspective. The questions that this paper tries to answer are very relevant for both model interpretability and further development
* The theory is presented in an understandable manner and results for specific popular tokenization schemes are provided.

Weaknesses:
* The theory presented in this work is for a specific type of data-generating distribution (kth order Markov) and we can’t directly extrapolate these results to linguistic distributions, which do not necessarily follow such a distribution. There is minimal discussion about the relationship between kth order Markov and linguistic distributions, which leaves the reader questioning how relevant these results actually are.
* Ultimately, the results are limited; they essentially show an expected result (the existence of an optimal unigram language model as the dictionary size grows to infinity). While some intuition can be gained from these results, the theoretical implications are limited.
* There is minimal discussion of the empirical results and what conclusions can be drawn from them. Given how much of the theory is not directly applicable to real language modeling settings, it feels like such a discussion should be very important

Limitations:
Limitations are not discussed in depth. The authors should address their limited experimental setting and the applicability of the results to linguistic distributions (which are not evidently k-order Markovian)

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the learning dynamics of unigram language models on top of tokenised vs non-tokenised data, comparing these models’ expected cross-entropy to the distribution’s entropy. The paper performs this analysis while considering different data generating distributions (mainly focusing on relatively simple markov chains), and different tokenisation methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper tackles an interesting and timely topic: how tokenisation enables language modeling. 

This paper provides an interesting theoretical analysis of the effect of tokenisation on unigram language modeling.

This paper also provides a couple of empirical analyses of how unigram models perform on real data.

The paper is relatively easy to follow, even though some of the mathematical results could be spelled out a bit more clearly to make it easier for a reader to follow them.

Weaknesses:
This paper’s framing, in my opinion, significantly over-claims its results:
* The title “Toward a Theory of Tokenization in LLMs” is very broad for the current contributions. A more appropriate title, in my opinion, would be “Analysing tokenisation’s effect on unigram distributions”, or something analogous to it. There is no “theory of tokenisation” being proposed here, but a theoretical analysis of how tokenisation affects a simple model’s cross-entropy.
* The abstract and introduction also significantly overclaim results, with statements such as “we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization” while focusing on unigram cross-entropies. Transformers may serve as motivation to this work (as they initially learn unigram statistics), but are not in fact analysed here.

I think the paper would also be significantly more straightforward to read if the framing was fixed and it was clear from the start that the paper's analyses would focus on unigram models.

Limitations:
I think some important limitations are not sufficiently discussed in this paper. The most important of which is that the analysis focuses on unigram statistics, and transformers can clearly learn more than that. Expanding the limitations pointed out by Remark 3.3 in a dedicated limitations section could also be useful.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
x7AD0343Jz;"REVIEW 
Summary:
This paper studies whether transformers can efficiently learn compositional discrete tasks. In particular, the paper introduces two new tasks: pointer execution neighbor and pointer execution reverse multicount as well as using multiplication and highest subsequence sum from prior work. First, small models are trained from scratch, showing substantially slower learning on the composition than on the subtasks. Next, API models are prompted to solve the same tasks and perform somewhat poorly. Some theory is also provided showing how models that memorize can struggle to learn compositions efficiently.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The paper proposes an interesting question as to whether we can determine whether a language model has some higher level concept of task composition that allows it to learn compositions of previously learned tasks efficiently. 

2. The paper includes a nice theoretical result via a complexity theory reduction that shows how composition is hard if we assume stylized models that memorize the training data.

Weaknesses:
1. H1 as written cannot be disproven empirically since the ""constant"" could just be larger than those tested. It seems in the experiments ""constant"" means 100. If that is what is meant, then just say so in the hypothesis. 

2. It is not clear if the notion of ""sub-task"" is somehow atomic and unique. This makes hypothesis H2 and H3 somewhat ill-defined too. It is possible that there are different sub-tasks (and perhaps many more of them) that better track how the model actually learns. Just because we can posit one way to compositionally solve the task does not mean that the model will learn that way (or even that it can necessarily represent that composition). 

3. It is not clear why the new tasks are necessary or what specifically they add over prior, simpler tasks. There needs to be more justification of the somewhat complicated tasks to explain why they are necessary. Generally the presentation of these tasks and the results was unclear and could use improvement to make it more visually clear how matches and transitions are meant to happen in the tasks and more precisely what all the baselines are doing in the experiments.  

4. It is not clear why one would expect an untrained small (150m) language model to somehow be able to compose subtasks without being trained to perform composition. As such, the results that the composition does not just arise and indeed takes longer to learn is not surprising. 

5. I am somewhat worried that the way the strings representing the nodes are designed is interacting badly with the tokenizers in the API experiments. These are clearly ""out of distribution"" types of words and they may be tokenized in ways that make it very difficult to solve the task. Did you do any analysis of how these strings get tokenized? The tokenizers are publicly available. Also, it is difficult to fit this section into the story of the paper since there is no comparison to the learning of the subtasks.

Limitations:
Limitations of not solving the issues raised are addressed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on analyzing the transformer language models' learning and transferability on compositional discrete tasks. Specifically, it has four hypothesis, and the author studies for a variety of language models, whether does these hypothesis hold. 
H1. An LLM can learn to perform a compositional task with constant number of datapoints. 
H2. An LLM can learn to perform a compositional task, given as many sample as the most difficult sub-task required.
H3. An LLM can learn to perform a compositional task, given the data samples of relearning all sub-tasks for learning the composition.
H4. An LLM can learn to perform a compositional task, given more data samples in H3.
The authors introduces a new benchmark for creating systematic sub-tasks and testing compositionally.
With LLaMA model, H4 holds; with both GPT-4 and Gemini, using H1 (prompting) fails to perform the tasks, or multi-round code generation with COT technique.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality: 3.5 / 5

This paper examines how the number of datapoint samples affects the learning of compositional tasks in existing transformer-based large language models (LLMs). The authors created a new, challenging compositional dataset based on computation graphs and demonstrated that learning compositional tasks with LLMs is highly data-inefficient. While the BIG-bench paper indicates the insufficiency of reasoning and compositional abilities in LLMs, this paper innovatively provides a concrete, quantitative, and extensive study on the extent of this insufficiency.

Quality: 3.5/5

The empirical study is pretty extensive with both LLaMA, GPT-4 and Gemini. There are multiple prompting techniques adopted with GPT-4 and Gemini, all of them fails to generate a reliable result. There are also very interesting theotrical proofs in the appendix to bolster the authors' claims. 

Clarity: 3/5

Figure 1 is hard to understand just by staring at the graph. For each task, it only provides one example which is non-trivial at all. One can hardly figure out its ground truth program for each example, and whether in a task of PE, is the underlying program the same across all the datapoints. I believe a descriptive caption by the side of each task is necessary. For example, PE refers to a program that takes a sequence of words and returns a list of words all colored green, where the first output word matches the first input word, and any subsequent output word starts with the last two characters of the previous word. However, the figures and tables in the experimental section are pretty clear and helpful to understand. 

Significance: 2.5/5

Understanding the problem of the data inefficiency in transformer based LLMs is important to the community which focuses on data efficiency and reasoning, such as neuro-symbolic community.

Weaknesses:
As stated in the strengths above. One of the main issue is the clarity issue of the tasks. Besides ""what is the task"", I also want to understand ""why these two tasks are needed"". What do PEN and PERM these two datasets bring?

Limitations:
I believe the stated limitation regarding addressing weaknesses in the LLM is not appropriate for this specific paper. Instead, the limitation should focus on the choice of the compositional dataset.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper evaluates the compositional learning abilities of Transformer-based models with LLaMA-like architecture on tasks requiring the composition of several discrete sub-tasks. To this end, the paper reuses two existing compositional algorithmic tasks and introduces two new ones, focusing on how many samples are needed for models to learn to compose the sub-tasks compared to the sample efficiency of learning the sub-tasks themselves. The study measures the efficiency of models when trained from scratch and the effectiveness of prompting the pretrained language models GPT-4 and Gemini. The experiments suggest that hypotheses that compositional learning requires no more samples than the sum of samples needed for each subtasks should be rejected. The paper also performs few-shot prompting with GPT-4 and Gemini with different prompting techniques to investigate their ability to learn to compose or decompose algorithms in-context and find that they are unreliable for executing sub-tasks or correcting errors in multi-round code generation. Finally, the paper uses complexity theory to support these findings, suggesting that when training feedforward models to memorize information with gradient descent, the sample inefficiency is inevitable.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Aside from achieving state-of-the-art performance on many academic benchmarks, transformer-based language models are the undisputed workhorse for numerous real-world applications. However, their training does not necessitate compositional learning explicitly, while many of the tasks they are tasked at solving do require such capability. As such, understanding the limits and requirements for these models to learn to compose independent skills is key to drive our understanding of these foundational models and to improve them.   
2. The analyzed tasks in the paper are very well defined to verify that a model that learns a task must know how to perform the subtasks, and that given capability to solve the subtasks, a model must only learn to compose these abilities to solve the task itself. Creating such settings is not trivial, and goes a long way to enhance our understanding of the compositional learning abilities of transformer models.
3. The paper provides a very thorough literature review and contextualizes the work around prior work very well.
4. The presentation of the paper is generally very nice, making a technical read easier and fluent.

Weaknesses:
1. The authors correctly identify tokenization as a possible negative confounder in the defined testbed, and thus use character-based tokenization for the training experiments. However, the same care is not taken when investigating the abilities of GPT4 and gemini to perform in-context-learning. Namely, given the highly synthetic nature of the inputs, it is highly possible that both the out-of-domain distribution of these inputs (deviating from natural texts) as well as differences in how inputs are tokenized (for example, one key can be tokenized with a single token, another by three tokens, and a third be tokenized along with parts of the corresponding value) confounds and skew the results, hindering their usefulness.
2. Moreover, while the authors study many prompting techniques to evaluate GPT4 and Gemini, they use a constant 8-shot prompting. It is known that these models can benefit greatly from increased number of demonstrations, and has been shown that for out-of-domain tasks, one has to prompt the model with significantly more than 8 prompts to perform well (e.g. Levy et al., 2022, Bertsch et al. 2024, Agarwal et al. 2024)
3. The proposed testbed is very well defined, but a single transformer based model is being studied. Understanding the contextualization of the results given difference in the model and tasks properties (for example width-depth ratio, scaling behavior with respect to parameter count or effect length of the inputs) would be very beneficial.ֿ

Limitations:
The authors discuss some limitations, but do not directly address the generalizability of their findings to natural tasks subtasks composition.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates the capabilities of Transformer-based language models in learning compositional discrete tasks. The authors evaluate both training LLaMA models and prompting GPT-4 and Gemini-Pro on tasks that require the learning of compositions of several discrete sub-tasks. The results indicate that these models exhibit significant sample inefficiency: LLaMA models require more data to learn compositional tasks than to relearn all sub-tasks from scratch, and in-context prompting with GPT-4 and Gemini is unreliable and often fails in multi-round code generation. The findings are supported by a theoretical analysis showing the sample inefficiency of gradient descent in memorizing feedforward models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper evaluates both training from scratch and in-context prompting methods, providing a thorough analysis of the models' capabilities.
- The authors introduce new algorithmic tasks designed to test compositional learning and providing a theoretical framework to support the empirical findings.
- The study offers a deep dive into the limitations of current LLMs, supported by both empirical data and theoretical arguments, which can guide future research in learning compositional tasks.

Weaknesses:
- The tasks and settings used in the experiments may not cover the full range of real-world applications, limiting the generalizability of the findings. 
- The performance and conclusions drawn are heavily dependent on the specific tasks designed by the authors, which might not fully represent other compositional learning scenarios.
- Personally, it took me a while to understand how the given algorithmic tasks are designed and how they relate to the broader context of compositional learning. For instance, the `PERM' problem was not immediately intuitive to me.

Limitations:
- The study focuses on a limited set of compositional tasks, which may not fully capture the diversity of problems faced in real-world applications.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
x69O84Df2G;"REVIEW 
Summary:
The present article extends the track-and-stop approach of Garivier et al. to a multi-reward MDP setup. Given an MDP problem with a finite number of reward functions the aim is to develop an algorithm that learns optimal policies for all reward functions simultaneously. Under (drastic) assumptions the authors present a sample based variant of track-and-stop in the multi reward setup. The algorithm is based on replacing the theoretical model complexity $T^*$ (in a multi reward variant) from Garivier et al. by an upper bound $U^*$ that can be estimated. Estimating $U^*$ during the exploration phase results in a practically implementable termination condition that results in worse complexity than the theoretical termination condition. The algorithm is tested on a few (very similar) tabular examples and compared to simple algorithms. An educated guess is performed to design a deep variant of the algorithm that is tested on a multi reward card pole variant and deep sea. Results beat easily the (terrible) results of the benchmark algorithms used.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
The article is extremely dense with information, perhaps it would have been better to split the article in 2 or 3. The theoretical development is an extension of several earlier articles of the authors with very detailed mathematics. While the tabular examples are of limited practical interest (the assumptions are just too strong) the deep variant is interesting. While I am not so sure about the relevance of the tabular setting, the relevance in the deep setting is obvious (and was realised of the authors for cardpole). The ability of generalisation of NN makes it interesting to train the network simultaneously on perturbations of the true reward function in order to learn a more stable policy that might work well for even more reward settings.

The scientific quality of the article is very high. There is hardly any typo. I appreciate a lot the critic discussion of limitations, this is far above the rather low scientific standard in ML. I could not check the entire proofs of the appendix, what I read seemed solid. 

Good article! I am curious to follow up on the future development of the robust DQN variant.

Weaknesses:
- I did not enjoy reading the article very much as I was pushed into first reading other articles to get a rough understanding of what is going on. Even reading the basis of the present article ([17], [40]) was not enough. To get an idea why $T^*$ is considered one needs to go all the way back to [20], and even further. It feels a bit like a community wants to stay among each other, the usual RL researcher is excluded by the style of writing. I would suggest to completely skip the discussion in the end (almost a page) and instead use the space to explain the reader what the technique is about and why the theoretical estimate naturally lead to Algorithm 1.
- The assumptions are drastic and should be discussed more clearly. I guess the authors have bandit background and skip discussions of issues that are typical for bandits. In particular, assuming knowledge of rewards and/or the reward gaps. While this is not unusual in bandits it is very much in RL. I am perfectly fine with such theoretical results in particular, as the authors implemented an educated guess algorithm in the deep setting that addresses this issue with the reward gaps.

Limitations:
The tabular setting requires drastic assumption (but gives a clean analysis), the deep setting is an interesting educated guess but does not allow a mathematical analysis. I would say that is quite normal for RL, except, perhaps, the deviation of the educated guess Algorithm 2  from the tabular algorithm 1 is pretty big.

There is no good benchmark to compare to, this makes the numerical results a bit boring. Perhaps a better benchmark would be to compare to training individually the policies. While that might even beat the proposed algorithm in the tabular case (for few rewards) I doubt the same holds for the deep case as the NN would only remember to solve the last reward problem.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of best policy identification for RL with multiple rewards. The goal is to efficiently identify the best policy for given rewards with a high-level confidence. Authors provide an instance-dependent lower bound for the studied problem and introduce a provably-correct algorithm for a convex approximation of the original problem in the tabular setting. Extensions to deep RL is discussed with numerical results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors demonstrate how to efficiently identify optimal policies across multiple rewards. The strengths of this work are summarized as follows:

1. The studied setting is interesting and of practical concern when we seek to optimize the performance across a set of rewards.

2. An instance-dependent lower bound is identified for any Probably-Correct algorithm in the setting of multi-reward best policy identification problem.

Weaknesses:
While this paper clearly articulates the idea of performing, here are some problems and potential improvements that authors are suggested to address and consider. More specifically,

1. Environments for the deep RL part is too simple. The Cartpole swing-up and DeepSea environments cannot fully demonstrate the performance of the proposed algorithm in more complex, real-world scenarios. It would be beneficial to include experiments on more challenging benchmark environments for better assessment of scalability and practical applicability.

2. The policy optimality is only considered and defined for stationary deterministic policy (as in definition 2.1), which can be too restrictive. It is not clear when considering the set of Markovian policies (which can be stochastic), whether the proposed lower bound still holds, and whether the performance of the algorithm is still optimal. 

3. Theoretical guarantees for the deep RL extension is unavailable. Sample complexity bounds are only provided for tabular settings, which leads to the dependencies on the cardinality of state and action space. And empirical studies for the deep RL settings are not sufficiently convincing due to the simplicity of the environments.

4. In terms of the theoretical results of the lower bound, the proof structure closely follows the prior results (e.g. Marjani et al. 2021, Taupin et al. 2022) in single-reward RL. It is not quite obvious what are the main technical challenges and novelties of extending the prior results (e.g. Marjani et al. 2021) from single-reward RL to multiple-reward RL.

5. While the studied setting can be interesting, the relationship between the MR-BPI problem and reward-free RL as well as multi-objective RL somewhat remains unclear throughout the context. Though discussion has been provided in Section 1 and 5, I am not fully convinced that reward-free RL cannot solve the concerned practical scenario in multi-reward best policy identification problem. Indeed, reward-free RL assumes rewards are unknown, whereas the studied settings assume the knowledge of rewards. As a result, it is not surprising to see that properly utilizing the knowledge of rewards can lead to better performance as shown in the numerical results: the proposed algorithm (MR-NaS) significantly outperforms RF exploration strategies (RF-UCRL, ID3AL). However, reward-free RL is a more general type of algorithms and can be particularly useful in practice when it is hard to accurately learn rewards or when rewards are sparse.  As such, the emphasis of the two settings are rather different. It might not be a fair comparison, and it is desirable to provide the fundamental reasons that can explain such performance improvement in numerical experiments. Therefore, more thoughtful insights should be provided to clearly explain the difference and relationship between these settings.

6. Some minor aspects:
    - Grammatical errors need to be addressed, e.g. Line 354, line 91.
    - In line 75-80, if only deterministic policies are been considered, it is more appropriate to write $a_t = \pi(\cdot|s_t)$ etc. Do not use the probability simplex notation for policies.

Limitations:
There are no unaddressed ethical considerations in the studied context.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the challenge of identifying the best policy in RL when there are multiple rewards. The authors get a lower bound on the sample complexity and design an optimal exploration policy. The authors propose two algorithms: MR-NaS for tabular environments and DBMR-BPI for Deep RL. These algorithms efficiently identify the best policy across multiple rewards in RL.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The paper presents a comprehensive and well-balanced analysis of theoretical and empirical results. The appendix provides supplementary evidence that strengthens the authors' arguments.

Weaknesses:
1. The paper  is inspired by [17], but it would be better to explicitly acknowledge this inspiration in the main part, rather than mention it at Remark C.2. Furthermore, a more in-depth discussion of the challenges in proof caused by the novel forcing policy would strengthen the paper's contribution.

2. Even though more details is covered in the appendix, the paper should provide some details on the convex optimization. For example, a discussion of the computational costs associated with these methods would provide valuable context for readers. Sometimes, the computational cost of convex optimization methods are high. 


3. The abstract would be better if providing a more impressive motivation for multi-reward RL, emphasizing its significance and potential impact.

4. Didn’t put some innovative aspects in the main text, leaving vital details to be discovered in the appendix. This can lead to important contributions being overlooked.

Limitations:
As noted in the paper, the assumption of a unique optimal policy limits the method's applicability to a broader range of scenarios. The computational cost of convex optimization may prevent it to deal with large-scale dataset.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
x4Kk4FxLs3;"REVIEW 
Summary:
The paper introduces PARD, a graph generation model that combines autoregressive and diffusion models. Traditional autoregressive models are effective but sensitive to order, while diffusion models are permutation-invariant but need many denoising steps and extra features. PARD overcomes these issues by generating graphs block-by-block using a partial order of nodes and edges. It employs a shared diffusion model with an equivariant network and a higher-order graph transformer, supporting parallel training like GPT. PARD achieves state-of-the-art performance on various datasets, including large ones like MOSES, without extra features.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
1. This work presents a successful showcase of the combination of autoregressive modeling with diffusion model on graph.
2. The proposed partial order ensures the permutation-invariance in the autoregressive generation process.
3. Impressive experimental results show the effectiveness and efficiency.

Weaknesses:
1. It is unclear how the diffusion model is employed in PARD. Sec 3.1 and the second part of Eq. 6 are not quite relevant to each other. Can you elaborate on that?
2. Please provide proof or reference for some statements. e.g.: 2-FWL expressivity for the proposed higher-order transformer
3. Please provide the results of other baselines on QM9 if possible.

Limitations:
There is no discussion about the limitation in the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a graph generation method that combines AutoRegressive (AR) models and diffusion models. By utilizing a unique partial order, it addresses the issue of non-exchangeable probabilities in AR models and the efficiency problem in diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed block-wise AR diffusion model in this paper offers a new idea for graph generation, particularly by introducing the use of weight-degree to differentiate blocks.
2. The limitations of equivariant networks demonstrated in this paper also hold value for further exploration and resolution within the community.
3. The overall structure and writing of the paper are relatively clear.

Weaknesses:
1. There is a part in the paper that I believe needs to be clarified more clearly to ensure logical coherence. Why does diffusion based on equivariant network solve the flaw in equivariant modeling? I think besides the analogy of tempering iron (or higher/lower energy), more mathematical proofs are needed.

2. Ablation of PPGN is necessary to demonstrate its effectiveness.

3. Following the experimental settings of GDSS, NSPDK is also an important metric for QM9 and ZINC250K.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes a new graph generative model based on an autoregressive procedure. It proposes an approach to deciding a partial order of graph nodes according to their degrees in a node-removal procedure. Based on the partial order, the work devises a new graph generative model.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
The graph algorithm of deciding a partial order of graph nodes would be interesting if such an algorithm does not exist in the literature of graph theory.

Weaknesses:
The work lacks justification. As the field has moved to generative methods with discrete-diffusion models, which are already permutation-invariant, it is less clear about the advantage of designing a complex autoregressive model to satisfy the permutation-invariant property. 

The advantage of the model is not obvious even considering only autoregressive models. Note that Chen et al. [9] have an approach of ""optimizing"" node orders for the generative model and show that the likelihood calculation is more accurate with their approach than a pre-determined order. How does the work justify its advantage over such an approach?

The analysis in 3.3 does not seem to be reasonable. The **probability calculations** are indeed the same for nodes in the same orbit, but they may get different connections in the sampling procedure and then break the symmetry. The analysis in 3.3 is well known, and it is not a concern for generative models. In some diffusion-based generative models, the starting graph is a graph with no edges, then all nodes are in the same orbit, but it is not an issue at all because the edge sampling process will break the symmetry. 

Without clear justification, I don't know where performance improvements are from (maybe architecture improvement?). I feel that the work should have a thorough investigation of the model.

Limitations:
The proposed model seems to have a long running time because it needs to run a diffusion model at the generation of each block.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to integrate autoregression models with diffusion models seamlessly to harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without order sensitivity. It also proposes architectural improvement to make the model and algorithm efficient and scalable. The presentation is smooth and the experimental results on both molecular and general graph generation demonstrate its effectiveness.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
It proposes a novel graph decomposition method considering not individual node and its degree but subsets of nodes with structual similarity. In this way, it removes node order sensitivity in the graph but only needs to maintain the order of the blocks. Within each block, the diffusion model focuses on a much smaller graph and thus has the efficiency to generate a denoised graph.

Weaknesses:
It would be better if the authors can provide some insights about the hyperparameter the maximum of hops $K_h$.

Limitations:
The authors have adequately mentioned several limitations of their work which sound quite reasonable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
x4HMnqs6IE;"REVIEW 
Summary:
This paper presents a method called $\text{ID}^3$ for the task of synthetic face recognition. The authors highlight that the accuracy of face recognition using generated data still lags behind that of training directly on real face data. They propose optimizing the generation process from the perspectives of diversity and consistency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Clear explanation of formulas and algorithm flow.
- Achieved SOTA results compared to methods from the past two years.

Weaknesses:
- There has been extensive research on ID preserving, and recent models based on LDM (e.g., Face0, PhotoMaker, FaceStudio, InstantID) can also be used for synthetic face recognition. The paper lacks analysis and comparative experiments on these models.
- The Face Attribute Conditioning Signal includes age and pose (pose angle range: [-90°, 90°]). However, the visual results in the paper do not reflect these attributes. The variation in pose is minimal, and there is no demonstration of different levels of age (which you mentioned as [0-100]).
- The paper devotes too much space to mathematical derivations and lacks intuitive visual results. For example, using different attributes and ID information to guide the model could be visualized by showing how the various layers of the Unet perceive this information.

Limitations:
Using ID attributes to assist in the generation results is already common in diffusion-based tasks. This method is essentially a conditional guided generation, and its technical contribution is limited.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on synthetic face recognition and proposes to concentrate on three aspects: inter-class diversity, intra-class diversity, and intra-class identity preservation. Based on those, an ID-preserving loss is employed to generate diverse but identity-preserving facial images. This paper also demonstrates the proposed loss is equal to lower bound of an adjusted conditional log-likelihood over ID-preserving data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work is well-written and well-organized. It brings some insights for SFR.
2. The idea of 3 aspects is good, and quite general for SFR
3. The proposed method shows advances when using the FFHQ dataset

Weaknesses:
Here are several concerns regarding this work:
1. The idea of Attribute Conditioning Signal is not fit for synthetic face recognition tasks, because factors contributing to solid FR training cannot be determined by simply adjusting face attributes. One reason is that the attribute network (e.g., pose, age) is not generalized enough, as the pre-trained models are obtained from relatively small-scale datasets compared to FR datasets. Additionally, the authors have not addressed which attributes are effective for FR, leaving this important question unanswered.

2. The performance trained on FFHQ dataset appears good; however FFHQ dataset has explicitly banned its use for face recognition applications. Furthermore, FFHQ is relatively small(210k images) which doesn’t contain enough diversity, that’s the reason facial attributes can be of improvement in this experiment. For more details on FFHQ please refer to: https://github.com/NVlabs/ffhq-dataset

3. When it comes to the relatively large dataset CASIA-WebFace, the improvement over DCFace is marginal. One problem is that DCFace is trained with CASIA-WebFace only, not the FFHQ+CASIA mentioned by the author.

4. Experiments are not sufficient. For example, DCFace provides experiment results on 3 data volumes: 500k, 1M and 1.2M. These are not included in this paper.

5. There are some typos, for example, Y_i should be given in line 194

Limitations:
This paper shows an attempt to generate diverse facial images of each identity. However, what makes solid FR training is not studied. Furthermore, domain GAP exists when adopting the trained diffusion model for generation, the input embedding might differ from the embedding of the synthesized image. Consequently, the focus is on changing facial attributes but preserving identity is interesting but the overall improvement is marginal, and FFHQ has license issues related to the application of Face Recognition. I think it will be more convincing if the results of CASIA-WebFace under 1M and 1.2 M settings are presented.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes ID3, an identity-preserving-yet-diversified diffusion model for generating synthetic face data for face recognition. ID3 leverages identity embeddings and facial attributes to control inter-class and intra-class diversity of generated faces while preserving intra-class identity consistency, demonstrating state-of-the-art performance on multiple synthetic face recognition benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
See questions section in detail.

Weaknesses:
See questions section in detail.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper ""ID3: Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition"" introduces a novel synthetic face recognition (SFR) approach using diffusion models. It focuses on maintaining identity consistency while providing high diversity in generated face images. The proposed ID3 model leverages identity-preserving losses and a structured sampling algorithm that respects identity characteristics. This effectively addresses the common pitfalls of existing SFR approaches that lead to poor generalization on real-world data.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
*   **Originality**: The paper presents an innovative use of diffusion models tailored to synthetic face recognition, emphasizing identity preservation.
*   **Quality**: Demonstrated improvement over state-of-the-art models through extensive benchmarking.
*   **Clarity**: Exceptionally clear presentation and thorough explanation of the methodology and results.
*   **Significance**: This paper addresses significant challenges in synthetic data generation and offers substantial benefits for training more robust and generalizable face recognition systems.

Weaknesses:
*   **Generalization**: Additional tests on further diversified real-world datasets could strengthen the generalization claims.
*   **Complexity**: It would be beneficial to have details on the computational demands and scalability of the model when deployed in practical, real-world scenarios.

Limitations:
The paper discusses potential limitations, including the need for extensive computational resources and the model's performance dependency on the quality of input identity embeddings. It also mentions the ongoing challenge of bridging the gap between synthetic and real-world face recognition performance.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
x4EoTQW7ka;"REVIEW 
Summary:
The paper introduces DropBP, an innovative approach to accelerate the fine-tuning of Large Language Models (LLMs) by selectively dropping layers during backward propagation. This method is presented as a means to reduce computational costs and activation memory, significant challenges in the efficient fine-tuning of LLMs. The authors have provided a clear implementation of DropBP as a PyTorch extension and demonstrated its effectiveness through experiments on various LLMs and datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The concept of dropping backward propagation layers to reduce computational overhead is differential from previous work and addresses an important issue in training large models.

- The paper includes extensive experiments that validate the effectiveness of DropBP in reducing training time and memory usage while maintaining accuracy.

- The development of a PyTorch extension for DropBP facilitates easy integration with existing training codes, enhancing the practical applicability of the method.

Weaknesses:
- The motivation is not well illustrated. I agree with that dropping sublayers could lead to training efficiency as the model turns to a shallower counterpart. However, I mean, pervious work like LayerDrop and others omit the layer computation in the forward pass. Then the computation could be removed in the subsequent backward computation with essential engineering efforts. Thus it lacks a clear distinction in terms of technical innovation compared to these previous works.

- While the paper proposes omitting sublayer computation in the backward pass, it's unclear why the forward pass computation remains unchanged. Justifying this choice or exploring alternatives would strengthen the contribution.

- The faster convergence observed in Figure 5 with DropBP compared to the vanilla model is counterintuitive. The observation here quite confuses me since the backward pass optimizes a partial computation graph, concerns regarding overfitting arise. The paper would benefit from a discussion on potential regularization techniques employed to address this, and a comparison with related work (e.g., [1]) that utilizes sublayer dropping for regularization in training a deep Transformer model. 


  
  [1] Li et al., 2021 (AAAI) Learning Light-Weight Translation Models from Deep Transformer

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel method to reduce the computational and memory costs associated with fine-tuning large language models (LLMs). The authors introduce DropBP, a technique that randomly drops layers during backward propagation, effectively reducing the computational operations (FLOPs) and activation memory needed. This method assigns drop rates based on the sensitivity of each layer to ensure stable training. The approach is applicable to both full fine-tuning and parameter-efficient fine-tuning (PEFT) methods. The paper reports significant improvements in training time, convergence speed, and maximum sequence length when fine-tuning LLaMA2 models with DropBP.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- DropBP introduces a novel method for reducing the computational and memory costs associated with fine-tuning LLMs. This is an important contribution to the field, given the increasing size and complexity of these models.

- The paper provides empirical evidence that DropBP significantly reduces training time (by 44%), accelerates convergence (1.5× faster), and increases the maximum sequence length (up to 6.2×) on a single NVIDIA A100 GPU. These results demonstrate the effectiveness of the approach. The authors conduct thorough experiments on multiple datasets and models, providing a robust evaluation of DropBP's performance across different scenarios.

Weaknesses:
- The paper mentions that the sensitivity calculation is done only once and has negligible overhead. However, more details on this process and its potential impact on training time would provide a clearer understanding of any trade-offs involved.

- The paper could benefit from a more detailed theoretical analysis of why DropBP works as effectively as it does. This would strengthen the paper by providing a deeper understanding of the underlying principles.

Limitations:
Limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed to drop layers during backward prop (BP) based on layer sensitivity. The method aims to reduce the cost for gradient computation and storage for intermediate activation in full BP.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Reducing the cost of full BP in PEFT has been an important challenge. 
2. The method is simple and is easy to integrate to either full fine-tuning or PEFT. 
3. Experiments demonstrate that DropBP can speed up the training while retaining the accuracy. The resulting memory reduction makes longer sequence modeling accessible.

Weaknesses:
1. The idea of optimizing NNs with sparse gradient is not new. This paper needs to add more discussion and comparison with related works in sparse learning e.g., [1-3]
2. Table 1 only shows results on two datasets and limited benchmark. It is unclear if the method works well for generation tasks and domain-specific transfer learning.
3. It is unclear which algorithm is used to solve the constraint minimization problem, i.e., to determine the layer-specific rates based on sensitivity, and its extra computational cost.
4. (Minor) In fine-tuning, DropBP drops a set of layers. However, the sensitivity of a set of layers may not be accurately represented by the direct summation of the sensitivities of individual layers in the set.

[1] Sun, Xu, et al. ""meprop: Sparsified back propagation for accelerated deep learning with reduced overfitting.""
[2] Sung, Yi-Lin, Varun Nair, and Colin A. Raffel. ""Training neural networks with fixed sparse masks."" 
[3] Brock, Andrew, et al. ""Freezeout: Accelerate training by progressively freezing layers.""

Limitations:
The authors have addressed the limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wlqfOvlTQz;"REVIEW 
Summary:
This paper introduces reinforcement learning (RL) problems where agents observe one-step lookahead information (either rewards or transitions) before choosing actions in episodic tabular MDPs. Two relevant lines of work exist: the control literature, which studies a similar lookahead concept in the continuous state-space scenario, and the RL planning community, which commonly obtains lookahead information from learned transition models. However, this paper assumes the reward/transition information to be available before selecting an action. The core contributions are:

1) Formalising the look-ahead setting for the reward and transition in an episodic MDP setting.
2) Derivation of the Bellman equations in the original space by setting up an equivalence with an equivalent new MDP.
3) Development of two algorithms for reward (MVP-RL) and transition lookahead ( MVP-TL).
4) First sub-linear regret bound win the lookahead setting.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is the first to provide regret bound on the lookahead learning setting. This encompass a somewhat broad spectrum of problems that were independently studied such as the Canadian traveler problem and the prophet inequalities. 

They paper is well written and easy to follow for non-expert in learning theory. It presents the core ideas in an understandable way in the main paper and use the appendix for technical proofs.

Weaknesses:
The paper could be strengthened by adding experimental results studying the difference in behaviour and performance between standard RL algorithm, MVP and the proposed solution MVP-RL. More specifically, I would be interested in understanding the difference in behaviour when changing the tails of the reward/transition distributions.

Limitations:
The limitations outlined in the paper provide a fair representation of how the theoretical results could be extended in various directions, such as multi-step and stochastic action sets.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors proposed new forms of Bellman equations for environments where the agent knows the reward or transition outcomes one step ahead (without knowing the full model).

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
While previous papers (e.g., Boutilier et al. 2018) discussed utilizing lookahead information (and proved convergence), the authors claim they are the first to present regret results.

Weaknesses:
While the theoretical contribution is clear, the authors must also provide practical validation.

Limitations:
Some aspects of the approach might be seen as incremental advancements  rather than groundbreaking theoretical analysis .

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This manuscript proposes the RL method with lookahead information. The authors discuss two scenarios: reward lookahead and transition lookahead. Under such scenarios, the proposed method estimates the reward distribution and transition distribution, respectively. Then the monotonic value propagation skill is applied to calculate the value function. The authors show that the proposed method has strong theoretical properties and the reward regret is strongly bounded under two circumstances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The manuscript is well organized, and the structure is clear. The authors shows very promising bound for both reward lookahead and transition lookahead scenarios.

Weaknesses:
This is a theoretical paper, however, the authors miss to deliver some numerical or empirical studies. It is suggested to add some empirical experiments, at least with simulated data. 

Algorithm 1&2 shows the procedure for training, I am confused about the inference process. How to select the action give certain state in inference? The authors are suggested to give some explanations in the Algorithm 1&2.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the setting where the agent can see the possible next rewards and next states without assuming a prior knowledge of the environment dynamics. The predicted next rewards and next states are estimated by empirical distribution. The paper considers extending Monotonic Value Propagation to such a setting and proves that the proposed algorithms can achieve tight regret bounds.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A tight regret bound is proved for the proposed algorithm, establishing theoretical justification for lookahead information and advantages of planning in RL in general.
- The paper does not assume known environment dynamics as in most previous works, which makes the algorithm applicable to standard RL settings. The lack of known environment dynamics may bring various challenges to planning, such as agents not relying on the lookahead information when the estimated environment dynamics are still far from the true one in the early stages. The paper shows that the lookahead information can still be very beneficial despite such challenges.
- The paper is well-written, and the proof is easy to follow.

Weaknesses:
Even though a tight regret bound has been proved, empirical experiments with examples showing how the agent uses the lookahead information will strengthen the paper.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies an RL problem with a special setting, called one-step lookahead, where the agent can observe the reward or the state at the next step before the current action is taken. The paper focuses on the problem with an unknown environment (transition function). The authors proposed an efficient algorithms leveraging the empirical distribution of the lookahead information and claimed that the algorithms achieve tight regret against a strong baseline.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper studies an interesting RL problem where one-step lookahead information is available to the agent while the environment is unknown. 

2. The paper clearly presents the problem, the solution, and a comparison between the proposed algorithm and the baseline in terms of regret bound.

3. The paper offers explanation of the terms in the regret bounds and justified its explanation.

Weaknesses:
1. One concern is the application of such a lookahead setting. The agents during training and running needs to know what will be realized in order to make actions at the current state. Not sure what real-world scenarios this setting can be applicable to.


2. RL with lookahead information has been investigated before from a theoretical point of view. See [R1, p64]. [R2] [R3]. [R1] discusses the lookahead in the approximation of the bellman function. [R2-R3] considers controlled lookahead where the agents decide the step of lookahead as a strategy. It is not straightforward to see in this paper how the lookahead studied in this paper different from those references. 

[R1] Bertsekas, Dimitri. Reinforcement learning and optimal control. Vol. 1. Athena Scientific, 2019.
[R2] Biedenkapp, André, et al. ""TempoRL: Learning when to act."" International Conference on Machine Learning. PMLR, 2021.
[R3] Huang, Yunhan, Veeraruna Kavitha, and Quanyan Zhu. ""Continuous-time markov decision processes with controlled observations."" 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, 2019.


3.  It is not clear the source of the baseline mentioning in the paper. For example, ""compared to a stronger baseline that also has access to lookahead information"". The paper should includes the reference whenever the baseline is compared with the proposed solution.

Limitations:
Discussed in the Weakness section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
veMnGKXvTx;"REVIEW 
Summary:
A Homology Consistency (HC) constraint for efficient transfer on VLMs is proposed in this paper, which explicitly constrains the correspondence of image and text latent manifolds through structural equivalence based on persistent homology in downstream tuning.
The proposed method tracks the persistence of the homology classes of topological features across multiple scales and guide the directions of persistence tracks in image and text manifolds to coincide each other. Additionally, a deviating perturbation is applied to generalize the persistence coincidence to unseen data. Experiments on recognition and generalization tasks show the superior performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper is well-written with a straightforward motivation.
2. A theoretically well-founded homology consistency (HC) constraint based on persistent homology is proposed for efficient transfer on VLMs.
3. Experiments on recognition tasks show the superior performance.

Weaknesses:
The hyper-parameters η, λ, ω should be determined at 16 shots and then migrated to other few-shot settings. If the number of samples is less than 16, how should the aforementioned hyper-parameters be set, and will there be a significant difference in performance?

Limitations:
The experiments only conducted on recognition tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper identifies a key issue with existing methods for tuning pre-trained vision-language models to downstream tasks with limited data: they adjust the alignment between image and text based solely on observed samples, which may not generalize well beyond the training data. To address this issue, the paper proposes a novel constraint from the perspective of topological data analysis.

This constraint employs persistent homology to ensure the structural equivalence of image and text latent manifolds during tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.  The paper offers a new way of looking at model tuning through the lens of topological analysis, with a focus on understanding the structure of data spaces for better semantic alignment in vision-language tasks. I appreciate this perspective on the issue.

2.  The proposed method exhibits a thoughtful theoretical underpinning, using persistent homology to enhance the generalizability of image-text alignment adjusting.&#x20;

3.  The paper is well-written and the reason for leveraging topological data analysis to enhance semantic alignment during the tuning process is reasonable and easy to follow up.

Weaknesses:
The paper does not adequately discuss how it relates to existing image and text alignment techniques, including those based on distance metrics, mutual information, adversarial training, and attention mechanisms. This lack of comparative analysis creates a gap in fully appreciating the distinctive contributions and potential advantages.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a Homology Consistency (HC) constraint for efficient transfer learning on vision-language models (VLMs), ensuring task-specific image-text alignment while preserving general knowledge by using structural equivalence based on persistent homology. This approach mimics the topology of latent manifolds and tracks the persistence of topological features.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well motivated, and the motivation of using homology consistency is interesting. 
2. This paper has a good theoretic support.

Weaknesses:
1. The performance of the proposed method is worse than the baseline method in low-shot (1-shot and 2-shot) tasks. 
2. The improvement in Table 2 is marginal. Is the comparison fair with the same random seed? How many runs did you conduct? Could the authors also report the standard deviation of the score? 
3. Moreover, is 16-shot common in this benchmark? 16 shot seems a lot in few-shot learning. 
4. Can you also elaborate more why with only DP, the performance drops in Table 3? 
5. In addition, could you elaborate more why choosing 0-th homology classes? What are the potential effects of using other homology classes?

Limitations:
My concern is the performance improvement is marginal and limited to more shots setting (16-shot).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes Homology Consistency (HC) constraint for transfer learning on VLMs, and it explicitly constrains the correspondence of image and text latent manifolds by structural equivalence based on persistent homology in downstream tuning.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is well-founded and clearly explains the proposed homology consistency (HC) constraint.

2. Extensive experiments are performed on 11 benchmark datasets.

Weaknesses:
1. The paper lacks discussions on the computational cost of the proposed techniques.

2. The proposed method for constraining the structural equivalence of image and text latent manifolds seems generalizable to other learning tasks for vision-language models. However, the proposed method is only evaluated for few-shot learning of vision language models.

3. Although the model outperforms other methods in most cases, the improvements are relatively marginal.

4. The paper only applies the method to a limited number of adapter models (TaskRes and Tip-Adapter-F).

Limitations:
The limitations are not discussed sufficiently (see weaknesses).

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
x33oWJQyH0;"REVIEW 
Summary:
This paper proposes an autoencoder based object detection model that makes predictions about object positions in an unsupervised manner. Imporantly, the authors can provide theoretical guarantees/ bounds about the degree of the model's detection error.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well written and it is easy to follow the authors motivation and structure of the work. I also find the idea very valuable to investigate the theoretical bounds of such models.

Weaknesses:
I have summarized my questions and issues and limitations that I see here: 

In the context of the CLEVR experiments, I am wondering why the authors don’t evaluate concerning the Gaussian standard deviation as they did for the first dataset?

The authors claim the method requires dynamic objects, but they never mention this in main text. Can the authors provide more justification of this? I.e. why this is/ what part of the approach requires this?

I don’t understand how the decoder can learn to reconstruct complex shapes other than spheres (due to the Gaussian assumption). Also the authors mainly evalaute on data with objects that are spherical. Thus, is it possible to evaluate on other shape forms? If so what is the error here compared to spherical shapes? I do not mention this as a limitation, but it seems quite important to put the method into context. What would be potential ideas to mitigate handling more complex objects?

I do not have enough knowledge about the details of the CutLer and SAM models, but why should the theoretical bound of this work hold for these works as well (the authors compare these in Fig. 6)? Specifically, the authors state ""only for our
method are the position errors always guaranteed to be within our theoretical bound."" so my question is: why should the other methods lie within this theoretical guarantee?

I am a little confused by the related works section. The authors discuss object-centric representation methods whose goal, unlike that of their method, is to learn a full representation of an object. This includes much more information than just position. In other words, it seems the method of this paper focuses “only” on the learning the position of an object. While this does not diminish the significance of the work, I think the work could benefit from discussing more on this difference between these works, to make the comparisons more fair and also focus more on works that focus on unsupervisedly localising object in images (i.e., works that only focus on position and not on the other aspects of object reperesentations), e.g., [1,2]. So in the end I am also wondering if the authors should actually narrow down the title/contribution claims to ""Unsupervised Object Localisation with Theoretical Guarantees""?

If the authors can remark on these issues above, I am happy to consider raising my score.

[1] Siméoni, Oriane, Chloé Sekkat, Gilles Puy, Antonín Vobecký, Éloi Zablocki, and Patrick Pérez. ""Unsupervised object localization: Observing the background to discover objects."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3176-3186. 2023.
[2] https://github.com/valeoai/Awesome-Unsupervised-Object-Localization

Limitations:
see above

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores Unsupervised Object Detection with Theoretical Guarantees. This method is a significant advancement in the field of object detection as it provides theoretical guarantees on the accuracy of the detected object positions. By introducing a new approach that ensures reliable object localization, the research contributes to enhancing the robustness and accuracy of unsupervised object detection systems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The method provides theoretical guarantees on recovering true object positions up to small shifts, which is a significant strength compared to traditional empirical approaches in object detection. The ability to interpret the latent variables as object positions enhances the interpretability of the model and facilitates understanding of the learned representations. The use of an autoencoder with a convolutional neural network (CNN) encoder and decoder, modified to be translationally equivariant, offers a unique and innovative approach to unsupervised object detection.

Weaknesses:
This work explores the unsupervised object detection, and theoretical analysis. However, the dataset for the experiment is not common, and few comparative experiments with common SOTA object detection model. Besides, although this work provides the theoretical guarantees to recover the true object positions up to quantifiable small shifts, there is no analysis whether it only exists in the unsupervised domain,.or can be adopted in the supervised domain.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new idea for unsupervised object detection where an CNN based auto-encoder architecture is employed and the latent representation is trained to learn position of objects in images. They further provide theoretical analysis of the proposed idea under strong assumption about the input data and model characteristics. Results from on synthetic data experiments is also provided

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea presented in the paper is interesting as it tries to solve the object detection problem in an unsupervised manner by modeling the latent space such that it explicitly learns object positions.

Weaknesses:
The paper lacks results and discussion on the experimental details on how the idea can be effectively implemented. This is particularly important to understand the merits of the proposed idea as it has strong assumption on model architecture and input data (e,g, size of objects). For example, it is not clear how the authors processes input data during training, how the min-batch sampling is done, what input-target pairs are?, what regulations are important to use if at all, how the over-fitting is prevented given the very simplified experimental setting.
Furthermore, it is not clear from the paper how the latent space can learn any semantic information to reconstruct the images as it modeled to learn the position of the objects.

Limitations:
- The work is limited by its assumptions on the characteristics of the input data and model architecture. 
- The theoretical analysis is dependant on strong assumptions like ""the objects are reconstructed at the same positions"" which itself is not guaranteed. 
- Furthermore, the evaluations do not provide insight into what challenges one should address to successfully train a model based on the proposed idea. 
- Please see the Weaknesses for more details.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents the first unsupervised object detection approach that is theoretically shown to recover the true object positions up to quantifiable small deviations that are related to the encoder and decoder receptive field sizes, the object sizes, and the widths of the Gaussians used in the rendering process. The authors conduct a thorough analysis of how the error depends on each of these variables and conduct synthetic experiments that validate our theoretical predictions up to a precision of individual pixels. 
On a high level, their architecture is based on an autoencoder that is fully equivariant to translations, which they achieve by making the encoder consist of a CNN followed by a soft argmax function to extract object positions, and making the decoder consist of a Gaussian rendering function followed by another CNN to reconstruct an image from the object positions. 
The authors also conducted synthetic experiments, CLEVR-based experiments, and real video experiments that validated their theoretical findings up to a precision of individual pixels.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
I do like the analysis of the current state-of-the-art detection models SAM and CutLER and it is interesting to find that in some cases their errors are much higher than the bound derived by this method.

This paper is well-written and easy to follow.

Weaknesses:
1. It is interesting to learn that SAM and CutLER's errors are sometimes much higher than the bound derived by the proposed method. I would be interested to hear from the authors if they have any insights on how this finding could be used to improve these methods, especially CutLER, which is also an unsupervised object detection and instance segmentation model.

2. The majority of the experiments in this paper are conducted on synthetic datasets, and it is questionable whether the findings can be generalized to real images and videos. Could the authors provide some experiments on real images or videos? 

3. Continuing on the previous point, most objects in the synthetic datasets are rigid and have very consistent shapes. However, the challenges in object detection are often in detecting the non-rigid objects or partially occluded objects. I am curious to see if the proposed method can be used to handle these cases.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
x2zY4hZcmg;"REVIEW 
Summary:
Naive model predictive shielding may overly restrict exploration thereby preventing an RL agent from learning a policy with good performance. In order to prevent this, the authors propose a method to optimise a backup policy that is provably safe using an online planner. An approximate model such as double integrator or differential drive is used for planning. Improvements are demonstrated on five benchmarks that involve static or dynamic obstacle avoidance as compared to provably safe and approximately safe RL methods. A provable guarantee is provided for recovery regret.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
Presentation is clear with backing proofs and demonstrable results

Problem that is being solved is clearly delineated and addressed using sound techniques

Experimental comparisons are performed rigorously with attention to detail

Weaknesses:
Literature review and comparisons are partial to the RL literature. There is a long-standing literature in control [A, B, C] to use an approximate model to plan using predictive control. A whole host of methods to learn a safety-filter/shielding on the fly has been explored with robust optimization-based offline and online control techniques. Most of these methods would implicitly solve the problem this paper is trying to address. However, it is interesting that the paper uses the Q function in the online optimization. This aspect is novel and unique to this paper.

It is unclear how much computation and time it takes to run MCTS online at each time in order to do dynamic shielding at runtime.

Dynamics model such as double integrator and differential drive are too simple. It would be interesting to see how well these would work with more complicated and/or higher-dimensional dynamics.

[A] Breeden, Joseph, and Dimitra Panagou. ""Predictive control barrier functions for online safety critical control."" 2022 IEEE 61st Conference on Decision and Control (CDC). IEEE, 2022.

[B] Wabersich, Kim P., and Melanie N. Zeilinger. ""Predictive control barrier functions: Enhanced safety mechanisms for learning-based control."" IEEE Transactions on Automatic Control 68.5 (2022): 2638-2651.

[C] Wabersich, Kim P., et al. ""Data-driven safety filters: Hamilton-jacobi reachability, control barrier functions, and predictive methods for uncertain systems."" IEEE Control Systems Magazine 43.5 (2023): 137-177.

Limitations:
The authors have discussed limitations and there is potential for the approach to scale even though the experiments in the paper are on simple examples.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new method for safety shielding. More precisely, the authors extend Model Predictive Shielding (MPS), where an agent reverts to a safe backup policy if, for the next predicted state, this policy would not be able to guarantee safety anymore. MPS is often overly conservative, particularly in cases where the backup policy is very different from the final policy (for example, it may only consider breaking, while the final policy may be able to steer around an object). To improve this, whenever an action is potentially unsafe, the agent first uses a short-horizon planner to see if there exists some safe action that may be better than the one of the backup policy (i.e., one for which the backup policy could still recover in the future, but for which our learned agent predicts a higher reward). The authors formalize this framework and show recovery regret for this framework diminishes exponentially with the horizon. Next, they show that an implementation of this framework outperforms prior methods, both in terms of performance and the number of required shield invocations.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The topic of the paper, safety shielding, is relevant and significant. Safe RL (and particularly, safety shielding) is a promising line of research but is often overly conservative in practice: the methods proposed in this paper take a step toward reducing this problem while still giving formal guarantees about safety. The topic is relevant for the NeurIPS community (particularly those interested in RL), both as a method that could immediately be used or to extend the method to more complex settings (i.e., with a stochastic/unknown model).

The paper is well-written and easy to read: the intuition behind the method is clear, and the analysis of the results is easy to follow. The framework is well formalized (using visualizations where helpful), and the given pseudo-code helps with reproducibility. The experiments are extensive and convincingly show the advantages of the proposed method.

Weaknesses:
Apart from some minor remarks that I add below, this paper has one main weakness: it does not clearly indicate the computational complexity of its method nor the scalability. The results do not show computation times, and (as far as I could tell) no mention is made of either the average planning time or some time limit for this planning phase. From some ball-parking, the additional time required for this method may be significant (solving up to millions of short-horizon planning problems), so a quantification of this computational cost should be provided.

Some more minor remarks:
* The paper only mentions how the framework is implemented (i.e., what RL & planning method it uses) in the appendix: it would be nice to (briefly) mention this in the results section as well;
* In Table 2, the results of CPO and TD3 are not bold, even though some are equal to those of the best frameworks: this should be fixed;
* One limitation of the proposed framework is that it assumes the environment is deterministic: it would be nice to mention this in the limitations section.

Limitations:
Limitations are adequately addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce Dynamic Model Predictive Shielding (DMPS) an extension of Model Predictive Sheilding (MPS) that adress some of its key limitations, such as overconservatism when deploying the backup policy which consequently hinders exploration of the neural 'task' agent and slows down conergence. The key innovation of DMPS is that it incoropoates a local planner for dynamic recovery that leverages both a pre-computed backup policy and the neural 'task' policies Q values for planning for short and long horizon returns while maintaining safety of the system. DMPS is a provably safe reinforcement learning (PSRL) method, meaning that it guarantees safety of the system (regardless of the underlying neural 'task' policy) by only exploring within the set of safe and recoverable states defined by the backup policy.  This realised by planning for a fixed n step trajectory and checking whether the system can be recovered by the backup policy given the action proposed by the agent. The authors demonstrate that DMPS outperforms MPS and other baselines in terms of performance and safety in various benchmarks. It also emphasizes the importance of aligning reinforcement learning agents with real-world safety requirements, while discussing some of the limitations of their approach.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper has several strengths: I find that the paper is very well written and easy to follow, with sufficient details in necessary places and abstractions in other places where the details may not immediately matter, as such, it is a very nice read. The theoretical analysis of the recovery regret is convincing and interesting. Furthermore, the overall framework is attractive from the point of view that it is provably safe, something I personally find is crucial for deploying RL in the real world, rather than safe at convergence or in expectation like a lot safe RL methods. I find that the dynamic planning module is an innovative solution to the intuitive issue faced by most shielding methods (Figure 2) and I feel that this work constitutes a step in the right direction for improving shielding methods and making them more practical. The experimental evaluation I feel is strong and thorough as in most cases DMPS clearly outperforms MPS and REVEL, although I think it is missing something (see weaknesses).

Weaknesses:
The key weakness of the PSRL framework is the reliance on a perfect (or sufficiently accurate) dynamics model of the environment, the safety performance of the backup policy and the computation of the safe invariant set. In contrast to the first shielding approaches for RL [1], which operate primarly on discrete state and actions spaces, DMPS does not need to compute the shield before learning can begining which significantly reduces the engineering overhead before training. This of course comes at a cost, in practice the shields in [1] are substatially more lightweight during ""inference"", (although in theory there could be exponential blow up) in part due to only operating on discrete or discretized state/action spaces but also because a lot of the necessary computation is done before hand. This is a key limitation of DMPS as it relys on planning at each timestep which might be costly and infeasible for on-board computation or edge devices. Fruthermore, it seems that there is still a significant amount of prior knowledge required for DMPS to work effectively, first we have to have a ""perfect"" dynamics model (for provable guarantees) secondly I presume we need to handcraft a backup policy and then compute its invariant safe set so as to plan for recoverability. The first limitation is mentioned in the paper but not really discussed in much detail, the second limitation is find is crucial and I don't think is really mentioned in the paper. In particular it is a non-trivial challenge to come up with a backup policy that has a maximal safe invariant set, perhaps for the environments the authors consider it is easy (just decelerate) but for more dynamics environments and in general this is not the case and I feel like more discussion about both these limitations (i.e. the limitations of the PSRL setting) is needed. 

While I find the experimental evaluation compelling I feel it is slightly misleading and it is missing something. In Table 2 CPO and TD3 score the same or higher in a few of the static benchmarks but there scores are not in bold, is there a reason for this that I am missing? I also feel like a comparison to PPO-Lag or DDPG-Lag would really help make the results that bit more convincing.

All that being said, in principle I advocate for acceptance of this paper.

[1] Alshiekh, Mohammed, et al. ""Safe reinforcement learning via shielding."" Proceedings of the AAAI conference on artificial intelligence. Vol. 32. No. 1. 2018.

Limitations:
See weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The approach called dynamic model-predictive shielding for safe reinforcement learning is proposed as an improvement over its static counterpart. The main idea is to optimize for expected return on action with respect to the reinforcement-learning task when choosing a shielding backup action, and to incorporate planning horizon prediction into learning for the policy to learn to avoid unsafe actions. This dynamic version is evaluated on several static and dynamic obstacle-avoidance benchmarks and compared to static model-predictive shielding and three more planning-based approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The core idea of the approach is interesting and potentially valuable: to achieve synergy between safety and optimal performance in model-predictive shielding via incorporating planning into policy learning and taking expected performance into account during backup planning. Similar attempts have been done previously. In comparison, this work proposes a novel notion of ""recovery regret"" as a heuristic to guide mutual integration of planning and reinforcement learning. 

The strength of the paper is in extensive evaluation and comparison to multiple approaches. The notion of recovery regret can also be of independent interest for model-predictive shielding research. Dynamic shielding outperforms other approaches in the evaluation in terms of the number of shielding invocations, which indicates synergy between planning and learning over time.

Weaknesses:
Potential weaknesses of the approach are in scalability of the planner and tightness of the probabilistic bounds on safety.

Minor:
- ""more optimal recovery approach"" --> an optimal/a better

Limitations:
The authors explicitly discuss limitations which are fairly common for problem domain.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper seeks to address provably safe RL problems where safety must be ensured even during training. It proposed DMPS, which enhances prior Model Predictive Shielding approach, to dynamically select safe actions when danger is imminent. DMPS employs local planner to plan for recovery actions and the planner objective consists of both short-term and long-term rewards. Feedback from the planner can then be used to incrementally train the neural policy to guide it towards safe policy set.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Quality
* Overall, the approach described in the paper is sound and it combines many established components (e.g. backup policy, local planner, estimate future reward using model unrolling and Q-estimate) to facilitate safe RL.  
* The paper provides theoretical bound on the recovery regret as the sampling limit in the local planner approaches inifinity.
2. Clarity
* The paper is written in a clear and lucid manner. The figures, algorithm and equations are structured in a way that is easily understandable to the readers.

Weaknesses:
1. Originality
*  The main difference between DMPS and MPS is the use of local planner when backup policy is triggered. The technical approach used in DMPS is not particularly new as there are already some similar approaches of estimating a safety Q-value and perform planning based on the Q-value [1, 2].
2. Significance
* The only difference between DMPS and the prior MPS seems to be the local planner and (as discussed in point 1) this local planner is not particularly novel. Having said that, I do agree that the proposed DMPS does show improvement over MPS in some experiment scenarios.
* While the paper mentions a small planning horizon is sufficient for the local planner to plan safe yet rewarding actions, I feel that this may not be true in most cases. To steer the agent back to safety (and yet rewarding), a long sequence of actions may be required. If the planning horizon is set too small, then DMPS falls back to backup policy and the performance would be the same as MPS. In this case, I guess the only solution is to increase in planning horizon and in turn increase the computational overhead of DMPS exponentially.
* The local planner requires perfect information of the transition and the transition must be deterministic. This may restrict its applicability, especially given that there're prior work on model-based RL where transition can be stochastic and learned instead.  

References  
[1] Clavera, I., Fu, Y. and Abbeel, P., Model-Augmented Actor-Critic: Backpropagating through Paths. In International Conference on Learning Representations.  
[2] Thomas, G., Luo, Y. and Ma, T., 2021. Safe reinforcement learning by imagining the near future. Advances in Neural Information Processing Systems, 34, pp.13859-13869.  
[3] Nagabandi, A., Kahn, G., Fearing, R.S. and Levine, S., 2018, May. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE international conference on robotics and automation (ICRA) (pp. 7559-7566). IEEE.  
[4] Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems. 2018.  
[5] Janner, M., Fu, J., Zhang, M. and Levine, S., 2019. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32.

Limitations:
The paper discussed the known environment model as its limitation. I agree that this is a limitation which warrants further investigation. As studied in [5], it is very challenging for a model to accurately predict future trajectories with long horizon. Since DMPS relies on having an accurate environment model for safety adherence, this may limit its applicability to practical scenarios where environment model is not given and needs to be learned.  

Another related point is that it is unclear what value to set for the local planner horizon. Different tasks may require corrective action sequence of different lengths. Setting the horizon too short may revert DMPS performance back to MPS and setting the horizon too long may increase the computational overhead beyond acceptable threshold.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
x2780VcMOI;"REVIEW 
Summary:
Whereas prior work (Hewitt and Manning 2018) probed syntactic distance and depth, this work proposed to push that forward by also probing headedness and dependency type.  Specifically, this doesn't separately probe those three, but aims for a single vector space where euclidean distance defines syntactic distance but the difference vector maps to a UD dependency label (optimized with a contrastive learning objective).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
It is a pretty well-written paper, and the framing of the angular probe idea seems well explained and to have some elegance to it (in aiming for a single underlying representation); parts of the implementation seem well-considered to get that single representation.

Weaknesses:
- If viewed merely as a combination of probing structure and labeling, it is very similar to a work like Muller-Eberstein et al. 2022.   The advantage of this paper -  having more of a shared representation -- is appealing, but I wish the consequences of that shared space were better explored.
- Analysis was somewhat lacking: for a probing paper, there were relatively work showing what this tells us about the syntactic behavior of models.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Previous work introduced linear probes to explore how syntactic relationships are encoded in LLM embeddings.  This work aims to take it a step further and examine how types of syntactic relationships are encoded in the LLMs.  They introduce a polar probe that when optimized can predict the type of syntactic relations via the angle between them.  In a multi-faceted evaluation, the model outperforms baselines (which are essentially ablations of the model) in terms of stronger cosine similarity between the same relations, and in terms of tree accuracy (UAS/LAS).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- An interesting paper with a clear contribution, building on existing probing work while asking a couple new research questions

- The results appear convincing

- The potential to explore syntax through the lens of LLMs, especially when LLMs can be easily trained on unlabeled text, or especially when LLMs are increasingly multilingual, points to some exciting future directions.

- The evaluation also includes some linguistically interesting example cases.  Essentially exactly what I would have asked for (in addition to the larger corpora studies)

Weaknesses:
- I find the distinction between probing and parsing to be not entirely clear.  At the point where the evaluation is in terms of UAS/LAS, could this not be compared directly to parsers on the same data (especially since building on top of LLM embeddings would be the most performant solution)?  And where would the discrepancies be, and what would that mean?  Do LLMs not encode those relationships?

- In general the paper seems to suffer from a lack of convincing baselines.  The baselines presented -- the structural or angular probe, are steps along the path to the polar probe.

- Cosine similarity between identical syntactic categories is surprisingly low (to me).  The ranking of categories in terms of the strength of that correlation is also surprising, ith things like 'case' being quite strong.  In general there are many ""odd"" patterns that I don't have an intuitive explanation for why they occur, and aren't discussed in detail in this work.

- There is no dedicated related work.  I do think the parsing literature, and especially the parsing-on-top-of-LLMs literature is relevant.

Limitations:
Yes, the limitations are described.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes polar probes, a kind of structural probe that learns a distance and rotation function that can more accurately classify syntactic structure from language model representations than previous approaches. In particular, the question of whether direction can represent the type of syntactic relationship is answered. The authors find that their polar probe outperforms previous methods quite significantly and show

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is very well presented and a pleasure to read. The empirical findings are strong and clearly support the hypothesis that the direction, as well as the angle of the representations of an LM projected on a plane represent the syntactic relationships encoded by the model. The authors show that this interpretation is able to much more strongly reconstruct ground truth syntactic parses from hidden state representations than structured probes. The controlled dataset provides a clean comparison in a challenging setting and is a useful resource for future work. A major finding of this work is that it vastly raises the upper bar for how well we should consider syntax to be encoded by language models.

Weaknesses:
Weaknesses like the focus on dependency parses and drawbacks of current tokenizers are addressed in limitations, but are still weaknesses nonetheless.

Please include  UUAS, LAS and Balanced Accuracy for the evaluation on the controlled dataset separately for comparison.

As thorough as this paper is, I think it could go deeper on the model analysis. It's nice that the layer-wise analysis is consistent with previous work, but this would be mostly expected. For example, could the authors show that models of different sizes capture more/less syntactic complexity? Is there a critical point where syntax becomes well represented and gains are diminishing after more scaling? Do larger models capture more of the ""tail"" of rare syntactic constructions? This could be carried out on the GPT2 or Pythia family of models.

Nits:

- please make the text in the legend/axis labels for figure 3 bigger

- Typo L36: ""proposed settle""

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wzof7Y66xs;"REVIEW 
Summary:
The paper introduces a hierarchical selective classification technique that incorporates hierarchical risk and coverage. The authors additionally proposed an algorithm that guarantees target accuracy. Experimental results demonstrate the method's effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Hierarchical selective classification is a new area and therefore the current method is one of the first techniques to deal with such problem. Its application to critical settings can be substantial.

Weaknesses:
•	The need of a prior tree among classes can limit its usage for complex scenarios. The construction of such tree can be a non-trivial step for the applicability of the approach. 

•	The main contribution looks an extension of previous methods for the hierarchical case.

Limitations:
•	The requirement for a hierarchical class tree prior to using the method is a limitation.

•	The current experimental section is limited in terms of baseline approaches and datasets, making it challenging to gain a comprehensive understanding from the existing experiments.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose hierarchical selective classification, a method that selects the hierarchical granularity of its prediction based on uncertainty.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is well-written, and the proposed method is quite intuitive.
* I like the idea that if uncertain, it makes sense to predict at a higher level of granularity.
* The theoretical results & statements are sound.
* Extensive experimental results showing the superiority of the proposed method to DARTS and a non-hierarchical baseline.
* Applicability to pre-trained models

Weaknesses:
* My biggest uncertainty is the similarity of this work to conformal prediction. To me, it seems that this method is very similar to conformal prediction, where the set of possible prediction sets is restricted via this pre-defined hierarchy. While, as far as I know, it has not been explored, it decreases the perceived novelty. 
* A weakness of the setting rather than the method is that it assumes the knowledge of the underlying hierarchy. As such, the applicability is somewhat limited. The paper would benefit from a way to unsupervisedly learn this hierarchy, e.g. based on classes whose predicted probabilities are positively correlated.
* As also touched upon in the concluding remarks, the method is post-hoc rather than being optimized during training, thus, likely not performing up to the highest possible level.
* Minor: Line 158-159 is a worded badly, similar to ""... thus, we do A. Unlike others that do A, we do A+B"".

Limitations:
Limitations are adequately discussed apart from the necessity of assuming knowledge of the underlying class hierarchy.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework for selective classification called hierarchical selective classification. In a setting where a hierarchy in the classification task is present, the authors devise a selection strategy that considers confidence at different levels of the classification hierarchy. Extensive experimental analysis is performed over 1115 ImageNet classifiers.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The main strengths of the paper are:

1. the idea of applying selective classification in a hierarchical setting is novel;
2. the theoretical analysis relies on conformal prediction, which guarantees the soundness of the results;
3. the proposed framework can impact high-risk settings, as shown in the healthcare example.

Weaknesses:
Overall, I think the paper is solid. My main concern is that the empirical evaluation could be improved, especially regarding motivations and attention to detail. 
A few examples:
* I do not fully understand why the authors focus so much on showing how different training regimes affect HSC performance. I guess this improves the overall predictive performance of the (hierarchical) classifier, which is expected to impact the HSC task positively.
* As the authors correctly claim, the training regimes were not optimized for hierarchical selective classification. Despite the clear computation-wise motivation, I argue that including regimes optimized for HSC would make the empirical evaluation sounder.
* a few lines are off: for instance, I would argue that line 279, i.e.,
>CLIP achieves an exceptional improvement, surpassing 40%
>
 does not match what is shown in Figure 4 (which shows an improvement below 40%).

Limitations:
The paper briefly discusses limitations. I think this section could be expanded, e.g. considering Q1.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes an extension of selective classification following a class hierarchy to reduce the specificity of model prediction when there is a high uncertainty. In particular, if the prediction confidence of a class is smaller than a predefined threshold, the proposed algorithm would proceed towards a higher class level in the hierarchical structure (the parent node), until the confidence of the considering node exceeds that threshold. The paper also formulises hierarchical risk and coverage, so that the area under curve can be used as a metric to benchmark different selective classification methods. An extensive number of pretrained classifiers on ImageNet dataset are then used to evaluate the proposed method and show promising results. The paper also include a PAC-like theoretical result, so that when finding the optimal threshold, one can select appropriate hyper-parameters to achieve their desired outcome with certain confidence level.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper goes into details to provide an adequate background about selective classification, the definition of heirarchical risk and coverage as well as its area under curve as a metric to quantify the performance of hierarchical-based selective classification. It also links to previous studies in the same subfield. In general, the paper is well written and easy to follow.

The paper also includes a theoretical result on the guarantee of the learning algorithm when one wants to find the optimal thresholding value for their hierarchical selective classification. This simple theoretical results does strengthen the paper.

The paper also include an extensive number of experiments and ablation studies to provide insights into the newly-proposed method.

Weaknesses:
The paper relies on the setting with the following assumptions:
- It is an inference rule. This means that the algorithm is used at test time only. If this could be even integrated into training is a plus.
- It needs a validation set to find the optimal hyper-parameter $\theta$, or the threshold (partly mentioned in the conclusion). It is understandable because there is no training involve here, so there is a need for that. However, in some cases, there may not be additional data available.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wz2KvvEk44;"REVIEW 
Summary:
Visual-based Reinforcement Learning (RL) often fails to generalize across unseen environments. This work proposes SMG (Separated Models for Generalization) to improve the generalization in VRL by introducing two models to separately extract task-relevant and task-irrelevant representations through image reconstruction. Specifically, SMG proposes two additional consistency losses on relevant features, improving generalization. Extensive experiments, including video-hard DMC, color-hard DMC, and manipulation tasks, show SMG excels in diverse settings and tasks, demonstrating robust performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Separating foreground and background for reconstruction makes sense for improving the generalization in VRL.

- Extensive experiments in various experimental settings demonstrate the effectiveness of SMG.

- The learned mask looks very effective (Fig. 3 and Fig. 7).

Weaknesses:
- Distinguishing between controllable and uncontrollable parts for learning a mask model has been widely discussed in the community, like TIA [1], Denoised MDP [2], ISO-Dream [3] and so on. Although I appreciate authors' efforts to discuss its difference against TIA (appendix E.2), I think the novelty of learning mask models to distinguish noise from the environment is limited. Nevertheless, I believe that this paper has made contributions in applying mask models to the field of visual RL generalization.

- I'm curious about the performance of the proposed method in some more challenging settings, like RL-Vigen [4].

- As there are many losses, it is better to add a detailed pseudo code about how to calculate all these losses, which can make the paper more readable.

- This proposed SGM is considered to be seamlessly combined with any existing off-policy RL algorithms. As the experiments mainly consider SAC as the RL backbone, I'm curious about its performance with other methods, like DrQ or SVEA.

- The related work part only discusses observation generalization in RL and some other types of generalization also should be discussed, like dynamic generalization [5,6] and task generalization [7,8].

Overall, I lean toward boardline of this work. I will participate in subsequent discussions and would like to adjust my scores, especially for the response to my concerns about experiments.

[1] Learning Task Informed Abstractions

[2] Denoised MDPs: Learning World Models Better Than the World Itself

[3] Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models

[4] RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization

[5] Context-aware dynamics model for generalization in model-based reinforcement learning

[6] Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability

[7] Zero-shot task generalization with multi-task deep reinforcement learning

[8] Task Aware Dreamer for Task Generalization in Reinforcement Learning

Limitations:
Yes, this work has discussed its limitations in Sec. 6.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel method that utilizes two model branches to extract task-relevant and task-irrelevant representations separately from visual observations, aiming to enhance the zero-shot generalization ability of RL agents. The approach introduces four additional loss terms and two consistency losses to guide the agent's focus towards task-relevant areas across different scenarios. The proposed method can be seamlessly integrated into existing standard off-policy RL algorithms as a plug-and-play module. Experimental results demonstrate the effectiveness of the proposed model on two environments, surpassing previous benchmarks such as SAC and DrQ.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is clearly written and easy to follow.
2. Based on the separated models architecture, this paper proposes multiple effective loss functions to focus on task-relevant features in visual-based RL generalization.
3. The authors provide detailed validations on the DMC environment and robotic manipulation tasks. They demonstrate the advantages of the proposed loss terms across multiple tasks in DMC (Table 3) and showcase the state-of-the-art performance of SMG (Table 1, 2).

Weaknesses:
1. While the paper compares the performance with model-free RL methods, it would be beneficial to also include a comparison with model-based RL methods. Previous works such as DreamerPro [1], Iso-Dream [2], and Denoised-MDP [3] have addressed visual distractions to enhance the generalization ability of RL agents.

[1] Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations.

[2] Iso-Dream: Isolating Noncontrollable Visual Dynamics in World Models.

[3] Denoised mdps: Learning world models better than the world itself.

2. The paper lacks sufficient discussion and analysis of its limitations. 
3. The serial numbers in some figures appear to be somewhat disorganized.

Limitations:
This paper discusses the limitations, but that is not enough.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel approach called SMG (Separated Models for Generalization) to improve generalization in visual-based reinforcement learning (RL). The approach works by using separate foreground and background encoders/decoders and employing a mask to isolate task-relevant regions. In addition, it also applies four additional losses(mask ratio, background, Q-value and empowerment losses) to to enhance the model’s ability to distinguish between two types of representations. To make the learned models generalize to different visual styles, it introduces attribution augmentation and consistency losses. The authors position this as a plug-and-play method that can enhance existing RL algorithms' generalization capabilities.

Experiments show SMG outperforms baseline methods, particularly in video-background settings where it maintains performance even with significant visual changes. Ablation studies validate the importance of each component.

The main contributions are:

- SMG: A separated model architecture with two branches to extract task-relevant and task-irrelevant representations from visual observations.

- Two consistency losses to guide the agent's focus on task-relevant areas across different scenarios.

- Strong performance on DMControl benchmark tasks, especially in video-background settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper has several strengths:

- SMG achieves state-of-the-art performance on the DMControl Generalization Benchmark, particularly excelling in the challenging video-background settings. This demonstrates the practical effectiveness of the approach.

- SMG is a plug-and-play method that can enhance existing RL algorithms' generalization capabilities. It is designed to be easily integrated with existing off-policy RL algorithms, enhancing its practical value and potential for wide adoption.

- This paper includes detailed ablation studies that validate the importance of each component in the SMG architecture, providing insights into the method's workings.

- This paper is well-written. And it also provides clear visualizations of the reconstruction process, helping readers understand how SMG extracts and utilizes task-relevant features.

Weaknesses:
This paper has several weaknesses:

- My major concern is the overclaim made by this paper. While it claims to address the generalization gap in visual-based reinforcement learning, the method proposed primarily tackles scenarios where only the backgrounds differ. However, visual generalization challenges are more diverse and include variations such as different lighting conditions and textures, which are common in real-world robotics applications. These scenarios appear to be overlooked in this paper.

- SMG introduces a lot of loss terms and associated hyperparameters, which could complicate tuning in practical applications.
    - Specifically, the mask ratio $\rho$ appears to be crucial for performance, as it is the sole factor preventing the model from classifying everything as foreground. Given that $\rho$ represents the ratio between the foreground and the entire image, it likely necessitates per-task tuning, which could prove to be challenging and not scalable.

- The foreground consistency loss, as discussed in Section 3.3, heavily depends on the predicted mask to construct the augmented observation. During the initial stages of training, this process relies on potentially inaccurate mask predictions and attributions. Although the authors describe this as a bootstrapping process, further analysis regarding its stability and potential failure modes would be beneficial.

- The paper could be strengthened by considering a broader range of baselines. For example:
    - Recent studies [1] suggest that visual encoders pre-trained on large-scale image datasets can improve the visual robustness of a policy. This paper does not make any comparisons with visual pre-training methods.
    - Large vision foundation models like SAM [2] could potentially be utilized to provide supervision for generating foreground masks. Would this approach be more effective than training a mask predictor from scratch?


- The additional computation overhead introduced by the extra modules is concerning.
    - The architecture, which involves separate models, essentially doubles the number of parameters compared to baseline methods. Although the authors argue that the performance improvements are due to the novel architecture rather than the increased number of parameters, this could still be problematic for practical applications with limited computational resources.
    - Training time: The reported wall time for SMG is significantly longer than that of the baseline methods (22 hours versus 8-13 hours for 500,000 steps).

[1] Hansen, Nicklas, et al. ""On pre-training for visuo-motor control: Revisiting a learning-from-scratch baseline."" arXiv preprint arXiv:2212.05749 (2022).

[2] Kirillov, Alexander, et al. ""Segment anything."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

Limitations:
There is no separate section for limitations in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a novel objective to improve robustness of the visual encoder in RL to background noise and to color perturbations. First, the authors split the visual encoder into two models: background encoder/decoder and foreground encoder/decoder. The proposed training objective contains multiple components: 
- overall reconstruction loss that combines outputs of the background and the foreground decoders modulated by a mask;
- mask ratio loss that prevents the foreground mask from taking up too much of the image;
- background reconstruction loss that uses the learned mask to generate a new data sample;
- q-value loss that makes the foreground representation capture value information;
- empowerment loss that makes the foreground representations capture the agent actions;
- foreground and q-value consistency losses that make sure that changing the background (using the learned mask) doesn't change the foreground features and q-values

The method is tested on DMC generalization benchmark and on robotic manipulation tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method performs really well with various distractors;
- The idea of re-using the learned masks for augmentations is interesting and, as far as I can tell, novel;

Weaknesses:
- The writing is a bit sloppy, with many typos and confusing sentences;
- The resulting objective is too complex and has too many terms;
- No comparison to TIA, although the presented method is quite similar. Was that because you only compare to model-free methods?

Typos (some of them, I didn't write down all of them, please run a spell checker on the text):
line 13: achieving free from overfitting : not clear what this means
line 38: further strengths -> further strengthens
line 100: focused in -> focused on
line 536: Comparision -> comparison

Limitations:
The authors have adequately described limitations and potential negative societal impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wyYsCI3K7U;"REVIEW 
Summary:
The paper presents LoRANN, a novel algorithm for Approximate Nearest Neighbor (ANN) search that leverages low-rank matrix factorization and k-means clustering. The core idea is to approximate the ordinary least squares solution of the inner product computation via reduced-rank regression. The authors also introduce a quantized 8-bit version of LoRANN, which is memory efficient and performs well on high-dimensional data. The experiments demonstrate that LoRANN outperforms existing methods on both CPU and GPU.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors provide extensive experimental results, reporting that their method outperforms leading product quantization-based algorithms and has faster query times than graph-based methods at certain recall levels.

Weaknesses:
There exists room for improvement in the visual presentation in this paper. Additionally, it is best to keep the starting or ending points consistent to better compare all methods.

 At different recall levels, LoRANN is sometimes faster, and sometimes slower compared to other methods (GLASS, CAGRA). The authors should analyze the reasons that lead to this phenomenon

Limitations:
The limitations of this work have been discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates approximate nearest neighbor (ANN) search, where, given a collection $\mathcal{X}$ of points in $\mathbb{R}^d$, the task is to find the top $k$ data points that are closest to a query point $q$ according to some similarity or dissimilarity measure (denoted by $\delta(\cdot, \cdot)$), such as inner product. There are many classes of algorithms in existence[1], with this particular work falling into the clustering-based paradigm.

In clustering-based (aka Inverted File or IVF) ANN search, $\mathcal{X}$ is partitioned into a set number of clusters, often using a geometric clustering algorithm such as (spherical) KMeans, with every cluster represented using some sketch of the cluster such as its mean. When presented with $q$, the algorithm first identifies $\texttt{nprobe}$ clusters to search by ranking the clusters according to the distance between $q$ and their representative points. It then computes $\delta(q, \cdot)$ with points within the $\texttt{nprobe}$ clusters, and returns the top $k$ points from that set.

This work concerns the second step. Typically the computation of $\delta(q, \cdot)$ uses Product Quantization (PQ) to reduce memory consumption and to perform the distance computation efficiently. Instead, this work reduces the dimensionality of the data matrix within each cluster using its low-rank approximation. The key insight is that, the low-rank approximation is constrained to the space of rank $r$ matrices that predict the inner products well on a specific query distribution.


[1] ""Foundations of Vector Retrieval"" by S. Bruch. Springer.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* The proposed method relies on a very simple yet effective method for supervised dimensionality reduction in the context of ANN search
* The paper is easy to read and arguments straightforward to follow
* Results are encouraging

Weaknesses:
Post-discussion Update: The authors have addressed my concerns around the experimental setup, and have expressed interest in adopting a more clear narrative and framing of their contributions.

-----------------

* Presentation:
  - I think the authors can shed quite a bit of fluff by positioning the work as I did in my summary. This work's contribution is very much in the speeding up and improving the accuracy of the score computation phase in clustering-based/IVF ANN search. Presented that way, the authors can immediately focus on the regression problem instead, and not introduce distractions such as the details of clustering, the importance of MIPS (section 2.1), and more. It'd make for a cleaner presentation of your idea, and lets your readers understand the scope of your contributions more clearly.
  - As a minor point, Theorem 1 is a vacuous statement. It's neither necessary to explain the findings of the paper, nor is it insightful enough to birth new research directions. Perhaps you can move it entirely to the appendix if you insist on including it in the work.
  - It must be noted that the method presented in this work is supervised. That is a critical differentiating factor between LoRANN and existing methods such as PQ and Scann.

* Methodology: One of the interesting insights that led to Scann is that not all inner products are equally important. For a quantization method to be successful, it needs to preserve the inner product between $q$ and high-ranking data points better than inner product between $q$ and low-ranking points. In your work, you model the problem as regression, and attempt to minimize the error of inner production approximation equally for all data points. What motivates this uniform weighting? Have you considered a ranking formulation of the problem rather than regression? There is a vast literature on learning-to-rank which, in fact, is very relevant to your idea, but where Scann's insight is baked into its machinery/objective.

* Experiments: Because the methodology is very straightforward and the novelty is minimal, I expect a much stronger experimental evaluation of the proposed method. Here are a few points to consider:
  - Your main experiments conflate two orthogonal axes of evaluation: effect of clustering vs effect of score computation; this I believe stems from the way you present your work. Your contribution, as I noted above, is to the score computation phase of IVF-based ANN search. To evaluate your contributions fairly against SOTA IVF methods, you must partition the data once. Given this fixed set of partitions, you can directly compare the efficacy of LoRANN against PQ and Scann's quantization protocol. By running each method independently as you do now, such that each produces its own partitioning of the data separately, you run the risk of conflating the effect of clustering on IVF's accuracy with the effect of the specific choice of dimensionality reduction/quantization. As it stands, I cannot deduce the exact reason why your method should work better.
  - You are also comparing a supervised method that adapts to a query distribution, with unsupervised baselines. Not only is it not a fair comparison, your results are also not informative. It is not surprising that your method does well: you give it an unfair advantage (as confirmed by Figure 1 - left) by finding a matrix that can predict inner products on *a specific query distribution*. A more reasonable experiment would be to (a) compare a variant of LoRANN that's trained on the data points only (i.e., without training queries) with other IVF methods, and (b) incorporating the query distribution into Scann (its objective can use information about the query distribution). There are other methods that can use a query set to improve quantization ([2,3] are a couple of examples).
  - As a very simple baseline, consider partitioning the data using centroids obtained from a partitioning of queries!
  - Frankly, a comparison with graph methods is nice, but is rather tangential. I encourage you to contrast your method with other IVF methods first, focus your discussion to justifying your proposal against SOTA IVF methods, and then conclude your work with a comparison with graph methods for completeness.


[2] ""Query-Aware Quantization for Maximum Inner Product Search"" by Zhang et al. AAAI 2023.
   
[3] ""A Learning-to-Rank Formulation of Clustering-Based Approximate Nearest Neighbor Search"" by Vecchiato et al. SIGIR 2024.

Limitations:
I really like this work and the simplicity of the idea. I'd love to see this work in print, but I think it can be so much more complete with a proper set of experiments. As it stands, the incomplete experiments limit the reach of this work. A stronger formulation (using ranking, e.g.) can even enhance the results too. I hope my feedback proves helpful in strengthening the arguments of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces a new method for the nearest neighbor search problem. Leveraging the low-rank assumption, the authors combine low-rank matrix factorization, clustering, and quantization to enhance the speed of nearest neighbor search. The authors conducted extensive experiments to demonstrate the advantages of their method over numerous baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The authors conducted extensive experiments to compare their methods with other baselines.
2. The method proposed by the authors is easy to follow and implement.

Weaknesses:
1. It seems that all the techniques mentioned in this paper have already known to be useful for nearest neighbor search.
2. As shown in Figure 2, all the components contribute to the final results. I don't see any reason why any component applied there is unique to the new algorithm. For example, the clustering and 8-bit quantization techniques appear to be applicable to any existing nearest neighbor search algorithm or library. Thus, I question whether it is fair to employ too many techniques when comparing with other standard nearest neighbor search libraries.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper describes a method for computing approximate nearest neighbors in
high dimensions. Computing nearest neighbors is a classical problem in
computational geometry, with applications in many areas of computer science.
The classical solutions in low dimensions do not generalize to high dimensions.
The approach in the paper has two main ideas: the first is performing k-means
clustering, computing nearest neighbors on the means, and then computing 
more accurate nearest neighbors inside the cluster.
The second is reducing the computation in each cluster to multivariate
regression which can be solved approximately by low rank matrix factorization.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The result appears to be very useful in many applications.

Weaknesses:
Unfortunately I am not an expert in this field and cannot comment on how this
result compares to the current state of the art.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";1
ww62xltEfB;"REVIEW 
Summary:
This paper investigates and proposes a novel bi-Lipschitz neural network architecture. This architecture provides a simple, direct and tight control of the Lipschitz and inverse Lipschitz constants through the use of two parameters, the ideal minimum, equipped with theoretical guarantees. To devise their architecture the authors exploit convex neural networks and the Legendre-Fenchel duality. The authors also propose a variant of their bi-Lipschitz architecture that is more scalable by exploiting partially input convex neural networks. Finally, the authors propose a set of experiments to showcase the utility of our model in concrete machine learning applications, namely, uncertainty estimation and monotone problem settings and show that it can improve previous methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written. After writing a clearly structured related work (with an extensive background and related work proposed in Appendix A), the authors propose their new design and explicitly explain how the forward pass of their network is computed as well as the expressivity and how the backpropagation can be done. 
- The authors acknowledge that the computational cost of their approach can pose serious limitation and propose to overcome this problem with partially input convex neural networks.

Weaknesses:
- It would be interesting of the authors could provide experiments with both their architectures with respect to computational cost, and highlight time of training etc.

Limitations:
Lack of experiments wrt to computational cost and maybe an experiment with a more larger dataset than the current ones use in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel neural network architecture called BLNN (Bi-Lipschitz Neural Network) that allows direct control and parameterization of the overall bi-Lipschitzness of the network. The main contributions include: i) a framework that allows tight control of Lipschitz and inverse Lipschitz constants of networks via using convex neural networks and the Legendre-Fenchel transformation, ii) comprehensive theoretical analysis, iii) empirical evaluation showing the nice performance of BLNN on tasks like function fitting, out-of-distribution detection, and monotone regression.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality:**

The paper presents a novel approach to constructing bi-Lipschitz neural networks that is distinctly different from existing methods. The use of convex neural networks and Legendre-Fenchel transformation to directly parameterize overall bi-Lipschitzness is quite novel. The extension (e.g. partially bi-Lipschitz networks, etc) is also new.


**Quality:**

The quality of the paper is good. The authors provide detailed proofs and analyses for their key claims, including the bi-Lipschitz properties of their construction and the expressive power of the resulting networks. The experiments cover various scenarios, from simple function fitting to uncertainty estimation and monotone regression. The results are quite competitive.


**Clarity:**

The paper is generally well-structured and clearly written. However, given the technical nature and the length of the paper, understanding the paper fully is still a tough task.

**Significance:**

The paper's contributions are significant in its solid theoretical developments. The significance is further underscored by the improved performance on tasks like out-of-distribution detection and monotone function learning. In conclusion, this paper presents a novel approach to an important problem in deep learning theory and practice.

Weaknesses:
1. Computational Complexity: A detailed analysis of time and space complexity compared to traditional networks can be helpful.

2. Scalability and Practical Implications: There's insufficient exploration of how the method scales to very large networks or complex datasets (e.g. TinyImageNet).

3. Hyperparameter Sensitivity: More discussions on this issue will be beneficial.

4. The paper could be more explicit about scenarios where the theoretical guarantees might not hold, and could explore potential extensions to other network architectures beyond feedforward networks.

Limitations:
It seems that an improved discussion on potential negative societal impacts or broader ethical considerations is still missing.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to control the bi-Lipschitzness of a neural-network by parameterizing the output by the Legendre-Fenchel-Dual. This involves parameterizing a strongly convex function and computing the minimum of that function in the forward pass. Several benchmarks are studied in simple regression tasks and uncertainty quantification.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
-The framework is interesting because it parameterizes bi-Lipschitz networks in a way that is not layer-wise and instead takes advantage of the Legendre-Fenchel transform LFT / convex conjugate of parameterized strongly-convex functions (ICNN), which only modifies the output of the output.

-Computing the LFT of a given function can be costly, however the paper offers a non-asymptotic bound for the Lipschitz constant and tractable gradient.

-The experimental results show a considerable improvement in tightness and regularity over other Lipschitz controlled networks like spectral normalization, AOL and Sandwich layers on small regression tasks. In particular BiLipNet behaves a lot better when the Lipschitz constant is overestimated in existing parameterizations.

Weaknesses:
-Computing the LFT seems to be quite expensive, hence why the experiments are only on simple 2d problems and fashion-MNIST. For this reason I'm doubtful that it will be used for any large-scale network training pipelines where tight Lipschitz control and estimation is challenging.

-The provable approximation class is limited to alpha-strongly monotone functions and is the derivative of some function almost everywhere. Lipschitz layers like AOL, SLL and Sandwich layer are all solutions to the LipSDP framework which only requires the activations themselves to be alpha-strongly monotone for alpha >= 0 (Fazlyab et al., 2019).

Limitations:
Limitation are adequately addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wvQHQgnpGN;"REVIEW 
Summary:
This paper studies the two-player zero-sum stochastic Markov games (2p0s-MGs) with large scale or continuous state spaces. These problems have a large cardinality and function approximation methods are needed. The paper consider a spectral dynamic embedding method and proposed SDEPO. This methods utilized the transition dynamics in the construction of the state-space value function. SDEPO is able to converge with order $1/\epsilon$, which matches the optimal rate in single agent RL.
Theorems are provided for the last iterate convergence of the SDEPO algorithm. The effectiveness of the algorithm has been verified in games against baseline methods.

Generally the paper is well structured, but section 3-4 should be better explained while section 5 and 6 focused on the ""practical algorithms"" is stretched quite far from the analytical results in the previous sections.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The function approximation approach for markov games is a necessity for those problems with large/infinite state cardinality. It is indeed true that the dynamics of the problem was not utilized in previous methods. This algorithm seems to be the first work addressing this.

This work adapted the spectral dynamic embedding for stochastic nonlinear control problems and proposed SDEPO, the motivation is clear and well stated.

Weaknesses:
The only evaluation for the proposed algorithm is a rate of success in playing games with baseline algorithms. To the reviewer, this seems to be very limited. For a submission with strong theoretical focus, current result fails to validate the convergence properties of the proposed algorithm.

The sections for main results are not very well-written and is a bit difficult to read, more explanation would be appreciated. Although this could be due to the page limit.

Assumption 5 is in fact quite strong, a brief discussion on the impact and reasoning should be provided.

Section 5 and 6 seems a bit rushed and is intended to bring out the neural networks, the prior sections discussed the setting with tabular actions, where in these sections the action space is seen as continuous and more algorithms have been added, with no analytical results. I suggest the authors focusing on the existing setting with better presentation, explanation and more experiments.

Another problem this paper did not address is what are the current existing algorithms involving dynamics and function approximation in the single agent setting. The single agent RL with function approximation literature should be somewhat addressed in general.

Limitations:
The authors did not fully address the limitations of this paper. The authors mentioned that the algorithm is non-independent and relies on some assumptions. There are some other limitations of this work, which has been raised in the previous sections.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new algorithm named Spectral Dynamic Embedding Policy Optimization (SDEPO) to solve the zero-sum Markov games with continous state and finite actions. The convergence analysis indicates that the proposed method achieves the best-known sample complexity as the case of finite-state space; this paper is the first theoretical result in handling the continuous state space with known dynamic and infinite horizon.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is the first result for solving the NE of infinite-horizon two-player zero-sum Markov games  with continuous state space when the dynamic is known. Moreover, this paper resents sufficient introduction on the technical backgrounds and preliminaries. All assumptions are clearly listed. Lastly, the theoretical results are verified using emprical experiments.

Weaknesses:
1. The Assumption 1 is not reasonable. It says, whatever the state $s$ and the action $(a,b)$ are, the agent can move to any state $s'$ with a positive probability. Please correct me if I am wrong.

2. I am confused about what is new in the Spectral Dynamic Embedding method. It seems that both Bochner and Mercer theorem are well-known. This paper simply applies them to represent the transition probability and the reward using some kernels. Then everything is the same as traditional method in RL. 

3. A mild comment on Assumption 3: Since the optimal policy might be deterministic, it means that $\pi(a|s)$ is likely to be zero for some $a$. During the training, the policy $\pi_k$ will tend to the optimal policy; the mass at non-optimal action will also approach to $0$. It means if  $\underline{c}$ is larger than $\epsilon$, then $\pi_k$ will never converge to the optimal action in the sense of $L_\infty$ norm. From my understanding, the author needs to set $\underline{c}$ to be $\epsilon$ and it won't affect the complexity.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce an innovative approach to solving 2p0s-MGs with continuous state spaces, providing both theoretical guarantees and practical improvements over existing methods. The SDEPO algorithm and its variants offer efficient and scalable solutions for complex Markov games, potentially applicable to various domains in reinforcement learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes a new Spectral Dynamic Embedding Policy Optimization algorithm that effectively addresses two-player zero-sum Markov games with continuous state space and finite action space.
2. To handle the finite action spaces, a practical variant of SDEPO is proposed to manage continuous action spaces, with empirical results showcasing its superior performance.
3. The complexity result of SDEPO matches the best-known results for policy gradient algorithms in the single-agent setting, proving its efficiency.

Weaknesses:
1. The spectral embedding methods can be computationally intensive in practice due to the complexity of handling spectral dynamic embeddings.
2. Why were these specific feature generation methods chosen? Is the proposed method sensitive to feature generation methods?
3. The experiments are somewhat limited, expanding the empirical section to include more complex and diverse scenarios would significantly strengthen the paper.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wlcm21C4nk;"REVIEW 
Summary:
Recent research indicates that rate-coding is crucial for information representation in deep Spiking Neural Networks (SNNs) trained via Backpropagation Through Time (BPTT). Building on this insight, a new training strategy called rate-based backpropagation has been developed to leverage rate-based representations, reducing the complexity of BPTT. This approach focuses on averaged dynamics to simplify the computational graph, thereby lowering memory and computational requirements. Theoretical and empirical analyses demonstrate that this method closely approximates BPTT's gradient optimization, maintaining comparable performance while surpassing other efficient training techniques. This advancement is poised to enable more scalable and resource-efficient SNN training, particularly in environments with limited resources.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is very well written and documented.
2.	The contributions have been discussed comprehensively.
3.	The experiments have been conducted on multiple benchmarks.

Weaknesses:
Some important details (such as the top-level algorithm of the proposed rate-based backpropagation method and details of the experimental setup) are reported in the appendix, while, due to their importance, they should be moved to the main manuscript.

Limitations:
The limitations and societal impacts have been discussed in Appendix D.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel rate-based backpropagation method for spiking neural network (SNNS) training, which effectively separates the time-dependent backpropagation (BPTT) process and thus reduces computational and memory costs. The method employs a rate-encoded approximation to capture the basic information and is validated by empirical experiments on various datasets, demonstrating that it is superior in terms of training efficiency and accuracy when compared to the traditional BPTT.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Empirical results on multiple datasets (CIFAR-10, CIFAR-100, ImageNet, CIFAR10-DVS) support the theoretical claims and ensure accuracy while reducing memory and time costs.
2. The paper is well-written, clearly explaining the proposed method, theoretical underpinnings, and experimental validation.

Weaknesses:
1.	In lines 53-55, this paper mentions that the proposed method reduces training time, but there is no relevant experimental proof in the experiments section.

Limitations:
The authors fully explain the limitations and potential social implications of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work falls into the category of efficient SNN training methods. This paper proposes a reduced computational graph to reduce the memory and computational demands of SNNs training. This work has the potential to train SNNs on resource-limited devices. The paper evaluates the methods on CIFAR-10, CIFAR-100, ImageNet, and other datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper addresses the issue of high time and memory costs in training spiking neural networks. 

This paper provides solid theoretical insights into the error bound and its relation to SNN BPTT training. 

The results of this work are comparable to the performance of the BPTT counterpart.

Weaknesses:
Not a clear comparison of the differences with existing e-prop methods in terms of methodology. 

No generalization results on hyperparameters (e.g., $\lambda$) are presented in this work. I raise this question because most work on SNNs uses large values of $\lambda$, but this work used 0.2 as $\lambda$.

Limitations:
See weakness and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a rate-based SNN training method, which can effectively reduce memory and time cost during training. They proved the efficiency of the rate-based back-propagation training and demonstrate that the rate-based training outperforms other back-propagation methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The rate-based method achieves better performance and uses less computing resource compared with BPTT, which is impressive.

This paper is well-written and well-organized.

Weaknesses:
The novelty is weak. There are two previous works that share similar idea with this paper, since they all use rate-based backpropagation [1,2]. The author needs to briefly explain the differences between these papers.

The rate-based backpropagation is not suitable for sequential tasks.

[1] Li, Yuhang, et al. ""Differentiable spike: Rethinking gradient-descent for training spiking neural networks."" Advances in Neural Information Processing Systems (2021).
[2] Bu, Tong, et al. ""Rate gradient approximation attack threats deep spiking neural networks."" Computer Vision and Pattern Recognition (2023).

Limitations:
See in weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
wsqDJHPUHN;"REVIEW 
Summary:
The authors theoretically analyze the properties of the learnware paradigm. In the learnware paradigm, a model developer can provide their trained models for other developers to use. To enable re-use, along with the model the developer provides a model specification that adequately represents the model's training data. This allows developers looking for models to find those that are most useful for their tasks of interest. Importantly, this specification should preserve the privacy of the original training data of the model.

While, the reduced kernel mean embedding specification has been proposed in the literature, a theoretical analysis that guarantees the protection of the model's training data is missing. The authors prove that RKME can simultaneously have the following three desirable properties:
* It does not contain any of the original training data points
* It is robust against inference attacks
* It still preserves sufficeint information about the training data for effective use as a learnware specification.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
To the best of my knowledge, the results provided by the authors are novel. While I am not an expert neither in learnware systems nor in reproducing kernels, the results provided and the tools used in the analysis are non trivial. The result should be of high significance to the learnware community, especially since disclosing a model specification may carry risk if there are no formal guarantees. In terms of writing, the authors introduce the learnware problem and their contribution in a clear manner in Sections 1 and 2. The figures presenting the trade-offs between the different choices of $m$ are also very helpful for readers who may not be able to follow the theoretical results.

Weaknesses:
I think the clarity of Sections 3 and 4 can be significantly improved, so that they can be more approachable to a broader audience. 

For Section 3, the authors present core results that are the basis of Theorems 3.4 and 3.5 but the connection to these theorems is not particularly clear. I would advise the authors to first explain the proof sketch and then present the key lemmas and how they connect to the proof sketch. See also the questions section for more.

For Section 4, I understand that the setting is even more complicated compared to Section 3 but providing some more intuition behind Theorem 4.2, especially the parts that are not already covered in Section 3, would also be helpful.

Limitations:
The authors have addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents the ""Reduced Kernel Mean Embedding (RKME)"" specification, which represents a model's capabilities while ideally preserving the privacy of the original training data. The paper provides a theoretical analysis and proves that the RKME specification can protect the training data against common inference attacks and maintain its utility in the learnware search process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This paper aims to resolve the crucial data privacy challenge while enabling the effective reuse of pre-trained models under the learnware setting. 
* This paper provides a comprehensive theoretical framework to prove the efficacy of RKME in preserving privacy. The proofs are detailed and robust, offering a strong theoretical foundation for the claims about data privacy and security against inference attacks.
* The paper also discusses the practical implementation of the RKME specification in learnware systems.

Weaknesses:
* The paper focuses on theoretical proofs and lacks extensive empirical evidence to support the effectiveness of the RKME specification in real-world scenarios.
* The analysis primarily hinges on the assumption that the RKME specification works optimally with certain types of data distributions and kernel functions.

Limitations:
The authors have included discussions about potential limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper analyzes the data preserving properties of Learnware, wan interesting idea involving a marketplace of pretrained ML models. In Learnware, new inference tasks are matched to ML models capable of solving that task without any raw data being shared. Rather, the method leverages RKME to construct a smaller, synthetic representation of the model's distribution over inputs and outputs. In this work, the paper explores whether Learnware is secure against data privacy attacks (linkage, attribute inference) when using the Gaussian kernel and various assumptions on the data. More compact representations are shown to be harder to attack. However, this reduces model search (retrieval) quality inducing a tradeoff.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ Analysis of the ability of Learnware to resist privacy attacks against the dataset used to train the model makes the Learnware ecosystem more robust. Demonstrating the tradeoff between privacy and search (retrieval) quality is an intuitively clear result.

+ The theoretical results and analyses seem novel to me, as far as I can tell. A brief search didn't turn up anything relevant. (However, this is outside my area of expertise so I'm unable to assess validity.)

Weaknesses:
- The paper analyzes the privacy-preserving properties of Learnware. However, I remain unsure about the benefits of the core Learnware system. Reading through the recent references (""Learnware: Small models do big""), I'm left with many questions which are not really addressed in any of the papers. I don't see how Learnware is better than the existing model sharing infrastructure (model hub, data and model cards, benchmark results, open-source training and inference code). The existing ML model sharing infrastructure is widely used already and doesn't require the new user to even label any data first. Please see the questions below.

- The Learnware ecosystem seems like a very niche area. Without additional details of system usage, it becomes difficult to assess the impact of contributions in this paper.

- I'm not really equipped to comment on the quality of the theoretical analyses. That said, the paper could do a better job of describing how the analyses build on and fit into the larger body of work on related tasks.

- Experiments exploring the tradeoff between data linkage protection and search performance would have been nice to have. Without these, I'm again left wondering if the existing ML model sharing infrastructure (which does not have this issue) is indeed better.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wsHMb4J2o9;"REVIEW 
Summary:
The paper introduces the BFA - a novel quantity to predict and control feature learning in DNNs, as well as the feature speed formula which allows expressing the magnitude of feature updates after one GD step. The paper recovers key properties of known HP scalings, and also extends these results by introducing a new HP scaling for large depth ReLU MLPs.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The BFA and BFK are interesting objects to study and the geometrical picture that arises (mentioned in the introduction) gives a nice intuition.

2. The main results (Thms 2.1 and 3.2) are clearly stated and the proofs are straightforward. 

3. The contributions are clearly stated and the relation to previous work distinguishes these contributions. 

4. Earlier results are recovered here with a transparent derivation, but Ref [1] also provided quite an intuitive derivation, as you mentioned. 





[1] https://arxiv.org/abs/2310.17813

Weaknesses:
Despite the strengths mentioned above, I did not give a higher score for the following reasons: 

1. Novelty for HP scaling: 
As far as I can see, the main takeaway regarding HP scaling is the extension of known results, such as muP, to the limit of large width-then-depth. While this is indeed new, this is a somewhat limited contribution. 

2. Applicability of results: 
While some of the results are rather general (like Thm 2.1), some other parts of the results seem to apply only under rather limited conditions, e.g. only a single input. 

3. Experimental findings: 
I found issues with some of the experimental findings: I did not find a mention of what is assumed about the data: is it synthetic, random, from some known benchmark etc. Also, by inspecting Fig 2b I was not convinced that the output sensitivity is bounded away from zero. 

4. I feel that the paper could be made less technical and more readable by delegating some of the proofs to the Appendix and using the space for some visualizations. 




typos: 
- Fig1 caption 1st line: witdh -> width

Limitations:
The authors adequately addressed the limitations of their results.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel perspective on infinite width and depth feature learning networks. It introduces the backward-to-feature kernel (BFK) as a central quantity determining the evolution of the intermediate layer features. The paper shows that the movement of the hidden layer features can be exactly related to an angle $\theta_\ell$ between the backward pass and the feature velocity, and uses insights on the scaling of the cosine of this angle with width to recover essentially all known infinite width and depth feature learning limits, as well as a novel large depth MLP limit.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper studies an extremely important topic in deep learning theory. Given the technically challenging nature of the study of large width and depth limits, the paper is superbly well-written and accessible. Prior papers by Yang et al and Bordelon et al have done important work in developing the study of large width and depth limits, but their derivations are either very dense or otherwise rely on non-rigorous methods to derive the scalings. This paper manages to both rigorously motivate feature learning at infinite width and depth while simultaneously making the paper short and accessible. This is a major strength and no easy feat. I commend the authors on it. 

Beyond this, there are several results of strong technical merit that will be of value for researchers studying infinite width and depth limits. The infinite depth MLP and scale invariant learning rate discussions are particularly interesting. The authors do a good job placing their work in context by presenting tables comparing their parameterization to others. 

Ultimately, I believe that this paper is not only technically novel and sound, but is also a service to the community. I strongly recommend it for acceptance.

Weaknesses:
There are no technical weaknesses that I have found, and I have gone through the derivations in detail. My only comment is expository:
In equation 1, the definition of the forward pass $T_{\ell}(f_{\ell-1}, w_\ell)$ as well as its discussion in terms of selection derivatives is quite technical and may confuse readers from outside sub-communities in machine learning. I recommend stating more clearly that this includes a simple forward pass such as $W_{\ell} \cdot f_{\ell-1}$ and perhaps adding a footnote to make this first paragraph a bit more readable.

Limitations:
Large width and depth limits may serve to determine the scaling laws for the next frontier of language and vision models, which may have major societal impact. However, the theoretical nature of the paper limits any major negative impacts.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a technical strategy for deriving neural net parameterizations that relies on controlling the angle between the activation gradient and the feature update. The authors derive various theoretical results about this quantity, including a formula for computing it, and some analyses in the context of MLPs and ResNets. The authors claim to use this principle to derive new parameterizations, but crucially they never test them in a real learning problem.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- the authors propose an interesting notion and derive interesting analyses surrounding it
- the parts of the math I checked seem rigorous and sound
- the authors do a good job of connecting their work to related work
- the ideas are quite creative

Weaknesses:
I need to preface this review by saying that this feedback is intended to be constructive and to help you improve the paper. My current impression is that the paper is not ready for publication. I strongly encourage you to keep working in this direction, and I hope this feedback will be useful for that.

With that said, the main issues I see with the paper are:

### **No real experimental evaluation**

My understanding is that the main practical outcome of your work and theoretical analysis is a new parameterization for training neural networks. I feel that it is really important for you to test this parameterization to check that it is useful, or at least to see what its properties are in an actual training situation. It's so easy to come by free cloud compute (e.g. Google Colab) that I can't really see a reason for not doing this.

I don't feel that the experiments in Figures 1 and 2 are enough to convince me of the utility of your framework. Also I'm not sure how to reproduce these experiments. For example, what dataset did you use? What is the loss function?

As a side note, I'm also a bit doubtful that you can even train MLPs effectively beyond depth 20 or so. I read the Jelassi et al paper (https://arxiv.org/abs/2305.07810) and noticed they don't test their parameterization either. I may be wrong here, but I don't think you can hope for some engineer or experimentalist to pick up the paper and implement things for you. I think you have to be proactive here.

### **Doesn't go that far beyond existing ideas**
A lot of the paper focuses on dealing with analyzing or re-deriving existing parameterizations---e.g. muP or the 1/sqrt(L) depth scaling in ResNets. But this is not so interesting because it has already been done and there are already ways to analyze these things. What does your analysis offer that prior analyses do not? I also want to point out that concurrent works to this paper go beyond 1/sqrt(L) depth scaling rules. For example arxiv.org/abs/2405.15712 and arxiv.org/abs/2405.14813. And these papers actually experimentally test these deviations. Clearly these are concurrent works, but I just mention it to demonstrate that there is more out there.

### **Paper only seems to focus on batch size one**

In my opinion, doing things only at batch size one is a bit toy, and it would be better to directly analyze larger batch sizes.

Limitations:
Overall, I think it's hard to assess the limitations without having more thorough experimental evaluation. I would encourage you to work out how to streamline the mathematical exposition, and then to start testing these ideas.

I realize this feedback might be construed as being fairly negative, but I hope that it can help to improve the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the feature learning speed of the layers of Neural Networks (NNs). Specifically, it proposes to measure it through the quantity *Backward-Feature Angle* (BFA), denoted by $\theta_l$ for a layer $l$. This quantity is directly related to the layer-wise decomposition of the Neural Tangent Kernel (NTK). In practice, the BFA is measured experimentally and several properties (feature learning, signal propagation, etc.) can be related to the BFA.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
# Originality

This paper tackles an important problem: the relation between the optimal hyperparameters of a neural network and its architecture.

# Clarity

This paper is easy to read and the statements are clear.

Weaknesses:
# Originality

The BFA is closely related to the layer-wise decomposition of the NTK, which is already widely used in the NN optimization literature [1, 2, 3, 4]. Overall, the BFA does not contain any information that is not already available with previous objects.

# Significance

The benefits and the properties of the BFA are still unclear.

For instance, Section 5 proposes a new scaling of the hyperparameters, that is not clearly related to the BFA. Besides, the experimental validation of this new scaling is not provided.

# Quality

The contribution of this paper is unclear. The usefulness of the BFA, either theoretical or experimental, is still unclear, and the proposed hyperparameter scaling is not tested experimentally.

# EDIT: References

[1] Gradient descent provably optimizes over-parameterized neural networks (2018), Du et al.

[2] Gradient descent finds global minima of deep neural networks (2019), Du et al.

[3] A convergence theory for deep learning via over-parameterization (2019), Allen-Zhu et al.

[4] Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks (2020), Zou et al.

Limitations:
Lack of experimental validation.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wsGzvhnoaX;"REVIEW 
Summary:
This paper considers using quantum methods for stochastic optimization, using zeroth order queries. It looks like the main idea is that using quantum methods, one can summarize over finite difference calculations quickly and efficiently, to arrive at approximate subgradients efficiently; this would usually be very inefficient for classical methods. Overall, they are able to show speedup, from $O(\epsilon^{-4})$ to $O(\epsilon^{-3})$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The problem is well contained and the premise is believable. 

The classical optimization bits looks reasonable, and the results make sense. 

I skimmed through the appendix, and the classical optimization parts are reasonable.

Weaknesses:
Section 3 is a bit hard to follow. The specific speedup offered by the quantum method is not entirely clear, though it is likely coming from Theorem B1. Perhaps a deeper discussion of this, and why this quantum speedup exists (e.g. is it a consequence of Deusch Josza? Can you provide a more complete argument for where the speedup appears? )

Limitations:
no societal limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates quantum algorithms for finding the $(\delta,\epsilon)$-Goldstein stationary point of a potentially nonconvex and nonsmooth objective function $f$. Utilizing quantum variance reduction techniques as outlined in [42], the authors have developed a zeroth-order quantum estimator for the gradient of the smoothed surrogate of $f$. The stationary point of this smoothed surrogate is also the Goldstein stationary point of $f$ when using an appropriate smoothing parameter $\delta$. Leveraging this zeroth-order quantum estimator, the authors propose two algorithms, QGFM and QGFM+, to find the Goldstein stationary point, achieving a quantum speedup on the order of $\epsilon^{-2/3}$. Additionally, the QGFM+ framework adjusts the variance level during each variance reduction step, providing further acceleration to the Q-SPIDER algorithm described in [42] for smooth nonconvex optimization.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper initiates the study of quantum algorithms for finding Goldstein stationary points, a significant problem in continuous optimization. Additionally, the authors present an explicit construction of the quantum sampling oracle using the quantum zeroth-order oracle, including a detailed discussion on the number of qubits required.

Weaknesses:
Despite the detailed implementation and calculations, the overall technical approach remains relatively straightforward. The zeroth-order quantum estimator combines the classical stochastic gradient estimator for the smoothed surrogate with the quantum variance reduction algorithm in [42]. The quantum algorithms for finding the Goldstein stationary point are obtained by replacing the classical estimators with quantum estimators. Moreover, the narrative is somewhat incomplete due to the absence of lower bound results.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies quantum algorithm for non-smooth non-convex stochastic optimization with zeroth-order oracle. It introduces an effective quantum estimator that reduces the variance compared to classical zeroth-order estimators. Upon substituting this estimator into known zeroth-order non-smooth optimizers, namely GFM and GDM+, the resulting quantum optimizer achieves improved rate $\tilde O(d^{3/2}\delta^{-1}\epsilon^{-3})$ and $\tilde O(d^{3/2}\delta^{-1}\epsilon^{-7/3})$ respectively for finding a $(\delta,\epsilon)$-Goldstein stationary point. Notably, quantum speedup improves upon the classical lower bound $\delta^{-1}\epsilon^{-3}$ by a factor of $\epsilon^{2/3}$. Moreover, a modified algorithm achieves $O(\sqrt{d}\epsilon^{-7/3})$ for smooth optimization, improving upon the best known rate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes a new zeroth-order quantum estimator. This leads to new quantum algorithms that solves zeroth-order non-smooth non-convex optimization problem, which is not well studied in the literature. Moreover, the proposed algorithms show quantum speedup compared to their classical (non-quantum) counterparts. Notably, it improves over the classical lower bound of $\Omega(\delta^{-1}\epsilon^{-3})$ by a factor of $\epsilon^{2/3}$. Overall, these results represent a significant contribution to the understanding of optimization with quantum oracles. Given my expertise lies primarily in optimization and not in quantum computation, I am only able to assess the optimization-related aspects of this work.

Weaknesses:
Although the dependence on $\delta,\epsilon$ is improved, the dimension dependence is suboptimal. In particular, since GFM and GFM+ are known to have suboptimal dimension dependence $d^{3/2}$, so do QGFM and QGFM+. On the other hand, as observed by Kornowsky and Shamir [1], optimizing the random smoothing $f_\delta$ with a non-smooth optimizer, such as online-to-non-convex (o2nc) [2], eliminates this $\sqrt{d}$ factor and achieves $O(d)$ in dimension. Hence, my intuition suggests that upon substituting the quantum estimator into o2nc and following a similar approach to Kornowsky and Shamir, the authors might be able to recover $O(d)$ (or even better) dimension dependence. 

[1] Kornowski, G. and Shamir, O., “An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization”, 2023. doi:10.48550/arXiv.2307.04504.

[2] Cutkosky, A., Mehta, H., and Orabona, F., “Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex Conversion”, 2023. doi:10.48550/arXiv.2302.03775.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces new quantum algorithms for non-smooth non-convex optimization problems. The authors propose a quantum gradient estimator for smoothed objectives and develop the Quantum Gradient-Free Method (QGFM) and its enhanced version, QGFM+, which achieve better query complexities than their classical counterparts. These complexities demonstrate a marked quantum speedup over classical counterparts, indicating the potential of quantum computing in optimizing complex functions more efficiently. The paper also discusses the construction of quantum oracles and the application of variance reduction techniques, paving the way for future research in quantum optimization.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper proposed new zeroth order quantum optimization algorithms achieving better computational complexities compared to classical methods for non-smooth and non-convex optimization.
- Technically, they construct efficient quantum gradient estimators and quantum superpositions over required distributions as a key subroutine.
- They also proposed a quantum algorithm for non-convex smooth problems with an adaptive variance level, accelerating prior quantum algorithms to get more speedups.

Weaknesses:
- The assumptions of having a quantum stochastic function value oracle may be strong. Could the authors explain more about why it is reasonable and important to have such a function oracle?
- The technical core for quantum speedups seems to be the quantum mean value estimation procedure, which is already used in many other optimization problems and scenarios. Could the authors explain more about the technical novelty of their work?

Limitations:
The quantum complexity lower bound on this problem is not proved in this paper, which is mentioned in the conclusion part.  Also, as noted in remark 3.7, implementing quantum sample oracle may require the uses of QRAM, which is currently limited by the physical realizations.
This is a theoretical work, so there is no potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wqs2RMq4CW;"REVIEW 
Summary:
This paper studies corruption-robust linear bandit optimization and characterizes the regret bound in terms of both weak and strong corruption measures. Under the stochastic setting, this paper proposes a phased elimination algorithm, and the regret bounds match the lower bound. Under the adversarial setting, the paper proposes two individual algorithms for the two corruption measures respectively. In addition, this paper studies gap-dependent misspecification setting through reduction, and discusses a use case for linear MDPs.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The regret bounds in terms of both corruption measures are provided, where the regret bound depending on $C_\infty$ is first introduced in this paper.
- The theoretical results are supported with detailed proof.
- This paper is generally well-written.

Weaknesses:
- The algorithms are efficient regarding regret bound, but the computational complexity is not discussed.
- A conclusion section could be added.

Limitations:
Some limitations are discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors characterize the problem of learning the presence of reward corruption in the linear bandit setting. They provide matching upper and lower bounds in the corrupted stochastic setting, and initiate the study on the corrupted adversarial setting, for which they obtain optimal scaling in the corruption level.

Not only that, the authors prove a general reduction that efficiently handles gap-dependent misspecification with corruption-robust algorithms. They were able to show that linear MDPs with gap-dependent misspecification are efficiently learnable. While this reduction is general, interestingly they denied the possibility to obtain the tightest rate for gap-dependent misspecification. This observation leads them to develop a specialized algorithm which, in the linear bandit setting, obtains the optimal rate. According to their argument, this resolves the open problem of Liu et al. (2023a).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Interesting results
    - Deterministic algorithm cannot avoid suboptimal regret (Proposition 1)
    - Matching upper and lower bound on the stochastic setting, by just changing deterministic sampling to stochastic.
    - Solving an open problem of instance-dependent misspecified setting.

- Clearly state the limitations of previous works and their technical novelties.
    - Easy to understand their contributions.

Weaknesses:
- (Minor) The algorithms are not seriously different from the previous works as they mentioned, but this is just a minor point - every theoretical improvement is important. 

- Not clear what they tried to say on page 9
    - Why Theorem 6.2 shows that $\rho \leq \frac{1}{d}$ is not optimal?
    - Impossibility result (from line 304): so basically what authors are trying to say is, that 'their' reduction is not applicable for a tighter result, right? It is not about any reduction from corruption to misspecification. 

- No future works.

Limitations:
The authors adequately addressed the limitations. It would be great if authors provide possible future works. 
There is no potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studied the corrupted linear bandits. The authors propose four different metrics to evaluate the total corruption in Eq. (1). Many settings are considered in this paper. For stochastic LB, the proposed algorithm achieves a regret bound of $d\sqrt{T}+\sqrt{d} C_{\infty}$. For adversarial LB, the proposed algorithm achieves a regret bound in the order of $d\sqrt{T}+\sqrt{d} C_{\infty}$ or $d^{3}\sqrt{T}+d^{5/2} C$. The authors also consider the gap-dependent misspecification, where the misspecification level of an arm $a$ can be evaluated by $\rho$ times the gap of arm $a$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
See summary.

Weaknesses:
**Weaknesses and Questions:**
1. At lines 107-109, the authors claim that the strong adversary is equivalent to the CM viewpoint. This doesn't seem right. For regret, the strong adversary is harder than the CM viewpoint. Thus, it is unfair and wrong to compare He et al. (2022) in the same way.
2. At line 131, adversarial linear bandits are discussed. However, no problem definition of this problem is introduced before line 131.
3. This paper studies the fixed action set, while the previous works He et al. (2022) and Foster et al. (2020) allow the action set to be chosen by an adaptive adversary, which is much harder than this paper. Table 1 is not fair. He et al. (2022) is for the adaptive adversarial viewpoint, which is totally different from the stochastic LB. For a fixed action set, the optimal regret without $C$ should be $\sqrt{d T \log k}$, where $k$ is the number of arms.
4. Assumption 1 is not very reasonable.

Limitations:
N.A.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wqLC4G1GN3;"REVIEW 
Summary:
The paper addresses the limitations of existing diffusion-based inverse problem solvers, which typically frame signal recovery as a probabilistic sampling task. The authors propose a novel approach that redefines the generative process as a discrete optimal control task. Inspired by the iterative Linear Quadratic Regulator  algorithm, this new framework named diffusion optimal control, can handle various differentiable forward measurement operators, including super-resolution, inpainting, and deblurring.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper introduces a novel framework based on optimal control theory to solve diffusion-based inverse problems, moving away from the traditional probabilistic sampling approaches. This is a significant theoretical advancement.
2. The framework addresses critical drawbacks of current methods, such as the intractability of the conditional likelihood function and dependence on score network approximations. This leads to more robust and potentially more accurate solutions.

Weaknesses:
1. The method involves complex mathematical formulations and optimal control theory, which may pose challenges for implementation and understanding by practitioners who are not familiar with these concepts. The need to compute Jacobian and Hessian matrices, as well as the regularized inverses, may lead to significant computational demands, particularly in high-dimensional settings.

2. Lacking of enough experiments, such as MRI reconstruction or other medical images. Including more diverse datasets and additional baseline methods would provide a more comprehensive evaluation.

Limitations:
The method requires the forward measurement operator to be differentiable. In cases where this is not possible or practical, the applicability of the proposed framework may be limited.

The performance evaluations rely on specific pretrained models. The method’s robustness and performance with other pretrained models, or models trained on different data distributions, would need further investigation.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes diffusion optimal control that solves inverse problems via posterior sampling by combining the power of a pre-trained unconditional diffusion model and the iterative Linear Quadratic Regulator algorithm to produce optimal controls that steer the reverse diffusion process to correctly recover the original signal. 
The framework is general and able to handle any differentiable forward measurement operator and establishes a new baseline in image reconstruction.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The idea of augmenting the reverse diffusion sampling process with a perturbation control is quite novel and general for arbitrary cost functions, although the paper focuses specifically on the cost for posterior sampling.
* The writing is generally good (see more comments regarding writing in questions). It is concise and to the point with good intuitions provided.
* Efforts (e.g. low-rank approximations) have been made to bring down the computational cost in iLQR for the high-dimensional image setting.
* The empirical performance of the proposed method is strong and establishes new state-of-the-art results.

Weaknesses:
* The runtime of the proposed method seems high and is not much discussed. iLQR is a global-in-time iterative method that could potentially require a large number of iterations to converge (and all nice things discussed in Section 4. rely on this convergence). On top of that, for each iteration, there needs to be $\Omega(T)$ matrix solves which can be quite slow given the dimension of the images (even with techniques like low-rank approximation).  It would be interesting to see ablation studies on the effect of num_iter in Algorithm 1. It would be also more convincing to report the runtime of each method in Table 1.
* There is no analysis of the approximation error of iLQR (the first and second-order Taylor approximations) in the studied setting. Specifically, it seems to me that a lot of heavy lifting is done when the control $u_t$ is designed to be only a perturbation of the reverse diffusion step. For instance, does this imply that the value function is smoother (hence the Taylor approximation is more accurate) when parameterized by $u_t$?

Limitations:
There's not much discussion about the limitations in the paper. There is no potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new approach to conditional generation tasks through score-based diffusion models, with a focus on inverse problems.   As an alternative to using the likelihood $p(y | x_t)$ to guide the time-reversed SDE towards the posterior distribution, the authors reformulate this as an optimal control problem.    Starting from the ODE-based time-reversed flow for the unconditioned prior, the authors derive a controller based on the iLQR algorithm to guide the particles towards high posterior probability regions.    The authors provide theory to demonstrate that the optimal guidance coincides precisely with the desired conditional scores.  They demonstrate the method on a number of benchmarks including image inpainting and other inverse problems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well written and very clear.   The method appears novel and addresses a legitimate challenge in conditional diffusion models.  As the authors acknowledge: optimal control formulation of diffusions exist, but not (to my knowledge) in the context of guiding conditional diffusion models.     The theoretical results provide a sound justification of the validity of the approach.   The numerical results demonstrate that it is competitive in terms of accuracy compared to baseline, established methodology.

Weaknesses:
The main weakness is the sheer computational cost of the algorithm,  the need to compute very expensive hessians drastically limits the practical use of this method.    The authors suggest a number of low rank approximations to mitigate this, but it is unclear how much is lost by introducing them.      One point of question is the interplay between the number of diffusion steps $m$ and $T$.   As $m\rightarrow \infty$, for $T$ fixed and large we expect that the baseline conditional diffusion model will improve significantly in accuracy.  Generally, I feel that the configuration of the baseline has not been explored (or if it has, it has not been reported carefully).   Similarly, the authors claim that they have done equivalent budget analysis in the experiments -- I could not find the details of this: is it the case that the computational cost is the equivalent?  Have the author really explored the hyper-parameter space for these methods.

Limitations:
The main limitation is the large computational cost of this methodology.   This has been identified and acknowledged.  The authors have claimed to do an equivalent budget analysis, but this maybe needed a bit more details (wall-clock time, etc).   Potential societal impacts are addressed in the Appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper uses the optimal control theory to solve the diffusion posterior sampling problem by iterative Linear Quadratic Regulator (iLQR) algorithm. The method could be utilized to solve both linear and nonlinear inverse problems.  Experiments on MNIST and FFHQ demonstrate the outperformance of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-written, with a good summary of previous methods and their shortcomings. 

2. The proposed method that interprets the reverse diffusion process as an uncontrolled non-linear dynamical system is novel. Theoretical support is provided to verify the algorithm.

Weaknesses:
1. The method is well-backed but might be computationally exhausting.

2. The experiments are limited. Quantitative results on different datasets and nonlinear inverse problems are lacking.

Limitations:
Yes. They mentioned limitations and impacts in the appendix, which looks good to me.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper tackles inverse problem via the perspective of optimal control. By treating the diffusion process (ODE) as a non-linear dynamic system and the extra guidance term as control signal, the authors manage to optimize the diffusion trajectory via the iterative Linear Quadratic Regulator (iLQR) algorithm. Several techniques are used to make the iLQR algorithm more efficient. This paper show good results on FFHQ dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea is interesting and reasonable. Using optimal control to solve the inverse problem enables us to optimize the whole sampling trajectory and avoid the error for estimating $x_0$ via Tweedie's formula. And the results on FFHQ dataset looks good.

Weaknesses:
1. High computation cost: Despite the advantages mentioned above, one obvious drawback of this method is the potential high computation cost. This includes: 

      a. Computing and storing the Jacobian matrices, which can be of very high dimension, can be very costly. Although the authors 
      further propose some techniques to reducing the cost, these methods might also bring extra approximation error as well as more hyper-parameters to tune; 

     b. Optimizing the the whole trajectory requires evaluating the whole trajectory for many times and do iterative updates. This requires more computation. Thus, though in Table 1, the authors denoted their methods as $T=50$ and $T=20$, considering the iterative update nature over the whole trajectory, this might not be directly comparable (and might actually need more computation) to other methods, which are denoted as $T=1000$. And the authors might have to greatly reduce the timesteps to make the whole algorithm affordable, this might also bring extra approximation error.


2. Lack of more complex dataset: Though the authors achieve good performance on FFHQ dataset, considering the human face data is relatively easy (aligned, not very multimodal), it is still not very clear to me how the proposed method can work on more complex dataset, for example, on ImageNet. From my own experience, the ImageNet data can be much harder than the FFHQ human face data in the inverse problem. And considering the approximation error introduced in iLQR algorithm, computing the Jacobian matrices as well as using less timesteps, it might raise concerning regarding whether the proposed algorithm can work well on more complex dataset.

3. Minor suggestion: I think it might be better for the authors to add more introduction for the optimal control part in the main paper. Or at least give more clear introduction for the notation used in 2.3. Currently, I find it not very clear to people without much background in optimal control.

Limitations:
The authors has adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper uses tools from optimal control to introduce a novel approach for solving inverse problems with diffusion models. The authors propose reframing the generative process of diffusion models as a discrete optimal control problem allowing to leverage the iterative Linear Quadratic Regulator (iLQR) algorithm. Tackling limitations of existing probabilistic sampling methods, the resulting method demonstrates promising performance for inverse problems on FFHQ, such as super-resolution, inpainting, and deblurring.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
While many connections between optimal control and diffusion models have been established, the proposed algorithm leverages variants of iLQR to provide a fresh perspective on training-free posterior sampling with diffusion models. The paper provides additional theoretical guarantees as well as multiple modifications (randomized low-rank approximations, matrix-free evaluations, and adaptive optimizers) to reduce computational costs. Finally, several ablations are presented for the proposed method.

Weaknesses:
1. The claims of the paper are not sufficiently supported by experiments and/or theory (the first statements in the following are just examples---similar statements can be found throughout the paper):
	* ""dependence on the approximation quality of the underlying terms in the diffusion process"": only a result for a single image is provided (Fig. 6). The current paper also does not seem to provide theoretical results for such robustness as claimed in ""reconstruction performance is theoretically and empirically robust to the accuracy of the approximated prior score"".
	* ""its sensitivity to the temporal discretization scheme"": for the baselines, again only a result for a single image is provided (Fig. 3). Moreover, the number of steps is typically reduced to accelerate the algorithm. Accordingly, methods should be compared in terms of performance vs. runtime/flops and not the number of diffusion steps. It seems that the proposed method is significantly more expensive than competing methods (in particular, since `num_iters>=50` full simulations are used).
	* ""its inherent inaccuracy due to the intractability of the conditional score function"": The conditional score function remains intractable, one just obtains an approximation via iLQR, since the obtained $x_0$'s obtained from the iLQR iterations only *approximately* converge to the posterior distribution *in the limit*. Statements like ""Moreover, our model always estimates $x_0$ exactly, rather than forming an approximation $\hat{x}_0 \approx x_0$"" sound misleading. Using iLQRs, we simulate ""nominal"" trajectories and thus iteratively obtain an approximate candidate for $x_0$ which will be used for the refinement of the control. In a similar (however, useless) fashion one could also use, e.g., DPS to obtain an estimate of $x_0$ and then run a probability flow ODE simulation where the scores are conditioned on $x_0$ (instead of $x_t$) to have a ""method [that] produces controls that coincide precisely with the desired conditional scores"". However, the advantage of DPS lies in the fact that only a single simulation is needed.
	* ""on several inverse problem tasks across several datasets"": Apart from a single figure on MNIST (without metrics and for only a single baseline and task), results are only provided for FFHQ.

2. Moreover, several of the mentioned limitations have been already tackled by alternative approaches to posterior sampling with diffusion models, e.g., variational approaches (https://arxiv.org/abs/2305.04391) or resampling strategies (https://arxiv.org/abs/2307.08123).
3. Finally, the appendix could provide further details on
	* hyperparameter choices and optimization for the baselines.
	* precise assumptions for the theorems.

Limitations:
See ""weaknesses"" above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wpGJ2AX6SZ;"REVIEW 
Summary:
This paper introduces a new framework into algorithmic predictions. The paper asks and answers the question ""how can we incorporate human input into the prediction algorithm, which may not even be captured in the training data""? The authors develop a method that first runs the predictor, and then runs a second predictor using the human input. The authors show that even a simple instantiation of their method can outperform existing predictors. They use the X-ray classification task as experimental datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is written very clearly, and offers a novel method to incorporate human input into algorithmic prediction. Both theoretical derivations and experiment results are sound. The contributions of this paper is significant, and I believe this paper deserves to be accepted in its current form.

Weaknesses:
The paper would be even more satisfying if the method is presented as a framework rather than a specific instantiation. In addition, it would be great if the authors can discuss potential ways to improve on the method they propose, and what these methods mean in the broader context of incorporating human feedback into algorithmic predictions. Nevertheless, these small weaknesses does not diminish the significance and novelty of this paper.

Limitations:
The authors have addressed the limitations in the conclusion section

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework to incorporate human expert knowledge in algorithmic predictions. Under this framework, the authors introduce a meta-algorithm that uses a training dataset including human expert predictions together with a multi calibrated partition of the data; a partition of the dataset into bins, where each bin contains data that are indistinguishable to the predictive model. Using the data of each bin the meta-algorithm trains a regression algorithm to predict the true label from the human expert prediction. In this way, the authors aim to leverage the human expertise, that may be more accurate than the predictive algorithm on specific instances, to achieve complimentary—to achieve higher predictive accuracy through human AI collaboration than the performance of a human expert or AI in isolation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper suggests an elegant method to improve algorithmic predictions in light of human expertise, that could have significant applications such as the medical domain, where the additional information of human experts may lead them to more accurate predictions on certain instances compared ot predictive models.    

The paper is very well and clearly written, nicely motivated and follows a clear structure. There is a thorough and comprehensive discussion on related work as well as a comprehensive and clearly presented experimental evaluation.

Weaknesses:
Since the theoretical results of section 6 complement the ones of section 4, it would be perhaps more natural to follow them, rather than placing them after the experimental evaluation, which appears a bit odd.

Limitations:
The authors adequately discuss the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper first presents some theory for the modelling of how to identify when human judgements may offer a better diagnosis - through access to additional information - than machine predictions, despite the latter typically being more accurate. This is followed by exploring how to integrate the human input with the algorithmic (model) input. Subsequently, the authors present some focussed experimental results using chest x-ray interpretation that support their proposition.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: carefully drawn comparison with the literature, situates and differentiates the contribution.

Quality + Clarity (addressed together):

Clear abstract and intro with well-defined contributions. Content offers a reasonable balance between technical and intuitive. Recognition of the value of the human contribution and seeking to integrate it in decision making.

The later mathematical results (section 4) have effective accompanying interpretations (see complementary point in weaknesses).

Effective, selective presentation of results: choosing one and going into detail, while two other cases in the appendices support the same observation, rather than trying to squeeze them all into the paper body. Same applies to results in section 5.2.

Significance: provides a sound framework for a particular, amenable class of collaboration problems that allows for the proper incorporation of human prediction where machine prediction could fall short.

Weaknesses:
Clarity: Indistinguishability and multicalibration are critical elements to the contribution; it would be helpful if the interpretation of their definitions (3.1, 3.2) went into a bit more detail for accessibility.

This reader is not succeeding in following the argument about robustness (section 6).

Limitations:
Section 7 provides some properly reflective critique on scope and applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a framework for joint human-AI prediction, where human experts can augment AI predictions in particular ex ante identifiable subsets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper makes a lot of interesting contributions. First, its scope is broad and important: it tackles the question of how and whether human judgment can improve the predictions of any learning algorithm. That is and will remain to be a very important question in our time. It contributes a very interesting framework, rooted in algorithmic indistinguishability and multicalibration, to find subsets in which no algorithm in a user-specified class has predictive power (because they are algorithmically indistinguishable) but human experts do (because they might have more access to the instances, such as doctors examining patients). It demonstrates that using this framework, we can find subsets of instances where human experts can outperform algorithms, and thus the combination of the two can outperform either alone. It applies this to an important medical problem and in another domain of making predictions from photos of people. It even extends the framework to apply to a setting with noncompliance. The community stands to learn a lot from this paper.

Weaknesses:
As the authors mention, the framework is dependent on minimizing mean squared error only.

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
woRFmNJiLp;"REVIEW 
Summary:
This paper proposed a new method for LLM alignment during pre-training. The proposed method is call ""native alignment"". This method include three steps: pretrain date duplication, alignment rewriting, and model training. They trained small size alignment expert model for alignment rewriting and use the model to rewrite large-scale pre-training data. The rewriting process suppose to solve format issue, value/fairness issue, unsafe content in pre-training data. They experimented with Arabic data and LLMs. Their experiments shows that the proposed method can help LLMs be more safe and helpful.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper proposed a new idea to align LLMs during pre-training. It seems an interesting topic. 
2. The paper writing is clear and well-organized.

Weaknesses:
1. Lack of comparison to existing post-alignment methods. The proposed method is a ""native alignment"" during pre-training. I wonder if this method can outperform the post-alignment methods. While the author acknowledged this limitation, I still feel it is important for strengthening their claim. 
2. Need more analyses to better understand their method's potential trade-off. For example, I wonder if rewriting pre-training data undermines the LLM's capacity to understand and learn Arabic dialects. The rewriting process may convert Arabic dialects into MSA. I also wonder if the rewriting data inherited hallucinations from LLM and deteriorated the trained model. 
3. The paper needs more clarification on experiment details. For example, they exploit Arabic data to investigate their method, however, the evaluation dataset, BeaverTails dataset, is an English dataset. I wonder how they evaluate and if they translate the samples.

Limitations:
I think that the paper needs more diverse analyses to understand the potential trade-off of their method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduce a method called ""native alignment"", which is a set of procedures to create data and train an LLM to rewrite raw text into ""useful"" texts for pretraining. They apply this technique specifically for Arabic LLMs and conduct experiments to show that this pre-processing of pre-training data helps produce better Arabic LLM down the line. As bonus, they release open-source Arabic LLMs for the communities

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper ideas are presented clearly and easy-to-understand

Weaknesses:
* As a proclaimed novelty, the paper draws itself between pre-alignment and post-alignment, indicating that previous work only focus on post-alignment but not pre-alignment. However, I afraid the paper misunderstands the concept of post-alignment (RLHF) and fails make an accurate comparison. Post alignment (RLHF) is finetuning technique to train the models to reward good-vs-bad response according to human values, and train the policy models to lean on the good behavior and stay-away from the bad behaviors gradually, often with the existence of a reference model (DPO and RLHF).

Meanwhile, the ""native alignment"" presented in the paper is a data-cleaning procedure, and it does not having any resemblance or contrast with ""post-alignment"". Furthermore, using LLMs or training LLMs to rewrite raw text to produce cleaner data is not new or novel, there are many techniques out there that do so, and there are abundant open-source data on huggingface which were produced in similar ways.
This confusion between data cleaning and alignment makes the paper less credible and the lack of novelty it the methodology itself, as a data cleaning method, is also troublesome.

Obviously as a result, the paper did not provide any necessary and required experimental comparisons with other data cleaning methods.

* Though I do appreciate the paper's effort for Arabic community, the scope of only Arabic LLM is small and generally inconclusive, that such method is not shown to generalize to other languages, domains. Perhaps, thus, the work is really not suitable for NeurIPS but more suitable for CL-type venues

* It is unclear from the writing whether the authors pretrained Llama-3 with Arabic from scratch or further finetune from Llama-3 checkpoint. In either case, there should be explanation and further ablation studies.

Limitations:
The authors discussed limitations

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a data augmentation pipeline which modifies the pre training data for large language models in key aspects such as formatting, values, content moderation and knowledge preservation. The resulting pipeline, termed native alignment, is applied Arabic LLMs due to the relatively small pretraining corpus available and the difference between Arabic and western culture. Experiments are conducted to test the performance on a few metrics including trustworthiness, knowledge, and Arabic localisation.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This is a well written paper targeting the important topic of llm alignment. It also addresses the relatively under explored sub question of how to improve alignment at pretraining. The resulting pipeline presents a reasonable idea, and the evaluations are clear and I find them comprehensive too. The author(s) should also be commended for their transparency regarding the limitations of the paper.

Weaknesses:
Although this might have become the norm of recent LLM papers, I still think it is important to include a discussion of the metrics used to measure things like 'trustworthiness' and 'knowledge', as these are qualitative metrics, whereas in the paper, it seems like the authors just quoted some existing evaluation pipeline.

Limitations:
As the authors are already open about, comparisons with other post alignment methods are not included. The authors attribute this to an absence of existing alignment evaluation benchmark, but I don't fully understand this - what is stopping the authors from using the same alignment benchmarks as ones they have already used to compare with other pretrained models?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on alignment of LLMs to human preferences and suggests to shift the alignment step from instruction-tuning (post-alignment) to the earlier stage of continued pre-training (native alignment). For that end it proposes an approach to creating aligned pre-training data, consisting of three steps: (1) seed data cleanup and rewriting with humans'/LLM help, (2) training a supervised cleanup model on that seed set and (3) processing the final pre-training dataset with that cleanup model. Presented experiments show that alignment data results in higher final quality compared to unprocessed pre-training data and that the performance gain does not reach a plateau at 12B tokens, suggesting that the amount of alignment data should be limited by the budget allocated to train an LLM. Experiments are performed on Llama-3-8B and Llama-3-70B and the Arabic language.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A high-impact and efficient approach to pre-aligned model training is introduced

- Two pre-aligned LLMs for Arabic are released openly based on the experiments in this paper

- Related work is excellent, the paper is written very clearly and is easy to comprehend

Weaknesses:
1. No direct comparison between native alignment and post-alignment is reported

2. Minor text discrepancies are present:
- rows 16-18: partial sentence ""while.."" is not finished
- row 47: missing verb: ""LLaMA3-Tamed-8B could beneficial"" --> ""LLaMA3-Tamed-8B could be beneficial""
- row 326: typo: ""instruction tinning"" --> ""instruction tuning""
- row 150: ""pre-training"" should be called ""continued pre-training"" in this case

3. The created seed data and cleanup models are not released

Limitations:
Ok

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
woENr7FJaI;"REVIEW 
Summary:
This paper presents the Automated Multi-level Preference (AMP) framework for improving MLLMs by addressing hallucination issues. The framework introduces a multi-level preference system for RLHF, aiming to enhance the learning process by providing more granular feedback.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The introduction of multi-level preferences rather than binary ones narrows the gap between adjacent levels, enabling MLLMs to discern subtle differences and integrate cross-level comparisons.
- The automated pipeline for generating high-quality multi-level preference datasets without human annotators is a significant contribution, potentially reducing bias and noise while saving time and resources.
- Extensive experiments across multiple benchmarks demonstrate the effectiveness of the proposed method.

Weaknesses:
- The contribution of the paper heavily relies on the preference fine-tuning algorithm, showing limited innovation beyond this aspect.
- The method does not demonstrate significant improvements on the LLaVA-Bench benchmark.
- The method's performance on the adversarial tasks of the POPE benchmark is moderate, suggesting a need to reconsider the impact of MDPO on model robustness and how to balance performance and robustness.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors develop an automated dataset generation pipeline capable of producing multi-level preference datasets without the need for human annotators. This paper introduces a novel multi-round dialogues hallucination benchmark, MRHal-Bench. Additionally, the authors design the Multi-level Direct Preference Optimization (MDPO) algorithm, which employs a specifically crafted learning objective to facilitate multi-level preference learning. Extensive experiments conducted on both the hallucination benchmark and a general benchmark demonstrate the effectiveness of this method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. To make the labeling of multi-level preference datasets cost-effective and efficient, this paper proposes an automated dataset generation pipeline capable of producing high-quality preference datasets.

2. To narrow the gap between two preference samples in DPO and make the model more easily distinguish the differences between preference data, this paper proposes a multi-level DPO algorithm that use multi-level preference data to provide a broader range of comparisons with hallucination examples.

Weaknesses:
1. It is recommended to provide more quantitative information on the preference dataset generated by the automated dataset generation pipeline. For instance, the authors could use a subset of the dataset to demonstrate the similarity results compared to human annotators.
2. In this paper, the authors conduct experiments on three hallucination benchmarks and only one general benchmark. To verify the more general applicability of the method, additional experiments are needed on general benchmarks such as TextVQA, GQA, and IconQA.
3. In Table 1, the authors compare several MLLMs and RLHF-based MLLMs across MMHal-Bench, MRHal-Bench and LLaVA-Bench. However, the baseline model should be more up-to-date. Could you compare it with more current models such as LLaVA-v1.6, DeepSeek-VL, or MiniCPM-V?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work aims to mitigate hallucinations in Multimodal Large Language Models through preference optimization. Motivated by two limitations of binary preferences widely used in existing work, authors proposed a multi-level preference framework. The framework consists of 1) an automated dataset generation pipeline that converts each image-text pair into an image with multiple text descriptions from superior to inferior quality 2) a Multi-level Direct Preference Optimization algorithm that enumerates over all preference pairs with the standard DPO objective. Additionally, authors introduce a new hallucination benchmark, MRHal-Bench. The proposed framework has been evaluated on three benchmarks: MMHal-Bench, LLaVA-Bench, and MRHal-Bench against 5 base models and 5 preference fine-tuned model. The proposed framework achieves best state-of-the-art on MMHal-Bench and MRHal-Bench, although only improved over the second best FGAIF by a small margin. Authors also include comprehensive ablation studies on the effects of multi-level preference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The application of multi-level preference alignment to the problem of mitigating hallucination in multimodal LLMs is novel.
* Conduct a comprehensive comparison with existing preference fine-tuned multimodal LLMs and baselines on three benchmarks. Improve over existing methods by a small margin.
* Provide an extensive ablation study of the multi-level preference term.

Additionally, automating of the multi-level preference data generation could be a potential strength as well, but currently lacks evaluation to justify its quality (see weakness).

Weaknesses:
I would like to see authors address the following weaknesses: 

* **Lack intrinsic evaluation of the automated multi-level preference dataset**. The quality is only implicitly justified by the improvement on the three final benchmarks (L258-L264), which makes it unclear what are the artifacts introduced in the automated data generation. Although human or GPT-4 annotation can be inconsistent sometimes, it is still good to collect some annotations to directly assess how the generated preferences align with the degree of hallucination. Similarly, the current auto-check mechanism is ad-hoc and introduces another component, i.e., CLIP, which could introduce additional errors into the system. It would be good to conduct some evaluation on the auto-check mechanism as well. 
* **Missing comparison with rank-based preference alignment approaches**: Despite being a novel application, non-binary preference alignment has been studied both theoretically and empirically in context other than hallucination in MLLMs, for example Zhu et al. 2023 [1], Brown et al. [2], Myers et al. [3],  Song et al. [4]. It would be great if this work could engage with prior literature on non-binary preference alignment, for example, discussing how does the proposed objective compare with ranking-based approach in prior work?
* **Missing results of FGAIF on MRHal-Bench** In Table 1, FGAIF has a performance that is considerably close to the proposed methods (-0.14, +0.05) on MMHal-Bench and outperform the proposed method on LLaVA-Bench, yet it's missing results MRHal-Bench. These missing numbers could affect the comparison between the two methods.

References:
* [1] Zhu et al. Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons.
* [2] Brown et al. Safe imitation learning via fast bayesian reward inference from preferences.
* [3] Myers et al. Learning Multimodal Rewards from Rankings.
* [4] Song et al. Preference Ranking Optimization for Human Alignment.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wnPlJNiqfA;"REVIEW 
Summary:
This paper proposes a novel algorithm, KFNN (K-free Nearest Neighbor), which is specifically designed to enhance label integration for crowdsourcing. KFNN integrates two key components named label distribution enhancement and K-free optimization, which significantly contribute to improving the effectiveness and robustness of the label integration process. The idea of automatically determining the optimal neighborhood size for each instance is particularly innovative and well-executed. The experimental results further validate the effectiveness and robustness of the proposed algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The KFNN proposed in this paper is interesting and innovative. The authors reveal the limitations of fixed neighborhood sizes in existing label integration algorithms and propose an algorithm that automatically determines the optimal neighborhood size based on instance attributes and noisy labels. This algorithm significantly improves the robustness of label integration.
2.	The paper provides a solid theoretical foundation for the proposed KFNN algorithm, followed by comprehensive experimental validation. The theoretical analysis is robust and convincingly demonstrates the expected performance improvements. The experiments are well-designed and cover a wide range of datasets, both simulated and real-world, to ensure the generalizability of the results. The experimental results, including comparisons with baseline algorithms, further validate the effectiveness and robustness of the proposed algorithm.
3.	The paper is well-written and clearly presents the proposed methodology and findings. The structure of the paper is logical, making it easy to follow the complex concepts introduced. The use of figures and tables to illustrate key points is effective and aids in comprehension.

Weaknesses:
1. While the paper provides strong theoretical and experimental results, there is limited discussion on the computational efficiency and scalability of the proposed KFNN algorithm. I suggest moving the algorithmic flow and time complexity analysis from Appendix A to the main text.
2. There are some repetitive sentences and structures in this paper that should be further condensed. For example, Sections 5.1 and 5.2 should be merged and the repetitive statements in them should be deleted.
3. The experiments are already comprehensive, but analysis and discussion of the optimal neighborhood size determined by KFNN could still be added, which would help to understand how the neighborhood size should be set. Moreover, according to the results presented in Tables 1-4, KFNN is generally highly effective. However, on few datasets, KFNN does not perform as well as MV. These anomalies are valuable for identifying deficiencies in KFNN and should be further investigated and discussed.

Limitations:
The authors openly discuss the limitations of their work, particularly the empirical parameters in the Kalman filter and the roughness of the distribution transformation process. Please refer to the Weaknesses for other limitations I have found.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel label integration algorithm, KFNN (K-Free Nearest Neighbor), designed to enhance the performance of crowdsourcing platforms by intelligently determining the optimal neighborhood size for each instance based on its attributes and noisy labels. The authors propose a two-component solution involving label distribution enhancement and K-free optimization, which leverages the Mahalanobis distance and a Kalman filter to mitigate noise from neighbor instances. The paper's claims are well-aligned with the theoretical and experimental results, demonstrating the effectiveness and robustness of KFNN against existing state-of-the-art algorithms in various crowdsourcing scenarios.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.	Novel contribution to an important problem
The innovative approach of highlighting the limitations caused by fixed neighborhood sizes in existing label integration algorithms, and using attributes and noisy labels to determine the neighborhood size for each instance automatically, is a significant contribution to crowdsourcing. 

2.	Complete and rigorous theoretical proof
The theoretical underpinnings are sound, with clear assumptions and proofs provided for the proposed methods. The use of the Mahalanobis distance and the Kalman filter is well-justified.

3.	Good writing quality and clarity
This paper is well-written and enjoyable to read. The challenges are clearly stated and the contributions are easy to capture. 

4.	Reproducibility
The paper's open data and code policy is highly appreciated, promoting research transparency. Enhancing reproducibility with clear versioning and setup instructions would be a valuable addition, showcasing a strong commitment to open scientific practices.

Weaknesses:
1.	Simulation experiment results
The symbol • indicates that the algorithm in the row significantly outperforms the algorithm in the corresponding column. How is ""significantly outperforms"" defined for Macro-F1 score and integration accuracy?

2.	Ablation experiment results
Since this study focuses on automatically adjusting neighborhood sizes, how does the performance of this method compare with baselines that use fixed neighborhood sizes?

Limitations:
see weaknesses

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel label integration approach KFNN by adaptively determining the optimal neighborhood size. KFNN utilizes a Mahalanobis distance distribution to model the relationship between each instance and all classes. The authors also provide adequate theoretical analysis to illustrate the effectiveness of the proposed method. Experiments demonstrate that the proposed method can achieve the state-of-the-art performance on simulation and real-world dataset. The paper is well-written and easy to follow. This idea is very intuitive and effective for crowdsourcing task. The paper proves the effectiveness of introducing Mahalanobis distance distribution for crowdsourcing from the perspective of methodology, theory and experiments.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to follow. The logic of the whole paper is clear.
2. The paper’s idea is very intuitive and effective for crowdsourcing task. The authors introduce the Mahalanobis distance distribution to model the relationship between each instance and all classes. Experiments verify that the proposed method can achieve the best performance compared with SOTAs.
3. The authors provide adequate evidences to verify the effectiveness of the proposed method from the perspective of methodology, theory and experiments on simulation and real-world datasets.

Weaknesses:
1. In section 2, the authors introduce two categories of label integration algorithms. And the proposed KFNN belongs to the algorithms which leverage neighbor instance. I suggest adding some discussion about the pros and cons of these two categories of approaches.
2. In methodology part and theoretical analysis part, the authors discuss the superiority of Mahalanobis distance compared with Euclidean distance. Can the authors verify the difference between Mahalanobis distance and Euclidean distance on this task from an experimental perspective?
3. In Table 3 and Table 4, why some results are missing? Appropriate explanation facilitates reading of the paper.

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new algorithm for label integration called KFNN. Existing methods related to KNN produce more noisy labels; however, they fix the neighborhood size, regardless of the fact that instances close to the center of classes should have more neighbors than instances close to the boundary of classes. To tackle this problem, KFNN estimates a Mahalanobis distance distribution between each instance and all classes to enhance the multiple noisy label distribution and utilizes a Kalman filter to mitigate the impact of noise. Finally, KFNN can automatically determine the optimal neighborhood size through max-margin learning.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1. The paper studies an important problem.   
S2. A new solution is proposed to tackle the problem.   
S3. Experiments are conducted on several datasets.

Weaknesses:
W1. The motivations need more enhancements.   
W2. Some technical details require more explanations.   
W3. The application scope of the proposed method in crowdsourcing is limited.    
W4. The performance improvement of the proposed method is unsatisfactory.    
W5. Experiments are conducted in a simulation environment, which can be much simpler than a real-world crowdsourcing platform.

Limitations:
Please refer to the weaknesses and questions.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
HTLJptF7qM;"REVIEW 
Summary:
The paper addresses the problem of learning with noisy labels by considering a model where instance-dependent confusion matrices occur occasionally across the samples, and the rest of data share a common nominal confusion matrix. The paper claims two main contributions: (1) showing that a single confusion matrix is insufficient to identify outliers and proposing a crowdsourcing strategy with a column sparsity constraint to overcome this, and (2) presenting an end-to-end, one-stage learning loss that is differentiable and easily optimized. These contributions are validated through experiments that demonstrate improved testing accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
● The problem setting, considering the instance-dependent noisy samples as outliers, is realistic and really interesting.
● The paper posits that existing methods relying on sparsity priors are insufficient for outlier detection and proposes a novel crowdsourcing strategy as a solution with theoretical grounding and generalization guarantees.

Weaknesses:
Lack of large-scale datasets in experiments

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the challenge of instance-dependent noisy labels, which are modeled using an instance-dependent confusion matrix reflecting annotator errors. Traditional approaches assume a consistent confusion matrix across instances, which simplifies the problem but is unrealistic. This study models the instance-dependent confusion matrices as outliers. The authors propose using a crowdsourcing strategy with multiple annotators and a specialized loss function to effectively detect outliers and identify the target classifier. The method is supported by extensive theoretical results. Experimental results also confirm the efficacy of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The work aims at an important and the proposed solution is solid.
2. The presentation is clear and the proposed method is supported by thorough theoretical results.
3. Experiments on real-world label noise such as CIFAR-N validate the effectiveness of the proposed method.

Weaknesses:
1. Theorem 3.6 requires $S\rightarrow \infty$. However, we know if we have sufficient annotators, the aggregated noise rate could be 0, making the result trivial. It is interesting to show the result with finite $S$. 
2. The organization part of the proposed method could be polished.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper extends further estimation of transition matrix in label noise learning. In previous studies, label noise is often assumed to be class-dependent. Hence, one can use the noisy label data and apply loss correction to learn a good classifier. The paper extends from such a modelling approach by considering most of samples having class-dependent label noise and some samples having class- and instance-dependent noise. Those with instance-dependent label noise are then considered as outlier in the newly proposed modelling approach. Despite such a modelling, it still results in a non-identifiable solution. To overcome that, the paper incorporates multiple labels from multiple annotators, so that one can solve for the nominal transition matrix and the classifier of interest. The paper also provides theoretical results to guarantee that the estimation holds under certain confidence level. Empirical results show that models trained on the new loss function performs competitively with prior methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper thoroughly presents the motivation and the formulation of their modelling approach. The paper also connects to previous studies when formulating the noisy label learning with instance-dependent noise as outliers. In general, the paper is well-written and easy to follow.
- The main contribution of the paper is to extend further the modelling considering only class-dependent transition matrix by adding *perturbation* induced by instance-dependent label noise samples. The paper also analyses the theoretical guarantee when estimating the classifier of interest in terms of PAC-style formulae.

Weaknesses:
The major weakness of the paper may lie at the assumption that majority of samples would share the same nominal transition matrix, while a few would have their own transition matices. However, this does not affect too much to the contribution of the paper.

Another weakness is the usage of multiple annotators, while the conventional noisy label learning does not consider any additional labels. Of course, some studies (e.g., [43]) show that using a single noisy label would result in non-identifiability, unless additional constraints are exploited. Nevertheless, multiple annotator setting would be closer to current practice. The downside is that there are not many multi-rater learning datasets for benchmarking.

### Minors
- Line 76: typo ""traget"" -> target
- The abbreviation ""NMF"" at line 100 is not defined.
- Line 111: $E(x_{n})$ is a matrix. Thus, please make it clear when specifying $E(x_{n}) > 0$.

Limitations:
As mentioned in the weakness section above and in the conclusion of the paper, the paper only considers a few samples with instance-dependent label noise, while the remaining samples are class-dependent label noise, to reduce the complexity of the modelling and analysis. Such an assumption may not always hold in all settings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studied the identifiability problem of instance-dependent label noise with multiple annotators. To achieve the identifiability, this work first claimed a fact that only a proportion of all instances may have a labeling difficulty that significantly deviates from the general population. Then, it connected the problem to the uniqueness of non-negative matrix factorization with mild assumptions. Inspired by the identifiability result, this work proposed a end-to-end one-stage method to learn from crowds via identifying instance-dependent label noise. Experiments on multiple datasets with machine annotations and human annotators showed the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work provided some interesting identifiability results for learning from crowds, which may inspired further work.
2. The writing is excellent, making it easy to understand.
3. The proposed method is end-to-end and one-stage, which is nice for application.
4. The case study in Figure 2 clearly showed the rationality of the results.

Weaknesses:
1. As this work claimed, the number of annotators is important to identify the instance-dependent label noise, is there some experimental results verifying this analysis？

2.  For human annotations,  the annotations are usually sparse. Does the annotation sparsity level influence the identifiability and the performance of the proposed method? I suggest the authors do some experiments as existing works to clarify it [1-3].

3. The meaning of some terms, like ""outliers"", ""the model of interest"", ""neural systems"", is not clear when just reading the abstract. Besides, since the sparsity prior of the outliers is a basis assumption for this work, it should be referred to in the abstract.

[1] Label correction of crowdsourced noisy annotations with an instance-dependent noise transition model. NeurIPS 2023

[2] Coupled confusion correction: learning from crowds with sparse annotations. AAAI 2024

[3] Trustable co-label learning from multiple noisy annotators. TMM 2023

Limitations:
See above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wl44W8xpc7;"REVIEW 
Summary:
This paper proposes using neural ODEs to parameterize symmetries by viewing the ODEs flow as an element of a one-parameter group. They show that by learning the parameters of the neural ODEs, they are able to recover ground truth symmetries in image classification and PDE tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is easy to read. The proposed ideas are clear, and appear to be mostly novel.

Weaknesses:
See questions below.

Limitations:
N/A.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper pertains to the topic of data-driven symmetry discovery. The authors propose a method allowing symmetry discovery beyond pre-defined Lie groups, by learning to transform datapoints, potentially in a non-affine manner, via a learned ODE (referred to as the *one-parameter group*, where the single parameter is the time variable in the ODE), the velocity field of which is typically parametrised by an MLP. 

Crucially, to optimise this, the authors choose an objective - *validity score*, which is a predefined measure of the extent to which a transformed datapoint is symmetric to the input one (in their examples: for images, they use the cosine similarity between features extracted by a pretrained NN, while for PDEs, they measure the value – error – of the PDE for the transformed datapoint). Additional regularisers are used to ensure the diversity of the symmetries learned (orthogonality between different learned velocity fields) and smoothness (minimisation of an estimate of their local Lipschitz constants). Experimentally, the method is tested on image classification (CIFAR10) and PDE solving (KdV, KS, Burger’s equations) showing that known symmetries are retrieved along with additional approximate symmetries, while the learned symmetries are subsequently used for data augmentation showing competitive results to methods using pre-defined augmentations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
**Significance** . The paper studies an important problem (*data-driven symmetry discovery*) in machine learning, but also physical sciences where symmetries are abundant but potentially unknown. Identifying unknown symmetries and incorporating them in downstream ML models (e.g. via augmentation) can improve generalisation, especially in low-data regimes, while additionally, it can potentially provide novel insights about the task at hand.

**Novelty/Generality**
- The presented methodology has the capacity to recover symmetries arising from *non-affine* data transformations. This is contrary to prior work, where mostly linear/affine transformations are dealt with.
- Additionally, this method does not require making assumptions about the structure of the target group. This is common in prior art, where typically a subgroup of a predefined group is learnt.
- The authors take advantage of well-established concepts that are underexplored by the ML community (e.g. modelling transformations via the one-parameter group) - this helps to broaden the available toolbox in the field of ML & symmetries/ equivariant ML.

**Execution/Implementation**
- Although the proposed method has multiple complicated components (NeuralODEs, difficult objective to optimise for), it is nonetheless well-executed yielding competitive results and recovering known symmetries in popular testbeds.

Weaknesses:
**Applicability and scope**. Perhaps the biggest limitation of the proposed method is the *reliance on the validity score*. Although the authors claim to be able to learn symmetries by making as few assumptions as possible (see strengths), this seems to be contradicted by the need to manually design a validity score. Moreover, I have the impression that the validity score is not merely a hyperparameter, but it is decisive for the symmetries that will be learnt (basically it is the objective function of the optimisation problem). 
-  For example, in the case of images, the choice seems ad hoc (closeness in the representation space of a pre-trained encoder NN). What leads the authors to believe that the features of equivalent (symmetric) images extracted from the pre-trained NN should be close? Have the authors tried to verify this assumption? I think the empirical validation is insufficient here (section 5.1.), so I am not entirely convinced.
- In general, I do not see a generic way to define validity scores and perhaps the authors have slightly overclaimed in that respect. I would like to read the authors' viewpoints on that. For PDEs, the validity scores are indeed reasonable and generic, so perhaps, they would like to put more emphasis on this perspective.

Furthermore, the authors introduce the concept of learning symmetries via the one-parameter group, claiming that it is more general than prior parametrisations that can only learn linear/affine groups. However, it is unclear what the present parameterisation can express, e.g. does it allow learning any continuous group or implicit assumptions are made here as well? 
- Additionally, could the authors discuss if it would be possible to learn finite groups with this method as well and if not, how could those be incorporated?


**Related Work/Comparisons**. The work of Forestano et al., MLST’2023 is quite relevant to the present manuscript, with the main difference being that in that work, the transformations are parameterised by an MLP instead of a NeuralODE (the oracle used in this work seems similar to the validity score used here). Since the two works have many similarities, I think that the authors should discuss in more detail their differences and the advantages of their work (e.g. as far as I understand the MLP cannot guarantee that the transformations form a group). Note that modelling transformation via an MLP (or any NN in general) instead of a NeuralODE seems more straightforward and probably easier to train and more computationally friendly.

**Experiments**. I believe some additional empirical evidence would strengthen the authors' claims.
- Most importantly, an experimental comparison against the type of parameterisation used in Forestano et al. (MLP) should be provided, to verify if NeuralODEs are indeed a more appropriate parameterisation.
- Moreover, baselines are mostly missing, e.g. comparing against other methods for data-driven symmetry discovery (I am not super familiar with these works, but if I am not mistaken LieGAN by Yang et al., ICML’23 is a recent example). 
- The reported results after augmenting with the learned symmetries do not seem to improve significantly compared to known/default augmentations. Can the authors discuss why this might be the case? This is important since it might undermine the necessity of the proposed approach.  To be more convincing, perhaps the authors should perform experiments on problems where the symmetries are not known a priori.
- Additionally, ablation studies seem to provide important insights but are only discussed in the appendix. I would recommend being more upfront in the main paper and discussing in the rebuttal the following: sensitivity to hyperparameters (multiple are needed: loss coefficients, $\sigma$ and $\tau$), the method for choosing them, the difficulty of optimisation  (3 terms are used in the objective) and if all losses are optimised in a balanced manner. Similarly, for the parameter $N_sym$, which is now chosen based on prior knowledge of the number of existing symmetries.

**Presentation/Exposition**. (disclaimer - this is a minor weakness) Given that the notions discussed here are probably not widely known across the ML community, I believe that the authors should aim to provide more in-depth explanations to make their work more accessible. For example,
- Multiple group theory/symmetry concepts are discussed without definitions (group generators, Lie group, Lie algebra, Lie bracket etc.). Additional examples that are not well-contextualised include in section 2 the one-parameter group discussion, the Lie algebra of the affine group and the discussion on the PDE symmetries (Lie point symmetries etc.). Adding some references here and providing some examples for the mentioned PDE symmetries would help.
- In section 5.2., some concepts regarding the experimental details are mentioned without appropriate explanations, while others are only mentioned in the appendix, although it appears that they are crucial for the method. Perhaps the authors should be more upfront and explanatory regarding the aforementioned.

Limitations:
Some of the limitations are adequately discussed. An important missing point is, in my opinion, the need to manually design the validity score in domains beyond PDEs.

No foreseeable negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a symmetry learning algorithm based on transformations defined via infinitesimal generators. Using Neural ODE, an infinitesimal generator is learned that is capable of producing a sequence of transformed data through ODE integration. Validity score has been defined to check if the transformed data is valid wrt to a given task. For images, the validity score is picked to be cosine similarity while for PDEs the validity score is defined as the numerical errors in the original equations after the transformation. In addition to symmetry loss, two regularizations, orthonormality loss, and Lipschitz loss have been added, to remove trivial solutions. The authors present experiments on CIFAR10 and KdV equation and Burgers' equation in 1D for PDE.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper motivates the need for learning continuous symmetries well. 
2. The idea presented in the paper is very neat and shows potential beyond the presented experiments. 
3. This approach can learn both affine and non-affine symmetries in image classification and PDE tasks as shown in the experiments section.

Weaknesses:
The discussion on compute and model parameters comparisons with baseline missing. No other method was shown as a baseline in either of the experiments. There is some ambiguity in how exactly the validity score is used and in some cases, can be defined if the given task is equivariant.

Limitations:
1. The authors do not compare with other methods of learning symmetry, as those methods use datasets that have explicit symmetry (like Rot-MNIST). Similarly for that of the PDE experiments. It would have been useful to have them in the paper.
2. The number of parameters and compute comparison across the proposed method and baseline approach like [9,14] is missing.  Also discussion the scale of numerical computation with an increase in dimension missing.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wkwGedn19x;"REVIEW 
Summary:
This paper introduces CRATE-α, an enhanced variant of the CRATE (Coding RATE Transformer) architecture, designed to scale efficiently while maintaining mathematical interpretability. The authors address the open question of CRATE's scalability by proposing strategic modifications to the sparse coding block and a refined training recipe. Extensive experiments demonstrate CRATE-α's effectiveness, showcasing improved performance on ImageNet classification tasks compared to the original CRATE model. Notably, the CRATE-α-B model achieved an 83.2% accuracy rate, a significant improvement over the previous best CRATE-B model.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents a novel architecture, CRATE-α, that builds upon the existing CRATE model with minimal yet strategic modifications, enhancing scalability without compromising interpretability.

The authors provide a wealth of empirical evidence supporting the effectiveness of CRATE-α, including comparative results on ImageNet classification tasks and a detailed analysis of training behaviors across different model scales.

A key strength is the paper's focus on maintaining the interpretability of the model, which is often a trade-off in scaling deep learning models. The authors demonstrate that CRATE-α models retain high-quality unsupervised object segmentation capabilities.

The paper includes a thorough exploration of scaling behaviors, from Base to Large to Huge model sizes, using both supervised learning on ImageNet and vision-language pre-training with contrastive learning on DataComp1B.

Weaknesses:
Could the proposed architecture work well on other tasks like NLP?


While the paper provides a detailed analysis of the model's performance on ImageNet, there might be a need for more discussion on how these results generalize to other datasets and real-world applications.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores how to train white-box Transformers at scale for visual tasks. The authors propose a new model architecture called CRATE-$\alpha$, which extends the sparse coding block of the original CRATE model. A series of CRATE-$\alpha$ models were trained with varying model sizes, data sizes, and patch sizes using optimized training recipes. The main experiments focus on supervised classification and contrastive CLIP learning, with additional demonstrations of unsupervised semantic segmentation capability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Originality:** The paper continues the white-box design philosophy of the original CRATE model while integrating advanced techniques such as overparameterized sparse coding, decoupled dictionary, and residual connections. Although some of these techniques have been previously validated, successfully combining them with a white-box Transformer is a noteworthy achievement. The integration not only works effectively but also yields commendable results.

**Quality:** The paper is technically sound overall, employing rigorous notation and formula definitions to elucidate the design principles. The proposed models demonstrate significant improvements compared to the previous generation of CRATE models. Additionally, the authors are careful and honest in evaluating the weaknesses and limitations of their work.

Weaknesses:
**Clarity:**
- The paper is heavily symbolized, relying extensively on intricate mathematical formulations rather than clear diagrams and straightforward language. Although this maintains academic rigor and professionalism, it severely hampers understanding of the paper's details and the broader dissemination of the model. Incorporating corresponding illustrations to explain the three modifications and comparing them with the standard Transformer structure would be beneficial.
- The organization of Section 4 is not concise, making it easy for readers to lose track.
  - The distinction between the paragraphs ""Dataset and Evaluation"" and ""Training & Fine-tuning"" is not well-defined, especially with the scattered descriptions of the data used.
  - The frequent interleaving of experimental setup descriptions with the presentation of experimental results disrupts the flow and coherence of the narrative.

**Significance:** 
- Although CRATE-$\alpha$ shows significant improvements over the original CRATE model, it still lags behind the state-of-the-art. For example, in the right side of Figure 1, CRATE-$\alpha$ typically requires nearly double the training FLOPs to achieve the same accuracy as ViT. 
- If the scalability and interpretability of a white-box Transformer architecture does not offer substantial insights and improvements, practitioners might prefer models with stronger performance but lower interpretability.

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the scalability problem of white-box transformer CRATE and proposes CRATE-$\alpha$ to enhance the scaling ability of CRATE. To be specific, the authors propose three strategic but minimal modifications for the CRATE model architecture: Overparameterized sparse coding block, Decoupled dictionary, and Residual connection. Extensive experiments across different datasets and settings demonstrate the effectiveness of the proposed approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. It is quite meaningful to study white-box transformers and try to increase their scalability which promises its application in potential usage.

2. Comprehensive evaluation. The proposed method is validated on multiple datasets and tasks which demonstrate the scalability of CRATE-$\alpha$.

3. The presentation is clear. Overall, the paper is well-organized and the method is easy to follow.

Weaknesses:
1. Performance gaps with vanilla ViT. As shown in Figure 1, CRATE-$\alpha$ still lags behind vanilla ViT across different scales remarkably which may limit its application in real scenarios. Besides, it is suggested to compare with vanilla ViT in computational costs, number of parameters, and inference speed as well.

2. According to the model configuration, the number of parameters of CRATE-$\alpha$ is almost four times as CRATE and it is strange to consider those as the same scale models. Moreover, how do the proposed new modules contribute to the performance gain of CRATE-$\alpha$? Is it simply because of larger models?

3. Although the authors made lots of efforts in scaling CRATE to CRATE-$\alpha$, they only spent limited space in the paper to discuss the interpretability of the proposed method. This short paragraph may not be enough to justify why the authors are motivated to study the white-box transformers.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to train CRATE at a large scale for vision tasks. The contribution includes an architecture modification to the sparse coding block and a light training recipe. The new model, called CRATE-alpha, shows large improvements compared with the previous CRATE model. The experiments also show promising results on unsupervised object segmentation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a careful study to enhance the performance of CRATE. The paper introduces key modifications to the existing CRATE, including the sparse coding block, decoupled dictionary, and residual connection. 
- The paper investigates its scaling behavior and shows promising improvements of the newly introduced CRATE-alpha.
- The paper presents in-depth experiments, such as the scaling analysis on ImageNet. The paper also shows improvements for semantic interpretability. 
- The figures and model architecture are well-illustrated.

Weaknesses:
Overall I find the paper is well-presented and solid. Below are my minor concerns for this paper:
- The paper is highly centered on improving CRATE. Most of the findings might not be transferable to other models. This may limit its impact to the general audience in NuerIPS community.
- It would be interesting to further understand its potential downstream applications (not only vision but also language data)

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wjbTHLUSzU;"REVIEW 
Summary:
This paper proposes a method for data selection in foundation model fine-tuning. The proposal contains a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution, a regularizer to encourage the diversity of the selected data, and kernel density estimation to reduce the negative effects of near-duplicates among the candidate data. Experimental on fine-tuning the language model are reported.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposal studied in this paper is interesting since how to select data, and how to improve the data quality is important for the training and fine-tuning of the foundation model.
2. The proposed method which considers distribution discrepancy minimization, diversity, and near-duplicates, is technically sound.

Weaknesses:
1. The novelty and contribution of the proposed method are limited. Data selection is important and well-studied in the machine learning community. For example, in active learning, we need to select examples to label according to some metrics; in domain adaptation, we need to select data to help the model reuse. Some widely adopted methods can be applied to the problem of data selection for the foundation model and the authors didn't provide a comprehensive study and comparison. Moreover, the techniques adopted in the proposal are also widely used techniques. 

2. For the experiments, the authors only conduct experiments on the language model, can the proposal be applied to other foundation models, such as the vision-language model?

3. It seems that the random selection method can also achieve good performance. So I am wondering about the difficulty of the problem,  maybe we can improve the performance using some trivial techniques.

4. The time cost between fine-tuning with the full dataset and the selected dataset should be reported.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper formulates data selection for task-specific fine-tuning as an optimization problem based on optimal transport for distribution alignment. It proposes two KNN-based implementation methods and evaluates them on datasets for task-specific instruction fine-tuning and domain-specific continued pretraining. The experimental results demonstrate that their methods are more effective than the baseline systems (LESS and DSIR).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper formulates data selection as an optimal transport problem, providing a detailed problem definition and a closed-form solution. Additionally, it proposes using Kernel Density Estimation to address the issue of near-duplicates.

2. The authors introduce KNN-Uniform and KNN-KDE algorithms for data selection, showing that their performance is superior to the baseline systems in both task-specific instruction fine-tuning and domain-specific continued pretraining experimental setups.

Weaknesses:
**Regarding the methodology:**

1. The connection between data selection and the optimal transport problem is not clearly established. Despite mentioning it in lines 114-115 of Section 3, it remains unclear why data selection can be considered an optimal transport problem.

2. Much of the paper is based on LESS, including the representation of samples and the task definition. However, there is minimal mention of LESS, making it challenging to understand without prior knowledge of LESS.

3. The method still relies on M query samples for a specific task, which poses certain limitations.

**Regarding the experimental section:**

4. The experimental section contains too many specific settings. For instance, special settings mentioned in lines 248 and 286 make it difficult to determine how these parameters were chosen, even after reviewing the appendix.

5. The two experimental setups are inconsistent in task-specific instruction fine-tuning and domain-specific continued pretraining. In Table 2, the Ratio of 0.5%-5% can be understood as the number of data samples selected. However, in Table 4, the 1K, 3K, and 10K seem to refer to the number of query samples, but there is no comparison of the number of selected samples.

6. There is a lack of comparison with various baseline systems. Only one baseline system is used for comparison, and its performance differs from that reported in the original paper.

Limitations:
They acknowledge one limitation in conclusion: the method still relies on M query samples for a specific task. It is also raised in my comments.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a method for data selection for task-specific model finetuning. The method relies on a small, representative sample of data from the target task to select matching, relevant data from a corresponding corpus. The method relies on framing this task as an optimization problem, utilizing an optimal-transport-based distribution alignment loss and a KDE-based regularizer to avoid oversampling (near-)duplicates. 
The authors show this method to be highly scalable and data efficient, being competitive with, and often outperforming state-of-the-art methods for domain-specific continued pretraining and task-specific instruction tuning.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper rigorously presents and tests the proposed method, with a detailed theoretical motivation. 
- Sections 2-4 are well structured and introduce the method in a clear, progressive way. 
- Performance results, especially for very small sample sizes, strongly support the utility of this method.

Weaknesses:
- Section 5.1: Given how different some of the performances are between llama and mistral, including other LLMs may give a more complete picture of the efficacy of this method.
- Section 5: Efficiency claims would benefit from context. How does the 28 hour initialization time compare to other SOTA methods on this dataset? How does it scale after initialization is done when repeatedly drawing task-specific samples compared to other methods?

Limitations:
The authors address limitations.

Rating:
8: accept, good paper

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes task-specific training data selection for language model fine-tuning. Given a (small) set of representative examples for a task and a large set $D$ of possible training examples, the proposed method uses (regularized) optimal transport to assign a probability distribution over $D$ that matches the distribution of representative examples while also encouraging diversity among the elements of $D$ assigned a nonzero probability.
The authors prove that with a certain choice of regularization function, this is equivalent to (an adaptive version of) $k$-nearest neighbor selection of candidate data similar to the representative examples. Since $k$NN treats near-duplicates as distinct examples (which would decrease diversity of the selected data), the paper additionally introduces another regularization term based on kernel density estimation; the optimal transport with this regularization is a weighted $k$NN that intuitively accounts for the frequency of near-duplicates for each example.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Good data selection is an important problem given that today's models are both expensive to fine-tune and very sample-efficient *if* they are given the ""correct"" high-quality fine-tuning data [1]. Most high-performing efforts still tweak the composition of these small task-specific datasets by hand. This paper has an interesting new take on framing task-specific data selection as an optimal transport problem between representative task examples and a large pool of candidate training data.

- Theorems 3.1 and 3.2 shows that with certain regularization terms, the optimal transport selection procedure is equivalent to certain variations of $k$-nearest-neighbor. This allows for efficient computation of the optimal data selection under this objective.

- The proposed approach can naturally be combined with approximate nearest neighbor search methods for efficiency.

- Strong empirical results showing that the proposed selection procedure can even outperform tuning with the full candidate dataset.

- The experiments include standard deviations across three runs, giving a sense of how big the gains are compared to noise.

[1] Zhou, Chunting, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma et al. ""LIMA: less is more for alignment."" In Proceedings of the 37th International Conference on Neural Information Processing Systems, pp. 55006-55021. 2023.

Weaknesses:
- Missing an ablation using embeddings instead of gradients, or any other distance function for the examples.

- Missing several relevant data selection baselines that also encourage diversity, e.g. Deita [1], QDIT [2], and methods based on DPPs [3].

- Changing the data mix changes the optimal learning rate (e.g., since it changes the scale of the loss function at initialization). The paper compares models trained on different data mixes with the same learning rate, but the fair comparison is optimal : optimal. It's not clear from the experiments whether the reported gains are due to the learning rate being more optimal for the selected data mix, especially since the metric used to select the data is based on the gradients of a model.

[1] Liu, W., Zeng, W., He, K., Jiang, Y., & He, J. What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning. In The Twelfth International Conference on Learning Representations.

[2] Bukharin, Alexander, and Tuo Zhao. ""Data diversity matters for robust instruction tuning."" arXiv preprint arXiv:2311.14736 (2023).

[3] Wang, P., Shen, Y., Guo, Z., Stallone, M., Kim, Y., Golland, P., & Panda, R. (2024). Diversity Measurement and Subset Selection for Instruction Tuning Datasets. arXiv preprint arXiv:2402.02318.

Limitations:
The paper includes a reasonable discussion of its limitations, but it could be improved by discussing some of the additional limitations with the empirical results mentioned above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wiMaws0FWB;"REVIEW 
Summary:
In this paper, authors study the implicit bias of the mirror descent algorithm, from the perspective of the optimization trajectory of the continuous flow version. They propose the conceptions of horizon shape and horizon function $\phi_\infty$ to help characterize the properties of mirror flow at infinity. Since $\phi_\infty$ defines a norm, they prove that the mirror flow will eventually converge in direction to a max $\phi_\infty$-margin solution under the linear exponential-tail classification setting. Their findings contribute to a deeper understanding of the inherent characteristics of mirror descent across a wide range of potential functions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The result of this work is solid, containing the general class of potential functions, and the authors derive a calculation rule of $\phi_\infty$ for a general class of potential functions.
2. The paper is informative and well-structured, particularly in section 3. By using the example of gradient descent within the framework established by the preceding lemmas, which is a special case of mirror descent, the authors clearly outline the reasons why mirror flow will eventually converge in direction without complicated formulas.

Weaknesses:
1. Since mirror descent is not so popular in the practice of machine learning problems,  there could be more discussion about the implications of their results. For example, Figure 1 is really interesting as it reveals that the mirror descent shares the same structure of implicit bias with the steepest descent [1], what is the essence of such similarities?
2. The setting of an infinitely small learning rate, i.e., optimization flow,  might be a little strong under a simple linear classification problem compared to the previous works. I suggest the authors state the technical challenges of the discrete optimization process of mirror descent.
3. I might be wrong, but it seems not strict to apply the Bolzano–Weierstrass theorem to an uncountably infinite set at page 5, line 160 and 61.

[1] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832–1841. PMLR, 2018.

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper examines the implicit bias of mirror flow (the continuous-time counterpart of mirror descent) in the context of binary classification for linearly separable data. Given that the problem has infinitely many solutions, obtained at infinity, the authors aim to identify which solution is achieved through mirror flow. Assuming an exponential tail on the loss function, the authors demonstrate that mirror flow converges directionally to a maximum margin classifier, where the margin is characterized by a horizon function of the mirror potential. This result extends many existing works, and numerical experiments are provided to verify the findings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and well-organized. Despite the technical nature of the analysis and results, the paper is relatively easy to follow.
2. The paper is also well-motivated. Although mirror descent is not commonly used as an algorithm for training neural networks, analyzing its convergence is valuable for understanding the implicit bias of gradient descent in various neural network architectures.
3. I did not verify the details of the proof. However, the paper provides several motivating examples, including the quadratic potential corresponding to gradient flow, which makes the results quite convincing.
4. The main results extend several prior works.

Weaknesses:
Although the authors have stated that the convergence rate is left for future study, it would be beneficial to provide at least empirical evidence of the convergence rate. The authors mentioned in line 294 that the convergence rate varies across different potentials.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This manuscript examines the implicit bias of mirror descent on a classification problem when the dataset is linearly separable. Assuming a coercive gradient, it demonstrates that the implicit bias is characterized by the shape of the level set of the mirror potential near infinity. Their analysis successfully recovers existing results for p-norm potentials and identifies the implicit bias of the potentials emerging in the analysis of linear neural networks. Additionally, it leaves the characterization of the implicit bias when the gradient is not coercive as an interesting open problem.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
I think the paper is very well-written and has a solid contribution.

It addresses an important problem, aiming to understand the implicit bias of neural networks. Prior work has shown that the dynamics of linear networks can be characterized by mirror descent, highlighting the relevance of this study.

Weaknesses:
NA

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers the asymptotic behaviour of the mirror descent (MD) algorithm for a linear classification task. It is shown that the classifier (hyperplane orthogonal to $\beta$) will be a max-margin classifer, where the margin is determined by some unknown horizon function $\phi_\infty$. This works extend prior work which consider $\ell_p$ and homogeneous potential functions for MD, and shows this result for very general $\phi$.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper makes an interesting statement about behaviour of mirror descent on classification tasks, will minimal assumptions. In doing so, it takes a big step and extends previous work to the cover general potential functions.
The paper is well written and the figures help with understanding the concepts of convergence.

---
*While I could understand the paper, this is not my area of research. I do not find myself fit to evaluate the paper on soundness, relevance to the sub-field and importance of contributions.*

Weaknesses:
- The paper does not characterize $\phi_\infty$ in terms of the bregman potential $\phi$ (and other relevant entities). 
The main result expresses that there exists some function, that is minimized by $\bar \beta_\infty$, the direction of the classifier as $t\rightarrow \infty$. 
I think this limits the relevance and strength of the result. For instance, this does not help with interpretability compared to the case where we can prove the optimization algorithm converging to a max-margin classifier wrt the $\ell_2$ norm.

- I am not sure about relevance and use-cases of the mirror descent algorithm with very general potentials. As far as I know, typically, a small set of norm-based or entropic (neg-ent, tsallis, etc) are used within applications of ML. So while the theorem makes an interesting statement for an optimization standpoint, I'm not sure how relevant it is for the ML community. The theorem is also not entirely relevant to the pure optimization community since it's for the specific case of linear classification with finite data.
---
*While I could understand the paper, this is not my area of research. I do not find myself fit to evaluate the paper on soundness, relevance to the sub-field and importance of contributions.*

Limitations:
The limitations are discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wiK6bwuxjE;"REVIEW 
Summary:
This paper proposes a monocular 3D detection framework inspired by Masked Autoencoders (MAE), designed to address the challenge of object occlusions in 3D object detection. It utilizes a unique depth-aware masking module that simulates occlusions by adaptively masking non-occluded object features based on depth information, coupled with a lightweight completion network that reconstructs these masked features to learn occlusion-tolerant representations. It generates training pairs of non-occluded and occluded object representations directly, enhancing its capability to handle occlusions effectively. The framework is optimized for low computational overhead during inference, as it does not require object masking at this stage.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method outperforms the conventional methods across various datasets such as KITTI and Nuscenes. It demonstrates the effectiveness of the proposed method. Moreover, the proposed method achieves real-time inference time.
2. An extensive ablation study is proven to demonstrate the impact of the proposed module. 
3. The idea is simple yet effective.

Weaknesses:
1. The performance improvement is marginal, especially on the cross-validation in Table 6. 
2. Missing evaluation on the Waymo dataset

Limitations:
See the weakness and question parts.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces a novel framework for improving monocular 3D object detection, particularly in handling object occlusions. The proposed MonoMAE leverages depth-aware masking to simulate occlusions in the feature space and employs a lightweight completion network to reconstruct occluded object regions, thereby learning occlusion-tolerant representations. Experiments show that this learning stratgy helps to improve the performance of monocular 3D object detection.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper is well-structured, with a clear problem statement, methodology, experiments, and ablation studies that substantiate the contributions and effectiveness of MonoMAE.
2. This paper addresses a significant challenge in monocular 3D object detection, object occlusion, with a novel approach using depth-aware masked autoencoders.

Weaknesses:
1.  The reliance on depth-aware masking to simulate occlusions may not perfectly replicate natural occlusion patterns, potentially affecting the model's reconstruction accuracy. The gap between synthetically masked and naturally occluded object queries could limit the model's robustness in real-world scenarios.
2. While this paper claims generalizability, the lack of extensive cross-dataset validation leaves the true scope of its generalization capability somewhat unproven.

Limitations:
1. The paper could provide a more detailed analysis of the computational efficiency, including speed and resource usage, to fully assess the practicality of MonoMAE for real-time applications.
2. This paper only present results of vehicle detection. The performance of detecting objects with small sizes is unknown.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper applies Masked Autoencoder to 3D object detection. It distinguishes object queries into occluded and non-occluded categories, and during training, it applies depth-aware masking to the non-occluded queries and learns by completing them. At test time, the completion is applied to the occluded queries.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It achieved state-of-the-art performance on the KITTI 3D dataset.
- The idea of interpreting occluded queries as masked queries to solve the problem is interesting.
- The training and test times are illustrated clearly in figures.

Weaknesses:
- As stated in the limitations section, occlusion at the image level and masking at the feature level of object queries are not the same. Further analysis is needed to understand the actual implications of masking in object queries.
- If masking serves the role of occlusion at the image level, there should be no reason for the mask ratio to vary with depth, yet depth-aware masking is highly beneficial. An analysis is needed to understand why depth-aware masking works well compared to random masking.
- In my opinion, the performance of the Non-Occluded Query Grouping classification is crucial for the framework to function properly. Although classification accuracy is provided in the supplementary material, it would be helpful to include various metrics such as precision, recall, and F1-score. If the results of the Non-Occluded Query Grouping classification are biased, it might be interesting to apply completion not only to the occluded queries but also to the non-occluded queries at test time.

Limitations:
Limitations are included in the main text.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces MonoMAE, a novel monocular 3D object detection framework designed to improve detection performance in the presence of object occlusions. MonoMAE leverages the concept of Masked Autoencoders, treating object occlusions as natural masking and training the network to complete occluded regions. This innovative approach addresses the pervasive issue of object occlusions in monocular 3D detection, leading to superior detection performance. Extensive experiments on datasets like KITTI 3D and nuScenes show that MonoMAE outperforms state-of-the-art methods in both qualitative and quantitative measures.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The introduction of depth-aware masking to simulate occlusions and the use of a lightweight query completion network are innovative and address a significant challenge in monocular 3D detection.
2. MonoMAE improves detection performance without the need for additional training data or annotations, making it a practical solution for real-world applications like autonomous driving and robotics.
3. The framework demonstrates superior performance on benchmark datasets (KITTI 3D and nuScenes), outperforming existing state-of-the-art methods in both occluded and non-occluded scenarios.
4. MonoMAE shows strong generalization capabilities to new domains, which is critical for deploying models in diverse environments.

Weaknesses:
1. In many datasets and methods, objects are not merely labeled as ""occluded"" or ""non-occluded."" Instead, they may be assigned occlusion levels or degrees that quantify the extent to which an object is occluded. These levels provide more granularity and can influence how models are trained and evaluated. It would be beneficial to specify how occlusion levels are defined and used. Clarifying whether discrete or continuous levels are employed and how these influence the labeling, training, and evaluation processes will provide a clearer understanding of the methodology and its robustness in handling occlusions.
2. The paper does not provide explicit details about the accuracy of the occlusion classification network or how this accuracy influences the overall 3D object detection network. This information appears to be missing.
3. The paper does not explicitly report the performance or accuracy of the query completion network. Including a report on the performance of this network, such as quantitative results or visualization of the reconstructed queries, would be valuable. It would demonstrate whether the query completion network is learning meaningful features and contributing effectively to the overall 3D object detection performance.

Limitations:
The authors discussed some failure cases in their paper, as well as the gap between the generated occlusion pattern and natural occlusion patterns.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wiEHZSV15I;"REVIEW 
Summary:
This paper proposes  a Selective Structured Components-based Neural Network for Long-term Time Series Forecasting

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper demonstrates originality by addressing a crucial limitation in existing SOTA methods, maintains high quality through thorough experimentation and clear presentation, offers significant advancements to the field of time series forecasting, and ensures clarity that aids in understanding and reproduction of the work.

2. The motivation of this paper is intuitive and compelling. Given the large model sizes of current SOTA methods like PatchTST, the idea of using a smaller model to achieve comparable or better performance is highly attractive. 

3. The experiments are thorough, and the proposed method achieves state-of-the-art performance. The effectiveness of each sub-module is demonstrated through detailed ablation studies. 

4. The code is open source and reproducible, with a straightforward and clear usage process.

Weaknesses:
1. In the experiment section, it is noted that most papers use the ETT dataset for ablation studies, likely due to its smaller size, which allows for quicker results. However, you chose the ECL and Traffic datasets instead of ETT, which is a more comprehensive and reliable approach. While this choice is commendable, there is no explanation provided for not using the ETT dataset. 

2.It would be more informative to report the model size directly in Table 1. Including the model size would provide a clearer comparison with other SOTA methods and highlight the efficiency of your proposed model. 

3.Baselines: Some MTSF models based on LLM have been widely applied [1]. If the authors can demonstrate that SSCNN has advantages in both performance and efficiency, this paper will be more convincing.

4.Some extremely lightweight models have also been proven to have satisfactory performance [2] . Compared to these methods, what are the main advantages of SSCNN? 

[1]One Fits All:Power General Time Series Analysis by Pretrained LM

[2]FITS: Modeling Time Series with 10k Parameters

Limitations:
No problem

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper identifies data decomposition as a core bottleneck in time series forecasting and proposes a novel model named SSCNN, a decomposition-based model innovatively enhanced with a selection mechanism. SSCNN is specifically designed to adeptly capture complex regularities in data while maintaining a minimal parameter scale. This paper also provides an in-depth comparison between decomposition and patching, examining both capability and parsimony.  Comprehensive experiments show the superior performance of SSCNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strong Points:

1. The insight of this paper is attractive and compelling. One of the most crucial characteristics of time series is that they can be viewed as composed of components with different natures, e.g., season, trend, and residual. However, this characteristic has been rarely utilized in related works, or it has been implemented in trivial ways. This paper identifies data decomposition as a core bottleneck in time series forecasting and proves its effectiveness. By decomposing complex data into more learnable components, SSCNN achieves state-of-the-art performance with a minimal number of parameters.

2. The writing of this paper is very clear. I can easily follow the author's logic and understand their points.

3. The experimental results are extensive, including overall performance results, ablation studies of each component, hyperparameter experiments, etc., which validate the effectiveness of SSCNN.

4. The code is reproducible and well documented. I have successfully replicated the authors' results.

5. The authors also provide an in-depth comparative analysis and experimental results between patching and decomposition, which help readers understand the advantages of SSCNN’s insights.

This paper emphasizes the importance of decomposition in long-term time series forecasting, addressing the analytical gap in feature decomposition for the first time and providing insights into its rationale for capability and parsimony compared to patching.

Weaknesses:
I have some minor questions and suggestions. If the author addresses the following points, I will increase my score.

Weak Points:

Experimental Setting: Most works using the Time-Series-Library repository predict up to 720 steps, yet your results do not include this prediction horizon. It would be beneficial to explain why 720-step predictions were not included.

Figures: I suggest the authors add more explanatory information to Figure 1 to help readers grasp the main architecture of SSCNN from the figure and its caption alone. Moreover, some font styles (italic) in Figure 1 seem different from the character styles in the main text. I recommend unifying the styles.

Minor Issues: The operator $\lfloor \cdot \rfloor$ is used in the paper but not explained. In Figure 3(a), if I understand correctly, “HDformer” should be replaced by “SSCNN.”

Figures: The text size of the legends in the figures is too small, making them difficult to read. Adjusting the text size to be consistent with the main text would enhance the readability of the figures and improve the overall presentation quality of the paper.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses long-term time series forecasting and critiques the reliance on complex models with extensive parameters. It proposes a decomposition method specifically designed for time series dynamics, achieving better forecasting performance across various datasets. Remarkably, the new model uses over 99% fewer parameters than other methods, highlighting the efficiency of domain-specific approaches. This research calls for a move away from complexity in LTSF, showcasing the effectiveness of focused decomposition techniques rather than relying on large-scale models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is praiseworthy for its intuitive approach. It tackles a significant problem by proposing a method that matches or surpasses current state-of-the-art models like PatchTST while using a smaller model footprint. The experimental results strongly validate this approach.

2.	The model consistently performs well under various experimental conditions, including different input window sizes and hyperparameter settings. Statistical tests demonstrate the reliability of the results across multiple initializations, strengthening the study's credibility.

3.	The authors provide a thorough comparison between decomposition and patching in terms of effectiveness and simplicity, demonstrating the superior benefits of decomposition over patching.

Weaknesses:
1.	The clarity of the methodology could be improved with further elaboration.
2.  The evaluation could be strengthened by including comparisons with LLM-based models, such as:

 [1] Jin, Ming, et al. ""Time-LLM: Time Series Forecasting by Reprogramming Large Language Models."" The Twelfth International Conference on Learning Representations.

[2] Bian, Yuxuan, et al. ""Multi-patch prediction: Adapting llms for time series representation learning."" arXiv preprint arXiv:2402.04852 (2024).

Limitations:
Whether this model can be applied to other types of time series data, e.g. trajectory.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This study unveils a groundbreaking approach to time series forecasting, notable for its minimal parameter count. It stands as the first model to consistently outperform state-of-the-art (SOTA) techniques while remaining compact. Unlike prevalent methods such as PatchTST and iTransformer, which are powerful but cumbersome, and emerging methods like TimeMixer and SCNN, which are lightweight yet inadequate for complex tasks, this model achieves superior performance without the associated heft.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The model consistently delivers superior accuracy compared to state-of-the-art (SOTA) methods while maintaining a minimal model size. This accomplishment distinguishes it from other methods.

2. The framework unifies the ability to capture various patterns in time series data, offering a streamlined and enhanced alternative to existing models built with MLPs or Transformers.

3. The authors conduct extensive experiments, showcasing the model's strong performance compared to selected SOTA models, which are sufficiently representative of the latest advancements in the field.

Weaknesses:
1. There is a gap between the introduction and Section 3 regarding the decomposition of the time series into four components. The authors do not explain why these four components are sufficient. For longer sequences, is there a need for more components? Are there references that support this approach? This discussion should be included at the beginning of Section 3.

2. Manually disabling the spatial component for certain datasets appears suboptimal. It would be more effective if the algorithm could automatically determine whether including the spatial component is beneficial for each dataset.

3. The paper's formatting needs improvements. It seems the authors may have additional content to include. Although the figures in the methodology section are clear and informative, resizing and rearranging them could provide more space for adding valuable content to the main text.

Limitations:
The authors have raised the limitation of the model concerning computational efficiency, along with potential solutions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Title: Parsimony or Capability? Decomposition delivers both in long term time series forecasting.

Long term time series forecasting has been an important research problem which applies to different problem domains. This paper proposes a decomposition method which shows significant performance on the benchmarks with less parameters. This method been evaluated extensively on the various datasets and been competitive to existing models. With such approach models can be enhanced to adapt domain characteristics more effectively in various time series applications.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. SSCNN reduces the parameter count substantially compared to traditional models, holding onto less than 1% of the parameters while still performing well across different datasets.
2. The model captures complex data patterns effectively using fewer parameters, utilizing a structured component-based approach with a selection mechanism to improve prediction accuracy.
3. SSCNN excels in time series forecasting, managing diverse data types and confirming its effectiveness through thorough experimentation.
4. SSCNN improves plain feature decomposition by incorporating a selection mechanism. This allows the model to identify fine-grained dependencies at each time step, which is essential for enhancing the accuracy of the decomposed structured components and, consequently, the overall prediction accuracy.
5. Extensive analysis has been performed to validate the method on existing benchmarks and compared with state-of-the-art methods.
6. Supplementary materials are satisfactory and provide explanation about the dataset and the implementation.

Weaknesses:
1. Figures lack captions.
2. Include some limitations of the model as well.
3. second contribution and third one looks quite similar.

Limitations:
Authors have adequately addressed the limitations related to computational efficiency and capability of model.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper approaches the problem of long term time series forecasting (LTSF) using a compositional technique to reduce the model size without compromising the quality of solution. The proposed technique is a transformer based architecture with a lower number of parameters, and delivers similar performance as state of the art models for LTSF.

The limitation of existing approaches, such as data patching, is that they fail to take into account the spatio-temporal dependencies, and end up with a blow up in the number of latent variables. This results only in a very small improvement even if the model size is increased substantially. The proposed technique in the paper is based on a inference step and an extrapolation step without any information loss.

The paper evaluates the proposed approach, called SSCNN, with seven datasets, which has a combination of regular and volatile patterns. The baseline and state of the art approaches compared against include iTransformer, TimeMixer, and PatchTST. SSCNN consistently achieves the best scores, with respect to MSE and MAE. The paper also conducts ablation studies to show that each new component in the architecture is vital to the performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The work studies an important and hard problem in time series forecasting which is the problem of efficient and accurate long term forecasting. Compositional techniques have been successful in other areas of AI including reinforcement learning, planning, and finite state controller synthesis. So, it makes sense to apply similar ideas in the space of long term time series forecasting.

Weaknesses:
While the high level message is presented well, I found the details of the proposed method and experiments are hard to follow. A running example with the explanation of the new layers will be useful.

The main contribution with respect to results is somewhat hard to grasp and align with the theoretical claims of the paper. Overall, I think there is room for improvement in the presentation of experimental results. I found some missing details in the experimental section that include:

1. Why is SSCNN missing Figure 3(a)?
2. What is the value of T_{out} in Figure 2?
3. What is the forward window size in Figure 3?

In figure 2, it would be useful to move some of the methods to the appendix, and keep only the critical ones  in the main body of the paper. Same is true for Figure 3. It is hard to go back and forth between figures 1, and figures2&3.

Minor:

1. I would suggest providing some more details about the experimental results in point 3 of the contributions (lines 80-82)
2. Figure 3 is hard to read in print.
3. Having only one legend for all the subplots (Figure 2(a)-(d) and Figure 3(a)-(d)) will better than repeating the legends in all subplots.

Limitations:
Yes, the paper has outlined the limitation of computational efficiency and provided some insight into how it can be improved in future work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wgpmDyJgsg;"REVIEW 
Summary:
The paper introduces a novel optimization-based method for sparse-view 3D reconstruction from unposed images. The method uses off-the-shelf pose estimator to get pose initialization, then it uses rendering loss and generative priors to optimize the pose and 3D reconstruction. In detail, the generative priors involve a multi-view SDS loss on generated novel views using Zero123. The method demonstrates satisfying results on the evaluation data, and the ablation study shows the effectiveness of each proposed technique.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Good performance. The reconstruction quality and pose estimation accuracy are satisfying.
- The paper is well-written and is easy to follow.
- The idea of rejecting images with large pose error is interesting.
- The technical part of the paper is solid.

Weaknesses:
- Missing baseline. For the reconstruction methods, the only baseline is LEAP, which is a feedforward method. In contrast, the proposed method is an optimization-based method, which introduces pose-processing to estimated poses. I would suggest adding baseline of SPARF [1] and using the same pose initialization. Moreover, why not comparing with UpFusion?
- Unknown inference speed. Will the joint optimization of pose and shape be slow? Could you provide a analysis of inference time?
- Related work. One related work is iFusion [2], which uses generative priors for pose estimation and is very relevant to the philosophy of the proposed method. Another related work is FORGE [3], which introduces pose optimization for sparse view reconstruction. Moreover, the authors should discuss the prior sparse-view reconstruction from unposed images works with more details, the authors should provide more comparison and contrast with prior work. The current discussion is too short (Line 90-92).
- Ablation study. The ablation study is performed with the Ray Diffusion pose initialization. How will it look like using Dust3r initialization? This is important as the ablation should be performed with the best base model.


[1] Truong, Prune, et al. ""Sparf: Neural radiance fields from sparse and noisy poses."" CVPR 2023.
[2] Wu, Chin-Hsuan et al. “iFusion: Inverting Diffusion for Pose-Free Reconstruction from Sparse Views.” ArXiv 2023.
[3] Jiang, Hanwen et al. “Few-View Object Reconstruction with Unknown Categories and Camera Poses.” 3DV 2024.

Limitations:
Please see weaknesses and questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a framework for joint 3D reconstruction and pose refinement. Specifically, given estimated camera poses from off-the-shelf models, the proposed method first leverages diffusion priors and rendering loss for 3D reconstruction. The 3D reconstruction is further used to refine the current pose parameters. The 3D reconstruction and pose refinement are conducted in an alternative way. An outlier identification and correction strategy is also introduced to make full use of the given image while mitigating the adverse effect of noisy camera estimations at the same time. Experimental comparison with several pose estimation baselines shows that the proposed method can refine inaccurate pose estimation effectively.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper tackles a practical problem in real-world scenarios, where ground truth camera poses are not always available.
2. The proposed method is shown to be effective when applying to different pose estimation baselines.
3. The proposed outlier removal and correction is effective from the ablation study results in Table 4.

Weaknesses:
1. The proposed method is compared with SPARF only in the setting of using pose from different pose estimation baselines. However, it would be more convincing to also present the results using the same setting of SPARF, which adds noise into the GT camera pose. This will be a direct comparison with SPARF’s original results reported in their paper.
2. The proposed method is compared with LEAP for 3D reconstruction results. However, the comparison is a bit unfair since LEAP does not require any initial camera poses. 
3. The description of how to effectively detect the outliers (line 212 - line 214) is not very clear. Similarly, the procedure of how to correct the outlier poses (line 223 - line 225) is not very clear either. How the MSE and LPIPS are computed and compared since there is no correspondence?

Limitations:
Limitations are addressed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for the joint reconstruction of camera poses and 3D objects given sparse input views. The core idea is to use a pose-conditioned diffusion model (Zero-123) as a prior, impose the SDS loss, and jointly optimize the poses and objects, similar to the approach in ID-pose. To improve the robustness and quality of the optimization, the authors made several modifications: (1) Using a 6 DoF pose-conditioned diffusion model instead of a 3 DoF model. (2) Adding strategies for outlier detection and correction. (Although somewhat empirical, it proves effective.)

This approach requires initial camera poses (from methods such as RelPose++, RayDiffusion, etc.) and is not capable of reconstructing poses from scratch (e.g., purely random camera poses). Experimental results demonstrate that, compared to SPARF and ID-pose, the proposed method achieves better pose estimation quality. Additionally, it provides better object reconstruction in terms of novel view synthesis quality compared to LEAP.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1) The approach is technically sound, and I believe the reported results are reproducible.

(2) The reconstructed results look good and represent the state-of-the-art in object-level pose-free reconstruction.

(3) The paper is well-written, making it easy to read and understand.

Weaknesses:
(1) This optimization-based method requires more time compared to a feed-forward model, taking about 5-10 minutes. Additionally, the writing discussing this aspect is somewhat unclear: the paper states, “with increased inference time depending on the number of outliers.” Could this statement be more specific? How much does the time increase with the number of outliers? The correction of outliers may be time-consuming as it requires dense searches of initial camera poses.

(2) (Minor) The method focuses only on object-level reconstruction, which makes the scope seem narrow.

(3) The authors do not sufficiently discuss experiments in a more “standard” sparse-view setting, such as using 3 or 4 views. The reported experiments use at least 6 views, which is not a particularly small number.

Limitations:
As discussed in the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method named MV-DreamGaussian for tackling the problem of 3D reconstruction from sparse multi-view inputs. In particular, the paper extends the DreamGaussian work to use multi-view images as the inputs and proposes a scheme to optimize the inaccurate camera poses of the multi-view images.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well written and I can follow smoothly.
- The authors proposed a finetuned version of Zero-1-to-3 with 6 DoF camera parametrization which shows an advantage over 3 DoF camera parameterization in the original paper.
- The proposed pose refinement scheme is novel and very effective according to the authors' experiments compared with SPARF as well as the ablation study which shows that adding the proposed pose refinement improves the pose accuracy and reconstruction quality significantly. The design of the outlier removal based on photometric error ranking and discrete search is empirical but works quite well.

Weaknesses:
- This paper presents very limited novelty in the reconstruction part with a trivial extension to DreamGaussian to use multi-view images, which is already implemented in a public repository [stable-dreamfusion](https://github.com/ashawkey/stable-dreamfusion).
- The major weakness of the paper is the lack of fair comparisons in terms of the 3D reconstruction. The authors only compared with LEAP for the 3D reconstruction. However, LEAP is a work that **does not require any pose inputs**, whereas the proposed work needs relatively good pose initialization (e.g., Dust3r) and conduct refinement on it. In addition, the underlying 3D representation is different, too: LEAP uses NeRF while the proposed work uses 3D Gaussian. I'm confused as to why the authors did not compare with SPARF for the reconstruction quality too since SPARF shares the same input setup as the proposed work. Besides, the very recent work DMV3D would also be a good method to compare with.

Limitations:
The authors have discussed the limitations of the paper and I generally agree with them.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wSpIdUXZYX;"REVIEW 
Summary:
This paper presents a new operator learning method for solving multiphysics PDEs. The attention scheme is designed on channel space to capture multiple physical variables, which is called co-domain. Moreover, positional encoding and normalization layers are considered. Such a strategy enables self-supervised pretraining of PDE systems. The experiments have shown the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed idea is interesting. It enables the generalization to coupled physical systems, which is of interest to the scientific machine learning (SciML) community. Also, self-supervised pretraining is one emerging tool in SciML and will gain a lot of attention in the future. 

- This paper provides the experiments on a Navier-Stokes equation and its coupled version with the elastic wave equation.

- This paper is well-organized and well-written. The details are easy to follow.

Weaknesses:
- This paper only considers one coupled system, i.e., NS and NS+EW. It may not validate the general applicability of the proposed method. The motivation of using this case should be enhanced. Also, considering some other PDE systems might strengthen the paper, such as the Rayleigh-Benard convection system. It is also a coupled system with NS + temperature.  

- The motivation for the combination of positional encoding, self-attention, and normalization layers seems to be better clarified. Although those parts are modular (claimed in Line 68), the connections between each other are also important. 

- In Appendix B.1, it would be good to include more details of self-supervised pretraining, such as masked ratio.  
The evaluation metrics might not be sufficient. This paper only considers L2 errors. There are many papers considering relative l2 error [1]. For the turbulence data, researchers also care about the infinity norm a lot. It would be better to add more evaluation metrics in this paper.

**References:** 

[1] Hao, Zhongkai, et al. ""Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training."" arXiv preprint arXiv:2403.03542 (2024).

[2] Ren, Pu, et al. ""Superbench: A super-resolution benchmark dataset for scientific machine learning."" arXiv preprint arXiv:2306.14070 (2023).

Limitations:
Please see my concerns in **Weaknesses** and **Questions**.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduced an innovative attention-based neural operator and evaluated it against various baselines. They employed masked pretraining and finetuning techniques, comparing the model's performance to multiple benchmarks. Their study included interesting problems such as fluid-structure interactions. The authors showed that their approach is effective for few-shot learning in their experimental evaluations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper presents a new neural operator architecture based on attention mechanisms. This architecture demonstrates superior performance compared to tested baseline models on the NS and NS+EW benchmarks, highlighting its potential advancements in solving PDE-related problems.
- To the best of my knowledge, the authors are the first ones to use masked training for PDE learning effectively
- The mathematical formulation of the proposed model is well-articulated in the paper. This clarity helps readers understand the underlying principles of the model's operation.
- The study addresses a compelling and very relevant multiphysics problem involving fluid-structure interactions.
- The authors demonstrated through empirical evidence that their approach is effective for few-shot finetuning in various scenarios.

Weaknesses:
- The study in Table 1 demonstrates the model’s performance on two specific PDEs: Navier-Stokes for fluid flow and a coupled Navier-Stokes with elastodynamics equations for fluid-solid interactions. While these cases provide some insight into the model's capabilities, they are not sufficient to generalize the model's applicability to a broader range of multiphysics problems.
- For the NS dataset with Reynolds number Re=400, the model trained from scratch with only 25 samples matches the performance of the pretrained model. In the case of NS+EW benchmark, when the Reynolds number increases to 4000, even with just 5 samples, both the finetuned and scratch-trained models exhibit similar testing errors. This suggests that pretraining may not provide significant advantages in many cases.
- The use of the L2 loss metric to evaluate model performance is problematic because it aggregates outputs of different physical meanings, such as pressure p, velocity u, and displacement d, into a single loss value. This can obscure individual variable contributions and lead to misleading conclusions about model accuracy.
- The absence of prediction visualizations diminishes the interpretability of the L2 loss values. Visualizing predictions could provide more intuitive insights into model performance and clarify discrepancies in the loss metric.
- The study in the Table 1 does not include a Fourier Neural Operator. Including such a benchmark is crucial to fairly evaluate CoDA-NO’s performance against an FNO model of similar size.
- The FNO model used for comparison in Table 2 has 1.9 billion parameters, vastly outnumbering the CoDA-NO's 11 million parameters. This overparameterization likely affects the model's performance due to the relatively small training set sizes and makes the comparison with CoDA-NO’s performance misleading. Smaller FNO models could provide a more realistic performance benchmark. The claim that CoDA-NO’s better performance compared to a much larger FNO model demonstrates parameter efficiency is misleading. Parameter efficiency should be evaluated with models of comparable sizes, and overparameterized models may not reflect typical scenarios.
- CoDA-NO has significantly higher inference times compared to other baseline models. 

These points collectively highlight the need for more comprehensive experiments, appropriate metrics, realistic model comparisons, and practical considerations like inference time to fully evaluate the model's capabilities.

Limitations:
The authors explained the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Codomain Attention Neural Operator, which tokenizes function along the channel dimension. It allows to learn representations of different PDE systems within a single model. The authors shows that finetuning a pretrained CoDA-NO on different physics yields good accuracy.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- I like the problem setting and the idea of the algorithm.
- The experimental task is quite interesting.
- The results are convincing, and the fact that the model can generalize to higher Reynold numbers seen during training is promising.
- I liked the used of GNO for handling non-uniform grids.
- The code seems solid.

Weaknesses:
- To me, the main weakness of the paper is that the presentation lacks of clarity. I don't see the point of doing 3 pages of mathematics in function space if, in practice, everything is done in discrete space. I think this blurs the message of the paper and it is difficult for the reader to understand what is the relevant information for understanding the actual CoDA-NO algorithm. In my opinion, these mathematics are not essential to the algorithm and could be put in appendix. I can always express a neural network architecture in function space, but since in practice we are working on discretized space, it is never done in experimental deep learning papers. Moreover, no discussion on how to go from infinite-dimensional space to discretized space is given by the authors.
This space could be used to have the actual detailed architecture. I may have missed the point on the usefulness of these sections and am willing to understand the point of view of the authors regarding this. 
- I don't fully understand the CoDA-NO algorithm and I think a Figure showing the whole architecture would have clarified this.

Limitations:
Overall, I think my main obstacle to provide a better score is the fact that the paper is not very clear due to the introduction of a lot of mathematics not needed to understand the algorithm in practice. These mathematics do not bring any theoretical insight of the algorithm. They are just expressing the architecture in function space. I think this space could have been used to be clearer to explain what is the architecture or the philosophy of the work. Usually, in papers where I see such mathematics, a study of the sample complexity is provided (I know it is almost impossible to do for neural networks).

I am willing to discuss these points with the authors and to modify my score accordingly.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose CoDA-NO, a neural operator architecture that captures interactions across different physical variables of coupled PDE systems. The method involves a generalization of the transformer architecture, including self-attention, positional encodings, and normalization, to function spaces. On two novel datasets for fluid-structure interaction and fluid dynamics, the authors show that their method achieves state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper investigates an interesting problem of how to appropriately capture interactions across different physical variables, that allows for generalization to new codomains.
- As far as I am aware, the generalization of the Transformer architecture to function spaces is novel.
- The experimental results, especially the generalization capabilities (from fluid dynamics to fluid-solid interactions) are impressive.
- Ablation studies on the proposed architectural changes are thorough.

Weaknesses:
Overall, the experiments seem quite compelling. However, it could be illuminating to provide a graphical visualization of the data from Table 1, regarding efficiency of fine-tuning and robustness to out-of-distribution inputs: see questions.

Limitations:
The authors address the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
u7JRmrGutT;"REVIEW 
Summary:
This paper studies the approximation of non-uniform Graph Edit Distance (GED) using graph neural networks. It considers four types of edit operations with different costs: edge deletion, edge addition, node deletion, and node addition. The paper proposes explicitly accounting for the costs assigned to each type of edit operation to adapt to both uniform and non-uniform cost settings. Moreover, it introduces node-pair embeddings for connected and disconnected node pairs within graphs as edge and non-edge embeddings. It computes the node and node-pair alignments through three types of distance functions to approximate the considered operations. Experiments on real-world datasets under a variety of edit cost settings show that the proposed method outperforms selected baselines on chosen metrics.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* This paper studies an important task with many real-world applications.
* The authors propose generating learnable embeddings for disconnected node pairs, which is new to me.
* Extensive experiments have been conducted.

Weaknesses:
* The clarity of the paper should be improved. For example:
  * What are the differences between the pad indicator $q$ and  $η$? Are they inconsistent notation?
  * The paper is not self-contained. The related work should be in the main paper rather than in the appendix. Additionally, some related work, such as [1], which clearly highly influences this paper, is missing.
* The technical contribution of this paper is somewhat limited, as it directly uses MPNNs for representing nodes and the Gumbel-Sinkhorn network to establish alignment.
* The size of the graphs used in the experiments is relatively small (with at most 20 nodes) compared with the graphs used in other neural approaches.
* Restricting the model parameters for the baselines does not seem fair to me, since the proposed model heavily relies on the iterative non-parameterized Gumbel-Sinkhorn refinement.
* The size of the graphs used in the experiments is relatively small, so it is unclear if the proposed model can scale to graphs with, say, one or two hundred nodes. Moreover, since obtaining training data for GED is nontrivial, it is also unclear how the proposed model performs when the supervision signal is not the optimal GED value. I suggest that the authors conduct experiments on commonly used GED datasets with larger graphs, such as the IMDB dataset provided in [2], or use large synthetic graphs following [3].


Minors:

* There are some typos (Line 278 Additiional -> Additional). Some sentences are missing periods, for example, in Lines 273, 551.
* The caption of tables should be above the tables.

Reference

[1] Piao C, Xu T, Sun X, et al. Computing Graph Edit Distance via Neural Graph Matching[J]. Proceedings of the VLDB Endowment, 2023, 16(8): 1817-1829.

[2] Yunsheng Bai, Haoyang Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. 2019. SimGNN: A Neural Network Approach to Fast Graph Similarity Computation. Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (2019).

[3] Roy, Indradyumna et al. “Maximum Common Subgraph Guided Graph Retrieval: Late and Early Interaction Networks.  NeurIPS 2022.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes GRAPHEDX, an innovative neural model that deftly handles Graph Edit Distance (GED) calculations with customizable edit costs. By representing graphs as rich sets of node and edge embeddings and using a Gumbel-Sinkhorn permutation generator, GRAPHEDX captures the subtle nuances of graph structure that influence edit distances. Experiments across diverse datasets demonstrate GRAPHEDX's superiority, with the model consistently outperforming state-of-the-art baselines by significant margins, especially in scenarios with unequal edit costs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The handling of GED metric with unequal cost is a timely and under-explored topic. In real world, many use cases involving GED need to consider such unequal cost scenario.
2. Rigorous evaluation on both the accuracy and efficiency (runnign time) is performed, showing that the proposed method achieves high accuracy while maintaining efficiency.
3. The current model may not fully address richly attributed graphs with complex node and edge features.
To show the transfer learning ability, I encourage the authors to try training on one dataset and testing on another. This would reduce concern on overfitting to one particular dataset
The paper is well-written.

Weaknesses:
1. The current model may not fully address richly attributed graphs with complex node and edge features.
2. To show the transfer learning ability, I encourage the authors to try training on one dataset and testing on another. This would reduce concern on overfitting to one particular dataset

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes GRAPHEDX, a neural model that learns to estimate the graph edit distace among a pair of graphs, not only in the case of equal and symmetric costs for edit operations, but in the case of unequal costs specified for the four edit operations. The core of the proposal represents each graph as a set of node & edge embeddings, designs neural set divergence surrogates based on those embeddings, and replaces the QAP term corresponding to each operation with its surrogate. The method learns alignments to compute surrogates via a Gumbel-Sinkhorn permutation generator while also ensuring consistency between the node and edge alignments and rendering them sensitive to the presence and absence of edges between node-pairs.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
S1. Addresses a gap in the field that previous works have left open.
S2. Outpeforms previous effort in both equal-cost and unequal-cost settings.

Weaknesses:
W1. The cost of training and estimating is not shown.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wfU2CdgmWt;"REVIEW 
Summary:
In this paper, the authors propose a novel learning algorithm, Stochastic Optimal Control Matching (SOCM), to numerically solve general formulations of Stochastic Optimal Control (SOC) problems involving affined-controlled diffusion processes. They build upon the Iterative Diffusion Optimization (IDO) [1] framework, which consists in iteratively refining a parametric controlled diffusion process by minimizing at each iteration (with stochastic gradient descent) a specific objective function with respect to the parametric control. Previous works had for instance considered the relative entropy loss, the cross-entropy loss, the log-variance loss or the moment loss as the objective function. In SOCM, it is a least-squares regression loss (with multiplicative importance weight) which aims at fitting the parametric control to a vector field which depends on a family of *reparameterization matrices* (also optimized). The design of this objective function relies on standard tools from SOC theory, as well as an original contribution, the *path-wise reparameterization trick*, to compute gradients of conditional expectations of a functional applied on a random process. The authors show that the SOCM loss can be decomposed as the sum of a bias term, that is linked to the cross-entropy loss, and variance term, which is only affected by the *reparameterization matrices*. Hence, these extra parameters can be seen as a way to reduce the variance of the SOCM loss, which motivates their introduction. Moreover, this loss has the computational advantage to avoid computing gradients of the control along the path measure (which is the main drawback of the relative-entropy loss). Finally, the authors conduct numerical experiments to compare their approach to existing designs of IDO losses. They consider four different settings ($d\in \{10,20\}$) with access to the ground-truth control, which allows them to compute the $L^2$ error on the control. Using this metric, their results indicate better performance on most of the settings while maintaining a certain training stability.

[1] Solving high-dimensional Hamilton–Jacobi–Bellman pdes using neural networks: perspectives from the theory of controlled diffusions and measures on path space. Nüsken et al. 2021

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The paper is very well-written, it is a pleasure to read it. In particular, the authors pay attention to define their notation, introduce with clarity the SOC framework, state clear mathematical statements, recall (and prove) standard results from SOC theory, provide intuition on theoretical results (with details on the proof and meaningful comments). This is really good work.
- The relation to prior work is clearly well established: in particular, the comparison between the SOCM loss and the previous IDO losses (presented in Section 2.2) is well highlighted. 
- This papers introduces an interesting contribution that may be applied beyond this framework (in particular, in the generative community where terms involving gradients of expectations often appear) : this is the path-wise reparameterization trick, which is proved to be decisive in the numerics.

Weaknesses:
- In my opinion, the major weakness of this paper is the lack of an additional numerical experiment, which represents a ""realistic"" setting (for instance, where the expression of the control is not known). For instance (as mentioned by the authors), a significant line of recent research has considered the sampling problem via a SOC perspective, see for example [1,2,3]. I am convinced that the SOCM contribution would have more impact with additional sampling numerics comparing SOCM, relative entropy [1,2] and log-variance [3] losses, for challenging distributions (namely, multi-modal distributions in relatively high dimension). In this case, the quality of the methods would be assessed with sampling metrics. This weakness explains my current score (although I really appreciate the paper).
- I find that the complexity/accuracy tradeoff of the IDO losses (including SOCM) is not well highlighted to me. The table provided by the authors only considers one setting. To have a full picture, it should be given for all settings.

[1] Path Integral Sampler. Zhang et al. 2022.

[2] Denoising Diffusion Sampler. Vargas et al. 2023

[3] Improving sampling via learned diffusions. Richter et al. 2023

Limitations:
The main limitation of the approach is discussed in Section 5: it is the variance of the importance weight in the SOCM loss, which may blow up.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel algorithm for approximating the solution to the Hamilton-Jacobi-Bellman (HJB) equation with a neural network control policy. Rather than backpropagating through rollouts of the dynamics, the authors develop a least-squares objective which resembles the score-matching loss used in diffusion models. However, this requires computing gradients of a diffusion process with respect to its initial condition. To address this, the authors develop a novel path-wise reparameterization trick which relies on a family of reparameterization matrices. They show how to optimize these matrices to reduce the variance of the objective estimate. They demonstrate that their method obtains a lower error with respect to the ground-truth control on toy problems, sometimes by an order of magnitude.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed objective function acts as a form of variance reduction for the cross-entropy loss when solving stochastic optimal control problems.
- The novel reparameterization trick for estimating gradients of diffusion processes with respect to its initial condition may be more broadly applicable.
- On toy problems, their method appears to generally outperform other approaches in solving the HJB equation for the optimal controls.
- The paper is well organized and overall written well. It provides a thorough related work section and does a good job explaining the novelty and results.

Weaknesses:
- The evaluations only consider simple toy problems. Moreover, they only plot the L2 error with respect to the optimal control. However, this does not necessarily tell us about the actual task performance due to compounding errors.
- On the Double Well system, there is not a clear advantage compared to the variance loss and adjoint method. However, the authors do discuss how their method appears more stable than the adjoint-based ablation.

Limitations:
The authors discuss limitations to scaling the approach up to more challenging problems due to the variance of the importance weight.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents stochastic optimal control matching (SOCM), which is an iterative diffusion optimization for optimal control aiming to fit a matching vector field. The authors introduce a new loss function and address the analysis and design of a learning-based control method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work is nicely motivated in Introduction, showing the drawbacks of traditional works.  The proposed control method is supported by the uniqueness analysis of the control logic (Theorem 1) and the sophisticated design methods (Propositions 1 and 2). In the reviewer's understanding, they are technically correct.

Weaknesses:
As stated in Algorithm 2 below, reducing noise in the gradient is crucial for the presented algorithm.  This weakness is addressed by Lemma 1 and extensions.

Limitations:
No limitations in this work.  This work is devoted to the theoretical analysis of control system design, and it does not directly bring a negative social impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
**Summary**

This paper introduces Stochastic Optimal Control Matching (SOCM), a novel algorithm for solving stochastic optimal control problems. Key contributions include:

1. SOCM algorithm, adapting ideas from conditional score matching in diffusion models
2. A new ""path-wise reparameterization trick"" for gradient estimation
3. Theoretical analysis including a bias-variance decomposition
4. Empirical evaluation showing superior performance on 3 out of 4 benchmarks

SOCM learns a control function by fitting a matching vector field via least squares, while optimizing reparameterization matrices to reduce variance. The method is currently limited to linear Gaussian models and requires knowledge of certain parameters. Experiments demonstrate SOCM's effectiveness on theoretical benchmarks, outperforming existing methods in most cases. The paper provides a solid theoretical foundation but lacks exploration of real-world applications or non-linear systems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduces Stochastic Optimal Control Matching (SOCM), a novel algorithm for solving stochastic optimal control problems. Its originality lies in adapting ideas from conditional score matching in diffusion models to the domain of optimal control. This creative combination represents an interesting cross-pollination between two active areas of research.

The quality of the theoretical work is notable. The authors provide a comprehensive mathematical foundation for their method, including detailed proofs and a novel ""path-wise reparameterization trick"". This theoretical rigor is a significant strength of the paper.

In terms of clarity, the paper is well-structured and clearly written. The authors effectively guide the reader from the problem formulation through the theoretical development to the empirical results. The use of illustrative examples and detailed appendices aids in understanding the complex mathematical concepts presented.

The significance of this work lies in its potential to improve the efficiency of solving stochastic optimal control problems. The empirical results, showing improved performance over existing methods on multiple benchmarks, underscore the practical impact of this approach. However, the significance is somewhat limited by the current restrictions to linear Gaussian models.

Weaknesses:
The primary weakness of this paper is its limited scope and applicability. The method is currently restricted to linear Gaussian models and requires knowledge of certain model parameters. This significantly narrows its potential impact on the broader field of stochastic optimal control. The authors should discuss potential approaches to extend SOCM to more general settings, such as nonlinear or non-Gaussian systems.

While the empirical results are promising, they are limited to theoretical benchmarks. The paper would be strengthened by including experiments on real-world problems or more complex simulated environments. This would help demonstrate the method's practical utility and potential for broader impact.

The scalability of the method is not thoroughly addressed. As the dimensionality of the problem increases, how does the computational complexity of SOCM compare to existing methods? A more detailed analysis of computational requirements and scaling properties would be valuable.

The comparison with existing methods, while showing SOCM's superior performance, could be more comprehensive. Including comparisons with the most recent state-of-the-art methods would provide a clearer picture of SOCM's relative performance in the current landscape of stochastic optimal control algorithms.

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
weemASPtzg;"REVIEW 
Summary:
This paper studies identifiability under unknown muilti-node  interventions (soft/hard), with general models (parametrtic/nonparametric) and **linear** mixing functions. This work provides both detailed proof which justifies the main theoretical statement, and a step-by-step algorithm which guides how to achieve identifiability in practice.
Overall, I find this work serves as an important step for interventional CRL towards more realistic settings.

&nbsp;

### References

[1] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning with interventions. arXiv:2301.08230, 2023.

[2] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning: Linear and general transformations. arXiv:2402.00849, 2024.

[3] Julius von Kügelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Kekic ́, Elias Bareinboim, David M Blei, and Bernhard Schölkopf. Nonparametric identifiability of causal rep- resentations from unknown interventions. In Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2023.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is extremely well written and clearly structured: it communicates clearly motivations, formulation, technical details, and theoretical implications. The experimental results adequately validate the theory in case of a linear causal model.

Weaknesses:
1. The proposed UMNI-CRL algorithm is claimed to work with *general* non-parametric causal models; however, the simulation experiment only showed results on *linear* structural equation model.  It would be great if the authors could report further experimental results on non-parametric causal models, to align with the theoretical claims. If there is a valid reason why it cannot be done, I am also very happy to hear.

2. Following the previous point, since this approach requires density estimation, it might not be scalable on nonparametric models. But to be fair, this seems to be a common limitation in many interventional CRL works [1, 2, 3].

3. Linearity assumption on the mixing function is restrictive, but the authors have acknowledged it and discussed possible future directions to overcome this limitation (sec. 6).

Limitations:
The authors discussed the remaining open problems and limitations in Section 6.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances Causal Representation Learning (CRL) by addressing the challenge of using unknown multi-node (UMN) interventions to identify latent causal variables and their structures.  The authors develop a score-based CRL algorithm that leverages UMN interventions to guarantee identifiability of latent variables and their causal graphs under both hard and soft interventions, achieving perfect identifiability with hard interventions and identifiability up to ancestors with soft interventions. Their method outperforms existing single-node approaches by ensuring robust recovery of causal structures in more complex, multi-intervention environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* Extending the causal representation learning to unknown multi-node interventions

* Proofs are provided 

* Pseudocode is provided

* Computational complexity is discussed

* Limitations are clearly stated

Weaknesses:
* The paper primarily focuses on causal models with linear transformations. This limits its applicability in many real scenarios

* The applicability of the assumptions in real scenarios was not discussed

* The method was not applied on real world-data

Limitations:
The paper acknowledges certain limitation. One notable limitation is the assumption of linear transformations in the causal models considered. This restricts the applicability to scenarios where causal relationships are adequately approximated by linear relationships. Additionally, while the paper addresses the challenge of UMN interventions, it acknowledges the complexity involved in identifying intervention targets in such settings, which can affect the ability to fully leverage the statistical diversity inherent in UMN interventions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies interventional causal representation learning, where one has access to interventional data, to identify latent causal factors and latent DAG in the unknown multi-node interventions regime. The authors consider a setting where the mixing function is linear and the latent causal model is nonparametric. Under the assumption of sufficient interventional diversity, the authors use score function arguments to show that the underlying causal factors of variation (and DAG) can be recovered (1) up to permutation and scaling from stochastic hard interventions and (2) up to ancestors from soft interventions. The authors propose a score-based framework (UMNI-CRL) and evaluate it on synthetic data generated from Erdős–Rényi random graph model.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- This work provides significant results in the unknown multi-node intervention setting, which is much more realistic than the common single-node intervention regime. As opposed to other works, this work studies CRL from a more general class of multi-node interventions (stochastic hard and soft).
- The paper is well-written, the concepts are explained well, and the theoretical identifiability results add a lot of value to the current CRL literature.
- The use of score functions and score differences in the observation space to estimate the unmixing function, especially for the UMN setting, is a novel and interesting approach for CRL.
- This work is the first to establish latent DAG recovery in the UMN setting under any type of multi-node intervention for arbitrary nonparametric latent causal models.

Weaknesses:
Although the theoretical contribution of this work is strong, the empirical evaluation is quite weak compared to other works in CRL. There are only experiments for n=4 causal variables. There is also no baseline comparison of the proposed framework with other methods in the UMN setting (e.g., [1]). Also, some discussions are a bit abridged and could use more elaboration in the paper (see below for details).

[1] Bing et al. “Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions” CLeaR 2024.

Limitations:
Limitations are discussed in Section 6.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper extends previous results on using score function for causal representation learning to the settings with unknown multi-node interventions. This new setting poses significant new challenges as opposed to the single node intervention case. The author first present theoretical identifiability result on hard interventions with latent additive noise model and on soft interventions. They then propose an algorithm called (UMNI)-CRL and test it on synthetic linear Gaussian dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written, easy to follow and with good motivations.

Weaknesses:
1. The transformation from latent to observed is noiseless, which could be a limitation. 
2. Line 199 says that: “This regularity condition ensures that the effect of a multi-node intervention is not the same on different nodes”. But how realistic or neccessary is this condition? It seems like it is very possible that an intervention can cause two downstream nodes to have the same effect although these two nodes is not influenced the same by all type of interventions. 
3. The experiments are only on synthetic dataset but I don’t think that is a big issue. 
4. Some potential missing citations
    
    [1] Kumar, Abhinav, and Gaurav Sinha. ""Disentangling mixtures of unknown causal interventions."" *Uncertainty in Artificial Intelligence*. PMLR, 2021.
    
    [2] Jiang, Yibo, and Bryon Aragam. ""Learning nonparametric latent causal graphs with unknown interventions."" *Advances in Neural Information Processing Systems* 36 (2024).

Limitations:
1. Theorem 1 only works for additive noise model.
2. The transformation from latent to observed noiseless.
3. Experiments are only on synthetic dataset. 
4. I am unsure if the algorithm is practical because it needs to estimate the score function.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces new identifiability results for CRL in environments with unknown multi-node interventions. It shows that, with sufficiently diverse interventional environments, one can achieve identifiability up to ancestors using soft interventions and perfect identifiability using hard interventions. The paper also provides an algorithm with identifiability guarantees.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper tackles the complex and underexplored multi-node intervention setting. The established identifiability can be crucial for extending current CRL theories into more practical contexts.
- The introduced algorithm that leverages score functions with different interventional environments is also interesting and insightful.
- The paper is well-motivated and articulated with high clarity.

Weaknesses:
- The proposed algorithm, while theoretically sound, seems computationally demanding. In fact, even a 4-node low-dimensional case requires a large number of environments and samples. The paper could benefit from a deeper discussion on the scalability of the algorithm.
- The current evaluation of the algorithm is limited to synthetic simulations. Expanding it to more realistic datasets would substantively improve its practical significance.

Limitations:
The paper acknowledges its main limitations in the reliance on linear transformations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wduRaBDRBS;"REVIEW 
Summary:
- This paper carries out an analysis of token merging[1] in the context of long-form video understanding and proposes learnable video token merging (VTM) to select semantics/saliency guided tokens for merging. 
- In token merging, at each layer, the tokens are divided into two sets source S and target T through uniform sampling. Tokens in S are matched to T based on similarity and merged (usually by average pooling). This paper compares this naive VTM with two other variants where selection of T is guided by informed heuristics: (1) region VTM where tokens at the center of each frame are more likely to be retained, (2) motion VTM where tokens with high motion are more likely to be retained. Through this analysis, authors argue that the strategy to select T plays an important role in the final performance.
- Motivated by this, authors propose a learnable VTM where it first predicts a saliency score for each input token. The target set T is sampled according to the probability distribution defined by saliency score. Since this partition operation is not differentiable, authors propose a novel training architecture where a parallel auxiliary network is trained alongside. The saliency scores are used to bias the attention score of the aux network, thereby supervising the saliency prediction to focus on important tokens. Aux network can be discarded at test time.
- Authors carry out a fair evaluation of the learnable VTM on LVU, Breakfast and COIN datasets by comparing against several baselines including ViS4mer, S5, D-sprv. Learnable VTM performs better than baselines in almost all evaluation tasks with low GPU memory usage and high throughput.

[1] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token Merging: Your ViT but faster. In ICLR, 2022.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- In learnable VTM, the idea of learnable and auxiliary path is interesting. There is no way to directly supervise the token saliency prediction of the main path because partition operation is non-differentiable. Hence the attention in auxiliary path is influenced by saliency scores of the main path, which encourages the saliency prediction to focus on important tokens.
- The evaluation is fair and consistent. The authors use the same backbone as the prior works to encode input video frames, thereby ensuring a fair evaluation.
- The results of learnable VTM on LVU dataset and Breakfast is noticeably better than baselines with less GPU memory usage. However, on COIN dataset, it doesn't perform better than S5 baseline.

Weaknesses:
### Major weaknesses
- One of the cited contribution is the exploration of region based and motion based VTM (Section 3.3) but it seems trivial. The effectiveness of token selection is already shown in learnable VTM. In light of that, there is an unreasonable focus section 3.3 which is unnecessary.
- Section 3.4 explains little about the details of learnable VTM, how it is trained, how the gradients flow in the presence of non-differentiable partition function, etc.
- There are some stretched claims based on qualitative and quantitative results. For example,
  - In Line 174, authors claim that center VTM performs better than naive VTM. However, according to Table 1, the results are mixed at best.
  - In Fig 5, authors also claim that the visualization of merged tokens show saliency based merging. However, the figure doesn't support the claim. There are many merged tokens on important salient features and some background tokens are not merged.

### Minor issues
- Line 19: it should be ""into the domain of video computer vision"" as all cited papers are video learning papers.
- Is there a difference between notation of C and D? It looks like both are used interchangably to denote token dimension.
- Table 2: How it throughput measured? fps?

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores various video token merging strategies in the context of long-form video classification and finally propose a learnable Video Token Merging algorithm that dynamically merges video tokens based on visual salient areas. The contributions are summarized as follow:
1.  Explore various video token merging methods including the naïve VTM, the region concentrated VTM, and the motion-based VTM.
2.  Propose the learnable video token merging algorithm, which estimates the saliency scores of each token and adaptively merge visual tokens based on those scores.
3. The proposed algorithm achieves the best or competitive results on various datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper explores various video token merging methods including the naïve VTM, the region concentrated VTM, and the motion-based VTM.
2. Compare with baseline and rule-based video token merging. The proposed learnable video token merging strategy has large improvement.
3. The two-paths design to deal with non-differentiable problem in partitioning process is interesting.

Weaknesses:
1. This paper proposes a leanable video token merging strategy. The similiar high-level idea can be found by CTS[1] in image domain. The novelty is insufficient。
2. This paper focuses on video token merging. However, I do not observe any specific design tailored for the video domain in terms of the methodology. let alone long video.



[1] Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper approaches the task of long-video understanding from token reduction perspective. Specifically, Transformer-based approaches suffers from memory bottleneck and quadratic computation complexity with increasing number of tokens, which is even more pressing with long-videos as input. The paper builds on a recently developed token merging mechanism, and proposes a learned saliency measure to modulate what tokens gets merged instead of using a random or hand-crafted saliency measure. The central hypothesis of the work is that typically techniques that use similarity as merging criteria may inadvertently lose out on salient tokens. The paper reports experiments on three conventional long-video benchmarks (LVU, Breakfast and COIN), and shows effectiveness of their approach compared to prior related works both in terms of performance and memory requirement. The paper also ablates the effectiveness of their proposed saliency measure (learned VTM) over hand-crafted measures including motion-based (using optical flow), center-biased and random schemes.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-written with most of the information presented for ease of understanding 
- The memory requirement is lower than S4, with competitive performance which highlights the importance of token selection in the case of long-videos

Weaknesses:
- Comparison to related token saliency approaches
    - The paper proposes a scheme to identify salient tokens by using a learned projection matrix $U_s \in \mathcal{R}^{D \times 1}$ with $\texttt{tanh}$ activation function
    - However, learnable token saliency methods have also been used in prior works, such as EVEREST [1], which uses a pair-wise learned saliency at feature-level (equation 2) using $\texttt{Conv3d}$. The resulting approach was shown to be effective in the Masked Autoencoding setup
    - Having a motion-based merging scheme is a good baseline, but some variants of learnable token saliency could also be tried to gain better understanding how token saliency gets influenced by different approaches   

- Role of $L_1, L_2, L_3$
    - The paper proposes to take tokens from $L_i$ consecutive frames for the $i^{th}$ VTM block
    - It seems that choosing the values of $L_i$ is quite crucial given its impact on performance and memory requirement (Table 6) that forms the central claim of the paper
    - However, the paper highlighted the contribution of token saliency more compared to the choice of $L_i$ hyperparameters
    - Did the authors experiment with a rather simplistic setup using a single VTM block and/or with all $L_i$ being 60? It would help the readers to gain better understanding of what works in long-videos 


- How saliency changes with tokens from different number of frames?
    - It seems that the saliency is being computed at each VTM block. It would be interesting to see how the saliency changes across the three VTM blocks
    - On that note, what VTM block’s saliency is being visualized in Figure 5?



### Minor
- Line 145-146: “$i$-th transformer block takes the tokens corresponding to $L_i$ frames without overlapping”
    - Confusing when $i$ is referred to as the frame number and the block number of transformer at the same time
- Line 145-147: $j$ is not defined


### Typos
- Line 24-25: “so the tokenizing the”
- Line 36: “selectio”
- Line 114-115: “applications are mostly remained”
- Line 117: “depedencies”
- Line 161: “in the videos, .”
- Line 165: “regarding less of the”
- Line 177: “sailent”, “the the”
- Line 306: “sailencies”


### References
[1] “EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal Tokens”. Sunil Hwang and Jaehong Yoon and Youngwan Lee and Sung Ju Hwang. ICML 2024.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper builds on Token Merging (ToMe) to improve its performance. In particular, the authors explore different ways to partition tokens so that the merging operation can lead to better performance while maintaining speed. They explore region-concentrated merging, motion-vector based merging and a learnable selector, and find that the learnable version works best. To make the network trainable, they employ a creative auxiliary path mechanism  to make everything differentiable. They find that their learnable VTM obtains good results compared to baselines on long form datasets (LVU, Coin, Breakfast), and that it outperforms the other methods they introduce.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem this paper addresses is an important one. Videos (especially long ones) have many redundant tokens and reducing their number while maintaining performance is a crucial problem to solve in the field.

The model itself is well designed and uses a creative auxiliary path to handle a non-differentiable partitioning process. Given the premise of the paper, the model is well-designed and seems to address the issue they propose. 

I also appreciate the exploration of different methods, and a comparison on which worked better. This kind of analysis is often missing from papers and I am grateful for the authors for including it.

Weaknesses:
I don’t really agree with the premise of the paper (and am open to a rebuttal to explain if I’m wrong here). Token Merging already explored merging for video in detail. The reason Token Merging is based on similarity is that by combining tokens that are extremely similar, the weighted average of the those tokens should produce an extremely similar result in the attention operation. This was also detailed more in the Token Merging follow-up TomeSD. If you use different criteria such as saliency (which is not really well-defined), this is no longer guaranteed, and from equation (10) it seems like the authors do not use the proportional attention scheme from ToMe (Eq 1 in the original paper). Table 8 doesn’t show the learnable method using this; it seems to just be about the pooling part rather than the attention operation.

I also don’t understand the intuition behind the saliency: shouldn’t we be aiming to combine together tokens that are NOT relevant, so that the transformer can focus more on the relevant tokens, rather than averaging (and thus losing) information from the more salient / important tokens? I’d really appreciate some clarification here. From Figure 3, it doesn’t look like learnable VTM is focusing on visually important tokens: it’s picking ones from the ceiling and wall in addition to the people.

My main issue is with the evaluation. The evaluation seems not quite fair, especially when measuring memory usage and throughput. Shouldn’t it be compared to baseline merging algorithms, like the naive ToMe? My impression is that the memory usage and throughput from VTM will be exactly the same as ToMe because it uses a similar partitioning scheme and constant factor reduction, which is why it may not be included in the results, but this seems important to include for context. Furthermore, the improvement on metrics is quite small, given that the speed is the same as other merging methods. Is this expected?

Also, The paper is motivated by “long-term” video, but evaluates on 64 frames, which isn’t really long and in my view, doesn’t merit only evaluating on LVU, Breakfast and COIN. Kinetics-400 has 300 frames per video, and is a more standard benchmark for evaluating video backbones - in fact, the original ToMe paper includes experiments on those datasets, which would make for a more fair comparison. Furthermore, nothing about the method itself is specific to these longer videos. I think evaluating on more standard datasets is crucial to measuring the actual strength of the method, especially compared to baselines like Token Merging. In particular, the long-form datasets are very compressible. 

The paper is not well-written and the grammar needs a lot of revision, making it hard to focus on the content of the paper itself. In addition, a lot of space is spent on methods that are not really used in the final results (center, region, motion vector) and on citing equations from preliminary works (token merging, attention). Given that a claimed contribution is an exploration of these different methods, I would also have expected more detailed ablations and experiments to understand exactly why some of the methods perform better than others.

Limitations:
The biggest limitation of this compared to baseline Token Merging is that it requires re-training. ToMe could be applied to a pre-trained network out-of-the-box. Learnable VTM cannot do this, making it impossible to apply to a pre-trained video network. The other proposed methods (region, motion vector) can do this though, and this would be a good thing to note somewhere in the paper.

In my view, the limitation of methods like VTM is that it always reduces a constant number of tokens per video, even though some videos are inherently more compressible than others. For example, Breakfast videos are extremely compressible compared to the average LVU video. However, this is beyond the scope of the paper, and using constant reduction is certainly more convenient, but this would be a good limitation to acknowledge and perhaps address in the future.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wdGvRud1LS;"REVIEW 
Summary:
The paper presents a novel approach called Functional Maximal Correlation Algorithm with Trace cost (FMCA-T) for estimating cortico-muscular dependence by leveraging orthonormal decomposition of density ratios. This method is designed to model the relationship between EEG (electroencephalography) and EMG (electromyography) signals, addressing the challenges of interpretability, scalability, and local temporal dependence in cortico-muscular connectivity. The key contributions include introducing a matrix trace cost optimization for improved stability and efficiency, demonstrating robustness against nonstationary noise and delays, and effectively capturing movement and subject information from EEG features for enhanced classification accuracy. The proposed method outperforms existing baselines, particularly in cross-subject scenarios, and provides insights into channel-level and temporal dependencies, reinforcing its potential applications in brain-computer interface development and neuromuscular disorder diagnostics.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Innovative Method: Introduces the Functional Maximal Correlation Algorithm with Trace cost (FMCA-T), providing a novel approach for estimating cortico-muscular dependence.
2. Improved Stability and Efficiency: Utilizes matrix trace cost optimization, which is more stable and computationally efficient compared to traditional log-determinant cost methods.
3. Enhanced Classification Accuracy: Effectively captures movement and subject information from EEG features, significantly improving classification accuracy, especially in cross-subject scenarios	
4. Validation on Multiple Datasets: Validated using both simulated and real EEG-EMG datasets, confirming the method’s effectiveness and robustness.
5. Open Data and Reproducibility: Offers open access to datasets and detailed implementation code, facilitating reproducibility and further research in the field.

Weaknesses:
The provided baselines are relatively few; future work could expand on this.

Limitations:
1.Limited Dataset Size: The cross-subject classification performance drops, potentially due to the limited dataset size of only 25 participants. Larger and more diverse datasets may be needed to validate the method’s robustness comprehensively.
2. Generalization to Other Modalities: While the method shows promise for EEG and EMG signals, its generalizability to other types of biosignals or broader neural data modalities has not been extensively disscuss.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new method to model the relationship between cortical and muscular oscillations using EEG and EMG recordings. Traditional methods like Cortico-Muscular Coherence (CMC) have limitations, so the authors propose using statistical dependence estimators to learn eigenvalues, eigenfunctions, and projection spaces. This approach improves interpretability, scalability, and local temporal dependence. Experimental results show that the method accurately classifies movements and subjects, highlighting specific EEG channel activations during movements, and demonstrates robustness against noise and delays, suggesting its potential for diagnosing neuromuscular disorders and developing brain-computer interfaces.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper combines statistical dependence estimators with neural network optimization techniques. This fusion of methodologies enhances the ability to capture high-level and contextual connectivity between cortical and muscular oscillations.

2. The paper provides a detailed description of the proposed methodology, including the mathematical foundations, algorithmic implementation, and practical considerations. The inclusion of eigenvalues, eigenfunctions, and projection spaces adds depth to the analysis.

3. The authors conduct comprehensive experiments to validate their method. The results demonstrate the method's robustness against nonstationary noise and random delays, confirming its reliability and practical applicability.

Weaknesses:
1. Mathematical and Algorithmic Complexity: The proposed method involves complex mathematical formulations and advanced statistical techniques that may be challenging for a broader audience to grasp. Simplifying some of the mathematical derivations or providing more intuitive explanations and visualizations could make the paper more accessible.

2. Interpretation of Results: While the method highlights specific EEG channel activations during movements, the physiological and neuroscientific significance of these results could be further elaborated. Providing more detailed discussions on how these findings align with or differ from existing neuroscience research would enhance the interpretability and relevance of the results.

3. Scalability: The scalability of the proposed method to larger datasets or longer signal durations is not thoroughly addressed. Discussing the computational complexity and providing benchmarks on how the method performs with varying data sizes would be valuable.

Limitations:
Plz go and check weaknesses and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors apply novel but already existing (https://www.sciencedirect.com/science/article/pii/S0047259X2300074X, https://arxiv.org/pdf/2212.04631) machinery based approach on the orthonormal decomposition of density to decipher the relationship between cortical brain activity and the electromyographic signals during basic hand movements. The work is based on the publicly available dataset and the code is available. The unknown decomposition is modeled by a pair of neural networks concurrently processing EEG and EMG data in order to arrive at the internal representation for each of the modality. The internal representations are then aligned to minimize the rank of the joint covariance matrix. To guide the learning process, the authors propose a somewhat novel loss function equal to the negative trace of the canonical correlation matrix calculated using latent representations. The authors test their approach on the downstream tasks of classifying movement types in both within and across subject designs. They also apply the obtained representations to distinguish between participants based on their EEG data. The authors provide some interpretation to the obtained solution in the form of channel and temporal maps indicating the electrodes and time moments that contribute to the decoding most.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The authors applied novel but existing methodology of orthonormal density decomposition to the EEG+EMG dataset for the first time 
2. The authors introduced a novel loss function and showed that it provides better performance in the downstream task of EEG-based classification of movement types
3. The authors used multi subject dataset 
4. The authors attempted to provide interpretation of the obtained decision rule
5. The authors present detailed results of their experiments in the appendix

Weaknesses:
1. Several inaccuracies and lack of details in the mathematical expressions:
1.1 line 100, last expression and additional p(z) is needed in the integral
1.2 equation 3  - do the eigenvalues need to be normalized? Does the sum exclude the first normalized eigenvalue?
2. I would argue against the suggested novelty of the proposed loss function as it seems like the loss function that is closely associated with the Canonical correlation analysis (CCA) (equation 4). Generally speaking, the proposed approach boils down to the CCA in the latent variable space with latents computed by means of a CNN.
3. The authors claim that they “..design a specialized network topology to generate features for individual channels and time intervals, ensuring that the internal layers of this network quantify channel-level and temporal-level features, similar to [22–24]” - however unlike for instance the EEGnet, the authors use non-linearities in the temporal network (prior to the spatial) which in my view prevents the straightforward interpretation of the obtained representations at least using simple correlational measures. 
See also Q.1 and 2. 
4. The authors did not validate their approach to interpreting the decision rule and obtaining spatial and temporal maps with simulated data. This needs to be done and the simulated data should contain not only the neuronal sources coupled to the simulated EMG but also the sources unrelated to the signal of interest (EMG). The authors then need to demonstrate that their methods infers the proper spatial patterns corresponding to the task-relevant simulated neuronal sources. Ideally,  the obtained maps should be ranked based on their importance for the overall decoding accuracy. If this is not possible within the review cycle, the authors should significantly reduce the proportion of the manuscript dedicated to the physiological plausibility of their solution and instead describe limitations related to potentially non-physiological origin of the extracted features.  
5. It is disappointing that when interpreting the decision rule the authors did not provide information regarding the EEG frequency domain their network got tuned to during the training.

Limitations:
The authors partly addressed limitations but several items, see Weaknesses section, are left out.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel approach to analyzing cortico-muscular connectivity using statistical dependence measures based on density ratio decomposition. The authors apply a method called Functional Maximal Correlation Algorithm with Trace cost (FMCA-T) to paired EEG and EMG recordings. The key idea is to decompose the density ratio between EEG and EMG signals into eigenvalues and eigenfunctions, which can capture important contextual information that affects the EEG-EMG dependency such as type of movement or subject without having them labeled. They also use the learned eigenfunctions as feature projectors and train a classifier on top for movement type classification tasks.

The authors test their approach on simulated data (SinWav) and a real EEG-EMG dataset with 25 subjects performing 11 different upper limb movements. They compare FMCA-T against several baseline methods for dependence estimation and classification. They find that the learned eigenfunctions capture factors such as movement type and subject identity. Further, FMCA-T outperforms the baselines, for example by 10% for cross-subject classification of arm-reaching, hand-grasping and  wrist-twisting.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* very sophisticated method with clear motivation
* original idea cleanly mathematically derived (as far as I can tell)
* produces good results

Weaknesses:
* classification baselines, could be stronger, e.g. by also using [EEG Conformer](https://pubmed.ncbi.nlm.nih.gov/37015413/)  and [Deep4](https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.23730)
* text is very dense at times, definitely found some part hard to read, but not sure how much it can be made easier, possibly you could explain some concepts used in 2.2 in more detail in the supplementary

Limitations:
Authors could discuss a bit more under what conditions the assumption of conditional independence may be problematic and when it is fine for EEG/EMG. In terms of what one may expect to see in analyses as performed here under different conditions.
Of course, evaluation on further datasets would also be helpful for this.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wcxHbAY8B3;"REVIEW 
Summary:
This paper proposes GaussianMarker, a novel method for embedding invisible watermarks into 3D Gaussian Splatting (3DGS) models to protect their copyright. The key idea is to use uncertainty estimation to add imperceptible perturbations to 3D Gaussian parameters with high uncertainty. The method enables extraction of copyright messages from both the 3D Gaussian parameters and rendered 2D images, and demonstrates robustness to various 3D and 2D distortions. Experiments on multiple datasets show the effectiveness of the approach in terms of message decoding accuracy and visual quality preservation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Timely contribution addressing copyright protection for 3D Gaussian Splatting models, an increasingly important 3D asset format
* Clever use of uncertainty estimation to guide watermark embedding in a way that preserves visual quality
* Demonstrates robustness to various 3D and 2D distortions/attacks

Weaknesses:
* The decoder is trained per scene, rather than being a generalizable decoder. This makes the watermarking process essentially impractical for real-world use. It's not feasible for people to store a separate watermark encoder and decoder for each scene for the vast number of Gaussians distributed across the internet. Reflecting on the logic of image watermarking, a single watermark encoder and decoder can encode and decode information for any cover image, so the sender and receiver only need to jointly possess one watermark decoder. This is a more reasonable setup.
* Experiments focus mostly on relatively simple scenes - more complex, dynamic scenes could be challenging
* The robustness to more sophisticated attacks (e.g. adversarial perturbations) is not explored
* Discussion of potential negative impacts of the technology could be expanded

Limitations:
The authors have included a brief limitations section that acknowledges potential vulnerabilities to malicious attacks beyond their technical solution. However, this discussion could be expanded to consider more specific limitations of the approach, such as potential challenges with very complex scenes or highly dynamic content. The societal impact is briefly mentioned, focusing on the positive aspects of copyright protection. A more thorough examination of potential negative impacts or misuse scenarios would strengthen the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
3D Gaussian Splatting(3DGS) has gradually become the mainstream method for acquiring 3D assets, which has led to a demand for copyright protection of 3DGS. In this paper, a watermarking method based on uncertainty called GaussianMarker is proposed. Firstly, 3DGS is partitioned based on uncertainty, and the watermark is only added to the model parameters with high uncertainty. Subsequently, the corresponding parameters are perturbed using both 2D and 3D watermark encoders, enabling the extraction of watermark information from rendered 2D images as well as directly from 3D model parameters. Experimental results demonstrate the robustness of the proposed GaussianMarker method against 2D and 3D distortions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper proposes a method that utilizes uncertainty to partition 3D Gaussian. By embedding watermarks specifically in the parameters with high uncertainty, the method aims to mitigate the impact on the quality of the model.

2. The paper considers the extraction of watermarks in both 2D and 3D scenarios, taking into account the robustness of watermark extraction in these two contexts.

Weaknesses:
1. The paper mentions that the calculation of uncertainty is related to the model parameters, and in 3D Gaussian, each point has multiple parameters such as $\mu, R, S, c, and \alpha$. It would be helpful if the authors could clarify which specific parameters are used in the proposed method. Additionally, the paper provides a formula for calculating model uncertainty, but it is unclear how the uncertainty of each Gaussian is computed and used for partitioning. The authors should provide further explanation or clarification on this matter.
2. The description of the densify function $g(\cdot)$ in the paper states that it randomly samples a new position from a distribution. According to my understanding, the original Gaussian $G_i$ should have been replaced. However, Figure 2 shows that the original Gaussian  $G_i$ still exists, which is confusing to me.
3. During the watermark embedding process, it is unclear whether the 2D and 3D watermarks are embedded into the same model parameters. It would be helpful if the authors could clarify which specific model parameters of the 3D Gaussian are used for embedding the watermarks.
4. In the section on ""Distilling watermarking knowledge,"" the authors mention that ""the pre-trained feature from 2D space can be distilled to the 3D space."" It is important for the authors to provide an explanation of how this is achieved.

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a new method for embedding digital watermarks in 3D Gaussian Splatting (3DGS) models to protect the copyright of 3D assets. Traditional watermarking techniques for mesh, point cloud, and implicit radiance fields are not suitable for 3DGS, as they can cause distortions in rendered images. The authors propose an uncertainty-based approach that constrains perturbations to the model parameters, ensuring that watermarks remain invisible while preserving visual quality. The method allows for reliable extraction of copyright messages from both 3D Gaussians and 2D rendered images, even under various distortions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method ensures that the embedded watermarks do not cause significant distortions in the rendered 3D scenes or 2D images, maintaining the visual quality of the assets.
2. The approach is designed to be robust against various forms of 3D and 2D distortions, such as noise, translation, rotation, cropping, JPEG compression, scaling, and blurring. This enhances the reliability of copyright protection.
3. The method allows for the extraction of copyright messages from both 3D Gaussian parameters and 2D rendered images, providing multiple layers of security and verification.
4. Extensive experiments demonstrate that the method achieves state-of-the-art performance in both message decoding accuracy and view synthesis quality.

Weaknesses:
The malicious scenarios considered are limited to traditional distortions. \
More sophisticated scenarios should also be explored. \
For instance, a malicious actor could fine-tune the downloaded 3DGS or use an auto-encoder to remove embedded information ([1],[2],[3]). \
In such cases, how would the proposed method perform?

Additionally, a more complex scenario to consider is when a malicious actor renders Bob's 3DGS and uses it as training data to create their own 3DGS. \
How would the proposed method address these advanced threats?

[1] Fernandez et al., The Stable Signature: Rooting Watermarks in Latent Diffusion Models \
[2] Kim et al., WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models \
[3] Zhao et al., Invisible Image Watermarks Are Provably Removable Using Generative AI

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an uncertainty-based method to achieve watermarking for 3D Gaussian Splatting. Specifically, the Hessian matrix is used to estimate the parameter uncertainty. Then, the 3D Gaussians with high uncertainty are densified. The densified 3D Gaussians are trained to embed watermarking using a pre-trained 2D message decoder. After that, a 3D message decoder is trained using PointNet. Experimental results show that the proposed method achieves the best performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to follow. 

2. The experimental results show that the proposed method achieves new SOTA results. 

3. The proposed method can decode watermarking both in 2D rendered images and 3D assets. 

4. An uncertainty-based method is proposed to select trainable 3D Gaussians, which is reasonable.

Weaknesses:
1. One concern about this paper is its novelty. The major contribution of this paper is the introduction of uncertainty into 3D Gaussians watermarking. As the definition of uncertainty using Fisher Information comes from [42], simply using uncertainty for 3D Gaussians watermarking is quite simple and straightforward. Regarding the message decoders, they are all standard operations. HiDDeN [11] is used for the 2D message decoder, and PointNet [43] is used for the 3D message decoder. Therefore, the major contribution of the proposed method should be further justified.

2. The proposed method utilizes the 3D Gaussians with high uncertainty to embed watermarking. What if an attacker also uses this feature? The attacker could first identify the 3D Gaussians (after training/fine-tuning) with high uncertainty and then only attack these 3D Gaussians using techniques such as Noise, Translation, Rotation, or Cropout. Additionally, the attacker might delete some of the identified 3D Gaussians to compromise the 3DGS assets.

3. The influence of the parameter uncertainty threshold should be included in the experiments to assess the sensitivity of the uncertainty threshold on the proposed method. 

4. The results with different bit lengths are missing.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wcX04Wn34u;"REVIEW 
Summary:
In this paper, the authors propose a method to help alleviate the domain gaps among different datasets with different LiDAR sensors, which can enable zero-shot detection on a new dataset. The proposed method including Scene Modeling for foreground and background reconstruction and LiDAR Modeling with statistical and ray-drop modeling. Another contribution is that the authors also accelerate the ray casting algorithm using GPU. The authors conducted single-domain and Multi-domain unification experiments on Waymo, nuScenes, and KITTI datasets, which achieves SOTA performance compared to previous works. The authors also provide ablation studies on foreground diversity and LiDAR noise injection. In addition, the authors show the run time performance after the GPU acceleration.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The foreground and background reconstruction and LiDAR Modeling and the statistical and ray-drop modeling in LiDAR Modeling make the paper differ from previous works.
Quality: The code is provided. The performance is evaluated on multiple datasets, and achieves SOTA performance compared to previous works, and ablation studies are good. The GPU acceleration is also good.
Clarity: The images in the paper are clear and easy to understand.
Significance: The paper demonstrates the potential of zero-shot detection on a new dataset by 3D reconstruction from multiple different dataset and LiDAR settings and LiDAR simulation.

Weaknesses:
In the title of the paper, the use of terms such as ""Language,"" ""Translator,"" and ""LiT"" appears to be capitalizing on the popularity of the trending terms ""LLM"", ""ViT"", and ""DiT"", potentially misleading readers.
SECOND and PV-RCNN are relatively old detection models, it's better to have experiments on more recent models such as CenterPoint, and other SOTA models to further demonstrate the effect of domain unification on SOTA models and even achieve new SOTA results. This would significantly enhance the paper's persuasiveness and impact.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
To address the significant gap between different LiDAR datasets (related to sensors, environments, etc.), this paper proposes a solution that differs from the existing model-based adaptation approach. By employing a scene-reconstruction-data-simulation approach, it achieves consistent representation of different LiDAR datasets. This data-driven method partially resolves issues such as domain shift in autonomous-driving-related 3D  point cloud learning.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Innovatively analogizing the domain gap between different LiDAR data to that between languages, this paper proposes a data-driven cross-sensor training method from a ""translation"" perspective.

- The proposed method shows good performance across different datasets, especially in terms of the AP3D metric.

- The paper is well-written with clear logic and comprehensive experiments.

Weaknesses:
- Does ""foreground"" only refer to vehicles? Do pedestrians, bicycles, and similar entities fall into this category?

- Similarly, in background reconstruction, is consideration limited to rigid bodies like the ground? In autonomous driving scenarios, is there no need to consider non-rigid objects such as vegetation?

- In the current version, it seems that scene variations are not significant. Does this mean it's difficult to address zero-shot scenarios? For instance, if the source data are all from residential areas, is it challenging to accurately simulate point clouds from downtown areas?

Limitations:
The analysis and discussion regarding scene reconstruction need improvement, as suggested by the previous  comments.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposed a unifying LiDAR Translator named LiT to achieve LiDAR domain adaptation. Differing from current model-driven approaches, LiT adopts a novel data-driven approach, embedding disparate LiDAR attributes into a common representation. LiT
proposes a generalizable scene modeling and LiDAR statistical modeling. Besides, an efficient ray-casting engine is proposed to accelerate the above models. LiT also achieves efficient SoTA performance on several LiDAR datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1. LiT adopts a novel data-driven approach instead of the classical model-driven approach, embedding disparate LiDAR attributes into a common representation. This research direction provides much value for real-world applications in autonomous driving industries. 

S2. An effective ray-casting engine is proposed to accelerate LiT on GPUs.

S3. Experiments on widely used datasets demonstrate the SOTA performance of LiT.

Weaknesses:
W1. This work looks like a data normalization operation, only modifying different datasets into a unified representation.

W2. The authors argue that model-driven approaches will introduce considerable costs associated with customizing model structure and training data for new, specific domains. However, this work has an extra LiDAR statistical modeling, this operation also causes additional costs.

W3. Table 7 shows that LiT may not avoid the problem of model-driven approaches, that is, requiring different configurations for distinct datasets.

Limitations:
Although the proposed data-driven approach seemed to be a promising research direction, LiT lacks sufficient comparisons with the model-driven approaches. Besides, LiT seemed to show ""ununified"" for the training process of different adaptation tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel framework designed to unify LiDAR data into a single target “language” and unified domain detection capabilities across diverse LiDAR datasets, marking a step toward domain unification for LiDAR-based autonomous driving systems. Experiments on dataset KITTI, Waymo, and nuScenes demonstrate the superiority of the proposed method in the task of single-source and multi-sources domain adaptation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The paper is novel in introducing LiDAR Translator (LiT) to joint training across multiple datasets. LiT enables efficient state-of-the-art zero-shot and unified domain detection capabilities across diverse LiDAR datasets. 
2.	The paper is well-written and easy to follow, especially the part explaining the background. 
3.	It presents good experimental results and intuitive visualizations, convincingly demonstrating its effectiveness.

Weaknesses:
1.	The motivation of this paper is not clear. If it is possible to accurately model the target domain data, why is there a need to translate the source domain data into the target domain data?
2.	As the core component of this work, the translator requires more direct experimental validation, such as measuring the distributional differences between the translated data and the target data, rather than solely relying on verification through downstream domain adaptation tasks.
3.	It  lacks of comparative experiments with the latest state-of-the-art methods.

Limitations:
The authors discussed potential limitations about the data, annotation and the category of object.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wblxm5zdkE;"REVIEW 
Summary:
The paper studies online sample selection with individual and interaction 
constraints simultaneously. Specifically, the goal is to control (variants) of 
the false selection rate (FDR) and the expected similarity (ES) under the 
empirical bayes framework. Under distributional conditions, the proposed method 
controls the target quantities asymptotically. The method is evaluated on synthetic 
and real data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-presented and easy to follow.
2. The problem under consideration is of interest and relevant.

Weaknesses:
Please see my comments in the questions section.

Limitations:
The authors have partically addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a method for online sample selection. The authors introduce the concepts of _individual_ and _interactive constraints_, and demonstrate theoretically and empirically that their method satisfies both.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
The problem seems important and the formulation and approach novel. The authors provide both theoretical guarantees and empirical evidence, in both synthetic and real-world applications, of the effectiveness of their approach. The mathematical formulation seems sound, and the assumptions and theoretical results are clearly stated.

Weaknesses:
I am not familiar with the FDR control literature, and had to read parts of the paper (specifically sections 2.2, 2.3 and 4) multiple times to get a gist for the logic of the method and its empirical perfomance. This is reflected in my confidence score. If the paper is accepted, I highly recommend the authors revise the paper to make it easier to follow. A flowchart to illustrate the steps of the algorithm and/or to illustrate the differences between the Oracle and Data-driven selection procedures may be helpful; a toy example could also help. I also suggest including a longer description and/or table of the benchmark methods against which the empirical performance of II-COS was compared.

Limitations:
The authors discuss limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors quantify the uncertainty of response predictions using predictive inference; and systematically addressing individual and interactive constraints in a sequential manner.  An online selection rule is developed to ensure the above two types of constraints are under control at pre-specified levels simultaneously.  Simulated and real-data examples  are used to evaluate and illustrate their approach in terms of both online individual and interactive criteria control.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This is a nicely, clearly written paper that develops an online selection rule that is simple yet effective.  The simplicity of the online selection rule will enhance the potential for  this rule to be used in real life.   The authors’ claims are well supported both via theory, simulations and application to real data. The paper along with the appendix provides detailed information that allow for replicability.  Very nice!  I particularly appreciate the comparison with the approaches based on conformal p-values.

Weaknesses:
See questions below.

Limitations:
Comments are needed on the practicality of \hat\mu being a bijection.  How critical is this assumption to the method and the theoretical analysis.  If critical then this is a limitation.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a framework to perform online sample selection such that the unseen outcomes are in specific target range while also optimizing for constraints like diversity that are dependent on the input covariates. The additional constraint involving input covariates can help ensure properties like the diversity of candidates when selecting individuals for interviews while also guaranteeing that most of the interviewed individuals accept the offer. The paper proposes a data-driven procedure to select the subset of candidates in an online fashion by implementing the proposed algorithm. Under reasonably weak assumptions, the paper provides theoretical guarantees on satisfying both the above constraints in online sample selection. The experiments confirm that this framework ensures low false selection rates (i.e. unseen outcomes are in a specific target range) while optimizing for the additional covariate-dependent constraints like diversity on synthetic and real data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper proposes an intuitive way to incorporate covariate-dependent constraints like the diversity of candidates when performing online sample selection to optimize for metrics like false selection rates. This paper solves an important problem in online sample selection and demonstrates that the proposed method improves the covariate-dependent objective while maintaining comparable performance on false selection rates.

Weaknesses:
It would be interesting to understand the gaps between an ideal diversity profile and the profile obtained by the proposed method in Fig 3. Analysing the gap w.r.t changing g(X_i, X_j) function choice could be helpful. Would it be helpful to increase the weight of the g(X_i, X_j) term to reduce this gap and understand its implications on the satisfaction of individual constraints?

It is evident that the SAST baseline outperforms the proposed method in terms of FSR sometimes, which is understandable given there; 's a tradeoff with the interactive constraints (Table 2b, Fig 1).  It would be helpful to learn if we can reduce the gap between SAST and the proposed method by balancing the tradeoff (perhaps using a tunable hyperparameter that balances the two constraints?).

Limitations:
Yes, limitations and broader impacts are discussed in the last section of paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wbE0QCBWji;"REVIEW 
Summary:
This paper tackles the field of adversarial image generation by proposing an unrestricted attack method that can be applied to both targeted and untargeted attacks. The innovative approach considers a probabilistic perspective, treating the victim classifier and geometric constraints as distinct distributions. By drawing adversarial examples from the overlap region, the authors ensure that the semantics of the original image are preserved. The efficacy of this proposed approach is convincingly demonstrated through extensive experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. I find the probabilistic approach proposed in this paper to be particularly innovative and refreshing. The motivation behind this perspective is clearly articulated, providing a solid foundation for the authors' methodology.
2. I am impressed by the encouraging experimental results presented in this paper. The inclusion of a human annotation experiment is particularly noteworthy, as it adds an important layer of validation to the authors' claims. Moreover, the study's success in handling both transfer attacks and adversarial defense scenarios further underscores the model's robustness and effectiveness.

Weaknesses:
While the experimental results of the proposed method show promise, I do believe there is room for improvement. Specifically, I think it would be beneficial for the authors to provide more detailed information regarding the human experiment methodology, such as how the five reviewers for the MNIST experiment were selected. Furthermore, I would suggest that the authors consider conducting a follow-up experiment where human annotators are asked to identify perturbed images in the absence of a reference image for the ImageNet experiment, which is a more realistic scenario in an attack setting.

In addition, I find it intriguing that both NCF and cAdv demonstrated higher success rates in generating adversarial examples compared to the proposed method, as shown in Table 2. This highlights some shortcoming in the proposed approach. While it is expected that NCF would generate images that can be identified as perturbed, I am more surprised that cAdv was able to create perturbed images that are highly similar to the original ones.

Lastly, I think it would be beneficial for the authors to explore targeted attacks on ImageNet, given the success of this approach in previous papers such as ""Towards Transferable Targeted Attack"". This could provide valuable insights into the robustness and effectiveness of the proposed method.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new type of adversarial attack, which generates adversarial examples by solving a box-constrained non-convex optimization problem. Different from the traditional norm-bounded attacks, this paper focuses on unrestricted adversarial attacks by replacing the geometrical distance measure with a semantically-aware distance measure. Specifically, The authors propose using a Langevin Monte Carlo (LMC) technique to sample adversarial examples from a probabilistic distribution. To preserve semantics, the authors use a learned energy function to guide the generation of adversarial samples. Following this, rejection sampling and refinement techniques are employed to select and improve the quality of the generated samples. Experiments show that this attack can fool classifiers while preserving the semantic information compared to baseline methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper introduces an interesting perspective on generating adversarial examples, which is significantly different from the traditional norm-bounded adversarial attacks. 
2. This paper is theoretically sound and the proposed solution is very intuitive.
3. It is suprising that the proposed attack can achieve a 100% success rate on an adversarially trained model. Adversarial training is often regarded as a SOTA defense method. Therefore, in my view, this work can motivate researchers in this area to design better defense methods. 
4. The proposed method can either outperform baseline methods by a notable margin or significantly improve the quality of the generated adversarial examples in terms of preserving semantic meanings.

Weaknesses:
1. Selecting 20 images from each class in the MNIST test set seems to be too little. I understand that it might be infeasible for human annotators to annotate all adversarial images for the entire MNIST, so I would encourage authors to report the success rate except for human annotations using the entire MNIST. I believe this will make the results more convincing.
2. This paper is missing ablation studies for rejection sampling and sample refinement techniques. Is it necessary to include these techniques? How would it affect the attack success rate if one of them is removed?
3. This paper proposes a new attack method but lacks a discussion on how to defend against it. Although it is not compulsory, I am more willing to see how to defend this attack. Can you provide some intuitions on it?
4. Standard deviations are not reported in this paper. Repeated experiments are encouraged.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a probabilistic framework for generating adversarial examples, focusing on maintaining the semantic integrity of the original images while implementing substantial pixel-level modifications. Unlike conventional adversarial techniques that rely heavily on minimal geometric perturbations, this approach integrates a semantic understanding into the adversarial example generation process, leveraging energy-based models and diffusion models. The core innovation lies in embedding the semantic interpretation as a probabilistic distribution, which guides the adversarial example generation. This allows for effective deception of classifiers, including those equipped with adversarial defenses, while preserving the semantic content to an extent that remains imperceptible to human observers. Empirical evaluations demonstrate that the proposed method outperforms existing techniques in terms of both effectiveness against defenses and undetectability by humans, establishing a new paradigm for constructing robust and stealthy adversarial attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is clear and well-written, effectively highlighting its contributions with accessible explanations of complex ideas.

2. This paper presents a new probabilistic framework for generating adversarial examples that goes beyond traditional norm-bounded methods by integrating semantic distributions. The approach is theoretically robust, with the theoretical analysis providing a solid foundation that supports the model's effectiveness and introduces innovative concepts to the field of adversarial machine learning.

3. The proposed method significantly outperforms baseline methods, particularly in preserving their semantic integrity.

Weaknesses:
1. The assumption in Equation 4 lacks a detailed derivation, leaving it unclear whether $x_{\text{ori}}$ and $y_{\text{tar}}$ need to be independent. Providing a clear derivation and clarifying this assumption would enhance the theoretical rigor of the paper.

2. The training process for the diffusion models is sensitive and requires careful parameter tuning. The paper does not provide enough detail on this sensitivity or potential solutions to mitigate training instability, which impacts the robustness and reproducibility of the method.

3. The paper does not report standard deviations in the performance results. Repeating the experiments is recommended to ensure the reliability and consistency of the findings.

Limitations:
Paper limitations can be found in the above comments.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
waQ5X4qc3W;"REVIEW 
Summary:
Latent-based image generative models, such as LDMs and MIMs, have achieved success, but autoregressive models lag behind in image generation. Our research introduces a unified perspective on latent space stability and proposes a discrete image tokenizer, DiGIT, that significantly improves autoregressive image modeling, outperforming LDMs and benefiting from scaling similar to GPT in NLP.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The results beat some baseline models, though under a specific (and somewhat confused) experimental setting.
- The topic of latent space property is worth investigating.

Weaknesses:
The paper has several weaknesses:

1. **Factual Errors**:

    1.1. The cited MIM models, such as MaskGIT and MAGE, cannot revise previously predicted tokens. This contradicts the claim in line 53 that ""iterative models like LDMs and MIMs can correct errors."" I recommend the authors to their papers for more details.

    1.2. In lines 72-73, the authors state that this work provides ""the first evidence that image autoregressive generative models behave analogously to GPT."" However, the Parti[1] paper has already demonstrated that image autoregressive models have similar scalability to GPT and successfully scaled the model to 20B. The authors have not cited this work.

2. The writing is poor and lacks rigor. For example, the discussion on the so-called ""common misconception"" in line 41 is not well-supported. What exactly is meant by the ""optimal latent space for reconstruction""? How many studies hold this view? There are no citations provided.

3. The quantitative comparisons are also peculiar. The authors cite many paper results without using CFG, while CFG has become a de-facto practice for augmenting generative models. Why not adopt CFG and perform more apples-to-apples comparisons to other SOTA methods with CFG?

4. Presenting two tables (Table 2 lower and Table 3) for image generation performance is confusing. Why not consolidate the results into a single, clear table?

[1] Yu, Jiahui, et al. ""Scaling autoregressive models for content-rich text-to-image generation."" arXiv preprint arXiv:2206.10789 2.3 (2022): 5.

Limitations:
The writing & presentation of this paper seems too rush and lacks rigor. I recommend the authors to refine and polish this paper. The current draft may not be qualified for the publication of NeurIPS.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper designs a better quantized autoencoder on top of VQGAN. It builds an image autoencoder which is able to both achieve good recognition performance for linear probing, and have a latent space which is suitable for training a generative model. It studies the existing autoencoders from a high-level theoretical perspective and proposes design ideas which are targeted to improve them. The paper claims that the modern autoencoders ignore the fact that they will be utilized for downstream generative modelling tasks and mainly focus on reconstruction. The paper argues that adding recognition losses on top of the encoder would help. To fulfill this desiderata, the model takes DINOv2 features and discretizes them via k-means. Then it trains a translation model into VQ-GAN decoder features. For image generation, it trains a LLM in the discrete token space. For classification, it does linear probing on top of discretized DINOv2 features. As a result, it attains reasonable generative capabilities while being able to keep a latent space suitable for linear probing classification.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- In terms of the scores, the paper achieves very good results in the sense of discrimination/generation tradeoff (judging by figure 1).
- It's an interesting finding that one can discretize dino-v2 via K-means and train a strong generative model on top of such tokens.
- The paper studies an important problem of more rigorous understanding of modern autoencoders

Weaknesses:
- The paper shows an equivalence between a linear AE and PCA, but it's a well known fact: https://arxiv.org/abs/1804.10253. One can also just google ""equivalence between a linear autoencoder and PCA"", and find a ton of resources on that.
- ""A reconstructive autoencoder does not necessarily establish an advantageous latent space for generative model"". That's a very well-known fact in the community (e.g., see Fig 5 in https://arxiv.org/pdf/2312.02116). The paper should not claim this observation as a novel contribution.
- The proposed stability metric is interesting, but it's unclear whether it will correlate with downstream generative models performance
- Proposition 2.4 is extremely vague and seems to be very different from its ""rigorous"" analogue in the supplementary.
- FID metrics for VQGAN on ImageNet are much higher than in the original paper.
- It's delusive to compare performance of the developed model vs those trained from scratch, since the developed model starts from strong pre-trained models.
- For image generation, the paper shows just 16 random samples, which is extremely little to get a good understanding of the model. It's better to show much more (e.g. stylegan2 provides 100k samples for FFHQ: https://github.com/NVlabs/stylegan2).
- Why DiT-XL/2 is included for comparison but not its guided variants? Why more recent papers are not included for comparison? (e.g., EDMv2).
- The logical transitions in the paper are unclear, e.g., it's unclear why the proposed training improves D^G, it's unclear, why it follows from the propositions that we should improve the stability of the latent space (where stability is also not defined well), etc.

Limitations:
One limitation that is not explored is whether the model is upper-bounded by the performance of the underlying SSL classifier. In other words, what would be the greater source of improved performance in the future — improving SSL or decoder?

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to understand why latent autoregressive image models perform worse than latent diffusion models. The key insight is that existing tokenizers are trained primarily with the reconstruction objective, whose latent space is unstable and thus may not be easy to model autoregressively. To solve this issue, the authors propose first learning a stable latent space, which autoregressive models can model easily, and then learning to reconstruct pixels from this latent space. Experimental results show that this modification enables latent autoregressive image models to match latent diffusion models' performance in terms of image understanding and image generation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper proposed a new perspective—latent space stability—on understanding latent autoregressive image models, which was neglected in previous works. I think this explanation is intuitive since a fixed-depth autoregressive model may not be able to model very noisy distributions (e.g., the language data have high regularity)
2. The proposed solution is straightforward -- just let image features with similar semantics share the same token.
3. The experiments are comprehensive and interesting. Both image understanding and image generation are evaluated; improvements over previous latent autoregressive models are significant. The ablation study also makes sense to me.

Weaknesses:
1. I think there is a tension between how stable the latent space is and how easily we can reconstruct the latent codes to pixels. The impact of the proposed method on reconstruction is not elaborated in this paper. For example, if we only care about reconstruction, how badly does the proposed method perform? This matters greatly if we are modeling high-resolution images and care about the visual details.
2. The theoretical analysis and the proposed algorithm seem loosely connected to me -- I don't see the proposed algorithm as a direct result of the theoretical analysis. The stability analysis is more straightforward, though.

Limitations:
I think the authors adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper propose to disentangle the encoder and decoder learning for image tokenzier which ultimately will be used for providing the latent space of AR generative model. In particular, SSL model such as DinoV2 is used for encoder (plus k-means clustering).

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
1. The idea of disentangling the encoder and decoder learning for image tokenizier is interesting and novel.

2. Strong empirical results can be obtained from the method. The fact that by changing a tokenizer and training the same AR model, FID can be halved to half is really impressive.

Weaknesses:
1. The motivation for adopting self-supervised model as encoder/tokenizer is not very clear. Since the method is easy (DinoV2 + kmeans), the motivation of why doing so is the most critical part of the paper. However, I don't think this is presented very clearly and explicitly. Large improvements of the presentation is needed. 

2. The term ""stability"" or """"stablize"" is a bit confusing. Explicit explanation is needed. When is a latent space not stable? If it means hard to learn an AR model, probably a better term such as learnability is better. 

3. While the argument of ""iterative decoding process can stabilize the sampling process by correcting the data falling in the low-density overlap between distributions"" makes sense, it still requires justification and evidence, not just conceptual analysis.

4. If you use SSL model as encoder, you need to train a decoder. Not much explicit detail is presented for this part.

5. The metric is not very clearly defined. What's the name of the metric? What is the definition? How to compute it? All these information should be highlighted.

Overall the presentation and organization is not very clear, some major rewrite is needed.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wZigMVFURk;"REVIEW 
Summary:
The preprint proposes to replace the collocation based PINN loss by a sum of local continuous integrals over regions around the collocation points. These continuous integrals are then again discretized using Monte Carlo integration with a single quadrature pint. The authors furthermore propose to adapt the region size during training using gradient statistics.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors report good empirical performance on a number of benchmark problems.

Weaknesses:
The introduction of continuous integrals over regions around the collocation points that subsequentially are discretized by Monte Carlo integration again seems tautological. After all, the loss function in PINNs is already a Monte Carlo discretization of a continuous integral (over the whole computational domain). Furthermore, the analysis that the authors present for the modified loss in equation (5) should not be carried out with the continuous integral over the regions $\Omega_r$ but with its Monte Carlo approximation. Otherwise, the comparison to the discretized PINN loss is unfair.

Limitations:
See questions

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper extends the optimization process of PINNs from isolated points to their continuous neighborhood regions, which can theoretically decrease the generalization error, especially for hidden high-order constraints of PDEs. A practical training algorithm, Region Optimized PINN (RoPINN), is seamlessly derived from this new paradigm, which is implemented by a straightforward but effective Monte Carlo sampling method. By calibrating the sampling process into trust regions, RoPINN finely balances sampling efficiency and generalization error. Experimentally, RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of extending the optimization process of PINNs from isolated points to their continuous neighborhood regions is novel.
2. Theoretical results on generalization error, convergence rate and estimation error of sampling are provided. 
3. A practical training algorithm, Region Optimized PINN (RoPINN), is seamlessly derived from the region optimization paradigm and associated theoretical results,
4. RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation.

Weaknesses:
1. It is better to include the main proof idea of theoretical results in the main text.
2. Although generalization error bound is provided, an intuitive explanation of the reason behind the success of region optimization is desirable. For example, when sampling one point in each region, why is the total loss decreased compared with point optimization?

Limitations:
The case of sampling more than one points in each region is not discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors developed a region optimized PINN to improve the prediction accuracy compared to the scatter-point based PINN.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors proposed the region optimization paradigm and conducted a theoretical analysis.

Weaknesses:
The practical application scope is limited.

Limitations:
Some initial guess methods can be developed to efficiently determine the preferable region size and sample number.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel optimization method for training physics-informed neural networks (PINNs): Region optimization, which extends a regular pointwise optimization of PINNs to neighborhood regions, named RoPINNs. The paper provides theoretical analysis explaining the decrease of generalization error with RoPINNs and high-order PDE constraints satisfaction. Then the paper presents a practical algorithm to enable the RoPINNs paradigm by introducing Monte-Carlo approximation of region integral and a region calibration scheme. Lastly, the paper assesses the performance on several well-known benchmark problems and showed the improved performance over the considered baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-motivated and tackles the important problem in training PINNs (leveraging more information than just a point-to-point type mapping).

- The paper presents theoretical analysis on benefits of RoPINNs, decreased generalization errors and satisfaction of higher-order PDE constraints.

- The experimental results show that the proposed algorithm is effective in solving some challenging benchmark problems (known as failure modes) and is capable of producing more accurate solution approximates.

Weaknesses:
- Although shown to be very effective in several benchmark problems, the paper does not seem to provide general guidelines on how to set some important hyper-parameters such as initial region size and the number of sampling points. (While acknowledging that the authors indicate this as one of the limitations,) it would be great to see some experts’ guidelines.

- If the authors could provide some analysis with regards to computational wall time, that would provide more complete pictures on how the proposed method performs. For example, it would be information to see a figure depicting a scatter plot showing computational wall time versus final rMSE type information, where a point in the plot corresponds to a different hyper-parameter setting (that is, the number of sample points). 

- [minor] there is a typo in the second paragraph of Section 4.2: line 289 Figure 2 => Figure 3.

Limitations:
- some more discussions on practical guidelines would be needed for users who want to utilize this method in different applications 

- some additional experiments (regarding computational wall time) would be needed to provide a complete picture of the proposed method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wZgw4CrxwK;"REVIEW 
Summary:
The authors formulate a theoretical setup for a LLM text generation service to incentivize the service to output high quality text the consumer. The authors formulate this setup as having the service having a set of models that has quality (as rated by a evaluator on the end of the consumer) that increases with the cost of running the LLM. The goal is to derive a framework for paying the LLM service based on that quality of the text generated that incentivizes the LLM service to always use its best model. The authors go about this by formulating the definition of a contract in this setting, and defining various metric (max payment, avg. payment, etc.) that consumer aims optimize. They show that the set of contracts that will incentivize the LLM service to output text with the best model can be derived from the set of optimal hypothesis tests that distinguish which model is being used from the evaluator outputs. They derive how the optimal contract can be formulated from these hypothesis tests, when only bounds are known on the costs of running each model for the LLM service.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The theoretical setup and monotone assumption of model performance, cost, etc. is quite reasonable and tackles and interesting and relevant problem with LLM queries. The results are simple and intuitive, and connect nicely with previous work on contract theory and hypothesis testing.

Weaknesses:
The main issue is the theoretical setup does require an assumption that the bounds on cost are known, which seems somewhat impractical. It might be useful to explicitly comment on how the different metrics degrade with increased looseness of the cost bounds (linearly, it seems like), since one can always pick extremely conservative cost bounds.

Minor issue:

- In Definition 3, it would be helpful to illustrate why $B_R^*$ and $B^*_\rho$ are defined as they are, correctly. Further, the definition with $b \geq c_n$ is a bit strange, since the definition of minmax hypothesis test does not involve worst case over cost vectors, so it doesn't seem correct to use $b$ to derive the minimax contract (and instead it should remain a function of $c_n - c_1$) --- maybe dropping that $b \geq c_n$ case would be more accurate, since it is used in the definition of cost-robust later.

Limitations:
I think the limitations are sufficiently addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issue of moral hazard in pay-per-token pricing for large language model (LLM) services. Firms may use cheaper, lower-quality models to cut costs, compromising text quality. Moreover, the firms's costs may be unknown to the clients. To counter this, the authors propose a pay-for-performance framework using cost-robust contracts that incentivize high-quality text generation under the uncertainty about the firms's costs. These contracts are designed based on and have a one-to-one correspondence to optimal composite hypothesis tests. Approximation guarantees are provided with respect to the optimal contracts. Empirical evaluations show that these contracts provide robust incentives with small cost increases.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The results of characterizing the forms of optimal cost-robust contracts using hypothesis testing, as well as the approximation guarantees, are interesting and have valuable contributions.

Weaknesses:
1. The model's complexity is unnecessary. The problem could actually be studied in the most basic contract setting. 
2. The authors do not discuss the computational complexity of finding the optimal cost-robust contract. 
3. The authors do not discuss the cases where the action with the highest cost may not be the best action to incentivize.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
* The paper concerns the problem of incentivizing LLMs to use the most costly model, which is assumed to be the model with the best performance. Without proper incentive, the LLM company has the incentive to charge customers for the highest payment, but deliver the service using a lower-cost model, because the performance of the model is usually not verified. Therefore, the paper proposes to use contract design to solve this problem. In particular, an automatic detector first gives an integer score for the performance of an LLM. Then, the task is to design a payment for the company for each of the integer scores. The goal is to minimize the total payment conditioned on incentivizing the best-performed model. 
* The main contribution is the discussion of the cost-robust contract, meaning how to design the optimal contract while the costs of LLMs are unknown. Empirical evaluations have shown how to use the theory in practical settings given LLM performance data.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
* (I’m not an expert in contract design.) I appreciate the theoretical contributions of the paper. To me, section 4 has several interesting insights into connecting cost-robust contracts with hypothesis tests. As claimed, this is the first paper considering cost-robust contract design. However, the real contribution should be better evaluated by experts.
* In general, incentive issues of LLM uses have been very critical and challenging. I also like the connection between contract design and the production of LLMs.

Weaknesses:
Although I believe contract design can speak with the production of LLMs, I’m not fully convinced that the proposed model is a good idea to solve the considered problem. 

* In practice, each company pricing its own AIs, so who should be the principal? In other words, the paper assumes there is a trust-worthy third party who can run the quality-detector and commits to a contract with the LLM companies. I’m not sure this is feasible in practice. I hope the authors can explain more carefully the application scenarios of their theory.
* Furthermore, there is no evidence in the paper (and, I guess, on the Internet) that can prove LLM companies are cheating about their service quality. I also don’t think this is very likely because LLM companies have other incentives to provide high-quality services, e.g. their reputation. So, how do we know we are not solving a problem that does not exist?
* Even though we go with the assumption that there is a contract that the company (agent) agrees on, I doubt cost-robustness is the first-order concern. The cost data is usually publicly obtainable from energy reports, as the authors did in their experiments. Even though this data is not public, the energy cost is usually easy to estimate. Therefore, I don’t think incentivizing LLM production is a suitable application for cost-robust contracts.

Limitations:
Limitations are reasonably stated.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wWyumwEYV8;"REVIEW 
Summary:
The authors aim to investigate spurious correlations learned by CLIP models. For this, they curate a novel dataset where animals are organized into common and uncommon backgrounds, e.g. a polar bear is more likely encountered in snow than on grass. The authors then perform experiments where they benchmark various CLIP and ImageNet models on the curated dataset. They observe that CLIP models suffer from spurious correlations which stem from changing the background.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
I think the issue of spurious correlations is important and one needs to understand how and whether VLMs learn spurious features. The paper presents many experiments and shows that scale or backbone capacity do not improve the effective robustness on CounterAnimal which is interesting.

Weaknesses:
The paper has many issues, both in terms of writing and the methodology which need to be fixed.

### Major:
**The authors missed important previous works**: The paper “Recognition in Terra Incognita” is very related to this work and also proposes a “dataset designed to measure recognition generalization to novel environments” based on camera traps. The dataset is sorted according to difficult environments for different animals, which makes it very similar to CounterAnimal. I think the authors need to cite and discuss this paper. Currently, I do not understand the benefit of having a new dataset in addition to the already present one. The waterbirds dataset is also highly similar and should be discussed (https://arxiv.org/pdf/1911.08731). The authors cite that paper, but do not discuss it in the Related Work section, nor put it into context with CounterAnimal. The backgrounds challenge (https://github.com/MadryLab/backgrounds_challenge) is also highly related and should be discussed. In general, the related work section is very weak, given how extensively spurious correlations and worst-group-accuracy have been studied. Another important work to be discussed would be ""Finding and Fixing Spurious Patterns with Explanations"" (https://arxiv.org/abs/2106.02112).

**The naming of the common vs counter groups is misleading**: 
Line 165: “Photos with the highest CLIP accuracy are assigned to the common group, and those with the lowest CLIP accuracy are assigned to the counter group.” I have a major understanding issue here. As far as I understood the paper before this line, the goal was to put images with common backgrounds into the common group and images with uncommon backgrounds into the counter group. This is also depicted in Fig. 1 or Table 1. The caption in Fig.1 says that “Most ice bears appear in a snow background (i.e., common), while it also is reasonable to find some ice bears in a grassy environment (i.e., counter)”. But here, the authors write that accuracy has actually been used to separate images into these groups? But then the frequency of the co-occurrence of certain backgrounds and classes has not been taken into account, or rather, it is a conjecture that those backgrounds where the CLIP model has higher accuracy on are more “common”? 

**The terms ""effective robustness"" and ""robustness"" are used interchangeably which is wrong and confusing**:
I think the paper conflates the terms “robustness” and “effective robustness” which is confusing. When looking at effective robustness plots, such as in Fig. 2, we are interested in the residual difference between the measured value and the value predicted by the linear fit. As I can see, all plotted markers (CLIP and ImageNet) lie on their respective linear fits, and none of the interventions, such as CLIP-DC or CLIP-DFN offer any effective robustness benefits. It is though true that the **absolute** robustness numbers are overall higher for the CLIP-DFN models, for larger models or models trained on more data. I am however confused by the authors discussion of this observation. On the one hand, they write that larger CLIP models are more robust but increasing the dataset size does not yield improvements. First, I am confused whether they mean “effective robustness” or “robustness” here. Second, I do not see the effect the authors are describing: Both more data and larger backbones have higher absolute robustness but the same effective robustness as the other models. The statement “CLIP models trained on high-quality data are more robust” is also confusing, because it is not clear whether “robustness” or “effective robustness” is meant. 

**Due to methodology issues, results on CLIP models cannot be compared to results on ImageNet models (or other advanced LVLMs):**
Line 60: “d) Spurious discovering: preserving classes and associated data based on the decrease in zero-shot performance (i.e., evaluating based on pre-trained CLIP models without fine-tuning) when shifting the backgrounds.” This step is really unclear. Do the authors curate the dataset based on the zero-shot accuracy of a CLIP model? From the introduction and the abstract, it sounds like the authors want to benchmark the robustness of CLIP vs ImageNet models on this custom dataset. But then, it is strange that CLIP models also seem to be used during the curation process. After reading the more detailed description in line 156, I think the statements made in line 85 are misleading. The authors write “ImageNet models are more robust to spurious correlations captured by CounterAnimal” and “Compared with CLIP models (colored in blue), surprisingly, we find that ImageNet models exhibit a stronger robustness to the spurious correlations in CounterAnimal.” Given that CounterAnimal has been curated based on the performance drop of a CLIP model, I find it very unsurprising that CLIP models perform worse on it compared to ImageNet models. I think that if CounterAnimal had been curated based on an ImageNet-trained ResNet50, the trend would have been reversed. I think all statements comparing CLIP and ImageNet trained models on CounterAnimal need to be relaxed and I think that this comparison is quite meaningless because of the described selection bias. I think that the whole Section 3.3. is misleading for this reason and statements such as the following cannot be made given the methodology issues: “Surprisingly, we find that ImageNet models are more robust to spurious features in the CounterAnimal dataset. This finding may contradict the common belief [Radford et al., 2021, Shi et al., 2023] that the CLIP models tend to be more robust to spurious correlations than single-modal supervised learning.” Similarly, the conjecture paragraph from line 265 onwards is wrong and cannot be made. 

For the same reason, the comparison to advanced LVLMs in line 273 onwards cannot be made.

Figure 1: Are these examples cherry-picked or are they representative of the data points present in CounterAnimal? I am asking this, because of the Winoground dataset [A]. This dataset tests the compositionality of VLMs by forcing a model to match two captions to two respective images. Winoground has later been criticized because the two images in the choice process are not equally hard [B]. For example, the model needs to match “the glass is on the grass” and “the grass is in the glass” to the corresponding images. However, there is much more grass in the image matching to the first caption, and the model likely picks that image for both captions just because there is more grass and it makes the decision in a bag-of-words-manner. To summarize, Winoground did not control for object size, orientation and other confounders. In Fig.1, it appears that the main objects (the polar bears) are equal in size, so size could be excluded as a possible confounder? Did the authors consider this possibility, i.e. that the drop in performance could be explained by other differences in the images from the respective domains?
[A] https://arxiv.org/abs/2204.03162
[B] https://arxiv.org/abs/2211.00768

### General:
Line 25: please cite CLIP

Line 64: “The resulting dataset covers a total of 45 animal classes, ends up with 7,174 common photos and 5,926 counter photos, aligning with the standard size as an evaluation dataset [Recht et al., 2019, Hendrycks et al., 2021].” -> I do not understand this statement. Different test sets have different numbers of test images. ImageNet Val has 50k images for example. In what sense are the presented numbers standard?


Line 94: “Overall, larger CLIP backbones (i.e., larger markers) can improve the effective robustness, implying that scaling up backbones may enhance robustness against spurious features.” -> I do not see this in Fig. 2. The larger markers appear to be on the fitted line, same as the smaller markers. Effective robustness measures the difference with respect to the linear fit, and there is none for the larger CLIP backbones. Please clarify this point.


Line 146: “feature noise involves severe feature corruptions” -> Please be more specific here. What do you mean with feature noise? Do features refer to animals features such as missing ears or such? Or to the images themselves?

Line 147: “clarity issues arise when animal objects are not in major positions” -> unclear formulation: what is a major position? Do the authors mean that the animals are too small or not in the center of the image?

Line 153: “Note that the class space of backgrounds as above is not entirely orthogonal with each other due to the inherent ambiguity of the real-world situations. Nevertheless, we try our best to discern the assigned background labels within each animal class.” -> This is unclear. How many images would be ambiguous? I could imagine that many images would have two backgrounds, such as e.g. grass and sky or snow and water. For example, the last image in Fig. 1 on the left has both snow and water. It is not clear to me that only picking the snow background and ignoring the water is correct here. Further, at least for CLIP, the caption can contain several background keywords.

Further, I imagine animals occur in all kinds of environments, but there are only two backgrounds for each animal. Were the other images also discarded?

Line 214: “Therefore, we conclude that our CounterAnimal dataset possesses some realistic shifts that are generally contained in large-scale pre-training data, regardless of backbones.” This conclusion cannot be drawn from this experiment since the backbone has not been varied here.

### Section 4:
The proposed experiment is very similar to the well-known ShiftMNIST [D] or ColoredMNIST [E] datasets, which test the influence of spurious correlations. The findings here are not novel and should be brought into perspective with previous work. I do not understand how Fig. 11 relates to the text. What is “supervised”, “obj”, “objbkg”?
[D] https://arxiv.org/pdf/1811.00401
[E] https://arxiv.org/pdf/1907.02893

### Typos, grammar:
The quality of the text is poor on some occasions which makes reading and understanding the paper difficult. The manuscript would benefit from a round of proof-reading. Some statements and formulations should be made more precise.
Line 32: “The performance boosts over ImageNet models seem to suggest that CLIP resolves distribution shifts and thus spark a rich discussion about its rationale.” Strange formulation. How can “distribution shifts be resolved”? Please rephrase for clarity.

Line 112: “More specifically, [Yang et al., 2023] report that CLIP models may misaligned frequently co-occured objects with the corresponding texts.”

Line 115: “[Tong et al., 2024] find that CLIP misaligned samples will further cause the hallucination of LVLMs.” I do not understand this statement, grammar errors.

Line 132: “Meanwhile, many existing datasets, e.g., DomainBed and Wilds, do not have overlapped label space with ImageNet, making the comparison between ImageNet and CLIP models hard.” There is a version of DomainBed [C] where the dataset has been filtered to only include classes compatible with ImageNet, such that an evaluation of ImageNet models is possible out-of-the-box.
[C] https://openreview.net/pdf?id=LiC2vmzbpMO

Line 171: “Recalling that, when CLIP models resort to the shortcut of data, the model performance will heavily correlate with the backgrounds presented in the common group yet is compromised when coming to the counter group.” Grammar errors, I do not understand this sentence. What is “the shortcut of data”?

Line 208: “It suggests that the CounterAnimal dataset captures some general spurious shifts that at least commonly present in the pre-train dataset of LAION400M.” grammar

Line 213: “Here, the spurious features degenerate the zero-shot robustness of CLIP models trained on both LAION2B and by OpenAI.” Typo? “degenerate”?

Line 243: “In Figure 7, we consider two pre-train datasets, namely, LAION2B and the close-soured data from OpenAI” typo

Line 297: “Nevertheless, in the following theorem, we justify that CLIP remains learning to use spurious features, aligned with our experimental observations on the CounterAnimal dataset.” grammar

Strange space break between line 310 and 311.

# Summary of the review:
We could fix the naming convention from ""common"" and ""counter"" to something like ""hard"" and ""easy"" since accuracy has been used rather than frequency of certain backgrounds to classify backgrounds into certain groups. Based on my arguments below, I believe we cannot compare CLIP models to ImageNet models on the proposed dataset in any sensible way due to the introduced selection bias. I believe the very title of the paper is misleading since the posed question cannot be answered based on the methodology issues. But if we remove the claims about comparing ImageNet models and CLIP models, then, the main point of the paper is that there exist backgrounds which are harder for CLIP models, given certain classes, and other backgrounds which are easier. I don't think that this observation is particularly interesting on its own. The authors did not relate the hardness of the backgrounds to their frequency in the pretraining dataset or anything else. The observation that backgrounds matter is also not novel but quite well-known and the authors do not offer a solution. Further, the writing is quite poor and confusing on many occasions; I provided many examples of incorrect and confusing sentences below.

Limitations:
The limitations discuss the comparison in performance of the CLIP vs ImageNet models which I believe cannot be made due to the methodological issues in this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents CounterAnimal, an evaluation dataset featuring two subsets: animals with common backgrounds and those with unusual backgrounds. The images were sourced from iNaturalist. Data with high CLIP accuracy are categorized as ""Common"",  while those with low CLIP accuracy are labeled as ""Counter"".  Results shows that CLIP models experience a greater accuracy drop compared to ImageNet models when tested on this dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper analyzes multiple factors affecting CLIP accuracy, including model size and training data quality.
- The paper combines both experimental results and theoretical analysis. The analysis in Section 5 is interesting and novel.
- The paper is well-written and easy to follow.

Weaknesses:
- The proposed dataset is not sufficiently robust to analyze the influence of spurious bias, as this is not the only difference between the common and counter datasets.
  - To analyze the accuracy drop caused by spurious features such as background, the background should be the only difference between common and counter image pairs. Prior work [4,5] has proposed such datasets focusing on background.
  - In the proposed dataset, other factors may influence the model accuracy gap besides background. For instance, as shown in Figure 1, the more varied gestures of ice bears on the right compared to the left could be a contributing factor to the accuracy drop.

- Current experiments cannot conclusively show that ImageNet models generalize better than CLIP.

   - As the common and counter groups are selected according to the CLIP accuracy (see line 165 in the paper),  they indicate easy and hard samples for CLIP.    Since ImageNet models have different training characteristics, it is natural that hard cases for these models may differ from those for CLIP, resulting in a smaller performance drop for ImageNet models. This result cannot support that ImageNet models are more robust than CLIP models.
   -  The accuracy drop from common to counter group can be greatly influenced by the model used to divide the common and counter dataset. Using the combined proposed common and counter dataset, a new Common' and Counter' dataset can be created based on the accuracy of ImageNet models. What is the impact of this dataset division on the accuracy drop for different models?


- Prior studies[1,2,3,4,5,6] have proposed datasets specifically to analyze the influence of background, which are not discussed in this work.  These datasets can be used for CLIP evaluation as they do not overlap with the CLIP training set. Additionally, creating datasets based on model accuracy in this work is similar to the approach in [6].

[1] Noise or Signal: The Role of Image Backgrounds in Object Recognition.

[2] Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models, NeurIPS 2019.

[3] Dataset Interfaces: Diagnosing Model Failures Using Controllable Counterfactual Generation.

[4] ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing, CVPR 2023.

[5] LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images, NeurIPS 2023.

[6] ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object, CVPR2024.

Limitations:
This paper has discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors create an evaluation dataset comprising two groups, one with animals in usual backgrounds (common group) and another with unusual backgrounds (counter group). They then evaluate a suite of models of different backbones, model sizes, and datasets. They find that CLIP models do poorly than ImageNet-trained models, and generally high quality data or bigger model size improves counter group accuracy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The CounterAnimal dataset is a nice contribution that can be of value to the community.
2. The authors have evaluated a number of models on the dataset and that too could be of value to the community.

Weaknesses:
Please see questions for more information.

Limitations:
The authors have addressed some limitations. For the rest, please see my questions block.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work asks one interesting question: ""Do CLIP models always generalize better than ImageNet models?"" Driven by this question, this work proposes a new benchmark dataset named CounterAnimal. This dataset consists of a) the common group: comprising animals in common backgrounds, and b) the counter group: including animals in plausible yet unusual backgrounds. The main idea is that the performance drops from the common to counter groups quantify the reliance on spurious background features for animal predictions. The main observation is that CLIP models exhibit notable performance drops when tested on the counter group. In comparison, ImageNet models can be more robust than CLIP models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It is always good to see a new and novel dataset proposed for evaluating CLIP and ImageNet-trained models. The proposed dataset CounterAnimal is complementary to existing datasets that cannot reflect the robustness of CLIP models to spurious correlations. 

- The dataset construction is well-presented. The statistics, curation, background labeling, and spurious discovery are well introduced in Section 2

- The analysis around spurious correlation is good. This work tries to give insights from several aspects, such as pre-trained datasets, scaling up, and learning paradigms. The observations are sound to me.

Weaknesses:
-  I found the analysis of why CLIPs rely on spurious features interesting. However, I think the claim is somewhat ""obvious"": there exists a relatively strong correlation between the object captions and the parts of image backgrounds, CLIP will learn to align the backgrounds, i.e., spurious features. If the training dataset contains many examples of spurious correlations, then models will tend to be biased.

- I am curious about why ImageNet models may not be so influenced by the spurious bias in CounterAnimal. Is this because the ImageNet training set does not have too many spurious correlation examples? Or ImageNet has a spurious bias but such bias is different from the one in CounterAnimal? Please provide a discussion or share some insights on this question. 

- This paper adopts absolute performance drop in Section 3.3. Such a metric may not be so robust. For example, model A drops from 40 to 39, and model B drops from 90 to 89. They drop the same but the I would say model B is better. Please comment on this, and discuss the metric of absolute performance drop.

Limitations:
The dataset is proposed, so a discussion on the potential bias/ privacy is needed. I appreciate this work highlights the future improvement of expanding semantic scope, data source, and ImageNet testbed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
wWiAR5mqXq;"REVIEW 
Summary:
The paper introduces COPPER, a novel framework designed to enhance collaboration in multi-agent systems using a learnable self-reflection mechanism. COPPER utilizes a shared reflector fine-tuned to adjust actor model prompts via a counterfactual PPO mechanism. This approach includes counterfactual rewards to address the credit assignment problem and enables the reflector to customize reflections based on agent roles, optimizing computational resources and training stability. The framework's efficacy is validated through experiments in multi-hop question answering, mathematics, and chess, demonstrating improved reflection capabilities and generalization across various actor models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is clearly written and explores a new setting — multiagent reflection. It also show improved performances on all three tasks. Using counterfactual rewards to perform PPO training sounds straightforward.

Weaknesses:
- My main concern is this paper involves a combination of various components and I could not clearly infer from the paper which part is most important. This makes the improvement for each part look marginal. Generally, this paper proposes a novel training method to enhance reflection, as well as use reflection-based multi-agent discussion to improve agent reasoning. I believe the method could be directly applicable to single agent scenario as reward for each agent is updated independently. Could you perform ablation in terms of single-agent?
 
- The test scenario focuses on single-step tasks, can this framework be applied to multi-step agent tasks like AlfWorld?

- How is the performance of COPPER compared to shared parameter & loss training for all LLMs?

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a multi-agent reflection framework COPPER to solve reasoning tasks on several datasets such as HotPotQA, GSM8K, and Checkmate in One Move. The two main contributions are:
1. designing counterfactual rewards to alleviate the credit assignment problem;
2. training a shared reflector to personalize the reflection for each agent.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Novelty: The paper introduces counterfactual rewards from RL to LLM agents, to deal with the credit assignment problem in multi-agent cooperation.
2. Soundness: The authors conducted extensive experiments to thoroughly analyze the proposed mechanism.

Weaknesses:
1. The motivation of the shared reflector may not align with reality. Embodied scenarios do not allow complete information sharing with a central reflector.
2. The computation of counterfactual rewards can be very high. Every agent demands two times of simulation to calculate the rewards, and the computational costs could be much higher when the number of agents increases.
3. The claims of personalized reflection may not be completely conducted. For the Cooperative Debate Paradigm, there are no roles for the debaters.

Limitations:
The efficiency of data collection and the length of reflection memory may limit the application of the method.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes COPPER to enhance the collaboration ability of multi-agent systems through a learnable self-reflection mechanism. It involves reflections from different agent-specific profiles. The contribution of each agent-specific reflector is measured based on their marginal reward. This reflector is shared among agents and generates personalized reflections according to agents' roles. Experimental results on several datasets demonstrate its effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper explores the reflection on multi-agent collaboration. Previous work on reflection mainly focuses on a single LLM, ignoring the complex environment and interaction in the multi-agent system.
2. The introduction of the counterfactual reward in PPO training assigns the reward to rate each agent's reflection, helping the credit assignment problem.
3. The comprehensive analysis of the counterfactual reward, the shared reflector, and different LLMs for reflectors provide a deep insight into the proposed method.

Weaknesses:
1. Including the Retroformer under the multi-agent setting as one of the baselines would be better.
2. When the environment provides a sparse reward such as the credit for the reflection of different agents may become very similar. For example, the removal of all reflections may result in a counterfactual reward of 0 because both trials fail. Then the counterfactual reward may degrade to the episode reward in Retroformer.
3. With the complex PPO training, COPPER's performance is not very impressive, especially when the trial is small (in GSM8k and Checkmate in One Move  of Figure 4 and Figure 8)

Limitations:
The authors discuss their potential limitations in their paper

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wQiJNyPENt;"REVIEW 
Summary:
The paper introduces a new approach to batch Bayesian optimization that explicitly trades off between exploration and exploitation via energy and entropy terms. The method is able to efficiently generate large batches for evaluation that outperform comparable methods for Bayesian optimization.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The method is novel and well-motivated. 

I found the analysis in Appendix B to be especially strong, in which the proposed method BEEBO is compared to other methods for batch Bayesian optimization. This analysis shows the originality of the method and gives strong context for it.

The paper is clearly written and easy to read. The appendix was especially useful and had many valuable parts.

The analysis of heteroskedastic noise was great to see and shows a useful and understudied setting where the method is especially valuable.

Weaknesses:
I think the method is interesting and will be of value to the field. However, I do not find that the experimental evaluation of the method provides full support for the claims of the paper.

**Issue 1: Evaluation limited to large-batch setting**

The major issue is that the paper claims the method is for general batch Bayesian optimization problems, without any qualifiers that I can see. The experiments all use q=100, which is a large batch size. Smaller batch sizes are often of interest too, e.g. batch sizes of 5, 10, and 50 in the GIBBON paper. The setting q=100 used here is also used in the TURBO paper (Eriksson et al. 2019) where it is described as a ""large batch size."" 

Given the experiment results in the paper, I don't know if this method will perform well for small- or medium-sized batches. Thus, either the experiments need to be expanded to include experiments with batch sizes such as 5 and 10, or the framing of the paper needs to be adjusted to emphasize that the method is specifically for large-batch Bayesian optimization, not general batch BO problems.

This issue also relates to the choice of baselines. The experiments only explore large-batch settings, where GIBBON (as the paper notes) is known to perform poorly. If the paper wants to claim that it performs better than GIBBON in general, then it needs to make that comparison on batch sizes of 5 and 10. If the paper wants to claim superiority only on large-batch settings, then that's fine to only use q=100, but then it needs to compare to state-of-the-art for large batch. Thompson sampling is a popular method for large-batch settings which is included as a baseline, but to my knowledge the state-of-the-art for large-batch BO is TURBO (Eriksson et al. 2019). In fact the q=100 and the robot pushing and rover trajectory problems are all exactly as in the TURBO paper, so its inclusion as a baseline is pretty obvious and, I think, necessary.

**Issue 2: Lack of statistical significance**

The results of the experiments do not appear to be statistically significant. The main results given in the main text are tables, and these tables do not have confidence intervals. The only place where uncertainty in the results is show are the figures in the appendix, and there the confidence intervals appear to overlap in most cases. This is due to the use of only 5 replicates. I appreciate that these experiments are costly to run since they are using 1000 iterations, nevertheless the lack of statistical significance in most of the results provides weak support for the claim that BEEBO is actually better, vs. what we're seeing just being noise in the experiments. The paper needs some form of statistical analysis to convince the reader that what we're seeing is not just noise in the experiments. The best way to do this would be to include confidence intervals in tables 2 and 3, and then increase the number of replicates as necessary to achieve statistical significance in the differences in means. I do not feel it appropriate to highlight the method as being ""best value"" when it is possible that the confidence interval for that value contains the values of the other methods.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces a batched acquisition function that balances exploration and exploitation by using a weighted sum of mutual information and expected value, with the weights defining the trade-off. The discussion links the proposed algorithm to UCB and asserts that it naturally addresses heteroskedastic noise.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed acquisition function and its optimization and approximation methods are straightforward and practical. 
2. The paper provides extensive empirical results to illustrate the proposed algorithm's efficiency.

Weaknesses:
1. The introduced parameter controlling the trade-off lacks interpretation as in previous methods.
2. The completeness of the related work discussion is concerning. This is potential because the summarization lacks high-level extraction of the design of the algorithm, and the focus of the paper is, to some extent, scattered。

Limitations:
Discussed above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new acquisition function BEBBO for batched BO. BEBBO tries to build (negative) free-energies-like acquisition function, enabling gradient-based optimization, tight exploration-exploitation control, and risk-averse BO under heteroskedastic noise. It tries to improve existing parallel acquisition functions in the following ways:
1.	uses a hyper-parameter T to directly balance exploration and exploitation by separating these two parts clearly;
2.	keeps the behavior predictable by scaling E and I with batch size;
3.	enables the optimization of gradient descent by holding the availability of closed-form expressions for GP.
This paper demonstrates several experimental comparisons and shows its effectiveness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	This paper shows an enlightening acquisition function method inspired by statistical physics. 

2.	The experimental results show the effectiveness of BEBBO on problems without noise or with heteroskedastic noise.

Weaknesses:
1.	The idea is straightforward: simply combine two common components—entropy reduction and the weighted sum of values. In my opinion, the novelty is not very strong. 

2.	The article doesn't discuss the situation when BEEBO is used with other surrogate models. It seems that if BEEBO is not used with GP, it loses the advantages of closed-form expressions and gradient descent optimization. 

3.	In control problems shown in Figure A2, the performances of meanBEBBO and maxBEBBO are not outstanding. Especially, these two variants are surpassed by KB in Robot pushing problem.

4.	Although BEEBO performs well on many synthetic test problems, its versatility and effectiveness require more experimental validation in specific applications.

5.	In the experiments in main text, the authors only showed the comparison with q-UCB. It would be better to show comparisons with other batched baselines and provide a thorough analysis. The comparison with q-UCB shows the advantage on the balance between exploration and exploitation. But other advantages emphasized in the paper, such as the benefits of gradient descent optimization and the tight control of the explore-exploit trade-off, are not fully demonstrated. I suggest that the authors can re-organize the paper to move the comparison with other baselines to the main paper. 

6.	The theoretical analysis is not deep. No regret bound is analyzed.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Proposing a new acquisition function inspired by statistical physics, which allows explicit control of exploration-exploitation trade-offs in a batch BO setting.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Drawing inspiration from statistical physics is a promising direction, as it naturally aligns with Bayesian approaches.

Weaknesses:
**Major Points**
1. **Lack of Unique Selling Point:** 
   The method does not appear to solve any unique cases that other methods cannot. While the related work section outlines many similar approaches, this work only compares itself to q-UCB. Without a theoretical study, such as convergence rate analysis, there is insufficient motivation to adopt another heuristic approach like this. To demonstrate efficacy, a comprehensive empirical study with extensive comparisons to existing works is necessary, given the unclear advantages.

2. **Review of Claimed Selling Points:**
   - **Not Based on MC Integration Like BoTorch:** 
     While this is true, it is unclear if it is beneficial. MC approaches are approximations but have convergence guarantees (refer to the BoTorch paper's appendix). This work lacks such guarantees.
   - **Tight Control of Exploration-Exploitation Trade-off:** 
     The proposed method is not the only solution. UCB theory can bound the feasible region by [max LCB(x), UCB(x)] with 1 - $\delta$ probability (e.g., see [1]). This region can be controlled by $\beta$ hyperparameters, corresponding to $\delta$. Constrained batch BO within this region would yield similar results with theoretical guarantees.
   - **Heteroskedastic BO:** 
     There are no comparisons with existing methods. UCB variants can address this problem. Vanilla UCB theory does not differentiate between epistemic and aleatoric uncertainty. Therefore, UCB with heteroscedastic GP can serve the same purpose. For example, training a heteroscedastic GP with the observed dataset and replacing the inner GP on noise variance with normal isotropic likelihood variance (pure epistemic uncertainty) will yield similar results as risk-averse batch BO, without needing the change in modelling the acquisition functions regardless of hetero-/homo-scedastic cases like this work.

3. **Setting k in Practice:** 
   Setting  $k$ is not an easy task for users. In UCB, $\beta$ presents a similar challenge, but there are theoretical guidelines and algorithm to compute this (e.g., [2]).

4. **Constrained Uncertainty Sampling:** 
   This work can be understood as a variant of constrained uncertainty sampling. As [3] explains, variance reduction can be written similarly to the entropy proposed in this paper (see section 2.2). It also shows that the variance-only approach is inferior to UCB both theoretically and empirically. The batch setting may lead to model misspecification, particularly when hallucination (aka fantasization) is applied. The concerns and approach are notably similar to ([49] in your citation number), making a comparison with their method unavoidable.

5. **Data Augmentation Procedure:** 
   The explanation is unclear. Is it fantasization (aka hallucination) or simply observed points? How does this differ from Eq.(3) in [3]?

- [1] Fengxue Zhang, Jialin Song, James C Bowden, Alexan- der Ladd, Yisong Yue, Thomas Desautels, and Yuxin Chen. Learning regions of interest for bayesian optimiza- tion with adaptive level-set estimation. In International Conference on Machine Learning, pages 41579–41595. PMLR, 2023.
- [2] Kihyuk Hong, Yuhang Li, and Ambuj Tewari. An optimization-based algorithm for non-stationary kernel bandits without prior knowledge. In International Conference on Artificial Intelligence and Statistics, pages 3048–3085. PMLR, 2023.
- [3] Srinivas, N., Krause, A., Kakade, S., & Seeger, M. (2010). Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. In Proceedings of the 27th International Conference on Machine Learning (pp. 1015-1022).

Limitations:
Limitations are discussed in the discussion section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
vtRotUd539;"REVIEW 
Summary:
Given the complexity of the process of neural network training, any understanding of robust phenomena that can be identified in the training process has potential value that can guide the design of models and algorithms. Neural Collapse (and its deep counterpart) is one such phenomenon that has been identified and reproduced across multiple model classes and datasets. This work shows that Neural Collapse also occurs for a recursive kernel-based model known as Deep RMF, when trained using an algorithm that is based on projection onto a matrix constructed from an outer products of gradients computed locally at each layer.
Additionally, the authors present experimental results that document neural collapse in these models when trained on standard datasets. They also show that in standard neural networks, the projection of features onto the gradient outer product leads to neural collapse, rather than the effect of the nonlinearity.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written, and presents both theoretical results and some empirical results that complement them, since they apply to datasets that violate the assumptions under which the results hold. They prove that deep Neural Collapse can indeed occur in models beyond standard neural networks trained with gradient descent.
The experimental results (specifically in Appendix D) demonstrate that the projection onto the gradient outer product matrix (AGOP) leads to neural collapse in standard models, motivating the further study of this object.

Weaknesses:
Given that the main results apply both to a non-standard kernel method and a non-standard training algorithm, it is unclear what the implications of the results are for more well-known models and algorithms. If the authors believe that these results have implications of this form, they should be presented more clearly. Algorithms that are not based on backpropagation are interesting both as possible means of explaining learning in biological systems where backpropagation is unrealistic, and in distributed settings where backpropagation may incur a prohibitive communication overhead. However, the motivation of the algorithm used appears to be that it is a simple model that demonstrates certain phenomena that arise in the training of deep networks. 

The authors assume that the gram matrix of the data is full-rank. This requires assuming that the number of datapoints is smaller than or equal to the input dimension (which subsumes assumption 4.1). Standard datasets violate this assumption.

Limitations:
Limitations and societal impacts have been addressed

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies deep neural collapse (DNC) in deep neural networks (DNN) through the prism of the neural feature ansatz (NFA) and deep recursive feature machines (RFM). It is comprised of several results:
- empirical evidence that DNC occurs in deep RFMs,
- a theoretical analysis of DNC in a high-dimensional RFM setting,
- a theoretical analysis of DNC in a kernel learning setting,
- empirical evidence that the mechanisms which lead to DNC in RFMs and traditional DNNs are the same.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
This paper shows that deep neural collapse occurs in a similar way in deep networks and deep recursive feature machines. It thus provides a simplified setting in which to investigate deep neural collapse, which is an important research direction to further our understanding of deep learning. Specifically, it shows that neural collapse can be obtained just by iterating linear regression problems, without backpropagating through a deep network.

Weaknesses:
My main issue with the paper is its writing, which makes it quite difficult to read.
- The notations could be improved in several places throughout the paper (see minor points below).
- I could not follow most of section 4.2, despite being rather familiar with kernel methods and their behavior in high dimensions. 
On a high level, I don't understand how a linear kernel could be the best setting for neural collapse. The text contradicts itself, as it simultaneously state that ""if [$\lambda_k = 0$] [...], collapse will occur in just one layer] , but also that ""this theory offers an explanation for why non-linear activation is needed"". A linear layer can collapse within-class variability but also typically collapses class means together, and thus cannot induce neural collapse (see paragraph below). 
On a technical level, $k_\Phi$ and $\lambda_\Phi$ are referred to before being defined, and I do not understand the roles played by $k$/$\lambda_k$ vs $k_\Phi$/$\lambda_\Phi$. Assumption 4.2 is also referred to before being stated.
- Section 4.3 is also slightly difficult to read. 
I took me several tries to guess that $k_M(x,x') = \tilde k_M(x,x')\mathrm{Id}$, which should appear in the paper. The terms ""input-level"" and ""output-dimension level"" kernels should be introduced for non-specialists in multi-task kernel learning.
I also do not understand the point of introducing $M$ if it is dropped afterwards. Theorem 4.4 could simply be stated as ""the optimal feature map for ridge regression is the one which already predicts the label: $\Phi(x) = y$"". This result is not very surprising, and is not very integrated in the paper. I suppose that it is some kernel-level equivalent of the unstructured feature model, and suggests that weight decay might be instrumental in bringing about neural collapse? The normalization of $k$ should be restated in the definition of Problem 3 (otherwise the optimal loss is obtained when $k \to 0$).
- The message of section 5 could be presented more clearly. What I understood was that it argues that RFMs and DNNs achieve neural collapse through the same means. I suggest making this point before introducing RFMs (in particular, stating the NFA correlations). I also did not understand why this mechanism is referred to as ""denoising"". 

My second issue is that I was not convinced by the claim that it is the right singular vectors and singular values which lead to neural collapse. By the same logic as lines 309-315, the right singular vectors do not change the DNC1 metric (with a ""full"" SVD where $U$ and $V$ are invertible). Similarly, if I were to divide operations in the network as $V^T\sigma$ and $US$ as opposed to $\sigma U$ and $SV^T$, I should see that it is now $US$ which is responsible for neural collapse (again with a full SVD). This conclusion also depends on the chosen metric for evaluating collapse. Why do the authors consider the ratios of traces of between- and within-class covariances, rather than the trace of their ratio (the Fisher linear discriminant)? It seems that it would reverse one of the conclusions of the analysis, since the trace of the Fisher discriminant ratio $\mathrm{tr}(\Sigma_W^{-1} \Sigma_B)$ is invariant to invertible linear transformations, and decreases under non-invertible linear transformations, so can only be improved through the non-linearity. If the conclusion of which part of the network is responsible for DNC depends strongly on the chosen metric, can we really ascribe meaning to the question? It seems to me that it is really the sequence of weights and non-linearity which _together_ induce DNC, and trying to separate their effects is not really possible.

Finally, Proposition A.1 was first published by Cho and Saul in _Kernel Methods for Deep Learning_, NIPS 2009. Besides, the expression of the kernel in eq. (5) can be simplified with algebra and trigonometry (compare with their eq. (6)).

Minor notes and suggestions:
- I suggest using a so-called ""diverging"" colormap (such as ""bwr"") in Figure 1 to clearly separate positive from negative correlations, and use the same range for both datasets.
- I suggest replacing ""Gram matrix"" with ""(uncentered) covariance"" to refer to $W^TW$, as weight matrices $W$ are generally decomposed in rows which correspond to individual neurons.
- The notation $||X||$ to refer to the vector in $\mathbb R^N$ of column norms of a matrix $X \in \mathbb R^{d\times N}$ is never introduced (and clashes with the usual convention that this is a matrix norm).
- Why is the last layer denoted $W_{L+1}$ instead of $m_{L+1}$?
- The choice of layer-indexing is confusing and seems inconsistent throughout the paper. Contrarily to what is stated in section 3.1, isn't $X_l$ the features after $l-1$ network layers? I suggest to denote the input as $X_0$ instead of $X_1$ to simplify the notations. Also, it seems that $M_l^{1/2} X_l$ should be referred to as $\tilde X_{l+1}$ rather than $\tilde X_l$ given the chosen conventions.
- Typo: missing a norm in the definition of $\bar H_l$ line 128.
-In section 4.2, I suggest defining activations before the kernels, e.g., $\tilde X_{l+1} = \kappa^{-1/2} M_l^{1/2} X_l$ and $X_{l+1} = \Phi_{\rm lin}(\tilde X_{l+1})$. I also suggest choosing a different notation for $k_{\rm lin}$ and $\Phi_{\rm lin}$ which are confusing as they imply linearity, and to avoid the awkward ""non-linear feature map $\Phi_{\rm lin}$"".
- Typo line 260: the output space of $k_M$ should be $\mathcal R^{C\times C}$.
- I suppose that $\lambda = \mu$ in section 4.3.
- In the caption of Figure 2, I suppose that ""fully-connected"" should be removed in the case of ResNet.

Limitations:
See weaknesses.

In its current state, I think that the paper is slightly below the acceptance bar and would require minor, if not major, changes before it can be fully appreciated by the NeurIPS community. I would be happy to raise my score if the authors address the points raised above.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The submission introduces a mechanism for Deep Neural Collapse (DNC) using the average gradient outer product (AGOP). The authors also propose the Deep Recursive Feature Machines (Deep RFM) model, which employs AGOP in its architecture to empirically and theoretically demonstrate DNC. The main contribution is that AGOP-based explanation is a data-based approach while prior work focused on data-agnostic explanations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* Using a data-based approach based on AGOP to explain DNC is novel to the best of my knowledge
* The paper offers both theoretical analysis and empirical evidence supporting the role of AGOP in inducing DNC
* The experiments are performed on different architectures and datasets

Weaknesses:
*  I found the paper challenging to read
* I am unsure about the practical implications of this work

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study two effects associated with neural collapse: the within class variability going to zero and the orthogonality/tight-frame of the class means. They study the deep recursive feature machine model, and show that neural collapse forms in that setting as well, due to the projection of the data onto the average-gradient outer product (AGOP). They show both empirical and theoretical results on this phenomenon, leveraging high-dimensional gaussian equivalence of nonlinear random feature models. Further, they show that the right singular vectors of the weight matrices are responsible for most the within-class variability collapse, projecting onto the subspace spanned by the gradients.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The writing of the paper is for the most part quite readable. The literature review is thorough and puts the results of this paper in a good context. The empirics are extensive and compelling. Moreover, the theoretical ideas leveraging the equivalence of nonlinear random features to a linear kernel with additional identity term make for a compelling argument about the mechanism for neural collapse in RFMs. Given the good mix of theory and experiment, I recommend this paper for acceptance.

Weaknesses:
Section 4 is doing many things at the same time. It may be better to split it into an empirical evidence section, and then do a section on the theoretical results. In particular, it would be good to give an idea of where the theoretical ideas are going at the start of 4.2 before setting out to prove the deep neural collapse results. This would substantially improve the readability of this section. 

This goes double for section 4.3. The opening paragraph of that section is unreadable:

*""Next, we show that the formation of the neural collapse is not only implicitly given by the specific
optimization procedure of the Deep RFM, but is also implicitly regularized for in the parametrized
kernel ridge regression, a model class that includes RFM (i.e., a single layer of Deep RFM)""*

I don't really understand what this is saying, or even what you're trying to accomplish in the entire subsection. I tried many times to read it. The whole subsection should be rewritten. There are many sentences there that make no sense to me. Here is another one:

*""Since we do not explicitly regularize M, we will drop the dependence on it, treat k as
 a free optimization variable and compute the optimal value of the following relaxed problem:""*

This is certainly not something you can do generally. For example if I had a matrix parameterized as $A = M_1 M_2$ and optimized just the $M_i$ with no explicit regularization, there are many cases where this isn't the same as optimizing $A$. Maybe you mean to say something else but once again I can't understand what you're trying to say. The prior subsection was compelling enough that I am discounting this rather poor form of writing. Please rewrite this section. 

More generally, there are many sentences throughout the paper that are not well-worded and seem to run on. Improving the writing would benefit the quality, precision, and reach of this otherwise strong paper. If in your rebuttal you can provide examples of improved presentation, I may raise my score higher.

Limitations:
This work elucidates an important phenomenon in deep learning theory. Developing a principled understanding of feature learning is likely to have implications for the interpretability and reliability of AI systems.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wWguwYhpAY;"REVIEW 
Summary:
This paper proposes a mixture of experts (MoE) approach for INRs, which allows the learning of local piece-wise continuous functions by subdividing the domain and fitting locally. The incorporation of a MoE architecture enhances speed, accuracy, and memory efficiency. They also propose a novel manager architecture and initialization that enable domain subdivision without ground truth.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and easy to follow.
2. The proposed MoE INR has a good performance compared to baselines.
3. The idea of delivering MoE as a learnable partition region for INR fitting with randomized initialization is novel to me. From the ablation study, the randomized initialization improves the performance a lot.

Weaknesses:
1. Missing some closely-relative works. I encourage the authors to have a detailed discussion of previous MoE INRs [1,2] and decomposition/partition-based INRs [3,4].
2. Lacking some key comparison experiments with decomposition/partition-based INRs. The authors only compare their method with the baseline SoftPlus and SIREN (and their wider version). However, some related works [4] have also shown the INR based on pre-determined masks can also outperform the wider version of SIREN.  I encourage the authors to experimentally compare your method with [4] to illustrate the necessity of learnable partition regions.
3. A detailed ablation study on the hyper-parameters of the MoE INRs is missing, such as the layer of encoder, manager, and experts. Given a fixed number of parameters, how to allocate the parameters to the three modules remains unknown.

[1] Zhao, Jianchen, et al. ""MoEC: Mixture of Experts Implicit Neural Compression."" arXiv preprint arXiv:2312.01361 (2023).
[2] Wang, Peihao, et al. ""Neural implicit dictionary learning via mixture-of-expert training."" International Conference on Machine Learning. PMLR, 2022.
[3] Rebain, Daniel, et al. ""Derf: Decomposed radiance fields."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
[4] Liu, Ke, et al. ""Partition speeds up learning implicit neural representations based on exponential-increase hypothesis."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

Limitations:
The limitations and potential negative societal impact of their work have been discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new architecture for implicit neural representations (INRs) based on the mixture-of-experts (MoE) architecture. This new architecture is differs from traditional MoE architectures in that now all the experts have a shared encoder and expert-specific decoders, while the manager also now has an encoder-decoder architecture, with the manager decoder taking as input both the manager encoder's representation as well as the experts' shared encoder representation. The authors also provide a pre-training method for the manager. The method is evaluated on image reconstruction, audio reconstruction, and 3D shape reconstruction by fitting signed distance functions (SDFs).

Soundness:
1: poor

Presentation:
3: good

Contribution:
1: poor

Strengths:
This paper proposes a novel MoE-based architecture for INRs together with a novel pre-training strategy for the MoE manager. 

The empirical results are good and there is an ablation study on the major components. 

The paper is well-written and easy to understand. 

Many details relating to reproducibility are provided in the supplemental material.

Weaknesses:
One of the major weaknesses of this paper is the experimental evaluation. The method does not compare against other methods that propose a new INR architecture (e.g. Gaussian activation function [1], WIRE [2], FINER [3]) or a standard MLP with positional encoding. The experimental evaluation is not also very robust, as only small datasets are used (Kodak, only 3 audio recordings, only 4 shapes) and more complicated tasks investigated by similar works are not considered (for example, WIRE [2] and FINER [3] both include evaluation on neural radiance fields). 

The proposed MoE architecture also did not show improvements when used with softplus activation. 

This paper also did not include other metrics normally used to evaluate the tasks, such as SSIM and LPIPS for 2D image fitting (e.g. FINER [3]). 

Minor point: inclulding the number of parameters in Table 1 may be helpful since comparisons between the number of parameters is discussed in the text.

References
1. Ramasinghe, Sameera, and Simon Lucey. ""Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.
2. Saragadam, Vishwanath, et al. ""Wire: Wavelet implicit neural representations."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
3. Liu, Zhen, et al. ""FINER: Flexible spectral-bias tuning in Implicit NEural Representation by Variable-periodic Activation Functions."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

Limitations:
Limitations and negative impact are addressed by the authors.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel INR framework that leverages the Mixture of Experts (MoE) technique. The proposed strategy consists of an expert and a manager branch. Each branch has an encoder that processes the input coordinate and extracts an embedding. By processing the two encoder embeddings, the manager predicts the probability of which of the N experts should be used for extracting the signal. They show how the proposed INR framework achieves better reconstructions than SIREN on several modalities, such as audio, image, or 3D surfaces. They also propose a manager pre-training strategy, which is necessary to exploit all the experts effectively.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
-	The original idea might be conducive to new research in this direction.
-	The paper is well-written and easy to follow. 
-	Supervising experts with semantic losses and obtaining networks specialized in specific semantic areas of the input signal might unlock several applications for INRs and ease their interpretability.
-	The proposed framework achieves good reconstruction performance.
-	The ablations in Table 4 and Table 5 are very insightful.

Weaknesses:
W1) The major weakness of the paper is the misalignment between the experimental results and the motivations of this research:

 a- In the introduction (L26-28), the authors correctly point out that, in traditional INRs, each coordinate needs to be processed by the whole network. Even though they claim that this problem can be solved by MoE INRs, in the proposed architecture, the input coordinate needs to be processed by the full manager and the encoder of the expert branch. The only saved computation is the one from the final expert, a much smaller network than the others. Thus, the saved computation looks minimal to standard SIREN (considering the same total weights). Moreover, no experiments regarding computational efficiency and the advantages of parallelized inference are necessary to motivate this claim. Maybe, instead of talking about the absolute efficiency of the proposed approach, it is better to show the better trade-offs in terms of efficiency and reconstruction quality than SIREN.

b- In the introduction (L28-30), the authors claim that standard INRs extract features vastly different for close spatial coordinates (i.e., locality problem). I am unaware of studies that formally investigate this INR property. Thus, I suggest adding a reference work or validating it with experiments. Moreover, the authors claim MoE INRs can learn local piece-wise functions (L259 in the conclusion section). Thus, they do not suffer from the problem above. Yet, the experiments show something different. For instance, by looking at Figure 3c, different experts predict audio signals for close temporal coordinates. I can notice the same behavior in the last column of Figure 6, in which many distinct experts predict pixels in the upper part of the image.

W2 ) The idea of using MoE resembles the idea of KiloNeRF [1]. In that case, the routing strategy is not learned and depends only on the input 3D coordinate, and each expert focuses on a pre-determined spatial region. I think the authors should add this reference, explaining the pros and cons of the two kinds of approaches.

W3) In recent years, INR frameworks have been proposed to be faster and more efficient than MLP-only techniques such as SIREN. For instance, hybrid INRs such as InstantNGP [2] (hash grid + MLP) can be used as the base framework to speed up computation when learning INRs of images and surfaces or NeRFs. The paper should also include more recent competitors than SIREN.


[1] Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. ICCV 2021.
[2] Instant neural graphics primitives with a multiresolution hash encoding. ACM transactions on graphics (TOG) 2022.

Limitations:
The paper includes a discussion on limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a MoE architecture for INRs, enhancing scalability, local fitting, and parallelization. Traditional INRs use a single network, imposing global constraints, while this method learns several local expert functions, subdividing the domain. Key contributions include a novel manager architecture and initialization method for better convergence and domain subdivision without ground truth. It show improved performance and efficiency over traditional methods, across image, audio, and 3D surface reconstruction,

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It demonstrates a neat architectural design along with a robust ablation study.
- It consistently shows performance improvement across various tasks, and the performance with respect to the number of parameters is also superior.
- It is interesting that random initialization, which includes no inductive bias in the manager pretraining process, outperforms initializations like SAM.

Weaknesses:
- It requires more parameters compared to the vanilla model. While it is fair to compare it with the wider version of the vanilla model, it is unclear if the proposed model still performs well when it has the same number of parameters as the vanilla model (not wider version). Tab.3 alleviates this concern to some extent.

- In addition to comparing with the vanilla model, it would be good to include a discussion on various INR methods that apply locality bias (e.g., spatial functa).

- As shown in the convergence graphs in the appendix, it shows more unstable convergence compared to the baseline.

Limitations:
The authors have adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wTIzpqX121;"REVIEW 
Summary:
This work introduces a VAE variant of GraphCast for global medium-range weather forecasting and a VAE variant of a UNet (that is formulated as a GNN) for limited area modeling over Scandinavia. For this, they adapt GraphCast to have a similar hierarchical structure to UNets, and then treat the coarsest hierarchical layer (the bottleneck) as a latent variable representing the mean of isotropic Gaussians. The ensemble predictions from the model are similarly fast as a single deterministic prediction, achieved through batching. Their calibration can be good for some variables (e.g. global t2m for 10 day lead time has a spread/skill ratio of 0.99), while poorer for others (e.g. local wvint has a spread/skill ratio of 0.57 for 24h lead time).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed VAE extension to GraphCast is significantly faster than diffusion-based approaches (like the GenCast model).
2. The work does not limit itself to just global weather forecasting, but also presents results for limited area modeling, which is the class of models used by many national weather services.
3. The paper is reasonably well written, keeping a good amount of detail in the main paper, and presenting many additional details in the appendix.

Weaknesses:
Major points:
1. Questionable baselines: I am unsure if the chosen baselines are very strong, let me name a few reasons for this:
    - Tab 1 presents performance for GraphCast, e.g. RMSE=387 for z500, 5 day leadtime. However, if I check the headline scores in the WeatherBench 2 (https://sites.research.google/weatherbench/) for GraphCast, i see RMSE=274 for z500, 5 day leadtime, which is significantly higher and beats all models presented in this study.
    - Both Tab 1 and Tab 2 do not include scores for the conventional weather models. I would expect Tab 1 to include IFS & IFS-ENS scores and Tab 2 to include MEPS scores.
    - Since this work introduces a probabilistic weather model, i would expect comparison with other recent works on probabilistic weather models, like the ones cited in this paper (e.g. GenCast).
    - Graph-FM has almost 6x the parameters compared to GraphCast* (Tab 5) - which quite possibly could be the major reason for its improved performance, and not the introduced architectural feature of hierarchical layers.
2. Overlooked connection to UNets: The Graph-FM that was introduced for the LAM setting looks to me as equivalent to a UNet:
    - The input data comes on a regular grid with 236 x 268 pixels. Which is subsequently downsampled using 3x3 windows. Processing at each depth level is done with a locally connected layer (in other words: a local convolutional filter). A semantically simpler description of such a model would be a UNet with 3x3 pooling (learned in this case) and 3x3 conv filters at each stage. Possibly, implementing it as a UNet could also be computationally advantageous, making use of the highly optimized kernels for 2d convolutions and pooling operations, instead of GNN layers that rely on scatter sums.
    - UNets have been previously used for Weather forecasting: e.g. https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2018GL080704 & https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020MS002203
3. Proposed VAE implementation physically not meaningful? Your VAE is implemented with a latent variable at the coarsest level. This is supposed to capture epistemic uncertainty related to the forward model (and not due to initial state). However, one may argue for atmospheric models most model uncertainty comes from the subgrid-scale parametrizations and not from the coarse-scale representation of atmospheric dynamics. Hence, to me it seems far more intuitive to introduce the stochasticity at the finest level, representing the small scales. I assume you chose the hierarchical description mostly for computational reasons, but given a lack of physical basis, i would at least expect a more thorough investigation of potential errors introduced by this, e.g. is the ensemble variability too smooth?
4. Missing reference to previously published work? A workshop paper at last years NeurIPS has introduced both the hierarchical GNN and the MEPS dataset https://arxiv.org/abs/2309.17370 , if I am not misstaken. I am not really sure about NeurIPS policy here, but even if this work is a direct extension of the previous work and the previous work is to be considered as non-archival, I still believe you should at least cite the workshop paper.

Minor points:
1. GraphCast + SWAG: This is a baseline with poor performance, that is somewhat arbitrarily picked from many possible approaches to obtain ensemble predictions from neural networks. I see two options here: Either you keep it, but also introduce many other such baselines, to make clear that you did not cherrypick a particularly weak one. Other approaches  that should not be prohibitively expensive to run could e.g. be MC-Dropout or Laplace Approximation. Or, you simply drop it, as is, it does not  add much to the paper.
2. Introduction lacks motivation for LAM: This is an ML conference that you are submitting to. It would probably be good to briefly motivate why doing LAM is even necessary (i.e., why can't we just rely on global models instead)?
3. Extreme Weather evaluation / case study: One key reason for ensemble prediction is capturing the tails, i.e. the extremes. You state in Appendix A that this is out-of-scope for the work. I would argue you are making your life too easy here. Since the presented models are likely not useful unless they display decent performance also for extreme weather, it would be important to evaluate just that. It may be enough for this paper to e.g. study a single extreme event as a case study.

Limitations:
The section on limitations is somewhat short. I believe the key limitation of this work is in the evaluation, i.e. it is unclear to me after reading the work how well the models perform. E.g., are these models robust for longer rollouts or if applied at prediction time further away from training time?
Moreover, the work does transparently communicate a limitation of their multi-scale Graph-EFM: visual artifacts. But, this is strange to me, as it could mean two things: a) the VAE formulation and multiscale edges simply don't work well together or b) since GraphCast did not observe such issues, the presented Graph-EFM (ms) is simply not trained sufficiently or suffers from a buggy implementation. While a) would be an interesting finding, i believe b) can not be ruled out given the presented results in this paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new method for predicting weather using advanced deep learning models. The approach, called Graph-EFM, improves accuracy and better handles uncertainties in weather forecasts. It uses a 1.5 degree version of ERA5 and making weather predictions more reliable and useful for real-world applications.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper's strengths include the innovative use of Graph-EFM for accurate probabilistic weather forecasting, detailed experiments on large datasets, and clear presentation of methods. Graph-EFM significantly enhances uncertainty estimation and forecast reliability adding value to both research and practical weather prediction applications.

Weaknesses:
What happened if 0.25 degree ERA5 is used?

Limitations:
Not many, this is a fantastic piece of work

Rating:
10: strong accept, should be highlighted at the conference

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors propose a graph-based ensemble forecasting model (Graph-EFM) to provide weather prediction with a hirearchical GNN framework. They used a hierarchical mesh graph to handle the challenges of capturing processes unfolding over different spatial scales and modeling the uncertainty in the chaotic system. The Graph-EFM provides a probabilistic weather forecasting with the benefit of capturing forecast uncertainity. The experiment results show the effectiveness and advantages of Graph-EFM compared to other deterministic models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The hierarchical mesh graph provides a reasonable idea to handle different spatial scales for weather forecasting, which could inspire other researchers to handle problems in different domains.
The spatial dependencies are considered and handled within GNN layers.
Using ensumble-based model could capture the uncetainty of weather system.

Weaknesses:
In Figure 3, it seems like the selected ensemble members vary a lot, and how close is your forecast to the ground truth. Possibly, explaining a little bit of the underlying meaning of the measures in table 1 & 2 in the paper.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes Graph-EFM, a method that combines a hierarchical multi-scale graph neural network with a variational objective for probabilistic weather forecasting. The method performs on par with Graphcast on deterministic metrics with the extra benefit of uncertainty estimation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The paper tackles probabilistic weather forecasting, which is an important problem in the field.
- The proposed method is intuitive and makes sense. Overall, generative modeling is a potential direction for probabilistic weather forecasting. People have used GANs and diffusion, so a latent variable model is a natural addition to the literature.
- The performance looks promising, and it is more efficient than existing methods using diffusion.

Weaknesses:
- The authors should replace Table 1 with a line graph figure instead, as it allows comparison across different variables and lead times.
- Please see my questions below.

Limitations:
See above.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
wT6GHk5ShC;"REVIEW 
Summary:
The authors provide a theoretical perspective on the stability of in context learning via implicit gradient descent trajectories. Ultimately, the analysis suggests that high condition numbers of the weight matrices belonging to layers with a high index can be pruned in order to achieve a model which performs better on ICL tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- In context learning is important, and something which has not been studied as deeply as other topics of ML due to the recent rise of transformers and ICL in general.
- The method intuitively makes sense and is something which can be conditionally tuned after training based on specific tasks if a validation set is available.

Weaknesses:
- It would be good to define deep and shallow, as these are subjective terms depending on the reference frame.
- Figure 1 cpation says: ""We operate on the whole of MLP or ATTN."" What does this mean?
- If as figure 1 states, you can clip 99.5% of the original weights, what happens if you just drop that layer entirely? Recent work has shown that the deeper layers can be completely dropped without much effect. [1]
  - I cannot see much benefit gained from pruning part of the weights with SVD when it seems that the in nearly all cases, the benefit can be had by dropping the layer entirely.
  
- Is the mask on L138 supposed to represent a causal mask? If so, I do not think the notation is correct, as the Identity matrix would only have $N$ binary values which is much less than is needed for a causal mask.
- How can equation 1 and 2 use the same mask?
- Example 1 appears to be incorrect:
  - There is no parentheses around $W_{V_r}^k + \delta_V h_i^{k-1}$ in the first line.
  - The triangle inequality seems to say that line 2 $\geq$ line 1
  - Given the above, I do not see what conclusion can be drawn from this equation.
  - Have I missed something here?

Limitations:
The limitations have been discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the effect of singular value decomposition (SVD)-based weight pruning on the in-context learning (ICL) performance of large language models. 

The Authors show that SVD-based pruning can enhance ICL performance, with deeper layers showing more stable improvements. 
They provide theoretical analysis to explain these findings, presenting implicit gradient descent trajectories for ICL and deriving generalization bounds. 

Based on their insights, they propose a simple algorithm for enhancing ICL inference in downstream tasks.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The Authors provide a theoretical analysis to explain their empirical findings, including the derivation of implicit gradient descent trajectories and generalization bounds for ICL.

- Furthermore, they propose a simple, derivative-free algorithm for enhancing ICL performance in downstream tasks, demonstrating the practical value of their theoretical insights.

Weaknesses:
- The theoretical analysis primarily focuses on linear attention, which may not fully capture the complexities of standard Softmax attention used in most transformer models

- The proposed algorithm is derivative-free, but the search for optimal clipping rates may still be computationally expensive for very large models or datasets

- There is a substantial lack of comparison with other pruning methods: the study focuses on SVD-based pruning but doesn't compare it with other pruning techniques, which could provide context for the method's effectiveness

- Poor language, frequent typos, and grammatical errors are significant issues in this paper. This does not help readability, and would likely be a barrier to publication in its current form.

- An essential part of the paper, which is the discussion of related works is not part of the main text. Furthermore, this discussion is prone to criticism. For example, quoting the seminal paper by Frankle and Carbin as an example of low-rank properties of neural networks is clearly misleading. I think that this discussion should be an essential part of the main text, and should also be substantially revised in order to avoid conceptual confusions.

Limitations:
Limitations are discussed in the final section (4).

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper demonstrates that (1) SVD-based weight pruning can sometimes achieve better in-context learning performance, and (2) pruning weights in deeper layers often results in more stable outcomes compared to shallow layers. The authors explain their findings through theoretical analysis and propose an intuitive matrix condition number-based weight pruning algorithm to achieve both stable and improved performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work conducts an in-depth analysis to explain the ""stability"" of transformer weight pruning across different layers. The framework is interesting and validated through experiments. Moreover, the theoretical analysis can be applied to design new algorithms like algorithm 1 in this paper .

Weaknesses:
Despite adopting various simplifications (such as using a linear attention transformer without MLP and layer normalization, treating each in-context example as a single vector, implementing attention masks for query tokens, and using meta-gradients for in-context learning) in their theoretical analysis, the results are still limited. They only explain why SVD-based weight pruning can achieve ""stable"" performance, leaving the more intriguing question of why transformers can achieve ""better"" performance with pruning unclear. Additionally, even with detailed hyperparameter tuning, the effectiveness of Algorithm 1 remains uncertain. Further details are provided in the questions section.

Limitations:
See Weaknesses and Questions. Besides, there seem many mistakes on pages 7 and 8, for example, the equation between line 228 and 229, and the equation for about F-norm in lines 230,231,232. These inaccuracies cast doubt on the overall reliability of the paper's findings. If there are any misunderstandings on my part, please point them out, and I will reconsider my evaluation of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses the phenomenon: SVD-based weight pruning can increase the in-context learning abilities of transformer based LLMs. In this paper, the authors conduct theorectical analysis by presenting the implicit gradient descent trajectories of ICL and providing the generation bounds visa full implicit gradient descent trajectories. This paper also provide a simple yet effective algorithm to clip the LLM by SVD to enhance ICL inference.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
First, this paper has a clear writing and is easy to follow. 

It provides a detailed theoretical analysis on why SVD based weight pruning will improve ICL performance by leveraging the implicit gradient descent trajectories. It also provides the generalization bounds of ICL, in Theorem 2, it can be inferred that the noise level and the norm of of gradient contribute to the error bound. It provides the theoretical insight of SVD based method.

The authors provides a simple algorithm to leverage the discovered phenomenon to improve ICL performance of LLM in a gradient-freee way. The ratio between $\sigma_{max} $ and $\sigma_{min}$ is a good choice of heuristic conditional number.

Weaknesses:
1. More details of algorithms is not shared. e.g. the range / number of clipping rate candidates set. 
2. In experiments result of C.5, the optimal $\xi$ varies a lot across different tasks and different modules. However, this phenomenon is not touched in the theoretical part.

Limitations:
No

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wT5AgMVkaJ;"REVIEW 
Summary:
This paper studies the problem of aligning vision models with human aesthetic standards in a retrieval system. There are three key parts in the proposed model including LLM rephrasing, re-ranking, and RL fine-tuning. Two novel benchmarks are also introduced to integrate aesthetic quality into evaluation metrics. Experimental results demonstrate the effectiveness of the proposed method and the benchmarks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper addresses the aesthetic quality issue in image retrieval systems and introduces a reinforcement learning fine-tuning strategy that enables the retrieval model to directly retrieve images based on both semantics and aesthetic quality, eliminating the need for multi-stage filtering. This approach holds significant value.
2. The paper introduces two evaluation benchmarks, addressing the limitation of current image retrieval benchmarks that fail to evaluate aesthetic quality.
3. The experiments are comprehensive, validating the importance of each component in the proposed method.

Weaknesses:
1. The methodological process described in the article is somewhat cumbersome, with Figure 2 merely outlining key processes and concepts in a rudimentary manner, thereby increasing the difficulty for readers to comprehend.
2. The authors appear to conflate ""no-reference image quality assessment"" with ""image aesthetic quality assessment."" While these tasks are indeed closely related, they are distinct. MANIQA, for instance, should not be regarded as an aesthetic quality assessment model, and its paper does not evaluate the model's performance on aesthetic datasets.
3. There remain some details in the article that are inadequately explained. It is peculiar that in Appendix Table 7, the same stride seemingly yields a different number of images.
4. The manuscript contains typos. For example, the indicator function symbol in Equation 11 is clearly garbled.

Limitations:
The paper does not discuss the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper looks into the alignment task for vision and language models within retrieval models where properties such as visual aesthetic comes to play. To achieve this, the paper collects some data to design a metric suitable for taking into account human aesthetic evaluation. And employs an RL-based technique to exploit the human opinion for better aligning the retrieved images with human aesthetic preferences.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* It is well-written paper
* The concept of aligning vision with aesthetic preferences is interesting and useful in some applications.
* The experiments are well-designed and quite convincing. 
* It is interesting that LLM rephrasing could improve the quality of results

Weaknesses:
* The proposed metric could be elaborated better and maybe explained how the study ensured the metric is not designed under influence of the model.

Limitations:
The paper does not discuss any limitations with respect to the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work aims to align vision models with human aesthetic standards in a retrieval system. To do this, the authors propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. The authors further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The idea of aligning vision models with human aesthetics in retrieval is interesting. This work has potential applications in various real-life applications. 
2.	The authors’ motivation of utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations is insightful. 
3.	The paper is well-written and informative.
4.	The proposed dataset HPIR can be used by fellow researchers in the related fields.

Weaknesses:
I feel it can be further improved in the following ways. 

1.	For benchmarking human preferences, it might be better to record down the human variance in their annotations. I understand the authors used multiple annotations to ensure robustness, but since aesthetics is a subjective concept, human variance itself tells something.
2.	Following point 1, I feel the work can be made more solid if it includes some human evaluation studies on the experimental results. For example, in Fig. 5, it does not seem so obvious to me on the respective enhancement with finetuning. 

Without the above two points, I feel the paper has somewhat overclaimed the ""alighing vision models with human aesthetics"".

Limitations:
The authors did not indicate limitations in their paper, and mentioned that they will discuss it in future. I feel this paper has clear limitations such as the indication of human variance and the evaluation of the results.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aligns the vision models with human values by leveraging LLM for query rephrasing and introducing preference-based reinforcement learning. The paper also presents a novel dataset named HPIR to benchmark the alignment with human aesthetics.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper introduces a novel approach to align visual models with human aesthetics, combining LLM rewriting to enhance query understanding and using preference-based reinforcement learning to fine-tune the model. The paper is comprehensive in experiments and introduces the HPIR dataset for benchmarking. The paper is well-structured and the methods are clearly explained. Key concepts are well defined and the use of diagrams helps to effectively illustrate the results. And this paper improves the aesthetic quality of results in image retrieval by aligning visual models with human preferences. The proposed method and dataset provide valuable ideas for future research in this area.

Weaknesses:
[W1] This paper lacks a detailed user study to validate the actual effectiveness of the proposed method. Including a user study with different participants to evaluate the subjective improvement of aesthetic alignment could provide stronger evidence for the actual effectiveness of the method.
[W2] Placing the related work in Section 6 makes it difficult for readers to have a clear understanding of the problem domain and existing research results before reading the specific methods and experiments, which is not conducive to the coherence of the paper structure.

Limitations:
Exploring possible negative social impacts, such as implications for privacy or how the technology might be misused in unintended ways Research could benefit from deeper analysis of how biases in training data affect model outputs beyond just aesthetics, particularly with respect to cultural and demographic diversity.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wT2TIfHKp8;"REVIEW 
Summary:
This paper addresses the challenge of predicting less frequently visited points-of-interest (POIs) in human mobility data, a problem known as the long-tail issue in spatial distribution. The authors introduce a new framework called Long-Tailed Adjusted POI Prediction (LoTNext), which includes two main components: long-tailed graph adjustment module and long-tailed loss adjustment module. Additionally, the framework employs an auxiliary prediction task to enhance the model's generalization and overall accuracy. The effectiveness of LoTNext is demonstrated through experiments on two real-world trajectory datasets, where it significantly outperforms existing state-of-the-art methods in human mobility prediction.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The code has been provided, which makes the reproducibility of this paper good.
2. The paper is generally well-writern and easy to follow.
3. The proposed method is motivation-grounded.

Weaknesses:
1. The presentation quality of this paper can be further enhanced.
2. The authors are encouraged to conduct experiments on more datasets and provide more detailed analysis.
3.  This paper can supplement more theoretical analysis to guarantee the proposed method's effectiveness.

Limitations:
See above weaknesses and questions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents the Long-Tail Adjusted Next POI Prediction (LoTNext) framework to address the long-tail problem in next POI prediction. This problem refers to the uneven spatial and temporal distribution of POI visits, making it challenging for prediction models to predict less frequently visited POIs. LoTNext combines a Long-Tailed Graph Adjustment module to reduce the noise and impact of long-tailed nodes in the user-POI interaction graph and a Long-Tailed Loss Adjustment module to balance the loss between head and tail POIs. Additionally, an auxiliary prediction task is employed to enhance generalization and accuracy. The proposed method was evaluated on two real-world trajectory datasets, Gowalla and Foursquare, where it significantly outperformed existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- LoTNext introduces a unique combination of graph adjustment and loss adjustment modules to tackle the long-tail problem, which is a significant contribution to the field of human mobility prediction.
- The framework is evaluated on two real-world datasets and compared with ten existing methods, demonstrating superior performance across multiple metrics.
- The paper provides a thorough explanation of the methodology, including the embedding generation, transformer encoder, spatial contextual attention layer, and the overall optimization process, making it reproducible and transparent.

Weaknesses:
- The proposed model is complex and involves multiple components and adjustments, but it is not clear how computationally expensive it would be to make predictions in services and elsewhere.
- The model performed well on the dataset used, but it is unclear under what conditions the proposed method will perform well, such as visit intervals and frequency of visits.

Limitations:
The authors have adequately addressed the limitation of their work, particularly the potential privacy risks associated with the extensive use of user trajectory data.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the LoTNext framework, which is designed to improve the prediction of human mobility patterns, specifically addressing the challenge of long-tail distribution in POI visitations. The authors propose a novel approach that includes a Long-Tailed Graph Adjustment module and a Long-Tailed Loss Adjustment module, along with an auxiliary prediction task, to enhance the model's ability to predict less frequently visited POIs. The paper demonstrates the effectiveness of LoTNext through comprehensive experiments on two real-world datasets, showing significant improvements over existing state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I like the research gap proposed by this paper. This is a worthwhile issue to study.

Weaknesses:
(1) The evaluation could be expanded to include a broader range of metrics to further validate the generalizability of the LoTNext framework.
(2) It's better to have more explainability related experiments.
(3) A more detailed literature review is needed (at least in the appendix) so that the novelty of the method could be better evaluated. 
(4) The comparison methods used are somewhat outdated. Why didn't you use the latest methods, such as TPG (https://arxiv.org/abs/2304.04151) or LLM-Move (https://arxiv.org/pdf/2404.01855), for comparison?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This study proposes the Long-Tail Adjusted Next Point-of-Interest Prediction (LoTNext) framework. By combining a Long-Tailed Graph Adjustment module and a Long-Tailed Loss Adjustment module, it reduces the impact of long-tailed nodes in the user-POI interaction graph and adjusts loss through logit score and sample weight adjustment strategies. Experimental results show that LoTNext outperforms several existing methods on two real-world datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The structure and organization of this paper are well-designed, and the writing is clear and easy to comprehend.
2. This paper investigates the long-tail problem by proposing a general framework for next POI recommendation, filling the gap in addressing the long-tail issue in POI recommendation. This work is meaningful and valuable.
3. To enhance the readability of the paper, the authors provide detailed results analysis, parameter settings, and the motivation behind the design of each module in the appendix.

Weaknesses:
1.  In the related work section, the authors review common methods for addressing the long-tail problem in recommendation systems. Since this paper focuses on addressing the long-tail problem, adding several baselines that tackle the long-tail issue in recommendation systems (e,g,, [1]) would better demonstrate the effectiveness of the proposed method.
2. The novelty of this paper is not very strong. The long tail effect of check-in data, such as the POI frequency distributions, has been studied before. 
3. Additional comparative analyses should be included to illustrate the shortcomings of baselines in handling the long-tail issue. For instance, comparing the proposed model's performance with all baselines (not just Graph-Flashback) on long-tail POIs would better demonstrate its effectiveness in addressing the long-tail problem.
4. The experimental results are not convincing enough, as the compared methods are not the SOTA method. More recent baselines should be compared (e.g., [2-4]).

[1] Meta graph learning for long-tail recommendation, SIGKDD, 2023.
[2] EEDN: Enhanced Encoder-Decoder Network with Local and Global Context Learning for POI Recommendation， SIGIR-23
[3] Adaptive Graph Representation Learning for Next POI Recommendation， SIGIR-23
[4] Spatio-Temporal Hypergraph Learning for Next POI Recommendation， SIGIR-23

Limitations:
The authors have already pointed out a limitation of this method: LoTNext relies on extensive user trajectory data, which, if deployed by certain institutions or companies, may pose a potential risk of privacy breaches, potentially leading to negative social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wT2KhEb97a;"REVIEW 
Summary:
This paper considers the approximate personalized page rank. Classical results for this problem have a runtime that is linear in $1/\alpha\epsilon$ where $\alpha$ is the damping factor and $\epsilon$ is the error parameter. The authors show that APPR is simply a local variant of Gauss-Seidel Successive Overrelaxation. Using this connection, the authors derive new run time bounds for APPR and also propose a new algorithm based on Gradient Descent. The execution time for both these are, in the worst-case, identical to the previous bounds. However, they are more sensitive to the state of execution of the algorithms (depend on the active nodes) and seem to mirror the actual performance of these algorithms. Also, under certain assumptions, they improve the worst-case execution time.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses an important problem, provides deeper insights into an existing algorithm, provides a new algorithm and also reanalyzes the algorithm in a more fine-grained way. All of this is done via connection to GSSOR which seems to be new.

I find the result quite interesting. However, I am not very familiar with recent work on personalized page rank.  For this reason, I recommend accepting but with a low confidence.

Weaknesses:
NA

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the study of local algorithms for graph clustering which is an important problem in the field of graph data analysis. In particular this paper is considers the task of computing Personalized Page Rank (PPR) vectors for a given graph. In this problem the algorithm is given a graph in the form of its adjacency and degree matrices, the goal is to approximate the Personalized Page Rank vector for a given starting vertex and dampening factor $\alpha$ up to precision $\epsilon$ without accessing the entire graph. The classical algorithm of Andersen, Chung and Lang runs in time $O(1/\alpha \epsilon)$, which independent of the graph size. The central question posed by subsequent works is whether the dependence on $\alpha$ can be improved to $1/\sqrt{\alpha}$. The main contribution of the paper is to propose a new algorithmic framework based on the locally evolving set process. Under this framework they are able to implement existing algorithms such as Andersen et al.'s APPR algorithm as well as localized implementation of standard gradient descent. They are also able to develop localized versions of chebyshev and heavy ball methods that do achieve the $1/\sqrt{\alpha}$ dependence for some fixed constant value of $\epsilon$. Finally they show that on several large scale graphs, their new localized chebyshev and heavy ball methods do outperform APPR and related methods empirically.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main strengths of this paper are to develop a new algorithmic framework that can not only encompass existing algorithms but lead to the development of better ones that overcome previously known limitations for designing local graph clustering algorithms. They also back their theoretical analysis with the practical implementation of their method which is also shown to be superior to previous algorithms.

Weaknesses:
One weakness is that the paper is only able to obtain a quadratic improvement in the dependence on the parameter $\alpha$, obtained by the local implementation of the Chebyshev and Heavy-ball method, only for a value of $\epsilon$ and not for all.

Limitations:
Authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper uses the evolving set procedure to give a local PageRank algorithm whose dependence on \alpha (the reset probability) is \sqrt{\alpha}.

It proposes accelerated local iterative methods with coefficients given by Chebyshev iteration. The convergence of this algorithm in both graph theoretic and general sparse linear systems settings are analyzed in detail. Discussions of the relations between this method and other local iterative algorithms are also given in detail.

The method was implemented and tested on a range of graphs, mostly coming from social networks. This includes two large ones with edges in the billions. On moderate ranges of \alpha (reset probability), the experiments show significant speedups (factor of about 3) and convergences (factor of 10) on most graphs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Local algorithms are widely used in graph analytics. The question studied is natural, and has been proposed before.

The method is theoretically well-founded, and has significant technical depth.

The experiments are thorough and well documents, and clearly demonstrate the advantages of this method in multiple parameter regimes.

Weaknesses:
The gains only kick in at a relatively large number of steps: it's not clear to me that these are the parameter regimes in which local algorithms actually get used.

Ideally for the empirical works I'd also like to see comparisons of downstream tasks and effects on overall accuracies (e.g. F-1 score), but the paper itself has already covered a lot of ground.

Limitations:
yes, limitations have been addressed, and are entirely theoretical w.r.t. some graph parameter regimes.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wSqpNeMVLU;"REVIEW 
Summary:
This paper presents a theoretical study on speculative decoding, an efficient inference method for large autoregressive models. It highlights practical implications, proposing a Pareto-optimal solution for the rejection-distribution bias tradeoff.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The authors provide a robust theoretical foundation, illustrating the practical implications of speculative decoding, such as the improvement of rejection accuracy, which cannot be achieved by simply changing the acceptance probability.
 - The study explores the trade-offs between inference cost and quality degradation, supported by an optimization model. This analysis is valuable for practical applications.

Weaknesses:
- The main figure does not clearly communicate the core concept of speculative decoding. It might lead readers to believe that speculative decoding primarily addresses hallucination, which is not its main advantage.
 - The experimental results are not distinctly highlighted, and the authors do not explain how these results support their theoretical analysis. While the theoretical contributions are significant, the paper would benefit from more extensive empirical validation.

Limitations:
The results may not guarantee optimality in practical situations because real-world circumstances are more complex and varied than those considered in the theoretical analysis.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a theoretical perspective on speculative sampling. Through Theorems 1 and 2, the authors demonstrate that the sampling method employed by speculative sampling is optimal and unbiased. Subsequently, Theorem 3 introduces a multi-candidate approach to enhance the acceptance rate of speculative sampling.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The writing is very clear, with takeaways provided under each theorem to explain the theory.

Theorems 1 and 2 are crucial for speculative sampling. In paper [23], the authors showed that speculative sampling is unbiased but did not prove its efficiency compared to other rejection sampling methods. The proof provided here is very important.

Weaknesses:
The experiments are not sufficient. I would like to see improvements in batch speculative sampling in real-world scenarios.

I am curious if batch speculative sampling can be combined with tree-style methods, e.g., [1] CaPE and [2] Medusa?

[1] Du, C., Jiang, J., Yuanchen, X., Wu, J., Yu, S., Li, Y., ... & You, Y. GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding. In Forty-first International Conference on Machine Learning.
[2] Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., & Dao, T. (2024). Medusa: Simple LLM inference acceleration framework with multiple decoding heads. In Forty-first International Conference on Machine Learning.

Limitations:
na

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The author aim to develop theoretical understanding of speculative decoding. The authors assume that given a large and small model participating in speculative decoding, the computation complexity of the small model is negligible. Under this assumption, they characterize the expected rejection rate of speculative decoding. They show that this bound depends on the total variation distance between the generations from small and large model. Next, the authors show that spectral decoding gives optimal rejection bounds in class of all rejection based methods. Motivated by recent works analyzing batch speculative decoding, where the rejection is done only if M tokens are rejected from a given sample. Finally, given an acceptance probability, the authors show an optimal solution solution to the total variation loss between the distributions of large model and the one found by speculative decoding. This objective changes linearly with the rejection probability. This provides insights on selecting the optimal value of rejection threshold as per requirement. The presented theoretical results are backed up with appropriate experiments validating them.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) The theoretical analysis presented by the authors provide several interesting insights about the inference efficiency observed by spectral decoding.

2) All the results are backed up with simulation experiments, which strengthen the results presented in the paper.

Weaknesses:
1) It is not completely clear, why making the assumption about negligible compute of the small model is not a strong assumption. Since the small model needs to generate the tokens autoregressively therefore even though its single pass could be small as compared to the larger model but it the context length is high i.e. several autoregressive passes are made, the compute of small model might not be negligible. It would be great if the authors can provide some empirical evidence to justify this assumption.

2) It would have been great if the authors provided evidence using real world models in support of their theory. Although, this is not a major weakness, but authors should consider it in the camera ready version.

Limitations:
Yes, the authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides detailed analysis to speculative decoding and batch speculative decoding. The conclusions of the paper are: (1) speculative decoding is unbiased and it shows the expected rejection rate; (2) speculative decoding has the lowest rejection rate in all the unbiased algorithm that belongs to the familty defined in Algorithm 2; (3) batch speculative decoding has lower rejection rate than speculative decoding; (4) it analyzes the trade-off between efficiency and effectiveness of the family of algorithm defined in Algorithm 2.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper provides comprehensive theoretical analysis.

2. The findings in Theorem 4 and 5 are interesting.

3. The paper is easy to understand.

Weaknesses:
1. Although the paper provides lots of theoretical analysis. But I find only Theorem 4 and 5 are somewhat interesting. Theorem 1 is already derived in the original speculative decoding paper. For Theorem 2, although speculative decoding is proven to be optimal in the family of algorithms defined in Algorithm 2, but I don't think there are a lot of existing algorithms can be formulated in Algorithm 2. In fact, is there any algorithm that belongs to Algorithm 2 and is unbiad and it not speculative decoding? For Theorem 3, the finding that batch speculative decoding has lower rejection rate than vanilla speculative decoding is not surprising. 

2. Although Theorem 4 and Theorem 5 are interesting, it only solves half of the problem: given b, what should P be. It would be better if the authors could also discuss the design of b.

3. I think the paper can also be improved if the authors could summarize a new speculative algorithm from Theorem 4 and 5 and running experiments to compare with vanilla speculative decoding.

Limitations:
see weakness above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wQpNG9JnPK;"REVIEW 
Summary:
This paper addresses the problem of spurious correlations caused by environments from where data are collected.
The proposed method applies a mask to input data to separate spurious and semantic features.
The masked input data are fed into a local model specialized to each environment.
Each local model is trained to induce neural collapse for OOD generalization.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- S1: Making use of neural collapse for OOD generalization is interesting.

Weaknesses:
- W1: Comparison with not only OOD generalization methods but also spurious correlation (sometimes called bias or shortcut) methods is necessary. Methods that can automatically detect and split spurious and semantic features have been developed [a-e].
- W2: Types of spurious features that the proposed method can handle need to be clarified. Can the proposed method handle spurious features in superposition, e.g., objects and textures?
- W3: The rationale behind the proposed method needs to be clarified. For instance, it is unclear why the method adds the noise to the mask when learning it.
- W4: Deeper analyses in the experiments would make the paper more interesting. For example,      
  - Whether the neural collapse is achieved by the proposed method should be confirmed in the experiment.    
  - Visualizing learned masks would produce more valuable insights.
- W5: What is described in the introduction and what is done in the proposed method seems to be different. Although L42 states that `we propose to compute the Frobenius norm (F-norm) of the difference between the feature prototypes and the standard simplex ETF`, the F-norm does not appear in the proposed method.
- W6: Writing and formatting can be improved. There are many inconsistent spellings. For example,    
  - Is ""variable features"" in L153 the same as spurious features?  
  - The meaning of ""interaction"" in L189, 192, and so on is unclear. Maybe ""training?""  
  - Such inconsistent spellings occur from Section 4.

[a] Tiwari, Rishabh, and Pradeep Shenoy. ""Overcoming simplicity bias in deep networks using a feature sieve."" ICML2023.    
[b] Bahng, Hyojin, et al. ""Learning de-biased representations with biased representations."" ICML2020.    
[c] Yang, Wanqian, et al. ""Chroma-vae: Mitigating shortcut learning with generative classifiers."" NeurIPS2022.    
[d] Liu, Evan Z., et al. ""Just train twice: Improving group robustness without training group information."" ICML2021.    
[e] Nam, Junhyun, et al. ""Learning from failure: De-biasing classifier from biased classifier."" NeurIPS2020.

Limitations:
Discussed in Section 6.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper leverages the neural collapse inspired ETF behavior to simulate different environments in datasets, and uses it for OOD classification.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper uses a phenomenon that's apparent in the standard setting, for a task that varies from the standard setting. It uses intuitive notions to tackle the task of OOD classification. The paper experiments are generally convincing.

Weaknesses:
The paper seems generally consistent and well merited. The experiments are a bit lacking, but are convincing.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The spurious correlation between image background features and their labels is a significant research problem, and the existing research suffers from the issue of difficult decoupling. In this paper, we propose a new approach to solve the spurious association problem by alternately performing environment segmentation and learning semantic masks from the perspective of neural collapse. Extensive experiments are conducted on four datasets and the results show that the proposed method significantly improves the out-of-distribution performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper explores an important and widespread problem in real-world applications with solid and extensive experiments. The writing is clear and the narrative is easy to follow, facilitating an understanding of the spurious correlations problem. The use of neural collapse is particularly innovative.

Weaknesses:
W1:  In lines 48-50, it is mentioned that IRM-based methods learn similar representations from different environments, indicating a lack of proper alignment. Could you provide a corresponding experiment to demonstrate this phenomenon?

W2: In Figure 3, the explanation of the middle module that uses logits to judge the environment is unclear. Could you please clarify the structure of the local models, the number of local models used, and the specific meaning of the logit values?

W3: Could you explain the differences between masks based on pixel-level and feature-level approaches? If using feature-level masks, what is the impact of different network-layer features on model performance?

This work addresses an important and interesting question by introducing neural collapse from an invariant perspective, which I believe can provide valuable insights to the community. However, my main concern is that the same mask is used to learn both invariant and variable feature information. What are the advantages of the mask learning mechanism proposed in this paper compared to HRM's [1] mask mechanism?

[1] Heterogeneous Risk Minimization

Limitations:
Yes, the authors have adequately described the limitations in their submission.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
wN5AgP0DJ0;"REVIEW 
Summary:
The paper presents a novel framework for solving Partial Differential Equations (PDEs) by leveraging the power of Equivariant Neural Fields (ENFs). The authors propose a space-time continuous approach utilizing the symmetry of the PDEs, which is crucial for improving generalization and data-efficiency. The framework is tested on various geometries and PDEs, showing its effectiveness in handling complex dynamics.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. **Data efficiency**: By designing a system that preserves the symmetries of PDEs, the proposed framework enhances the model's ability to generalize from limited data.
2. **Novel initialization method**: The use of meta-learning to structure the latent space of the ENF simplifies the learning process and leads to better performance than autodecoding.

Weaknesses:
1. **Error Accumulation:** The usage of ODESolver might pose a challenge with error accumulation over time, particularly for dynamics occurring beyond the training horizon, which could affect the model's long-term predictive accuracy. So it would be helpful if the model is tested in a longer timespan.
2. **Lack of Comparative Analysis:** While the paper compares its approach to a baseline method, a more comprehensive comparison with existing state-of-the-art methods in PDE solving would strengthen the paper's claims, such as Geo-FNO[1], GNOT[2], Transolver[3].

[1] Li, Z., Huang, D. Z., Liu, B., & Anandkumar, A. (2023). Fourier neural operator with learned deformations for pdes on general geometries. *Journal of Machine Learning Research*, *24*(388), 1-26.

[2] Hao, Z., Wang, Z., Su, H., Ying, C., Dong, Y., Liu, S., ... & Zhu, J. (2023, July). Gnot: A general neural operator transformer for operator learning. In *International Conference on Machine Learning* (pp. 12556-12569). PMLR.

[3] Wu, H., Luo, H., Wang, H., Wang, J., & Long, M. (2024). Transolver: A fast transformer solver for pdes on general geometries. *arXiv preprint arXiv:2402.02366*.

Limitations:
1. More experiments should be conducted to prove the model's efficiency in longer timespan.
2. More baselines should be listed in the paper so that model's efficiency can be thoroughly tested and confirmed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a space-time continuous method for solving PDEs that respects the inherent symmetries of the PDE via equivariance constraints. Building upon prior work which (a) fits a conditional neural field to output latent vectors and (b) evolves the latent state through time via a Neural ODE, the contribution of this work is to additionally enforce equivariance constraints in the latent space itself. Secondly, the work employs meta-learning to obtain the initial latent representation, which improves the structure of the latent space representation and accelerates the inference time. The authors show improved performance of the method for linear and nonlinear PDEs on complex geometries such as the 2d torus, 3d sphere and 3d ball.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method significantly reduces overfitting compared to non-equivariant baselines. 
 - The method shows good stability at 3–5x the length of the training regime (even though the error accumulates slowly).

Weaknesses:
- The computational cost of the method v/s baselines is not shown. Relatedly, do the DINo baselines have similar parameter counts as your method?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper attempts to learn the dynamics of certain PDEs from time series data using implicit neural representations, while encoding symmetry information of the domain. In fact, constructing a neural model that is aware of Euclidean transformations is the primary focus of this paper. To this end, the authors design two equivariant neural networks. Given an initial condition, a latent state is obtained by meta-learning. This latent state is then integrated in time by a neural ODE (first network) to obtain a final latent state. The second network then takes this final latent state as an input, and maps any given point coordinate (in the domain) to the solution at the final time. Examples are presented on periodic domains in $ \mathbb{R}^2 $, 2-torus, 2-sphere and the 3D-ball. The paper builds on a 2022 ICLR paper [1] which attempts the same, but without any symmetry assumption.

[1] Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy, and Patrick Gallinari. Continuous pde dynamics forecasting with implicit neural representations. 2022.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well written.

PDEs are often posed on domains that have symmetric properties. This is in addition to the fact that the operators appearing in the PDE have their own symmetry / directional properties. While learning from data, most of the existing methods attempting to learn PDE dynamics ignore the symmetry information. Therefore, this is a welcome idea.

The method exhibits impressive extrapolation results.

Weaknesses:
Only infinite domain (or periodic boundary conditions) are considered.

In the examples, the transformation groups are chosen by carefully considering the nature of the domain and the operators appearing in the equations. But in a real application, this information, especially the operator information, is not known a priori.

Extrapolation results are shown where results outside the training time horizon are predicted. The problem with such prediction is that they look good until they do not. And there is no logical or analytical bound on the time horizon where the extrapolation is supposed to work. The time horizon is always chosen so as to exhibit the effectiveness of the method. But no analysis is presented in that regard. Therefore such extrapolation results, even though impressive in some respects, do not add to either the understanding or the applicability of this method to a new application.

Memory and execution (training) times are not compared (only the training times of the proposed method are included). Error comparisons are made with other methods. Sometimes this method outperforms the other methods (e.g., Table 2), but in some cases, it is marginally better than the others (e.g., Table 3, 4). Providing memory and training times would make these comparisons more well rounded.

(Minor) Line 276: should be $ \nabla \cdot u = 0 $.

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes a novel framework combining equivariant neural fields and neural ODEs, providing a continuous space-time solution for PDEs while respecting associated equivariance constraints. The author uses PDE-specific bi-invariant attributes for equivariant neural fields and a meta-learning approach for learning the initial latent state. The proposed method achieves better performance on the chosen PDE problems.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The work addresses an important and complex issue of equivariance in the context of solving partial differential equations (PDEs). The proposed architecture is not only space-time continuous but also respects the equivariance constraint. This characteristic makes it particularly valuable and effective for various types of scientific research and applications.


2. The proposed method is well-motivated and clearly explained in the paper.

Weaknesses:
I have found the empirical study to be the weak point of the work. In order to argue the effectiveness of the proposed solution over existing approaches, the authors need to consider established benchmarks, large-scale datasets, PDEbench, and CFDBench, especially with irregular domains (domains with holes or solid objects as hindrances).

I also find that the choice of baselines is not extensive. For example, SFNO [a] is used for the shallow water equation. Also, baselines like [b,c] are not considered.

Also, as the proposed solution is claimed to be time continuous, zero-shot super-resolution along the time domain should be demonstrated (analogous to Table 3). 


a. Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere

b. GNOT: A General Neural Operator Transformer for Operator Learning

c. Geometry-Informed Neural Operator for Large-Scale 3D PDEs

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel framework that leverages Equivariant Neural Fields (ENFs) to solve Partial Differential Equations (PDEs). By preserving geometric information in the latent space, the proposed method respects the known symmetries of the PDE, enhancing generalization and data efficiency. The framework demonstrates improved performance in various challenging geometries, validated through experiments against other neural PDE forecasting methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents an innovative approach by integrating equivariant neural fields, which respect the symmetries of PDEs, thereby enhancing model performance.
The methodology addresses significant limitations of existing NeF-based PDE solvers, particularly in generalization to unseen spatial and temporal locations and geometric transformations.
Extensive experimental validation across various geometries (e.g., plane, torus, sphere) demonstrates the robustness of the proposed framework over existing methods.

Weaknesses:
The framework's performance decreases when extrapolating beyond the training horizon for complex PDEs.
While the approach shows competitive performance, the computational complexity due to the global attention operator in the ENF backbone can be high.
Error accumulation in long-term predictions could be mitigated with increased model capacity, but this comes at the cost of computational resources.

Limitations:
The framework assumes that the boundary conditions are symmetric and that the PDEs exhibit certain symmetries. In real-world applications, these assumptions might not always hold, potentially limiting the applicability of the proposed method to PDEs with different or more complex boundary conditions.

The use of a global attention operator in the Equivariant Neural Field (ENF) backbone increases the computational complexity. This can lead to high computational costs, especially when scaling the model for larger datasets or more complex PDEs.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wK0Z49myyi;"REVIEW 
Summary:
The manuscript #3263 entitled ""CRAYM: Neural Field Optimization via Camera RAY Matching"" proposes a novel uncalibrated NeRF strategy based on prior keypoints matching across images. Specifically, the authors propose two novelties to improve the quality of the reconstruction and the pose estimation of the cameras: 1) Enriched ray features using surrounding rays sampled around the keypoints and 2) a ray matching index which can be used to re-weight the color regression part, leading to better robustness to occlusions.
The proposed technique has been evaluated across various standard datasets and against meaningful NeRF-like algorithms.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The idea of separating key rays and auxiliary rays is interesting and meaningful.
- Numerous and conclusive results.
- Assessment on a large number of datasets.
- Good ablation study underlying the benefit of each novelty.

Weaknesses:
- The robustness of the approach against outlier matches is not evaluated. Introducing artificial outliers (wrongly matched keypoints) into the dataset to assess how well the technique can handle mismatches would be of some interest.
*Question*: Would the matched rays consistency and the epipolar geometry compensate for that? Or would the training diverge?

- As stated in the literature review of this manuscript, other approaches taking advantage of the epipolar geometry and prior matching have already been designed in this context. I have difficulty understanding what is significantly different with this work apart from the sampling of additional surrounding rays and the matching ray index used to weight color prediction using image pairs. These two novelties seem rather incremental, but they nonetheless lead to strongly improved results.
*Question*: I assume that the other keypoints-based approaches are not ""self-calibrated"". Is the proposed technique the first ""keypoint-based"" calibration-free NeRF? If it is not the case, it would be meaningful to compare against such techniques too.

- Adding surrounding rays around a key ray appears to be quite effective; however, the sampling of auxiliary rays is not well described in the paper.
*Question*: How are the rays sampled?

- The initialization of the pose lacks details.
 *Question*: What is the effect of the pose initialization on the result?

- The intrinsic parameters of the camera could additionally be optimized.
*Question*: Just out of curiosity, have you conducted such an experiment?

- In equation (4), it seems that the proposed solution considers only pairs of images. 
*Question*: How are those pairs selected?

The proposed approach is inspired by existing techniques integrating matched keypoints (like using the epipolar loss) and other techniques, such as NeuS.

- The loss function contains many regularization factors.
*Question*: Is the final loss hard to balance?


Overall, the paper is interesting and proposes a few contributions that seem to lead to strongly improved results. Moreover, the approach has been evaluated on various standard datasets and against representative methods. However, this novel approach remains relatively incremental, and many points remain to be clarified regarding the robustness of the technique. For all the above-mentioned reasons, I would like to issue a rather mixed opinion regarding the acceptance of this work for this conference.

Limitations:
The limitations of the paper may not have been entirely investigated, specifically in terms of robustness. For instance, the influence of the initial pose and outliers has not been demonstrated.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a new technique called camera ray matching, which is integrated into the joint optimization of camera poses and a neural field. The method utilizes an uncalibrated set of images as input, incorporating photometric and geometric constraints through key points and key rays matching, with the aim of enhancing the quality of novel view rendering and 3D surface reconstruction. The approach comprises two simple modules and is implemented using grid-based representation (iNGP). Photometric experiments were exclusively compared with MLP-based methods, specifically NeRF-like, on the synthetic dataset of the vanilla NeRF, while geometric experiments demonstrate some positive results. The authors provide additional results in the appendix.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work is a positive extension to the field of neural reconstruction (like NeRF and SDF) under the setting of images captured with noisy poses. Authors make efforts to simultaneously solve the problems involving the camera pose, detailed renderings, and accurate surface reconstruction. Experiments show good results.

Weaknesses:
Too many factors are taken into account in the writing simultaneously, which leads to a lack of clear theme or a clear academic or technical problem to be addressed in this paper. The work appears to build incrementally upon previous research and offers limited novelty. The so-called Epipolar loss and Point-alignment loss are actually based on Bundle Adjustment (BA), using key points matching, which has been previously applied in the optimization of neural reconstruction in works such as SCNeRF, BARF, L2G, and Level2sfm. The proposed two modules do not bring significant innovation. It is also confusing that this work is implemented using a grid-based representation (i.e., iNGP), while the compared methods are implemented using MLP-based representation, which does not allow for a precise and fair comparison. I suggest that the authors refer to ZipNeRF for guidance on how to formulate research problems and conduct appropriate comparisons.

Limitations:
The authors have identified a limitation where the meshes extracted from the constructed SDFs may still contain messy inner structures over invisible areas. I recommend that the authors explore the possibility of finding the SDFs of surface points instead of the SDFs of each sampled point along rays. This would involve assessing whether the depth accumulated by all points along the rays is more accurate than the output surface generated by all sampled points.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work suggests a novel neural representation and training scheme that jointly solves for the scene representation and the multi-view camera localization. It is done using several new ideas that generalize existing NeRF based methods. 
The representation itself is a combination of a geometry-network, which predicts a signed-distance-function (SDF) and a feature vector, that are fed into the texture-network that predicts the usual color and density values.
The main key novelty, is that the optimization is done over matching rays, obtained from matching keypoints using a pretrained network. The standard photometric loss function is extended to incorporate an epipolar loss (that constrains the camera positions) and a point-alignment loss that ensures the ray intersect at the predicted depth estimates along the rays. Another strong addition, is the use of 'auxiliary' rays around each matched pair of rays, from which features are fused to produce a more robust representation, that can aid the optimization under errors in matching and camera poses.
Extensive experiments demonstrate the importance of each component and the strong performance of their combination.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* The paper presents an extension of the NeRF framework, based on several novel and interesting additions that are framed in a single pipeline. The experimental results show that these contributions work well together and yield new state-of-the-art results, across the board.
* One promising idea, in my view, is the joint optimization of both geometry and texture networks, which clearly complement eachother and are helpful in obtaining stronger and more accurate constraints on the scene understanding (as opposed to most NeRF pipelines that focus on image reconstruction and are less accurate for 3D reconstruction).  
* The other strong idea, is the joint optimization of matching rays, once again - imposing consistency contraints (on both camera and surface locations) that were not previously exploited to such an extent in prior work.
* The paper is well written and the contributions are very clearly highlighted, while the understanding of the conventional parts is left for the reader (which is mostly fine).

Weaknesses:
* Reproducibility - I believe that many details are missing (including from the appendix) for one to be able to implement the proposed method. For example:
   * What are the settings of the preprocessing SuperPoint and SuperGlue matching? What is the typical match density?
   * How are the auxiliary rays sampled? How many and under which distribution?
   * What is the function g in Eq 2 that fuses the key and auxiliary features?
   * What are the balance weights in the final loss (Eq. 7)?
   * How are poses initialized?
* Complexity - There is no discussion what so ever about the impact of the suggested changes on memory and runtime complexity, but at traning and in inference.
* Qualitative results are relatively limited. 
   * Synthesized images are all very small, so it is difficult to appreciate the fidelity.
   * No depth images are shown
   * No examples of key and auxiliary point matches are shown (over entire images)

Limitations:
Adequately discussed.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces Camera Ray Matching for optimizing camera poses and neural fields from multi-view images. The optimized feature volume supports novel view synthesis and 3D geometry reconstruction by probing camera rays, which carry both geometric and photometric information. CRAYM claims to improves efficiency and accuracy by focusing on keypoints and integrating multi-view consistencies, enhancing both geometric reconstruction and photorealistic rendering. The method shows result in NVS and geometry reconstruction compared to baseline methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper is well-structured and easy to follow.

Weaknesses:
- Experiments were only conducted on NeRF-synthetic datasets and not on LLFF datasets.
- Comparison is made with older baseline methods (e.g., SPARF, BARF, L2G) which are more than 2 years old. It’s recommended to include more recent methods such as NoPe-NeRF and BAA-NGP.
- It is suggested that the authors perform Neural Image Alignment to enhance the evaluation.

Limitations:
YES

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wJaCsnT9UE;"REVIEW 
Summary:
This paper presents introduces a training approach for ensemble learning called SharpBalance to balance sharpness and diversity within ensembles. This paper shows theoretically that SharpBalance achieves a better sharpness-diversity trade-off.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Ensemble learning is an important research direction.
2. Understanding of sharpness and diversity within deep ensembles is important for the study of generalization to both in-distribution and out-of-distribution data.
3. The paper is technically sound.

Weaknesses:
1. Since SharpBalance focuses ""on a diverse subset of the sharpest training data samples"", it may not apply in small datasets where available data is already sparse.
2. Empirical improvement over existing methods is marginal.

Limitations:
Limitations are adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the sharpness and diversity within deep ensembles. Specifically, it identifies the trade-off phenomenon between sharpness and diversity with both theoretical and empirical evidence. Additionally, it proposes a method called SharpBalance, which trains individuals using selective 'sharp' subsets. Conducted experiments have demonstrated the effectiveness of the proposed SharpBalance when applied to deep ensembles.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
There are several strengths in this paper:

- The exploration of sharpness and diversity in deep ensembles is both interesting and novel.

- Sufficient theoretical and empirical evidence has been provided for validation.

- The proposed method is simple, effective, and accompanied by code for verification.

Weaknesses:
However, I still have the following concerns:

 - The evaluation seems a bit weak. The authors should consider comparing with more ensemble baselines.

 - What is the scale of $D_{SAM}^i$ and how does it change during training? Providing some details on this would help in understanding the proposed method.

 - Refer to Line 166: How do the authors train individuals with the full datasets? Are these individuals trained with different initializations?

 - (Optional) As described, the model's generalization is not merely correlated with sharpness, which aligns with some recent advanced SAM variants. Thus, integrating these advanced variants [1][2] with SharpBal would be more beneficial for studying the trade-off between sharpness and diversity.

References:

[1] Random Sharpness-Aware Minimization. In NeurIPS 2022.

[2] Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization. In CVPR 2023.

Limitations:
The authors have provided **Limitations** section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes SharpBalance, that is a method aiming to investigate the relationship between sharpness and diversity for deep ensembles.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- SharpBalance looks quite effective for the out-of-distribution setting. The goal of balancing sharpness and diversity within ensembles is an important idea.  
- Great theoretical analysis

Weaknesses:
- The authors are aware of the paper called “Diversity-Aware Agnostic Ensemble of Sharpness Minimizers” [1], the idea is quite like the proposed paper, they aim to investigate the relations between sharpness and diversity on ensemble learning. I suggest the authors to discuss the main differences between both.    

[1] Anh Bui, Vy Vo, Tung Pham, Dinh Phung and Trung Le, Diversity-Aware Agnostic Ensemble of Sharpness Minimizers, arXiv:2403.13204. 

- Regarding the baselines the authors only compare SharpBalance with SAM. Nevertheless, newer, and stronger baselines like GSAM [2] and OBF [3] should also be benchmarked since they are the current state-of-the-art. 

[2] Zhuang, J., Gong, B., Yuan, L., Cui, Y., Adam, H., Dvornek, N., Tatikonda, S., Duncan, J., and Liu, T. Surrogate gap minimization improves sharpness-aware training. arXiv preprint arXiv:2203.08065, 2022. 

[3] Vani, A; Tung, F; Oliveira G; Sharifi H. Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics, International Conference on Machine Learning (ICML) 2024.  

- Another point to improve are the datasets. I strongly suggest the authors to benchmark with at least a couple large scale datasets. Options are ImageNet-V1 [4] for training and ImageNet-Real [5] and ImageNet-V2 [6] for testing, ImageNet-R [7] for out-of-distribution robustness benchmark and ImageNet-Sketch [8].  

[4] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. IEEE, 2009 

[5] Beyer, L., He ́naff, O. J., Kolesnikov, A., Zhai, X., and Oord, A. v. d. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020. 

[6] Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do imagenet classifiers generalize to imagenet? In Interna- tional conference on 
machine learning, pp. 5389–5400. PMLR, 2019. 

[7] Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340–8349, 2021. 

[8] Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pp. 10506–10518, 2019.

Limitations:
- The proposed method is not a significant improvement for ID scenarios as the authors claim. I would tone that down over the whole text. Maybe clearly state that SharpBalance is indeed superior to OOD scenarios and competitive on ID settings.  

- I would not claim the phenomenon called sharpness-diversity trade-off is a discovery, paper [1] is addressing the same phenomena and it was publicly available on arxiv before submission and on openreview since the beginning of the year.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Ensemble methods and sharpness-aware optimization techniques are well-known strategies for improving generalization. This work identifies a trade-off between sharpness and diversity, observing that reducing sharpness can diminish diversity and harm ensemble performance. Through theoretical and empirical analysis of this sharpness-diversity trade-off, the authors present SharpBalance, an algorithm for training ensembles with sharpness-aware solutions without sacrificing diversity. Evaluation results on CIFAR-10/100, TinyImageNet, and their corrupted variants confirm the effectiveness of SharpBalance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Ensemble methods and sharpness-aware optimization techniques are both prominent approaches for improving generalization. The aim of this work, which combines these two approaches, is well-motivated.
- While the theoretical analysis uses the variance metric to indicate diversity, the experimental results show consistent trends across different diversity metrics. It suggests that the proposed analysis is widely applicable to the general concept of diversity.
- Extensive empirical results effectively validate the theoretical analysis. The summary plots of the results are generally highly readable.

Weaknesses:
- The evaluation results are centered exclusively on classification accuracy; since ensembling usually highlights both predictive accuracy and uncertainty, relying solely on accuracy to assess overall performance is insufficient. 
- Specifically, for the corrupted CIFAR benchmark, uncertainty metrics like negative log-likelihood or expected calibration error are more important than test accuracy, but these aspects are not currently considered.
- It seems that all experiments were conducted exclusively with residual networks. It is essential to verify if the proposed analysis and algorithm are applicable to other architecture families as well.

Limitations:
Section 5 addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wJAF8TGVUG;"REVIEW 
Summary:
The paper introduces S-MolSearch, a framework for ligand-based virtual screening in drug discovery that addresses the challenges of limited and noisy binding affinity data. By utilizing molecular 3D information and semi-supervised contrastive learning, S-MolSearch processes both labeled and unlabeled data to train molecular structural encoders and generate soft labels for the unlabeled data, drawing on inverse optimal transport principles. The framework outperforms existing structure-based and ligand-based virtual screening methods, as evidenced by its superior performance on the LIT-PCBA and DUD-E benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Well-written
- Well-organized experimental settings and comparison methods

Weaknesses:
- There is a lack of discussion on the reasons behind the performance differences and improvements, with only numerical comparisons of the experimental results.
- There is insufficient experimentation and consideration regarding the time required for virtual screening.
- There are no results for experimental metrics such as AUROC or BEDROC, which were used in previous studies.

Limitations:
The paper addressed limitations and potential social impacts in Appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ""S-MolSearch,"" a semi-supervised contrastive learning framework designed for ligand-based virtual screening in drug discovery. This framework uniquely leverages labeled binding affinity information to produce soft labels for unlabeled molecules, integrating 3D molecular structures and binding affinity data. The paper also proposes a novel semi-supervised learning paradigm that combines contrastive learning with Inverse Optimal Transport (IOT).

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The supervision idea is novel and useful, and the target application is very impactful with broad implications.
2. The paper is well-written and the experiments are comprehensive.

Weaknesses:
1.	Memory Consumption Concerns: The model employs a parallel architecture with two f_\theta encoders and one g_\phi encoder, based on the Uni-Mol framework. Although utilizing pretrained models has shown significant performance benefits, the paper should address potential memory management strategies, especially for future applications involving molecules with a greater number of atoms.
2.	Utilization of 3D Structures: The paper promotes a novel semi-supervised contrastive learning paradigm, yet the core contribution does not seem to revolve around the innovative use of 3D structures, as this capability primarily stems from the Uni-Mol architecture. It would be beneficial if the authors could clarify any specific enhancements made to ensure the effective preservation and utilization of geometric information within the model. Absent such enhancements, clearer distinctions should be made regarding the role of 3D structures to prevent misconceptions about the paper presenting a new geometric deep learning technique.
3.	Clarity in Section 3.4: The explanation of how $\Gamma$, which approximates the distribution of $C$ under constraints from $U(p,q)$, relates to the continuous optimal transport problem is not clear. Moreover, the motivation and necessity of soft labels, beyond experimental justifications, needs further elaboration. The section would benefit from additional visual aids or high-level descriptions, akin to the clarifications provided in sections 3.3 and 3.5, to aid in comprehension.
4.	Component Efficacy in Table 3: There appears to be a discrepancy in the impact of model components across different benchmarks—soft labels are pivotal for DUD-E, whereas pretraining is more crucial for LIT-PCBA, with soft labels showing minimal importance. Insights into this inconsistency would be valuable. Furthermore, an evaluation of how the Uni-Mol encoder alone performs on these tasks would provide additional context on the effectiveness of the proposed enhancements

Minor points and typos:
L153-154 is not clear.
L162: It would be beneficial to include illustrations of $M_{sup}$ in the figures for clarity.
Formula 2 and L 168: It is better to give intuitive explanations of $1_N$.
L184: Inconsistent notation. $g(\psi)$ or $g_\psi$?
L281: Misplaced comma.

Limitations:
The focus of the paper is predominantly on molecule binding affinity, heavily relying on a pretrained encoder. This reliance could limit the model's applicability across a broader spectrum of bioinformatics data. A more detailed discussion on the dependency on pretrained models and potential strategies to mitigate this limitation would enhance the paper's breadth and applicability.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed a Ligand-based Virtual Screening method S-MolSearch. which can  leverages molecular 3D information and affinity information in semi-supervised contrastive learning.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The method is able to leverage both labeled and unlabeled data simultaneously and achieves excellent performance on DUDE and Lit-PCBA benchmarks.
2. The approach of using the principles of inverse optimal transport for semi-supervised learning is quite innovative and worth adopting.
3. The ablation experiments are sufficient, and the experimental section is quite robust.

Weaknesses:
1. In the method section, it is unclear to me whether during inference only encoder$g_{\psi}$ is used, or both $\psi$ and $f_{\theta}$ are used  simultaneously?
2.  If the application scenario involves a newly provided protein without reference molecules, how should ligand-based virtual screening methods handle this situation?

Limitations:
S-MolSearch predominantly focuses on the molecular affinity data, omitting broader biochemical interactions, which suggests a potential area for improvement.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method for ligand-based virtual screening based on contrastive learning and inverse optimal transport. Two molecule encoders are trained. The first encoder is trained using a contrastive loss function on the ChEMBL data by pairing compounds that are active toward the same protein, and compounds active toward different targets are treated as negative pairs. Next, the second encoder is trained by using the pseudo-labels produced by the first model. The proposed model is tested on two benchmark datasets, DUD-E and LIT-PCBA. Additionally, an ablation study is conducted, and the impact of the labeled data scale is visualized.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Originality:
- The approach seems to be novel. I have not found any similar papers that use optimal transport for the ligand-based virtual screening task.

Quality:
- The theory described in the paper is formally proven in the Appendix.
- The proposed method obtains excellent results in both tested benchmarks.
- The quality of the learned representation is demonstrated in Figure 2.

Clarity:
- The paper is written clearly and is easy to follow.
- Figure 1 shows the idea of the model very clearly.

Significance:
- The presented method is an interesting and effective way to utilize all the available public data to build a strong model for ligand-based virtual screening.

Weaknesses:
Quality:
- It would be interesting to see some qualitative examples of molecules that were found to be similar to the active compounds in the virtual screening process. Do the trained similarities correlate with the Tanimoto similarity?

Clarity:
- Does the “sup” subscript in Section 3.4 correspond to the “label” subscript in Proposition 1? What is the difference between these two sets?

Minor comments:
- A typo in line 151, “we employs InfoNCE.”
- In line 183, something is missing before “1”.

Limitations:
The limitations have been described.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wIE991zhXH;"REVIEW 
Summary:
This paper considers bandits with preference feedback. It first constructs a novel confidence set that covers the ground truth with high probability. Then from a Stackelberg game perspective, it proposes an efficient algorithm that enjoys tighter regret bound than SOTA.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The technique used to construct the confidence set is interesting. The resulting confidence set is tighter.
2. The Stackelberg game perspective is interesting, and allows the author(s) to design the algorithm with better exploration-exploitation trade-off as demonstrated in the experiment.

Weaknesses:
The major concern is the practical applicability of the algorithm. Seems that the proposed algorithm can hardly scale up to a higher dimension (e.g., dimension equals to 7). In the proposed algorithm, a complicated sequential optimization problem needs to be solved. Notably, the experiment only considers two-dimensional problem, in sharp contrast to recent works (e.g., 12-dimensional problem considered in Xu et al. [2024]).

Limitations:
Some major comments:
1. The paper discusses the comparisons to [Xu et al. 2024]. Seems that the theoretical gain of $T^{\frac{1}{4}}$ mainly comes from tighter confidence set rather than the design of the algorithm. If the algorithm POP-BO in [Xu et al. 2024] is equipped with this tighter confidence set, it could also achieve the same regret bound. To see which algorithm is empirically better, it would be interesting to compare the proposed algorithm with POP-BO equipped with the tighter confidence set in this paper.

Besides the points mentioned above, here are some minor comments:
1. In line 109, I guess it should be {$0, 1$} instead of $[0, 1]$.
2. In line 178 to 179, typo in ""ridge estimator estimator"".
3. In line 574, abuse of the notation $s$.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers novel game-theoretic acquisition function for pairwise action selection with preference feedback. It is tailored to the setting with infinite domains and nonlinear kernelized rewards. The preference-based confidence sequences for kernelized utility functions are shown to be tight and anytime valid. The proposed algorithm MAXMINLCB is shown to satisfy a sublinear regret. Various simulations were conducted to showcase the advantage of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Although preference-based bandit optimization with linear utility functions is fairly well understood, such approaches cannot capture real-world problems with complex nonlinear utility functions. This paper aims to close this gap. The considered problem is timing and interesting. 

2. The technical contribution is non-trivial. Although there have been attempts to prove convergence of kernelized algorithms for preference-based bandits, such works employ a regression likelihood model which requires them to assume that both the utility and the probability of preference lie in an RKHS. Moreover, a sample-efficient algorithm is lacking for such approaches. In contrast, this work uses a kernelized logistic negative log-likelihood loss to infer the utility function, and provide confidence sets for its minimizer.

3. Some theoretical result, like Kernelized Logistic Confidence Sequences in Theorem 2, is also of independent interest. 

4. In spite of a theoretical paper, it is well written and is easy to follow.

Weaknesses:
1. In practice, how to determine the hyper-parameters like $\gamma_t$, $L$, and $B$ in (5)? Is there any data-driven way to select them?

2. In the main Theorem 6, the regret bound is $\gamma_T^{D}\sqrt{T}$. The term $\gamma_T^{D}$ is the T-step information gain of kernel, which is also a function of $T$. The authors claim that this rate improves that of Xu et al. (2024) by a factor of $T^{1/4}$. However, the cumulative regret bound in Theorem 5.2 of Xu et al. (2024) is of a similar order. Xu et al. (2024) also provided explicit regret upper bounds for various common kernels in Theorem 5.5. Hence, it is also interesting to provide an explicit form of $\gamma_T^{D}$ for some common kernels, and to compare these regret upper bounds in a fair way. 

3. In Figure 1, the authors presented the result for the Ackley function, which shows a clear advantage of the proposed method. However, in more extensive simulations (e.g., Matyas function in Figure 6, ) in the appendix, the proposed method is outperformed by the competitors. It is helpful to provide some discussion on these results, and offer some insights on when the proposed method would work well. This is helpful for practitioners.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper examines the problem of bandit optimization with preference feedback in large domains and nonlinear (kernelized) rewards. It introduces MAXMINLCB, which adopts a game-theoretic approach to action selection under comparative feedback. Additionally, it proposes kernelized preference-based confidence sets, which can be utilized in related problems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
(1) Rather than jointly selecting the arms in dueling bandits, the proposed method jointly optimizes both actions by choosing them as the equilibrium of a two-player zero-sum Stackelberg game. This approach enables a more efficient exploration/exploitation trade-off.

(2) The regret guarantee presented in this paper is tighter by a factor of  $T^{1/4}$ compared to Xu et al. (2024).

Weaknesses:
(1) Although the paper uses a kernelized logistic model to approximate the rewards, this approach may remain too simplistic for capturing the complexity of rewards in real-world applications.

(2) The paper lacks a comparison in the experiments with the related work by Xu et al. (2024).

(3)  In real applications, it is more common to rank between two state-action pairs. However, the paper does not consider contextual information and solely focuses on the multi-armed setting, which is less interesting and useful.

Limitations:
Please see weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers bandit optimization with preference feedback over continuous action spaces and kernelized reward function. The goal in this problem is to minimize the dueling regret against an optimal action over a finite time-horizon. Previous works on this problem are either restricted to finite action spaces or linear reward functions. The proposed algorithm casts the problem as kernalized logistic regression and designs confidence sets for the relative preference between two actions. It then proposes an action selection strategy based on a game theoretic Leader-Follower formulation that utilizes these confidence intervals. The paper provides a regret bound as well as empirical evaluation for the proposed algorithm.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The main contributions of the paper are two-fold:

1. Expanding the existing literature on dueling bandits by studying kernelized reward functions under infinite and continuous action sets. This requires new techniques to bound the confidence intervals. 

2. Proposing a principled game-theoretic approach to action selection in dueling bandits that can be of further interest.

In my opinion these are two important contributions to the literature. Since these ideas are likely to be relevant to other learning problems with preference feedback such as RLHF, I think that the results in this paper have a good scope. The paper is well-written and the contributions are clear.

Weaknesses:
Experimental evaluation can include other algorithms that are known to perform better than RUCB such as RMED (Komiyama et al., 2015) and Double Thompson Sampling (Wu and Liu, 2016).

Limitations:
The paper can include a section of limitations of the current work in terms of social impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wHFaAH3E8z;"REVIEW 
Summary:
This paper introduces FasMe, a meta-learning approach for efficient precision matrix estimation in small sample settings. By leveraging meta-knowledge and maximum determinant matrix completion, FasMe reduces sample size requirements and improves computational efficiency. Experimental results show FasMe to be significantly faster and more accurate than existing methods, particularly in low-data environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) Paper investigates a key issue in precision matrix estimation and proposes a reasonable method to address the problem.

2) Paper provides thorough theoretical and experimental analyses to justify the method’s ability to reduce the sample requirement and enhance learning efficiency.

3) Paper has good representation.

Weaknesses:
I have few doubts about the method and experiments presented in the article.

Limitations:
nan

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a method to estimate sparse precision matrices from few samples. Theoretical properties of the proposed method are studied, and experiments on synthetic and brain fMRI data are presented.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths:
* The paper is overall well written, and fairly easy to follow and comprehend.
* Theoretical guarantees for sub-Gaussian distributed random variables are presented, and they seem to be novel contribution. 
* The experiments on the synthetic dataset clearly demonstrate improvement over the relevant competing methods.

Weaknesses:
Weaknesses:
* Currently quantitative results are presented for synthetic data. It would be nice to see more quantitative evaluations on benchmark datasets.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a meta-learning method for estimating the precision matrix on a new task with small data.
The proposed method uses common edges estimated from multiple auxiliary datasets as meta-knowledge. Then, it estimates the precision matrix on the new task, assuming its true edges contain all the estimated common edges (meta-knowledge). Some theoretical guarantees are also provided.
Experiments with synthetic and real-world datasets show the effectiveness and efficiency of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well-written and easy to follow.
- Concrete algorithm and its theoretical guarantees are presented (but, I didn't read their proof).
- Strong performance in terms of both accuracy and efficiency in the experiments.

Weaknesses:
- As the authors stated in the Limitation section, the assumption of Eq. 8 might not be held in general machine learning tasks, although it fits well in biological domains.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
wGjSbaMsop;"REVIEW 
Summary:
This paper explores the possibility of boosting the recommendation of a song in an automatic playlist continuation system by using a collective strategy of adding the song into the training playlists of the APC at a specific position. The paper shows that adopting a strategy that targets low frequency context makes it possible to very significantly boost the exposition of the song in the output of the APC.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
1: poor

Strengths:
- Very interesting idea which is novel in the domain of music recommendation
- Quite surprising experimental result that is worth sharing (possibility of boosting exposition of a song with the DirLof strategy).

Weaknesses:
- Limited scope:
    - The idea is tested in the limited context of automatic playlist continuation and is likely not adaptable to broader applications.
    - Only one APC model was tested, so the results may be specific to this model. It would have been a good idea to test the impact of the collective action on other models (such as the baseline proposed in the reference paper for the APC model).  Also, it’s unclear whether the effect would be robust to hyperparameter changes in the APC model.
    - the method is tested in a static context (not in the real world), so the actual impact of the method on usage (for instance, whether a user exposed through the APC to a song boosted by the collective strategy would listen to it or not) is not tested (this would require access to the production of a streaming music service, though)
- Very limited insights on the design of the efficient strategy (DirLof) and why it works. The presentation of the strategy is actually quite unclear, while it’s central to the paper.
- There may be other simpler baselines that would be worth testing. The paper shows that inserting the song at the end of the playlists is less performance than inserting the song randomly, which suggests that inserting it earlier in the playlist sequence may help. So a baseline that would always insert the song at the beginning of the sequence could have decent results (the first place is likely to be avoided as it would mean that the song would never appear as the target in the training of the APC, but low positions that appear regularly as targets in the training could be considered).

Limitations:
- As mentioned in the weaknesses, there are other simple baselines that would be worth testing. That would help support the claim that the specially designed DirLof strategy is actually efficient.
- Also the authors should comment on the possible transferability of their results to other APC models, and to modifications of hyperparameters of the model.

On the ethical side, the method is a bit too much presented as a way for artists to attack a recommender system, somewhat rerank their songs and make money out of it, which has some important ethical implications in terms of fairness among artists: As the payment system of most streaming services is based on subscription (so a fixed amount of money), artificially boosting an artist comes at the detriment of other artists.
I would then rather avoid this message of ""opportunity for artists"" in the paper (and even in the title) and turn it instead as a warning to music streaming platforms that recommender systems may lack robustness to attacks and that they should be aware of it and take action. I think presenting the paper in this second way would solve this ethical issue, but the current message is, to me, ethically borderline.

Minor comments
- The authors claim that “most large platforms have shifted from relying on collaborative-based models for APC to building deep learning based recommenders …” but 1) the references are only Spotify based 2) the references are research papers which doesn’t mean that the shift actually happened and that platforms no longer use collaborative-based models part in their complex architecture.

- In equation (1):
    - The authors likely want to specify that the recommended song shouldn’t be part of the playlist i.e S’ **∩** p = **∅** in the argmax
    - The argmax actually corresponds to the top K elements in terms of similarity, which is quite straightforward. But the sum in the equation makes it not very clear at first read. It could be worth it to state it explicitly before the equation.
- There are several notations that use the letter h (song embedding, playlist embedding, playlist mapping). I think using different letters may help make things clearer.
- In Figure 3, it seems like s* is subtituted to s_i (there is no s_i in (b)) while just before equation 4, it’s said that only insertions are considered.
- “*Existing data poisoning techniques, for example, operate in different settings, not complying with Constraint 1 and typically assuming white-box access to the model or involving test-time profile manipulations.*” ⇒ this needs references.
- *“Thus, collective action can make a tremendous difference for these artists: suppose an artist’s song is streamed 10,000 times, yielding a revenue for the artist of $40 for royalties of $0.004 per stream [32]; an amplification of 25 would increase this revenue to $1, 000.”* I understand this is purely illustrative, but the figures won’t reflect much the truth. First 0.004$ per stream is an average that depends on the platform, but also on the kind of registration of the user and on the country of the user with quite important variations. Second, several platforms (Spotify, Deezer…) are shifting to a payment model were all streams don’t have the same value, especially recommended streams are “discounted”, and some “real” artist may get a boost. Once again, I get it’s for illustrative purpose, but it brings confusion on how music payment system works, and then should be avoided in my opinion.
- *“Moreover, when considering s* as a relevant recommendation, collective action even enhances the system’s performance”* Isn’t this completely obvious? if presentation of s* is boosted (which happens, given the amplification), other recommendations are barely affected (as shown on figure 6, solid line), and you consider s* a valid recommendation, then for sure recommendation metrics will increase, won’t they?

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
They proposed a strategy for the streaming platform users to collectively act in order to promote targeted songs. The promotion efficacy is measured by the targeted songs' recommendation frequency boost at testing time. This strategy is approved to be effective through simulation experiment.  Another finding is that this strategy has minimum impact to the performance of the recommendation system as a whole, i.e., by preserving user experience to other non-targeted songs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* This is a novel idea, presented in an interesting domain with a good amount of related work.
* The writing of the paper is clear. The motivation is sound. 
* The experiment performed by the author successfully verified the efficacy of the collective strategy.

Weaknesses:
* Limited scope. See limitations below.
* Lack of technical novelty and contribution. The evaluation result would be a good report but I would recommend the author to seek publication in a different conference.

Limitations:
The strategy could be effective but it is built on top of the assumption that the serving platform has deployed minimum control against collective behavior. As I am aware, there are various user-side anomaly detection mechanisms usually deployed in production in streaming services like what is described in the paper. As an example, there could be real time monitoring of such collective behaviors. Such an anomaly detection system could be constructed by building a user-entity graph. In this use case, we could use artist or songs as the entity. When in a short period of time, there are bursty events that are related to an entity that happens (user promoting a song in their playlist), it could be an important indicator that something unusual happens (because this is not a popular song, it does not receive so many promotes on average). A follow-up action in the system could tag those relevant data as ""spurious"" thus they never go to the training data of the transformer model (for continuous training). As a result, the collective action could have a significantly less effect when such monitoring is turned on; or depending on what threshold they are set up.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper shows that strategic collective action by a small fraction of the population can lead to significant amplification of a particular song in a recommender system.  The authors propose two strategies for the collective (for a transformer-based song recommender) that achieve this amplification.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The setup is original and focuses on strategic collective action by a fraction of users in a recommender system and how this can increase a target song’s reach.
- The performance loss for the platform is negligible, the collective’s strategies are not adversarial (for e.g. fake profiles, artificial perturbations) and based on 1-edit distance to the original playlist. The paper shows that recommendations are largely preserved.

Weaknesses:
- The experiments follow the MF-Transformer in [7], to make the paper self-contained it would be beneficial to have a description of $\phi(.)$ and of $g(.)$ and the loss function in Section 2.1 or the Appendix. 
- I found the strategies in Sec 3.2 hard to parse, perhaps a figure showing the original playlist and the possible changes a user in the collective can do under the two strategies would be helpful. 
- Minor:  I think the notation h(.) is overloaded for the song/playlist embedding in 2.1 and for the strategy mapping which inserts s* into a playlist. 	Also, fig 6 could use different colors for the different strategies.

Limitations:
The authors discuss limitations in Appendix  A.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This research work proposes a novel solution to promote songs on music streaming platforms strategically. 

Under the following assumptions:

1. Fans can collaborate to promote a specific song by collectively reordering playlists.
2. The visibility of a song in a playlist affects its recommendation frequency.
3. Users are influenced by the position of songs in playlists when making listening choices.
4. The impact of collective action on song visibility is measurable and significant.

The authors suggest that fans strategically reorder playlists to promote a targeted song, thereby increasing its visibility in the recommender system. By leveraging algorithmic collective action, even small groups of fans can substantially impact the recommendation frequency of the promoted song. This strategy aims to enhance the visibility (capability of being discovered) of songs and artists, which will benefit both fans and musicians in the music streaming industry. 

 The evaluation focuses on quantifying the amplification of recommendations achieved by strategically placing songs in playlists, using metrics such as recommendation probability and change in the number of recommendations for a song. The evaluation also includes the impact on the recommendations of other songs and the overall performance of the recommender system. The analysis of results reveals that the collective action strategies can lead to a substantial increase in the recommendation frequency of the targeted song, with up to 25x  higher recommendation probability compared to average songs.

The main contributions are:

1. The paper introduces **two innovative collective** action strategies where participants strategically insert a target song into their playlists to promote an emerging artist. These strategies aim to increase the recommendations of the targeted song at test time, thereby boosting the artist's visibility on the platform.

2. The research demonstrates that even **small collectives**, controlling less than 0.01% of the training data, **can achieve significant amplification of recommendations** by strategically placing songs in playlists. This finding highlights the effectiveness of algorithmic collective action in promoting songs without major disruptions to the user experience.

3. Preservation of Other Recommendations, as the study reveals that while promoting a specific song through collective action, the recommendations of other songs are largely preserved. This indicates that the proposed strategies can enhance the visibility of targeted songs without significantly compromising the overall recommendation system's performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- Its innovation is a significant strength as it provides a new approach to increasing the visibility of emerging artists in music streaming platforms.
- Empirical Validation and Real-World Application: the research is empirically validated using an open-source APC model deployed on Deezer, a platform with millions of users. 
- important result: the study demonstrates that even small collectives can achieve substantial amplification of recommendations by strategically placing songs in playlists
- The findings show the potential for diverse artist promotion, which can make fairer use of the platforms but also fights against the long-tail problem in recommender systems. It can also help the serendipity effect.

Weaknesses:
-  The paper assumes that users are influenced by the position of songs in playlists when making listening choices. This assumption **may oversimplify user behavior and overlook other factors** that influence song recommendations and user engagement, potentially leading to biased results.

Limitations:
The paper does not explicitly discuss possible limitations of the approach to addressing problems of privacy and fairness. However, considering the ethical implications of data manipulation and collective action in recommender systems is crucial for ensuring transparency and equity in algorithmic interventions.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wGP1tBCP1E;"REVIEW 
Summary:
This paper derives an upper bound of the Lipschitz constant for diffusion classifiers. Then, it proposes Exact Posterior Noised Diffusion Classifier (EPNDC) and Approximated Posterior Noised Diffusion Classifier (APNDC) by deriving ELBO upper bounds on $\log p (x_\tau)$ and thereby enabling classifying noisy images. The APNDC achieves state-of-the-art certified robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The theory is cool and the math is intriguing. I like this direction because it leverages the shared Gaussian structure in diffusion models and randomized smoothing while circumventing the challenges of attacking diffusion models. The empirical evaluation results (especially Figure 2a) are also impressive.

Weaknesses:
In my opinion, some of the contents are not explained very clearly. Please see below and the contents of the ""Questions"" section.

- Table 4 is nice. However, I wish it was in the main text instead of the appendix, because the current main text misses the discussion on how to calculate the certified robustness for the proposed models.
- Figure 2 doesn't present the certified radius with a conventional diffusion classifier, as derived in Eq. (11). Since (11) is an important contribution of this work, I believe it should be included.
- I would like to see an ablation study on $\sigma_\tau$, but could not find this result.

Overall, this is still a nice paper.

Limitations:
As is the case for numerous diffusion classifier paper, the computation complexity, while improved in this paper, is far from ideal. This paper evaluates the method on a small subset of CIFAR-10 and ImageNet, probably due to this reason.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors investigate the certified robustness of diffusion classifiers. For this purpose, they first show that these classifiers have O(1) Lipschitzness and subsequently achieve tighter robustness bounds through Bayes' theorem and the ELBO.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1: Using diffusion models to generate large amounts of synthetic data is one of the most promising approaches to improve empirical and certified robustness in recent years. The authors utilize diffusion models directly to achieve high certified robustness. 

S2: While prior work has investigated the robustness of diffusion classifiers, they do not provide certified guarantees. This gap is addressed in this work.

S3: The work provides both relevant empirical and theoretical contributions

Weaknesses:
W1: References could be ordered by appearance (minor)

W2: The nature of diffusion classifiers induces a considerable computational overhead compared to standard classifiers. However, the authors try to address this issue through their sift-and-refine algorithm. Still a comparison between different methods w.r.t. inference time would have been informative. (could also include standard classifiers). Note that I would not consider large computational cost as a negative point concerning paper acceptance I just believe that a comparison would be helpful for the reader. Still the appendix provides some information w.r.t. time complexity so I view this as a minor issue. 

W3: Appendix D is very short and could be incorporated into the paper (at least in the camera-ready version)

Limitations:
Limitations are included in the appendix.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work proves that diffusion classifiers possess inherent robustness to adversarial attacks by demonstrating their O(1) Lipschitzness and establishing their certified resilience. By generalizing these classifiers to handle Gaussian-corrupted data and using evidence lower bounds for likelihood approximation, the research demonstrates the superior certified robustness of Noised Diffusion Classifiers (NDCs).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper showcases the robustness of the proposed Noised Diffusion Classifiers (NDCs), achieving high certified robustness on the CIFAR-10 and ImageNet 64x64 datasets. The study also provides a proof of O(1) Lipschitzness for diffusion classifiers.

Weaknesses:
1. The proposed method combines two existing techniques, diffusion classifiers and randomized smoothing, which is not sufficiently novel. The paper needs to better highlight what sets this approach apart from existing methods and how it fundamentally advances the field. Although the authors attempt to establish a theoretical framework, the derivation of the Lipschitz constant and its implications are not sufficiently detailed, leaving unanswered questions about the robustness guarantees.

2. The experimental evaluation relies heavily on the small CIFAR-10 and ImageNet 64x64 datasets. Expanding the experiments to include larger datasets, such as ImageNet-1K, would provide a more comprehensive assessment.

3. The paper discusses techniques to reduce time complexity but does not convincingly demonstrate the practicality of the proposed methods with experimental results, such as throughput or inference latency. A more detailed analysis and comparisons of computational efficiency, especially in relation to existing methods, are needed.

Limitations:
No potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a theoretical analysis of the enhanced robustness in diffusion-based classifiers and introduces a generalized Noised Diffusion Classifier, EPNDC. The authors utilize the Evidence Lower Bound (ELBO) of each conditional log-likelihood $\log p(x_\tau | y) $and Bayes' theorem as the logits for each class. They identified that EPNDC is time-consuming due to the iterative computation of two conditional ELBOs. To address this, they leverage the ELBO of an ensemble of EPNDC to approximate the expected likelihood as logits without additional computational cost. Additionally, they developed variance reduction and sift-and-refine techniques to reduce time complexity. Experimental results demonstrate that APNDC achieves significantly better robustness without requiring extra training data, fewer diffusion steps, and a reduced number of samples needed to estimate the Lipschitz bound.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The entire paper is logically structured with a clear progression, enabling readers to understand it well. From Algorithm 1 to Algorithm 2 to Algorithm 5, the authors continuously explore problems, improve algorithms, and provide thorough analysis and theoretical proofs.

2. The experiments are comprehensive, and compared to the benchmark, EPNDC shows significant improvements in certified accuracy. This demonstrates EPNDC's high scalability in handling large datasets with numerous categories.

Weaknesses:
1. Some causal relationships are unclear or lack citations, requiring further explanation from the authors. For instance, in Line 156: What does the ""nabla operator"" refer to? It is neither explained nor cited. In Lines 161-164: ""However, similar to the weak law of randomized smoothing, such certified robustness has limitations because it assumes the maximum Lipschitz condition is satisfied throughout the entire perturbation path. As a result, the robust radius is less than half of the reciprocal of the maximum Lipschitz constant."" The causal relationship here is unclear and needs further clarification.

2. Although the diffusion classifier is highly scalable and robust, its clean accuracy on ImageNet is still far behind the current state-of-the-art (90%+). More details can be found at [https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet).

Limitations:
The authors didn't address the limitations of APNDC, but Diffusion Classifiers are still far behind the current SOTA in classification accuracy. These are some potential limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wFzIMbTsY7;"REVIEW 
Summary:
This paper investigates an emerging foundation model, Mamba, in Reinforcement Learning (RL) scenarios and compares it with Transformer in terms of effectiveness and efficiency. The authors find that in-context RL methods with Mamba as the backbone are generally more efficient than Transformer, but there is no significant improvement in effectiveness. Then, this paper proposes a Hybrid Mamba (HM) with the merits of transformers and Mamba in high-quality prediction and long-term memory. Finally, this paper conducts experiments on three benchmarks to exhibit its improved effectiveness and efficiency.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.	The paper is commendably well-written and coherent, effectively explaining complex ideas in an accessible manner. The authors explored the potential of the widely discussed model Mamba in the context of RL and compared it with Transformer in terms of effectiveness and efficiency.
2.	The authors proposed a novel hybrid model that inherits the merits of both Transformer and Mamba in a goal-conditional manner. The main advantage of using the hybrid structure is that when the time horizon is very long, as in the D4RL tasks, several episodes/trials are required for good in-context learning, as in the larger Dark Room and Tmaze environments.
3.	HM improves training and inference speed by reducing the horizon of the transformer model. This can be particularly important in applications such as robotics, which require high-frequency control.
4.	The experimental evaluation, meticulously designed to include several baselines and diverse tasks, demonstrates the algorithm's strengths.

Weaknesses:
1.	The baseline AD (Mamba) in Figure 2 and the baseline DM in Figure 3, which appear to be AD (Transformer) and DT variants, are crucial for the readers' understanding of how Mamba replaces the Transformer architecture. However, the lack of explanation of these two baselines in the experimental setup section might confuse readers.
2.	Some experimental settings are not explained clearly. In Section 5.3, the authors do not explain what GPU device they used. Although the device is introduced in Appendix A, it is recommended that it be explained clearly in the main text.

Limitations:
The author discusses limitations and potential negative societal impacts in Section 6.

Rating:
10: strong accept, should be highlighted at the conference

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents Hybrid Mamba (HM), a method that combines the Mamba model and Transformer to enhance reinforcement learning (RL) performance. HM leverages Mamba to generate high-value sub-goals, which then condition the transformer, leading to significant improvements in online testing efficiency and task-solving capabilities.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and clear to read.
2. HM significantly accelerates testing speed, achieving up to 28 times faster results than baseline methods.
3. HM demonstrates superior performance across various benchmarks, such as D4RL, Grid World, and Tmaze.

Weaknesses:
1. This paper claims to present a in-context RL approach. The motivation of this paper is concerned with the problems encountered with the no-gradient updates in-context approach (line 28), where the policy network does not require parameter updates. However, this paper uses a global update approach, which is closer to gradient-based and conditional-based offline RL. It seems to contradict the original intention of this paper. 

2. HM benefits from using a powerful subgoal encoder (Mamba in this case) and conditioning the policy network with subgoals. The performance improvement is expected and unsurprising due to the advantages inherent in conditional-based RL algorithms. Hence, it is necessary to further explain the unique contributions of combining Mamba and causal transformer in this paper.

3. If the sub-goal encoder are replaced with other advanced and efficient sequence encoders (e.g., flash-attention1/2 [1,2], x-lstm [3]), would it also yield better or more efficient performance? 

4. The experiments demonstrating HW's efficacy in capturing long-term dependencies are unconvincing. Achieving good results in tasks with an arbitrarily horizon (e.g., Tmaze) does not necessarily prove effective long-term memory embedding. It is crucial to test the stability and performance of HM with varying horizon lengths or other length-based settings. For example, Mamba’s original paper [4] demonstrated the ability to capture long-term dependencies through the scaling laws.

5. Could the authors clarify in which specific aspects HM's training time is faster than DT's? Since HM appears to be a combination of Mamba and DT.

6. There are parts of the paper that are not clearly explained. For instance, in lines 228-233, it is mentioned that the transformer predicts a c-step action sequence (named $a_1$ here) through the sub-goal $z_t$ and another c-step action sequence (named $a_2$) through valuable sub-goals from offline data. How are $a_1$ and $a_2$ subsequently updated or processed? 

7. (minor) The paper contains some typos and inconsistencies in tense usage. For example, in the related work section, the section on Mamba uses the present tense, while the section on in-context RL uses the past tense. These should be corrected for consistency. In addition, what's the meaning of the different gaussian distribution figures in Figure 1?

*Reference:*

[1] Dao T, Fu D, Ermon S, et al. Flashattention: Fast and memory-efficient exact attention with io-awareness. NeurIPS 2022.

[2] Dao T. Flashattention-2: Faster attention with better parallelism and work partitioning. ICLR 2024.

[3] Beck M, Pöppel K, Spanring M, et al. xLSTM: Extended Long Short-Term Memory. arXiv 2024.

[4] Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv 2023.

Limitations:
The authors raise some limitaions, for example, how to control the setting of hyperparameter $c$, which is not addressed in this paper but is claimed to be solved in the future.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates to utilize the Mamba [1] architecture for In-Context RL task. Addressing this task with Transformer architecture is effective while it is very inefficient due to the quadratic computation overhead of Transformer. The Mamba can reduce this overhead dramatically while sustain the performance somewhat. The application of State-Space Models (SSMs) to In-Context RL task is studied in [2], but different from [2], they combinationally utilize Mamba and Transformer as high-level memory and low-level (short-term) memory. Additionally, as Mamba predicts the sub-goal for the Transformer short-term memory, they improved the performance. Through this modeling, they can achieve better performance than previous works while improving the efficiency.

[1] Gu, Albert, and Tri Dao. ""Mamba: Linear-time sequence modeling with selective state spaces."" arXiv preprint arXiv:2312.00752 (2023).

[2] Lu, Chris, et al. ""Structured state space models for in-context reinforcement learning."" Advances in Neural Information Processing Systems 36 (2024).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Appropriate modeling is applied in this study. While the effectiveness of hybrid modeling of SSMs and local Attention has been previously explored in [1], the authors effectively implement this concept for the In-Context RL task with new functionalities, such as predicting high-value sub-goals.
- The introduction and methodology sections are well written. The motivation is clearly articulated, and the logical flow of their method proposal is coherent. The empirical analysis comparing Mamba and Transformer in RL tasks convincingly demonstrates the need for more advanced modeling.
- The paper provides extensive empirical analyses. It shares experimental results on multiple benchmarks, including ablation studies and performance changes with varying hyperparameter values.

[1] De, Soham, et al. ""Griffin: Mixing gated linear recurrences with local attention for efficient language models."" arXiv preprint arXiv:2402.19427 (2024).

Weaknesses:
- The high-level encoding is done by encoding the intervalled trajectories (e.g., every $c$ -th trajectory), which might miss important information in the middle of the interval.
- The section on Hybrid Mamba with Valuable Sub-goals is initially confusing, especially regarding the relationship between Mamba’s sub-goal prediction and the collected valuable sub-goals. Discussing this relationship at the beginning of the Valuable Sub-goal section could help readers understand the content more easily.
- One of the experimental results differs from my expectations, but the paper does not provide an analysis for this. I will address this in the Questions section.

Limitations:
The authors properly addressed their limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes Hybrid Mamba (HM) for in-context RL. Existing in-context RL methods are predominantly based on the Transformer architecture. Transformers come with quadratic complexity of self-attention and are computationally costly. Consequently, the authors propose a hybrid architecture that uses Mamba to compute sub-goals from long-context, which are fed into a low-level Transformer policy. The authors conduct experiments on grid-worlds and D4RL to evaluate their method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Relevance**

The paper aims at deploying the Mamba architecture for in-context RL, which is very relevant given the quadratic complexity of the Transformer architecture.
This results in clear benefits in terms of time complexity.

**Experimental results**

Empirical results on simple gridworld environments and D4RL seem convincing and their method exhibits significant gains compared to Transformers.

Weaknesses:
**Presentation**

The methodology raises some questions and should be improved, in particular:
 - What is the reasoning behind sampling the sub-goal from a multi-variate Gaussian?
 - How does this compare to using a fixed representation? (e.g., similar to CLS token)
 - Why is the done-flag in Hybrid Mamba necessary? Do other methods (e.g., AD [1]) use this as well?
 - What does “Extra high-value states” mean?
 - What is the intuition behind removing actions from the Mamba context?
 - What effect would dropping actions have in other methods?

Furthermore, the construction of “valuable sub-goals” is unclear.
One way to improve clarity would be to shorten the section on preliminaries and instead add more details to the Method section.
Figure 2 and Table 2 are missing the performance curves/scores for HM without valuable subgoals.
Finally, Figure 1 can be improved to enhance clarity.

**Significance of results**

The authors evaluate primarily on simple grid-world environments and rather simple robotics tasks. However, it is unclear how well HM generalizes to more complex tasks as used in other works [2].

**Evaluation**

The authors change their evaluation methodology from improvement curves on gridworlds (Figure 2) to average performance scores on D4RL (Table2).
On D4RL, HM seems to clearly outperform other methods.
However, the authors do not show in-context improvemenst which raises the question whether HM actually learns to improve in-context. Can the authors clarify, why no in-context improvement curves are shown for D4RL?

**Ablation studies**

Some ablation studies are missing and would add more depth to understanding the proposed method, in particular:
- What is the impact on performance of including the done-flag in Mamba?
- What effect does it have on other methods?
- What is the impact on performance of removing the action condition in HM?
- What effect does the same intervention have on other methods?


 [1] Laskin et al., In-context Reinforcement Learning with Algorithm Distillation, ICLR 2023
 [2] Raparthy et al., Generalization to New Sequential Decision Making Tasks with
In-Context Learning, ICML 2024

Limitations:
The authors highlight that setting the context length c to a fixed value as a current limitation of their method. However, a notable limitations are missing, namely that their evaluation is limited to simple environments, while it is unclear how well HM performs on more complex or new tasks.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wDirCeTIoz;"REVIEW 
Summary:
The paper introduces Distributed Lion, a variant of the Lion optimizer, tailored for distributed training environments. Lion, known for its memory and computational efficiency, is adapted to reduce communication costs between workers and a central server. This is achieved by communicating binary or low-precision vectors rather than high-precision floating-point vectors. The paper presents theoretical convergence properties and empirical results that demonstrate Distributed Lion’s robustness and efficiency across various tasks, worker counts, and batch sizes. It shows comparable performance to standard Lion or AdamW optimizers but with significantly reduced communication bandwidth.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ Innovation in Communication Efficiency: The use of binary or low-precision vectors for communication significantly reduces bandwidth requirements, which is a critical factor in distributed training.
+ Theoretical Validation: The paper provides a solid theoretical foundation confirming the convergence properties of Distributed Lion.
+ Empirical Evidence: Extensive experiments demonstrate the robustness and efficiency of Distributed Lion across a variety of tasks, making a strong case for its practical applicability.

Weaknesses:
- Incompatible with Allreduce: after converting the gradients to binary or low-precision, Allreduce cannot be used for gradient synchronization. One of my concerns is about its communication efficiency in real-world distributed systems, especially training with a high number of workers.
- Computation Overhead: While the communication cost is reduced, the overhead of converting updates to binary or low-precision vectors and back might offset some of the gains in certain scenarios. It helps if the end-to-end training throughput comparison is reported.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Large-scale AI model training has increasingly higher requirements on time, cost and environmental impact, so it is crucial to develop efficient optimizers. As an emerging optimizer, Lion optimizer has advantages in memory, computation and sample efficiency compared with AdamW. Distributed Lion: The paper proposes Distributed Lion, which is an innovative adaptation of Lion optimizer in distributed training environment. Using symbolic operations in Lion, Distributed Lion only requires binary or low-precision vectors to be communicated between working nodes and central servers, significantly reducing communication costs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Distributed Lion significantly reduces communication overhead by communicating only binary or low-precision vectors between workers, which is particularly beneficial for large-scale distributed training.
2. The paper provides theoretical analysis to prove the convergence of Distributed Lion.
3. Experimental results show that Distributed Lion can achieve comparable performance to the standard Lion or AdamW optimizer while reducing communication bandwidth.

Weaknesses:
1. The actual updating on local worker parameters is gradients, while the communicated message is signs. While the theoretical analysis shows this updating can guarantee the convergence, the actual updating style looks like the quantization. The important baselines like QSGD and SignSGD are missed. 
2. The performance of Distributed Lion can be sensitive to hyperparameter choices, especially those related to communication and aggregation strategies.
3. The code is not provided. Thus the reproducibility of the experiments is weakened.
4. The experiment performance on the CIFAR-10 is very low. Considering that the well-known validation performance of CIFAR-10 can be achieved as 94%, the proposed results are around 90%. Why the performance decreases?
5. The important baseline SGD with momentum is not provided.
6. The convergence curves on training with ImageNet and OpenWebText are not provided. This makes it hard to identify the convergence speedup between different optimizers.
7. The wall-clock time is not provided. The quantization operation and the majority vote require extra time, it will be better to show this optimizer can reduce the real-world throughputs.

Limitations:
Two minor points considering the realistic device constraints:
1. The experiment scalability is with 32 GPUs, which is not a large scale distributed training setting.
2. The training model is small-scale with less than 1B parameters.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper extends the Lion optimizer to data parallel distributed training. Unlike optimizers like SGD and Adam, the binary update in Lion can be exploited to minimize the communication. They investigate two cost effective methods for the communication of  binary updates; averaging and majority vote. Experimental results show that both methods yield competitive results to global Lion and AdamW.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The convergence analysis provided in Section 3 gives some reassurance to this non-conventional optimization method. Results are promising and experimental conditions seem adequate.

Weaknesses:
The proposed method is a trivial extension of Lion to data parallel distributed training, so the only interesting contribution seems to be the convergence analysis. 

The main contribution of this work is supposed to be the reduction of communication overhead, but there are no results showing the actual breakdown of the training time. Therefore, it is not possible to determine whether the reduction of communication volume is actually contributing to the reduction of the overall training time. Since the results seem to vary quite a bit for different models and datasets, such information is useful for determining whether the experiments are conducted for configurations that actually show a significant impact on the training time. There remains a possibility that the current method does not work as well for extremely large models trained with ZeRO 3 data parallelism, which is where the communication overhead really becomes a problem.

Limitations:
The limitations pointed out above are not explicitly stated in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes Distributed Lion, a new variant of Lion optimizer for distributed training. The proposed algorithm only requires to communicate binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. The theoretical analysis proves the convergence of the proposed algorithms. The empirical results show that the proposed algorithms have comparable model performance on CV/NLP applications but with significantly less communication overhead compared to the baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes Distributed Lion, a new variant of Lion optimizer for distributed training.

2. The proposed algorithm only requires to communicate binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost.

3. The theoretical analysis proves the convergence of the proposed algorithms. The empirical results show that the proposed algorithms have comparable model performance on CV/NLP applications but with significantly less communication overhead compared to the baselines.

Weaknesses:
1. According to Assumption 3.1, the convergence requires i.i.d. local datasets, while real-world distributed training typically uses non-i.i.d. local data.

2. In the empirical results, there seems to be no wall-clock time for training is reported. Note that the overall goal of communication reduction is to reduce the training time. Thus, it is important to report loss/acc vs. wall-clock time in the experiments.

Limitations:
The limitations are well discussed and addressed in this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
GtbwJ6mruI;"REVIEW 
Summary:
The paper presents an interesting way to deal with the ""log k curse"", that is novel while being sensible. The results seem promising, even though their presentation needs improving, and I believe it can serve as foundation for many future works to build upon. Overall I think the idea and execution are solid, but the paper as an artifact needs work, as I'll point out in detail in the Weaknesses section.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents an interesting novel technique that is intuitive without being trivial. There are experiments across 2 suites of tasks and solid mathematical work, and the overall idea is flexbible enough for future work to build upon.

Weaknesses:
I believe the results, and more specifically their presentation is a serious weakness of the paper. In Panda-Gym the authors claim their method ""boosts success rates by an average of 20.23% in moderate tests and18.36% in extreme tests"", but in fact when taking into account the standard deviation of theirs and previous method performances we *cannot* ascertain whether there was any improvement whatsoever on the moderate and extreme tests. This also points to an abuse of the use of the blue background to ""indicates that our method surpasses PEARL, TESAC and CCM"". 

Moreover in the MuJoCo results while we in general do see more statistically significant results for Test (Moderate) and Test (extreme) there are still a few misuses of the blue background, and also we in general don't see statistically significant improvement in the training setting, which leads to the question of why do we see such different behaviours in Panda-Gym (non-statistically significant improvement in Moderate and Extreme, but improvement in Training) and MuJoCo (the opposite), a question that would have been good if the authors had explored a little more.

Finally the interpretation of the t-SNE results seem quite problematic, where 2 clear clusters, one on the lower left and another on the lower right, containing context embeddings of the 4 kinds of settings for Panda-Gym seem to be ignored, but an artificial cluster is suggested by using a yellow bounding box. I consider such cluster artificial as  many of the points on the upper part of the bounding seem closer to points outside it then to points inside it, and it's always good to remember that for t-SNE local distances are meaningful, but global ones are not.

Limitations:
The paper is mostly about addressing a limitation of previous methods, and I do not believe it has any potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Proposes a new contrastive learning objective for use in meta-RL with contextual policies. The new objective incorporates a notion of skills into the mutual information estimate. Theoretical and empirical evidence in support of the superiority of the proposed method is presented.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper is clearly written.
- The proposed method is well motivated.
- The proposed benchmark is interesting.

Weaknesses:
- Meta-RL is presented as the problem of learning a contextual RL agent and a context generator for solving the task. However, there are many other kinds of meta-RL algorithms that do not use explicit context representations at all. For some canonical examples, see [RL^2](https://arxiv.org/abs/1611.02779) and [MAML](https://arxiv.org/abs/1703.03400). It is not important to include a thorough survey of other types of meta-RL methods, but for the sake of accuracy, the full scope of meta-RL should be mentioned. Especially important is to note that the proposed method requires a posterior sampling style meta-RL approach, which is not optimal at test-time compared to directly optimizing the meta-RL objective via, e.g., training a sequence model as the policy.
- Due to the above issue, it is inaccurate to say that SaNCE can be integrated with any Meta-RL algorithm.
- A new benchmark environment should come with thorough benchmarking. A good selection of contrastive meta-RL methods are included, but it would be more convincing if at least some RL^2 style method with suitable architecture (e.g. [transformers](https://proceedings.mlr.press/v162/melo22a.html) or [hypernetworks](https://proceedings.neurips.cc/paper_files/paper/2023/hash/c3fa3a7d50b34732c6d08f6f66380d75-Abstract-Conference.html)) was also run as a baseline. The current benchmarking doesn't try zero-shot meta-RL algorithms at all.
- The paper isn't self-contained in terms of understanding how the proposed method works during meta-training and meta-testing times. See questions for details.

Limitations:
Limitations are not discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces Skill-aware Mutual Information (SaMI) and Skill-aware Noise Contrastive Estimation (SaNCE) to enhance zero-shot generalization in reinforcement learning (RL). The authors address the challenges faced by Meta-Reinforcement Learning (Meta-RL) agents in tasks requiring different optimal skills by using context encoders based on contrastive learning. SaMI distinguishes context embeddings according to skills, while SaNCE, a K-sample estimator, optimizes the SaMI objective, reducing the negative sample space and mitigating the log-K curse. The proposed methods are empirically validated on modified MuJoCo and Panda-gym benchmarks, demonstrating significant improvements in zero-shot generalization and robustness to sample size reductions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
### S1. Novel Problem Formulation

The paper introduces a novel and relevant problem formulation by addressing the need for distinguishing context embeddings according to skills in Meta-RL. This approach targets the challenge of generalizing across tasks with varying optimal skills, which is crucial for effective RL in diverse environments.

### S2. Empirical Validation

The proposed methods, SaMI and SaNCE, are thoroughly validated through experiments on modified MuJoCo and Panda-gym benchmarks. The results demonstrate substantial improvements in zero-shot generalization to unseen tasks, showcasing the practical applicability and effectiveness of the methods.

### S3. Clear Presentation

The paper is well-structured, with clear explanations of the proposed methods and their benefits. The use of figures, such as visualizations of context embeddings and success rate comparisons, effectively supports the presentation of results. The theoretical proofs and experimental details provided in the appendices further enhance the clarity and comprehensiveness of the paper.

Weaknesses:
### W1. Scalability Concerns

The scalability of the proposed methods to more complex, large-scale environments is not thoroughly discussed. While the methods show promising results in the tested benchmarks, a broader analysis of their scalability and practical utility in more complex scenarios is needed to understand their full potential and limitations.

### W2. Dependency on Skill Definitions

The success of the proposed methods depends on the accurate definition and identification of skills. The paper does not address potential issues related to the variability in skill definitions across different environments and tasks, which could impact the robustness and generalizability of the methods.

Limitations:
The authors acknowledge the limitations related to the requirement for accurate skill definitions and the focus on specific benchmarks. However, a more detailed discussion on potential negative societal impacts and strategies to mitigate them would be beneficial.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Learning generalizable skills across different tasks is desirable in Reinforcement Learning. Some methods embed task information into a context latent space which is then used to train policies. This paper proposes a new objective Skill-aware Mutual Information (SaMI) that incentivizes the distinction of context embeddings according to skills. If optimized, the agent can then execute a skill depending on the underlying context. In addition to the objective, the paper proposes a K-sample estimator, Skill-aware Noise Contrastive Estimation (SaNCE), that enables an agent to optimize the SaMI objective in a sample-efficient manner.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- relevant topic: Generalising RL agents across tasks is an important topic that will enhance the field of RL
- well-motivated: The example in the introduction in addition to Fig. 1 is helping the reader to visually understand the considered problem
- The paper clearly states the contributions

Weaknesses:
only minor things such as an algorithm box would be useful to understand the learning procedure

Limitations:
Overall I found this a good paper with minor issues/unclear points (see Weaknesses/Questions).

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes an alternative approach to infoNCE for learning contrastive task representations by using the structure of skills in a meta learning setting. In doing so, the algorithm samples more negatives within the same task as the positive (but with lower reward), in a procedure that somewhat resembles hard negative mining. A benefit to this procedure is it is less sensitive to the choice of infoNCE batch size K, which in large multi-task settings is often only able to approximate mutual information for infeasibly large values. Empirical results show this alternative contrastive loss improves performance on RL benchmarks.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The log K curse is an important practical issue when using contrastive infoNCE losses in RL settings with many tasks—often the classification becomes trivial without hard negatives. Approaches like those proposed in this paper will be important to scaling contrastive learning methods for control.
- Empirical results show strong benefits against the baselines tested in many environments.

Weaknesses:
- There seem to be issues with the mathematical formulation (see questions)
- I found the presentation of the method to be confusing, in particular regarding how the learned representations were actually used for meta learning

Limitations:
Addressed

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wDDvJzvvBR;"REVIEW 
Summary:
This paper presents an approach to learning spatially aware language representations. The authors propose a contrastive representation model that integrates spatial context into language representations, aiming to enhance the performance of tasks that require spatial reasoning. The model combines visual and textual data to create embeddings that are sensitive to spatial relationships. The contributions include an architecture of the proposed model, extensive experimental results demonstrating improved performance on spatial reasoning benchmarks, and an analysis of the model's ability to generalize across different spatial contexts.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper makes a strong contribution to spatial reasoning. The integration of spatial context into text-to-audio generation models is an important and underexplored area, and this work offers a novel and effective solution.
2. The experimental setup is rigorous, with well-designed experiments that effectively validate the model's performance.
3. The paper is well-written, with clear explanations of the methodology and results. 
4. The findings have significant implications for improving spatial language understanding in various applications.

Weaknesses:
1. The reliance on synthetic datasets may limit the generalizability of the findings. The authors could explore the way to train the model on in-the-wild data.
2. The current interpretation experiments (Sec. 5.4) only study a four-class classification (""left,"" ""right,"" ""up,"" ""down""), which is insufficient for real-life scenarios. For instance, spatial audio applications often require more nuanced classifications, such as distance perception (e.g., strong/weak reverb in indoor/outdoor settings), which are critical for capturing and representing spatial information. The authors should consider extending the experiement to handle a wider range of spatial attributes to enhance its applicability in diverse settings. For example, the authors should consider using prompts like ""xxx is making a sound in the distance"" and ""xxx is making a sound nearby"" to figure out if the results are different.
3. The paper could benefit from a more detailed error analysis, identifying common failure cases and understanding why the model fails in certain scenarios. This analysis would provide insights for further improvement and refinement of the model.
4. While the model performs well on tasks like retrieval and source localization, its ability to generalize to spatial text-to-audio generation remains to be seen.

Limitations:
The authors have addressed the limitations of their work by discussing the datasets used and the model's computational requirements. Additional limitations can be found in the Weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper describes a method for learning to represent spatial audio (and text).  The proposed model is trained on synthetically spatialized audio data with corresponding text prompts.  The authors evaluate the system on audio captioning/retrieval and localization tasks, showing that the proposed model effectively represents both semantics and locality of audio.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well written, clearly organized, and easy to follow.  The proposed method makes intuitive sense and appears to be effective.  The empirical evaluation is reasonably thorough and the choice of tasks and baseline models seem appropriate.  Spatial audio is a growing area (in the context of machine learning / event detection / representation learning), and I think the work described here does fill an unaddressed area of the literature in an interesting way.  Overall I think they did a great job here.

Weaknesses:
I don't have much to fault here, but there are a few points that I think could be expanded to improve clarity and help readers understand the contribution here.  I'll elaborate on these points in the questions section, but the high-level gloss is:

- While the spatial representation part of the work (ie FOA-derived input) is explained well, there is almost no explanation of how the spatialization was implemented.
- There is little (if any) qualitative analysis of the results, only aggregate scores reported in tables.

Limitations:
The limitations sections seems sufficient to me.

One potential caveat here is that the authors do not explicitly mention any limitations imposed by the accuracy of the spatialization process, e.g., whether it will only work well for simulated closed environments (shoebox model) or if it can accurately capture open environments.  This I think would be easy to resolve with a bit more information about the process and an extra line the limitations (if necessary).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents ELSA (Embeddings for Language and Spatial Audio), a novel model designed to learn spatially-aware audio and text embeddings using multimodal contrastive learning. The primary aim is to address the limitations of existing audio foundation models, which lack spatial awareness, and sound event localization and detection models, which are constrained to a fixed number of classes and absolute positional descriptions. The authors spatially augment several classical open-source audio datasets in order to train ELSA.  Results show that ELSA is able to capture spatial attributes and semantic meaning of the audio.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The focus of this paper is on learning spatial audio embeddings associated with natural language description, which is a very interesting and rewarding problem for which there is a lack of models.
- These authors synthesize large amounts of non-spatial audio data under various spatial configurations, which is a valuable contribution to the field of spatial audio understanding.

Weaknesses:
- For this paper, my biggest concern is the generalizability of the model to real scenarios. 
  While the synthetic dataset is extensive, there is a risk that the model might not generalize well to real-world scenarios due to potential biases in simulated environments. To show the performance of model generalization to real scenarios, the experiments only on a small real-world dataset appear too thin. Would it be possible to test ELSA in other real scenarios, for example, in some of the tasks in the latest DCase competition, e.g. Sound Event Localization?
- For paper writing, too much important information is put in appendices, such as the structure figure of the whole model. Perhaps the layout of the writing could be adjusted to make it easier to read.
- The citation format of the article is highly problematic and needs to be standardized.

Limitations:
- As the authors mentioned, for creating a spatial audio caption dataset, using LLM to rewrite the caption might lead to hallucinations.
- Model performance in real scenarios is yet to be verified.
- ELSA looks very suitable to be used as a spatial audio encoder for a caption model to conduct spatial audio captioning, but unfortunately, the authors did not show this kind of capability in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents ELSA (EMbeddings for Language and Spatial Audio),  a spatially aware-audio and text embedding model. The training data is created by synthesizing spatial audio in ambisonic format and augmenting text captions with spatial information. A small real world data is also collected for evaluations. The model training itself largely follows standard CLIP/CLAP training by using contrastive losses. Additional losses for direction and distances are added for the spatial part. Evaluations are done on both semantic retrieval tasks and spatial tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
– The paper addresses a key part of multimodal contrastive embeddings. Sounds contain a significant amount of spatial information and humans naturally rely on directional information from sounds. Considering this it is expected that embeddings with spatial information are created. The paper is a good step in the right direction. 

– For the most part, the paper is well done. Spatial audio can present several challenges with respect to data (more so in multimodal settings, training approach). Considering the challenges around learning from spatial audio, the paper presents a good approach for learning spatially-aware language embeddings. The experiments are also reasonably good. 

– The paper is also well written and mostly clear.

-------
Score increased after rebuttal.

Weaknesses:
There are a few weaknesses which are worth addressing in the paper. 

– For table 2, I would be curious to see what CLAP on its own can achieve. It would be good to contrast this zero-shot classification on the spatial task. 

– How were the non-spatial audio-text pairs used in training (as shown in Table 3, last row) ?

– Using non-spatial audio-text seems crucial for good semantic retrieval. This is evidenced by A.6 as well where the models training on just spatial audio-text pairs do not do well on semantic retrieval task. This is a bit surprising. The CLIP loss is still present in training, the semantics are also intact in spatial audio-text pairs. Why should there be a performance drop in that case ? it would be good to provide a  good discussion and justification

– In Table A.7, the performance of the model trained on spatial Clotho and Audiocaps is better on RWD data than even on Clotho and Audiocaps itself. That is a bit surprising. We would expect that the model would be better in it’s own domain. The difference also is pretty big. 

– The discussion in Section 5.4 is a bit adhoc. I would suggest not referring to anecdotal observations. The experiments could be better designed. 

– Several of the classification experiments end up using 3-4 layers MLP. I think a more shallower model (maybe even just linear classifier) would provide a better confirmation of what information the embeddings store. Otherwise such deeper networks are able to push the numbers on their and it’s not clear how good the embeddings are. 

– Some form of clustering and distance visualization would be good. It has been incorporated in some form in Table 2, but it would be good to explicitly show how the distances between embedding represent the spatial information.  

– All the spatial mapping in terms of the language is very discrete (A.2). The range for distance, direction etc. can appear a bit arbitrary and forced. While this is perhaps a good first attempt, a more continuous form of “spatial-language” is desirable. Another thing could be a perception driven approach can also be taken where the boundaries are decided by what people generally perceive as left or right w.r.t sound direction.

Limitations:
Please add some limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
wBzvYh3PRA;"REVIEW 
Summary:
The paper proposed a factorized approach to generate simulated games via LLM code synthesis. The code idea is that one doesn't need to generate the entire code at once, but rather generate different part of a POMDP game, such as controller, model, and view. The generated simulation allows RL policies to train on top. The authors introduced a benchmark to evaluate the proposed framework and show good results in terms of prompt alignment, transfer ability and human evaluation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper investigates an important problem, simulation generation. The evaluation over the mentioned environments is solid, spanning from automated tests to human evaluations.

Weaknesses:
1. The paper is poorly written. I have hands-on experience with almost all important concepts mentioned in the paper, yet still have a hard time understanding the paper, and have to read again and again including some code. Rather than talking about abstract terms like POMDP / factorization first, I think the authors can start easy with intuitions and explanations. The figure can also be improved. The main method figure shall spend more time showing what's special about ""Factored POMDP"" compared to prior methods. The benchmark claim should have its own section. The motivation is not clearly narrated either.  The world model section in related work doesn't seem to fit there.

2. On of the main contributions the authors listed in the introduction section is a benchmark. However, I think this benchmark seems to lack the technical depth I was expecting as a standalone contribution. I feel it's just a set of small metrics rather than benchmark.

3. The paper just lacks the level of technical contribution that meets my criteria for a Neurips paper. While there are many other prompting papers like CoT, ToT, the problem the paper is trying to solve is also very specific. 

4. While I have experience with both LLM and robotics, I believe the authors should not put Robotics as primary area, but NLP or code synthesis community.

Limitations:
The author discussed the limitations of automated evaluation by conducting a human study. No outstanding negative societal impact.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces an LLM-based method for generting code for simulations. After generating the simulation of famous games based on their manual and description, the authors show that policies trained in these environments transfer well to the real games.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- **S.1 Great results.** I think that the results from Fig.3 are very impressive. Zero-shot transfer is very hard, and doing so much more reliably than vanilla GPT-4 is impressive.
- **S.2 Overall good idea.** The idea to deconstruct the game development task into M-V-C makes a lot of sense to me. I just thought that's already kind of captured in the POMDP formulation. 
- **S.3 Good presentation.** The overall presentation and writing are good, although there is much left to be desired in terms of implementation details.

Weaknesses:
- **W.1 Implementation details and relationship to formulas.** I'm happy that there was code provided with the submissio and I hope that it will be released publicly because based purely on the main body of the paper, the method is not reproducible. Including the prompts in the appendix helps but I wish they were commented a bit more on why certain phrases and sections are there. And I'm wondering if the theory that's presented in the paper holds water wrt the actual instructions in the appendix. Because as far as I understand, there aren't any restrictions on what code the LLM can generate for each component, right? Also, the paper mentions graphs every so often and I don't know how they fit into this. I also think the context selection is crucial to your method and from the main body of the paper, it's completely unclear how that's implemented.
- **W.2 Missing examples.** Along a similar idea, I'd have loved some examples throughout the paper to illustrate what some of these instructions actually mean.
- **W.3 Unclear robotic experiments.** The robotic experiments seem to be more of an afterthought in the paper, and it's unclear what existing assets there are, what control code is assumed given, what camera parameters are assumed, etc. 
- **W.4 Unclear input mapping.** The appendix mentions that the controller is given or that button presses always mean the same thing. I completely don't understand what's meant by that.

Overall, I think the paper shows a great idea and is probably beneficial to the community, but some work should go into tweaking the main body of the paper and making the method more clear and reproducible.

Limitations:
The authors adequately address the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents FACTORSIM, a framework that converts any language specification into a complete simulation for training RL agents. FACTORSIM decomposes the input prompt into steps and uses a factored Partially Observable Markov Decision Process (POMDP) to minimize the context needed for each generation step. It also introduces a method to benchmark generative simulation and demonstrate the capability for FACTORSIM to be used in robotics setting.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Develop a robust pipeline for constructing game environments from language descriptions, which could significantly enhance the scalability of training generalist agents.
2. Formalize the framework as a Partially Observable Markov Decision Process (POMDP), reducing the need for full context during generation and improving outcomes.
3. Demonstrate the potential of this method to generalize to embodied scenarios.

Weaknesses:
1. The evidence for generalizing to embodied scenario is limited.
2. The successful rate in Table 1 and Figure 3 is low. Could there be some potential way to improve it?

Limitations:
The limitations are addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a LLM prompting method to generate full game / robot simulations in code based on text descriptions.  Given a long text description, the method first utilizes an LLM to decompose it into multiple sentences, and then use them to iteratively generate and update simulation code. For each iteration, the code is generated and updated separately as three modules, i.e., controller, model and view. The update happens in a factorized way - the authors use the LLM to identify relevant state and context to avoid feeding the full generated code into LLM.

In experiments, the method is evaluated on game and robot simulation code generation benchmarks. The method shows superior results against other LLM prompting baselines in generating valid simulation code that aligns with text description.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed method exploits the structure of simulation to modularize and factorize code generation. This strategy significantly improves LLM's capability to generate full simulation code.
- The method is comprehensively evaluated on game and robot benchmarks.
- The paper is well written and easy to follow.

Weaknesses:
The major contribution of the paper seems to be a prompting technique crafted for the specific task of simulation code generation. While such a technique does improve performance on the task, my concern is it is neither fundamental nor sufficiently novel. The proposed prompting technique highlights two key designs:
- modularize simulation code generation manually, which aligns with the common practice to manually decompose a complex task into sub-tasks for LLMs to handle more effectively.
- extract relevant portion of code for LLM to consume and update, which is also an implementation-wise design that many works have already incorporated.

While the paper writes factorized POMDP formulations, they don't seem to make a difference on how the prompting method is implemented. So I'm concerned that the contribution of this paper is more as a practical application rather than a general method or framework.

Limitations:
See Weakness

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wBtmN8SZ2B;"REVIEW 
Summary:
The paper introduces a novel regularization method, HypStructure, which utilizes hyperbolic geometry to improve the embedding of hierarchical relationships within feature representations. This approach enhances the learning of structured representations, reducing distortion and boosting generalization in low-dimensional scenarios. It also demonstrates superior performance in Out-of-Distribution (OOD) detection across various datasets through extensive empirical evaluation. Additionally, the paper includes an eigenvalue analysis that provides deeper insights into the structured representations, correlating positively with improved OOD detection performance. This advancement extends structured representation learning to hyperbolic spaces, achieving more discriminative and interpretable features that effectively capture the inherent hierarchies in complex datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is the first to formally characterize properties of hierarchy-informed features via an eigenvalue analysis, and also relate it to the OOD detection task.
2. The paper is easy to read and follow, making complex concepts accessible. The use of clear definitions, structured methodology sections, and detailed discussions helps in understanding both the theoretical underpinnings and practical implications of HypStructure. Visual aids and empirical results are presented in a manner that clearly supports the claims made.
3. The significance of this work lies in its potential impact on a range of applications that require understanding and leveraging hierarchical relationships in data, such as image recognition and OOD detection.

Weaknesses:
1. The main concern of this paper is the novelty. I believe the method proposed by the author in this work has been explored in many previous works. For instance, in ""Hyperbolic Image Embeddings,"" ""Hyperbolic Contrastive Learning for Visual Representations beyond Objects"", etc. Although the paper characterizes properties of hierarchy-informed features via an eigenvalue analysis, the contribution is not significant enough to be accepted. 
2. The writing is also not good enough for me. For instance, two examples starting from line 107 are not necessarily included in the formal paper; it is better to be placed in the supplementarity materials. There are also some repetitive expressions, for instance, in line 351 and line 353 (While the current work).
3. In summary, I believe the technical contribution of this paper is not significant enough to be accepted.

Limitations:
As I said in the previous section, the main concern is the novelty. Potential improvements include extending the application scenarios of hyperbolic structured representations on more vision or language tasks that have not been explored before. Further theoretical investigation into establishing the error bounds of CPCC-style structured regularization objectives is of interest as well. The writing needs to be improved as well.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a novel approach, HypStructure, for learning structured representations. Comparing with the existing method, the proposed method adds an regularizer calculated from hyperbolic geometry. This approach aims to reduce distortion and improve generalization performance, particularly in low-dimensional scenarios. Extensive experiments are conducted on both the classification task and the out-of-distribution detection task.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well organized. The paper extends and studies the existing L2-CPCC to the hyperbolic space, which effectively reduces distortion and enhances the representation of hierarchical relationships in the data. The paper also conducted comprehensive experiments as well as detailed theoretical analysis of the eigenspectrum of structured representations.

Weaknesses:
If my understanding of the proposed loss term is correct, $L_flat$ is not calculated in the hyperbolic geometry. Have you tried the $L_flat$ with hyperbolic network or hyperbolic geometry. I hope the authors could provide more explanation of the combined loss in different geometries. 

In Section 2.1, it mentions that $D_i$ is the subset of data points with a specific class label, and $d(D_i, D_j)$ is the distance between the feature vectors of $D_i$ and $D_j$. However, it is not mentioned how the vectors for $D_i$ and $D_j$ are calculated. Is it simply the average of all the feature vectors in the subset? 

For Example 1 in Section 2.2, tree $T$ and nodes $G, C, D, E$ are referenced in a way that implies there should be a figure accompanied by the example. While Figure 1 is referenced shortly before this, it is meant to be accompanied by Example 2. Is there a figure that has been omitted here?

Also, I would recommend proofreading the paper to correct all grammatical errors. For example, in the paragraph of Section 2.1, the first sentence “Using a composite objective as defined in Equation (2), we can enforce the distance relationship between a pair of representations in the feature space, to behave similarly as the tree metric between the same vertices” should be corrected to “Using a composite objective as defined in Equation (2), we can enforce the distance relationship between a pair of representations in the feature space to behave similarly to the tree metric between the same vertices.” This version removes the unnecessary comma and corrects “behave similarly as…” to “behave similarly to…”.

Limitations:
There are two limitations mentioned in this paper. The first being that the L2-CPCC can only approximate the structured information since it is impossible to “embed a tree $T$ into $L2$ without distortion,” and as this work extends the L2-CPCC, it would similarly have this limitation. However, this is never explicitly stated in Section 7, where it mentions that HypStructure is only limited to Euclidean classification losses. There could have been more said about the limitations, but it does suffice.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces a regularization scheme based on Cophenetic Correlation Coefficient to more appropriately embed semantic label hierarchical structure in the representation. The method exploits the hierarchical benefits of hyperbolic space reformulating the CPCC regularization term to operate on the Poincare ball. The proposed method sees improvement in empirical performance demonstrating the effectiveness of the approach to learn a more separable embedding space for classification.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
⁃	The authors present the work clearly with effective use of visual and writing structure. All figures/diagrams are useful in supporting the narrative and findings.

⁃	The method is simple, highly generalizable, and leads to improved performance on benchmark tasks. It can therefore, be seen as an advantageous tool in hyperbolic learning that could possibly lead to impact and adaptation by practitioners in the field.

⁃	The theoretical and analysis are generally good, with eigenspectrum analysis supporting your claims of hierarchical structure for the most part. This is a useful analysis that provides confidence in the findings supported by appropriate proofs.

⁃	Extensive details to support replication are provided.

Weaknesses:
⁃	From the visualizations presented of the embedding space, notably the UMAP, your embeddings seem to have collapsed to the boundary in many places limiting the inherent hierarchy of the embeddings, this results in a limited hierarchy being represented. This in turn, leads me to question the extent of hierarchies learnt, when discussing the core intention of the work, and the claims made. One would expect that greater performance could be achieved if this had been addressed. I am aware that boundary collapse is still an unsolved problem, but careful tuning can limit its effects.

⁃	The approach is simple but arguably not significantly novel given it is a hyperbolic reformulation of CPCC with minimal changes. With that being said, these simple methods do work somewhat well in practice and are useful to progressing the field. 

⁃	The use of a Euclidean linear evaluation is a confusing direction. You are aiming to learn a hyperbolic embedding space that preserves hierarchy, yet for downstream tasks you employ a Euclidean classifier, why? You will lose the desirable properties you are aiming to capture.

⁃	Further experimentation on different hyperbolic models and downstream tasks would have helped demonstrate the generalization of the regularization to all of hyperbolic learning. Although, this cannot be expected in the rebuttal, it would have helped support the findings to present the work a more generalized approach.

Limitations:
Broader impact statement and societal impact are seemingly missing from this work. These limitations including those limitations of the analysis should be highlighted in more detail, you mention the Euclidean loss as a limitation. If I have missed them however in the text, please correct me.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces HypStructure, a novel approach for learning structured representations using hyperbolic embeddings, which are well-suited for modeling hierarchical relationships due to their tree-like structure. The method incorporates a hyperbolic tree-based representation loss and a centering loss to embed label hierarchies into feature spaces with minimal distortion.

Experiments demonstrate HypStructure's effectiveness in improving classification accuracy, especially in low-dimensional scenarios, and enhancing Out-of-Distribution (OOD) detection performance without compromising in-distribution accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Although it is already theoretically proved in the related work of [70, 72] that it is not possible to embed a tree in Euclidean space without loss, it is still informative to see that In section 2.2, example 1 and example 2 give good counter-examples to show this property. 

2. The paper is well-written and easy to follow, and the proposed model is simple yet effective. 

3. The paper provides a formal eigenvalue analysis that links the geometry of the learned representations to their performance

Weaknesses:
1. Sections 2.1, 2.2, and 3.1 are all from existing literature, which limited the contribution of the paper. Although the operations described in Section 3.1 are common hyperbolic operations, this section still lacks proper reference to the related papers.

2. In HypCPCC, the authors proposed two alternatives of the loss,
    * Map Euclidean vectors to Poincaré space then average.
    * Averaging the Euclidean vectors then map to Poincaré space.
In the 1st alternative, The use of Klein weighted average incurs extra computation, Is it worth doing so?, In the 2nd alternative is exactly the same as [r2], which also calculates the prototypes for each class in hyperbolic space and then map to Poincaré space, [r2] also deploys supervised constructive learning, but the reference is missing and comparison is not stated.

3. The statement in Theorem 5.1 is incorrect, an entry of $K$, denoted as $r^h$ should be a vector, but the theorem stated that $\lambda_0 = 1 - r^1$, which does not make sense.

3. Incorrect (but fixable) definition in line 708, the proof used $\| u \| = \| v \| = 1-\epsilon $ but in the proof the authors used the fact that $\| u \| = \| v \| = 1-\epsilon^2 $

4. Incorrect proof in Corollary A.1, the last row of the proof does not hold, Poincaré distance cannot be the same as Euclidean distance, ""growing in the same trend"" does not mean ""proportional to"".

[r2] Long, Teng, et al. ""Searching for actions on the hyperbole."" CVPR2020

Limitations:
The authors did not discuss the limitations in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wAqdvcK1Fv;"REVIEW 
Summary:
This article, ""Energy-based modelling for discrete and mixed data via heat equations on structured spaces"", proposes to perform the training on EBM, using the Energy Discrepancy (ED) loss, in the case where having multi-modal dataset mixing eventually continuous inputs but also discrete (categorical) ones. The work describes into details how to parametrize in different setting the inclusion of discrete variables, and they apply it to various datasets.
The main contributions are the design of the continuous time Markov chain transition probability that lies at the heart of the ED approach and the application to tabular dataset for which generative approach is usually hard.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors show how their method can be efficiently used on Tabular dataset. In particular, they apply to several dataset and show that in average the EBM trained with Energy-Discrepancy using a discrete implementation of the Markov chain transition probability outperform the concurrent approach, ref[Xu et al 2019]. 

The authors also show experimental results on image modelling.

Weaknesses:
The authors extend the formalism of Energy Discrepancy to the case of including discrete state in addition to continuous features. Whether or not this justifies an entire publication can be debated, Although it should be emphasized that the datasets under consideration are quite original.

It might be because I'm not an expert on ED, but while traditional EBM relies on MCMC to compute the gradient, ED does not. However, it is not clear to me if sampling the EBM trained in such way need MCMC to be generative ? If so, the article should provide more details on the implementation. They should also check that the trained model is at equilibrium (the generated samples corresponds to the equilibrium properties of the model).

More importantly, the comparison for the tabular dataset is only done at the level of the AUC curves. Can at least the authors compare the frequency and correlations amongst the generated samples and the true ones ?

Limitations:
The limitations are correctly discussed in the article.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper extends the Energy Discrepancies framework introduced by Schroder et al. to the setting of discrete data. In order to do this, the authors first describe ways to perturb discrete data by modeling the perturbation process as a CTMC. They describe suitable perturbation choices for different types of discrete data (e.g. binary, categorical, ordinal, cyclical) and describe different considerations for the time parameter in the CTMC. They then propose an approach that performs a Monte-Carlo estimate of the contrastive potential needed for the Energy Discrepencies loss and compare their method to existing methods for training discrete EBMs.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: Energy discrepancy is a relatively new approach. While the original paper proposed some extensions to discrete data, this paper goes into extending energy discrepancy to discrete data in much more depth and includes new mathematical and experimental analyses. 

Clarity: Overall, the paper is well written.  

Quality: I believe the paper is technically sound. 

Significance: The authors show on toy examples that their method outcompetes contrastive divergence. The authors appear to generally outperform two methods proposed in 2019 along with contrastive divergence. While I have some minor concerns about these baselines, outperforming these baselines is at least demonstrating some empirical benefit of this approach.

Weaknesses:
Clarity: The clarity can be improved a bit (see my questions below). 

Significance: Despite demonstrating that the method can work empirically, I have some concerns with the overall significance. It seems that while the method works well on toy examples, the results are less impressive on real-world image modeling tasks. I am unfamiliar with the field of tabular data modeling and therefore, cannot properly assess the significance of the results. Beyond contrastive divergence the main baselines is a method from 2019 with 2,000 citations. Are there better baselines to compare against among these 2,000 citations?

Limitations:
I would like more honest discussion throughout the paper of the relative strength and drawbacks of their method. For example, while it is true that this method is “MCMC-free” it still requires Monte-Carlo estimation and the relative tradeoffs here are not adequately discussed. I would also like to see more comparisons of CD with large numbers of MCMC steps v.s. ED with large numbers of samples.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a suite of methods for training energy-based models for discrete and mixed data using the Energy Discrepancy loss, a recently proposed method for training EBMs. Compared to contrastive divergence, it does not require MCMC sampling from the model distribution during training, improving training efficiency. This is done by simulating a diffusion process on the discrete states and effectively using those noisier samples as the negative samples. The paper introduces a connection between the new method and maximum likelihood estimation, showing that energy discrepancy as applied to discrete state spaces can converge to the negative log-likelihood. In experiments, the new method behaves favourably compared to contrastive divergence-based methods on synthetic data sets, on average better than baselines on real-world tabular data sets, and comparably to many competing methods generation on discrete image modelling. An application of the trained EBM on classification and improving uncertainty quantification compared to a direct classifier is also shown.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper proposes a relevant extension to recently published work. Especially Theorem 1 does not seem obvious, and the paper may open up the use of the Energy Discrepancy loss to a much wider variety of use-cases. 
- The method is also quite simple, and seems simple to implement. 
- The paper connections to recent work on discrete diffusion models, and proposes a variety of methods to estimate the energy discrepancy loss. 
- The results are good compared to standard contrastive divergence based methods
- The paper is well written, and I found it easy enough to understand even without prior knowledge on the Energy Discrepancy method.

Weaknesses:
- As noted in the limitations, the application to data such as images seems to be challenging as the noisy negative samples may not give very useful training signal in this case. 
- Although the energy discrepancy method has already been proposed and published in previous work, I found the justification for the method slightly confusing while reading this paper. What is Theorem 1 exactly saying? (see questions) The loss also is, in practice, approximated with something slightly different than the proposed loss, which seems conceptually a bit confusing. However, this is not a major concern given that the base method has been proposed and published in previous work.

Limitations:
Addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
: The paper introduces a novel method for training energy-based models (EBMs) on discrete and mixed data using heat equations on structured spaces. This method employs the Energy Discrepancy (ED) loss function, which eliminates the need for Markov chain Monte Carlo (MCMC) by using graph-structured perturbations. The proposed approach is evaluated on several applications, including discrete density estimation, synthetic data generation, and calibrated classification, demonstrating significant improvements in training efficiency and model performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper successfully extends the Energy Discrepancy method to discrete and mixed data with solid theoretical analysis, addressing the challenge of robust and fast sampling in these space. The designed experiments demonstrate the method's ability to accurately capture complex data structures and generate high-quality synthetic data, highlighting its practical applicability.

Weaknesses:
1.	Despite the method's solid contributions and experimental design, the motivations behind each step and their presentations are not very clear, making it hard to follow. For instance, in Section 3.1, the paper discusses different structured and unstructured categorical values, introducing the four types {cyc, ord, unif, abs}. However, it is not clear why these specific structures are chosen. Are they meant to cover all categorical values comprehensively, or are they the most common in tabular data? Providing a clearer rationale would help readers understand the choices made.

2.	The scalability of the proposed method in such scenarios is a significant concern. An analysis or discussion on how the method handles large categorical values would be beneficial. This could include potential modifications or considerations to ensure that the method remains efficient and practical when applied to datasets with large categorical variables. What’s more, I strongly recommend moving these algorithms from the appendix into the main body of the paper. This would make the paper easier to follow and more accessible to readers who need to understand the detailed workings of the method.

Limitations:
See weakness

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
w6vbfSC1y0;"REVIEW 
Summary:
Based on the observation that CLIP undercalibration will affect the existing prompt-tuning-based method's OOD regularization, i.e. samples with uncertain True-Class Probability (referred to as ID uncertainty in this paper) may provide false OOD features and harm to negative training used in the existing methods; therefore, the author proposes a simple training strategy Self-Calibrated Tuning (SCT) weighted with the ID uncertainty, which can help improve FPR95 through experimental verification.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The author also observed and attempted to study the important CLIP calibration problem.

- The paper is relatively easy to understand overall.

Weaknesses:
My main concerns about this work are that the work is relatively incremental and empirical; there is insufficient discussion on the pros and cons of field-related methods (including other paradigms); the experiments are not sufficient, rigorous, and analyzed; and the method's effect on improving common benchmarks is rather one-sided, etc. The details are as follows:

[Method]

- (Inaccurate motivation verifications) If I understand correctly, the misclassified ratio on the horizontal axis in the right panel of Fig. 2 refers to $p(pred\neq gt), pred=\arg\max_yp(y|x)$ while your ID uncertainty refers to sth like True-Class Probability, marginal softmax $p(y=gt|x)$. These are not two identical things, though there is a certain correlation: only within some ranges, TCP could indicate accuracy [1]. Therefore, I think the author may have a biased understanding here, and the experimental results cannot fully reflect the motivation of the work: when ""uncertain"" ID samples are used as OOD regularization, some ID data are misdetected as OOD (FPR). If I have misunderstood, could the author clarify? Or maybe additional correct experiments are needed?

- (No calibration verifications) The claim and results in the work show that SCT helps with CLIP calibration, but there is no visualization of the calibration after training to illustrate the point (e.g. could add the before and after calibration comparison like in Fig. 2).

- (Lack of discussion of weighted training; modulating function rationale) The idea of ​​weighted loss is very direct and easy to think of. Previous work should also be mentioned and discussed. For example, [2] is based on the feature representation paradigm and uses activation scale as the ID-ness (similar to ID uncertainty in the context) indicator for weighted training to improve OOD detection. In comparison, I do not quite understand why $\phi$ must be monotonically decreasing w.r.t. $p$ (e.g. $1-p$) and not monotonically increasing (e.g. $p$), because the weighting method in [2] is monotonically increasing, and the result is also improved. Could the authors elaborate on this?

- (How about post-hoc CLIP calibration?) Usually, calibration is divided into two types: training and post-hoc [3] (calibration related works are lacked in the paper). The former is used in this paper. The latter may be explored in OOD feature extraction methods, e.g. changing the rank operation (Eq. (6) & Fig. 3(d)). The author may lack discussion in this aspect.

[Experiments]

- (No much AUROC improvement) I understand that the method in this work is mainly to improve FPR95, but unilaterally only improving FPR95 does not seem to be comprehensive enough, because AUROC is also an equally important indicator and methods need to be proposed to improve it.

- (Lack of CIFAR results) Although the comparison method LoCoOp has not been experimented on CIFAR, CIFAR is indeed another important benchmarks in the field of OOD detection, and I think it is necessary to supplement it.

- (Discussions with simpler yet more effective pre-trained features + post-hoc?) I just would like to know what the author thinks about the (potential) advantages of the prompt-tuning-based method studied in the paper compared to the post-hoc method. After all, post-hoc does not require additional training and uses the basic ResNet backbone; the FPR95 and AUROC on the main task of Tab. 1 on ImageNet-1k have reached **20.05** and **95.71** respectively, which are much better than the results reported in the paper (26.47, 93.37).

- Could the authors clarify on what validation set are the hyperparameters tuned?

- (Interpretations of the ablations.) Figure 3(b) shows that the results of selecting other regularization functions are very different, and the paper (L294-298) does not provide any analysis. I am curious about how the author could try to interprete these ablation study experiment results. Similarly, the quality of OOD features extracted by different extraction methods also varies greatly, which seems very empirical (Fig. 3(d)).

- Table 1 is suggested to include results of combining more newer post-hoc methods (e.g. ASH (Djurisic et al., 2022), Scale [2]) and fine-tuned methods, which will give readers a more comprehensive sense.

[Presentation]

- The paragraph introducing the OOD features (L189) should be moved forward, or at least before refer to Fig. 1, which will give readers a clearer background.

- Why is the left panel in Fig. 2 not arranged in ascending order of softmax output? The arrangement of 0.02, 0.89, 0.04, and 0.67 affects reading. What is it trying to say? It would be better to display the classes and images together for clarity.

References:

[1] Corbière, Charles, et al. ""Addressing failure prediction by learning model confidence."" NeurIPS, 2019.

[2] Xu, Kai, et al. ""Scaling for Training-Time and Post-hoc Out-of-distribution Detection Enhancement."" ICLR, 2024.

[3] Guo, Chuan, et al. ""On calibration of modern neural networks."" ICML, 2017.

Limitations:
Limitations of lack of theoretical analysis (L571-574)'d better be put in the manuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper first reveals the relationship between the quality of out-of-distribution (OOD) features and the prediction uncertainty of in-distribution (ID) data. Then, the paper introduces modulating factors to weight the ID loss and OOD loss, with the weights being related to the ID data prediction confidence.  The experiments are carried out on standard datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The analysis of the relationship between OOD feature quality and ID prediction confidence is well-reasoned; lower ID confidence indeed affects the accuracy of foreground-background separation.
2. Weighting the loss components is a straightforward approach, making it easier to understand.
3. The writing is clear and easy to comprehend.

Weaknesses:
1. Overall, the technical contribution of this paper is relatively incremental, primarily focusing on how to weight the two loss components.
2. The effectiveness of the proposed method is quite limited. For example, as shown in Table 2, the improvement in averaged AUROC under the 16-shot scenario is minimal, only around 0.3%, and there is even a slight decrease in results on ID-like data.
3. There is a lack of comparison with existing state-of-the-art (SOTA) methods. For instance, the results reported in this paper are not as good as those of NegLabel[1] (AUROC 94.21 > 93.37), which is a zero-shot method that does not require training and training samples. The results for ID-like method reported in this paper are also lower; the official paper reports 94.36 AUROC under 4-shot, while this paper reports 92.14 AUROC under 16-shot.
4. More exploration is needed regarding the settings of function $\phi$ and $\psi$ in Equation 4.
5. The statement in lines 158-159 is somewhat unclear. Should it be that inaccurate OOD features hinder the effective learning of better OOD detection?

[1] Jiang, Xue, et al. ""Negative label guided ood detection with pretrained vision-language models."" ICLR (2024).

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a novel few-shot approach to regularizing prompt tuning-based OOD detection methods called Self-Calibrated Tuning (SCT). SCT is specifically built to address the problems of incorrect OOD features being used in prompt tuning-based OOD detection methods. More specifically, by weighting regions of the image based on model confidence, SCT can better alleviate these issues in prompt tuning-based OOD detection methods. The resulting SCT method shows strong empirical improvements across a wide range of OOD detection methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Strengths:
- The paper is well written and the authors provide a clear and concise motivation justifying the use of SCT.
- The author provides a timely analysis of the problem of incorrect OOD features extracted from ID data.
- SCT shows strong empirical performance across a wide range of traditional OOD detection methods and prompt tuning-based OOD detection methods.
- Additionally, given the nature of prompt tuning-based OOD detection methods, SCT can act in the more relevant few-shot setting.

Weaknesses:
Weakness:
- A primary concern of the reviewer is the lack of evaluations against the more traditional CIFAR set of benchmarks for OOD detection.
- Additionally, the empirical performance gain of SCT (table 2) in combination with other prompt-tuning-based methods, seems minimal.

Limitations:
The author provides adequate discussions on any limitations and broader impacts. Additionally, the reviewer does not forsee any potential negative social impacts from the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In response to challenges in OOD detection using CLIP-based methods, this paper introduces Self-Calibrated Tuning (SCT), a novel framework that addresses issues with unreliable OOD features extracted from ID data. SCT dynamically adjusts the influence of OOD regularization during model training based on the prediction uncertainty of ID samples. By introducing modulating factors into the learning objective, SCT directs the model's attention more effectively towards classification tasks, especially when training with low-confidence data. This adaptive approach improves the calibration of OOD features extracted from high-confidence ID data, enhancing the overall OOD detection performance of prompt tuning methods. Empirical evaluations on ImageNet-1k demonstrate SCT's effectiveness.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. This paper is well-motivated and well-written. In particular, authors propose to adaptively adjust the importance of OOD features and introduce SCT, which are motivated by the following finding: performance of prompt tuning based methods is significantly affected by the uncertainty of the given ID data. 
2. Authors have a comprehensive review of the whole research literature.
3. Authors conduct a large amount of experiments and the experimental results demonstrate the effectiveness of SCT on both official benchmarks and hard OOD detection tasks. 
4. In summary, I think SCT could become a great contribution towards OOD detection community.

Weaknesses:
None in particular

Limitations:
Please refer to weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper focuses open-set detection method based on CLIP. The authors propose an additional weighting mechanism based on the LoCoOp method to alleviate the problem that the outlier related regions extracted by the LoCoOp method are not trustworthy in some cases.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Outlier detection with VLM is an interesting research direction.

Weaknesses:
The contribution over LoCoOp is incremental. The only difference is an extra reweighting term based on the current prediction score. And the reweighting mechanism is purely based on heuristics - for example, $1-p(y|x)$ for $L_{ce}$ implicitly enforce hard sample mining.

Minor: 
The intuition in Figure 1/4 is not clear to me. The shown examples validate that LoCoOp can detect and mask-out the inlier-related regions well. Also, the GT label should be annotated.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
w6q46IslSR;"REVIEW 
Summary:
This paper investigates the training dynamics of a single-layer transformer followed by a single MLP layer on a synthetic binary classification task, where the objective is to identify the co-occurrence of two specific tokens in the input sequence. They analyze the gradient flow dynamics for the case that all the attention parameters (key, query, value) and the linear layer all trainable and show that the model can achieve low loss despite the non-convexity of the objective. They identify two phases in the training, 1) the MLP aligns with the two target tokens at the start of the training and the model learns to classify all the samples correctly, 2) the attention and MLP parameters update to increase the classification margin and drive the loss to zero. They also run a small scale numerical experiment in their synthetic setup tp confirm their analysis.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper makes no restricting assumptions on the weights of the transformer model and performs the analysis on the joint optimization of all the parameters.

Although the paper and its proof are notation-heavy, the authors have broken down the complexity of the proof and notation in the main body to clarify the steps needed to prove the results.

Weaknesses:
There are some restrictive assumptions on the synthetic data model: The vocabulary set $d$ is considered to be larger than the number of training tokens, which is not the case in realistic setups. Thus, some tokens are not visited at training time. Also, they assume, apart from the two target tokens, the remaining tokens appear at maximum once in the training set.  

The proof outline in the main body helps in understanding the high-level steps involved. However, it could still benefit from additional clarifications on some intermediate steps. For instance, in phases 1 and 2, it's mentioned how the alignment of the MLP with the target tokens $G^{(t)}(\mu_{1,2})$ behaves during training. However, it's not clear how this connects to the evolution of the attention scores in phase 2 as stated in Lemma 4.7.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the training dynamics of a single hidden layer transformer network (self-attention + linear MLP) trained on a binary word cooccurrence task. Specifically, given a data matrix $X \in R^{d \times L}$ representing L ""words"" (each column of X is a word vector of dimension d), the model must output +1 if words 1 and 2 both occur in X, and -1 otherwise. The paper shows that a transformer layer is able to learn this task, and that the training occurs in two stages: First, the linear MLP layer learns to classify data points correctly by positively aligning with the embeddings for words 1 and 2 (but without making large changes to attention matrices). Second, it drives the loss down further by using the attention matrices to positively correlate q,k,v for words 1 and 2, and anti-correlate the q,k,v for a common word (denoted word ""3"" in the paper) relative to words 1,2.  After these phases, both the training and generalization losses go to zero (as long as embedding dimension is large enough).

Overall, I found the results interesting and insightful, though not very surprising, and the practical implications of these results were not very clear to me. Thus, I currently recommend weak accept. Importantly, my primary research area is not learning theory, so my knowledge of the related work is relatively limited, and thus my review confidence is relatively low.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It is interesting to see that the training dynamics for this word cooccurrence task can be analyzed rigorously, with relatively few assumptions.
- The theoretical results are validated with a nice synthetic experiments, that demonstrates that the two phases predicted by the theory do occur in practice.

Weaknesses:
- This word cooccurrence task is very simple, and thus it is not surprising that a single transformer layer can easily learn it.
- Only full gradients are considered, whereas transformers are typically trained with mini-batch Adam(W).

Limitations:
Yes, limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This article delves into the gradient flow dynamics for detecting word co-occurrence, demonstrating that the gradient flow approach can achieve minimal loss. The training process commences with random initialization and can be delineated into two distinct phases.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This article noticed an interesting phase transition during training in this special setting and demonstrates it with solid calculation and experiments.
- A new property of gradient flow is noticed and contributes to prove near minimum training loss together with the analysis of softmax.

Weaknesses:
The setting of empirical experiments is also simple and ideal and readers may have no idea if this is a general phenomenon during training for detecting word co-occurrence.

Limitations:
The semantic mechanism this article focuses on is not general enough in NLP and the the results cannot give instruction useful enough to guide the training process of Transformer.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
w67vRHZF13;"REVIEW 
Summary:
This paper proposes a novel learning paradigm to learn MLLMs based on interleaved image-text corpora. 
It introduces a structure-induced training strategy that imposes semantic relationships between input samples and
 the MLLM’s hidden state.
This work apply the dynamic time warping framework to calculate the semantic similarity between different image-text sequences.
Then, a discriminative loss is applied to sequence similarity matrices calculated based on raw inputs and MLLM hidden states. 
The framework can also leverage the capabilities of multiple vision and language encoders to more accurately calculate the similarity matrices.
Experiment results show that the new learning paradigm demonstrates good performance on basic multimodal comprehension benchmarks, 
complicated multimodal comprehension benchmark DEMON, cross-model information retrieval, and retrieval-augmented generation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to follow. 
2. This paper proposes a novel learning paradigm based on interleaved image-text corpora.

Weaknesses:
1. This paper did not discussed the impact of including interleaved image-text pairs in MLLM learning. For example, how will it affect the performance on basic visual-language benchmarks (Table 1) and image-text retrieval. Will there be any negative effects?

Limitations:
The authors have adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the limitations of Vision-Language Models (VLMs) by proposing a unified approach that combines generative and discriminative training paradigms. This new method leverages interleaved image-text sequences and introduces a structure-induced training strategy. It aims to enhance the MLLM's ability to capture global semantics and fine-grained details, effectively balancing generative and discriminative tasks. The approach uses dynamic sequence alignment within the Dynamic Time Warping framework and integrates a novel kernel for fine-grained semantic differentiation. Extensive experiments demonstrate that this method achieves state-of-the-art results in various generative and discriminative tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces a novel method that successfully integrates generative and discriminative training paradigms, addressing the weaknesses inherent in each when used independently.
- The authors clearly articulate the challenges faced by existing VLMs and provide a well-defined solution.

Weaknesses:
- While the paper shows impressive results, there is limited discussion on the potential limitations and areas where the model might underperform.
- The paper primarily focuses on specific benchmarks. It would be beneficial to discuss how well the proposed method generalizes to other types of vision-language tasks not covered in the experiments.

Limitations:
- Considering the connection with real-world scenarios is necessary. The authors need to discuss: what requirements might the tasks introduced in the paper have in real-world scenarios?

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a method for unifying generative training and discriminative training of multi-modal LLMs. Generative training mainly uses auto-regressive formulation while discriminative training mainly performs contrastive representation matching. The goal of this paper is trying to use discriminative training to improve multi-modal LLMs.

The paper unifies generative training and discriminative training by introducing a Dynamic Sequence Alignment module which aligns similar text and image data on the hidden states of a multi-modal LLM. In addition, Detailed Semantics Modeling is proposed to effectively distinguish detailed semantics.

The paper conducts evaluation on a wide range of benchmarks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The motivation is clear and the paper is easy to follow.

The concept of unifying generative training and discriminative training is interesting.

Weaknesses:
It's unclear what is dynamic time warping framework.

The performance improvement of the proposed method sugar is not significant. Compared with some baselines, such as VILA and LLaVA-1.5, Sugar performs worse than them on many tasks, as shown in Table 1. This raises concerns about the effectiveness of the proposed method. 

This could be meaningless to align a visual token and a text token in an MLLM model since the LLM is trained with next-token prediction instead of contrastive learning like CLIP. The current token is conditioned on previous tokens. I can't think of a reasonable explanation for this mechanism. It **could be** meaningful to align visual tokens. In addition, the experiment results also suggest that this method is not effective as expected. 

What is the evaluation protocol?  Does Sugar train on each benchmark first then evaluate or directly zero-shot evaluation? In the former case, will Sugar lose generative ability after training with discriminative task data?

Limitations:
-

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
w50ICQC6QJ;"REVIEW 
Summary:
This paper presents Causal representatiOn AssistanT (COAT), which introduces large language models (LLMs) to bridge this gap. LLMs are trained on massive observations of the world and have shown great capability in extracting key information from unstructured data. Thus, employing LLMs to propose useful high-level factors and craft their measurements is natural. COAT also uses CDs to find causal relations among the identified variables and provides feedback to LLMs to iteratively refine the proposed factors. This mutual benefit enhances both LLMs and CDs.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Interesting topic. Employing LLMs to propose useful high-level representations for causal discovery.
2. Develop two benchmarks for the unstructured causal discovery. AppleGastronome and Neuropathic.
3. Derives the first metrics that measure the causal representation learning capabilities of various LLMs.

Weaknesses:
1. ‘We will release an anonymous link during the discussion period.’ I will consider raising my score if the code is reasonable.
2. The contribution of LLM in COAT is a little small. I assume LLM is used as a representation tool to learn the conceptual level attributes, including iterative refining. The causal structural learning can still be considered as the downstream task.
3. COAT will inherit the shortcomings of downstream causal structure learning algorithms.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper tackles the problem of discovering relevant features for recovering the underlying causal graph in the absence and/or in lieu of a human domain expert. The proposed method, COAT, first queries an LLM through a prompt elucidating the task (for eg., discovering relevant features that affect a product review using a few text reviews), then the proposed variables are fed into another LLM that assigns a value to each of these variables, thus outputting structured/tabular data that can be used for causal discovery. Finally, the tabular data is used in conjunction with a traditional causal discovery algorithm (FCI and LiNGAM in this case) to retrieve a causal graph with respect to a target variable (for eg., review score) using the proposed variables. The process repeats until the proposed variables form a markov blanket for the target variable w.r.t the raw unstructured input data (for eg., the text reviews), progressively expanding the markov blanket in each iteration. Additionally, the LLM can receive feedback at the end of each iteration in the form of samples that the proposed variables cannot sufficiently explain. In particular, the authors propose clustering the samples w.r.t the latent variables induced by the LLM and picking the samples in the cluster with the largest conditional entropy.

Initial theoretical analysis of the proposed method implies that the proposed method is able to identify the markov blanket for a target variable using the proposed variables given that enough iterations of COAT are performed.

The authors evaluate COAT empirically over two synthetic datasets and three real-world datasets. They compare COAT against two simple baselines 1) factors being directly proposed by the LLM based on the prompt without further iterations 2) factors being proposed by LLM when queried using both the prompt and some samples of raw observations. The second baseline is essentially COAT without the LLM receiving any feedback after each iteration. The experiments are conducted using 10 different LLMs and primarily one causal discovery algorithm (FCI), with additional experiments on one dataset using LiNGAM. Additionally, the paper proposes two novel metrics for quantitatively assessing the performance of LLMs for feature proposals to be used for causal discovery.


Update: I moved my rating up in the hope that teh authors will add the experiments as they promised to the final version. We have no way of enforcing it but hopefully, the authors will follow up on their promise.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper addresses the important problem of causal discovery and employs an effective two pronged approach involving LLMs and traditional causal discovery algorithms. This approach leverages the strengths of both the LLMs and the causal discovery algorithms i.e, ability to respond to complex prompts and unstructured data with high-level and possibly noisy information, and robust causal discovery with strong theoretical guarantees although requiring strong assumptions on the faithfulness of the data and causal mechanisms, respectively. Overall, I believe this is a promising direction wherein the two components complement each other effectively.

The empirical evaluation is sufficient in terms of the large number of LLMs considered and the moderate amount of datasets evaluated. The results, based on the chosen metrics, sufficiently demonstrate the effectiveness of the proposed method over the simple baselines.

Finally, the paper is well-written and clearly explains the steps involved in each iteration. The further explanations provided in the appendix also aid in this.

Weaknesses:
The theoretical aspects of the proposed algorithm are exaggerated in the introduction. Given the strong assumptions of “sufficiently powerful” LLMs, “sufficiently diverse” examples and further assumptions pertaining to the chosen causal discovery method, the propositions, while appreciated, are rather straightforward. In particular, it would be far more interesting to theoretically analyse the impact of modules involving the LLMs themselves, such as the chosen prompt template, quality of factor annotations and responsiveness of LLMs to feedback regarding causal discovery, even though some of these are evaluated empirically. Also, an analysis on the rate of convergence of COAT would be beneficial.

Secondly, while the modularity of the proposed approach facilitates utilising a cross product of LLMs, causal discovery methods and feedback mechanisms, it also necessitates extensive ablation studies. In particular, the paper would be strengthened by a thorough ablation of the initial prompts and feedback. In particular, a discussion and ablation on the chosen prompt template and its effect on the proposed factors, or lack thereof, is needed. A robust template would allow more seamless adoption of the proposed method. Finally, the chosen baselines are far too simple to make any strong claims on the effectiveness of COAT. Comparing against some of the methods covered in the related work section would help bolster this claim.

Limitations:
See the weakness and questions above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes COAT (Causal representation AssistanT), a novel framework to leverage LLMs to assist with causal discovery from unstructured data. COAT aims to combine the advantages of LLMs and causal discovery algorithms. To do so, COAT employs LLMs to identify high-level variables and parse unstructured data into structured data. On the other hand, causal discovery algorithms read the parsed data to identify causal relations. To improve the reliability of the results, COAT also constructs feedback from the causal discovery results to iteratively improve the high-level variable identification. The authors conduct extensive case studies ranging from synthetic data to realistic data, and find COAT effectively helps with discovering meaningful causal structures that well explain the target variable.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. This work identifies a crucial and timely problem for how to advance the causal tasks including causal learning and reasoning with foundation models likes LLMs;
2. COAT is novel, interesting and well-motivated. The authors also provide theoretical discussion to justify its soundness;
3. COAT is model-agnostic and robust to the choice of LLMs, and input data modalities;
4. The authors construct several benchmarks, present comprehensive case studies, and conduct extensive experiments to verify their claims. The improvements over direct prompting LLMs are significant.

Weaknesses:
1. The authors should provide more comparisons with advanced prompting techniques such as CoT.
2. More discussions should be provided on the hyperparameters used in COAT, such as the group size in feedback.
3. Model names are inconsistent. The name in Fig 4(c) is not the same with other names.
4. GPT-4 reasoning in Fig 7(c) is unclear in meaning.

Limitations:
No concerns regarding limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper combines the power of LLMs with that of causal discovery by proposing a Causal representatiOn AssistanT (COAT) approach. Specifically, it considers datasets with textual descriptions, and tries to identify the Markov blanket with respect to a target variable (such as customer ratings and medical diagnosis). The key contribution is discovery of the causal factors through a pipeline that uses both LLMs and a causal discovery algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
I find this an interesting and practical paper that combines the advantages of LLMs – such as the vast amount of knowledge that they encode – and that of causal discovery approaches. The ideas around combination are generally simple but novel, and I believe the approach could potentially be valuable in a suite of applications, although the extent of the value is unclear from the paper.

Weaknesses:
A major limitation of the work is the empirical evaluation, even though it comes across on the surface as being extensive. I sympathize with the authors about benchmarks for causal discovery, but it seems they have used GPT-4 to generate the textual description of the data, and then used LLMs in their COAT procedure. This is clearly a synthetic dataset that can be problematic. Even the “realistic” benchmarks do not come across as sufficiently realistic, based on my understanding and the lack of details in the main paper.

I don’t understand why key aspects of the evaluation were moved to the appendix. I find it impossible to fully evaluate the work based solely on the contents of the main paper. I understand the need to make space and to move things to the appendix, but it’s never suitable to move key aspects such as the description of the benchmarks and the key results that show value of the work. This has impacted my assessment of this work and I have had to decrease my score because of the authors’ choices around appendix content.

A related weakness is the lack of any attempt at describing limitations, of which there are clearly many.

Limitations:
The authors have not suitably described limitations. I consider this a major weakness; please see my previous comments.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
w4AnTVxAO9;"REVIEW 
Summary:
The paper explores the ability of language models to skip steps in their reasoning processes. The authors introduce a controlled framework to stimulate step-skipping behavior by iteratively refining models to generate shorter and accurate reasoning paths. The study demonstrates that models can develop this ability under guidance, leading to increased efficiency without sacrificing accuracy. The paper presents empirical results showing enhanced generalization capabilities in out-of-domain scenarios after fine-tuning on expanded datasets that include both complete and skipped reasoning sequences.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The empirical results are robust in three domains, showing benefits in efficiency of the proposed method.
- The paper is clearly written and well-organized, making it easy to follow the authors' methodology and findings.

Weaknesses:
- Only one backbone model is considered. Experiments across model families and model sizes should be considered to show the generalization ability of the proposed methods.
- The OOD test is actually the harder in-domain test. I am curious about the across domain effect of the proposed method For example, how's the effect of training on ""Analog of Algebra"" and test on  ""Multi-digit Addition"", given that the skip ability should be a general ability across different domains?
- In methodology: ""We begin with a training dataset D0, which contains detailed full-step reasoning answers to the questions."" -> How the full-step reasoning data created?

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes an iterative training method that helps sequence models learn to skip steps. The method starts from a training set with full-length solutions or mixed with some skipped-length solutions. At each stage a model learns these solutions with the instruction “Solve it in n steps” and is prompted to generate shorter answers. Correct shorter answers are added to the training set. The effect of this approach is tested using LlaMa-7B on three tasks, including algebraic evaluations, multi-digit addition, and a direction inference task.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed skip reasoning pipeline is interesting and was evaluated against a diverse set of tasks with different levels of OOD generalization tests.

The authors conducted detailed analyses to understand the effect of the training pipeline, e.g., Figure 5 with the multi-digit addition is very informative.

The overall presentation is very clear and easy to follow.

Weaknesses:
My main concern is the generalizability of this method. As shown in the paper, the model largely benefited from the warm start setup that includes some skipped problems in the first training set, and has trouble generalizing to problems requiring more steps. One interesting generalization test would be to train on a mixture of all three tasks, but withhold adding skipped step instances for one task, and see if the model can generalize skipping steps on the withheld task.

The shorter answers generated during the iterative process also don't seem quite “generated by the model itself”, as filtering out correct answers would require oracle knowledge. Is it assumed that correct answers are any exact subset of the full-length solution?

Limitations:
Noted in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to teach LLMs to deliberately skip steps when doing complex tasks involving multi-step reasoning. The authors use self-generated inference paths with fewer steps to fine-tune the models, which is similar to self-distillation. The authors conduct experiments on a few controlled tasks show that the proposed approach can effectively reduces the reasoning steps while maintain performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of teaching LLMs to skip steps following the human reasoning process is intuitive and makes sense.
2. The proposed method is overall technically sound and well described.
3. The paper is in general well-written and easy to follow.
4. Experimental results confirm the effectiveness of the proposed approach, at least on these ""artificial"" tasks.

Weaknesses:
1. The experiments are not solid because the tasks considered in the experiments are very artificial and not representative for real-world reasoning tasks. The paper could be made much stronger by conducting tasks/datasets such as GSM8K/MATH or coding tasks, instead of simple reasoning tasks. Without the empirical study on realistic tasks, it is hard to confirm the contribution and usefulness of the proposed metric.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a method for training an LLM to solve reasoning problems using fewer verbalized reasoning steps than it is naturally encouraged to by a fixed training dataset. The resulting model is shown to maintian or improve performance on in-distribution data and OOD data testing extrapolation w.r.t. length or compositionality, while using fewer reasoning steps at inference time. Analysis shows that performance gains are concentrated around problems requiring an intermediate number of reasoning steps, rather than very few reasoning steps. Experiments are conducted with Llama-2-7b on three synthetic datasets. The method itself works by using warm-start data with mixed length reasoning demonstrations, followed by bootstrapped training data created by controlling model generations with control codes (instructions) combined with filtering model generations for correctness to create new gold data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Very important: The idea of shortening reasoning steps, particularly to mimic human reasoning that is variable in its verbalized length, is a very interesting and practical direction.
- Very important: Results are positive and promising for model generalization at an increased level of efficiency. Particularly interesting are results suggesting that model performance can increase on difficult OOD data by virtue of skipping some reasoning steps. Initially, CoT was found to improve OOD generalization, but it seems that this iteration on CoT could improve OOD performance even more in some situations.
- Important: The paper is overall straightforward to read and understand, with only a few exceptions.
- Of some importance: The connection to easy-to-hard generalization was interesting to me. That this method could improve OOD performance, specifically length/compositional generalization, was very interesting.

Weaknesses:
- Important: What are the instructions at inference time? Do you require a ground-truth number of reasoning steps to run the model at inference time? If so, this important detail is missing from the paper and could make the method difficult to use in practice for problems if it is not know how difficult they are in advance. Would the method be robust to misspecified instructions at inference time?
- Important: I find it a little confusing to reconcile the results of Sec. 5.1 with Sec. 5.2. Sec. 5.1 makes it look like using fewer steps greatly hurts model performance, while Sec. 5.2 makes it seem like using fewer steps does not hurt performance (specifically the Warm start rows, relative to Cold start baselines).
- Of some importance: The data is a little artifical. There are existing reasoning and compositional reasoning benchmarks that could be appropriate for this work (though they could require stronger models), including SCAN (https://arxiv.org/abs/1711.00350), GSM8k, StrategyQA, and MATH datasets. However, this is not a major weakness as using clean, controlled datasets is advantageous for studying these kinds of phenomena and they enable automatic construction of warm start data.

Limitations:
Discussion was adequate, but it could be worth mentioning that experiments were conducted with only one model and three synthetic datasets.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Grd7yzFm5V;"REVIEW 
Summary:
The paper proposes a novel method, ""Gaussian Mixture Domain-Indexing"" (GMDI), to address domain adaptation with inaccessible domain indices. The technique improves upon prior work by modeling the domain indices prior with a Gaussian Mixture. Empirically, it has been shown that the proposed method achieves state-of-the-art performance in classification and regression tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Novelty**: The paper proposes a novel technique to address the issue of domains in domain adaptation having multiple semantics. The method is a natural extension from prior work (VDI) by changing the Gaussian prior to a Gaussian Mixture.

**Theoretical Support**: The paper proves the correctness of the proposed method with solid theoretical derivations.

**Empirical Validation**: Extensive experiments show that GMDI significantly outperforms state-of-the-art methods in both classification and regression tasks, with substantial improvements in accuracy and mean squared error (MSE). I particularly like the illustration of the learned domain indices. Figures 4 and 5 clearly show that GMDI learns the domain indices more accurately compared to the prior state-of-the-art method, VDI. Good job!

Weaknesses:
**Clarity**: Some equations in the paper might have typos. Please see the details in the next section.

**Computational Overhead**: The use of CRP and dynamic mixtures increases computational overhead, which might make the method less practical for large-scale or real-time applications without further optimization. It would be helpful if the authors could provide some comments and empirical analysis on the computational overhead of GMDI.

Limitations:
I do not see specific limitations that might lead to potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a Bayesian Domain Adaptation method with Gaussian Mixture Domain-Indexing (GMDI) to address the challenge of inferring domain indices when they are unavailable. Existing methods often assume a single Gaussian prior for domain indices, ignoring the inherent structures among domains. GMDI models domain indices as a mixture of Gaussian distributions, with the number of components dynamically determined by the Chinese Restaurant Process. This approach provides a higher level of flexibility and effectiveness in adapting to diverse target domains. Theoretical analysis demonstrates that GMDI achieves a more stringent evidence lower bound, closer to the log-likelihood. Extensive experiments on classification and regression tasks show that GMDI significantly outperforms baselines, achieving state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. GMDI is the first to model domain indices as a mixture of Gaussian distributions, allowing it to capture the inherent structures among different domains. This approach provides a more flexible and powerful way to infer domain indices.

2. By using the Chinese Restaurant Process, GMDI can dynamically determine the number of mixture components, adapting to varying numbers of domains. This enhances its capability to handle complex datasets with an unknown number of domains.

3. The paper provides a detailed theoretical analysis, demonstrating that GMDI achieves a more stringent evidence lower bound and a tighter upper bound of the objective function compared to existing methods. This theoretical foundation supports the effectiveness of the proposed approach.

Weaknesses:
1. GMDI relies on the availability of domain identities but cannot infer them as latent variables. This limits its applicability to scenarios where domain identities are also unknown.

2. The use of the Chinese Restaurant Process and Gaussian Mixture Model can be computationally intensive, especially for large-scale datasets with numerous domains. This could hinder the scalability of GMDI.

3. In the experiment, the binary classification task is not challenging, which makes the performance advantage not convincing. The multi-classification tasks are necessary.

Limitations:
See weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces the Gaussian Mixture Domain-Indexing (GMDI) algorithm for domain adaptation when domain indices are unavailable. Unlike traditional methods that use a simple Gaussian prior, GMDI employs a Gaussian Mixture Model adjusted by a Chinese Restaurant Process, enabling adaptive determination of mixture components.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of a Gaussian mixture to represent complex domain indices seems interesting.
- The proposed method demonstrates superior performance over baseline models in experimental results.
- The technical part (the proposed method) seems non-trivial and its complexity seems sufficient, yet it might not be necessary.

Weaknesses:
- The learning loss for the proposed model is over too complex, featuring multiple conditional Kullback-Leibler divergences, which might complicate implementation and interpretation.
- The paper lacks clear definitions for the 'local' and 'global' domain index within the probabilistic graphical model, which could confuse readers about the model's scope and applicability.
- While introducing the Chinese Restaurant Process (CRP) adds flexibility to the Gaussian Mixture Model, it also increases computational costs. For the problem described in Figure 1, a fixed-component GMM might have been a simpler and more effective solution, though the number of components may not be that flexible. 
- The authors should also seriously consider empirically comparing the proposed method with a fixed-component counterpart, e.g. by simply using Gumbol softmax to infer the Gaussian component.
- The related work may not be sufficiently discussed. For example [a] discussed an end-to-end approach that learns the domain index using adversarial learning; [b] takes the domain index/identity as a latent dynamical system, coupled with adversarial learning. 

[a] Out-of-distribution Representation Learning for Time Series Classification
[b] Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation

----------------Minor------------------

- The connection between Equation (5) and Equation (6) is not clearly explained, leaving a gap in understanding the sequential logic of the model's formulation.
-The paper's clarity and accuracy in writing could be improved. For instance, the statement ""DP requires a predefined number of components"" is misleading, as Dirichlet Processes are inherently nonparametric and do not require a predefined number of components.
- Several symbols used in the equations are not adequately explained (both in the major paper and appendix), making it difficult to fully grasp the proposed model and its mathematical foundations.
- The proofs and the theoretical part seem to closely follow that of VDI, making it hard to evaluate its novelty.

Limitations:
The limitations are  addressed in the Sec. conclusion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
w3JCTBRduf;"REVIEW 
Summary:
This work shows that a deterministic optimization procedure can find a matrix $A$ that satisfies the Johnson Lindenstrauss guarantee. That is, a matrix $A$ maps a set of $n$ vectors to a lower dimensional space while preserving all pairwise distances up to some chosen multiplicative distortion. Typically, $A$ is constructed by sampling it from a random matrix distribution with i.i.d. entries. The authors prove that attempting to directly optimize the entries of $A$ through an optimization procedure by minimizing the maximum distortion is prone to being stuck at local minima. However, the authors show that by optimizing the mean of each entry and the entry-wise variance of the distribution $A$ is sampled from, one can maintain a fixed probability of $A$ being a JL-embedding while at the same time guaranteeing the entry-wise variance ‘sigma’ goes to zero. They then show that, when ‘sigma’ is sufficiently small, one may use the optimized expectation of $A$ as the embedding matrix while only slight increasing the maximum distortion, thereby deterministically finding the desired JL embedding matrix $A$. They show that $\rho$-SOSPs (second order stationary points) have sufficiently low variance when $\rho$ is small, and finally show that a method for finding $\rho$-SOSPs suffices to solve the designed optimization problem.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Overall, the paper is clearly written and well-motivated. The intuition of the approach and analysis is easy to follow.

The key idea of optimizing the parameters of a random matrix distribution to preserve the JL-embedding property while reducing the entry-wise variance seems like an innovative approach. The authors point out the original space of matrices is contained in this larger probabilistic space, since a deterministic matrix $A$ is equivalent to having mean $A$ and zero variance. Hence, this can be seen as a probabilistic relaxation of the original matrix optimization problem. I have not seen this type of relaxation used in the field of matrix sketching or more generally randomized numerical linear algebra before, and I believe it may be useful for other problems in the area. I am not very familiar with diffusion models, so I cannot speak on the novelty of the approach regarding that area.

The empirical results are also strong in the sense that they show this procedure for constructing a JL embedding tends to achieve a much lower distortion factor than randomized constructions for a fixed dimension.

Weaknesses:
The iterative method to find the matrix $A$ takes $\operatorname{poly}(n, k, d)$ steps, i.e., the complexity is proven to be polynomial but not explicitly determined. Since the paper is primarily theoretical with only limited experiments, it is unclear how efficient this method is in practice.

While the results seem very interesting theoretically, the paper could be strengthened by pointing out some practical applications where this improved deterministic JL embedding would be useful. In the applications I am familiar with, oblivious JL embeddings are needed due to the large number of points in the high-dimensional space (e.g., preserving k-means loss). The authors point to embeddings in deep learning as motivation. It is unclear to me as to how the authors expect progress in understanding deterministic JL embeddings to relate to these embeddings in deep learning. Additional clarification of this point would be helpful.

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes to calculate the embedding matrices used in the statement of the Johnson-Lindenstrauss lemma using optimization instead of randomization. The proposed algorithm is a Hessian descent. Authors prove that the algorithm finds the matrix of minimum distortion. Numerical results display the findings

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
3: good

Strengths:
JL is a well celebrated result used for proving existence of optimal (low distortion) embeddings. It is stated in the formal result of JL that such embeddings can be found in polynomial time. But we often rely on randomization to exhibit them. It is useful to have an algorithm to calculate the embeddings. The paper tackles a well motivated problem and their presentation is clear and clean.

Weaknesses:
The paper lacks complexity analysis of the algorithm. The algorithm proposed requires a full eigenvalue decomposition at every step. It is prohibitive to use this method in any practical scenario. A discussion on the complexity and how to scale the method up (using randomized methods??) would be nice.

Limitations:
scaling the method up and a clear argument why relaxation of Eq (3) is not making the problem trivial would impove the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers using the optimization method to ""learn"" the Johnson Lindenstrauss transform. The paper first shows that the naive objective may not be good enough -- there are stationary points that are sub-optimal. Instead, they consider the way that optimize the random Gaussian space rather than the concrete matrix. Then the authors give an optimization method and show that using this way, every stationary point is a good point and claim that this gives way to deterministic learns the JL matrix. Finally, the paper gives an experiment that shows the advantages of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The theoretical analysis of this paper is very interesting. To my knowledge, there are very few results about analyzing the landscape of the learned sketching matrix and this paper gives a strong analysis. The experiments also show the advantages of the proposed method. The presentation of the paper is also good.

Weaknesses:
I am still confused about some parts of the paper. I can raise the score if the authors can adjust this. (see the below question)

Limitations:
See above questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the problem of using optimization-based approaches to learn Johnson Lindenstrauss(JL) embedding. The authors proposed a new framework to achieve the JL guarantee via optimization, instead of the traditional randomized methods. Similar with diffusion models, the authors proposed a novel method that uses optimization in an extended space of random Gaussian solution samplers, which circumvents direct optimization in non-convex landscape. Their approach uses second-order descent, gradually reduces the variance without increasing the expected distortion of the sampler, then can identify a specific projection matrix with the Gaussian distribution. Overall, theoretical guarantees and empirical results demonstrate that this method efficiently achieves a deterministic solution that satisfies the JL guarantee.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper is well-written. The state of the art is well discussed by an extensive literature review. The proposed method combining optimized-based approaches and Johnson Lindenstrauss embeddings is an innovative contribution to the field. 

The paper is technically sound, provides rigorous theoretical analysis and proofs.

Weaknesses:
It would be helpful to understand the main results if section 4 could be more organized, such as using subsections.

Limitations:
No potential negative societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
w2L3Ll1jbV;"REVIEW 
Summary:
In this study, the authors explore adversarial multi-task representation learning, where a predictor and feature extractor are trained on multiple source tasks with adversary and then another predictor following the feature extractor is trained on a target task with adversary. They provide bounds on the excess risk under mild assumptions, showing that these bounds decrease with larger training sample sizes for individual source tasks $n$, the total sample size of all source tasks $nt$, and the sample size of the target task $m$. These results suggest that large sample sizes and diverse source tasks contribute to robust learning in adversarial transfer learning. Additionally, the input and feature dimensions increase these bounds. The excess risk decreases more rapidly when using smooth and non-negative losses compared to Lipschitz losses from a sample size perspective. Furthermore, based on the multi-task results, the authors consider excess risk in a single-task setting.

The authors first derive Theorem 1 for Lipschitz loss and Theorem 4 for non-negative losses based on [38]. Rather than directly addressing the adversarial loss class, they consider the inflation of the sample space $S$ by an adversarial attack $A$, examining its coverage by balls and standard volume arguments. These results bound the Rademacher complexities of function classes in source and target tasks with adversary (Theorems 2 and 5).

Additionally, a new reduction method from a multi-task setting to a single-task setting (Theorem 3) may aid future work in both adversarial and non-adversarial settings.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The problem settings and assumptions regarding data distribution (Lines 140--145), function properties (Assumptions 1--4), and the fat-shattering dimension and size of the inflated dataset (Theorems 2 and 5) are mild. The derived results, such as the order in terms of sample size and input or representation dimensions, seem appropriate. The bounds are interpretable and offer important insights for adversarial transfer learning: diverse source tasks and sample sizes facilitate robust transfer learning.

Many prior studies emphasize the importance of sample complexity in adversarial training. However, obtaining sufficient training samples for a target task is not always feasible. This study theoretically provides valuable guidance for such situations from the perspective of transfer learning.

Moreover, the derived upper bounds have the same order (growth rate concerning dimensions and sample sizes) as prior work on the non-adversarial setting [38]. This indicates that even in adversarial training, it is sufficient to prepare training samples similarly to standard training, ignoring constant and logarithmic terms, which is a positive outcome for the community.

Weaknesses:
One might (easily) predict this result from [38]. Under Assumption 4, the sample complexity of the perturbed dataset can be regarded as the finitely scaled sample complexity of the original dataset (as the authors exploited this concept). From the perspective of covering number and Dudley's integral, this leads only to logarithmic differences in orders. It might not be very difficult to conclude that the same order controls the bounds of the excess risk even in adversarial transfer learning as in standard transfer learning. Nonetheless, I acknowledge the authors' effort in providing a formal proof, even if the results are predictable.

The looseness of the bound is also a weakness, though it is a natural property of Rademacher complexity-based bounds. For example, the bound in Theorem 1 includes two worst-case Rademacher complexities $\hat{R}(\ldots, n)$ and $\hat{R}(\ldots, m)$, and $\sup_h R(\ldots)$ (the worst-case in terms of the hypothesis class of representation). This looseness may be due to the mild assumptions. Tighter bounds for more restrictive cases might enhance the interpretability of the derived bounds.

Limitations:
The authors address the limitations of their assumptions in Appendix C.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper conducts theoretical studies on adversarially robust transfer learning, which is to learn a model with small robust error on a downstream (target) task from a model pretrained on multiple other (source) tasks. Considering the specific multi-task representation learning (MTRL) setting, this paper provides rates on the excess adversarial (transfer) risk for Lipschitz losses and smooth non-negative losses, showing that a representation derived from adversarial pretraining can assist in defending against adversaries on downstream tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.  This paper theoretically shows bounds on the excess transfer risk for the adversarial loss class for both Lipschitz losses and smooth nonnegative losses, demonstrating the benefits of adversarial pretraining on source tasks for downstream tasks in transfer learning.

Weaknesses:
1. The proposed theoretical results are interesting, but empirical experiments are missing to support the presented theories, such as the benefits of adversarial pertaining to downstream tasks and that it takes fewer samples to learn a good predictor for downstream(target) tasks with adversarially robust representations learned from related source tasks,
2. As the paper introduces some additional empirical assumptions, such as assumption 4 which requires adversarial attack functions to be bounded within the known input domain, some practical examples or empirical experiments will be helpful to justify it.

Limitations:
Limitations are discussed in the paper. No potential negative societal impact is found.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies adversarially robust transfer learning, wherein, given labeled data on multiple (source) tasks, the goal is to train a model with small robust error on a previously unseen (target) task. The paper considers a multi-task representation learning (MTRL) setting, i.e., assuming that the source and target tasks admit a simple (linear) predictor on top of a shared representation (e.g., the final hidden layer of a deep neural network). The paper provides rates on the excess adversarial (transfer) risk for Lipschitz losses and smooth nonnegative losses. These rates show that learning a representation using adversarial training on diverse tasks helps protect against inference-time attacks in data-scarce environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper has good originality, quality, clarity, and of important significance.

Weaknesses:
No experiments are provided.

Limitations:
It's better to verify the effectiveness of the proposed approach on realistic datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the adversarially robust multi-task representation learning. They introduce the definition of robust $(\nu, \epsilon, \mathcal{A})$-task diversity and the algorithm of two-stage adversarial MTRL. Using these, they show novel results on excess transfer risk for adversarial loss under mild conditions. The authors then present the proof sketches and compare the results with previous work in detail.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. It is a valuable work to study adversarially robust multi-task representation learning. The notations, definitions, and assumptions are all clearly written, which makes it easy to understand.
2. The algorithm 1 is reasonable in practice for me. the authors discuss the novel assumption to show that it is also reasonable. Most of the assumptions in this paper seem to be mild. 
3. The authors carefully discuss the differences of results shown in this work and related works. They also compare the techniques used in this work and previous works. It is clear to understand the contribution of this work.

Weaknesses:
1. The proofs shown in section F.1 are not clear. The authors do not show the formal proofs of these theoretical results.
2. The authors introduce the definition of the vector-valued fat-shattering dimension, which is a generalization of the fat-shattering dimension, while it does not seem to appear in the theoretical results, which makes it confusing.

Limitations:
N/A.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
w28i9oe9Xr;"REVIEW 
Summary:
The paper addresses the issue of weak convergence of stochastic processes, whereby evolving information is generally unaccounted for. This can lead to discontinuities when applying these processes to multi-period decision-making problems. Prior work has proposed the concept of extended weak convergence, as introduced by Aldous (1981), but practical numerical implementations have been challenging. To address this, the authors introduce a novel metric called High Rank PCF Distance (HRPCFD) which is shown to overcome computational issues encountered in previous attempts. The paper then demonstrates the utility of HRPCFD via experiments on hypothesis testing and generative modelling of time series data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Unfortunately this paper lies well outside my area of expertise and I am unable to review it effectively. The mathematical framework around extended weak convergence is not an area I’m familiar with, and I consequently found it challenging to grasp the nuances of the problem statement, the significance of the proposed HRPCFD metric, and the potential implications for applications in finance and economics.

So as not to negatively impact the paper’s chances of acceptance, I have defaulted to a mid-range score in my review, which reflects my assessment that the paper could nevertheless still be made more accessible for readers who are less familiar with the domain.

Weaknesses:
See above.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
Time series is ubiquitous in machine learning. They are modeled as stochastic processes and therefore notions of distance between stochastic processes and more generally convergence of stochastic processes are fundamental ideas. Weak convergence of probability measures occupies a central position in this area, but for many settings, like optimal stopping, or the one studied in this paper, it is not sufficient. Extended weak convergence, defined via weak convergence of prediction processes, is the right notion. This paper introduces a metric on the space of filtered processes that metrizes the topology of extended weak convergence, proposes statistical procedures to compute it, and then tests these ideas on GANs for time series.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This is a paper on a very nice topic and I learnt a lot while reading it. Some of the ideas are very abstract and the paper does a good job of organizing the topics and defining everything precisely so that a meticulous reader is not left confused. It is welcoming to see a rigorous paper in ML conferences. The ideas introduced in the paper are novel and the proofs of all the claim are carefully done, although I can't claim to have read every section in appendix in detail.

Weaknesses:
Although as mentioned above the paper defines everything clearly, the exposition on PCF and HRPCF could be improved. It took me quite some time after re-reading the paper multiple times and some other referenced paper to develop an intuition for these concepts, even though I have some background on probabilistic notions like weak convergence and extended weak convergence. I understand this is difficult to do well in a conference paper with page limits, but I think having a more detailed appendix on PCF and HRPCF would help.

I should mention here that I didn't find the experiments super convincing, but I am viewing this paper as a theoretical contribution, and thus any experiments it has as an added bonus and not a weakness.

Limitations:
N/A

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper constructs a computationally-implementable metric which metrizes an ""extended"" weak convergence for stochastic processes, which more plausibly accounts for the convergence of the process with respect to their filtrations.
The result can apparently more effectively account for similarities between controlled processes, at least in the class of linearly-interpolated stochastic paths considered here.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The method seems to construct high-rank analogues of classic tools, such as the characteristic function, such that the prediction processes arising from projection onto the filtration at each moment can be quantified and divergence between them metrized, without density evaluations, using empirical measures.

The classical SDE reasoning seems sound. I confess that I am less familiar with the rough path theory component, but there are no obvious red flags in the material.

Weaknesses:
The paper is long;
The results seem to be an improvement both theoretically and empirically over the main antecedents

* [18] Hang Lou, Siran Li, and Hao Ni. PCF-GAN: generating sequential data via the characteristic  function of measures on the path space. Advances in Neural Information Processing Systems,  36, 2023. 
* [19] Cristopher Salvi, Thomas Cass, James Foster, Terry Lyons, and Weixin Yang. The signature 392 kernel is the solution of a goursat pde. SIAM Journal on Mathematics of Data Science, 3(3):873–899, January 2021.
* [20] Cristopher Salvi, Maud Lemercier, Chong Liu, Blanka Hovarth, Theodoros Damoulas, and  Terry Lyons. Higher order kernel mean embeddings to capture filtrations of stochastic processes.  Advances in Neural Information Processing Systems, 34:16635–16647, 2021

But it is not clear whether the increment is ""important"" in practice; Is the increased performance ""worth"" the implementation effort and/or computational cost? The answer is probably problem-dependent.

Limitations:
No particular issues noted.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes High Rank Path method, motivated by the extended convergence notion and the rough path theory, to generate (conditioned) time-series data. A new metric HRPCFD is introduced, and experiments are conducted for Brownian motion, GANs with applications in finance.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is rigorously written, which introduces a new metric HRPCFD on the path-valued processes based on various ideas from probability theory -- extended convergence, rough path, signature... I checked most proofs, and they are correct.

Weaknesses:
Weakness and comments:

(1) The paper may be too heavy for the Neurips audience (though I enjoyed reading it). It seems to be more suitable for a rigorous mathematical or statistical journal (e.g., Annals of Statistics). 

(2) Many proofs of the results (e.g., Thm 3.3) are purely measure-theoretical, and I think the authors may shrink some proofs to keep the idea concise. 

(3) The authors may want to explain why the proposed HRPCFD outperforms others (e.g., signature...) Is there any possible theoretical guarantee?

(4) The authors may have a discussion on the computational efforts of the proposed method (e.g., computational complexity and running time). The path-space optimization (or signature-type methods) often suffer from computational efficiency.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vymkuBMLlh;"REVIEW 
Summary:
The paper leverages state-of-the-art conditional generative models and algorithms from causal do calculus to perform ""approximately correct"" high-dimensional interventional sampling. Their contribution is ID-GEN, a recursive algorithm that uses diffusion models (among other generative models) to sample from any identifiable causal effect estimand in high-dimensional settings. The efficacy of the method is demonstrated on three diverse datasets: Colored MNIST, CelebA, and MIMIC-CXR.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The paper introduces ID-GEN, a novel algorithm that integrates diffusion models with causal inference for high-dimensional interventional sampling.
- ID-GEN creatively exploits the causal structure via the known recursive algorithm for sampling in complex causal scenarios, particularly with latent confounders.
- The algorithm is applied in three applications across diverse datasets (Colored MNIST, CelebA, MIMIC-CXR).

Weaknesses:
- The paper is objectively hard to read. Many important graphical elements that should be paired with the text in the main paper are delayed to supplements. This is notably problematic for Example 4.1, where one would expect the full example to be self-contained in the main text. 
- The contributions are stated in the introduction, but it still seems hard to understand if the proposed method is ""just"" an implementation of the ID algorithm, replacing probability tables by samples from diffusion models. I appreciate that this is hard already, but it has much lower novelty then proposing a new recursive algorithm. This should be well explained in the manuscript. 
- The paper does not discuss the implications of the proposed algorithm. Is there any way to extend this to symbolic calculus, or to probabilistic programming? What are the obstacles for moving towards automatic causal inference with images (which would be a super exciting prospect).

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an algorithm for sampling from intervention distributions under known causal models with hidden confounders, using conditional generative models.
This allows nonparametric distributional estimation of high-dimensional outcomes such as X-ray image generation, unlike existing methods.
The proposed method combines the existing ID algorithm with a generative model and inherits the ID algorithm's theoretical guarantees. That is, non-identifiable quantities are indicated to be non-identifiable, while all identifiable quantities can be estimated.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
* They shed light on the new problem setting of causal simulation for outcomes with high-dimensional and multimodal distributions, such as image generation. This could open up new applications if well justified. Such a problem setting requires a very different approach to point estimation of expectations for low-dimensional, unimodal distributions.
* The theoretical background is solid and well-explained. The method is proved to be able to estimate all identifiable quantities and otherwise outputs ""unidentifiable.""

Weaknesses:
[W1] The motivation for high-dimensional distribution estimation is weak. For example, it does not seem very meaningful to me to generate synthetic X-ray images.

[W2] In particular, is it important in cases where there are bidirectional edges due to the presence of hidden confounding factors, but where the causal orientations are all identified among variables? A clear comparison with similar methods would be beneficial for readers, e.g., comparison in assumptions and targets (e.g., parametric/nonparametric, latent confounder, distributional estimation, etc.).

[W3] The base procedure seems to come from existing methods, such as the ID algorithm, and they just combine it with a generative model.

Limitations:
The method is limited to the cases where the causal direction is known.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of sampling from an interventional distribution of high-dimensional data, where the causal relationships are described by a acyclic directed mixed graph (ADMG). Motivated by the ID algorithm that provides a recursive way of identifying any causal effect from a conditional probability table, the authors propose ID-GEN that follows the recursive steps of ID but instead trains deep generative models to fit the conditional distributions. The final sampling model is then obtained by connecting all the trained networks together in some suitable way. The authors prove that ID-GEN is sound and complete, and run extensive experiments on both synthetic and real dataset to demonstrate the effectiveness of their approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies the problem of sampling from an interventional distribution, an arguably important problem with broad applications. Current approaches, as the authors point out, are either restricted to simple causal graphs or face computational challenges.

2. Most parts of the paper are clearly written, and sufficient explanations are provided for the key steps in the ID-GEN algorithm. Also, simple examples are provided that help with the understanding of the paper. The paper is also well-organized and the authors put most complicated details into the Appendix.

3. The authors conduct extensive experiments to demonstrate the superior performance of their model, by comparing with other sampling models proposed by previous works.

Weaknesses:
I don't think this paper has obvious weaknesses. One thing that the authors may wish to improve is that the notations are a litle bit complex; and it would be better to more often remind the authors of their meanings.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides an algorithm for sampling from a causal interventional distribution using conditional generative models, building on Shpitser and Pearl's ID algorithm. They discuss how their algorithm, ID-GEN, can sample from any identifiable distribution given a causal graph, and handles the presence of unobserved confounders (when identifiable). Empirically, they demonstrate their method can work for measurement, evaluation and interpretability purposes in the challenging setting where both the target and treatment are high dimensional e.g. images.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- interesting work, the case of high-dimensional variables in a causal graph is a super important and under-discussed one
- thorough theoretical treatment of the extension of the ID algorithm
- experiments show a nice range of usages of the suggested ID-GEN approach

Weaknesses:
- I frankly got a bit lost in a few key parts of Section 3. I got the main ideas (I think) but missed a bunch of nuance. Some spots were: Example 4.1 (I don’t understand why ID fails in this case but ID-GEN succeeds - specifically the importance of merging is a bit lost on me), Step 4 of ID-GEN (again, merging), and Step 7 of ID-GEN (I think the logic around how training data is sampled, used and modified wrt the graph needs to be explained more clearly)
- Step 1 of ID-GEN confuses me - I don’t see why we can’t just learn a model of P(y) directly in this case? Also the 2nd equality in 203 doesn’t make sense to me - how is the sum over values of v equal to P(y)?
- in each experimental section I find I have at least a medium-sized point of confusion around the setup or evaluation - more care should be taken to explain empirical setup + results overall
- In 5.1, the authors state that U_color makes W1 and Y correlated by color - however, X contains color information and is a direct ancestor of Y, so this unobserved confounding seems trivial
- in 5.1, it seems like a better metric than a classifier (which may be unreliable and as you note isn’t useful for all possible images) would be something based specifically on the RGB values of the pixels themselves
- in 5.2, I don’t quite see why Young & Male have unobserved confounding - are they not fully determined by the shared + observed parent I_1?
- in 5.3, I don’t understand why the report is a causal ancestor in the graph - isn’t it generated upon viewing the X-ray?
- in 5.3, I think the setup with the labeller can be made clearer - how good is this labeller? How is it structured? Additionally, is the bottom row intended to be a success or failure examples? (label says it should be right lung base but all inferences name the left lung)


Smaller points:
- L115: are unobserved confounders only allowed to affect 2 variables in this framework? Is that more limiting than general SCMs?

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
vx4NgdyyVG;"REVIEW 
Summary:
This paper presents a dynamic re-weighting method for imbalanced learning. The author defines the ratio of the balanced data set distribution to the training set distribution, and tries to estimate it with an iterative update method. The effectiveness of this method is proved by experiments.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	This paper points out a problem with distribution differences, which leads to the potential missing feature patterns in general re-weighting methods.
2.	This paper proposes a new method, which approximates the ratio of the balanced data set distribution to the training set distribution using methods of density ratio estimation. As far as I know, a dynamic re-weighting strategy is novel in this field.
3.	The experimental introduction of this paper is clear, and extensive experiments have been carried out, which validates the effectiveness of the proposed method.

Weaknesses:
1.	The formula derivation in Sec. 3.3 can be more detailed. It is suggested to explain how formula (7) is obtained in the appendix.
2.	The introduction may have overlooked some key articles. For example, the article mentions Wang et al. 's article at the end of Sec.3.3, but does not discuss this paper in the introduction section.
3.	Does the new method enjoy the same theoretical boundaries as the general reweighting method? It is recommended to provide more analysis.
4.	Besides, there are some typos in the details:
  - In the experimental section, 'class[390,385] 'may be a typo.
  - In table 3, the interpretation of Tr_{Few} is supposed to be there.

Limitations:
No. Although the authors say they clarify the limitations in Sec.3.4, I find they mainly highlight the efficiency of the proposed method. More discussion about limitations and potential negative societal impact is recommended.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel approach called Re-weighting with Density Ratio (RDR) to address the challenges posed by imbalanced data distributions in machine learning.  The RDR approach aims to mitigate overfitting on majority classes and enhance adaptability across diverse datasets by continuously updating the weights in response to observed shifts in class density.  Extensive experiments on various large-scale, long-tailed datasets demonstrate that the RDR method significantly improves the model's generalization capabilities, particularly under severely imbalanced conditions.  The analysis of the weight changes during training reveals that the method increasingly focuses on minority classes as training progresses, initially learning common features across all categories and then targeting learning towards minority samples to enhance generalizability.  The paper also provides an ablation study to further validate the effectiveness of the proposed approach.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper introduces a novel approach called Re-weighting with Density Ratio (RDR) to address the challenges posed by imbalanced data distributions in machine learning. 

2. Extensive experiments on various large-scale, long-tailed datasets demonstrate that the RDR method significantly improves the model's generalization capabilities, particularly under severely imbalanced conditions. 

3. The analysis of the weight changes during training reveals that the method increasingly focuses on minority classes as training progresses, initially learning common features across all categories and then targeting learning towards minority samples to enhance generalizability. 

4. The paper provides an ablation study to further validate the effectiveness of the proposed approach. 

5. The results show that RDR generally outperforms other methods, including Inverse Frequency (1/n) and SAM variants, in both the Many and Few classes, indicating that RDR can efficiently address the overfitting issues for Few classes.

Weaknesses:
- The paper does not provide a detailed theoretical analysis or justification for the proposed Re-weighting with Density Ratio (RDR) method, beyond the intuition that it can mitigate overfitting on majority classes and enhance adaptability across diverse datasets.

- I am interested in how RDR might perform in the presence of extreme imbalance, noisy data, or other challenging scenarios. The current experiment is well-established but dataset itself is relatively simple.

- The paper discusses reweighting/non-reweighting for classification problems. I suggest the authors also briefly discuss reweighting methods in imbalanced regression problems, e.g., VIR [1] for reweighting problems and ConR [2] for non-reweighting problems.

[1] Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing, NeurIPS 2023

[2] ConR: Contrastive Regularizer for Deep Imbalanced Regression, ICLR 2024

**Summary** I think the theoretical analysis or at least insights is needed for acceptance, so my suggest score is 5, as the experiment part is excellent in this paper.

Limitations:
authors discussed in sec 3.4

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a weighting strategy in order to handle class imbalance. Contrary to existing method, they propose to adapt the weight throughout the training procedure. 

Their method estimates the discrepancy between the sample distribution and the balanced sample distribution for parameterization w and updates the estimate through the training.

The authors use two resnet architectures to evaluate their contribution on multiple datasets. They also compare to other baselines and show significant gain.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
* The paper develops a novel approach for handling class imbalance.
* The methodology is derived theoretically from the problem formulation
* The authors propose an analysis of the complexity of the method and empirically evaluate the training time.
* The methodology is evaluated on multiple datasets and compared to multiple baselines.

Weaknesses:
* The paper is sometimes difficult to read:
  * Row 125, the authors refer to the distribution of training set, which get parameterized by w. Thus, my understanding is that the authors refer to the distribution of the training set ""captured by the model"". 
  * row 134, P_bal = pi P.. P_bal is the distribution of y in the balanced case ? But should therefore be 1/number classes... and P, should just be the class proportion and we should have P = pi P_bal ?
  * LDAM and LA terms are not defined at first. First definition of LA is at row 208
  * row 212 ""trategies"" => strategies

Limitations:
The paper would benefit from a clarification in notification (what is the true data distribution, what is the feature distribution, what is an estimate of what quantity, etc.). I believe the contribution is novel and worth it.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
vwgWbCxeAQ;"REVIEW 
Summary:
This paper investigates the two different misalignment issues between CLIP and downstream tasks, i.e., task misalignment and data misalignment.  The author designed several experiments that demonstrated that over-fitting occurs when tuning with the learnable prompt. They propose the Causality-Guided Semantic Decoupling and Classification(CDC) method to mitigate the impact of task-irrelevant generative factors on downstream tasks. The extended experiments demonstrate that the proposed CDC method is effective.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper investigates the difficulty of adapting CLIP to downstream tasks via two-level misalignment, which is a bright idea to help the community understand the working mechanism. Then the author provides a comprehensive experiment that reveals how the overfitting occurs and impacts the recognition of new classes. The author uses the perspective of causal inference to alleviate the data misalignment and proposes CDC with front-door adjustment for implementation, which predicts with explicit evidence.

Weaknesses:
1. There is no obvious evidence to show that the CDC will improve the prediction in certain cases, which category previous wrong and correct with CDC. It would be better if several compared failure cases to demonstrate that the CDC can solve the question at which level and which case still needs more advanced methods.
 2. It is interesting how the misalignment between CLIP and downstream pattern will change as the model’s capabilities increase, such as ViG-Large, and whether the more powerful model can solve the misalignment problem. Hence, it will be better if some comparisons of models can be added.
3. As we all know, MLLMs already sweep through the multimodal community, it will be better if expand some discussion about the misalignment in this paradigm and current method whether easily general to that.
4. How many parameters are tuned during downstream adapting? Compared to the pre-trained model, what is the ratio of tuned parameters?

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the two-level misalignment (task and data) issue in adapting CLIP to specific tasks. The authors develop a structural causal model to analyze CLIP's pre-training and adaptation processes, revealing how task-irrelevant knowledge interferes with predictions. To mitigate this, they propose Causality-Guided Semantic Decoupling and Classification (CDC), which implements front-door adjustment. CDC includes Visual-Language Dual Semantic Decoupling (VSD) to represent different semantics through multiple prompt templates, and Decoupled Semantic Trusted Classification (DSTC) to perform classification based on each decoupled semantic while estimating uncertainties. Experiments demonstrate CDC's effectiveness in enhancing CLIP's performance across various settings and tasks, addressing the challenge of data misalignment in vision-language model adaptation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* CDC is well motivated from a causal perspective and has significant technical novelty.
* Clear writing and well organized.
* Experiment results show the effectiveness of CDC.

Weaknesses:
* Figure 1(a) appears to illustrate task misalignment. Consider enhancing the caption of Figure 1 with more detailed explanations to clarify this concept.
* Regarding data misalignment, it would be beneficial to provide a more precise definition. Does it specifically refer to discrepancies in classes between training and testing processes? It's important to clarify that data misalignment encompasses both label misalignment and distribution misalignment. A brief explanation of each type would improve understanding.
* In Figure 3, the term ""fuse"" is used. It would be helpful to clarify the meaning and context of this term within the figure.
* How about accuracy if we directly use zero-shot test for CDC?

Limitations:
See Weaknesses

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the task and data misalignment issues in pre-trained vision-language models such as CLIP. It discovers that the task-irrelevant information significantly affects the prediction of CLIP and soft prompt tuning cannot mitigate the data misalignment issue. The authors propose a novel Causality-Guided Semantic Decoupling and Classification method to mitigate the interference of task-irrelevant information. The experimental results show that the proposed method effectively mitigates the data misalignment and improves the generalization of CLIP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-organized. The introduction of the method and the figures are clear and easy to understand. The description of the experiment setting is detailed, which makes the paper reproducible.
2. The proposed methods to mitigate the task and data misalignment of CLIP are highly-motivated and intuitive.
3. The authors design and conduct exhaustive experiments to demonstrate the effectiveness of the propose method. The proposed methods provide significant improvements on the generalization of CLIP.

Weaknesses:
1. In the experiments section, the method is currently adapted solely to the CLIP model. This limitation may not fully demonstrate the model's universality. The authors can adapt the method to various vision-language models with different architectures to showcase broader applicability.
2. The experiments are exclusively conducted on image classification tasks. The authors can explore adapting vision-language models (VLMs) to a wider range of tasks, such as object detection, image captioning, or visual question answering, to further validate the model's versatility and performance across diverse applications.

Limitations:
The authors have discussed the limitations in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vvpewjtnvm;"REVIEW 
Summary:
This paper proposes an efficient federated learning (FL) paradigm, where the local models in the clients are trained with low-precision operations and communicated with the server in low precision format, while only the model aggregation in the server is performed with high-precision computation. The performance is comparable to full-precision training, and sometimes even better since the over-fitting issue in local training is relieved.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1. The idea of applying SWALP's low-precision training within each cycle for local training in FL is meaningful and effective.
S2. There are theoretical analysis on convergence.
S3. Experiments are comprehensive and demonstrate the effectiveness of the proposed method.

Weaknesses:
W1. The biggest concern is regarding the novelty w.r.t. SWALP [40]. The entire Sec 3.2 and Sec 4.1 are almost the same as in [40]. The only difference seems to be Sec 4.2 but still very similar to the idea of SWA, just the setting of aggregation changes from by cycles to by clients, and a moving average of parameters is used.

W2. Writing needs improvement. For example, there is a typo ""sever"" in Line 108.
Eq (5) is different from its counterpart in [40] where the power was F-1 but now W-F-1. please explain the reason of difference.
Line 135, missing a space before ""i.e.""
Eq(7) uses E which is not clear until continuing reading to Line 161 and Algorithm 2 Lines 3-4.
Algorithm 2 Line 11, t' is only briefly mentioned in Line 161 without even referring to the used lines.

W3. It would be good to estimate the time reduction with the professional hardware (real acceleration).

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a federated learning approach that performs local training on low precision through quantization combined with a high-precision averaging and a moving average at the server. The paper guarantees convergence and empirically compares several levels of low-precision local training to full-precision training on 4 baseline FL methods. It remains unclear, though, what the contribution of the method is: the main focus seems to be on performance in terms of test accuracy, but the experiments do not show a significant improvement over existing methods. The method supposedly improves communication and computation efficiency but is not empirically compared to state-of-the-art methods, such as [1,2,3].

In their rebuttal, the authors provided novel results that address my concerns about missing baselines. While I remain concerned about the limited novelty and the presentation, I believe that the authors will be able to address these issues to some extent in the next version of the manuscript. Therefore, I have decided to increase my score.

\
[1] Liu, Shiyu, et al. ""Quantized SGD in Federated Learning: Communication, Optimization and Generalization."" International Conference on Neural Information Processing. Singapore: Springer Nature Singapore, 2023.

[2] Kamp, Michael, et al. ""Efficient decentralized deep learning by dynamic model averaging."" Machine Learning and Knowledge Discovery in Databases: ECML PKDD, 2018.

[3] Reisizadeh, Amirhossein, et al. ""Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization."" International conference on artificial intelligence and statistics. PMLR, 2020.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- convergence guarantees
- clear explanation of the method

Weaknesses:
- the presentation should be improved (e.g., several typos, such ass ""accept"" instead of ""aspect"", the term ""a avg"" for ""with moving average"" is unintuitive)
- the ablation study does not clearly show that low precision local training improves performance, since it is combined with a moving average that has a strong positive impact on performance.
- lack of comparison to baselines
- unclear use-case for the method
- the paper does not discuss existing federated learning with quantization literature in sufficient detail.
- for the non-iid experiments it would be interesting how quantization interacts with local BN layers in FedBN [4]

\
[4] Li, Xiaoxiao, et al. ""FedBN: Federated Learning on Non-IID Features via Local Batch Normalization."" International Conference on Learning Representations, 2021.

Limitations:
Section F in the appendix addresses the limitation of simulating quantization. It does not address issues like computation and communication complexity, the applicability of the approach, or limitations in the empirical evaluation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies an FL system with data heterogeneity, a topic has been extensively studied in the past few years. The idea is to perform local training with lower precision through applying block floating point quantization. The idea per se is not new, but proving that the convergence can be achieved using low precision local training is an interesting contribution.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well written and is easy to follow. The idea is also interesting but it is not necessarily new. The most important contribution is the theoretical proof for convergence. The evaluation results in Section 6 validate the theoretical results.

Weaknesses:
The paper mainly focuses on data heterogeneity in FL systems. What about resource heterogeneity? It would be important to have a discussion (or better some experimental results) on how the proposed solution perform in such setting. Should we use the same quantization level for all clients, or can we adjust the precision according to the resource availability? Also, the current state of the art of quantization in conjunction with Federated Learning is also missing, e.g.:

- FedQNN[1] uses quantized training in FL.

- CoCoFL[2] uses a combination of quantization and freezing for heterogeneous resources in FL.

How does these SOTA techniques perform compared with the proposed solutions?

[1] Y. Ji and L. Chen, ""FedQNN: A Computation–Communication-Efficient Federated Learning Framework for IoT With Low-Bitwidth Neural Network Quantization,"" in IEEE Internet of Things Journal.
  
[2] Kilian Pfeiffer, et al. ""CoCoFL: Communication-and Computation-Aware Federated Learning via Partial NN Freezing and Quantization."" Transactions on Machine Learning Research., 2023

Limitations:
Appendix F clarifies that the fake quantization is applied in the experiments. I really appreciate learning this information, as one of my question was how the quantization is implemented in PyTorch (as PyTorch only supports int8 inference out of the box). 
Regarding the broader impact, the discussion is about FL in general. The question is if performing lower precision training at clients improves or worsen these privacy and security aspects (e.g. if data poisoning cab be done easier, as the local updates are low precision).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes an efficient Federated Learning (FL) paradigm where local models are trained using low-precision operations and communicated with the central server in low precision format. The aggregation on the server, however, is performed with high-precision computation to ensure accuracy. The authors demonstrate that high-precision models can be recovered from low-precision local models with proper aggregation on the server side. This approach significantly reduces the computational load on client devices and the communication cost. The method is theoretically proven to converge to an optimal solution, even with non-IID data distributions, and extensive experiments show that models trained with low precision (as low as 8 bits) are comparable in performance to those trained with full precision.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method reduces the computational and communication overhead for client devices, which is crucial for resource-constrained environments for large models. The paper also provides theoretical guarantees for convergence to the optimal solution, even with non-IID data distributions.
2. The method is effective on the datasets and models in the experiments where low precision training has little to no impact on utility.

Weaknesses:
1. The integration of low precision training and high precision aggregation may add complexity to the implementation.The performance improvements are partly dependent on the hardware capabilities, such as the availability of processors supporting low precision operations.
2. The experiments are limited. Only image datasets are considered. Evaluation on other types of data, like text or tabular, can strengthen the results. 
3. No integration with differential privacy or other privacy protection mechanisms. Federated learning itself is not private and it would be interesting to see what privacy mechanisms are suitable for low precision model updates.

Limitations:
The authors have discussed the limitations in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vunJCq9PwU;"REVIEW 
Summary:
The authors propose the GREAT score that uses conditional generative models to mimic the data generating distribution. Thereafter, the classification margin on a set of generated images can be used to obtain a global robustness score. For this, the authors make the connection between local and global robustness explicit and show that the classification margin yields a lower bound on the robustness w.r.t. L2 perturbations (convolution with Gaussian). The authors empirically validate their GREAT score on various benchmarks and highlight the strong correlation to other common robustness evaluations while reducing the computational cost for the robustness evaluation significantly.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors propose an efficient procedure to rank the robustness of models using generative models
1. GREAT can be used with ""off-the-shelf"" generative models and does not require specialized training etc.
1. GREAT does not require access to the gradient
1. The paper is well-written and easy to follow

Weaknesses:
1. There are several assumptions on the generative model that are not sufficiently/prominently enough covered. The assumptions are: (a) the model generates an instance actually belonging to the conditioned class, (b) the true class is unambiguous (e.g., conversely, there might be cases where the ""Bayes optimal"" model cannot decide between two or more classes). (c) the generative model is a good approximation of the true data-generating distribution. The authors should highlight such limitations more and their implications for the guarantees/method.
1. Since the authors emphasize the guarantee on the average robustness of a model, the authors could elaborate more on the practical importance of such a guarantee
1. The derived Lipschitz constant might be a loose estimate since the Lipschitz constant also includes the generative model and not only the neural network. This is not accurately reflected in, e.g., Eq 10. Here it seems the model was convolved with the Gaussian ($g' * N(0,1))$), but it should actually be $((g' \circ G) * N(0,1))$.

Minor:
- The font size in figures and tables is very small

Limitations:
The authors sufficiently addressed the limitations (except for the weaknesses stated above).

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel framework called GREAT Score (Global Robustness Evaluation of Adversarial Perturbation using Generative Models), aimed at evaluating the global robustness of machine learning models against adversarial perturbations. Unlike traditional methods that aggregate local robustness results from a finite set of data samples, GREAT Score leverages generative models to approximate the underlying data distribution, providing a more comprehensive global robustness assessment.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The GREAT Score framework introduces a novel method for global robustness evaluation using generative models, which is a fresh and innovative approach in the field.
- The paper provides solid theoretical foundations with formal definitions, probabilistic guarantees, and detailed proofs, enhancing the credibility of the proposed method.
- The GREAT Score framework offers significant computational savings over traditional methods and can audit privacy-sensitive black-box models without accessing real data, highlighting its practical importance and broad applicability.

Weaknesses:
1. The GREAT Score in the paper primarily focuses on adversarial perturbations under the L2 norm. While this is a common setting in adversarial attack research, it lacks ablation studies for other norms, such as the L∞ norm
2. The GREAT Score framework relies on generative models (such as GANs or diffusion models) to approximate the true data distribution. If the quality of the generative model is not high, the generated samples may not accurately represent the true data distribution, thus affecting the accuracy of robustness evaluation. Besides, the evaluation results of the GREAT Score also depend on the generated sample set. If the sample set is biased or fails to comprehensively cover the diversity of the data distribution, the evaluation results may be inaccurate or unrepresentative.
3. The evaluation of online facial recognition APIs using GREAT Score is innovative, but the paper could provide more detailed analysis and discussion on the specific challenges and insights derived from this application. For instance, exploring the variability in robustness scores among different groups (e.g., age, eyeglasses) in greater depth and providing potential reasons for these variations would add depth to the analysis.
4. The calibration process described in Section 3.5 appears somewhat ad-hoc, relying on grid search for optimizing temperature parameters. This could be perceived as lacking robustness and generalizability. A more systematic approach to calibration, possibly incorporating advanced optimization techniques or sensitivity analysis, would strengthen the framework. Discussing the stability and consistency of the calibration process across different models and datasets would also be beneficial.
5. Despite claiming computational efficiency, the paper does not provide a detailed analysis of the scalability of the GREAT Score framework, especially in the context of extremely large datasets and models. A thorough examination of how the computation time scales with increasing data size and model complexity would add significant value. This could include empirical results demonstrating the method's performance on larger datasets or theoretical analysis of its computational complexity.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an innovative adversarial robustness measure, which leverages a generative model to produce data samples, record the marginal confidence score as a local statistic and average them over the data distribution. The proposed measure is designed to be efficient, scalable, and potentially applicable to unknown data distributions. Empirical validation is conducted using local models and commercial inference APIs, demonstrating the utility of the robustness evaluation.

The concept introduced in this study is commendable for its originality, and the metric indeed offers valuable insights into model robustness. Nonetheless, it requires substantial revisions to enhance its clarity, presentation, and justification of claims before it can be accepted.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
1. The metric introduced is a pioneering approach for assessing model robustness, characterized by its attack-independence, scalability, and potential applicability to unknown data distributions.
2. A theoretical analysis is provided, establishing that the metric serves as a lower bound for the probabilistic minimum adversarial perturbation.
3. The practicality of the proposed measure is supported by experimental validation on commercial black-box APIs.
4. There is a demonstrated strong correlation between the proposed metric and robust accuracy, suggesting the metric's effectiveness.

Weaknesses:
1. Presentation issues that may lead to confusion include:  
   (1) The second paragraph of introduction lacks a precise definition of adversarial robustness evaluation, which could be problematic for less experienced readers.   
   (2) Putting the testing algorithm in the appendix hurts the coherence of the paper. It would be better to include it in the main text.     
   (3) Figure 2 requires additional clarification to elucidate how robust accuracy (RA) and the proposed metric are integrated into the same plot. Current discussion is insufficient. 

2. The generative model's training requires at least partial knowledge of the data distribution. So the claim that the proposed metric can scale to unknown data distribution needs justification.

3. The metric's performance is contingent on the generative model's capacity to produce benign samples, yet no guarantee is provided that the generative model's ability to do so.

4. The endeavor to train a generative model to produce benign samples should be considered in making it a cost-effective and scalable solution. Maybe this metric can consider online learning to update the generative model. Hope a discussion can be provided on this.

5. The claim regarding the limitation to white-box settings (Page 2, Line 56) is inaccurate, as adversarial accuracy can also be assessed in black-box scenarios, evidenced by the effectiveness of the Square Attack method.

Limitations:
See weakness and questions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the important and under-explored problem of ""global robustness evaluation"" for neural networks. It proposes GREAT Score, a novel framework for assessing global robustness using generative models (GMs).  Besides, through Monte Carlo sampling from GMs and using Hoeffding's concentration bound, the algorithm can reach an epsilon probabilistic guarantee on the sample mean's closeness to the true mean. The paper then applies their proposed algorithm on various classifiers using GMs to measure global robustness scores.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The paper attempts to tackle a significant gap in global robustness assessment, offering a reasonable and innovative contribution to the field.
2) The paper is well-organized, clearly written, and easy to follow.
3) The experimental results show high consistency between GREAT Score and attack-based model rankings on RobustBench, demonstrating its potential as an efficient alternative to existing robustness benchmarks.

Weaknesses:
1) The reliance on GANs as a proxy for the true data distribution raises concerns about the method's accuracy. To the best of my knowledge, current GANs do not generate better coverage than the test set. GANs are a bad estimation of the underlying data distribution with known issues such as bias and model collapsing. Considering model collapse, the fixed test set is likely to have even better distribution coverage than the samples generated from GAN. It would be much more reliable and convincible by involving  the recent class generative models.
2) I also encourage the authors to include experiments with other local robustness estimators, further strengthening the submission.
How does the choice of generative model and local robustness estimator affect the reliability of the global measure computed by the paper?
3) Theoretically, while the authors provide a probabilistic guarantee on the obtained estimates derived from GMs and true estimate, there's a lack of theoretical bound on gap between the true estimate and models' global robustness arising from the distance of the  generative distribution and underlying data distribution. Otherwise, the significance and utility of the GREAT score is unclear for me, and this omission makes it unclear how the accuracy of the empirical distribution affects the overall error, beyond just sample complexity.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
i5PoejmWoC;"REVIEW 
Summary:
This work attempts to solve a filtered list of Sudoku puzzles by training a transformer model with data derived from solutions produced by a mechanistic 'simple solver' (rather than a sophisticated recursive planner).  They show that the training regime transformer model can be engineered to enable the model to learn to do the task effectively.  In addition, the authors perform linear probes to show the extent of the model's internal representation of the ongoing problem solution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This work tackles a problem that is readily understood by the public, using transformers which might be expected to struggle (compared to GOFAI solvers, which work on even harder Sudoku puzzles than tested here).

The number of experiments, looking at different aspects of the learnability of the task, and the probing of the resulting model internal states is very nicely done.

Weaknesses:
Selecting only those solvable by a simple solver is quite a simplification : Puzzles that require a full-backtracking approach are excluded.  This means that the results show that Transformers are capable of planning when the situation is simple, which is far from being fully capable of planning/reasoning.

Should link to dataset (https://www.kaggle.com/datasets/radcliffe/3-million-sudoku-puzzles-with-ratings).
According to the dataset description on Kaggle, 43% of the puzzles in the dataset have a difficulty of zero, meaning that it can be solved using a simple scanning technique.  The filtered dataset is 1.9M of the original 3.0MM (63%), so only 31% of the dataset being used is not amenable to the 'simple scanning technique'.  Perhaps this should be highlighted.

Should mention the 42M param GPT-2 architecture earlier in the paper than Appendix B.

Minor quibbles:

* L32: ""In this work, we aim to understand how complex a reasoning task can Transformers trained with next-token prediction solve by focusing on a synthetic task: Sudoku puzzles."" 
  + ->   ""In this work, we aim to understand how complex a reasoning task Transformers trained with next-token prediction can solve by focusing on a synthetic task: Sudoku puzzles.""

* L70: ""This ensures that all our puzzles are solvable in polynomial time.""  Should be 'reasonable time' - the polynomial time claim is beyond what's proven.

Limitations:
The filtering of the initial dataset should have been emphasised more : It may be that the model learns only puzzles that would be in an 'Easy Sudoku' collection.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper applies causal language models (Transformers) to solve Sudoku puzzles, reporting a 94.21% accuracy rate in solving the puzzles completely. The authors claim to demonstrate that these models can develop human-like reasoning capabilities by employing insights from CoT prompting through carefully structured training data and probing analyses.
The contribution of the paper is:
1. Demonstration that causal language models can learn complex reasoning tasks like Sudoku solving through carefully structured training data.
2. Development of a novel training methodology that leverages solver-decomposed reasoning order to enhance model performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Novel Methodology and Application: Applying Transformers to Sudoku Solving is an interesting extension of Transformers techniques, and the use of sequences mimicking human reasoning steps is a creative training method.

Weaknesses:
1: Limited novelty: While applying Transformers to Sudoku is new, the underlying techniques are not innovative. The paper lacks significant theoretical or methodological advancements.

2: Narrow scope: Focusing solely on Sudoku limits the paper's impact and generalizability. It's unclear how well this approach would generalize to other reasoning tasks or more complex Sudoku variants. Testing models across various types of reasoning tasks, especially those requiring different logical structures or knowledge types (rule-based vs. rule-less puzzles), could significantly enhance the understanding of the model's generalization capabilities. [1]

3: Inadequate comparisons: A major oversight is the lack of comparisons to traditional Sudoku-solving algorithms or other AI approaches.  It's necessary to compare neural approaches with traditional algorithms to assess advancements meaningfully [1] 

4: Overstated claims: The paper may overstate the model's ""reasoning"" capabilities. It's not yet convincing that what's described could be pattern matching rather than actual reasoning. Distinguishing between genuine reasoning and sophisticated pattern matching can be challenging. Further evidence could be demonstrated by testing the model's ability to solve puzzles it was not directly trained on, or by altering puzzle formats to see if the model can adapt its solving strategy without retraining.

5: Computational efficiency: There's insufficient discussion of the computational costs involved comparing the different order/decomposing as well as beam search appraoches.

6: Lack of error analysis: A detailed examination of where and why the model fails would provide more insight than focusing on its successes.

[1] Giadikiaroglou, P., Lymperaiou, M., Filandrianos, G., & Stamou, G. (2024). Puzzle Solving using Reasoning of Large Language Models: A Survey.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
* This work presents a study of solving sudoku puzzles via causal language modeling. 
* Given the sequence of filled places and their values in sudoku, the model must output the series of empty cell positions and the values that correspond to them.
* They study how the model performs with various input representations of a sudoku puzzle. (considering sudoku puzzle as a matrix)
    * Fixed cell order (from top-left to bottom-right)
    * Random cell order
    * Chain-of-thought prompting (using a solver to provide the method to solve the sudoku)
* Through experiments, the authors have demonstrated that appropriate training data that breaks down the problem into smaller components is essential for the model to solve the puzzle correctly. 
    * The model's performance improved with CoT prompting. It's even enhanced with the use of position hints and beam search.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The problem definition, model description, and experimental setup are presented clearly, making the paper accessible and informative.
* Introduced a sudoku puzzle dataset with steps to solve the puzzles (1.9M puzzles).
* Probing analysis for tracking the candidate set of cell,

Weaknesses:
1. No SoTA models are evaluated on the sudoku puzzle data.
2. A full ablation analysis is not included. This is to understand better how different settings (CoT, beam search, puzzle complexities) affected the model performance and where the model is struggling. Only improvements in accuracies are mentioned in the paper.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper assesses causal language models', particularly transformer decoders', abilities to solve Sudoku puzzles. The authors encode Sudoku puzzles into sequences, representing each cell as a (row, column, value) triple, and train a model from scratch on 1.8M puzzles. They then evaluate the trained model on a 0.1M holdout test set. Results indicate that when unfilled cells in the training data are arranged in an easy-to-hard order (based on a solver's results), the model can solve Sudoku puzzles with a 94.21% full-puzzle solving rate. The authors also use linear probing to show that the model's activations contain information about possible values for any given cell. The paper concludes that causal language models may exhibit complex reasoning capabilities when trained on data that informs appropriate reasoning steps, without requiring techniques like Chain-of-Thought or external reasoning engines.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The selected task and settings are suitable for studying the reasoning and planning capabilities of language models;
- The paper presents strong results that causal LMs can solve the Sudoku puzzle by training with appropriate data, without the need of techniques such as using CoT, search or external solvers.
- The results indicate that causal LMs may be able to perform search and planning internally, which seems novel and insightful for further research.
- The writing is easy to follow.

Weaknesses:
- The probing study's methodology is somewhat questionable. It merely compares the top-k predictions for each cell against the ground truth candidate set. This approach may not accurately be termed ""probing"" as it doesn't examine intermediate representations. Furthermore, this study might not conclusively demonstrate that the language model internally tracks candidate sets, given that the model is explicitly prompted to predict the cell. A more effective approach could involve probing potential values of one cell while prompting the model to predict another. Positive results from such a method could more convincingly show that the model can internally reason about other cells relevant to solving the current one.
- I’m not sure about what the takeaway of Sec. 3.5 is.
- It seems like the paper used the wrong template.

Limitations:
The authors have properly adequately limitations of their work in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
voJCpdlw53;"REVIEW 
Summary:
The paper presents UltraPixel, an innovative architecture for ultra-high-resolution image generation that tackles semantic planning, detail synthesis, and high resource demands. UltraPixel uses cascade diffusion models to generate images at multiple resolutions within a single model, efficiently guiding high-resolution generation with lower-resolution semantics. It features implicit neural representations for continuous upsampling and scale-aware normalization layers. Moreover, it requires less than a 3% increase for high-resolution outputs, boosting efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1、The paper demonstrates impressive results, with generated high-resolution images exhibiting remarkable detail. The proposed method outperforms existing approaches in terms of speed and flexibility, supporting arbitrary resolution image generation with a single model. This represents a significant advancement in the field.

2、The authors present a clear and well-motivated approach. They provide compelling evidence (Figures 2 and 6) to support their argument that the absence of low-resolution (LR) guidance can lead to suboptimal generation results.

Weaknesses:
1、 The manuscript's layout requires some refinement. For instance, Figure 4 extends beyond the page margins, and the text adjacent to Figure 9 appears overly condensed.

2、 Given that this is a text-to-image generation work, the paper would benefit from a more comprehensive set of visual results, including additional comparisons with state-of-the-art methods.

Limitations:
See Weaknesses.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces UltraPixel, a method for generating high-quality ultra-high-resolution images. It utilizes the semantics-rich representations of lower-resolution images in a later denoising stage to guide the overall generation of highly detailed high-resolution images. The method incorporates implicit neural representations for continuous up-sampling and scale-aware normalization layers that are adaptable to various resolutions. The experimental results show that it has excellent ability in generating high-resolution images of different sizes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The introduction of implicit neural representations for continuous up-sampling and scale-aware normalization layers adaptable to various resolutions is a creative solution that addresses a challenge in the scalability of image generation models.

2. The methodology is well-articulated, with a clear explanation of how the model manages to generate high-quality images while maintaining computational efficiency.

3. The ablation experiments are thoroughly conducted, systematically revealing the contribution of each component to the overall performance.

4. The paper proposes an innovative method for generating high-quality, ultra-high-resolution images efficiently, tackling a major challenge in image synthesis.

Weaknesses:
1. The explanation of the implicit neural representation (INR) requires further clarity regarding its ability to enable continuous upscaling. Moreover, an in-depth analysis and dedicated ablation study of the Scale-Aware Normalization (SAN) feature would provide insights into its role in resolution adaptability.

2. To underscore the advantages of the proposed framework, the experiments should be expanded to include comparative analyses with Latent Diffusion Model (LDM)-based and pixel-based image synthesis methods, showcasing the superior performance of the framework in high-resolution image generation tasks.

Limitations:
1. The paper may not sufficiently address how well the model generalizes to datasets beyond the training distribution. It is crucial to understand if the model's performance degrades with different or less common image content.

2. There is a need for more rigorous testing of the model's robustness to various corruptions and perturbations that could be encountered in real-world applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method for Ultra-High-Resolution image generation from text prompts. The method is based on StableCascade. The original StableCascade can generate 1024x1024 images. This paper proposes another HR latent diffusion model that can utilize the guidance from 1024 x 1024 images and generate 4096 x 4096 images. Unlike previous methods that directly use the low-resolution output, the method chooses to use the features of the base model as guidance and proposes an implicit-based method to upsample the low-res guidance features.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of guidance feature and implicit-based upsampling is simple but effective.
- The paper reads well, and the presentation is clear.
- The results are very impressive. 
- The proposed method only needs light-weight finetuning from StableCascade.

Weaknesses:
- More validation and analysis are needed. In the comparison, a traditional image upsampler is used, but the traditional image upsampler is often smaller and also trained on much smaller datasets. For a fair comparison, it will be good to compare with the state-of-the-art generative image upsampler such as StableSR and Stable Diffusion Upscaler.
- A comparison of this baseline is missing: instead of using guidance features, the HR latent model can directly use the LR images / latents from the base model.
- It would be good to have visual results of the ablation on LR guidance timesteps.
- Ablation on scale-aware normalization is missing.

Limitations:
It will be good to show some visual failure cases.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
vo5LONGAdo;"REVIEW 
Summary:
This paper proposes Remix-DiT, which creates multiple experts by mixing fewer basis diffusion transformers, allowing each expert to specialize in the denoising task for corresponding timestep intervals. It achieves performance improvements by having each expert responsible for a larger number of timestep intervals with fewer total trainable parameters than previous multi-expert methods. Also, the paper analyzes the coefficients of how much each expert uses bases, demonstrating the denoising task similarity for adjacent timesteps, as well as the use of specialized bases for lower timesteps.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper is structured well, making it easy to understand and follow.

* The proposed mixing basis strategy is interesting as it achieves better performance with fewer parameters compared to existing multi-expert methods.

* Ablation studies on mixing methods are comprehensive.

Weaknesses:
* **Lack of experiments.** The authors have to validate the performance of Remix-DiT by reporting comparisons with previous methodologies on the FFHQ or MS-COCO datasets. It would make the manuscript more solid if Remix-DiT achieves consistent performance improvements on multiple datasets.

* **Lack of comparison.** There are two methods, DTR [1] and Switch-DiT [2], to address the multi-task learning aspect of diffusion training by designing distinct denoising paths for 1000 timesteps in a single model. These are more parameter-efficient methods where they use no additional parameters or 10%, respectively. The authors should analyze them with respect to Remix-DiT.

[1] Park et al., Denoising Task Routing for Diffusion Models, ICLR 2024.

[2] Park et al., Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts, ECCV 2024.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces Remix-DiT, a modification to the diffusion transformer architecture that incorporates the multi-expert denoiser framework during both training and inference. Unlike traditional multi-expert methods that train $N$ separate individual experts independently for each time interval, Remix-DiT employs $K$ base models combined with $N$ mixing coefficients to dynamically compute time-specific experts. This approach enhances efficiency and leverages task similarities between adjacent intervals more effectively. Experiments on ImageNet demonstrate that Remix-DiT improves the performance of DiT across various model sizes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-motivated and represents a valuable step towards integrating the multi-expert denoising framework into standard diffusion models. 

- The main idea of the paper (using global mixers to compute the final experts) is novel and interesting to me in this context.

- The method is simple and effective, making it more suitable for practical use cases. 

- The experiments are well-designed, and the ablations clearly illustrate the impact of various aspects of Remix-DiT. 

- The paper is well-written and generally easy to understand.

Weaknesses:
- While the authors show the benefits of Remix-DiT on finetuning a pretrained DiT model, it would be interesting to see its effect when training all components from scratch. If the compute budget allows, I suggest that the authors also add this experiment for better insights into what happens if one uses the remixing scheme from the beginning of training (perhaps after a small warmup)

- The performance gain seems to diminish as the size of the base model increases. Hence, a more detailed discussion on this issue is needed for the final version. For example, the performance gain is almost 30% for DiT-S, while it drops to only 15% for DiT-L.


**Minor comments:**

Please fix the following issues in terms of writing in your draft:
- L114 ""refer to"" -> ""refers to""
- L144 -> citation is missing
- L215 -> I assume 100M steps should be 100K steps
- L290 -> it seems that it should be written as N experts because K is the number of base models
- L295 -> ""can found"" should be ""can find""

Please also cite GigaGAN [1] as the mixing part of the paper is related to their method of mixing different convolution kernels during training.

[1] Kang M, Zhu JY, Zhang R, Park J, Shechtman E, Paris S, Park T. Scaling up gans for text-to-image synthesis. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (pp. 10124-10134).

Limitations:
The authors have mentioned this in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes Remix-DiT, a model architecture designed to enhance the capacity of a standard DiT model without significantly increasing inference costs. This is accomplished by training mixing coefficients to adaptively fuse multiple DiT models and developing specialized experts for multi-expert denosing. A key advantage highlighted in this paper is that Remix-DiT achieves better generation quality while maintaining inference speed comparable to that of a standard DiT. Experimental results on ImageNet-256 demonstrate favorable outcomes compared to baseline methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.	The visualization results in Figure 4 are very interesting. It seems that the model has a certain preference in allocating the capacity of basis models, with clear segmentation across the timesteps. Additionally, a high coefficient is observed at early timesteps, such as 0-150. Does this imply that those steps are more challenging for the diffusion model to learn?
2.	The idea of mixing multiple basis models is clear and easy to implement. It does not requires the expensive training of independent experts for different steps.

Weaknesses:
1.	Using multiple base models may introduce more training costs. However, in Table 3, the GPU memory usage only slightly increases from 13G to 16G for DiT-B. Can the authors provide more details about the reason? Will Remix-DiT introduce a substantial backward and forward footprint?
2.	This method utilizes the pre-trained model as the initialization. This might make the mixed experts always the same after mixing since they are working on the same basis model initially. Will this be a problem?
3.	Why does the proposed method outperform naively training independent experts? In this method, the experts are crafted by mixing, which should theoretically be upper bounded by the naïve method mentioned above.

Limitations:
This paper discusses limitations such as sparse gradients and the training difficulty associated with a large number of experts.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To improve the generation quality of diffusion transformers, Remix-DiT proposes to enhance output quality at a lower cost and aims to create N diffusion experts for different denoising timesteps without the need for expensive training of N independent models. Remix-DiT achieves this by employing K basis models (where K < N) and using learnable mixing coefficients to adaptively craft expert models. This approach offers two main advantages: although the total model size increases, the model produced by the mixing operation shares the same architecture as a plain model, maintaining efficiency comparable to a standard diffusion transformer. Additionally, the learnable mixing adaptively allocates model capacity across timesteps, effectively improving generation quality. Experiments on the ImageNet dataset show that Remix-DiT achieves promising results compared to standard diffusion transformers and other multiple-expert methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty: Model mixers for efficient multi-expert diffusion model training is innovative and unique.

Significance: Addressing the challenge of efficient training of multi-expert diffusion transformers is significant in the field of diffusion models.

Methodology: The proposed algorithm is well-formulated and clearly explained.

Results: Experimental results demonstrate promising improvements over existing methods such as DiT.

Weaknesses:
1. Lack of Visualization Results: The paper does not include any visualization results. Providing visual examples of generated outputs is crucial for qualitatively evaluating the effectiveness of the proposed method.

2. Insufficient Motivation for Multi-Expert Training: The rationale behind adopting a multi-expert training approach is not fully well-motivated, particularly in the context of quantitative comparisons. A more detailed explanation of why multi-expert training is beneficial and how it compares quantitatively to other methods would strengthen the argument. Clarifying the advantages and potential trade-offs in performance and efficiency would provide a more compelling case for this approach.

3. High Training Cost: The training cost associated with the proposed method is substantial. It would be beneficial to provide a thorough analysis of the computational resources, time, and energy required for training compared to other existing methods. Discussing potential ways to mitigate these costs or offering insights into why the increased training cost is justified by the performance gains would add valuable context for evaluating the practicality of the method.

Limitations:
Please refer to the weakness and question part.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vjw4TIf8Bo;"REVIEW 
Summary:
This paper introduces a novel approach to reduce generation latency in Named Entity Recognition (NER) using Large Language Models (LLMs). The primary issue addressed is the high latency caused by the sequential decoding process in LLMs, which significantly lengthens the sequence by autoregressively generating all labels and mentions for NER. To tackle this, the authors propose Parallel Decoding in LLM for NER (PaDeLLM-NER), which integrates into existing generative model frameworks without requiring additional modules or architectural changes. PaDeLLM-NER enables simultaneous decoding of all mentions, effectively reducing generation latency. Experimental results show that PaDeLLM-NER can improve the inference speed than the traditional autoregressive approach.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The parallel decoding strategy is well-designed and experimental results prove the effectiveness.
- The authors provide comprehensive experiments with different setting and with furthre analysis.
- The paper is easy to follow.

Weaknesses:
- The proposed method cannot improve the inference speed in scenarios where only one type of entity is predicted.
- Since the method focuses on the inference efficiency of LLMs-based NER, it is better to report both inference speed and performance compared to zero-shot (Table 3) and supervised (Tables 4 and 5) methods. Notably, Table 3 only reports performance without considering the efficiency of different LLMs. Furthermore, why not report the performance of AutoReg_aug and AutoReg_struct in Table 3?
- For better understanding of the training resource usage when compared with other methods, it is better to report the base language models used (SOTA methods) in Tables 4 and 5.
- The writing of this paper could be further improved. For example, Line 219, “As per Ning et al...” appears to be a typo; the meanings of the underline (second performance) and bold (best performance) are not provided; and there is no explanation for why “*” indicates that results are not directly comparable in the Table 5 caption.
- Comparing with fixed few-shot in-context learning of LLMs may also be worth considering, as caching the fixed prompt could improve the inference speed of LLMs.

Limitations:
The authors provide one limitation section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
They create an NER system where an LLM first outputs the number of mentions there are of a given type (for all possible types). Then all mentions can be generated in parallel.

This results in faster inference times as each generation is short, and they can be done in parallel.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
They compare to several different baseline on multiple NER datasets in multiple different settings.

Their method is much faster than others.

Weaknesses:
Their reformulation of NER as predicting (label, mention) pairs removes a critical component of classical NER, the actual alignment of the mention to the tokens. Polysemous words are often mentions in some context and not in others and it if often important to know which one was the actual mention, especially if it is used for things like editing downstream.

The deduplication strategy is very aggressive and removes the possibility that some surface text is a label for multiple types in a single sentence. For example, ""It is England vs. Italy on this sunny day in England"", England is both a place (LOC) and a sports team (ORG) this would get filtered  by their setup.

The prose's definition of "" prediction quality [...] that is on par"" is rather loose, with their model being 6 points behind on average for zero-shot (table 3) and behind by a point of two on most supervised datasets.

Limitations:
yes

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents PaDeLLM-NER, a novel approach for accelerating Named Entity Recognition (NER) inference in Large Language Models (LLMs) through parallel decoding. A reformulation of the NER task that enables parallel generation of label-mention pairs, significantly reducing inference latency. A two-step inference process involving mention count prediction and parallel mention generation.
Extensive experiments demonstrated significant speedups (1.76x to 10.22x faster) compared to autoregressive approaches while maintaining or improving prediction quality across multiple datasets and two languages.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The parallel decoding strategy for NER is innovative and addresses a significant bottleneck in LLM inference speed, which is important in some speed-sensitive applications. The authors conduct extensive experiments across multiple datasets, languages, and settings (zero-shot and supervised), proving the method's effectiveness. The reported speedups are substantial and could have a meaningful, practical impact on NER applications. The method is compatible with existing LLM architectures and can be integrated with other acceleration techniques. The methodology is well-explained with helpful diagrams and examples.

Weaknesses:
Some details and corner cases are not well explained. For example, I didn't see the token location information in Figure 2. If the input has multiple and same mentions (e.g., ""Donald Trump owns the Trump Organization"" ), how does this framework distinguish with the same mentions? (e.g. Trump in the above example)

In addition, it is not clear how the de-duplicate model processes the partially duplicated mentions. For example, in the above case,  the ""Trump organization"" was recognized as ORG, and what if the person module predicted the ""Trump"" in the ""Trump organization"" as a person? Will the de-duplicate model filter this case?

Limitations:
No

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an interesting extension of the parallel text generation paradigm, where the authors tackle the NER task and propose to generate the labels independently. For each label prediction, the proposed method first predicts the number of mentions and then predicts the exact entity. The results show that the proposed model performs reasonably well, while achieving faster inference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed method is a pioneer work to accelerate LLM generation following the parallel generation paradigm. 
2. We do observe significant speed-up empirically, which suggests the proposed method may be of value in real word applications.

Weaknesses:
1. The importance of the two-step prediction for each entity is not justified. I feel there should be a baseline such that the multiple mentions can be predicted together in an autoregressive fashion. For example, I can predict ""entity type: LOC Italy English"" as a whole.
2. Fundamentally, parallel predictions should be weaker than autoregressive predictions due to the drop in dependency capturing. However, we observe from Table 4 that AR models are noticeably worse than the parallel approach. Since these results contradict common wisdom, there needs more effort to justify them. For example, the authors may need to reveal the full training/testing configurations of both the AR and parallel models, and there could be some more detailed error analysis to show how AR models are making more mistakes than the parallel approach.
3. The proposed approach may face difficulty when a word is used multiple times with different types. For example, in ""Washington lives in Washington,"" the proposed approach may predict ""LOC"" and ""PER"" for both ""Washington""; however, it can not align them because the parallel approach is ordered agnostic among entities.    
4. The proposed method needs finetuning to adjust the LLMs, which can be difficult when it comes to very large LLMs.

Limitations:
The authors make earnest efforts to discuss the limitations. In addition, the previously mentioned Weakness 3 is another potential limitation. The authors may include further discussion in this regard.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vjsd8Bcipv;"REVIEW 
Summary:
This paper proposes $\epsilon$-softmax to deal with label noise.  $\epsilon$-softmax modifies the outputs of the softmax layer to approximate one-hot vectors with a controllable error $\epsilon$. Both theoretical and empirical studies show the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The writing of this paper is good.
2. The robustness of the proposed loss is theoretically proved.

Weaknesses:
1. I think the motivation or the underlying reason for the effectiveness needs further explanation.
2. In experiment, the advantage of the proposed method over the competitors is probably not statistically significant.

Limitations:
I think this work will not have negative social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This submission proposes a enhanced softmax layer for label-noise learning, namely $\epsilon$-softmax. By incorporating with the well-known $\epsilon$-relaxation, the proposed $\epsilon$-softmax can regularize the outputs of the model and avoid fitting the label-noise sample. This simple and plug-and-play method theoretically bounds the output logits to be an approximated one-hot vector. Extensive experiments demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method is simple, plug-and-play, and effective. Unlike other label-noise robust losses, the proposed method not only works well by itself, but also can be integrated with other label-noise robust method such as DivideMix. To the best of my knowledge, this could one of the first works endow such property.
- The theoretical analysis is comprehensive and make sense. The theoretical results suggests the proposed method possesses the Top-K error consistency and label-noise robustness.
- The analysis between the most related previous works is in reason. The basic idea that balances the label-noise robustness and learning effectiveness has been researched for a long time, e.g., GCE. This submission clearly presents the connection between the proposed method and other symmetric losses.
- The empirical results is effective and enough. This submission presents the comparison results between many label-noise robust losses and the proposed method achieves the best performance in most cases. Additionally, this submission provides the experimental results that demonstrated the plug-and-play property of the proposed method on sample-selection based method and loss-correction based method.

Weaknesses:
- The ablation studies on the gradient clipping should be conducted and providing experimental results with different backbones would be better.
- It is exhaustive and labor-expensive to find the optimal $m$ for diverse datasets.

Limitations:
No obvious limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This manuscript proposes a novel method to approximate the symmetric condition of the loss function, which is necessary for robustness to label noise. Specifically, the proposed method, named \\( \\epsilon \\)-softmax, can adjust the model output to approximate one-hot vector. However, the proposed method alone suffers from underfitting, so the authors combined it with MAE to achieve better performance. The authors evaluated the proposed method on datasets with different noise types and rates.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This manuscript proposes a novel and simple method to approximate the symmetric condition of loss function.
2. The theoretical analysis focuses not only on robustness to label noise but also on the top-k consistency of the loss function.
3. The proposed method was evaluated on various noise types and rates, including class-dependent noise and real-world noise.
4. It good to see the authors compare their proposed method with temperature-dependent softmax combined with MAE. The experimental results demonstrate the superiority of their proposed method compared to temperature-dependent softmax.

Weaknesses:
1. The theoretical discussion with temperature-dependent softmax is missing. As the authors mentioned in L42 to L53, there are other output restriction-based methods in the literature, such as temperature-dependent softmax. Although the authors claim that these methods “lack predictability, fail to achieve a quantitative approximation to one-hot vectors, and exhibit limited effectiveness,” there is no detailed discussion on why the proposed \\( \\epsilon \\)-softmax has superior properties.
2. A direct comparison with sparse regularization [1] is missing. Sparse regularization utilizes temperature- dependent softmax, which this manuscript has already compared, to approximate one-hot vector output. However, sparse regularization also employs an additional regularization term, \\( \\ell_p \\)-norm \\( \\| p(\\cdot | x) \\|^p_p \\) to enhance performance, and this regularization term is equivalent to MAE only if \\( p = 1 \\). It’s necessary to highlight the advantages of the proposed method compared to this highly relevant approach.
3. There is no ablation study on \\( \\alpha \\) and \\( \\beta \\). As the authors mentioned in L218, \\( \\epsilon \\)-softmax alone suffers from a loss in fitting ability, and they combined it with MAE to balance the robustness and effective learning. However, without the relevant ablation study, it’s unclear how this “trade-off” is achieved.
4. The theoretical discussions and experiments regarding instance-dependent label noise are overlooked. In recent years, the instance-dependent label noise has attracted increasing attention [2,3,4]. Experimenting the proposed method on instance-dependent label noise can provide a better understanding of how the proposed method performs with different types of label noise. I encourage the authors to include related discussion in the revised manuscript.

[1] Learning with noisy labels via sparse regularization, ICCV, 2021.

[2] Part-dependent Label Noise: Towards Instance-dependent Label Noise, NeurIPS, 2020.

[3] Learning with Instance-Dependent Label Noise: A Sample Sieve Approach, ICLR, 2021.

[4] Instance-Dependent Label-Noise Learning With Manifold-Regularized Transition Matrix Estimation, CVPR, 2022.

Limitations:
The authors have acknowledged the limitations and societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces “ϵ-softmax,” a method to adjust softmax outputs for better approximation to one-hot vectors, thereby mitigating the impact of label noise in classification tasks. The approach modifies the softmax layer outputs to include a controllable error term ϵ, aiming to improve noise robustness without extensive alteration to the network architecture. The authors provide theoretical backing for the effectiveness of ϵ-softmax in achieving noise-tolerant learning across various loss functions. Extensive experiments with both synthetic and real-world noisy datasets are conducted to validate the claims.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.ϵ-softmax is presented as a plug-and-play module compatible with any existing classifier that uses a softmax layer, enhancing its practical utility.

2.The paper proves that ϵ-softmax can achieve a controlled approximation to one-hot vectors, which is significant for learning with noisy labels.

3.The methodology is backed by extensive experimental results showing its superiority over existing methods in handling label noise, with detailed results across multiple datasets and noise configurations.

Weaknesses:
This paper should pay attention to the axis labels of its figures. In Figure 1, the x-label is Epoch and y-label is Test Accuracy. In Figures 2 and 3, the axis labels are missing.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The author proposes the epsilon-softmax technique as a method to address label noise. Epsilon-softmax facilitates peaky predictions by increasing the value of the highest prediction, and it also functions to reduce the magnitude of the gradient when the prediction aligns with the given label. The author introduces the concept of All-k consistency to interpret this paradigm and presents experiments on prominent real-world benchmark datasets in the field of label noise learning, specifically WebVision and Clothing1M.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed epsilon softmax by the author takes a different approach compared to existing symmetric-like functions, which aim to reduce the gradient value for entirely incorrect predictions. From my understanding, the author’s approach reduces the gradient value for predictions that match the given label. By providing the value and interpretation of this novel approach, the author has significantly broadened the scope of the label noise learning field with their straightforward yet impactful idea. The proposed method has the advantage of simple gradient computation without requiring additional high-cost operations, making it applicapable to other Label Noise Learning (LNL) methods. The author demonstrates the experimental value of this approach by applying it to both the cross-entropy loss function and the focal loss function.

Weaknesses:
This section addresses two major concerns. For minor concerns, please refer to the ""Questions"" part.
1. The author mentions in line 40 the necessity for a method that can achieve both effective learning and robustness. While the proposed method offers a different perspective compared to symmetric-like loss methods, it is challenging to assert that it fully meets this necessity. Ironically, to balance the trade-off between effective learning and robustness, the author combines CE_(epsilon) loss and MAE loss. This ability to manage trade-offs is also found in other symmetric-like loss-based methods. In this context, I am interested in understanding why the proposed method might offer a better trade-off and whether it truly provides a better trade-off. I attempted to verify this through experimental comparison, but several issues arose: (1) There are no experiments that allow for a comparison of trade-offs. Experiments demonstrating the trade-off by varying alpha and beta are necessary. (2) The performance of existing methods is reported to be lower. For example, refer to the SCE paper.
2. The proposed method introduces two additional hyperparameters: m and alpha / beta. Unfortunately, based on the recorded experimental results, the proposed method appears to be sensitive to these hyperparameters. If this is not true, providing comprehensive experimental results that show the effects of varying these hyperparameters would enhance the perceived value of the proposed method. 
And if the proposed method is indeed sensitive to changes in hyperparameters, I would like to see evidence that it is not sensitive to hyperparameter variations within the in-distribution domain. I recommend performing validation and test processes to identify optimal hyperparameters (refer to the processes outlined in the GJS and JS papers).

Limitations:
-

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vjCFnYTg67;"REVIEW 
Summary:
The robustness of previous watermark algorithms would lead to a type of spoofing attack where attacker would modify the watermarked text to contain harmful contents while ensuring the watermark can still be detected. This paper introduce a bi-level signature scheme called bileve to mitigate spoofing attacks and enhance detectability. Bileve could recognize 5 scenarios during detection, compared to only 2 for previous methods. Using bileve, the LLM owners could verify if the source of the given texts. From experiments, the effectiveness of bileve against spoofing attack is validated.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The 3 types of spoofing attacks are clearly listed in this paper, with exploited vulnerabilities explained, which makes the motivation reasonable.
2. The paper provides comparsion between single-level signature and bileve, which is good for understanding.
3. Bileve could produce a total of 5 different detection results, which meets real-world cases.

Weaknesses:
1. Although with the proposed WRA, the text quality is improved as compared to SLS, the difference between bileve and unigram is still noticable.
2. For case 4&5, if the watermarked text is inserted into a long document (copy-paste attack), then the global alignment test would not produce a small p-value while the detected text does contain watermarked text.

Limitations:
The paper could provide more info on the complexity of the generation and detection process.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a novel approach to secure the provenance of texts generated by large language models (LLMs) through a bi-level signature scheme. This method aims to mitigate spoofing attacks—where malicious actors alter the content generated by LLMs to forge harmful content or misattribute blame—by integrating fine-grained signature bits for integrity checks and a coarse-grained signal for source tracing when signatures are invalidated.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper reveals a spoofing attack that takes advantage of the robustness features of state-of-the-art watermarking schemes.

2. This paper improves the ability to trace the provenance of text and regulate LLMs by differentiating between five detection scenarios. 

3. This paper introduces a novel bi-level signature scheme that enhances text provenance security for large language models (LLMs). It combines fine-grained signatures for integrity checks with coarse-grained signals for source tracing.

Weaknesses:
1.  While the experiments demonstrate effectiveness in specific settings with OPT-1.3B and LLaMA-7B models, the generalizability and scalability of the Bileve scheme to other models are somewhat uncertain. Authors could consider using larger or more powerful LLMs to demonstrate the effectiveness of the proposed algorithm.

2. The authors could consider using a more powerful LLM to measure the perplexity, like GPT-3/GPT-4.

3. I suggest reporting TPR scores at fixed low FPR (FPR = 10% or 1%).

4. This paper demonstrates detectability by modifying 10% of the tokens. It would be good to test with a higher rate of token modification, like 20%, 30%, to further validate the detectability.

Limitations:
Please see above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to consider spoofing attack, where an attacker wants to prove the proposition like ""The person holding this watermark private key used an LLM to write this text A."" where text A is constructed by the attacker. The paper proposes a defense against spoofing attacks.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
This paper points out the fundamental trade-off between defending against removal attacks and spoofing attacks.

Weaknesses:
I have doubt on the significance of spoofing attack. It is important to first clarify a potential misunderstanding. Authors may believe that a watermark in the text proves ""the person holding this watermark private key used an LLM to write this text A."" But that's not accurate.

However, the watermark only proves that the probability of text A being generated by a process independent of the watermark key holder is very low. It does not conclusively prove the key holder generated that specific text A.

Therefore, I believe the spoofing attack lacks real significance from the outset. If someone wants to prove they said certain things and nothing else, they can just use a traditional digital signature.

The problem is also framed as ""How to avoid an LLM being wrongly blamed?"" But what can we really blame an LLM for? Sure, there may be instances where a single LLM inference generates a token sequence that is interpreted as harmful by humans.

However, LLMs are probabilistic models that can potentially generate any harmful content given enough inferences. We can only blame an LLM for having a high average probability of generating harmful content, not for the existence of individual harmful inferences.

Moreover the paper appears hard to read to me. For example, ""instead of ranking them based on probability like conventional methods [13]"" doesn't specify what conventional methods mean in paper [13] Pre-trained language models for text generation: A survey.

Furthermore, t's unclear if the signature preservation attack requires constructing two messages with the same hash, as implied by ""replaced token hashes to the same signature bit."" If so, that would be extremely difficult for modern hash functions.

More importantly, the paper does not provide any rigorous theoretical guarantees that Bileve actually solves the spoofing attack issue as claimed. The key assertion is that ""it is less likely to simultaneously align well with $\Xi$ sequences, thereby effectively mitigating such attacks."" However, this statement is quite vague and unconvincing on its own.

What does it mean for a method to be ""less likely to simultaneously align well with $\Xi$ sequences""? How much less likely is it quantitatively? Under what assumptions or conditions does this property hold? The paper does not provide clear answers to these crucial questions.

Limitations:
This paper is difficult to read, e.g. the reference to ""conventional methods"" in ""[13]"" is unclear.

Does not provide clear theoretical guarantees that the Bileve method effectively mitigates spoofing attacks as claimed.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The submission proposes a spoofing attack on LLM watermarks and a new bi-level scheme meant to protect against spoofing by distinguishing five possible scenarios. The scheme is based on signature bits for integrity checks and rank-based sampling on top of a Kuditipudi-like random key sequence.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- The paper takes a somewhat original approach compared to most contemporary methods. 
- On a high-level, the problem of preventing spoofing is well-motivated and important for the community.

Weaknesses:
- Weak experimental results, bringing the practical value of the defense into question: 
  - The provided quality evaluation, despite its limitations (see below), clearly shows an order-of-magnitude increase in perplexity which strongly suggests that produced text are of impractically bad quality; there is no evaluation that would test this. This is the most important weakness in my opinion.
- Limited experimental evaluation, in ways that make it hard to evaluate the merit:
  - Text quality is measured only as PPL of Llama-13B and only on one small 1.3B model; there is no qualitative evaluation of text quality so the negative effect on text quality can't be well understood.
  - Only Unigram and SLS are considered as baselines, while self-hash and other variants of the KGW scheme are generally considered more promising, esp. from the perspective of spoofing.
  - Watermark removal is evaluated only as 10% editing attack which ruins text quality, no paraphrasing attack is evaluated.
- Bigger framing issues around Table 2 and the attack:
  - The framing of Table 2 seems inappropriate. ""Knowing the secret key"" is not a spoofing attack but simply an application of the watermark, this seems to be introduced as a way to suggest that symmetric schemes are flawed by design, which is not necessarily true in cases where there is no detector access. 
  - The attack is framed as a ""novel advanced spoofing attack"" while it is (1) in the opinion of this reviewer a direct result of scheme robustness and very limited in scope and thus hardly advanced (2) more importantly, already proposed in a different form in prior work [1] which was not cited, making this an overclaim. To elaborate on (1), for example, [7, 9] would be able to produce a detailed watermarked response to a harmful query such as ""Teach me how to steal someone's identity"" while there is no way to produce such a response by a few token modifications of a non-harmful response. 
   - This attack type is used as a key motivation, setting aside the true spoofing attacks from [7,9], which are much more relevant. This is evident in claims such as ""anti-spoofing requires perturbation-sensitivity"". Further, the robustness of Bileve to such approaches based on learnability is claimed but not substantiated.
- Poor writing: The paper is often quite hard to read and understand. On top of that there is a very large amount of typos. I advise the authors to work on improving the writing for the next version. Here is a list of some examples that I found, in hopes this helps. 
  - ""Symmetric characteristic"" and ""learnability"" in Introduction are unclear without being defined
  - Paper keywords typo: ""provence""
  - L50: unforgettable 
  - L285 L325 L50: tempering / temper-evident
  - Table 2: model' 
  - L87: simply 
  - Algo1: $h$ is undefined, although $H$ (a different symbol) is defined outside in the main text 
  - L211: ""associate""
  - L283: resulted 
  - L284: ""the source are""
  - L284: the failure verification
  - L308: ""tokens also""
  - L311: ""return"" 
  - L312: ""the rest segments""
  - L314: ""shows"" 
  - L315: ""t0""
  - L316: ""cause"" 
  - L327: ""limitaition"" 
  - L456: ""neucles""

[1] Attacking LLM Watermarks by Exploiting Their Strengths, Pang et al. arXiv 2402.16187

Limitations:
The authors include a discussion of limitations and societal impact in Section 6.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vjAORqq71s;"REVIEW 
Summary:
The paper presents an alternative backpropagation scheme for deep learning with algorithmic losses that combines a preconditioned step on the loss with a gradient step on a least square objective. Two preconditioning methods are investigated: using the Hessian, or the empirical Fisher. Experiments demonstrate that the proposed plugin consistently improve performance of algorithms across architectures and losses on two benchmarks. An ablation study of the potential additional tikhonov regularization is give, as well as a discussion of runtime comparisons.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The main strength of the paper is its experimental evaluation. Two relevant benchmarks are analyzed. 4 losses are considered in the frist benchmark, 3 in the second benchmark. Ablation studies and runtime comparisons provide a rather full picture on the algorithm.
- Overall the proposed method clearly provides gains across settings. Its theoretical motivation may be unclear but such experimental evidence invites for further research on the subject.

Weaknesses:
- The soundness of the approach from a theoretical viewpoint is lacking. However, it is probably better to have clear experimental evaluations than wobbly theoretical explanations. And theoretical explanations can be given later.

Limitations:
Limitations (or rather scope) of the approach are discussed in Appendix A.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes second-order optimization with splitting for hard objectives that arise as smoothing of such hard problems as sorting and ranking to address the problem of vanishing/exploding gradients.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
It is a well-written and very complete description of algorithms for reproducibility, which is a very good thing in itself.

Weaknesses:
1. Insufficient experiments. I'd appreciate adding a comparison here with the SFA technique from there, as it will rely only on first-order information: https://arxiv.org/pdf/2003.02122

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a new method to optimize complex possibly non-smooth and algorithmic losses for neural networks. The approach is based on splitting the problem into two-step procedure, where in the first step we construct and optimize the so-called Newton loss and the second step is based on SGD-type procedure for MSE loss with the first step. The authors present a wide experimental comparison of the proposed Fisher and Newton approaches with existing methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper has a strong literature review and motivation for solving different applications. The experimental section is well described, it contains 4 different non-trivial problems to solve. For the presented cases, the proposed methods outperform the baselines.

Weaknesses:
The paper does not contain any proofs or convergence guarantees. The mathematical formulation of the main problem is also quite confusing for me.  For example, is vector $x$ fixed for all steps or is it a batch of data?  Is it a sum-type problem or an expectation problem? What are the properties for $l(\cdot)$? Is it differentiable, smooth? Because some parts of the text said that the loss is non-smooth and later we calculate the Hessian of such a function.  In Formulas 1 and 2, it is not clear what are the fixed parameters or data. Should $\theta$ in 2a be $\theta_{t-1}$? Also, I think the mention of Lemma 2 in the main text could be very helpful.

For the experimental section, personally, it feels that the most of space is taken by the description of the problems and the setup and not the actual comparison. As the paper is mostly experimental and empirical, one would expect a better comparison of the proposed methods with the multiple benchmarks. There are no convergence figures with the per-iteration or per-gradient performance. As the authors claim, the main issues in the existing approaches are vanishing and exploding gradients. However, I didn’t find any clipping method for the comparison, which are the possible solutions for exploding gradients.

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
vieIamY2Gi;"REVIEW 
Summary:
The paper studies the problem of training diffusion models to sample from a target distribution. The contributions are summarized as follows: 

1. A codebase is provided for the study of diffusion-based samplers, due to the issue of inconsistent experimental settings in previous research; 

2. Exploration in the target space can be enhanced by GFlowNet-based off-policy training objectives and local search with the use of replay buffer. 

3. Experimental results validate the effectiveness of the proposed approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Sampling from a target distribution can be challenging in high-dimensional spaces, especially when the distribution of interest has many separated modes. This paper explores diffusion models to address this challenge. Unlike existing reverse KLD-based methods, such as PIS and DDS, this paper considers GFlowNet-based training objectives (e.g., trajectory balance, sub-trajectory balance), which enable off-policy training. This means that training trajectories are not necessarily from the current forward process, thus enhancing exploratory capability. Additionally, local search using a replay buffer can further enhance exploration in the target space. In general, the paper is well-written and well-organized.

---

**After rebuttal:** I will increase my score to 7. Typos or incorrect writing should be corrected upon acceptance.

---

Weaknesses:
Please see the below questions.

Limitations:
Yes. Limitations were included.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes an off-policy diffusion-based sampler training method to match a target distribution and a corresponding exploration strategy and credit assignment to improve it.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1.	The proposed idea of this paper is interesting, which connects the Euler-Maruyama sampler and GFlowNets.

Weaknesses:
1.	Although the authors mention that traditional MCMC have high cost in sampling, the proposed method based on neural sde seems to still have this problem. To the reviewer’s knowledge, the solving procedure of neural sde is time-consuming as well.
2.	The experimental target distribution also seems relatively simple. In the reviewer’s opinion, for GMM, we can first sample a mode according to the weights of different modes and then obtain a sample in this mode. Hence, it seems unnecessary to use complex model like diffusion.
3.	In many real-world applications like image generation, the pdf (may be unnormalized) of the target distribution is unavailable and we can only achieve data samples from the target distribution. Hence, the application scenarios of the proposed model are limited. 
4.	Besides, as mentioned in the conditional sampling case, the proposed method seems to need an extra trained vae to perform sampling. However, the vae can directly do the image generation. In that case, what is the real contribution of the proposed method?

Limitations:
See Weakness 3.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on the problem of sampling with distributions defined by a black-box and unnormalized energy function. This work provides a comprehensive review of existing works, including both variational methods and policy-based methods, and offers a codebase and benchmark to replicate and evaluate the existing works. Additionally, this work proposes a method to improve existing policy-based methods via local search and a replay buffer.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The studied problem of sampling from a distribution is an important issue with a long history in statistical inference. The paper provides a good review of recent works on this topic by leveraging diffusion models. The codebase that unifies existing methods is certainly useful to the community for continuing research on this topic.

2. The experiments are comprehensive in baselines, including not just diffusion-based methods but also classical MCMC algorithms. The results clearly show the advantages of diffusion-based methods and the techniques proposed in this work.

Weaknesses:
1. It appears to me that this work only tests the algorithm on relatively simple and manually-constructed scenarios. Are there any real and important applications within the field? I am not very familiar with this field, but I think that only conducting experiments on synthetic datasets makes this topic less practical. I believe the main advantage of the diffusion-based method over classical methods is in modeling complex distributions, making experiments on synthetic examples less meaningful.

2. Additionally, the tested scenarios are all low-dimensional cases. I wonder how this algorithm performs on high-dimensional cases, such as when the energy function is learned through neural networks. For example, is it possible to apply this algorithm to image generation where the energy function is represented by an image classifier? Testing the algorithm on high-dimensional tasks like these would provide a better understanding of its scalability and practicality in more complex and realistic settings.

Limitations:
See my comments on the weakness above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a variety of improvements to off-policy strategies for training diffusion models to sample from unnormalized densities. Equation 13. These include maintaining a replay buffer (obtained with Langevin sampling) to enable efficient off-policy exploration and incorporating an inductive bias into the neural network which estimates the SDE drift term. They also present a software library containing unified implementation of these techniques and e.g. diffusion model training.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Enabling diffusion models to be efficiently applied to sampling from unnormalized probability distributions is a problem with high potential for impact
- Thorough experimental analysis and comparison of different alterations to the training procedure. On most problems considered, the authors' contributions are necessary to achieve good results with trajectory balance. 
- The contribution of a software library could be valuable to the community.

Weaknesses:
- The results are not overwhelming - although the proposed contributions are helpful compared to a basic version of TB, there is only one modeling task in Tables 1-2 (25GMM) where they provide a statistically significant improvement over the baselines.
- The experiments are on synthetic energy functions and MNIST VAE. Including more real-world data or models would be informative.

Limitations:
Adequately addressed

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
vh9yEPLeyD;"REVIEW 
Summary:
This study introduces a novel training strategy for Deepfake detection using real, blendfake, and deepfake datasets. By designing an oriented progressive regularizer and a feature bridging module, the proposed approach effectively extracts forgery information from the training data, resulting in enhanced generalizability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method categorizes forgery faces into several types: SBI, CBI, and Deepfake faces, each containing distinct forgery artifacts, such as blending clues, identity inconsistencies, and generative fingerprints. The fine-grained learning scheme encourages the model to learn representative features from the training data, thus achieving robust and general face forgery detection.

Weaknesses:
1. The method employs a progressive transition from real to blendfake to deepfake samples. However, the necessity of continuity in these features remains unclear. The transition from real to fake faces, as depicted in Fig. 2, appears conceptually weird. The rationale behind the feature bridging and transition design is not well-explained. The progressive transition between adjacent anchors seems unusual, and the reasoning for a continuous rather than discrete transition is not justified.
2. Despite the generative artifacts present in deepfake data, it remains ambiguous why directly incorporating blendfake and deepfake data during training degrades performance. The authors suggest that direct VHT may fail to disentangle the learned representation in the latent space, but no experiments support this claim.
3. Fig. 1(b) does not appear to be an experimental result, which is crucial for validating the work's motivation.
4. In Line 44-45, the authors raised a question “Can the blendfake face entirely substitute the actual AI-generated deepfake face in training deepfake detectors?” However, this question has already been addressed by Face X-ray and SBI, which successfully use blendfake data to train general models.
5. The term A^T in Eq. (6) is not explained.
6. It is unclear why features augmented with noise should be mapped to a more fake distribution.
7. More Deepfake datasets, such as WildDeepfake and DeepForensics-1.0, should be included in cross-dataset evaluations.
8. For robustness evaluations in Table 6, the method should be compared with recent state-of-the-art deepfake detection methods, and more severity levels for each perturbation type should be included to mimic complex real-world scenarios.

Limitations:
N.A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors introduced a method aimed at detecting deepfakes. Their approach, known as Oriented Progressive Regularizor (OPR), employs a progressive transition strategy. This strategy is designed to enable the model to effectively train on a combination of blendfake and deepfake data, ultimately leading to improved performance. The experimental results indicated that this method surpasses current state-of-the-art (SOTA) approaches when tested on deepfake datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper provides a fresh perspective on the well-known problem of deepfake detection, which should be appreciated.

The paper is mostly well-written. The arguments and results presented are easy to understand.

The authors performed an extensive evaluation.

Weaknesses:
Argument on Blendfake: The argument that blendfake data alone is sufficient for training deepfake detectors is based on empirical observations on certain datasets or benchmarks with some particular deepfake detection models and may not hold universally. I would suggest toning down that claim or providing the exact conditions when this argument holds.

CDFv1 vs CDFv2: I believe that using CDFv1 for evaluation may not be necessary. It would have been more beneficial to utilize a different deepfake benchmark dataset such as FakeAVCele, DFD from Google/Jigsaw, or RWDF-23 (please refer to this repository for additional information https://github.com/Daisy-Zhang/Awesome-Deepfakes-Detection). The same applies to DFDC and DFDCP. I advocate for incorporating more diversity in the selection of benchmark datasets. In essence, the authors compared against three datasets instead of five, which is still an acceptable number.

Datasets: The authors utilized widely known deepfake datasets from 2019 in their research. However, considering the rapid advancements in deepfake technology since then, I believe these datasets may no longer accurately represent the current landscape. It would be valuable for the authors to include an assessment of their method using real-world deepfake videos sourced from social media and other online platforms. By doing so, they can demonstrate the effectiveness of their proposed solution in addressing contemporary and future iterations of deepfakes.

Progressive transition: Currently, the Progressive transition goes like this ""Real --> Blendfake (SBI) --> Blendfake (CBI) --> Deepfake"". I could imagine it being further extended to have addition of compression or adversarial artefacts (i.e., ""Real --> Blendfake (SBI) --> Blendfake (CBI) --> Deepfake--> compression and other artefacts""). That way one could really see a generalisable pipeline that could incorporate the variance in the types of deepfakes available on social media and will greatly increase the quality of the work.

Limitations:
See the above comments.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper investigates the generalization ability of deepfake detectors and proposes a novel training approach using ""blendfake"" data to enhance the model's learning of generic forgery artifacts. The authors point out that existing state-of-the-art methods do not incorporate deepfake data in their training process, which contradicts previous empirical observations. The paper introduces an ""Oriented Progressive Regularizor"" (OPR) to establish constraints on anchor distribution and proposes feature bridging to facilitate smooth transitions. Experimental results indicate that the proposed method effectively utilizes forgery information from both blendfake and deepfake.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Proposes a new training method that may enhance the generalization capability of deepfake detectors.
- Introduces OPR and feature bridging techniques to improve the model's recognition of forgery features.

Weaknesses:
- The attribution of the unorganized latent-space distribution lacks comprehensive experiments.
- There are some minor writing issues, such as the consistency of using SOTA and SoTA.

Limitations:
The authors believe that the reason VHT performs worse than blendfake-only is due to the unorganized latent-space distribution. Although the results indicate that the proposed method is effective, there is a lack of detailed experimental validation for attribution.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores the utilization of blendfake and pseudo-fake data in training deepfake detectors. It argues that the significance of deepfake samples has been underestimated due to insufficient exploration. To better exploit both pseudo-fake and deepfake data, the paper introduces a progressive transition from ""real to blendfake to deepfake"" and proposes a hybrid training scheme. This scheme includes an oriented progressive regularizer (OPR) to model the transition and a feature bridging strategy to simulate a continuous transition.The paper explores the utilization of blendfake and pseudo-fake data in training deepfake detectors. It argues that the significance of deepfake samples has been underestimated due to insufficient exploration. To better exploit both pseudo-fake and deepfake data, the paper introduces a progressive transition from ""real to blendfake to deepfake"" and proposes a hybrid training scheme. This scheme includes an oriented progressive regularizer (OPR) to model the transition and a feature bridging strategy to simulate a continuous transition.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1.The paper is well-motivated, and the proposed solution is both intuitive and effective.
2.The experiments robustly demonstrate the rationality and effectiveness of the proposed design.

Weaknesses:
1. Choice of Blend Algorithms: The paper does not provide sufficient explanation or discussion on the choice of blendfake image algorithms (SBI and CBI). As mentioned in Section 2.2, there are many other methods for crafting blendfake images. Would these methods be effective as well?
2. Interpolation Strategy: In Section 3.2, the paper introduces an interpolation strategy to achieve a smoother transition from real to deepfake. Why was interpolation performed at the feature level, and would setting multiple mixing parameters (alpha) for more interpolations further improve performance?
3. Possible Typos: There might be a typo on line 149, $M_a$.

Limitations:
See in weakness

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vcGEV6m5m2;"REVIEW 
Summary:
The authors propose to combine a reposable 4D reconstruction from multi-view video based on a skeletal LBS model with 3D Gaussian splatting. To this goal they introduce a novel strategy for estimation of the skeletal model from a superpoint clustering. The results demonstrate a superior image quality and, thanks to the representation, also fast rendering.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
S1) Implementation details provided for reproducibility.

S2) The claims are validated on common datasets.

S3) The result quality is visibly better than the prior work.

S4) The skeleton construction is novel, technically sound and produces reposable models in variety of scenes.

S5) The quality of exposition is good.

Weaknesses:
W1) The skeletons are over-segmented and unnatural and likely would not be very friendly for a human animator. They may still be suitable for data fitting but do not provide nearly as much regularization as an ""optimal"" (ground truth) skeleton would.

W2) The limitations and broader impacts are only discussed in the Appendix which I do not see as a responsible practice. It suggests that the authors do not give downsides of the method the same importance as to the upsides.

W3) The authors claim to report Statistical Significance without further comments (checklist item #7), but I cannot see any such features in the paper.

W4) It may be a good idea to consider a higher quality captured dataset than ZJU-Mocap. It does not seem to allow for a useful comparison between the methods.


Other minor issues and suggestions:

- Figure 1: Each superpoints -> superpoint

- L145: related rotation matrix -> relative?

- $\mathbf{W}$ is overloaded in Eq. 1 and Eq. 6 for two distinct things which is not ideal.

- Eq. 13: $\mathcal{L}$ without suffix is not defined.


------------------------
**Justification of recommendation**
A solid paper with its incremental but non-trivial contribution stemming mainly from the novel skeleton construction. The experimental results are convincing and the main downside is the clutter and complexity of the recovered skeleton. Despite this, I am currently comfortable recommending acceptance under the assumption that the exposition issues are addressed (especially limitations). My final decision might change based on the rebuttal.

Limitations:
The limitations and broader impacts are only discussed in the Appendix which I do not see as a responsible practice. It suggests that the authors do not give downsides of the method the same importance as to the upsides.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel approach for learning articulated objects, alongside the skeletons/kinematic trees directly from input videos, eliminating the need for pre-defined meshes or hand-crafted skeleton priors.

Specifically, the paper introduces a hierarchical 3D Gaussian representation, where a set of superpoints are used as guidance for deforming the sub-points using linear blend skinning (LBS). The skeletal structure is further derived from the superpoints, providing articulated structures that enable explicit pose control. Rendering is done by Gaussian Splatting, enabling real-time performance.

Overall, the paper tackles a very challenging and useful problem for 3D modeling, the manuscript is easy to follow, and the approach is interesting.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The strengths of this paper lie in how it brilliantly leverages 3D Gaussians for capturing the underlying articulated structures in a video, without the need for 3D annotations or pre-defined structure priors.

 The design of superpoints naturally models prominent candidates that can serve as control points. Furthermore, as the control points are “learned” automatically, it can potentially arrive at a representation that better suits the possible motion of the articulated subject.

Overall, the approach presented in the paper is pretty neat, and the experiments show pretty promising qualitative and quantitative results.

Weaknesses:
The approach does have some room for improvement. Specifically,
- Limited reposability. As mentioned in L372-373, the approach is limited to the motion space in the input video. It would be great if the papers could include visual results for these failure cases. It will be interesting to see how good the learned LBS weights are.
- Evaluated on datasets with limited motions: the videos used in the paper mostly contain repetitive motion sequences, and/or with small motions. It will be interesting to see how the proposed method performs on videos with complex/diverse/large motions (e.g., AIST datasets). Also, it is similarly unclear how the method can perform on in-the-wild videos with uncontrolled lighting, or with only a single view. 

Overall, these weaknesses are very common among template-free approaches, not specifically to the proposed method itself. Nevertheless, it would be great if the paper could include more figures, visual results, and analysis regarding these cases.

There are also some issues regarding the experiments, which I detailed in the Questions section below.

Limitations:
The paper discussed some of their limitations, but it would be great if the paper could include more analysis/visual results for the issues mentioned above in the Weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a method combining 3D Gaussian Splatting and superpoints for dynamic object modeling, achieving real-time rendering and high visual fidelity. Empirical results show that the proposed method achieves state-of-the-art results on several benchmarks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and easy to follow. The main contribution and methodology are well illustrated.
2. The use of an adaptive control strategy to manage superpoints is innovative and helps in optimizing the model, avoiding redundancy, and maintaining efficiency.

Weaknesses:
1. Although this paper achieves real-time rendering compared to AP-NERF, I find it somewhat incremental and lacking in innovation since most parts of the method are existing concepts.
2. This paper emphasizes the concept of ""Reposable,"" but the related experiments are very limited. A thorough analysis of this aspect could effectively distinguish this paper from AP-NERF.
3. This method compares fewer baselines, and the quantitative results do not show significant improvements in rendering effects and speed compared to the baselines, as shown in Table 3, Table 4, and Table 5.

Limitations:
Please refer to the Weaknesses and Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel approach for reconstructing reposable dynamic 3D objects from RGB videos using Gaussian Splatting, without requiring any template as input.

To achieve this, the paper suggests grouping Gaussians around superpoints, which are intended to represent rigid parts of the scene. By optimizing and analyzing these superpoints, a full skeleton model of an articulated object in the input video can be built, refined, and used for reposing purposes.

**Details**

The approach consists of two main stages:

1. *Dynamic Stage*. After optimizing a canonical 3D Gaussian Splatting (3DGS) representation for a few iterations, a set of superpoints is initialized in the scene. A deformable field mapping each superpoint to a time-variant 6DoF transformation is optimized. These transformations are used to derive the motion of Gaussians by interpolating transformations with neighboring superpoints through linear blend skinning (LBS). The paper also proposes a gradient-based strategy to control (prune, merge, or densify) the number of superpoints in the scene. Toward the end of the dynamic stage, a skeleton structure with joints is enforced and discovered in the scene by analyzing the distance between and configuration of superpoints.

2. *Kinematic Stage*. After discovering the skeleton model of the scene, the number of Gaussians and superpoints is fixed and optimized along with a new MLP mapping skeleton joints to time-variant rotation matrices. These matrices are used to compute the motion of each Gaussian along the kinematic chains using LBS. After full optimization, the skeleton can be used for reposing and editing the reconstructed object.

The paper presents extensive experiments and demonstrates higher rendering performance and speed compared to concurrent reposable dynamic Radiance Field methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper is well-written and easy to follow.

2. The task addressed by the paper is challenging but crucial for many applications in both graphics and robotics. I appreciate the proposed strategy, which successfully retrieves kinematic chains from RGB videos.

3. The quantitative evaluation presented in the paper is convincing and clearly demonstrates the superiority of the approach over concurrent methods.

Weaknesses:
1. The paper may lack sufficient skeleton examples to effectively demonstrate that the proposed approach can recover meaningful structures from RGB videos. Indeed, the primary goal is to recover skeleton structures and enable reposing capabilities, but only a single skeleton example is provided (Figure 5). Including more qualitative examples would likely make the paper more convincing.

2. The paper does not provide details on the optimization time and required resources (e.g., VRAM) for the proposed approach. It appears that a large number of training iterations is needed; a comparison with previous state-of-the-art models would be valuable.

3. The limitations of the approach are interesting but are only discussed in the supplementary material, which is problematic in my opinion. These limitations are crucial for further research and should be included in the main text.

Limitations:
As I already mentioned it, the limitations are only discussed in the supplementary material, which is problematic in my opinion. I encourage the authors to try to move the limitations to the main paper in the final version.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
GqrWhROxrG;"REVIEW 
Summary:
This paper mainly focuses on the problem of 3D object detection form multi-view images. It introduces MVSNet-like method for depth prediction, brings out probabilistic sampling, soft weighting and pixel-aligned Gaussian Splatting to improve the correctness, robustness of depth prediction, especially with sparse images. Therefore, it improves the performance for 2D features to be projected to the 3D space, therefore improves the performance of 3D object detection.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
A substantive assessment of the strengths of the paper, touching on each of the following dimensions: originality, quality, clarity, and significance. We encourage reviewers to be broad in their definitions of originality and significance. For example, originality may arise from a new definition or problem formulation, creative combinations of existing ideas, application to a new domain, or removing limitations from prior results. You can incorporate Markdown and Latex into your review. See https://openreview.net/faq.

This paper is clear-written. It introduces probabilistic sampling, soft weighting and pixel-aligned Gaussian Splatting to improve the performance of MVSNet-based depth prediction to improve the performance of 3D object detection without the need of ground truth geometry.

Weaknesses:
1. The 2D feature extractor, the detection head, and the detection head of this work is not clearly described.
2. The MVSNet-like depth prediction with probabilistic sampling and soft weighting, as well as the pixel-aligned Gaussian Splatting are all common methods or from previous works, I consider the novelty to be limited.
3. Also, I do not agree with the opinion of the article that ground truth geometry for supervision on the training stage is difficult to obtain. As far as I know, To obtain supervision for 3D object detection (AABB or OBB bounding boxes), you need to use a lidar or a RGBD camera to obtain the geometry of a 3D scene, and then label the bounding boxes.

Limitations:
Yes, the authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript proposes MVSDet, a multiview 3d object detection model that is evaluated on indoor scene datasets. Multiview information is lifted to 3D via an efficient per-frame depth sampling scheme. The most probable top-k depth values per pixel are used to lift 2D features into a global feature volume in a weighted way. Based on the thus accumulated 3d feature volume a 3d network regresses 3d bounding box parameters. In order to regularize the depth regression which is essential for constructing a occlusion-aware 3d feature volume, the manuscript proposes leveraging pixel-aligned Gaussian splats to construct a rendering loss against nearby views. Both the probabilistic depth sampling and the rendering loss are shown to contribute significantly to the performance of the model. Overall the model outperforms other models that do not use GT depth supervision during training.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The probabilistic depth estimation shows strong performance improvement over the dense planesweep depth approach when considering memory. Tab 3 is very effective in convincing me of the need for the probabilistic depth sampling. I do wonder how the numbers change without the regressed depth offset correction?

Showing that Gaussian Splat-based rendering loss supports the performance of object detection model is useful since not all 3d object detection datasets do have surface GT for training. Tab 4 shows a small improvement for the more strict mAP threshold.

These two contributions are strongly supported by the ablation studies in the experiment section and useful to be shared with the community.

Weaknesses:
I do wonder how much the difference is to supervising with GT depth instead of gaussian splats. (The experiment in Fig 6 is similar but not quite the same since placing features at GT depth locations does not need the probabilistic depth model to regress the depth). This last experiment might drive home the effectiveness of Gaussian splats and allow direct comparison to the ImGeoNet and CN-RMA related works in Tab 1 and 2. 

The writing and illustrations are mostly clear and support the understanding of the manuscript. There are quite a few open questions (see questions) that should be addressed to improve the presentation of the method.

Limitations:
The limitations are addressed adequately in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents a method for multi-view 3d object detection. The method computed a MVS cost volume using a few planes, it then samples k likely depth values per pixel and builds a 3d feature volume based on the voxels close to the samples depth values weighted by their confidence. Additionally, during training pixel aligned gaussian splatting (GS) is used to provide an additional rendering loss to guide the depth estimation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed 3d feature volume construction seems to outperform the existing baselines.
The use of GS during training seems to also slightly improve the 3d object detection.

Weaknesses:
It is unclear how the nearby views are selected. Is it the same as in the existing works?
To evaluate the contribution of the probabilistic sampling alone it would be good to add a line in table 3 without probabilistic sampling and soft weighting.
What is the benefit of using the PAGS rendering loss over the classical MVS photometric losses?

Limitations:
Limitations were mentioned.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
vYUx8j5KK2;"REVIEW 
Summary:
The authors propose Cufit, a curriculum fine-tuning method for improving the performance of medical image classification under the noisy labels setting. The method shows strong performance against other baselines on several medical datasets. The authors have also provided results on a non-medical dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Strengths**

1. The proposed strategy Cufit outperforms other baselines included in the evaluation
2. The strategy is agnostic to models (CNNs and ViTs) and images (medical and natural)
3. Cufit does not require knowledge about certain hyperparameters which are required in other methods.
4. The authors have presented results on non-medical images as well.

Weaknesses:
**Weaknesses**

1. The paper is very poorly written with spelling and grammatical errors throughout the text. Certain elements in the text are written in a convoluted way that confuses the reader.
2. The entire paper is about the proposed method “Cufit” but no section in the paper describes the algorithm in detail. Section 6.1 (“How does Cufit work?”) does not talk about the method but only about the results. 
3. It is important to cite previous work on PEFT in medical image analysis such as [1] in the text.
4. The paper lacks an explanation of the baseline methods used in the evaluation. Methods like CoDis are briefly mentioned in the Related Work section but have not been defined anywhere else. People unfamiliar would not be able to understand Cufit and how it differs from previously proposed methods.
5. The experiments should include the CheXPert dataset. Apart from being frequently adopted for medical image analysis problems, it would also evaluate the proposed methodology for multi-label classification problems. Furthermore, CheXPert is supposed to contain noisy labels due to automatic labelling from free-text reports. Hence, it would adequately test the proposed method Cufit under a noisy label setting.
6. To make the experiments more extensive, more datasets and PEFT methods (see [1] for reference) should be included. LoRA is one of the most popular PEFT strategies used for transformer-based models (especially in the case of medical image classification [1]) and should be included in the experiments.
7. For natural image classification, the authors have adopted the CIFAR dataset. Firstly, in order to provide conclusive results, several natural imaging datasets should be included. Secondly, there are many datasets much more appropriate than CIFAR that should have been used instead.

References
1. Dutt, Raman, et al. ""Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed Opportunity."" *Medical Imaging with Deep Learning*, 2024, https://openreview.net/forum?id=LVRhXa0q5r.

Limitations:
Please address the **Weaknesses** section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a curriculum learning strategy for fine-tuning on noisy medical datasets. The key insight comes from that linear probing with limited training samples can be more robust to label noise. The performance is good compared to the former methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Generally, I think this is a good paper.  Noisy label is a severe and practical problem for medical scenarios due to diagnosis uncertainty and the intuition behind the proposed method is clearly stated. The performance looks good and the proposed method is clean and efficient.

Weaknesses:
However, I still have to point out some details are not clearly demonstrated. Please check the questions for more details. I will change my score based on the authors' responses.

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a curriculum fine-tuning paradigm called Cufit. This method is designed to fine-tune Vision Foundation Models (VFMs) for medical image classification tasks under the presence of noisy labels. The approach leverages the robustness of linear probing and the generalization capabilities of fine-tuning adapters to improve model performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Cufit is technically sound and is well-validated through extensive experimental results.
- The paper is well-structured and effectively conveys the motivation, approach, and outcomes.
- Demonstrated significant improvements in medical image classification performance under label noise.
- Applicability to both medical and natural image classification enhances the relevance of the framework.

Weaknesses:
- The training process seems to be complex and computationally intensive.
- I have concerns about the scalability of the proposed method. It may not scale well for very large datasets or in resource-constrained environments.

Limitations:
None.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents Cufit, a curriculum fine-tuning paradigm for Vision Foundation Models (VFM) aimed at improving medical image classification under label noise. This method leverages the robust feature extraction capabilities of pre-trained VFMs and employs a linear probing strategy to mitigate the impact of noisy labels. The curriculum fine-tuning process then utilizes clean sample selection to enhance the classification performance. The experimental results demonstrate that Cufit outperforms existing methods on various medical image benchmarks, showing significant improvements in classification accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The presentation is good.

Weaknesses:
1. The paper includes experimental comparisons with methods like JoCor and CoDis, but the discussion about these methods' performance is insufficient. The authors should provide a more detailed analysis of why JoCor and CoDis do not perform as well as Cufit. Understanding the strengths and weaknesses of these methods in comparison to Cufit would offer valuable insights. 

2. The paper should further discuss the impact of noisy labels on different types of biomedical images. For some image types, noise may be less detrimental, while for others, it could significantly affect diagnostic accuracy. A more detailed exploration of how noise impacts various biomedical image datasets would enhance the comprehensiveness of the study.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
vWSll6M9pj;"REVIEW 
Summary:
This paper proposes a unified architecture and training method for auditory/visual speech recognition. Building upon this model, the authors introduce a semi-supervised pseudo-labeling method to leverage unlabeled audio-visual data, as well as self-supervised pre-training to enhance model performance. Experiments indicate that the model achieves state-of-the-art performance on A/V/AVSR.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This work for the first time proposes an effective model and training procedure for unifying auditory and visual speech content recognition, which is of high novelty and practical significance. 

2. The author conducted comprehensive and extensive ablation studies, verifying the characteristics of the model and the effectiveness of each step in the training paradigm. The experimental results are robust and credible, offering significant guidance for related research.

Weaknesses:
The article has no obvious flaws, but there are some questions that I hope the authors can clarify (see questions).

Limitations:
The authors discuss the limitation of their work in appendix A.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a training methodology for a *single* model which can use *either* audio, visual, or audiovisual features as input for automatic speech recognition. This is done by enforcing a training batch always includes (feature,label) pairs of all three modalities, using a 1D/2D ResNet-18 feature extractor for audio and video, respectively. These features are processed by a Transformer encoder-decoder model to obtain an ASR prediction. Furthermore, the authors explore a semi-supervised fine-tuning approach and a self-supervised initialization stage, both using a student-teacher approach, and within the same unified methodology. This allows the authors to produce a model which is competitive with state-of-the-art models while using a significantly less data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I think the proposed method is interesting for researchers in the audio-visual ASR domain and will spur future work. The paper is well-written with clear English, barring some questions I have stated below. The authors do a good job presenting their results, referring to details in the appendix where required. The ablation experiments clearly show readers how their proposed methodology behaves and why certain design decisions were made. The authors also shared their code and model checkpoints, which significantly increases the reproducibility and impact of this paper.

Weaknesses:
The model architecture seems a bit unclear to me. Specifically, line 88 states the use of a transformer encoder-decoder model. However, line 104 states a single FC layer on top of the encoder for vocabulary predictions, while line 107 states to use the decoder output sequence, which is subsequently not used as $1 - \lambda_{ctc}=0$. So the decoder is not actually used during fine-tuning? How is inference actually done?

I see no mention of a fixed random seed for running experiments, are all model initialized equal? This seems important as the paper does not have error bars/does not run experiments multiple times

Minor editing comments:
* Table titles must appear above the table as per the formatting instructions. 
* The table/figure combinations on Page 6 are confusing. Could you separate the figures as not part of a (sub)table?
* A small description of LRS3 would be desirable for those not familiar with the dataset (e.g., how many hours does the unlabeled portion have (line 190), what is the data source, how was it collected, how large is the test set?)
* line 97: 0.4 and 0.6 seconds for each second of ...

Limitations:
The methods requires all data to be audio-video paired. An interesting future direction could be the inclusion of audio-only data in the framework.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes USR, a unified speech recognition model that leverages pseudo labels during fine-tuning. It introduces a single model capable of handling three tasks—ASR, VSR, and AVSR—simultaneously, delivering state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-organized. Although the USR system is relatively complex, the paper presents each module with detailed descriptions and clear illustrations, making it easy for readers to follow.

2. The experiments, including ablations, are extensive. All experimental details are included, making it easy to reproduce the results.

3. The USR system leverages pseudo labels during the fine-tuning stage. While pseudo labeling is not a novel technique in ASR or AVSR, USR enhances the performance of ASR, VSR, and AVSR through carefully designed training procedures. The illustration of the pseudo labeling process is also clear.

4. The system achieves nearly state-of-the-art performance across all tasks.

5. The literature review is thorough.

Weaknesses:
1. While not a unique weakness to this paper, the complexity of training current SSL-based VSR or AVSR systems remains a challenge. Introducing additional modalities significantly increases complexity compared to speech-only SSL systems. Notably, the reduction in GPU hours is minimal compared to previous works, and the convergence speed is exceedingly slow. Future work should address these issues.

2. Performance is highly sensitive to certain configurations, such as the ratios of pseudo labels and the use of EMA. However, the paper lacks an analysis of why this sensitivity occurs or suggestions on how to mitigate it. These are common weaknesses in related work.

3. The results do not consistently achieve state-of-the-art performance. The authors should experiment with other hyperparameters, such as learning rates, during fine-tuning to improve outcomes.

4. Failure cases were not discussed too much.

Limitations:
The limitations have been discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper unifies the ASR, VSR, and AVSR tasks in a single model and shows the performance benefits of a single model in LRS3 data. There are several attempts at unifying these three models, but I think this is the first successful trial of realizing it. The paper proposes an effective training strategy to avoid losing performance on each task. Together with their self-supervised training, the model archives SOTA performance in a similar range of the training data.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- the first successful method of realizing the ASR, VSR, and AVSR tasks in a single model while maintaining/improving the performance for each task
- Good reproducibility based on the code release, use of the public data, and detailed experimental configurations/analyses.
- Easy to read. Although the technique is a little bit complicated with a lot of terms depending on the architecture (CTC, attention, modality, training modes (self-supervised/supervised), the paper always provides some rationales (e.g., from the reference or experiments) to justify their methods
  - detailed ablation experiments support their design choices and strategies. 
- The paper also shows the effectiveness with multiple databases (LRS3, LRS2, and WildVSR)

Weaknesses:
- the technical novelty is not very strong. Most techniques are well-known or straightforward (e.g., the use of CTC, pseudo-label filtering, etc.).

Limitations:
The paper has independent sections about limitations and Societal Impact, which describe the current issue due to the computational cost, the importance of the VSR, and the risk of general speech recognition technology.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vUrOuc6NR3;"REVIEW 
Summary:
This paper presents a self-supervised learning method for robot learning that learns representations by using data from demonstrations. The objective is based on learning latent actions from inverse dynamics, and learning forward dynamics model that uses such latent actions as inputs. Several techniques are utilized to prevent the model from finding trivial solutions and thus collapsing. Experiments are conducted in both real-world and simulation environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Clear writing with good figures
- Real-robot experiments!
- Focuses on the important problem of pre-training representations from demonstrations, as utilizing such limited but in-domain data can be crucial in the context of robot learning where in-domain data is scarse but important especially for fine-grained control tasks.

Weaknesses:
- As other self-supervised learning models are trained on non-standard robotic datasets, it is not clear whether they are trained well with good hyperparameters -- for instance without collapses -- is there a way to ensure that baseline methods are well-tuned?
- I understand that the main focus of this paper is to introduce a self-supervised learning method and compare its performance to other baselines. But what would the performance look like if you consider the full fine-tuning setup that uses gradients from behavior cloning for updating the encoder? Can we squeeze more information and maybe performance boost from fully fine-tuning the encoder? How would all the methods perform in this setup? This could further strengthen the claims of this paper that we should focus on extracting more information from demonstrations.
- One important missing baseline is [1] that pre-trains (optionally causal) transformer with masked modelling objective. Even though it uses a pre-trained visual encoder, using features from the causal transformer can be still a baseline Moreover, it's a bit awkward that MAE trained on demonstrations is missing from the baseline even though MVP is selected as a pre-trained representation baseline. Including MAE, maybe optionally its multi-view variant [2], can make results be more convincing.

[1] Radosavovic, Ilija, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik. ""Robot learning with sensorimotor pre-training."" In Conference on Robot Learning, pp. 683-693. PMLR, 2023.

[2] Seo, Younggyo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, and Pieter Abbeel. ""Multi-view masked world models for visual robotic manipulation."" In International Conference on Machine Learning, pp. 30613-30632. PMLR, 2023.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a way of pre-training vision encoder for robot control. Specifically, instead of using vanilla contrastive or masked autoencoder approaches, this method creates two models: 1) an inverse dynamics model that estimates the transition latent (actions) and 2) a forward dynamics model that takes in the current encoded visual latent and the transition latent and predicts the next latent observation. The results suggest that the method improves upon existing visual pre-training methods for robotics.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Few of the past works on visual pretraining for robotics consider the time / action but only focus on the visual observation aspect. This work presents a method that attempts to improve visual pretraining by modeling the dynamics present in the dataset. 
2. The results suggest that the method improves upon existing visual pre-training baselines

Weaknesses:
1. Prior visual pre-training for robotics operates under the premise that we have an image or video dataset, where we pre-train on these datasets and then finetune for a particular task. However, this method performs pre-training on the task-specific dataset, which is better aligned with the downstream tasks. Instead of having pre-training and fine-tuning using the same dataset and solving the same task, the objective of visual pretraining (exemplified by MAE, MoCo, etc.) is that if we train on a mass amount of data, we can finetune to a specific task (i.e. ImageNet pre-training then COCO segmentation finetuning). 
2. A few prior works [1,2] have tried to model forward and inverse dynamics concurrently. [1] also uses forward and inverse dynamics to train a visual encoder. The key difference between these works is that here action is modeled as a latent variable. Why ground truth action values are not used in pre-training (especially when pre-training and fine-tuning happen on the same task) is not justified in the manuscript. It would be quite convincing if pre-training is done on natural videos, or large-scale robot datasets where action spaces cannot be standardized, and then shows improved finetuning performance. 

[1] Agrawal, Pulkit, Ashvin V. Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. ""Learning to poke by poking: Experiential learning of intuitive physics."" Advances in neural information processing systems 29 (2016).

[2] Fragkiadaki, Katerina, Pulkit Agrawal, Sergey Levine, and Jitendra Malik. ""Learning visual predictive models of physics for playing billiards."" arXiv preprint arXiv:1511.07404 (2015).

Limitations:
Limitation section is present.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a self-supervised model, DynaMo, for pretraining visual encoders adopted for visuo-motor control. The targeted downstream task is imitation learning for robotic manipulation. Instead of using an out-of-domain dataset for pretraining and then transferring to a new domain using alternative techniques, the authors propose exploiting sequences of observations from in-domain demonstrations to pretrain three different models, a visual encoder, a forward and an inverse model. Once this is done a policy can be learned with observations encoded using the pre-trained visual encoder.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The most important benefit of DynaMo is that a visual encoder can be trained with limited risk of suppressing data dimensions necessary for visuomotor control, an otherwise frequently occurring problem.

Even if similar models that combine training of forward and inverse models have existed in literature before, the action representation is assumed unobserved in the proposed model, which has rarely been the case before. The literature on imitation learning from observed state sequences is vast, with little cited in the paper. However, the way this is done for pretraining in the proposed model is innovative and easily applicable to a practical scenario.  

The experiments are rather exhaustive with five different settings and embodiments tested, two of which are real-world scenarios. In experiments that compare to alternative self-supervised methods and pretrained representations, the proposed visual embeddings are shown to be very competitive. It is also shown that DynaMo can be used to finetune an encoder pre-trained on ImageNet for even better results while being relatively insensitive to the choice of policy class.

Weaknesses:
The paper is written as if there were no research in the area before the deep learning boom. Only one citation out of 70 citations is older than 10 years. The paper suggests that training exclusively on in-domain data is new, even if this used to be the way it was typically done before the arrival of data-hungry deep-learning-based models, models that forced people to a greater extent to rely on offline training on out-of-domain data with data augmentation, contrastive learning, etc. 

The idea to train pairs of inverse and forward models online has existed in psychology and robotics for at least 25 years, such as in the works of Wolpert et al [1]. Using similar models, imitation learning has been a common theme over the years, with [2] being just an example. Without this connection back to earlier research, this paper gives the impression of trying to reinvent the wheel, and it becomes unclear what the contributions really are. 

Even if the experiments suggest that DynaMo can be beneficial also in real-world settings, the presented experiments are too few to be conclusive. The real world is way more diverse with more than just a small selected set of objects that can be manipulated. However, this weakness is pointed out in the conclusions, which makes it less problematic.  

[1] Wolpert and Kawato, “Multiple paired forward and inverse models for motor control”, Neural Networks, 11, 1998.

[2] Demiris and Hayes, “Imitation as a dual-route process featuring predictive and learning components: a biologically plausible computational model”, in Imitation in Animals and Artifacts, MIT Press, 2002.

Limitations:
Yes, some limitations related the real-world experiments and the unimodality of models are brought up in the conclusions.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents DynaMo, using in-domain data for self-supervision. It jointly learns a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings. The

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
This paper is easy to follow.

Weaknesses:
Simplified Real-World Setup:
The real-robot experiments appear overly simplistic. Objects seem to be fixed in place, indicated by the red marker on the table, suggesting a lack of randomization in object placement. This setup makes the task easier for conventional imitation learning methods like diffusion policy and act, potentially allowing them to achieve a 100% success rate.

Suggestion: Introduce spatial randomization to the scene. Conduct additional experiments under these conditions to demonstrate Dynamo's superiority in more complex and varied scenarios.

2) Unfair Comparisons in Simulation: 
In Table 1, Dynamo is compared with several baselines that use different backbones, which makes the comparison potentially unfair.
Impact: The difference in backbones could skew the performance results, making it difficult to accurately assess Dynamo's relative performance. 

Suggestion: Include more experiments of Dynamo with various backbones such as ViT and ResNet-50. Compare these results against the baselines to provide a fairer and more comprehensive evaluation.

3) The motivation for using SSL in this context is unclear. Typically, SSL is advantageous due to its ability to learn from massive datasets without human labels. However, in the field of robotics, in-domain data are often scarce. This could make the application of SSL less persuasive and potentially less effective.

Limitations:
No.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
vU512K8vrR;"REVIEW 
Summary:
The work presents an algorithm for adapting the rank of the LORA matrices according to a novel “saliency metric”  assigned to each singular value of the LORA matrices. 

The saliency measure is computed taking into account a sequence of steps (time window) during training and computing two quantities at the end of each time window: the orthogonality-aware singular values and the domain influence of each singular value. The orthogonality-aware singular value is a weighted average of the singular value where the weight takes into account the orthogonality of the SVD decomposition at that step. The domain influence takes into account the correlation between singular values within each time window. 
At the end of each step sequence, the ranks are adjusted based on this salience measurement.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors propose a novel and interesting algorithm. The chosen setup speeds up the LoRA fine-tuning while maintaining accuracy or slightly outperforming other methods on the reported benchmarks. The experimental evaluation is convincing since the authors compare the proposed algorithm with other LoRA improvements on a reasonable number of tasks.

Weaknesses:
The main weakness of the work is the clarity of the exposition, which is obscure in some parts. 

For example, one of the methods' core building blocks is the decycling operation of the dependency graph mentioned between lines 164-165: the authors must reference the algorithm they use for “de-cycling” the graph, describing its steps at least in the appendix.\
I leave other statements that require clarification in the question section below.

In addition, the paper does not discuss the limitations of the methods.

Limitations:
No.
The authors do NOT seem to discuss the limitations of their method in the current version of the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces SalientLoRA, an approach designed to optimize the intrinsic ranks of LoRA components in LLMs through salience measurement. The method first utilizes salience measurement to analyze the variations and inter-dependencies of singular value magnitudes over time, which helps assess matrix importance while mitigating instability and randomness. This analysis informs the adaptive adjustment of the time-series window used for significance measurement and rank reduction during training. This adaptive mechanism allows for rapid and stable rank allocation, permitting an initially higher rank setting to expand the allocation space for ranks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. SalientLoRA's use of salience measurement to analyze and utilize the variations of singular values effectively addresses the challenges of instability and randomness in rank optimization. The adaptive adjustment of the time-series window for significance measurement during training enhances the efficiency and stability of rank allocation.

2.  Demonstrating substantial performance gains over state-of-the-art methods on diverse NLU and NLG tasks highlights the effectiveness of SalientLoRA in practical applications.

Weaknesses:
The proposed method incorporates a sophisticated multi-stage process that involves several critical hyperparameters, such as $\beta$, $\gamma$, $T_i$, and  $T_f$. However, the paper currently lacks a detailed analysis of these hyperparameters, which is crucial for understanding their roles and optimal settings within the methodology. Systematically exploring how each hyperparameter impacts the model's performance, including sensitivity analyses or hyperparameter tuning results, would greatly enhance the paper's scientific rigor.

Limitations:
There are no potential negative societal impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes SalientLoRA, a new method for adaptively optimizing the intrinsic ranks of low-rank adaptation (LoRA) matrices. The key ideas are:

Using singular value decomposition (SVD) to decompose the LoRA matrices and measure the salience/importance of each singular value based on its magnitude, orthogonality constraints, and influence on other singular values within a time window during training.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Novel salience measurement technique that considers singular inter-dependencies and temporal variations.
- Comprehensive evaluation across many datasets and model types (encoder, decoder, encoder-decoder).
- Achieves new state-of-the-art results on multiple benchmarks while being more efficient than prior LoRA methods.

Weaknesses:
- The article contains some details that are not clearly explained, such as how the R function on line 145 is calculated, and what specifically is done in the de-cycling process introduced on line 165.
- More analysis could be provided to interpret why the salience measurement works well. For example, are the average of influence domains consistent across models fine-tuned on different types of datasets?

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vU1SiBb57j;"REVIEW 
Summary:
This paper introduces DDiffPG for online reinforcement learning with multi-modal behaviour discovery. DDiffPG consists of two parts: 1) a new policy improvement method to stabilise the diffusion policy by cloning a target action; 2) a mode discovery mechanism to train mode-specific and intrinsic Q functions. In their experiments, the authors have shown that DDiffPG can achieve comparable performance with the baselines while producing multi-modal behaviours, which provides a series of benefits like avoiding mode collapse.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper has introduced an interesting idea. To the best of my knowledge, this is the first work that allows diffusion policy to learn multi-modal behaviours during online RL. According to the experiments, the proposed method has produced a reasonable performance with nice multi-modal behaviours. Besides, the paper has provided nice visualisations and discussions to help understand the proposed approach.

Weaknesses:
There are several main weaknesses of the paper.

- The paper is hard to follow and the presentation has a certain room for improvement.

- In section 3, the formal theoretical derivation of the newly introduced policy improvement objective is missing. Although it shows that this method worked empirically, it remains unclear how the resulting policy theoretically maximises the expected return in general.

- I feel the paper is a bit over-claiming for certain aspects. In section 5.3, the authors claimed that DDiffPG can *overcome* local minimum issues and encourage exploration. However, the exploration comes from the use of RND when learning $Q_\mathrm{explore}$, rather than the architecture itself. In addition, it is a very strong claim that DDiffPG **overcomes** the local minimum issues. The experiments are conducted on only 8 state-based tasks, from my point of view, which is insufficient to support such a general claim. I understand that by capturing multi-modal distributions, DDiffPG allows better generalisation, but I would suggest the authors moderate the claims a bit.

Minor issues:
- In line 157, is this a typo? In $r^\mathrm{intr}(s, a, s’) = \max(\mathrm{novelty}(s’) - \alpha \mathrm{novelty}(s’), 0)$, should this be $r^\mathrm{intr}(s, a, s’) = \max(\mathrm{novelty}(s’) - \alpha \mathrm{novelty}(s), 0)$?

Limitations:
- In line 175, the definition of DTW requires task privileged information, e.g., object positions. This is a major issue and unrealistic. In more realistic scenarios, people normally have no access to such information, and as a result, this clustering approach is not applicable.
- The paper only conducted experiments on 8 simple state-based environments. It is unclear how the approach could generalise to more realistic environments and tasks with visual observations. Although at the current stage, I understand that the current results reasonably demonstrate the capability of the proposed method, more realistic tasks will better support many claims made by the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenges associated with employing diffusion policy in online reinforcement learning (RL), particularly the intractability of policy likelihood approximation and the bias towards a single mode. The author introduces the Deep Diffusion Policy Gradient (DDiffPG) method, which decouples exploration from exploitation. For exploration, novelty-based intrinsic motivation and hierarchical clustering are utilized to identify modes, while for exploitation, the author describes the mode-specific Q-function and a multimodal data batch. Empirical evaluations demonstrate that DDiffPG effectively masters multimodal behaviors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ The application of diffusion policy for multiple modes in an online setting is promising and addresses a previously unexplored area in the literature.
+ The introduction of a diffusion-based policy gradient method is novel and represents a significant contribution to the field.
+ The work is well-motivated, and the visualization of multimodal behaviors using antmaze examples effectively enhances understanding and illustrates the practical utility of the approach.

Weaknesses:
+ Several claims require additional support. For instance, the author asserts that standard exploration-exploitation strategies may easily converge towards a single mode (Lines 25-27) without providing theoretical or experimental evidence. Similar issues are present in Lines 35-36 and Lines 52-53. These statements are crucial for constructing the paper's motivation and thus require more substantial support to enhance their reliability.

Limitations:
--

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to solve online RL problems with diffusion policy. It includes 1. a diffusion policy optimization method for diffusion online training. 2. A combination of intrinsic rewards motivated skill discovery method and model-seeking Q-learning to facilitate exploration and prevent mode-collapse behavior. 3. Several self-designed environments where there might  be multiple optimal solutions and thus require expressive exploration policy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper shows diffusion policy has a big potential in online RL because it enables multimodal exploration.
2. The self-designed environments are a good contribution to the research field by showcasing the necessity of diffusion exploration.
3. Performance clearly surpasses several baselines.

follow up:

The experiments basically support comments in the paper.  The paper sets out to handle the single-mode exploration problem in online RL, and the self-designed environments, unlike most previous classics, allow diverse optimal behaviors and can benefit from multimodal exploration. The experiments show that the proposed method outperforms several classic baselines including some diffusion-based methods.

Weaknesses:
1. The proposed diffusion training objective seems handcrafted and requires a lot of tunning. This may limit the algorithms' further application.
2. Besides the diffusion optimization methods. Other proposed techniques are more like a good combination of previous work. This indicates limited theoretical novelty.
3. Code is not provided. For this style of paper, I think code quality is essential, and a mere promise to release the code is not convincing.

follow up:
1. The ablation studies are not strong enough to prove the improved performance number actually comes from multimodal exploration. I cannot be certain which part of the method works from the experiments.  More visualization/empirical results/analyses should be given.
2. The formatting of the table/figure can be greatly improved. For instance, the title of Figure 4 is wrong/incomplete. Table 3/4 is referenced as the main results in the paper but only put in the appendix. 
3. The diffusion optimization results also lack very strong novelty. The loss function is basically a supervised learning loss adapted for online RL, without strong convergence or policy improvement guarantee. Still, the diffusion+online RL theories are a known unsettled and hard problem, so this kind of exploration is fine and meaningful.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vS5NC7jtCI;"REVIEW 
Summary:
This paper introduces a new algorithm for Out-Of-Distribution (OOD) sample detection. First, it analyzes the shortcomings of previous Vision-Language OOD detection methods and proposes improvements based on these findings. Specifically, the paper presents a scheme for online updating of the memory bank during testing to design better negative proxies. The authors conducted experiments on datasets such as ImageNet and CIFAR. According to the experimental results, the newly proposed method can enhance OOD detection performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Currently, vision-language models are developing rapidly, and using them for OOD sample detection is a promising direction. Approaching from this perspective may yield better results.
2. The experiments in this paper are relatively thorough, encompassing both large datasets based on ImageNet and smaller datasets based on CIFAR. According to the authors' experimental results, the newly proposed method can improve the accuracy of OOD detection.

Weaknesses:
1. The motivation in this paper is not very clear. Specifically, in Figure 1(a), it is not evident why the newly proposed AdaNeg is better than NegLabel. On the contrary, the distribution of OOD samples seems to be closer to NegLabel.

2. The method proposed in this paper is based on the features and results of test samples during testing, which limits the upper bound of the method. In my opinion, the effectiveness of the proposed method relies on the vision-language model's strong inherent OOD detection capability, meaning that most test samples can be correctly processed. Based on these correctly processed samples, the method can further improve the detection accuracy of other samples. However, if in a certain scenario, the model itself cannot correctly estimate most of the samples, this method might actually make the results worse.

3. This paper merely performs optimizations based on the NegLabel framework, without many innovative points. The novelty of this improvement is insufficient to support a NeurIPS paper.

Limitations:
Potential negative societal impact is not applicable.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper ""AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language Models"" presents a novel approach to out-of-distribution (OOD) detection using pre-trained vision-language models (VLMs). The primary innovation is the introduction of adaptive negative proxies, which are dynamically generated during testing by exploring actual OOD images. This method addresses the semantic misalignment issues of previous approaches that use static negative labels. AdaNeg utilizes a feature memory bank to cache discriminative features from test images, creating task-adaptive and sample-adaptive proxies that better align with the specific OOD datasets. The approach combines static negative labels with adaptive proxies to enhance the performance of OOD detection, achieving significant improvements in benchmarks like ImageNet. The method is training-free, annotation-free, and maintains fast testing speeds.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.	Innovative Approach: The introduction of adaptive negative proxies to address semantic misalignment is a significant advancement. This dynamic generation of proxies during testing offers a novel solution to improve OOD detection.
2.	Effective Use of Vision-Language Models: Leveraging VLMs to integrate textual and visual knowledge enhances the robustness and accuracy of OOD detection.
3.	Performance Improvement: The method shows substantial improvements in standard benchmarks, particularly a 2.45% increase in AUROC and a 6.48% reduction in FPR95 on the ImageNet dataset.
4.	Training-Free and Annotation-Free: AdaNeg does not require additional training or manual annotations, making it highly efficient and practical for real-world applications.
5.	Scalability and Efficiency: The method maintains fast testing speeds and can dynamically adapt to new OOD datasets without significant computational overhead.
6.	Comprehensive Evaluation: Extensive experiments and analyses demonstrate the effectiveness and robustness of the proposed approach across various benchmarks.

Weaknesses:
1. Potential Overhead in Memory Management: The implementation of a memory bank for caching features may introduce significant overhead in memory management, especially when dealing with large-scale datasets or high-dimensional feature spaces.
2. Generalization to Other Domains: Although the approach demonstrates promising results on existing public datasets, its effectiveness in other domains or with different types of data remains uncertain and requires further investigation.
3. Testing Phase Dependency: It is unclear whether the approach can maintain the same level of reliable performance when only a small number of images are tested in practical applications. This dependency on the number of test images warrants additional examination.

Limitations:
1. Generalization to Other Domains
2. Dependency on test data.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the authors propose AdaNeg, a test-time adaption method for CLIP-based post-hoc OOD detection. AdaNeg is an extension of NegLabel and introduces a class-wise memory bank for each ID and negative labels. The memory bank is gradually filled with ID and OOD features during the model deployment. The author design a margin-based approach to select positive and negative samples with high confidence. And they propose a cache elimination mechanism to update the memory bank. Besides, AdaNeg uses cross attention between the input sample and the memory bank to reweight the cached features. The experimental results show the proposed method outperforms the baseline methods under various benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
<1> AdaNeg uses dynamic OOD proxies instead of the static design of NegLabel, achieving SOTA performance in CLIP-based zero-shot OOD detection. 

<2> The multi-modal score is an interesting design and explanation that demonstrates the improvement brought by using both text and image encoding capabilities in a multi-modal model.

<3> The paper is well organized and easy to follow.

Weaknesses:
**Major concerns**

<1> AdaNeg is a test-time adaption approach that caches features w.r.t. ID labels and negative labels by maintaining a class-wise memory bank. For OOD detection, the biggest problem of the test-time adaptation method is that the arrival time of the OOD sample is uncertain. Compared with non-TTA methods, AdaNeg has greater uncertainty in its performance during the deployment phase and may even risk causing model collapse. 

For example, when the model is deployed to a close-world environment, almost all input samples are ID samples (I believe this is a very common scenario). In this case, the memory banks of negative labels will gradually be filled with ID samples (in long-term deployment, there will always be misclassified ID samples that enter the negative memory banks). Since the number of negative labels is much greater than that of ID labels, more and more ID samples will be misclassified as OOD over time. I suggest the author conduct an experiment using the 1.28M training set images of ImageNet-1k as input (this still meets the zero-shot setting of CLIP) and observe how the proportion of samples misclassified as OOD changes with the number of input samples. If 1.28M images are repeatedly input into multiple rounds, will the misclassification rate increase further? In contrast, the other case is that the OOD samples are far more than the ID samples. Will this cause a greater false positive risk? I hope the authors can test their method with different ID and OOD sample mixture ratios, such as 1:100, 1:10, 1:1, 10:1, 100:1.

In summary, I suggest the authors to further study the setting of TTA in OOD detection to improve the motivation of the work, since the input samples may come from two different distributions, ID and OOD. How to ensure the stability of TTA OOD detection algorithm when the input stream is a non-stationary process is a problem worth studying.

<2> The negative labels provide the initial memory bank slots for AdaNeg, but it seems to me that the negative labels are not necessary. This suggests that we need to rethink AdaNeg's motivation for negative labels. Why do samples that are judged as negative need to be placed in the memory bank w.r.t. the negative label? What if the authors directly use the MCM score to judge negative samples and then let them organize themselves into OOD proxies? The authors need to provide a more detailed analysis (preferably theoretical analysis) to prove that the **negative label-based** memory bank design is necessary.

Further, negative labels simply select words that are semantically far from ID labels. For some OOD samples, they may be far away from both ID labels and negative labels. According to the mechanism of AdaNeg, they cannot enter the memory bank of negative labels. Is this a negative impact of designing a memory bank based on negative labels?

**Minor concerns**

<1> The authors need to provide more detailed experimental settings. The paper mentions that memory banks are task specific. When evaluating the model, taking the ImageNet-1k benchmark as an example, do the authors maintain an independent memory bank for each OOD dataset (precisely, each ID-OOD pair), or did the four OOD datasets share one memory bank?

<2> There seem to be some typos and symbol issues in the paper.

a) L247: temperature $\tau = 100$ seems to be $\tau = 0.01$ because $\tau$ is in the denominator.

b) The subscript NL is not case-inconsistent, e.g., Eq. (4) and Eq. (8).

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors introduce a new approach to leverage the pre-trained vision-language model for identifying out-of-distribution (OOD) samples. Compared to prior works that employ consistent negative labels across different OOD datasets, they introduce adaptive negative proxies to dynamically generate text labels during testing by exploring actual OOD images, thereby aligning more closely with the underlying OOD label space. Empirically, the proposed method demonstrates state-of-the-art performance across various OOD detection benchmarks especially on the large-scale ImageNet benchmark.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Dynamically generating negative proxies is a simple and effective strategy. 

-  The setting studied is very natural and this paper can easily stimulate further research in the area.

- The proposed approach performs well, particularly on large-scale datasets such as ImageNet, effectively demonstrating its scalability.

- The paper is nicely written.

Weaknesses:
- While the proposed AdaNeg shows clear improvements over training-free baselines, its overall performance on ImageNet still lags behind training-based methods. This raises the question of whether there are opportunities for complementarity between the two approaches.

- Can the dynamic update of the memory bank and refinement of OOD proxies during the testing stage be considered a form of test-time training? The authors are requested to clarify the inherent connections and distinctions, especially from the perspectives of training versus training-free approaches.

- If negative proxies can directly identify true out-of-distribution (OOD) test images during the testing phase, is it possible to use the identified OOD samples to update the model parameters online?

Limitations:
yes

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
vP9qAzr2Gw;"REVIEW 
Summary:
This paper introduces a new method called Supra-Laplacian Encoding for spatio-temporal Transformers(SLATE) to deal with dynamic graph challenges. Its core approach is to enhance the graph transformer(GT) architecture by integrating spatio-temporal information more efficiently. It deploys a new technology to convert discrete-time dynamic graphs into multiplayer graphs and exploit the spectral properties of their associated super-laplacian matrices. SLATE also implements a cross-attention mechanism to explicitly model pairwise relationships between nodes. SLATE can capture the dynamic nature of graphs more accurately with this implementation. SLATE provides a powerful tool for applications ranging from social network analysis to understanding complex biological networks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1.SLATE applies spectral graph theory to the dynamic graph domain in a novel way. 
2.The quality of this study is evident in the rigorous experimental setup and the comparion with SOTA methods. It is able to outperform many existing models on nine datasets. 
3.The authors provide a detailed explanation of the method and the underlying theoretical concepts. And the open-source code and the instructions for reproducing the results enhances the clarity and accessiblility.

Weaknesses:
1. The experimental results of CanParl in Table 2 is not very good.
2.The permutation setting of SLATE may limit its ability to generalise to unseen nodes and large-scale graph data.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes SLATE, a novel method for link prediction in dynamic graphs. SLATE transforms dynamic graphs into multi-layer networks and generates a unified spatio-temporal encoding by leveraging the spectral properties of the supra-Laplacian matrix. It uses a fully connected transformer architecture to capture long-range dependencies between nodes across multiple time steps. The authors introduce a cross-attention-based edge representation module for dynamic link prediction. They claim that SLATE significantly outperforms existing state-of-the-art methods on several benchmark datasets

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The idea of transforming dynamic graphs into multi-layer networks and utilizing the supra-Laplacian is innovative.

2. Extensive experiments were conducted on various datasets and baselines.

3. The method shows superior performance compared to state-of-the-art approaches on multiple datasets.

Weaknesses:
W1. The explanation for adding temporal connections in the supra-Laplacian construction stage seems insufficient.

W2. The description of how to construct the supra-Laplacian is not comprehensive enough.

W3. The characteristics of the SLATE model are not clearly defined. For example, the necessity of each step (a)-(d) in Figure 2 lacks convincing arguments.

W4. This paper discloses all data and code upon acceptance, which limits the ability to verify the reproducibility of this paper.

Limitations:
This paper discloses all data and code upon acceptance, which limits the ability to verify the reproducibility of this paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a spatial-temporal encoding for transformers on dynamic graphs. Specifically, graphs at each time step are treated as a single multilayer graph and packed into a larger adjacency matrix, with temporal self-connections between each node and its past. Eigenvectors of the constructed Laplacian are used as positional encoding and concatenated with node features. A standard transformer encoder layer then generates all representations for each node at each time step within a selected time window. To predict a link between two nodes, the model does cross-attention between their representations within the time window. Experimental results show that the proposed model performs better than existing methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The positional encoding proposed by this paper aims to jointly model spatial and temporal dependencies, which is a plausible improvement over existing methods.
* The proposed model shows strong empirical performance compared with existing approaches. In particular, the proposed positional encoding works better than simply concatenating LapPE and sin/cos

Weaknesses:
* Scalability/efficiency may still be a concern on large graphs, though the paper shows good engineering (e.g., Flash-Attention) can help
* Some finer-grained ablation studies are missing. For example:
    * Instead of removing isolated nodes in preprocessing, can we keep the disconnected graph and just use eigenvectors corresponding to non-zero eigenvalues?
    * The transformer itself can already get global information and I see no strong reason to use virtual nodes additionally. How would the model behave without virtual nodes? How would ""virtual nodes + GNN"" behave?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work introduces Supra-Laplacian encoding for spatio-temporal Transformers (SLATE) which aims to learn both spatio and temporal information in a dynamic graph with a transformer architecture. The key is to convert Discrete Time Dynamic Graphs into multi-layer networks and then extract the spectral features of their supra-Laplacian matrix to improve upon existing dynamic graph transformer designs. Additionally, SLATE employs a cross-attention mechanism to accurately model nodes' pairwise relationships, improving dynamic link prediction. The proposed SLATE model performs competitively to both CTDG and DTDG methods on discrete graphs.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- **originality**: connecting DTDGs into a multi-layer graph and then compute spectral properties of a Supra-Laplacian matrix is a novel approach in the literature. The empirical performance also demonstrates that this approach can outperform existing methods with its spatio-temporal reasoning capabilities. 

- **extensive evaluation**: The proposed SLATE method compares favorably to both CTDG and DTDG methods on discrete datasets with existing evaluation of testing 1 positive edge against 1 negative edge. In addition, model analysis experiments and ablation studies provides insights into the model components and choices. Additional experiments with hard negative samples are also included in the appendix.

- **clear presentation**: the paper is easy to follow and the main idea is presented well

Weaknesses:
- **scalability**: my main concern is the scalability of the method as the authors also pointed out as a limitation. Even with the time window (which truncates the history of the temporal graph), the $N^2 w^2$ complexity remains very high and only feasible for networks with up to thousands of nodes, In addition, there is a large amount of precomputation needed for the supra-Laplacian and computing its eigenvectors. 

- **window size**: one of the core hyperparameter of SLATE is the choice of window size, as the study in Figure 4 shows that there are some common optimal window size for the CanParl Colab and USLegis datasets. These datasets mostly contains a small number of snapshots thus might be why 4 is a good choice. In practice though, it might be difficult to tell which window size is optimal without extensive experiments to select it. It would also be interesting to see if the length of the window is related to other factors in the architecture, size of the multi-layer network, transformer dimension etc.

Limitations:
The limitations are discussed sufficiently in the paper. More discussion on negative societal impact might be included such as downstream applications of anomaly detection or recommendation systems for temporal graph learning methods,

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vMMzjCr5Zj;"REVIEW 
Summary:
Training large time series (TS) models is often limited by the scarce data available for a specific application. Existing pretraining methods use a simplistic tokenization scheme where the TS is cut up into equally sized parts, independent of its content. The newly proposed method *Large Pre-trained Time-series Models*, therefore, adaptively segments the input time series into (potentially) overlapping tokens depending on the TS itself. It shows very good forecasting performance in zero-shot and finetuning settings. It can also be used for classification.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The relevance of the problem and motivation for adaptive segmentation is convincing.
- The method of adaptive segmentation is an interesting solution to the issue.
- LPTM is compared against a plethora of appropriate and challenging baselines.
- It shows promising empirical results.

Weaknesses:
- I am under the impression that this paper may have found a strong method yet does not sufficiently investigate *why* it works. The interplay between learning the scoring function and training the encoder is not very clear. See below.
- A lot of experimental claims are not adequately substantiated. It is claimed in question 6 of the checklist that error bars are provided and that statistical significance tests are performed, yet I did not find them. See below.
- The overall presentation (language and formatting) should be improved.
- The provided implementation is not accessible. (Error: ""The repository is expired"") In the current state, results are not reproducible since key hyperparameters are missing. The authors claim in question 6 of the checklist that they state how hyperparameters were chose, yet I could not find it in the paper.

Limitations:
Depending on the answers to the questions above, possible limitations mentioned could be made more transparent. For example, the insights into the newly induced biases are currently limited.

The answer to section 2. in the checklist is neither sufficient nor truthful. For example, ""multivariate"" is never mentioned in Section 7.

The discussion of the societal impact could also consider a possible data leakage from one private application (e.g., in the medical domain) to another one. Even a rather mundane problem, like a feasible membership inference attack, could be problematic in privacy-sensitive scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new approach for creating pre-trained models for time-series data, similar to those used in language and vision tasks. The authors propose a model called Large Pre-trained Time-series Models (LPTM), which includes an innovative adaptive segmentation module to handle diverse time-series data from multiple domains.

Key contributions include:

- Developing a framework for pre-training time-series models on multi-domain datasets, using a novel adaptive segmentation module to tokenize inputs effectively. This is achieved via a self-supervised learning objective.
- Demonstrating that LPTM performs as well or better than state-of-the-art domain-specific models when fine-tuned for various time-series tasks, such as forecasting and classification, with less training data and compute time.
- Proving that LPTM achieves superior results in both zero-shot and fine-tuned settings across diverse domains like epidemiology, energy, and economics, requiring up to 40% less data and 50% less training time compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper has the following strengths:

- Well-written, clear, easy to follow. Algorithm is a nice plus.
- Baseline choice reasonable: most recent methods are considered.
- Experimental results good, when considered on the set of datasets chosen (more points on that in the weaknesses section).

Weaknesses:
- It's a bit hard to get a good feel for the relative advantage of the proposed method. In table 2, the approach is clearly better, but we are left to infer that from that fact that it is commonly second or first in the rankings. Could the authors maybe add some for of aggregate metric, e.g. the average rank across datasets of a given method?
- Despite mentioning code is available, the link does not work (subscript 3 on page 7, time of access 2024-07-12, and previously): ""The repository is expired"".
- For a paper dealing in large part with forecasting, I was surprised by the absence of almost all of the classical long-term forecasting datasets used by other papers: traffic, electricity, weather, illness... Given that these are by far the most heavily studied ones in the literature, including them (as proposed in the questions section). While I don't find it a critical (but still important) concern, I strongly advise the authors to consider adding them as it will help avoid concerns other readers might have about cherry-picking of results.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes Large Pre-trained Time-series Models (LPTM), a novel method designed to improve the efficiency and performance of time-series analysis across multiple domains. 
The key contribution is an adaptive segmentation module that automatically identifies optimal segmentation strategies for diverse datasets during pre-training. 
This approach aims to overcome the limitations of fixed-length segmentation, which may not adequately capture the temporal patterns of heterogeneous time-series data. 
LPTM demonstrates superior forecasting and classification performance, requiring up to 40% less data and 50% less training time compared to state-of-the-art models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
S1. This paper focuses on the time series segmentation problem.
As the basic semantic unit in time series is not as clear as in text, a proper segmentation is a promising direction towards better series modeling.

S2. The proposed segmentation method is adaptively calculated over each specific input series.

S3. The experiments are extensive.

Weaknesses:
W1. Although time series has a weaker semantic structure than natural language, it is closer connection to images.
In both time series and images, a semantic unit, e.g., a small item or a texture in an image, can have different lengths and scales.
This raises a challenge against the main motivation: why a full self-attention-based architecture works for images (e.g., ViT), why for time series the segmentation needs to be explicitly done?
It would be interesting if the authors can further discuss this problem and provide their intuitions.

W2. The introduction of the adaptive segmentation module seems to bring instability in the initial model training, as well as requiring longer training time (although the authors propose to backpropagate the gradients every 10 batches).
Specifically, the loss function for segmentation is a hard loss based on the selected subset of best segments.
However, the parameters seem to be randomly initialized, which could provide highly random ""best"" segments.
Hence, the convergence stability and the training time with and without the dynamic segmentation modules should be discussed.

W3. The dynamic segmentation modules seem not to be fine-tuned with specific attention.
However, as the author(s) mentioned, different datasets could have very different best segmentation.
Hence, it would be interesting to discuss why this is sufficient and provide theoretical or empirical evidences.

Limitations:
L1. There is a lack of explanability and interpretability in the adaptive segmentation results.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel contribution to pretrained time series models for forecasting and classification by paying attention to the fact that currently several transformer models take time series segmentations of the same size, regardless of the particular characteristics of the time series in consideration. For instance, time series that have yearly frequency or minute frequency might require different segmentation lengths, or it might be that dynamics are more complex in certain time intervals requiring a more detailed segmentation. Based on this observation the authors proposed a model that can find a suitable segmentation schema that later on allows to observe where are the time intervals where more complex dynamics are shown.

The authors perform several experiments and claim empirically that the proposed approach is at least competitive to the state of the art.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors study a clearly interesting problem: how to provide a suitable segmentation scheme for time series so that different time regions are segmented in different ways, depending on their complexity and amount of information. The motivation for this is well stated by the authors, leading to a novel approach to achieve this. 

The authors further set up this in an Self-supervised learning setting, and consider multiple datasets to pretrain their model and further provide several evaluations. This is interesting because depending on the field/topic/area of time series a different segmentation scheme might be more suitable.

Weaknesses:
Some of the main limitations are as follows:
- The proposed framework is not differentiable. The authors have acknowledged this in the paper and propose a workaround for this, basically to update the segmentation scores every 10 batches. Yet, this poses challenges like the interpretation of the training loss, and discontinuities in the test loss.
- It is unclear if the proposed approach is able to handle missing values. If not, is there anyway to overcome this? Missing values are very often present in practice and having a sound way to handle them is relevant.
- It is unclear if the current evaluation is fair. The authors present a corpus of datasets for which they pretrained the proposed model, but it is unclear which datasets where hold-out from pretraining. This is relevant as several of the pretrained models considered might have not been exposed to these datasets, which gives an unfair advantage to the proposed model. Further, since the amount of pretraining datasets is rather limited, there is the possibility that the proposed model is overly focused on these datasets, whereas other models, like (Ansari 2024) and (Woo 2024) were trained in a larger corpus of datasets.

Limitations:
The authors have acknowledged limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vJSNsSFO95;"REVIEW 
Summary:
The paper presents a novel approach to handling the inherent ambiguities in the SAM used for image segmentation. SAM, despite its robustness, often exhibits sensitivity to slight variations in prompts and object granularity, leading to inconsistent predictions. The authors propose a new framework leveraging a conditional variational autoencoder to model these ambiguities probabilistically. This approach enables SAM to produce diverse and reasonable segmentation outputs by adapting to the inherent ambiguities in the data. The paper details extensive experiments demonstrating the effectiveness of this framework across various practical scenarios involving ambiguous segmentations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	This work addresses a critical challenge in image segmentation, especially in medical imaging and other fields where ambiguous data is common. By turning SAM's sensitivity into an advantage, the paper contributes to the advancement of robust and adaptable segmentation models.

2.	provides a thorough analysis of SAM's sensitivity to prompt variations and object granularity, backed by detailed experiments and statistical evaluations.

3.	The paper is well-structured, with clear definitions and explanations of the proposed methods. The use of figures and tables enhances the understanding of the framework and its performance.

Weaknesses:
1.	The paper primarily tests the framework on specific medical imaging and synthetic datasets. There is a lack of diverse real-world datasets, such as those from different domains (e.g., natural scenes, industrial applications), which might exhibit different types and degrees of ambiguity.

2.	I have a concern that the framework might be overfitted to the specific characteristics of the tested datasets. This concern is evidenced by Table 6, where the ""No Prompt Ambiguity"" configuration demonstrated metrics comparable to those of A-SAM. Would it be possible that the test datasets might be biased, exhibiting little ambiguity in prompts?

Limitations:
The authors have addressed their limitations and discussed the broader impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a SAM-based framework to address the ambiguous image segmentation problem. The authors present an optimization framework based on a conditional variational autoencoder, which simultaneously models the prompt and the granularity of the object using a latent probability distribution. This approach allows the model to adaptively perceive and represent the real ambiguous label distribution, enabling SAM to controllably produce a series of diverse, convincing, and reasonable segmentation outputs. Experiments on multiple datasets and metrics demonstrate the effectiveness of the method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. To the best of my knowledge and as indicated by the authors, this paper is the first work that leverages the inherent properties in vision foundation models (SAM) for ambiguous image segmentation.
2. The experimental results demonstrate impressive advantages. Compared to the original SAM, the proposed method shows significantly better performance in the presence of prompt shifts. This high level of robustness is extremely valuable in practical applications.

Weaknesses:
1. The task setup of ambiguous image segmentation in this paper is somewhat confusing for me. I have read some referenced and comparative works cited in the paper, such as [a], and found that their task objective is providing multiple segmentation hypotheses for ambiguous images. However, this paper seems to focus more on increasing the accuracy and stability of the model's output when the input prompt has noise or shifts. More explanation about the task setup is needed. Accordingly, it is recommended to include a section in the main text that introduces the task setup, which can help readers who are not experts in this research area understand the paper better.

2. The comparison with conventional ambiguous segmentation models seems unfair because most of the compared methods do not use a network structure as large as SAM. Therefore, it is unclear whether the performance advantage comes from the increased number of network parameters in SAM or from the innovative designs proposed in this paper. I noticed that some of the compared methods, such as [b], can be applied with any encoder-decoder-based segmentation models. Thus, the results of these methods using SAM as the segmentation model should also be reported and compared. This would help evaulate whether the effectiveness of the proposed model is solely due to SAM's larger number of parameters.

3. The writing structure of the paper is somewhat unclear, making it a little difficult to read. For example, the inference method is illustrated in Section 3.1, but the training method is introduced in Section 3.4. It is recommended to create a section titled “Training and Inference,” which contains two subsections that respectively introduce the training and inference methods.

Minor Problem:

1. In Line 169, `Previous research indicates that...' should have corresponding citations added.

[a] A Probabilistic U-Net for Segmentation of Ambiguous Images

[b] MODELING MULTIMODAL ALEATORIC UNCERTAINTY IN SEGMENTATION WITH MIXTURE OF STOCHASTIC EXPERTS

Limitations:
Please see the weaknesses section.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper builds a framework for amigous object segmentation on top of SAM prompted with bounding boxes, which is known to be sensitive to small prompt changes. 

The framework is based on a VAE, and the main idea is to jointly model the prompt and the object granularity with a latent probability distribution to gain more control over SAM’s output. In practice, the prompt  embeddings  and image embeddings (controlling granularity) are formulated as a distribution.
 
The method is evaluated on 3 medical imaging datasets and on a synthetic driving dataset, showing superior performance over the baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The method is the first to use a promptable large-scale pretrained model like SAM for ambiguous image segmentation
2. The methodology is in general clearly written and easy to follow, figure 2 provides a great overview of the method
3. Extensive evaluation and ablations were performed, showing the method’s superior performance compared to baselines on all of the datasets. (the method is not evaluated on any non-medical real dataset though, see weaknesses)
4. The joint modeling of promts and image embeddings of the proposed method is efficient since the probability sampling is only performed after the SAM encoder and thus the image embedding needs to be computed only once (SAM decoder is lightweight)

Weaknesses:
1. The paper contains several unclear statements or missing details, which make the reproducibility of the method difficult.
2. The evaluation is carried on a niche domain (medical) or on synthetic datasets only. It is hard to judge the performance of this method in general real-world setting.

Limitations:
Limitations are addressed in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to convert the flaws in the vision foundation model (e.g., SAM) into advantages for ambiguous object segmentation. To this end, the authors propose a novel framework that employs latent distribution and an optimization architecture. The authors validated the performance of the proposed methods through comprehensive experiments.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Unlike existing approaches that aim to stabilize the sensitivity to ambiguous objects in SAM, this paper suggests leveraging the vulnerability for ambiguous object segmentation. The proposed approach seeks to harness SAM's sensitivity, redeemed as a weakness, to address ambiguous and uncertain predictions.

Weaknesses:
1. The explanations are unclear and hard to follow. Specifically, it needs further explanation of how to extract the mean and standard deviation from the convolution blocks and how to utilize the ground truth labels in the posterior version of the prompt generation network.
2. Some symbols are used without explanation (e.g., Θ, Φ, N_i, N_p).
3. Missing reference: Previous research at line 169.
4. Since this paper focuses on clinical scenarios for ambiguous object segmentation, it seems unfair to compare the performance without including existing medical segmentation methods such as OM-Net [1], DC-UNet [2], and CE-Net [3].

[1] https://arxiv.org/pdf/1906.01796v2
[2] https://arxiv.org/pdf/2006.00414v1
[3] https://arxiv.org/pdf/1903.02740v1

Limitations:
The authors address limitations of this work and broader impact properly.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
vJMMdFfL0A;"REVIEW 
Summary:
This paper introduces a technique called iterative data balancing—altering data distributions to match predefined marginal distributions—that can lead to variance reduction in model predictions. The authors highlight its utility for self-supervised learning, which has been used to train several foundation models. The results demonstrate that iterative rebalancing of data leads to improvements in zero-shot learning performance and a reduction in variance among the empirical marginals with more than one iteration (k>1) of their technique.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper has theoretical contributions that include the derivation of non-asymptotic bounds that quantify the variance reduction achieved through their data balancing technique. The authors also present empirical studies that demonstrate the effectiveness of their proposed balancing technique. The authors discuss the utility of data balancing across different tasks, such as image-caption pair matching and self-supervised clustering, identifying the utility of their approach. Their approach has the potential for adoption in various domains, including in the training of foundation models.

Weaknesses:
The authors could expand the range of experiments to include a more diverse set of tasks, which in turn could enhance the generalization of the findings. Furthermore, their iterative data balancing technique relies heavily on predefined (uniform) target marginal distributions (see questions about this in next section). Finally, the iterative nature of the proposed data balancing technique may introduce significant computational demands. The paper could benefit by a more comprehensive overview of how the iterative technique computational overhead is impacted by very large datasets and/or models.

Limitations:
I don't feel the authors addressed the limitations of their work significantly, aside from noting that their methods were different as those ""in practice"". As already mentioned in the questions section, how robust are your results to imbalance and uncertainty in the target marginals? Furthermore, are there situations where your technique fails to improve the empirical results? If so, what are they?

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the use of data balancing in various self-supervised learning (SSL) frameworks. The authors argue that this iterative algorithm, which is typically used to avoid representation collapse in SSL models, also provides a benefit of reducing the variance of empirical functionals of the distribution over data sources. The paper establishes non-asymptotic bounds quantifying this variance reduction and relates them to the eigendecays of specific Markov operators.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. the paper provides a new perspective on the benefits of data balancing
2. provide different examples of data balancing in practice and prove a non-asymptotic bound on the MSE of balanced estimators.
3. The findings may have implications for improving SSL models

Weaknesses:
1.the experiments are somewhat limited in scope
2.adding more visualizations or intuitive explanations may be better for understanding the key finding of the paper.
3.will the assumptions limit the applicability of the findings?

Limitations:
In checklist the author mentions that the work is primarily theoretical and has no societal impact. but it will be better to discuss the positive social impact. The author also mentions the limitation that the setting studied has some dissimilarities with practice but haven't addressed the limitations yet.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work focusses on data balancing strategies in context of self-supervised learning. The main claim of the paper is that data balancing, commonly used to avoid representation collapse, has a variance reduction effect. The authors introduce an upper bound on the MSE of a balancing estimator, relating it to empirical risk minimisation. The main paper covers the key elements of the proofs, which is given in detail (and is extensive) in the appendix. Experiments are conducted to illustrate the impact of data balancing on examples described in the paper.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper attempts to shed light on SSL training and the role of data balancing. The paper formalises the problem and develops extensive theory. The main results is pretty cool and insightful in the sense that the upper bound on the MSE shows that data balancing has a variance reduction effect. The topic is of interest to the community and the work is focussing on a poorly understood paradigm that is becoming dominant.

Weaknesses:
I have three main concerns with this work:

1/ The theory is *very* extensive. The Appendix contains several pages of proofs that are difficult to parse and come on top of the formalism presented in the main paper. It seems like the main body could be simplified and made more to the point to convey the main gist of the contribution and make it more accessible.

2/ It is unclear how the data balancing examples in Section 2 map to the formalism introduced in Section 3. For example, what would (4) look like for example 1 and example 2?

3/ It is unclear what the experiments bring to the table and how they provide evidence to the main result. Making the link more explicit and explaining what are the key take aways from these results would help the reader.

Limitations:
The authors provided a brief discussion about future work at the end of the paper. The work presented here is theoretical in nature, attempting to provide new insights in existing approaches. There are not immediate implications in practice, but authors could discuss the scope of their work in more detail.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
VD3lb3XWqa;"REVIEW 
Summary:
This paper proposes a test of self-awareness similar to that of the Turing test. It starts by motivating the need for an ""objective measure"" of AI progress, given that the Turing test has been passed by LLMs. There is a brief discussion of literature on self-awareness and related topics in philosophy.  The test of self-awareness is then presented, which asks whether the system in question can distinguish its own output from external inputs. The *Nesting Doll of Self-Awareness* is presented as a generalisation of the aforementioned test. The test is then applied to autoregressive LLMs and it is found that LLMs are not self-aware as they are unable to reliably detect what text it produced. This is followed by a brief discussion on why self-awareness matters and whether humans would do well at this test.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
An interesting idea that could be developed into an interesting way of testing algorithms.

Weaknesses:
- The paper is drawn out in terms of substance. The idea of the test is only introduced on page 4.
- The writing is, at times, verbose and unnecessarily complex.
- The obvious question would be whether humans would pass such a test. Though this is briefly considered in Section 5.2, I think it is not properly discussed.
	- The author claims that this is easily solved by humans because of memory. This is not a fair comparison to LLMs. Goodhart's law would come into play here as we could easily add such a memory structure to an LLM system.
	- Would a human be able to resolve extremely generic text that any human wrote?
- The paper lacks rigorous analysis and testing that I would expect to see in a NeurIPS main track paper.

Line 29: ""Last year, however, the Turing test was broken"". I am inclined to say the Turing test was passed before and very few considered it to be a reliable indicator, even before GPT.
Line 32: ""AI has become untethered to any definitive, objective measure or permanent, agreed-upon benchmark."" There are certainly many benchmarks and standards in various subfields.

Limitations:
None are written about.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a simple test, along the lines of the Turing test, to determine whether or not an LLM (or an generative text model) is _self-aware_ (or _self-conscious_).
The general structure of the test is that the LLM participates as one of the interlocutors in a dialogue and then is asked aster the fact to identify which interlocutor it was.
What this test is primarily aiming to determine is whether the LLM can distinguish its own actions from those of ""the world"" (i.e., the other interlocutor).

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
### Originality
- (major) I am not familiar with the literature surrounding the philosophy of LLMs, but it seems like the paper is addressing this topic from a unique angle.
### Quality
- (minor) The self-awareness test operationalizes a notion of self-awareness.
- (minor) The experiments are straightforward and indeed test some notion of self-awareness.
### Clarity
- (minor) The paper is mostly easy to read.
### Significance
- (minor) This tool could be useful for for testing certain otherwise ""soft"" claims of whether a model is self-aware or not.

Weaknesses:
### Originality
- (minor) I did find this paper (https://arxiv.org/pdf/2401.17882) which seems relevant, although it is on the recent side, so I don't think its critical that there is an extensive comparison (although a brief would be nice).
### Quality
- (major) It seems like the notion of self-awareness which the proposed test measures is relatively narrowed or not well contextualized by the rest of the paper.
### Clarity
- (major) The paper needs to distinguish related concepts of related to self-awareness and adjust its discussion accordingly (see Questions).
- (minor) Quantitative summary of the results should be in the main paper.
### Significance
- See Quality.

Limitations:
I think the paper could use a more extensive and explicit discussion of the limitations of a test of self-awareness. For example, the test may be testing one notion of self-awareness is that ""less significant"" than some other notion, but people could misunderstand what it is attesting and over-ascribe capabilities to LLMs that they do not actually possess.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to answer the profound philosophical question of whether the state-of-the-art, transformer-based large language models (LLMs) pose self-awareness. As the author points out in this paper, this question, which I agree to be legitimate and important, is rarely addresses in a rigorous, academic manner, which renders off-the-cuff and often-sentimental discussions on social media the primary forum for its discussion. Starting from a well-written introduction that nicely sets the philosophical background for this question, the authors proceeds to present a dialog-based approach to probe self-awareness of LLMs. This approach can be concisely summarized as ""a binary selection task, namely identifying one of two conversing roles as the LLM itself in a multi-turn dialog, while controlling for the potential confound of role labels"". This approach is then applied on two of the most popular publicly-available LLMs, including llama3-7b-instruct and GPT-3.5-turbo-instruct, the result of which showed that the accuracies of the LLMs on this task are low and often at a near-chance level under challenging labeling schemes. The author therefore concludes that the current generation of LLMs do not possess self-awareness.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
S1. The intellectual audacity of this paper is laudable and impressive. As the author points out, it is imperative to address important philosophical questions such as self-awareness (the focus of this paper) at this stage in the development of LLMs and generative AI, where transformer-based language models can exceed human performance in many tasks involving natural language, including passing of the Turing Test. Yet there is a lack of systematic and rigorous effort in this topic in the field of LLMs and AI as a whole.

S2. This paper is written in fluent and idiomatic English language, demonstrarting the author's good command of philosophy of mind and its history. As such, this paper is easy to follow and a pleasure to read. Wielding this good knowledge of philosophy of mind, the author clearly spells out a working definition of self-awareness in this paper and skillfully avoids the potential pitfalls of entangling with complex and controversial topics such as consciousness and free will.

Weaknesses:
W1. Despite the strengths mentioned above, and contrary to the claims made by the author in this paper, the role-identifying methodology devised by the author in this paper is problematic as a test for self-awareness. Below I will lay out the rationales behind this critique of mine:

  1. Being able to identify which of the two interlocutor is ""the LLM itself"" is different from being aware of the fact that the said self is participating in a conversation. One one hand, the ability to perform this identification task is not a sufficient condition for self-awareness. One the other hand, neither it is a necessary condition for self-awareness. To see why it is not sufficient for self-awareness, follow the thought experiement in Section 5.2 of the paper. Suppose there is another human, e.g., the spouse or a close friend of the author of the hypothetical text (or the conversing self in the original problem formulation). It is totally conceivable that such a person, despite being a different individual (i.e., not the ""self""), would be able to identify the text written or uttered by the person of interest with high accuracy. If this is the case, can we say that the other person is ""self-aware on behalf of the person of interest""? Such a conclusion would be absurd. But by the same token, if a human person, or more relevantly, an entity such as an LLM, performs the identification task with high accuracy, it cannot be ruled out that such as system is simply good at this identification task per se, perhaps due to a good memory of previously-composed text or perhaps due to a certain mechanism that allows they/it to algorithmically execute this identification. These two possibilities are entirely feasible within the current technology surrouding LLMs. For example, one can add a memory cache to an LLM to store all the text generated by the LLM, and give the LLM access to this cache during subsequent text generate (i.e., a form LLM tool use or retrieval-augmented generation or RAG). Would this augmentation constitute a legitimate form of self-awareness? As another example, one can also write a program that uses the LLM to score the tokens from a turn of a dialog in a token-by-token fashion, and therefore assign an overall score to each turn of the dialog. Based on the scores, the program, built on the LLM core, would be able to identify the self role accurately. But would we be willing to call such a program (containing the LLM as a part of it) self-aware? In my opinion, answering yes to either or both of the two previous questions would effectively give self-awareness too general and perhaps too trivial a definition, in a way similar to acknowledging that someone who can identify a certain person's words with high accuracy is ""self-aware for the person"".
  
  2. To see why the ability at this role-identification task is not necessary for self-awareness, consider a human who has dyslexia and a form of amnesia that renders them 1) unable to comprehend visually-presented historical text and 2) unable to remember what was previous written or spoken by themselves (or by others), but is otherwise cognitively and linguistically normal. When faced with this identification task, such an individual would struggle at this role-identification task, but they are nonetheless self-aware at the moment when they are writing or uttering words. This is due to the presence of the ""efference copy"" in the intact sensorimotor loop of the individual's brain (cited by the author in Section 6 of the paper). Furthermore, such an individual would also be self-aware during other, non-linguistic activities thanks to motor efference copies and proprioception and other well-established sensory mechanisms of human self-awareness. This analogy illustrates the point that a back performance by an LLM at the role-identification task does not form a solid basis for claiming the LLM lacks self-awareness. The LLM may be self-unaware due to other reasons, e.g., the lack of an efference-copy mechanism during the auto-regressive inference that is comparable to efference copies seen in the human brain.

Limitations:
See weakness W1 and questions Q2 I wrote above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes an objective inventory for testing the self-awareness of an artificial intelligence agent. The core idea is to test whether an agent can distinguish content it has produced from content originating from the external world. The experiments show that no large language models have demonstrated self-awareness.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Self-awareness is an essential topic both philosophically and scientifically, especially in the era of AGI. A clear, objective, and commonly accepted test is needed. This paper endeavors to address this need.

Weaknesses:
1. The test proposed here is not sound at all. Can a human always distinguish the words they produce from those produced by others if their memory is impaired?

2. I am particularly confused by the concept of ""nesting doll."" In my opinion, this should be an analogy for a system in which each level can always contain, reflect, or dominate the previous level, which should be the essence of self-awareness. However, here I do not understand how thoughts, interoception, or material possessions are related to this concept.

3. The writing style is far from the standard expected for a NeurIPS conference paper. The paper is filled with informal expressions that belong in a blog or a Twitter debate, rather than a conference paper. For example, lines 49-50 (""even if you object...""), lines 69-70 (the Oedipus metaphor is hard to understand), line 155-156, line 166 (using a fictional movie for metaphor), and lines 172-176 (does the arm-moving example have anything to do with the rubber-arm experiment?). I acknowledge that it is challenging to present a novel work on self-awareness within a traditional writing paradigm, but the excessive use of imprecise metaphors and informal philosophical musings is unacceptable. A more formalized and precise expression is expected.

Limitations:
No.There are some obvious flaws in the proposed test that the author chose not to address.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
V5Sbh42uDe;"REVIEW 
Summary:
This paper proposes a method to improve weakly-supervised semantic segmentation using sparse annotations. The authors introduce a Potts relaxation method, which is an extension of the traditional CRF-like methods. The experiments are conducted on the PASCAL dataset.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Pros.

1.	The proposed method appears to be technically sound.

2.	The authors provide detailed explanations of their method.

Weaknesses:
Weakness:

1.	The comparisons with other methods are not comprehensive. Recent advances [1,2,3] in sparsely annotated semantic segmentation are not discussed or compared. Point-supervised is more challenging but it is ignored. The survey in the paper is limited, making it difficult to understand the contributions.

2.	The use of a small dataset like PASCAL may not demonstrate the superiority of the proposed method. Results from larger datasets like Cityscapes and ADE20K (Table 6) should be emphasized and compared with state-of-the-art methods.

3.	Self-labeling is also well employed in many weakly-supervised methods that use image-tags labels. It seems that your method can also applied to them, am I right? If so, experiments on the COCO dataset would be important.

4.	DeepLab is an old-fashioned network architecture. The authors should prove that Potts relaxation can also work on Vision Transformer since ViT has demonstrated the SOTA performances in both fully- and weakly-supervised semantic segmentation. If using ViT without Potts can already get good performances, the relaxation may be not important.

5.	In recent fully- and weakly-supervised semantic segmentation works, CRF-like methods have been discarded due to their computation cost. The efficiency of Potts relaxation (FLOPs) is not evaluated in your paper, which is very important. 

6.	With complex formulation, on the very toy dataset PASCAL, the mIoU only increases by 1% in Table 5 (77.1 to 78.1), while the increased computation cost has not yet been evaluated. 

7.	As stated in the abstract “ … can outperform full pixel-precise supervision on PASCAL”, which is not convincing to me. The fully-supervised performance should be considered as the upper bound of weakly-supervised learning. Such results may be caused by unfair settings. 

8.	The importance of relaxation should be introduced in the abstract since it is your main contribution. In Section 2.1, the two claimed reasons are not intuitive to me. “Manage the scope of this study” seems not a strong motivation. 

9.	With the development of ViT and vision pre-training, the improvements of CRF-based self-labeling become marginal. Thus, I think it is important to evaluate ViT and vision-pretraining (DINO[4]) backbone with your relaxation. I am wondering whether Potts can improve performances beyond these strong backbones.

Overall,  I think it would be better if the authors could conduct more comprehensive experiment comparisons and related work discussions. The motivation for relaxation should be introduced in the beginning.

Refers:

1.	Sparsely Annotated Semantic Segmentation With Adaptive Gaussian Mixtures. CVPR 23

2.	Label-efficient Segmentation via Affinity Propagation. NIPS 23

3.	Modeling the Label Distributions for Weakly-Supervised Semantic Segmentation. Arxiv 24

4.	DINOv2: Learning Robust Visual Features without Supervision. TMLR 24

5. CC4S: Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation. TPAMI 24

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper considers semantic segmentation under scribble supervision. The paper studies relaxations of the Potts model and proposes a framework for generating soft pseudo-labels, which benefit over hard pseudo-labels in that they can represent uncertainty. The paper highlights problem cases with two standard relaxations, the quadratic and bilinear, and proposes a normalized quadratic relaxation. Moreover, the paper proposes to use a collision cross-entropy loss between the prediction and the pseudo-labels. Different settings are evaluated experimentally, and the proposed approach is compared to the state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Overall, the paper is well written, and the proposed approach is intuitive and should be fairly easy to reproduce, even without code.

The problem under consideration is important as it aims to reduce the manual annotation challenge in image segmentation, which is otherwise costly and time consuming.

The choice of Potts relaxations and cross-entropy terms are supported by experiments, and the proposed approach is further compared to previous methods, showing strong performance.

Weaknesses:
Some details are missing or unclear in the main paper. Considering that the soft self-labeling loss in (6) is a key contribution, it would be useful to include some details regarding the optimization of the pseudo-labels in in the main paper from Appendix B. Additionally, pairwise affinities based on intensity edges are mention at line 42, but it is unclear whether they are used in the proposed approach, see questions.

The proposed approach is only evaluated on a single dataset in the main paper. The results on additional datasets in the appendix should be moved to the main paper to better communicate the empirical findings, as they are easy to miss otherwise. This also raises some confusion about Appendix C which says that three datasets are used in Section 3.5, but results are only reported on PASCAL. What was the reason to exclude these from the main paper?

No error bars. Considering that some settings have fairly similar performance, e.g. in Table 3, standard deviation or similar over multiple runs would be useful.

Some figures have excessively small fonts, e.g. Figures 3, 4, 6.

Limitations:
The paper does not discuss limitations.

Societal impacts are not discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes a soft self-labeling framework for weakly supervised semantic segmentation using scribbles. This model-agnostic framework requires only the joint optimization of network predictions and pseudo labels, guided by specific loss functions: collision cross-entropy and log-quadratic Potts relaxations. The design choices are supported by theoretical concepts and experimental results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The work systematically analyzes common loss functions for weakly supervised semantic segmentation. By investigating theoretical concepts and experimental results, it discusses the advantages and disadvantages of these loss functions. Based on this comprehensive analysis, the work concludes by recommending the use of collision cross-entropy and log-quadratic Potts relaxations for a soft self-labeling framework.

Weaknesses:
Method
- The proposed method is tested on DeepLab. Although the theoretical concept should hold for any model, its effectiveness on other segmentation models remains unknown. 
- The work explains the rationale behind design choices from a theoretical perspective, but it does not clarify why these choices lead to specific pseudo-labels from a vision perspective. For instance, in Figure 5, NN successfully segments the bicycle while DN does not. Is this because the bicycle is a minor class? 
- The work does not provide mIoU per category, leaving it unclear whether the method is effective for all categories or just a few major ones.

Minor Writing Issue:
- The legends of the figures, such as Figures 1, 4, and 6, are small.
- Figure 1 (b) contains an extra icon (house).

Related works:
- There are more image-level WSSS works than just [4, 21]. The author should consider including additional relevant works in lines 16-18.

- Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks
- Weakly supervised learning of instance segmentation with interpixel relations.
- Extracting class activation maps from on-discriminative features as well.
- Boundary-enhanced co-training for weakly supervised semantic segmentation.
- Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation
- Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation
- Group-wise semantic mining for weakly supervised semantic segmentation

Limitations:
I agree with the author that the training time is one of the limitations.
I hope the author can consider my suggestion in the above section to build a link between the design choice and the pseudo labels from the computer vision perspective.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new framework for Weakly-Supervised image Segmentation. The main contribution is to use a soft-labelling approach which is considered superior to classic hard labelling because it can keep track of the centanty of the label. Then different forms of second order potts relaxation and cross-entropy are evaluated.
Results show that the proposed approach with its best setting is able to perform better than previous approaches and comparably to a fully supervised approach with only 3% of annotated pixels.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
\+ The paper is in general well written and easy to follow

\+ Nice illustrations help to understand the proposed approaches

\+ Ablations on the most important components help to understand the significance of each component

Weaknesses:
\- It is not clear if results are significant. For instance is 1% a meaningful improvement or it can be generated by just noise? It is important to see results on multiple runs with also standard deviation.

\- The motivation about soft-labelling because it keeps information about the label certainty is not clear to me

\- In tab. 5 it is not clear what is the base model you start from. Could you report also the base model with standard cross-entropy and hard potts model? It seems that results are very good because the baseline is already quite good.

\- Authors did not say much about the computational cost of the proposed approach compared to common models.

Limitations:
Authors did not mention the possible limitations of the proposed approach.
One possible limitation is the need of additional hyper-parameters to tune which can take time. Authors should comment on that.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
UTwuDTpdNO;"REVIEW 
Summary:
The paper addresses the vulnerabilities of Federated Learning (FL) systems to various adversarial attacks, including model poisoning and backdoor attacks. The proposed solution is a Meta Stackelberg Game (meta-SG) framework designed to offer robust and adaptive defenses against these attacks. The approach formulates adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) and employs a meta-learning method to find optimal defense strategies. Theoretical analyses show the algorithm's efficiency in convergence, and empirical results demonstrate its effectiveness against different attack types.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed framework considers both untargeted and targeted attacks.

2. The paper uses RL to realize online adaptation which is close to the real world.

3. Inspired by meta-learning, the method is robust to unknown attacks.

4. In the pre-training phase, the paper uses generated data to decrease the concern of privacy leakage.

5. The paper provides sufficient experimental results.

Weaknesses:
1. Could you explain more about the necessity of adding the gradient adaptation (from BSE to meta-SE)? Although BSE is ex-ante when knowing the distribution $Q$, the model $\theta$ is changed during training and it could capture emerging information. Could you provide empirical results comparing BSE and meta-SE to show the advantage of meta-SE?

2. Considering adaptive/mixed attacks, the paper misses two relevant frameworks: MixTailor [1] and RobustTailor [2]. They can adjust aggregation methods during training. Especially, RobustTailor also simulates a game between the defender and the attacker, and it proposes a mixed attack. This kind of method could be included in experiments as a baseline.

3. Because the whole method is complicated with 2 stages, comparing computational cost with other baselines is necessary.




[1] Ramezani-Kebrya, Ali, Iman Tabrizian, Fartash Faghri, and Petar Popovski. ""Mixtailor: Mixed gradient aggregation for robust learning against tailored attacks."" arXiv preprint arXiv:2207.07941, 2022.

[2] Xie, Wanyun, Pethick, Thomas, Ramezani-Kebrya, Ali, and Cevher, Volkan. ""Mixed Nash for Robust Federated Learning."" Transactions on Machine Learning Research. 2023.

Limitations:
The authors mentioned limitations in Section 5. The main one is the privacy concern.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper titled ""Meta Stackelberg Game: Robust Federated Learning against Adaptive and Mixed Poisoning Attacks"" proposes a new approach to enhancing the security of Federated Learning (FL) systems. The paper identifies that existing FL defenses are inadequate against adaptive and mixed attacks. To address this, they introduce a Meta Stackelberg Game (meta-SG) framework, which employs a Bayesian Stackelberg Markov game (BSMG) and a meta-learning approach. This framework aims to provide a robust and adaptive defense mechanism against various poisoning attacks, including model poisoning and backdoor attacks. The proposed method is theoretically proven to converge to an equilibrium efficiently and is empirically validated to perform well against strong adversarial attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ Introducing the Meta Stackelberg Game (meta-SG) framework is innovative, offering a new perspective on defending against adaptive and mixed attacks in FL. 

+ The paper provides a solid theoretical foundation, proving that the proposed algorithm converges to a first-order ε-equilibrium, proving the method's efficiency. 

+ Extensive experiments demonstrate the effectiveness of the meta-SG framework, showing significant improvements in defense against various attack types compared to existing methods. 

+ The meta-learning component allows the defense mechanism to adapt dynamically to different attack scenarios, enhancing its robustness in uncertain environments.

Weaknesses:
- The proposed approach seems computationally intensive, requiring significant resources for pre-training and adaptation, which might limit its practicality in real-world applications. Although it proves that it can converge in Theorem 3.3, it would be beneficial to have empirical evidence, such as the method's run-time overhead. 

- While the framework is tested against several attack types, the scope of attacks considered might not cover all possible real-world adversarial strategies, limiting the generalizability of the results. The paper especially makes it unclear what attacks are used in pre-training, whether they are the same, and how different they are compared to the real FL environment when testing and generating results. 

- The proposed method's scalability to larger and more diverse FL environments remains unclear, especially given its computational demands.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a game theoretic model for robust federated learning. The technique is composed of pre-training and online adaptation. During pre-training, a meta-policy for the defender is solved as a Bayesian Stackelberg Markov game. The defense policy is further polished during the online adaptation stage.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The Stackelberg game is designed to counter unknown/uncertain attacks by an adaptive adversary. 

Theoretical bounds for sample complexity are provided. 

Empirical results demonstrate the effectiveness of the proposed technique.

Weaknesses:
The main weakness is the slight violation of privacy as the technique needs a portion of ground truth data from the clients. This has been disused as the limitations in the paper. 

Minor comments:

On page 2, ""including mixed attacks ,"" ---> extra space

Limitations:
Privacy violation is mentioned as a limitation of the technique. It's unclear whether future development can remove the dependence on the client-side ground truth.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a defense mechanism in federated learning that has adaptability inspired from meta learning. The authors formulates a Bayesian Stackelberg Markov game (BSMG), focusing on addressing poisoning attacks of unknown or uncertain types. The authors propose an equilbrium inspired by meta learning and then look at a local version of that. Empirical evaluations are done on MNIST and CIFAR.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper seems to have interesting results.

Weaknesses:
The writing is sloppy in many places and quite a few things are unexplored.
Federated learning's primary motivation is privacy, so the privacy loss from a core small dataset must be analyzed. The authors do acknowledge the loss but IMO that is not enough.

Behind all the motivation of federated learning, the core idea is in the equilibrium proposed - IMO, exploring this equilibrium in more detail would make the paper stronger (in fact, the problem makes sense even in simpler adversarial learning problems).

Is Def 3.1 just a differential equilibrium, in the style of ""Implicit Learning Dynamics in Stackelberg Games: Equilibria Characterization, Convergence Analysis, and Empirical Study"" but missing second order conditions?
Why are there no second order conditions? Just first order may not induce a local equilibrium, which is a meaningful equilibrium to achieve. This is where even more meaning needs to be discovered for the first meta-SE. The authors compare it to PBE in the appendix, but the notations of belief consistency and sequential rationality is what makes PBE (SE) convincing. Without any such (or similar) notion, a new equilibrium in a dynamic setting is not principled.

I do not understand why the ball $B(\theta^*)$ is used with a bound of 1? What is special about 1? (same for the other ball)
Proposition written informally (and no explanation) in the main paper does not make sense (e.g., Prop 3.4).

There are many typos when I started to look in appendix:
1) Eq F6, $\tilde{l}$ should have two inputs
2) Line 959, the parameters of $\tilde{l}$ is lost, without this the equations with $\theta'$ on left and no $\theta'$ on right is not well-defined.
3) I do not understand how the equation in line 964 came about - first it is said that it is an inequality but what is written is an equality.

Overall, typos do not inspire confidence.
Also, any defense mechanism should itself be subject to attack with the adversary possessing knowledge of defense mechanism - I do not see that in experiments.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers the problem of backdoor/poisoning attacks in federated learning (FL). In this setting, a single attacker has control over all malicious clients trying to employ different attack types (on each controlled client). This paper aims to create a defense mechanism against such adaptive attackers. To this end, the paper proposes a game-theoretic approach, which contains two stages: (1) Pre-training stage: before the FL environment, the defender first learns a good defense policy by simulating that environment using a small amount of truthful data against a simulated attacker with known possible actions (e.g. attack types used), and (2) Online-adaptation stage: the defender leverages the pre-trained policy and adapt it against the attacker at the real FL environment. This paper demonstrates the effectiveness of their proposed mechanism, as well as considers ablation studies where the previous assumptions do not meet: adaptation to attack methods at real FL environment are different (but similar) from ones seen at the pre-training stage.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is well-written and easy to follow.

Weaknesses:
I have some concerns, mostly about the practicality of the assumptions:

1. Assumption on the accessibility of data: the paper assumes that in the pre-training phase, the defender has access to a small amount of data, which is used to model the data distribution of clients using generative models. (1) It goes against privacy. (2) It is not clear that a small amount of data is representative enough to model client data distribution. (3) Things will be even more complicated if each client has its sub-population (data) that is (reasonably) different from each other. In this case, which malicious clients the attacker has control over will matter. 

2. Assumption of the similarity of attack types at the online adaptation stage: the paper assumes that the attack types in a real FL environment, though unseen, should be similar to those of the pre-training phase. This seems impractical, especially in a white-box setting, where the attacker will try to leverage the property of the defense mechanism to create an adaptive attack that is specified for that defense mechanism (see Carlini's works).

I also have concerns about the experiments:
1. Datasets and models used: Would it be possible to use more practical datasets and models instead of MNIST/CIFAR10 and ResNet-18? I would like to see results when the data distribution is complex enough that generative models cannot easily learn with a small amount of data. Right now, the amount of data used is still considerable, considering the dataset used. This would make the assumption of the accessibility to a small amount of data in the pre-training phase more persuasive.

Some comments on writing/paper organizations.
1. In Table 1, please highlight which results are the best. It would be more readable and easier for comparison. 
2. In Figure 2, it might be better to show smoothed curves.
3. In Appendix F, before each theorem/lemma/assumption, it would be better if an intuition/proof sketch for each one is provided. Also, if the proof technique/assumption is standard, please mention the corresponding references.
4. In the Conclusion, I think it would be more honest if explicitly stated that the major limitation of this paper lies in the practicability of the assumption. Right now, I only see privacy concerns mentioned, which is misleading. 

(Not really a weakness) It could be nice if source code is included (might be using an anonymous repo). 

Overall, I think this is a good paper if ignoring the practicability of the assumptions on the attack types/data (on the accessibility in the pre-training phase/distribution of accessed data in the pre-training phase/distribution of data in each client). I also did not find an experiment where the attacker leverages the information about defense mechanisms to instantiate a better attack scheme (white box attack). It is known that many proposed defense mechanisms failed in this scenario, though it seems obvious that it will go against the similarity assumption on the attack types at the online adaptation phase. However, I am also aware of the hardness of defense tasks in adversarial machine learning and it is good to have some initial results even under strong assumptions. I will try my best to be reasonable. Maths were not checked carefully, I will try to go into the details in the rebuttal phase.

Limitations:
See Weaknesses above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";0
URkFX1jmOd;"REVIEW 
Summary:
This paper presents an approach, namely N2D3, for night-to-day image translation. Specifically, the proposed pipeline involves two stages: illumination degradation disentanglement and degradation-aware contrastive learning. The first stage decomposes an image into darkness, well-lit areas, light effects, and highlight regions. The second stage applies contrastive learning to these four types of nighttime degradations. Extensive experiments conducted on the BDD100K and Alderley datasets demonstrate that N2D3 outperforms existing methods.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The paper addresses the critical problem of night-to-day image translation in computer vision.
- The authors provide comprehensive experimental validation of the proposed method.

Weaknesses:
The reviewer has raised this paper for an ethics review due to a significant omission of a key citation. In Section 3.1, the authors introduce a color invariant term for light effect detection. However, this term was originally derived by Geusebroek et al. in their paper *Color Invariance* [1]. The authors devote an entire page to deriving the invariant term without appropriately citing the original work, which violates academic integrity. The authors should explain why this citation is missing, as it does not seem to be an unintentional oversight. This intended missing reference also made Eq. (1)-(5) lack logical coherence and hard to follow.

[1] Color Invariance. J. M. Geusebroek, R. van den Boomgaard, A. W. M. Smeulders, H. Geerts. IEEE TPAMI, 2001.

**Note that although the reviewer has raised the ethics review flag, the reviewer’s rating does not take this into account.** 

In addition to the missing citation, the reviewer has concerns about the technical soundness of the paper. Specifically, why are four types of degradation considered? Since the disentanglement of well-lit and light effect regions is the paper’s main contribution, ablation studies using only three types of degradation (darkness, well-lit, and highlight) should be provided.

Besides, the paper’s citation style is inconsistent. For instance, citations for the same conference sometimes include the abbreviation and publisher while others do not (e.g., [1], [19], and [26]). Additionally, some citations include the month of the conference while others do not (e.g., [24], [28]), and some contain volume information while others do not (e.g., [22], [23]). Ensuring consistent citation formatting would enhance the paper’s overall presentation quality.

Limitations:
The authors have adequately discussed the limitations of the work, and this paper does not have any negative social impacts.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new framework N2D3 for solving night to day image translation problem. Their framework consists of a physics-based disentanglement module and a contrastive learning module for preserving semantic consistency. Their method shows improved performance in terms of FID and downstream task performance on BDD100K and Alderley dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Using the Kubelka-Munk theory for different degradation types and applying a patch-based image translation is a novel method. 
- The figures are well-made. For instance, the visualization in Fig. 1 and Fig. 2 are intuitive and helpful for understanding the whole architecture.
- Quantitative evaluation results are convincing, showing the effectiveness of the proposed framework in terms of various metrics.

Weaknesses:
- Clarity of the method sections can be improved. For instance, including more rigorous definitions or visualizations of what well-lit and different light effects mean and provide a motivation why it is helpful to disentangle those illumination causes separately. 
- The authors can also add proper citations to previous work when they mention “by common practice”, for instance in line 107, line 142 and line 196.

Limitations:
As the authors already mentioned in the appendix, the current physics-aware degradation disentanglement module is designed mostly for illumination related effects and does not handle other types of degradation such as raindrops. I wonder how the authors think the framework could benefit or inspire other types of adverse weather image restoration tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a comprehensive solution for Night2Day image translation by leveraging physical priors, photometric modeling, and contrastive learning, leading to state-of-the-art performance in visual quality and downstream vision tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors develop a photometric model based on Kubelka-Munk theory to extract physical priors from nighttime images. This model helps to disentangle different types of illumination degradations by analyzing the illumination distribution.
Overall, the paper presents a novel approach to handling nighttime image translation by considering the unique challenges posed by varying degradations and employing both physical modeling and advanced learning strategies to address these challenges effectively.

Weaknesses:
1. The writing is difficult to understand. The explanations and derivations for Eqs (1) to (5) lack logical coherence and necessary references, making them hard to follow.   The derivations for Eqs (7) to (9) also lack supporting references, casting doubt on their validity. 
2. The motivation for DAR is unclear. Please explain the motivation behind it.
3. There is no baseline network, making it difficult to determine the performance gain for the specific module. 
4. The ablation experiments lack in-depth analysis. For instance, there are no ablation experiments to verify the impact of introducing four regions for disentanglement versus three regions (e.g., excluding the light effects region).
5.There is a need to compare with more recent methods for unpaired image-to-image translation, such as COCO-FUNIT, StegoGAN, GP-UNIT, etc. Please check the reference [5], as it does not seem to be published in CVPR.

Limitations:
The motivation behind some methodological choices, is not clearly explained.
The ablation experiments are insufficient and lack depth. 
The paper lacks a discussion on the computational complexity and resource consumption of the proposed method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes to address the night-to-day translation problem in which its learning basically can be briefly described by two steps: 1) illumination distribution as well as the physic priors built upon the Kubelka-Munk photometric model are firstly adopted to separate/disentangle the image regions into four degradation categories, i.e. darkness, well-lit, light effects, and high-light, in which such illumination degradation disentanglement is the main contribution of the proposed method; 2) the degradation-aware contrastive learning module is applied to maximize the mutual information between patches in the same spatial location from the generated image and the source image, where the anchor and its corresponding negative patches should be from the same degradation category (i.e. degradation-aware sampling) and the weights for each negative patch are determined by similar matrix obtained from the optimal transport computation (i.e. degradation-aware reweighting). Moreover, the GAN-based objective function is employed to bridge the domain gap between (generated) daytime and nighttime images. The translated images (from nighttime to daytime) are shown to have better quantitative and qualitative performance (in terms of FID) for aligning with the real nighttime image distribution.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ In addition to provide better translation performance (both quantitative and qualitative), the translated images produced by the proposed method are shown to have better structural similarity with respect to the corresponding daytime images (evaluated in Alderley dataset)  in comparison to the baselines. Moreover, the typical semantic segmentation model (pretrained on typical daytime dataset, i.e. Cityscapes) applied on the translated images (produced by the proposed method) leads to better segmentation results in comparison to being applied on the images generated by the baselines (i.e. indirect evidence showing that the images generated by the proposed better follows the daytime image distribution which the semantic segmentation model is trained on). 
+ The ablation study does demonstrate the contribution of illumination degradation disentanglement for separating the image patches into four different degradation categories.

Weaknesses:
- Although experimentally shown to be effective, the mechanism and the basic ideas behind leveraging the illuminance distribution as well as the physic priors for realizing disentanglement of four degradation categories (i.e. darkness, well-lit, light effects, and high-light) are not well explained, in which the physical meanings for Eq.1 to Eq.11 are hard to understand and follow. Basically, as such illumination degradation disentanglement is the main contribution of the proposed method, the description should be more self-contained and explanatory. 
- As the illumination degradation disentanglement plays a key role in the proposed method, it would be great to have the robustness analysis on such disentanglement if possible (i.e. how accurate is the disentanglement, is there any related dataset we could apply such analysis?) and how would the translation model learning be affected once there are erroneous disentanglement?

Limitations:
no potential negative societal impact is found.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
UGwdz3kjht;"REVIEW 
Summary:
This paper makes the key observation that existing dataset distillation methods often introduce misaligned information during both the extraction and embedding stages, which leads to suboptimal performances. In response to this observation, the authors propose a method called Prioritize Alignment in Dataset Distillation (PAD), which aims to filter out misaligned information through two steps: 1). Pruning the target dataset based on sample difficulty according to the compression ratio, and 2) using only deep layers of the agent model during distillation to avoid encoding low-level, redundant information.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper synthesize a universal framework for existing data distillation methods by abstracting those methods into two steps: 1). Information extraction and 2) information embedding. Furthermore, it identifies a common theme of information misalignment in both steps. This observation enhances the understanding of current limitations as well as provides a clear direction for future research. 
2. The method presented in this paper effectively combines known conclusion from two distinct areas of research: data selection and representation learning. By leveraging established principles from both domains, the provide improvements to existing data distillation methods. More importantly, their analysis builds on developing an understanding of data distillation and the underlying mechanism: 1) small datasets require simple data 2) large datasets do not benefit from low-level information/features.

Weaknesses:
1. The proposed method’s filtering of information extraction is supported experiments shown in Figure 2. However, in practice, the method introduces two sets of hyper parameters - initial ratio and data addition epoch. The sensitivity to these hyperparameters (especially AEE shown in Table c) relative to the incremental performance gain presents a challenge, as running AEE can be complex and time-consuming (involves retraining the agent). This sensitivity and the associated tuning complexity could hinder its practical adoption in larger-scale datasets. 
2. The proposed method is adapted from the DATM framework with modifications at enhancing information alignment. However, the performance improves over DATM (shown in Table 1 and in the cross-architecture generalization results in Table 3) are not significant. This marginal improvement raises concerns about the practical value of the proposed changes, as they may not justify the added complexity.

Limitations:
Yes, the authors adequately addressed the limitations of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to study the information misalignment problem in dataset distillation.
It proposes two basic pruning strategies: (1) learn the synthetic data with easy real samples first, and gradually change to harder samples, and (2) only match deep layers of the network during trajectory matching. The proposed method could enhance the current method in a wide range of dataset settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written.
- The motivation is reasonable.
- The idea of using a scheduler to dynamically adjust the real sample difficulty is smart.

Weaknesses:
1. The experimental observations to support the two strategies (Information Extraction and Information Embedding) involving Figure 2 and 3, is not sufficient. Experiments on a wider range of datasets and more pruning ratios could rationalize the method. And a comparison of discarding deeper-layer parameters is missing.
2. The wide existence of difficulty-aware dataset distillation could **potentially** weaken the contribution. Some discussion is appreciable:
```
[1] Prune Then Distill: Dataset Distillation with Importance Sampling
[2] Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection
[3] (DATM) Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching
[4] On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm
```

Limitations:
The authors have discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors claim that existing data distillation methods introduce misaligned information, so they propose Prioritize Alignment in Dataset Distillation (PAD). PAD prunes the target dataset and uses only deep layers of the agent model to perform the distillation, achieving state-of-the-art performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is somewhat well-written and mostly easy to follow. And the tables/figures are well-demonstrated.

2. The authors analyze the misaligned information from two perspectives and propose method. 

3. PAD achieves improvements on various benchmarks, achieving state-of-the-art performance.

Weaknesses:
The performance gains brought by the method proposed by the authors are subtle and limited, potentially attributable to other explanations. For instance, as mentioned in [1], discarding original data in certain ways, or even randomly, can yield minor performance improvements under different IPC conditions. Or tricks mentioned in [2]. 

The trend changes in Figure 2 are not pronounced, and there are even instances where the trends contradict the explanations. Could additional test ratios or test IPCs be included to validate the findings?

[1] Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection.
[2] Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality

Limitations:
Due to the limitation of computing resources, the authors only validated their method’s effectiveness on DATM, DM, and DC.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
UGKgoAZuuL;"REVIEW 
Summary:
This paper finds that existing methods in continual contrastive self-supervised learning (CCSSL)--a class-incremental learning scenario where the data is unlabeled--overlook contrasting data from different tasks, leading to inferior performance compared to the joint training upper bound. The authors propose to sample external data that are similar to each of the learned tasks to augment learning the current task. The self-supervised learning (SSL) objective on the union of the selected external data and the current-task data encourages the model to distinguish the current task and the learned tasks better. 

The authors perform experiments with ResNet-18 and (mostly) BarlowTwins on CIFAR-100 and ImageNet-100, with a mix of other datasets as the external data. The authors find that their method, BGE, consistently improves existing CCSSL methods that do not perform inter-task discrimination. Differently, the joint training model does not benefit from external data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The finding that existing regularization-based CCSSL methods overlook inter-task discrimination and the proposed method leveraging external data are novel.

2. The experiments performed in the analysis section (Sec. 4.3) provide insights into why the method works and are interesting to me, especially on whether the benefit of external data comes from positives or negatives.

Weaknesses:
1. The SSL method used is mainly BarlowTwins (except in Table 4 where SimCLR is used), which is not usually considered as a contrastive learning method because it does not contrast the anchor with negatives. I wonder if (a) providing preliminaries in the contrastive sense (Sec. 3.1), (b) including ""contrastive"" in the setting name (CCSSL), and (c) arguing that OPO enforces diversity because of findings based on contrastive learning (L#191) are misleading. I think there also needs to be some intuitions on how non-contrastive SSL methods like BarlowTwins help distinguish inter-task data since it is the one used in the experiments, and such an analysis could be very interesting.

2. My general feeling about the writing is that, although the main ideas are conveyed clearly, some claims require justification, and can be improved. Besides some big words (""much more meaningful"" in L#69, ""widely agree"" in L#138, ""extremely low"" in L#167, etc.), please see the questions below for concerns regarding specific reasonings.

Limitations:
The authors mention that their method uses external data which preserves privacy. One concern is that when the external data is not curated (e.g., scraped from the internet), there is risk that they contain private or harmful information that can be learned by the model. 

Another point is that the findings are limited to BarlowTwins (and SimCLR in one experiment) and regularization-based CL methods, and may not generalize.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper focused on continual contrastive self-supervised learning (CCSSL), highlighting that the absence of inter-task data results in sub-optimal discrimination in continual learning. The authors then proposed a method that performed contrastive learning of external data as a bridge between continual learning tasks. The proposed method achieves some improvements in a plug-in manner.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is basically well-organized and easy to follow. 

2. I appreciate the idea that continual learning should consider the inter-task discrimination, which is limited by historical samples and under explored in literature. This results in a gap between ideal continual learning performance and joint training performance. 

3. The proposed method seems to provide plug-in improvements over continual learning baselines.

Weaknesses:
1. The proposed method is essentially a straightforward extension of contrastive learning with external data, which limits novelty and technical contributions.

2. As acknowledged by the authors, the similarity of external data to the continual learning tasks is highly relevant to the performance improvements. The use of relatively different / OOD data tends to provide less improvements. Compared with the large amount of external data in use, such improvements may not be significant enough.

3. The employed external data is basically public datasets with careful pre-processing. In realistic applications, the external data in the wild (i.e., without such pre-processing) may result in additional differences and thus further limit the applicability of the proposed method.

Limitations:
The authors have discussed their limitations and societal impact.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces BGE, a novel approach to address the challenge of inter-task data comparison in Continual Contrastive Self-Supervised Learning (CCSSL). BGE incorporates external data to bridge the gap between tasks, facilitating implicit comparisons and improving feature discriminability. The paper also presents the One-Propose-One (OPO) sampling algorithm to select relevant and diverse external data efficiently. Experiments demonstrate BGE's effectiveness in enhancing classification results across various datasets and its seamless integration with existing CCSSL methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.BGE offers a creative solution to a significant but often overlooked problem in CCSSL, enhancing the feature learning process through external data.
2.The paper provides extensive experimental results that validate the effectiveness of BGE in improving classification performance across different datasets.

Weaknesses:
1.The introduction of external data may increase the computational cost and training time, which could be a limitation for resource-constrained environments. The authors may provide more analysis about the extra time comsumption problem.
2.While BGE shows promising results, the paper could provide more insight into how the method scales with the size of the external datasets, which is crucial for very large-scale problems.

Limitations:
The paper acknowledges the increased computational cost due to the use of external data. However, it could further discuss the trade-off between performance improvement and time comsumption.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors:
- argue that an optimal model for continual contrastive self-supervised learning should perform as well as a model trained with contrastive learning on the whole set of data, including negative samples taken between different temporal slices of the dataset, no just within the same temporal slice
- propose a method for using pre-existing external data to augment the temporally constrained dataset

For context on my background, I am very familiar with SSL literature, only loosely familiar with continual learning, and have never heard of continual contrastive learning before.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Researchers measure the performance of their technique on top of several existing techniques for preventing catastrophic forgetting, showing the performance of the combination of methods. It is valuable to know this.

The authors make comparison against some baselines - not just training on the joint data from scratch (non-continual learning paradigm); but also with external data added. They also demonstrate the performance with random sampling of the external dataset vs smart sampling with their algorithm, and investigate some ablations. These results help to inform where their approach provides value.

The discrepancy between the performance of a model trained with negative samples between subdatasets and with negative samples only taken within subdatasets appears to be a noteworthy observation and one which should be discussed within the continual contrastive learning community. To my understanding, the joint task as the authors suggest sounds appropriate. However, this change could be construed as changing the task that is posed to the model and moving the goal-posts (defining an easier task than is used in the literature at present), indeed as could incorporating external data. The question is in some sense what the goal of CCSSL truly is. In standard continual learning, the goal is to retain performance on previous tasks in the face of training on new tasks. This is often simulated by having classes within a dataset arrive in staggered batches. However, in contrastive continual learning there appears to be only one task (contrastive learning) and the data on which that one task is trained is merely staggered. Does the authors' approach break down an artificial barrier that was in place to simulate a harder task? Or a barrier that should not have been present in the first place and is a vestigial barrier inherited from continuous learning? This is unclear to me.

The work is generally well presented.

Weaknesses:
My understanding of continual learning is that one experimental paradigm that is used is to retain previously presented subdatasets/subtasks and to prevent catastrophic forgetting by including the old tasks in the mix while introducing a new task (e.g. [Robins, 2001](https://www.tandfonline.com/doi/abs/10.1080/09540099550039318), [Aljundi, 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf)). This setup is not considered in the paper, but it seems it would address a significant fraction of the issues the method is attempting to address with regards to the joint vs intra-only training configurations. I imagine results may still be improved by incorporating external data in the ""imaginative"" capacity before the full dataset has ""arrived"", even in this scenario. It is unclear why the authors retain this barrier (refusing to continue training on datasets $D_{0, ... i-1}$ even whilst changing the task from a series of isolated contrastive learning tasks to a joint contrastive learning task where data arrives at staggered intervals.

The paper is missing comparison to some additional baselines which would be useful to see:
- What is the performance for a model trained solely on external data, without using the continual learning dataset?
- The performance Joint+ED uses a static subset of the external data. One could also consider using Joint + a subset of size $K$ of the external data that changes every epoch, so potentially the model eventually sees all samples from ED and not just a subset.
- What would the performance be if instead of finding external data proxies for the existing data, you simply retained the previous D_i datasets from previous tasks without discarding them?


### Statistical significance

There are no evaluation as to whether the difference in results is statistically significant. This could be done over repeated runs with different seeds; experiments appear to only be performed with a single random seed. As the authors have repeated runs with different experimental paradigms which could be combined together to make a test for difference without needing to perform experiments with multiple seeds, however there may be correlated randomness between the experiments (i.e. it would be better if experiments are not all performed with the same seed, nor the same ordering of tasks; these should be held constant between comparators and varied between runs to eliminate the effect of these hidden variables on the findings).


### Figures

Fig 1: tSNE has parameters that need to be tuned correctly (perplexity in particular) in accordance with the scale of the features, whereas the more recent technique PaCMAP doesn't and typically produces better results without tuning. The lack of tuning of tSNE may impact the distribution seen in the figure, resulting in one method appearing better than the other by chance where a different choice of perplexity may have resulted in different findings. It is not clear whether the classes were cherry-picked to give favourable results for the authors' method and bad for existing methods. (I am not asserting that they were cherry-picked, but it is not indicated how the classes were selected in the paper so it is not possible to know whether they were or if these results are representative.) These points are not so important as the figure is more illustrative than quantitative anyway.

Fig 2: Font size is too small; to maintain legibility, figure fonts should be no smaller than ~70% the font size of the main text.


### Tables

Table 5: Not clear why this experiment was performed with PFR only. The experiment does not necessarily need to be run with FT and CaSSLe too, but the authors should say on what basis PFR was selected (i.e. it performs better than FT and CaSSLe).

Table 6: Not specified which method was used (FT, CaSSLe, PFR)

Table captions should indicate what the initalisms (CP, CPI, IN, INP, IND) stand for, so readers don't have to look in a distant part of the text to find out. In general, these initialisms are not intuitive - the characters are all run together and the number of characters coming from a dataset in the group is sometimes 1, 2, or 5; ""I"" and ""P"" can not be intuitive when there are multiple datasets being used that start with this character - and this makes it hard to follow the results. The table headings could be restructured to make this clearer e.g. instead of CIFAR, CP, CPI; use as headings C-10, +P365, +IN-R, which are immediately readable and convey the difference between the columns from each other succinctly.

Tables would be more readable if you used `\cmidrule` to indicate the groupings that the headings apply to, instead of having a rule across the whole table.


### Typographical

- L59 Missing word ""with them. [This] enables the""
- L68 ""performance doesn't improve even sometimes decreases.""
- L89 ""Since no labeling requirement, incorporating""
- L239 sentence is not written correctly


### Citations

Casing of initialisms is wrong on numerous citations, e.g.
- [2] vit
- [15] Pathnet
- [40] icarl
- [47] t-sne
- [48] caltech-ucsd birds

Some citations provide no location at which the paper being cited can be found, e.g.
- [48]

Some citations cite arXiv versions of papers instead of peer-reviewed versions, e.g.
- [13] https://openreview.net/forum?id=YicbFdNTTy

Limitations:
The motivation for the method is a niche of a niche. I can not see the union of these restrictions being a scenario encountered in practice. The requirements for the paradigm are:
- A large repository of unlabelled training data for this task does not yet exist to train the model on.
- A continual stream of training data for the task will become available over the course of the period of time where the model is trained (and the model subsequently refined as more data becomes available).
- A very large repository of publicly available data that is near-OOD to the domain of the task does exist.
- There is domain-shift in the continual stream of incoming data that is of a magnitude comparable to the domain shift between the stream of data and the pre-existing external data.
- Although it is fine to train our model on the continual stream of data when it arrives, for privacy reasons we want to periodically destroy the in-domain data we have collected.

This set of restrictions seem unlikely to occur in practice:
- For modalities other than vision, contrastive learning is often challenging to deploy due to its reliance on a robust, manually-curated, augmentation stack.
- For photographs of objects in the world, large datasets already exist (such as is used in the paper).
- For medical images, large near-OOD datasets are not available; furthermore, if you have the rights to train the model on data in a way that is secure and retains the privacy of that data, you do not lose those rights to access the data, so you can keep training on previously collected data.
- For personal images that are requested to be deleted from the company's database by the owner, models may be _required to forget_ the personal images, in which case catastrophic forgetting is advantageous! These requirements have created the nascent field of machine unlearning [[1]](https://arxiv.org/abs/1912.03817), [[2]](https://arxiv.org/abs/2308.07061), [[3]](https://unlearning-challenge.github.io/).

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
TWfNFCOPaK;"REVIEW 
Summary:
The paper proposes the use of the Wireless Geometric Algebra Transformer (Wi-GATr) to model signal propagation. Based on the Wi-GATr network, it introduces a differentiable prediction model and a diffusion model. Compared to traditional statistical and ray-tracing methods, the proposed approach not only addresses conventional signal prediction problems but also tackles inverse problems such as receiver localization and 3D environment reconstruction. Experimental results demonstrate the effectiveness of this method. Furthermore, the authors present two large-scale wireless signal propagation datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The perspective is interesting. This paper models wireless propagation as a probability model based on diffusion, thus offering a unified approach to address signal prediction and inverse problems such as receiver localization or 3D scene reconstruction.
2) The paper is logically structured, with a comprehensive background introduction and high readability.

Weaknesses:
1) There is a gap between the challenges and the solutions. The author asserts that wireless surrogate modelling faces challenges like data scarcity and diverse data types. However, the lack of analysis on these issues makes the proposed solutions appear abrupt. It is recommended to provide insights that lead to the proposed solutions of this paper.
2) The innovation is somewhat limited. Wi-GATr primarily extends the GATr method into a wireless setting, with equivariance being a pre-existing property of the original framework. Apart from tokenizing input data, did the paper introduce any additional advancements? It would be beneficial for the authors to highlight these aspects.
3) The experimental evaluation is not sufficiently convincing. For more details, please refer to the ""Questions*"" part.
4) There are some typographical errors in the paper. For example, lines 56 and 57 do not correspond to Figure 1. Additionally, the abstract mentions transmitter localization, but the main text describes receiver localization, among other discrepancies.

Limitations:
Sufficiently discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes the use of a transformer architecture to model electromagnetic propagation of physical systems. The approach is claimed to outperform existing methods by (i) computational efficiency (compared to raytracers) and (ii) enabling solving inverse problems. The method is evaluated on a number of benchmark tasks and the paper is accompanied by two new datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea of modeling electromagnetic wave propagation using transformer architectures is novel and interesting. 

The generality of the approach enables a large number of tasks in wireless communication systems that would otherwise the use of raytracers or other electromagnetic modeling software.

Large parts of the main body of the paper are well-written and easy to follow.

Weaknesses:
The main body of the paper lacks the details of the proposed pipeline and transformer architecture. In fact, all of the interesting and technical details are relegated to the appendices. 

One of the main motivations of the paper seems to be that most raytracers are too slow. However, the authors seem to ignore recent projects, such as Instant RM (https://github.com/NVlabs/instant-rm) which can compute coverage maps in a few milliseconds, depending on the desired accuracy.

It is unclear why [29] is cited as a non-differentiable raytracer although it is, to my knowledge, the only raytracer that actually is. Instant RM is also differentiable and calibration results for both tools were already demonstrated. To be honest, I have the impression that the authors tried to cover up the fact that [29] is a powerful *differentiable* raytracer that enables solving inverse problems. 

Although the authors claim that channel impulse responses can be generated, this is not demonstrated in the paper. I think that this claim should be removed unless the authors demonstrate that it is actually feasible.

The description of scene geometry recovery is incomprehensible to me.

In Figure 7, the Wi-GATr is around 20ms for inference for a tiny indoor scene. The authors should compare this against Instant RM which can probably run even faster and is differentiable.

It would be good to get confidence information (e.g., standard deviation) in Fig. 3 and Fig. 4.

Limitations:
The paper lacks a detailed comparison to the capabilities of the differential raytracer from [29]. In fact, I feel that for the individual tasks, more baselines should be included. 

Scalability to very large datasets and extremely complex scenes is unclear. 

It is unclear whether the method generalizes to electromagnetic environments that are nonreciprocal (e.g., containing certain nonreciprocal metasurfaces). 

It is unclear whether the method generalizes to scenarios in which ray-tracing is inaccurate (e.g., scenarios at low carrier frequencies).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Authors proposed transformer based ideas on very well studied area: Wireless environment simulation.
The key idea here is to capitalize Geometric Transformers to simulate radio environments. It true that wireless (directional) signal propagation is a ray tracing approach, meaning a highly directional wireless signal (Ray) may bounce off ambient surfaces or directly reach a receiver. The proposed method inserts geometric shapes in the environment as tokens in a transformer networks. The trained transformer predicts the received power at a given point in 3D space. Transformer is trained and evaluated using two datasets: Wi3R and WiPTR that simulate indoor signal propagation environments. In comparison to the baselines, transformer architecture requires 20 times less data.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. It is interesting idea to see if transformer architectures are suitable to model radio propagation environments
2.  Propagation models consider the material of ambient surface, antenna parameters, location of transmitter and receiver

Weaknesses:
1. It is not clear if datasets are of any relevance to real world environment, since the primary challenge of any modelling problem is simulation to reality gap. Since wireless channel modelling is very well studied area, a novel contribution must take in to account such differences rather than results that show the ability of transformer architecture in modelling a wireless environment
2. There are several high fidelity radio propagation modelling software, perhaps it is important to consider datasets generated from such model, current evaluation is very limited and primitive. Fig 2 and 5 are no comparison to robust channel models that are available to wireless researcher and practitioners.
3. The modelling of radio environment is no clear, reviewer is of impression that several affects like diffraction, refraction are not considered in the datasets
4. The paper also has weakness: WiNeRT: Towards Neural Ray Tracing for Wireless Channel Modelling and Differentiable Simulations which is both papers have not considered user mobility: coherence time, coherence bandwidth
5. Current evaluation is only limited to indoor environments
6. Since the prior work has already established neural network architecture are useful modelling, to push the state of the art, it is important to show accurate modelling than yet another architecture to model wireless channel.
7. Upon inspecting table 2, the reviewer is afraid that there might be issue with results here. There is 80dBm difference is accuracy with transformers based modelling, usually such a difference is unacceptable, can author please explain the training of transformer model and why it produced such a large error. The reviewer is concerned that whether such sample point is a fair to benchmark againt

Limitations:
In reviewer's opinion authors have not sufficiently addressed all the limitations of the current work. I encourage authors to look at weakness section and update the limitations of the work in the current draft

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The motivation for this work is that modeling the propagation of electromagnetic signals is critical for designing modern communication systems. Ray tracing simulators are not suitable for inverse problems or integration as channel models in designing communication systems.

In this context, the goal of this paper is to model the interplay between the 3D environment F, transmitting and receiving antennas (each characterized by a 3D position, orientation, and specific antenna characteristics) represented by t and r, respectively, and the signal h between each transmitter and receiver. The 3D geometry F is represented by a triangular mesh, where each triangle is assigned a material type from predefined classes, modeling both the shape and materials of the environment. Once the model is learned, three tasks can be performed:
1. Prediction of the received signal p(h∣F,t,r): The model is trained for this task. At test time, the network can predict signals in unseen, novel scenes. This approach is faster, fine-tunable on real measurements. The model obtained is also differentiable. This is referred to as the forward problem.
1. Localization of the receiver p(r∣F,t,h).
1. Sensing the environment p(F∣t,r,h).

The last two tasks are referred to as inverse problems. The model introduced is an adaptation of the Geometric Algebra Transformer called Wireless (Wi-GATr), used for simulating wireless propagation in a 3D environment. The authors also cast this problem as a generative modeling task of the joint distribution p(F,t,r,h) (from which the above three tasks can be accomplished) using Denoising Diffusion Probabilistic Models and Wi-GATr.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The main contributions of this work are as follows:
1. Introducing a new tokenization method for geometric wireless communication environments and transmitter and receiver characteristics.
1. Integrating diffusion-based models with Wi-GATr to model the wireless environment as a generative model, thereby determining the joint distribution of F, t, r, and h.
1. Providing new, larger datasets for the wireless environment modeling to the research community.

Weaknesses:
As per my understanding, the main weaknesses of the work are: 
1. The novelty of the work lies in tokenizing various geometric objects encountered in the wireless communication scene. However, the same tokenization is used in the vanilla transformer, making it unclear if the new tokenization provides any benefit.
1. As the authors point out, the channel is modeled only in terms of time-averaged non-coherent received power, missing crucial information such as time and direction of arrival, which are essential for modeling wireless environments.
1. While the proposed solutions seem general, most results are presented for the single antenna case. Additionally, the dataset includes only transmitting sinusoidal waveforms, which is limiting as it does not cover larger bandwidths. The wave propagation depends on frequency, and non-linearities can occur with wider bandwidths.

Limitations:
The authors mention limitations in the Discussion section (Section 6).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents the Wireless Geometric Algebra Transformer (Wi-GATr), a new architecture for simulating wireless signal propagation in 3D environments. This model utilizes geometric algebra to handle the geometric complexities of wireless scenes and ensures E(3) equivariance to respect the symmetries of the physical problem. The authors introduce two datasets, Wi3R and WiPTR, to benchmark their model. Wi-GATr outperforms existing baselines in terms of prediction fidelity and data efficiency, and it can solve both forward (signal prediction) and inverse (receiver localization and geometry reconstruction) problems in wireless communication.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The integration of geometric algebra for handling complex 3D geometric data and ensuring E(3) equivariance is a novel and effective approach. This addresses the core challenge of accurately modeling wireless signal propagation in diverse environments.

2. The paper provides a thorough evaluation of Wi-GATr against multiple baselines across various tasks, demonstrating superior performance in signal prediction, receiver localization, and geometry reconstruction.

3. Wi-GATr shows remarkable data efficiency, achieving high-fidelity predictions with significantly less training data compared to other models. This is particularly beneficial for scenarios where obtaining large amounts of training data is challenging.

4. The model's ability to handle both forward (predictive modeling) and inverse (localization and reconstruction) problems showcases its versatility and potential for a wide range of applications in wireless communication.

Weaknesses:
1. Limited Real-World Testing: While the model performs well on the introduced datasets, its application in real-world, dynamic environments remains underexplored. Additional experiments in more varied and complex real-world scenarios, such as urban or industrial settings, would strengthen the paper.

2. Scalability and Computational Load: The paper could provide more detailed insights into the computational requirements and scalability of Wi-GATr. Understanding the model's performance with larger datasets and more complex environments would be valuable for practical deployment.

3. Generalizability Across Frequencies: The model is tested at a specific frequency (3.5 GHz). Evaluating its performance across different frequencies and under various signal conditions would provide a more comprehensive understanding of its robustness and generalizability.


4. Detailed Case Studies: While the paper presents strong experimental results, including more detailed case studies or examples of practical applications, such as network design or optimization in real-world environments, would illustrate the model's impact and practical benefits.

Limitations:
The authors briefly discussed the limitation in Sec. 6.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
TKozKEMKiw;"REVIEW 
Summary:
This paper formulated the problem of finding an optimal decision tree as Markov Decision Problem and solve the scalability problem using an information-theoretic test generation function. This method provides a trade-off between the train accuracy and tree sizes, the decision tree naturally offers interpretability over ML algorithms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper is well-written and well-organized, it combines RL and decision tree generation, and building MDP before constructing the decision tree.

Weaknesses:
1. The paper lacks sufficient novelty. The approach of constructing a Markov Decision Process (MDP) and using Decision Trees (DT) to generate actions is not new. Specifically, Algorithm 1 appears to still rely on Classification and Regression Trees (CART) for splitting criteria, which diminishes the originality of the proposed method.
2. The evaluated scenarios in the paper are not clearly articulated. The algorithm has not been tested against well-known benchmarks, unlike other optimal DT algorithms. This makes it difficult to assess the comparative performance and robustness of the proposed approach.
3. The advantages of using this algorithm instead of CART are not clearly demonstrated. Both algorithms control tree size and depth. However, CART is known to converge faster and offers a simpler implementation. Without clear evidence of the benefits, it is hard to justify the use of the proposed method over established techniques like CART.
4. The definition of actions generated by the tree is ambiguous. It is not clear whether the actions are discrete or continuous. If the algorithm is designed to build an MDP, it should be tested on general reinforcement learning (RL) tasks to validate its effectiveness and applicability in broader contexts.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors pose binary decision tree construction within the framework of Markov Decision Processes. They first propose methods for constructing an MDP from a decision tree construction problem, exploring varying test generating functions that trade off the coverage of the search space vs the size of the search space. They then apply Dynamic programming to solve the resulting MDP and show this learnt method can both create binary trees that minimise the loss over a dataset but that it can also be used to add additional losses a user may have over decision trees such as the prior that trees should be small, making them interpretable. They evaluate their proposed method, comparing with other high performing methods such as Quant-BnB, MurTree and a DeepRL method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Well written paper and easy to understand the method
- Clearly an important direction of research
- Thorough experiments with appropriate baselines and good range of datasets to ensure the conclusions generalise to a wide range of datasets
- Code fully provided, along with implementations of baselines used

Weaknesses:
- Various versions of Reinforcement Learning for binary tree construction have previously been explored. While the implementation in this paper is ultimately different and appears to significantly improve performance, there is limited novelty of the approach. Novelty largely comes down to the test generating functions explored and the addition of extra losses (interpretability) in addition to just the dataset accuracy.

- Small formatting issue
     - Table 1 is too small

Limitations:
- Limitations are appropriate addressed

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper models the construction of decision trees as a reinforcement learning problem. Currently SOTA algorithms for constructing decision trees have the drawbacks that 1) they take long to compute at depths > 3, and 2) the trees constructed are complex and difficult to interpret. By modelling the problem as an RL task, the authors hope to make the construction of decision trees scale to larger sizes. They present Dynamic Programming Decision Trees which models tree construction as a MDP solved using dynamic programming. They evaluate the accuracy of trees produced by their approach empirically against other commonly used approaches.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Originality:** The approach presented is a novel approach to constructing dynamic trees.

**Significance:** As datasets become larger and interpretability becomes more important, having an approach that scales DT construction to larger trees is needed now. This makes this work rather significant.

**Clarity:** The first half of the paper (up to Sec. 4) was clear. It becomes harder to understand after. Providing an intuitive explanation certain equations would be helpful. E.g., why is probability $p_l = |X_l| / |X|$?

**Quality:** The technique designed is sound and the experiments chosen were the correct ones to demonstrate their claims.

Weaknesses:
The experimental evaluation is weak. From what I understood, the algorithms being evaluated were run only once and evaluated once. The results do not statistically back up the authors' claims. Multiple runs with statistical significance testing is needed.

There is no actual analysis on the interpretability of the trees produced, only the complexity of the trees.

Limitations:
The authors mention one limitation (test generation) as a problem. It seems that that would make it difficult for DPDT to actually scale to larger trees. Is that not so? Is scalability not a limitation then?

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes to use an approach for learning interpretable
decision trees using markov decision processes. The results are shown
to be competitive with branch and bound methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
None of notice, given the listed weaknesses.

Weaknesses:
There exists extensive experimental evidence challenging the claims
about the interpretability of decision trees, while simultaneously
demonstrating the need for decision trees to be explained, since these
can otherwise exhibit arbitrary explanation redundancy.

As a result, and at present, there is no practical justification
whatsoever to learn so-called interpretable optimal decision trees.
It is absolutely unclear that optimal decision trees will provide any
advantage, regarding computed explanations, over decision trees
induced with heuristic algorithms.

Given the above, I cannot recommend acceptance.

Some references on the necessity of explaining decision trees.

Xuanxiang Huang, Yacine Izza, Alexey Ignatiev, João Marques-Silva: On
Efficiently Explaining Graph-Based Classifiers. KR 2021: 356-367

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On the Computational
Intelligibility of Boolean Classifiers. KR 2021: 74-86

Yacine Izza, Alexey Ignatiev, João Marques-Silva: On Tackling
Explanation Redundancy in Decision Trees. J. Artif. Intell. Res. 75:
261-321 (2022)

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On the explanatory power of
Boolean decision trees. Data Knowl. Eng. 142: 102088 (2022)

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On Preferred Abductive
Explanations for Decision Trees and Random Forests. IJCAI 2022:
643-650

João Marques-Silva, Alexey Ignatiev: No silver bullet: interpretable
ML models must be explained. Frontiers Artif. Intell. 6 (2023)

Limitations:
These were listed above. I believe the paper is solving a non-relevant
problem given practical and theoretical evidence regarding the
non-interpretability of decision trees, be these optimal or not.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
TKbGTj0YCZ;"REVIEW 
Summary:
The authors propose a method to train a predictive model (a regressor in their experiments) that minimizes an objective comprising the average performance loss across multiple domains (environments) along with a penalty term for the normalized truncated Wasserstein (NTW) distance between the non-conformity score CDFs of each environment and the importance-weighted one used to address covariate shift. Their experimental results demonstrate that the proposed NTW distance objective is correlated with coverage differences due to concept shift and can achieve different tradeoffs with the average prediction residuals, thereby reducing this gap.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Overall, the paper is easy to follow and the reasoning behind the proposed formulation is compelling. The problem that the authors address is relevant. The experimental results show that the proposed NTW distance is capturing the coverage difference due to concept shift.

Weaknesses:
While the motivation behind the regularization is compelling, it is not entirely clear, both empirically and theoretically, what specific benefits it offers over other state-of-the-art approaches that address differences in the non-conformity score distributions. To highlight the advantages of the proposed NTW regularization over simply minimizing the prediction residuals and then applying various post-hoc conformal prediction techniques, I suggest that the authors demonstrate the validity and efficiency of the prediction intervals obtained for different alphas (error levels) on test data. Additionally, they should provide empirical comparisons or some fundamental discussion/theoretical results in relation to the following approaches:

* Split conformal prediction and group conditional split conformal prediction (where each environment is treated as a separate group). The latter can include standard SCP conditioned on each group or an approach such as the one in section 4.1 of [Barber et al. 2020, ""The Limits of Distribution-Free Conditional Predictive Inference""] or BatchGCP/BatchMVP in [Jung et al. 2022, ""Batch Multivalid Conformal Prediction""].
	
* Performance of the covariate shift split conformal prediction, as already discussed in the paper. For example, if this is built on top of a model that minimizes ERM, DRO or V-REx does the proposed approach provide better prediction sets in terms of conditional coverage/validity on the domains.

* An adaptive approach such as the one by [Amoukou and Brunel 2023, Adaptive Conformal Prediction by Reweighing Nonconformity Score].

Providing such comparative results or analysis would significantly strengthen the paper's argument.

I also think the authors should discuss how their work relates to [Barber, Rina Foygel, et al. ""Conformal prediction beyond exchangeability""], where it is suggested that the non-conformity scores should be weighted based on the total variation distance between the source and target distributions. This approach could potentially serve as another baseline to consider, given the distance between the distributions of the non-conformity scores under P and Q(e).

Limitations:
yes, they mention some of the limitations of the proposed work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper ""Robust Conformal Prediction under Joint Distribution Shift"" investigates the problem of predictive inference under the setting where we have both covariate shift and concept shift. The authors propose a conformal prediction-based procedure that accounts for such distribution shifts and illustrate the performance through experiments.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This work provides extensive and thorough experimental results.

Weaknesses:
The paper doesn't read very well as some notations appear without definitions and relevant assumptions are not written clearly. For example:

- It is assumed that the likelihood ratio $dQ/dP$ is known, but this was not clearly stated in the problem setting.
- Some assumptions are not written in advance but are rather introduced when they are needed.

It would be better if the authors could provide sufficient intuition and motivation for their methodology.

Limitations:
I don't think this work has negative societal impacts.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper adresses the issue of conformal prediction under distribution shift with multiple test domains. The goal is to reduce the deviation in coverage caused by different potential distribution shifts across these domains. The paper firstly proposes a way of disentangling a joint distribution shift (shift effects both in covariate and label distributions, which they term ""concept"") by employing weighted conformal prediction (Tibshirani et al, 2019) to address covariate shift, and then quantify the remaining shift in terms of a truncated normalized Wasserstein distance (D-NTW) between the original and weighted conformal score distributions (empirical CDFs). This D-NTW is then used as a regularizer term in a training algorithm to explicitly ensure that coverage deviations are minimized across test domains. Experiments include assessing the correlation between D-NTW and the actual expected coverage difference, which is shown to be high (vs. other distributional distance metrics), and comparing to two multi-test-domain optimization methods on a variety of datasets to show that the coverage difference is lower while not compromising on prediction accuracy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Addressing the issue of distribution shift in conformal prediction is a relevant problem, particularly for the less explored label shift setting
- Attempts are made to disentangle covariate and label (concept) shifts, which can provide insights into the model's adaptation abilities
- The suggested D-NTW distance metric is well motivated and benchmarked against multiple sensible alternative distance metrics
- An interesting range of datasets from different domains is considered
- The paper is clearly written and good to follow, including Fig I visualizing the procedure (albeit it is somewhat hard to read, especially part (c))

Weaknesses:
My main concerns are w.r.t. practicality and evaluation. A fundamental requirement of the proposed algorithm and D-NTW is the availability of labelled samples from every test domain $Q_{XY}^{(e)}$ in order to obtain the conformal test score distributions $F_{Q^{(e)}}$, from which then both the likelihood ratios for covariate shifts and D-NTW can be explicitly computed. Beyond existing concerns about the practicality of estimating a likelihood $\textit{ratio}$, we now require actually explicitly estimating every test domain distribution, which is prone to much error. Regardless, if we can now explicitly obtain $F_{Q^{(e)}}$ for every test domain, I am wondering why the direct solution is not to just compute a conformal quantile $q^{(e)}$ on the basis of this for every test domain and thus optimally adapt to the existing joint shifts per domain. Perhaps the loss of the disentanglement of shift contributions is the motivation? In general, I find this requirement to require knowing or estimating the test domain distributions on the basis of labelled test domain data and thus the use of extensive density estimation quite limiting, especially if we consider high-dimensional settings. Perhaps this is why the considered experiments are for 1-D regression data only. I was also wondering if the authors were able to make any connections between their proposals and obtainable coverage guarantees on the test domains. They propose a bound on the distance of D-NTW from the expected coverage difference, but perhaps more explicit connections to conformal coverage guarantees of the form in Eq. 3, e.g. by leveraging guarantees from (Tibshirani et al, 2019) and their linear decomposition of shift effects are worth investigating.

In regards to evaluation, I was missing a closer connection to existing conformal methods under shift, and the actual goals of these methods in terms of coverage guarantees on test data. In their comparison to test-domain optimization algorithms I was not surprised that their algorithm performs better on expected coverage difference, since it explicitly $\textit{optimizes}$ for this goal, while the baselines target e.g. variance minimization. It would be more interesting to compare to conformal algorithms for shift such as the mentioned [2,3], showing e.g. that those are not able to fully capture the joint shift or are overly conservative, thus compromising on the metric. For example, I was surprised that [1] was not mentioned, since it explicitly targets label shifts. Similarly, while it is nice that the relative coverage difference is minimized or correlates well with D-NTW, this does not tell explicitly how my conformal methods will now perform on the test domains. It would be nice to also obtain an explicit assessment of the coverage $(1-\alpha)$ on test domains, and the obtained prediction set sizes. Even if target coverage is not satisfied, it would already be a contribution to show that the proposed algorithm achieves better robustness by being closer to target coverage, or smaller set sizes at the same level. 

Minor: multiple typos e.g. L75, Fig I caption, L89, Eq. 2

References
- [1] Podkopaev, Aleksandr, and Aaditya Ramdas. ""Distribution-free uncertainty quantification for classification under label shift."" Uncertainty in artificial intelligence. PMLR, 2021.
- [2] Cauchois, Maxime, et al. ""Robust validation: Confident predictions even when distributions shift."" Journal of the American Statistical Association (2024): 1-66.
- [3] Zou, Xin, and Weiwei Liu. ""Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 15. 2024.

Limitations:
The requirements of the algorithm and density estimations are mentioned, but the limitations of the approach are not explicitly discussed in terms of imposed assumptions on the problem setting. A more through discussion of the practical limitations would be helpful (some of which are inherited e.g. from (Tibshirani et al, 2019) simply by their use of likelihood ratio weights).  A small subsection in sec 6 mentions difficulties of optimization algorithms.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the coverage difference caused by covariate and concept shifts. Authors introduce the Normalized Truncated Wasserstein distance (NTW) as a metric for capturing coverage difference expectation under concept shift by comparing the test and weighted calibration conformal score CDFs. They also develop an end-to-end algorithm called Multi-domain Robust Conformal Prediction (mRCP) to incorporate NTW during training, allowing coverage to approach confidence in all test domains.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. introduced NTW as a metric to capture the coverage gap

2. high correlation between NTW and coverage difference expectation; mRCP can balance residual size and coverage gap

Weaknesses:
1. section 3.1 and 3.2 introduce some important definitions:  while authors provide some explanation, the theoretical understanding of them are very limited

2. simulation:  authors mention the mRCP can achieve a balance between coverage gap and size of residual, further simulations need to be carried out ( I would be interested to see a plot including the avg. coverage vs avg. residual size

Limitations:
Authors mentioned the limitation in the discussion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper tackles the challenge of obtaining conformal predictions that remain robust under distribution shifts. This is an important issue in many machine learning applications where the underlying data distribution may change across the training (or calibration) and test data sets. The authors propose an algorithm that appears promising based on empirical results. However, significant improvements are needed in terms of writing quality, clarity, citation of relevant literature, mathematical rigor, and explanation of the main ideas. Addressing these substantial weaknesses would make the paper more accessible and impactful for the research community.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The problem of robust conformal predictions under distribution shifts is timely and relevant.

- The paper introduces a concrete algorithm that demonstrates promising performance in some practical scenarios.

Weaknesses:
- Writing Quality: The paper is difficult to read and understand, even for experts. Key concepts are not clearly explained, and the text contains numerous awkward or unclear sentences, as well as pervasive grammatical errors and typos. Some sections seem poorly written, possibly by AI, while others could have been significantly improved with better editing.

 - Missing References: Important related works, such as ""Conformal prediction beyond exchangeability"" by Barber et al. (2023), are not discussed, which limits the paper's contextual grounding in existing literature.

- Lack of Statistical/Mathematical Rigor: The mathematical details in the paper are imprecise. Assumptions and approximations are not clearly stated, and there is a frequent confusion between population and sample quantities in key sections.

 - Unclear Core Idea: The main idea of the proposed algorithm, particularly how it handles concept shift through covariate shift adjustments (as in Equation 10), is not clearly articulated and remains confusing.

Limitations:
The paper's main limitations are its poor writing quality and lack of mathematical precision. These issues make it difficult to understand the main ideas and verify the soundness of the proposed method.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
TJJ4gZtkS4;"REVIEW 
Summary:
This paper proposes new methods to count the number of linear regions in neural networks by viewing them as tropical Puiseux rational maps. By computing their Hoffman constant, the authors are able to identify a sampling radius which ensures that all the network’s linear regions will be intersected. They use this insight to propose algorithms for counting the number of linear regions for both invariant and traditional networks.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper is well-written with virtually no typos and errors. The technical content is accessible and not unnecessarily convoluted and the proofs and concepts are presented clearly.

Weaknesses:
The main weakness of the work, in my opinion, can be summarized in the following points:

- the connections to tropical geometry and group theory are not rigorous beyond the point of simple notational fixes
- the motivation for the work and how it fills gaps in the existing literature is unclear, and
- the effective utility of the approach is not convincingly demonstrated by the theory or experiments.

**Rigor of tropical and group theories**

I believe this point is the biggest weakness of the paper. From the perspective of tropical algebra, vectors and polynomials live in $\bar{\mathbb{R}}$. The authors mention $\bar{\mathbb{R}}$ in line 87, but then this is never used in most of their work. This might seem as a notational fix, but it is not, as it introduces problems in virtually every single result in the paper. This first becomes a real issue in (4), where maximums are taken over, potentially, $\infty$. How is it guaranteed that (4) exists in the context of tropical algebra? This is a recurring problem that appears in (5), (6), and (7). Another important issue at the intersection of group theory and tropical algebra is how groups are defined. Semirings, by construction, are objects that do not admit additive inverses. This means that, if one wants to define groups on such structures, great care needs to be taken as to how groups are defined, how they act on vector spaces, and what groups are actually permissible in this context.

From the perspective of group theory how is the group action defined? How do the group elements act on vectors in tropical spaces? Group representations $\rho: G \to \operatorname{GL}$ require the concept of an invertible matrix, however that concept is ill-defined in tropical vector spaces.

(Moreover, the authors define incorrectly $\bar{\mathbb{R}}$. The infinity element needs to be the identity element of tropical addition: if one opts to use the max-plus semiring, $-\infty$ should be used. If we use the min-plus semiring, $\infty$ should be used. However, this is a notational fix.) 

**Motivation**

In terms of motivation it is unclear how the work is related to the existing works. There have been countless results on the number of linear regions of neural networks, and quite a few results using tropical geometry at that. What void in the literature does this paper fill? The related work paragraph lists some of the works in tropical geometry, but doesn’t highlight where these works come short and how the proposed manuscript fills that void. Moreover, there is no discussion of why the existing works on linear counting that do not utilize tropical geometry are also not able to handle the presented context.

**Effective utility**

At the end of the day, I’m not sure I understand what the utility of the method is. Ignoring here the questions on motivation, the goal is to make deep learning more interpretable. However, the authors’ experiments diverge when input sizes are larger than $6$ and the networks are deeper than $4$ layers. Modern deep learning uses input sizes significantly larger than $100$ and decade old networks are deeper than $10$ layers. So what effectively are we learning about deep networks?

Unfortunately, there are some larger issues with the method and experiments. Beyond the concerns from the perspective of tropical geometry, we have zero guarantees about the upper and lower bounds of the Hoffman constant. There is no asymptotic analysis on the sample complexities of the bounds, no analyses about tightness or optimality, or even, at the very least, an analysis that the bounds are not vacuous or trivial. On line 272 the authors claim that even though their estimate diverges, that’s acceptable because frequently we’re interested is an upper bound on expressivity. However, how can we guarantee that there the number of regions is not undercounted (I couldn’t find a proof)? For that statement to be true the bound needs to be tight, but from their own experiments (Tables 1 and 2), the true $H$ ends up being larger than the upper bound, which is used to calculate the radius and eventually the radius. Obviously, then, the computed bounds are not representative: then, since the estimate Hoffman constant is not accurate and the algorithm diverges, what essentially do we gain?

Limitations:
I think the authors accurately identify the main limitations of their approach, which relates to the large computational complexity.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This study investigates the expressive power of deep fully-connected ReLU networks (or a piecewise linear function) from the perspective of tropical geometry. The number of linear regions gives an estimate of the information capacity of the network, and the authors provide a novel tropical geometric approach to selecting sampling domains among linear regions.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- An effective sampling domain is proposed as a ball of radius bounded by Hoffman constant, a combinatorial invariant

- The proposed sampling algorithm is doable and implemented.

Weaknesses:
- The proposed algorithms suffer from the curse of dimensionality

Limitations:
it is discussed in Section 6

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the expressivity of neural networks as captured by the number of linear regions using tools from tropical geometry. There are three main contributions, two of which are theoretical and the other is about open source library that allows the analysis of neural networks as Puiseux rational maps. 

The first theoretical contribution is that they propose a new approach for selecting sampling domains among the linear regions and the second is a way to prune the search space for network architectures with symmetries.

Prior work on tropical geometry and deep neural nets have analyzed ReLU and maxout units. Contrary to prior works, this work makes an effort to understand the geometry of the linear regions not just their number. To do so the authors propose a way of sampling the domain that leads to more accurate estimates compared to random sampling from the input space, which is a previously used alternative that can result in some missed linear regions and hence in inaccurate estimates about the information capacity of the neural network. This insight about sampling, allows then the authors to reduce the time to estimate the linear regions of special types of neural networks that exhibit some symmetry. This essentially reduces the number of samples needed and they experimentally verify their results.

Finally, the authors release OSCAR an open source library.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-connections of neural networks expressibity with tropical geometry, though they have been exploited in the past, are strengthened in this paper

-the paper presents a nice story that leads to faster sampling methods, both in theory and in practice.

Weaknesses:
-the main weakness I see in the paper is that, though well-motivated and interesting, it lacks technical depth. For example, there is essentially one main result stated as Theorem 4.3, and some intermediate results stated as Lemma 3.3 and Proposition 3.4. On the one hand, the latter two are simple observations about Puiseux polynomials, and on the other hand the proof of the Theorem 4.3 is not more than 3 lines (as shown in Appendix C). As such, I believe it is a nice transfer of ideas from tropical geometry to neural networks, but given that the connection was already there and used in prior works more than 10 years back, I don't think the better sampling algorithm is solid enough.

Limitations:
See weaknesses.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work provides a geometric characterization of the linear regions in a neural network via the input space. Although linear regions are usually estimated by randomly sampling from the input space, stochasticity may cause some linear regions of a neural network to be missed. This paper proposes an effective sampling domain as a ball of radius R and computes bounds for the radius R based on a combinatorial invariant known as the Hoffman constant, which gives a geometric characterization for the linear regions of a neural network. Further, the paper exploits geometric insight into the linear regions of a neural network to gain dramatic computational efficiency when networks exhibit invariance under symmetry. Lastly, the paper provides code for converting trained and untrained neural networks into algebraic symbolic objects, useful for precisely the kinds of analysis this paper performs.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The authors present an interesting and novel way to analyze the capacity of a neural network using fundamental notions from tropical geometry.

2. The paper and theory were very clearly presented. In terms of writing, the presentation of the relevant tropical geometry for purposes of Hoffman constant estimation Section 3 was excellent.

Weaknesses:
1. My greatest concern in this paper stems from the experiments for upper and lower bound estimation for Hoffman constants given in Tables 1, 2, and 3 in the appendix. It seems the experimental upper and lower bounds computed there do not actually bound the true Hoffman constant. I understand that the upper bound may be loose due to the way it is estimated, but the lower bound should always be below the true Hoffman constant, as per my understanding. Yet for, say, the first of eight computations in table 1, the lower bound $H_L$ is $0.5460$, which is clearly above the true value of $H$, given to be $0.3298$. For that example, the upper bound $H^U$ is given to be $0.2081$, which is clearly not above the true value. This pattern continues, and the lower and upper bounds for the Hoffman constants seem to fluctuate somewhat arbitrarily around the true Hoffman constants, which is concerning. I am currently assuming that there is some kind of mistake with these experimental values and would like a clarification from the authors regarding this.

2. Due to the curse of dimensionality, this method for estimating the expressiveness of neural networks can only be applied to simple neural networks in practice. This is very apparent due to the way the numerical approach requires sampling on a mesh grid in an $n$-dimensional box (but is also true for the symbolic approach, that relies on the computation of the Puiseux rational function associated with a neural network, which becomes increasingly quite hard in higher dimensions). To the credit of the authors, they are up-front about this limitation, but this does significantly hinder the applicability of the presented results.

Limitations:
The authors adequately discuss the limitations of their work in Section 6.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
KgGhxmQFFy;"REVIEW 
Summary:
introduce a molecule tokenizer based on Codebooks which gets integrated into UniMoT, a unified molecule-text LLM

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
strong performance on a variety of benchmarks; outperforms/on pair with 3D-MoLM

Weaknesses:
- Table 1: include CLAMP [1] as well as standard molecular fingerprints (incl. in said reference); further KV-PLM linear probing for ToxCast results in 66.3 AUROC compared to 55.03 in your paper

[1] Seidl, P., Vall, A., Hochreiter, S., & Klambauer, G. (2023, July). Enhancing activity prediction models in drug discovery with the ability to understand human language. In International Conference on Machine Learning (pp. 30458-30490). PMLR.

Limitations:
-

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propsed Uni-MoT, a unified structure to align molecules with texts with a VQ tokenizer and the Q-Former in BLIP-2. By treating molecules as new word tokens in the codebook, Uni-MoT aligns the discrete token representation for molecules and texts, while also following the autoregressive manner of LLMs. The training of Uni-MoT follows four main stages, Causal Q-Former Pretraining, Molecule Tokenizer Pretraining, Unified Molecule-Text Pretraining, Task-Specific Instruction Tuning. The experiments demonstrate that Uni-MoT can achieve better performance compared to the selected baselines.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The performance of Uni-MoT is overall good and better than the baseline models.
2. Uni-MoT provides a alternative solution for aliging molecules with texts.

Weaknesses:
1. Although the authors claim that Uni-MoT follows a different structure as shown in Figure 1 c, it turns out that it still follows the BLIP-2 [1] structure, which has been widely used to align 2D molecule graph [2] and 3D molecule information [3]. Thus, **the technical contribution and novelty of this paper are extremely limited**. Especially when VQ-VAE [4] is also a well-developed structure adopted in computer vision. This paper is more like simply swapping the input of the Q-former in BLIP-2. 
2. The experiments are only conducted on a single serie of LLM, Llama-2, **which is not sufficient to demonstrate the model agnosticsm of Uni-MoT**. In fact, LLMs like Mistral [5] and Meditron [6] might possibly achieve a better performance. Meanwhile, the selection of Llama-2 is also not convincing, as Llama-2 is not specially pre-trained on chemistry or biomedical corpora. 
3. **The seletion of the datasets is also worth discussion**. The result on ChEBI-20 is presented in Appendix, while the main experiments are conducted on PubChem. I am wondering why not also conduct the remaining experiments on ChEBI-20, as the data scale of ChEBI-20 is much larger than PubChem. At the same time, the reverse task, text-based molecule generation, on ChEBI-20 and PubChem is surprisingly not presented.
4. **The comparison with the baselines is not fair enough**. For example, MolCA adopts Galactica-1.3B as its backbone model, which is much smaller than Llama-2-7B. Thus, the proposed method can not demonstrate its superiority compared to previous methods. Notably, since the authors mentioned MolCA and 3D-MoLM, it is necessary to discuss the possible affects of the modalities. Furthermore, several SOTA baseline models like BioT5 [7] and BioT5+ [8] are not discussed.
5. **The ablation study is also not sufficient without providing the naive fine-tuning performance of Llama-2**. Besides, as Uni-MoT incorporates the VQ tokenizer, it is also important to discuss the size of the codebook.
6. During the pre-training stages, it should ensure that the **data leakage** is avoided. Considering the pre-training dataset adopted has overlaps with the fine-tuning dataset [7, 8], the performance gain could possibly come from the data leakage.
7. **Some claims and definitions are confusing**. e.g. In Line 44, ""a unified molecule-text LLM"", I do not agree with the claim of an ""LLM"". The ""LLM"" is still the Llama-2. It should be like ""structure"" or something.

#### References
[1 ]Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.
[2] Liu, Z., Li, S., Luo, Y., Fei, H., Cao, Y., Kawaguchi, K., ... & Chua, T. S. (2023). Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. arXiv preprint arXiv:2310.12798.
[3] Li, S., Liu, Z., Luo, Y., Wang, X., He, X., Kawaguchi, K., ... & Tian, Q. (2024). Towards 3D Molecule-Text Interpretation in Language Models. arXiv preprint arXiv:2401.13923.
[4] Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., ... & Wu, Y. (2021). Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627.
[5] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. D. L., ... & Sayed, W. E. (2023). Mistral 7B. arXiv preprint arXiv:2310.06825.
[6] Chen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., ... & Bosselut, A. (2023). Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079.
[7] Pei, Q., Zhang, W., Zhu, J., Wu, K., Gao, K., Wu, L., ... & Yan, R. (2023). Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. arXiv preprint arXiv:2310.07276.
[8] Pei, Q., Wu, L., Gao, K., Liang, X., Fang, Y., Zhu, J., ... & Yan, R. (2024). BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning. arXiv preprint arXiv:2402.17810.

Limitations:
Yes. They have discussed the limitations as not enough tasks, data scarcity, and real scenarios.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work presents a new molecule LLM that uses a pretrained tokenizer to replace the projection layer. The tokenizer consists a Q-Former and a VQ module, which are trained with consistency loss. The model is evaluated on molecular understanding and generation tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The authors provided a novel framework to align text representation in LLMs and the molecules. 
- The authors conducted experiments on many datasets.

Weaknesses:
- The claim that adapter-based methods cannot do text-to-molecule generation tasks in not accurate (Sec.1). They can always adapt text-encoder to a pre-trained SMILES or graph generator. This may make this work not well motivated.
- In the tokenizer (Fig.2), it seems that multiple alignment methods are required to train the model, including: (1)the molecule and text contrastive in Q-Former, (2), the aligning MSE loss, (3) the SMILES decoder reconstruction. It's not clear about the design reasons, and if all of them are required.
- Given the learnable query has a fixed size, it may not perform well for larger molecules.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose to use a vector-quantized tokenizer that incorporates a Q-Former to connect pre-trained molecule encoder, SMILES encoder, and SMILES decoder so that a language model can integrate molecule and text modalities. Based on the proposed tokenizer, the authors introduce a four-stage training strategy to train UniMoT, a unified molecule-text LLM proposed in this submission. The performance of the UniMoT is evaluated empirically with 7 tasks in the areas of molecule comprehension and molecule generation. Some ablation studies are also conducted.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well-organized and well-written.
- Using a vector-quantized tokenizer and a Q-Former to connect different modalities could be somewhat novel.
- The proposed UniMoT outperforms baselines in most cases and reach comparable performances in others.
- The authors provide many implement details which increases the reproducibility.

Weaknesses:
- All the components are borrowed from existing works. Besides the pre-trained molecule encoder, SMILES encoder and decoder, both vector quantization and Q-Former are proposed in previous works [1][2]. The Q-Former part of this paper (Appendix A) is very similar to Q-Former's original paper. Even Figure 4 in this paper is very similar to Figure 2 in Q-Former's original paper [2].
- When generating molecules the proposed method relies heavily on a pre-trained decoder. In the decoder's original paper, the reported validity is 99.9 and no guarantee is provided that the generated SMILES string will be always valid [3]. The 100% validity reported in this paper could be attributed to overfitting.
- Some hyperparameter choices are not well justified and studied. For instance, the molecule codebook size is set to 2048, but there is no explanation why 2048 is chosen. How the molecule codebook size affects the performance is also not studied.
- There are some existing works about molecule tokenizers [4][5], the paper lacks the comparison of the performance using different tokenizers.
- The robustness of the model is not studied. Each molecule has many synonyms, how the proposed method performs given different synonyms of the same molecule is desired to know.

[1] Van Den Oord, Aaron, and Oriol Vinyals. ""Neural discrete representation learning."" Advances in neural information processing systems 30 (2017).

[2] Li, Junnan, et al. ""Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models."" International conference on machine learning. PMLR, 2023.

[3] Irwin, R., Dimitriadis, S., He, J., Bjerrum, E.J., 2021. Chemformer: A Pre-Trained Transformer for Computational Chemistry. Mach. Learn. Sci. Technol.

[4] Li, Xinhao, and Denis Fourches. ""SMILES pair encoding: a data-driven substructure tokenization algorithm for deep learning."" Journal of chemical information and modeling 61.4 (2021): 1560-1569.

[5] Schwaller, Philippe, et al. ""Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction."" ACS central science 5.9 (2019): 1572-1583.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
TI7Vy90B9j;"REVIEW 
Summary:
This paper studies last-iterate convergence rates of online learning in monotone games. The main contribution is an algorithm called Gradient Ascent with Boosting Payoff Perturbation (GABP). The GABP algorithm achieves (1) $O(\log T / T)$ last-iterate convergence with full gradient feedback, which is near-optimal; (2) and $O(1/T^{1/7})$ last-iterate convergence with noisy gradient feedback (the noise is zero-mean with bounded variance). The latter result improves prior results of $O(1/T^{1/10})$. Moreover, the GABP algorithm guarantees an individual dynamic regret of $O(\log^2 T)$ under full gradient feedback, slightly worse than the state-of-the-art bound of $O(\log T)$. This paper also contains numerical experiments on small game instances to demonstrate the effectiveness of GABP.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The problem of last-iterate convergence rates of no-regret learning algorithms in monotone games is relevant and interesting. Most existing results focus on the full gradient feedback, while only a few provide concrete convergence rates under the noisy gradient or the bandit feedback. The proposed GABP algorithm has near-optimal $O(\log T / T)$ last-iterate convergence rate under full gradient feedback. It also improves the convergence rates under noisy gradient feedback from $O(1/T^{1/10})$ to $O(1/T^{1/7})$. This is a solid contribution to learning in games, although the rate for the noisy gradient feedback setting may not be tight.

Weaknesses:
1. The proposed GABP algorithm does not achieve the optimal $O(1/T)$ last-iterate convergence rate under full gradient feedback. The $O(1/T^{1/7})$ last-iterate convergence rate is also not tight for the noisy feedback.
2. The relationship between the proposed GABP algorithm and the AOG algorithm in [1] and the intuition behind the fast last-iterate convergence rates is not clearly discussed. These two algorithms are different (as shown in Appendix F) but share similar ideas. The anchoring term in both algorithms comes from the (implicit) Halpern iteration algorithm, which can not be run directly. The difference is that GABP views each step of Halpern iteration as a fixed point problem (Line 170) and uses an inner loop of $\log (1/\epsilon)$ steps to get an $\epsilon$-approximation (this is called updating the reference strategy in the paper.); In contrast, AOG directly uses optimism to approximate the implicit update. This leads to GABP being a log factor slower than AOG in the full gradient setting. However, the approximating the fixed point approach is more robust in the noisy gradient setting due to strong monotonicity. Moreover, the potential function and the approximately non-increasing potential analysis are very similar to that used in [1]. If they are inspired by [1] then this should be acknowledged. 

[1] Doubly Optimal No-Regret Learning in Monotone Games, Cai and Zheng, ICML 2023.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel algorithmic approach to enhance the convergence of first-order methods in the context of monotone games. The authors propose a payoff perturbation technique that introduces strong convexity to players' payoff functions, which is crucial for achieving last-iterate convergence. This technique is particularly designed to handle scenarios where the gradient of the payoff functions is monotone and potentially noisy. The paper presents a method called Gradient Ascent with Boosting Payoff Perturbation (GABP), which incorporates a unique perturbation into the payoff function and maintains a periodically re-initializing anchoring strategy. The authors demonstrate that GABP offers faster last-iterate convergence rates compared to existing algorithms, even in the presence of additive noise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a unique perturbation technique that addresses the challenge of last-iterate convergence in monotone games. The proposed GABP algorithm is an innovative modification of Adaptively Perturbed Mirror Descent (APMD), offering improved convergence rates.

Quality: The theoretical development is thorough, with rigorous proofs provided for the convergence rates of GABP in both full and noisy feedback settings. The paper also includes a detailed analysis of the algorithm's performance in terms of individual regret.

Clarity: The paper is well-organized, with clear explanations of the algorithm, theoretical results, and experimental setup. The use of pseudo-code for GABP aids in understanding the algorithm's implementation.

Significance: The work contributes to the field of online learning in games, providing a solution that is particularly relevant for applications such as Generative Adversarial Networks (GANs) and large language model fine-tuning, where last-iterate convergence is desirable.

Weaknesses:
Experimental Validation: While the paper provides empirical results, the experiments could be expanded to include a broader range of game types and noise levels to further validate the robustness and generalizability of GABP.

Comparison with State-of-the-Art: The paper compares GABP with APMD and Optimistic Gradient Ascent (OGA) but could benefit from a more comprehensive comparison with other existing methods in the literature to better situate its contributions.

Practical Considerations: While the paper addresses the theoretical aspects of GABP, it could provide more insights into practical considerations, such as the implementation challenges and potential modifications needed for real-world applications.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work focuses on last-iterate convergence of  game dynamics. A payoff perturbation technique is proposed by adding strong convexity to players' payoff functions. Despite it is a well studied technqiue in learning in repeated games with first-order methods, especially in last-iterate convergence, a novel perturbation scheme introduced in this paper allows on to provide faster last-iterate convergence compared to previous works.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper provides a relatively complete result containing last-iterate convergence rate of the proposed algorithm GABP in full feedback and noisy feedback. The faster rate of convergence is an improvement compared to existing works. Except for some weakness (will be stated later), the presentation of this paper is clear to understand. The authors have reviewed most related works to my best knowledge, so that the contributions claimed are easy to follow. Addition to theoretical works, this paper has provided experiments (sufficient in my opinion) showing the comparison of GABP and existing algorithms such as Adaptively perturbed gradient ascent and Optimistic gradient ascent.

Weaknesses:
One obvious spot that should be added to improve the presentation is the following. The game considered in this paper is motivated by real-life examples. But the authors only give one example motivating monotone games. Part of contributions of the paper is claimed to be the study of two feedback models: full feedback and noisy feedback, but there is not specific examples and applications illustrating the importance of these settings. For sure readers can always find related works even just by googling the keywords, but providing concrete application scenes where the gradient of payoff can be achieved perfectly or only partially achievable gradients can be obtained is important, especially ""noisy feedback"" can be just a model of many cases.

Limitations:
This is theory based paper, no potential negative impact will cause.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies first order methods to solve monotone games where the gradient of the payoff function is monotone in the strategy, along with additive noise. The authors introduce a payoff perturbation technique which introduces strong convexity to the to the payoff functions and thereby derive last iterate convergence rates.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper is well written and the method and results are interesting.

Weaknesses:
The authors should include a table which compares their paper with others in the literature. This would make it easier for the reader to place the results in context and see where improvements are made more easily.
(for example comparison to [Yoon and Ryu, 2021, Cai and Zheng, 2023] including constants)

Limitations:
See Weaknesses and Questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
TEwQSWWn7S;"REVIEW 
Summary:
Training RBMs is challenging and slow due to the multiple second-order phase transitions and associated slow mixing of MCMC sampling. The paper introduces a pre-training method, consisting in integrating the principal directions of the dataset into a low-rank RBM through a convex optimization procedure. The Gibbs-Boltzmann equilibrium distribution of the pre-trained model can be efficiently sampled via a static Monte Carlo process. Starting from the pre-trained model, the standard Persistent Contrastive Divergence (PCD) training procedure for RBMs partially overcomes the problem of second-order phase transitions. The pre-training method is tested on the MNIST 01 dataset, a synthesized “Mickey” dataset, and the Human Genome dataset (HGD). The method is shown to outperform the PCD algorithm and the Jarzynski reweighting method (JarRBM).
The paper also introduces a new method to sample from the trained model, called Parallel Trajectory Tempering (PTT), and compares it with the Annealed Importance Sampling (AIS) method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Technically sound.

The proposed pre-training method is shown to improve on other training schemes for RBMs (namely the standard PCD method, and the Jarzynski reweighting method).

The novel PTT method is shown to improve on standard Gibbs sampling.

Weaknesses:
My main concern is about the usefulness of the RBM approach to generative modeling. (cf question Q1 below).

The PTT sampling method seems to require more memory than standard methods for RBMs.

It is unclear how novel the proposed pre-training method is. (cf question Q2 below)

The paper contains a link to a github repository that reveals the author’s identity (section 8 page 9):
https://github.com/nbereux/fast-RBM

Minor. Line 465: Appendix A.2 references itself.

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes to pretrain RBMS with a recently developed convex approximation, the restricted coulomb machine and then fine-tune the model using standard techniques like PCD. Further, a novel sampling technique, PTT is proposed that can sample from the final trained model by employing a sequence of model snaphsots during training, that are then connected via replica exchange in the style of parallel tempering.

Experiments are conducted and results show that the sampled distributions, when projected to the first few principal components, match the true distribution better. Moreover, log-likelihood comparisons based on single training runs show that the proposed method starts at much higher likelihood values due to the initialisation and there is some evidence that the resulting model als reaches higher likelihood values. Further experiments for PTT show that it is more likely to jump between clusters of the distribution than PCD based on gibbs sampling

Disclaimer: This is an emergency review. I have not had the time to do a detailled analysis, or implement/reproduce any of the results. While I am expert in the field, I will adapt my confidence accordingly. I will not fullfy abide to the review format.

Edit: An edit has been performed that only included changes of the format, but not the content.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The use of the approximation using the restricted coulomb machine as an initialisation is an interesting idea that is worth investigating and the results in Fig 3C suggest that this pretraining approach is very effective.

- The idea of using the training models as sampling steps is an interesting approach as well.

In general, i can see that both techniques can become tools in a more general RBM toolbox, however both of them seem to be incremental changes, even though they could impact RBM training a lot.

Weaknesses:
The main weaknesses of the present work are in two areas: experiments/comparisons and language, of which I only deem the former critical, while the latter will limit the potential impact of the article in the machine-learning community. As a result of the weaknesses, this paper has a number of misleading claims or claims that are not supported by the data presented in the work.

Experiments/Comparisons:

- The authors dismiss using PT for being ""expensive"" and do not compare against it. This is against evidence that even single steps of PT chains with a moderate number of parallel chains can be more efficient than PCD using similar resources. This is especially interesting, since the authors use PCD-100 for training, which allows a lot of resources for PT.  Note that the authors themselves reference [40] which shows an order of magnitudes improvement over Gibbs sampling already with 10 chains.
Another example is given by 
[*] https://www.sciencedirect.com/science/article/pii/S0004370219301948
where the authors used PT for training with k=50 chains and only a single sampling step per iteration. Note that this paper uses a very simple heuristic to improve PT sampling: choose the reference (temperature ->infty) distribution as not the uniform, but the marginal data distribution. This is in contrast to [40] that used the uniform distirbution, leading to a significantly worse baseline.

- There are almost no comparisons of log-likelihood values or values of normalisation constants for the proposed technique. While some are given in Figure 3C, they are only single run and only using approximated likelihoods. Due to this, the phrase "" significantly higher log-likelihoods"" in the conclusion is NOT supported by the data, given there is not enough data to test for significance or even measure the variability.

- The training is also cut short, or the RBMs trained are not powerful enough, since 3B shows clear artefacts in all samples, indicating that none of the machines approximate the dataset well. Since RBMS with enough latent variables are universal function approximators, we can not get a definitive statement of whether the proposed pretraining does allow for better likelihoods. Since the experiments are not very expensive, this reviewer would propose to at least repeat experiments in order to obtain error bars on Fig 3C. 

- The learning rate of 0.01 used in the experiments seems to be on the high end. This is not only bad for PCD training, but also for obtaining high likelihood values, and could explain big parts of the leveling out of the graph in Fig 3C. While it is okay to use a high learning rate in the beginning, keeping it constant over the course of training seems like an oversight.

- For PTT especially, there are no good comparisons that compare the quality of the samples in terms of representing the true distributions. While visual examples are shown that show visually good mixing, the baseline to compare to is again PCD, and not PT, nor stacked tempering. Since, again [40] showed that both alternatives clearly beat gibbs sampling given the same amount of resources, we do not know how good this sampling scheme really is, compared to strong baselines. This reviewer suggests to compare in at least one experiment the proposed approach to an approach with known normalisation constant and then measure how well estimators based on these samples work, similar to [*]. This would also partially verify 3F. 
- As an addendum to the previous point: [*] showed that the performance of AIS improves significantly with the choice of reference distribution. Using the same distribution as [*] or the pretrained distribution proposed in this work, might diminish any performance gains by PTT. This would still be a major improvement to the state of the art.



Language:
- While in general well written, this work reads like targeting a physics community, not the ML community. As a result, this paper includes  slang terms mostly encountered in statistical physics/thermodynamics, which do not have a clear meaning in the statistics community. This includes terms like ""phase transitions"" (first and second order), ""equilibrium models"" (not consequential for the article, nor the review, but this reviewer genuinely does not know what this term is supposed to mean), ""critical transitions"", ""relaxation times"", ""free energy"". Since most of these terms appear in sections where the authors try to explain the method and/or its consequences, a significant number of readers will not be able to understand those reasonings as they do not have a grasp of the physical analogies. This reviewer would propose to replace some of the terms by the statistical equivalent, or to introduce them. 
- Some of the explainations and reasonings are misleading. The initial paragraph highlights that RBMs are supposedly interpretable. While they are simple models, Binary RBMs are still universal function approximators and thus it is highly unlikely that the latent space has any meaning that aligns with any human interpretable semantics. If the authors disagree with this, This reviewer encourages them to add a citation to line 25.
- Missing citations: the datasets used should be cited.
- Figure references missing: the article does not always refer to the figure they are talking about, e..g, line 205. In general references should include the subfigure letter as the article does later, e.g., lines 263+

Limitations:
The authors mark ""Yes"" for point 2 ""Limitations"" on the checklist. This reviewer has not found that the authors discussed the limitations of their work. This especially includes the guidelines that say that authors should ""reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs."" However, in fairness, the authors answer ""[NO]"" in question 7, discussing the presence of error bars or significance tests to measure significance. However, since this is not part of the final publication, and the authors include misleading claims about significance of results, this must be discussed in the main text, or the phrasing weakened.

I have not found any information on runtime or CPU/GPU hours, but the CPU/GPU were reported, so point 8 is partially fulfilled.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The manuscript suggests to apply 

Aurélien Decelle and Cyril Furtlehner. Exact training of restricted Boltzmann machines on intrinsically low dimensional data. Physical Review Letters, 127(15):158303, 2021.

to initialise persistent contrastive divergence (PCD) learning for RBM training and estimating the log likelihood / partition function of RBMs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
While one may argue that the novelty is limited because the interesting theoretical work was done in the abovementioned paper by Aurélien Decelle and Cyril Furtlehner, the idea is sound and the approach may be useful in practice.

Weaknesses:
The novelty is limited because the interesting theoretical work was done in the abovementioned paper by Aurélien Decelle and Cyril Furtlehner.

My **main criticism** refers to the empirical evaluation, and I think these questions should be addressed:
- Was enough effort out into the baseline methods (including hyperparameter choice)?
- What would be an example where the PCA decomposition is misleading (i.e., not helping or even slowing down the process)?
- What about MNIST with all 10 digits?

**Details** (not ordered by importance):

* „On the diametrically opposite side (on interpretability)  are generative ConvNets [9, 10], where the energy function is formulated as a deep neural network, which are capable of synthesizing photorealistic images but are almost impossible to interpret as a physical model.“: Not clear, perhaps add half a sentence to elaborate.

* „second-order phase transition“: define what this is already in the beginning

* Beginning of section 2: I suggest to add the analysis in 

  Fischer, Igel. Bounding the Bias of Contrastive Divergence Learning. Neural Computation, 2011
  https://direct.mit.edu/neco/article-abstract/23/3/664/7646/Bounding-the-Bias-of-Contrastive-Divergence?redirectedFrom=fulltext

  to the discussion of the limitations of CD.

* „much better than those obtained with the standard Annealing Important Sampling (AIS) techniques“:
In [42], several methods are discussed, in particular one based on Bennett’s Acceptance Ratio method (BAR), which performed in general better than standard AIS. How does the proposed method perform in comparison to BAR?

* What if linear PCA is not well suited to find a good representation of the data because of a highly non-linear latent structure?

* I am not fully happy with the selected benchmark tasks. 
In particular: How does the method perform on MNIST with all 10 digits?

**Minor** comments: 

The reference list should be revised. Inconsistent capitalisation, author first name abbreviations, etc.

Limitations:
I think the limitations should have been explored in more depth.
What would be an example where the PCA decomposition is misleading (i.e., not helping or even slowing down the process)?

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
SqW42eR2wC;"REVIEW 
Summary:
This paper studies inverse RL with learnable constraints in the offline setting, focusing on practical applications in healthcare tasks. The main approach appears to be combining the decision transformer architecture in the offline RL literature with inverse constrained RL with max entropy framework. Experiments were conducted on two healthcare tasks: sepsis and mechanical ventilation.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Studies an interesting and important problem.

Weaknesses:
There is significant issues with the writing: the text is incoherent throughout and really hindered my understanding of the paper.

Limitations:
Limitations are discussed.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper uses the Inverse Constrained Reinforcement Learning (ICRL) framework to infer constraints in healthcare problems from expert demonstrations. It proposes the Constraint Transformer (CT) to address the dependence of decisions on historical data, which is generally ignored in ICRL methods with Markovian assumptions. It borrows the causal transformer from the previous decision transformer to incorporate history into constraint modeling. Additionally, a model-based offline RL model is trained to generate violating data. The CT demonstrates improved safety by effectively modeling constraints based on both violating and expert data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
-	The paper addresses a gap in existing ICRL applications by integrating historical data into the decision-making process.
-	The paper augments the violating data in the offline training dataset with a generative world model.
-	The proposed method has been thoroughly evaluated in three aspects: effective constraints, improved sepsis strategies, and safe policies.

Weaknesses:
-	The proposed method depends heavily on the generated violating data, which defines the objective function in the constraint transformer. How sensitive is the estimated policy to the generative world model? Figure 12 shows that the action distributions in the expert dataset and the violating dataset are different. The VASO action seldom takes a large value in the violating dataset. Will this distribution difference cause any trouble in the learning of the constraint?
-	Could the authors provide some details on how the DIFF between the estimated policy and the physicians' policy is calculated through graphical analysis? It would also be helpful if the authors could explain how Figure 7 is plotted. Is it calculated based on the dosage differences at each timestamp? In addition, what are the implications of the three DIFF evaluation metrics in Table 2? Since both IV and VASO are part of the action space, is the ACTION DIFF alone sufficient to evaluate the estimated policy?

Limitations:
The authors have addressed the limitations of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the Constraint Transformer (CT) framework to enhance safe decision-making in healthcare. The proposed CT model uses transformers to incorporate historical patient data into constraint modelling and employs a generative world model to create exploratory data for offline RL training. The authors supported their points by presenting experimental results in scenarios like sepsis treatment, showing that CT effectively reduces unsafe behaviours and approximates lower mortality rates, outperforming existing methods in both safety and interoperability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper shows its strengths in the following aspects:
- The paper addresses the novel angle of ensuring safety in offline reinforcement learning (RL) for healthcare, a critical and previously underexplored issue.
- It incorporates Inverse Constrained Reinforcement Learning (ICRL) into offline reinforcement learning (RL) for healthcare, introducing a novel approach to inferring constraints from expert demonstrations in a non-interactive environment.
- The implementation of a causal transformer to learn the constraint function is interesting, allowing the integration of historical patient data and capturing critical states more effectively.
- Extensive results on 2 datasets are presented. The proposed Constraint Transformer (CT) framework is shown to reduce unsafe behaviours and approximates lower mortality rates.

Weaknesses:
Despite its strengths and novelty, this paper suffers from several critical technical flaws, primarily concerning the soundness of evaluation rather than the method itself:

1. **Definition of Metric**: The metric $\omega$ is defined by comparing drug dosages related to **mortality rate**, which I believe is a flawed definition, even though it has been used in previous papers. Mortality rate can be influenced by numerous factors, making it unsuitable as a reward for RL, which considers a limited number of drugs. It is challenging to convince clinicians that mortality can indicate the 'treatment quality' of vasopressor or mechanical ventilation. This suggests that the reward is not solely a function of the previous action and state but also many unconsidered features (hidden variables) in the datasets, such as adrenaline, dopamine, historical medical conditions, phenotypes, etc. [1] pointed out that doctors usually set a MAP target (e.g., 65) and administer vasopressors until the patient reaches this safe pressure; [2] suggests using the NEWS2 score as the reward supported by clinical evidence. None of these directly use mortality. While it is understandable that this paper is not a clinical study, and hence, it is not the authors' responsibility to identify clinically appropriate reward designs, I recommend referring to [1] and [2] for a reward design that makes more clinical sense.

2. **Definition of Optimal Policy**: This paper follows [3]'s definition of optimal policy. From my understanding, the clinician's policy $\hat{\pi}$ is approximated by a neural network. [2] pointed out that in the sepsis dataset, the behaviour policy can result in critical flaws in a very small number of states. Although the number of incorrect predictions is limited, they can still bias the off-policy evaluation results severely. I suspect this paper may encounter a similar issue. The authors should provide experiments and visualizations on the learning quality of the behaviour policy to justify their approach.

3. **Model-Based Off-Policy Evaluation**: The data imbalance in both the sepsis and ventilation datasets is significant. It is questionable whether the learned model can generalize well. The most acceptable way to validate the method remains using simulated data where all policies can be tested online. One possible testbed is the DTR-Bench[4] medical simulated environment.

The paper has a few other minor technical flaws compared to the above three.

[1] Jeter, Russell, et al. ""Does the"" Artificial Intelligence Clinician"" learn optimal treatment strategies for sepsis in intensive care?."" arXiv preprint arXiv:1902.03271 (2019).

[2] Luo, Zhiyao, et al. ""Position: Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination."" Forty-first International Conference on Machine Learning.

[3] Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter Szolovits, and Marzyeh Ghassemi. Continuous state-space models for optimal sepsis treatment: a deep reinforcement learning approach. In Machine Learning for Healthcare Conference, pages 147–163. PMLR, 2017

[4] Luo, Zhiyao, et al. ""DTR-Bench: An in silico Environment and Benchmark Platform for Reinforcement Learning Based Dynamic Treatment Regime."" arXiv preprint arXiv:2405.18610 (2024).

Limitations:
There is no negative societal impact or limitation that needs clarification. 
In addition to the weakness I mentioned, this paper:
1. lacks off-policy evaluation results.
2. may summarise more related work in this 'dynamic treatment regime' field. A few examples are listed below:

[1] Kondrup, F., Jiralerspong, T., Lau, E., de Lara, N., Shkrob, J., Tran, M. D., Precup, D., and Basu, S. Towards safe mechanical ventilation treatment using deep offline reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 15696–
15702, 2023.

[2] Liu, Y., Logan, B., Liu, N., Xu, Z., Tang, J., and Wang, Y. Deep reinforcement learning for dynamic treatment regimes on medical registry data. In 2017 IEEE international conference on healthcare informatics (ICHI), pp. 380–385. IEEE, 2017.

[3] Nambiar, M., Ghosh, S., Ong, P., Chan, Y. E., Bee, Y. M., and Krishnaswamy, P. Deep offline reinforcement learning for real-world treatment optimization applications. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 4673–4684, 2023.

[4] Peng, X., Ding, Y., Wihl, D., Gottesman, O., Komorowski,M., Li-wei, H. L., Ross, A., Faisal, A., and Doshi-Velez,F. Improving sepsis treatment strategies by combining deep and kernel-based reinforcement learning. In AMIA Annual Symposium Proceedings, volume 2018, pp. 887. American Medical Informatics Association, 2018.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors consider healthcare applications of RL algorithms in which implicit constraint modeling is critical for safe recommendations. 
This is modeled as an RL policy optimization with constraints. However, the constraints are often unknown and need to be inferred from expert data trajectories in the healthcare applications. The authors propose a neural network estimator to combine with the constrained MDP formulation. They also identify that the naive way to represent states leads to non-Markov structure. To address these issues, the paper proposes a simple modification (based on prior work on the ""preference attention layer"") to a causal transformer based policy which attempts to model a parametric constraint function. Evaluations are based on healthcare benchmarks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper considers a practically motivated approach grounded in real data for a healthcare application where data could be challenging to obtain.
- Proposes simple addition to the final layer of a causal transformer to parametrize constraints on trajectories learned from expert data.
- Evaluations are conducted on real healthcare domain benchmarks and extensive ablations are included to ensure that the architecture change is meaningful in obtaining the improvements.

Weaknesses:
- It seems like the system (which is the patient) is not feasible to model as evolving according to an Markov process on the observed state at each time, but instead a POMDP with a high dimensional latent state. 

- Clarity and presentation can be improved. In Equation (4), what is $\hat{\tau}$, this was never defined for being such a key component of the procedure. Similar issues in Eq (8) related to clarity.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
SeesCzBelI;"REVIEW 
Summary:
The authors considers methods for removing bias in RMs, specifically the bias towards long responses and the bias certain prompts might have to generate high rewards (this stems from the BardleyTerry model being underspecified.). For the second problem the authors proposed PBC which adds a linear layer to the last token of the prompt, the output of which predicts the average reward of completions from the prompt. For the first problem the authors propose to combine PBC with existing length bias correction methods which adds a correlation term to the loss. For experimental results the authors considers RLHF training LLama-7B on the RM-static dataset. They find that their method outperforms baselines on academic benchmarks (Table 2) and in head-to-head comparisons (Fig 4). They also consider hyperparameter stability and ablations in Fig 5.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Bias in RLHF can potentially have a large impact if addressed correctly.

Weaknesses:
1. Academic metrics like MMLU are not a good fit for RLHF. MT-bench is better.
2. There are no error bars, unclear how strong the signal is. 
3. The writing is rather handwavy at times, e.g. the motivation in section 3.1. is very qualitative.
4. The novelty is low.

Limitations:
na

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the prompt bias in RLHF, especially the reward modeling --- beyond the length bias that might exist.
Alleviating reward hacking is an important topic in RLHF, however, with the current paper, some details or contributions are not very clear. I'll elaborate in the following sections.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The problem studied is important. The illustrative figures are helpful.

Weaknesses:
Some notations do not make sense, for example, in Equation (5), averaging over y does not make sense. Would not it be better to average over the C rather than y.

The presentation of the problem itself is not yet clear to me. Although the authors keep using examples in the context to anchor their ideas (which I appreciate), it is still unclear what is the problem this work aims to solve. I like the general idea of Figure 1, however, what does the red color highlighting mean? This figure makes a good contrast between your RM and conventional RM, yet it fails to illustrate the problem your RM aims to solve.

The experimental results are not supportive enough.

Limitations:
Please see weakness

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the Prompt Bias Calibration (PBC) method to address prompt-template bias in reward training of RLHF. The proposed PBC method is validated through extensive empirical results and mathematical analysis, showing its effectiveness in combination with existing length bias removal methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	Good Writing: The paper is well-written and easy to follow.
2.	Innovative Methodology: Introduces Prompt Bias Calibration (PBC) to address prompt-template bias in RLHF.
3.	Strong Empirical Evidence: Demonstrates significant performance improvements through comprehensive evaluations.

Weaknesses:
see questions

Limitations:
see the above

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the issue of reward hacking in RLHF training, superficially, identifying prompt-template bias, defined as when a reward model (RM) develops a preference for responses that adhere to specific formats or templates, even when these formats are not explicitly specified or desired in the prompt and proposes Prompt Bias Calibration (PBC) method that successfully tackles this issue. PBC can also be combined with existing length debiasing methods like ODIN to mitigate both hacks in the reward signal.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The paper identifies and analyzes ""prompt-template bias"" in RLHF, a potentially impactful issue.
* PBC is easy to implement and as shown can be combined with existing approaches.
* Strong empirical validation with good coverage in the experiments and ablation.

Weaknesses:
* Choosing one specific bias - while the title claims that removing the length bias is not enough, it seems to change the scene to potentially removing length and prompt-template bias not being enough. Leading to concerns about needing to combine many methods, one for each mitigation.

Limitations:
The limitations are covered.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
SOxxGATGsl;"REVIEW 
Summary:
The paper investigates a multi-armed bandit problem where the action space is a metric space a stochastic Lipschitz rewards. The authors present algorithms that use a constant amount of memory and achieve a near-optimal regret. This improves on previous results that had heavy memory usage.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper has good presentation, and the figures in the appendices are helpfulץ The contribution itself is useful in practice.

Weaknesses:
While the result is great, the ideas presented in the paper are modifications of existing methods

Limitations:
I did not find the limitations presented as sufficient and I would like to see more discussion on the downsides of the presented algorithm and future directions of research.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the Lipschitz bandit problem with a memory constraint. There are two algorithms proposed by the authors. The Memory Bounded Uniform Discretization (MBUD) algorithm uses a fixed discretization over the metric space and implements a strategy which explores first and then commits to an exploitation phase. The second algorithm, called Memory Bounded Adaptive Discretization (MBAD) , swaps arms in and out of the memory while creating a mesh over the metric space adaptively (ala zooming). The authors prove upper bounds which match lower bounds from previous work for Lipschitz bandits without memory constraints while maintaining linear time complexity and constant space complexity. Finally, the authors perform experimental validation of the theoretical results on small 1-dimensional datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel problem formulation in the Lipschitz bandit setting.
2. The authors show upper bounds matching with lower bounds from prior work while maintaining a memory budget on arms.
3. The concepts introduced in the paper are well explained for the most part. I did have a little trouble reading the parts about the crosscut and generating cubes but it might just be me not being familiar with prior work in the area.

Weaknesses:
I am unclear about the novelty and contributions of the paper. The problem formulation (limited memory) is new in the Lipschitz bandit setting but it has been studied in several papers in bandits with finite arms (as the authors point out in the related works). Moreover, the proof techniques used in the paper appear standard - MBAD is based on zooming introduced by kleinberg et al., the clean event analysis is from the recent textbook of Slivkins (and their papers), MBUD is based on an explore first strategy resembling the naive Explore-then-Commit algorithm (which trivially satisfies the O(1) memory constraint). In all, I’m not sure what specific parts of the paper are being claimed as novel vs that from prior work.

The experiments in this paper are very limited - only a 1 dimensional interval with an L1 metric. To show real world applicability, it would be nice to have results in higher dimensions and also on real world datasets (since that was the original motivation).

Minor/Typo:
I think the caption for Fig 1 should clarify what is on the X and Y axes. It is obvious from context but it would be nice to have from a readability perspective.

Limitations:
The Lipschitz constant needs to be known beforehand to apply these algorithms.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes two algorithms for Lipschitz bandit problems, with improved time complexity and memory requirements.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
If the algorithms and proofs are sound, then this is an excellent contribution. Developing these sort of streaming/sketching methods for key bandit problems (such as Lipschitz bandits) is an important area of research, and many people are likely to care about the results of this paper.

Weaknesses:
The paper is sloppy to the extent that it is difficult to understand the authors' algorithm or verify their claims. To be specific, consider the following sentences, all taken from a single two-paragraph subsection (section 2.2):
1. ""Let $\\{\mathcal{X}_1, \dotsc, \mathcal{X}_N\\}[\mathcal{X}_i \subset \mathcal{X}]$ be an cover of the action space $\mathcal{X}$"" --- okay, what is the $\\{\dotsc\\}[\dotsc]$ notation?  
2. ""Let $\epsilon$ denote the maximum diameter of $\mathcal{X}_i$ for all $i \in [N]$."" --- okay, but what is a diameter? Are we in a metric space? This hasn't been specified. 
3. ""Then the arm set $S = \\{ x_i \mid x_i \in \mathcal{X}_i, i \in [N]\\}$ is an $\epsilon$-mesh.""  --- what set is this? Now, I presume that the authors mean to say that they want $S$ to be any set that contains a single element chosen arbitrarily from each of the $\mathcal{X}_i$, but that's not written, instead the authors said its __the__ set, but the right hand side does not specify any unique set. Also, the concept of an $\epsilon$-mesh has not been defined, and when its defined, it needs to be with respect to some metric. And if this description was meant to be the definition of an $\epsilon$-mesh... then that's not clear either (and the definition given by the work the authors state these definitions are from, i.e. Slivkins 2019, is _very_ clear---all the authors needed to do was copy it).
4. ""The covering dimension $d$ of the action space $\mathcal{X}$ is defined as $d=\inf_{\alpha \geq 0}\\{|\mathcal{S}| \leq \epsilon^{-\alpha}, \forall \epsilon > 0\\}$."" But the set $\mathcal{S}$ does not depend on $\epsilon$ (not even implicitly)... (the correct definition, I presume, would be to ask that $\mathcal{S}\_{\epsilon}$ is a minimal $\epsilon$-cover of $\mathcal{X}$ in some metric $D$, and then have that infimum include $\mathcal{S}_\epsilon$ and not $\mathcal{S}$.)
5. ""Define $\mathcal{Y}\_j = \\{x \in \mathcal{X} \colon 2^{-j} \leq \Delta(x) \leq 2^{1-j}, j \in \mathbb{N}\\}$, then the set $\mathcal{Y}_j$ contains all arms whose gap is between $2^{-j}$ and $2^{1-j}$."" --- but $j \in \mathbb{N}$ is within the constructions of the set on the RHS, which could be read as asking that the condition holds for all such $j$, or for some $j$, but it breaks the dependence of the right hand side on the subscript $j$ of $\mathcal{Y}_j$. Of course, the definition shouldn't have the $j \in \mathbb{N}$ inside the $\\{ \dotsc \\}$ on the right hand side of the definition. 
6. ""Consider the $\epsilon$-mesh $\mathcal{S}_j$ for space $\mathcal{Y}_j$."" --- __the__ $\epsilon$-mesh? Also, $\mathcal{S}_j$ hasn't been defined. This should say instead 'fix some $\epsilon > 0$ and let $\mathcal{S}_j$ be an $\epsilon$-mesh of $\mathcal{Y}_j$', or something like that.
7. ""[...] the zooming dimension focuses only on the set $\mathcal{Y}_j$"" --- no, the zooming dimension depends on all the sets $\mathcal{Y}_1, \mathcal{Y}_2, \dotsc$, not only a single one of those sets.

While each individual mistake or ambiguity can be resolved easily enough, verifying the authors claims would require me to rewrite everything myself, and this goes beyond what I'm willing to do (and should do...).  The whole paper is like this, and it's just not acceptable.

I would urge the authors to, in the future, have someone _not intimately familiar with the work_ proof-read the work.

Note, I put down confidence as ""5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."" --- I am indeed very familiar with the related work, but I have not checked the math/other details carefully. It's too much work to read it. I am absolutely certain, however, that this level falls short of any level of clarity that might be expected in published work.

Limitations:
.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers regret minimization for Lipschitz bandits with time horizont $T$ and proposes an algorithm that provably achieves nearly optimal regret while having strictly smaller (by a factor of $T$) time (of order $O(T)$) and memory complexity (of order $O(1)$). This is achieved by considering a tree-like embedding of the state space and pairwise comparison between elements of the tree. A suboptimal method with uniform discretization called MBUD has dependence on the covering dimension of the state space, while MBAD, a method with adaptive discretization, instead has dependence only on the zooming dimension.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) Achieving nearly optimal regret bounds in minimax (MBUD) and instance-specific setting (MBAD), while reducing time and memory complexity. 

2) Both proposed algorithms are non-trivial and seem to be novel and interesting on their own.

Weaknesses:
1) It is not simple to parse algorithms in their current form in a short amount of time. Although you give comprehensive descriptions in text, I believe adding illustrations or additional explanations will significantly improve clarity of your algorithms. 

2) I would appreciate a more explicit comparison with previous work - what parts of the algorithms were already reported in the literature?

Limitations:
The authors have addressed limitations adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
SLXHVsbU8r;"REVIEW 
Summary:
This paper handles the costly modularization and 3D manual annotation in current end-to-end autonomous driving, which proposes an unsupervised pretext task to provide necessary environmental information, as well as a direction-aware training strategy to enhance the robustness in safety-critical steering scenarios. 

The authors conduct comprehensive experiments in both open- and closed-loop evaluation benchmarks, which demonstrate the effectiveness in various metrics. Moreover, the improvements are obtained with much less resource cost and faster inference speed, which is surprising and impressive.

In addition, this paper gives in-depth discussion and performance comparison about the usage of ego status in the open-loop evaluation of nuScenes. The considerable improvement in the intersection rate with the road boundary, which is proposed in recent BEV-Planner, again proves the superiority of the designed pretext task.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Overall, I am rather positive on this paper. In particular, I really like the motivation of this work that aims at finding a solution to relieve the heavy annotation and computation overload in current end-to-end autonomous driving. I believe this paper can inspire other works and facilitate this field. The strengths in this work include:

(1)	Enough novelty. This paper introduces an innovative unsupervised pretext task to perceive the environment, which is completely different from other works that accumulate subtasks requiring massive 3D annotation and computation resources. 

(2)	Good performance. This paper demonstrates excellent performance and fast inference speed in both open- and closed-loop evaluation compared with other end-to-end methods. In specific on the challenging metric, i.e., intersection rate in BEV-Planner, the proposed approach surpasses other methods by a considerable margin. This clearly shows the effectiveness and advantages of the proposed method.

(3)	Insightful analysis. The authors provide extensive experiments and analysis for the proposed method. I appreciate this. The experimental analysis with various ablation studies allows a better understanding of each module. Notably, the authors observe the different computation ways of open-loop evaluation metrics between ST-P3 and UniAD and provide performance comparison with different settings, showing the comprehensiveness.

(4)	Good writing and organization. This paper is well-written and organized. Each section has a clear motivation. It’s easy to follow the ideas. I enjoy reading the paper.

Overall, I believe this paper is significant to the autonomous driving community because it shows new insights and directions in designing simple but effective E2EAD framework with SOTA performance.

Weaknesses:
(1) In this work, the 2D ROIs are crucial for the designed pretext task. I noticed that the authors adopt the open-set 2D detector GroundingDINO to generate the ROIs. Then the results and discussion of using other third-party detectors should be presented.
(2) The proposed method is shown to be efficient with the unsupervised pretext task and self-supervised training strategy, which is nice. It is suggested the authors show the influence of the training data volume (e.g., 25% and 50%).

Limitations:
It is suggested to provide discussion of limitations and broader impact in the revision.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper addresses the limitations of current end-to-end autonomous driving models that still rely on modular architectures with manually annotated 3D data. The authors propose an unsupervised pretext task that eliminates the need for manual 3D annotations by predicting angular-wise spatial objectness and temporal dynamics. This is achieved through an Angular Perception Pretext that models the driving scene without the need for manual annotation. A self-supervised training approach is introduced to enhance the robustness of planning in steering scenarios. This strategy learns the consistency of predicted trajectories under different augmented views. UAD demonstrates significant improvements in performance over existing methods like UniAD and VAD in both open-loop and closed-loop evaluations. It achieves these improvements with reduced training resources (44.3% of UniAD) and faster inference speed (3.4× faster than UniAD).

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper introduces UAD, an unsupervised autonomous driving framework that eliminates the need for costly 3D manual annotations, which is a significant departure from traditional modular approaches. 
2. The Angular Perception Pretext is an innovative approach to spatial-temporal understanding without manual labeling, offering a new perspective on autonomous driving perception.
3. The experiments conducted are comprehensive, including both open-loop and closed-loop evaluations, which demonstrate the method's effectiveness across different scenarios. The paper provides a detailed comparison with state-of-the-art methods like UniAD and VAD, showcasing the improvements in performance metrics, which adds to the quality of the research.

Weaknesses:
1. UAD treats an entire sector as occupied when only a part of it contains an object. This seems imprecise. This could potentially lead to less accurate spatial understanding of the environment. In autonomous driving, overly coarse representations might result in the vehicle making less accurate decisions, such as unnecessary braking or incorrect path planning. Have the authors tried some open world segmentation models for more accurate spatial information?
2.  The paper draft does not provide explicit evidence or analysis on whether UAD can indeed benefit from training on a larger scale of data. The authors could conduct experiments with varying sizes of datasets to empirically evaluate how performance metrics change as more data becomes available. This could provide insights into the benefits of scaling up.

Limitations:
In the current draft, UAD might be limited to basic obstacle detection and does not extend to the interpretation of traffic signals.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to discard the requirement of 3D manual annotation in end-to-end autonomous driving by the proposed angular perception pretext task. Besides, this paper proposes a direction-aware learning strategy consisting of directional augmentation and directional consistency loss. Finally, the proposed method UAD achieves superior performance in both open-loop and closed-loop evaluation compared with previous vision-based methods with much lower computation and annotation costs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) This paper aims to discard the requirement of 3D manual annotation in end-to-end autonomous driving, which is important and meaningful for training larger end-to-end autonomous driving models at scale. I totally agree and appreciate this.
2) This paper proposes a direction-aware learning strategy, which will further improve prformance by self-supervised learning.
3) UAD is evaluated in both open-loop and closed-loop evaluation and different metrics (UniAD, VAD, and BEV-Planner).

Weaknesses:
1) There is a lack of explanation on how to use ego status. Besides, there should be more experiments about the performance of UAD without ego status.
2) There is a lack of explanation on how many frames are fused and what method is used for temporal fusion (sliding window or streaming).
3) The angular perception pretext task introduces 2D detection information for perception learning on BEV features. According to Table 6, it seems that angular design is very important for UAD. However, BEV-Planner can achieve a not-so-bad result without any 3D manual annotation and 2D detection information. Therefore, verifying the effectiveness of angular design on BEV-Planner will be more convincing.
4) For the proposed angular design, I do not think is very novel. Because the effectiveness of the 2D detection auxiliary head has been verified in BEVFormerV2 [1] and StreamPETR [2], the UAD just converts 2D object detection to BEV segmentation.
5) For the proposed direction-aware learning strategy, although it is useful, it is a method of data augmentation in EBV space, which I do not think is very novel.

Limitations:
See the weakness and questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The article proposes an end-to-end (E2EAD) autonomous driving method called UAD (Unsupervised Autonomous Driving), which achieves autonomous driving on a visual basis without the need for expensive modular design and 3D manual annotation. UAD aims to overcome the limitations of existing E2EAD models that mimic traditional driving stack module architectures. These models typically require carefully designed supervised perception and prediction subtasks to provide environmental information for planning, which require a large amount of high-quality 3D annotation data and consume significant computational resources during training and inference processes.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. The method is novel and a good direction for exploring the end-to-end model's dependence on 3D manual annotation.
2. The paper has rich ablation experiments to demonstrate the effectiveness of the method.
3. The paper has advantages in both speed and accuracy compared to previous articles

Weaknesses:
1. The paper lacks sufficient comparison with other methods(such as Interfuser[1],DriveAdapter[2],DriveMLM[3],VADv2[4]), which acheve better closed-loop performance on carla town 05 long benchmark.
2. In Table 6, the angular design brings too much gain, especially in terms of collision rate (from 1.37% to 0.19% ). It's strange. The angular design is about how to encode the sensor data and should not have so much impact on collision rate.
3. Angular design is widely used in BEV-related works, like PolarFormer[5]. The paper lacks citation about these works.And it's not proper to regard it as the main contribution of the work.
4. The paper lacks a part to introduce the use of 2D tasks as auxiliary tasks.

[1] Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and Yu Liu. Safety-enhanced autonomous driving using interpretable sensor fusion transformer. In Conference on Robot Learning, pages 726–737. PMLR, 2023
[2] Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, and Hongyang Li. Driveadapter: Breaking the coupling barrier of perception and planning in end-to-end autonomous driving. 2023
[3] Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu, Hanming Deng, Zhiqi Li, et al. Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving. arXiv preprint arXiv:2312.09245, 2023
[4] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024
[5] Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin Gao, Weiming Hu, Yu-Gang Jiang. PolarFormer: Multi-camera 3D Object Detection with Polar Transformer. AAAI 2023

Limitations:
1. Incompele comparison on CARLA benchmark. 
2. Lack citation about angular design. 
3. Angular design is not novel.
4. Some of the experiment results are not that convincing.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a new e2e driving model named UAD. In this paper, the authors propose an unsupervised method for effective training and inference of the e2e model. The paper mainly has two contributions: 1. It designs an angular-wise perception module. In this module, the authors directly project 2D GT labels onto the BEV and define a new BEV map label for perception training. This module design can efficiently reduce complexity and preserve effectiveness. 2. The authors propose a direction-aware method to augment the trajectory training and use consistency loss for further supervision. 

The final results show the effectiveness and soundness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of the angular-wise perception module is interesting. It can utilize a huge amount of 2D annotated autonomous driving datasets to train the e2e model, which removes the restriction of the limited number of 3D annotations.
2. The proposed direction-aware method for trajectory prediction is also meaningful since it can add additional consistency loss for better supervision.
3. The results are promising and the efficiency improvement is impressive.

Weaknesses:
1. The design of the angular-wise perception module is a little bit counter-intuitive to me. From my perspective, it works because (1) It can greatly enlarge the size of the training data. (2) It makes the perception task simpler, thus the model can do it better (knowing an object in a direction is much simpler than detecting the BBox). (3) The efficiency improves because of the light design of the perception task. I think it can be simply treated as a low-resolution object detection task without depth. Except for (1), I do not understand why it can improve the final results.

2. The design of the direction-aware planning training strategy is effective but simple. I cannot see too much insight here. Could the authors provide more insight into this? Or is this just an engineering trick?

3. For experiments, do you use exactly the same training data for both the open-loop and close-loop experiments? If yes, could you provide more analysis about why the results are surprisingly good even if you use a low-resolution detection module? If not, could you provide details about the pertaining data info? Can the impressive results come from leaked data? How to prevent testing data leakage?

4. In real-world applications, when the vehicle plans its path, it needs to ""see"" a lot of things, including objects and some other elements, like traffic lights, traffic signals, or some special marks on the road. How can you handle these elements based on your model? For example, can your model understand traffic signals and see red traffic lights? If not, how to extend your model to real-world scenarios and applications?

Limitations:
My main concern with this paper mainly comes from the insight of the angular-wise perception module. I still cannot understand why it works except for the huge amount of additional training data. Provide more details for this. 

Besides, how to deploy and extend the model to real-world cases, that requires depth information (e.g., ) or semantic information (e.g., traffic light)? The authors should provide more discussion about this to make the work more promising.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
JVtwC9RzlI;"REVIEW 
Summary:
This paper proposes a method to integrate the processing and generation of both text and graph data using a single transformer-based model. Structure Token encodes graphs with text labels into a sequence of tokens, enabling the handling of both data types interchangeably. This approach leverages a unified representation within a Transformer Encoder-Decoder model, enhanced to incorporate structure tokens. They evaluate TextGraphBART on text-to-graph (T2G) and graph-to-text (G2T) tasks, demonstrating comparable results to baseline models with fewer parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The introduction of structure tokens is a significant advancement, providing a unified method for processing and generating both text and graph data.
2. The method integrates seamlessly with existing Transformer architectures, requiring only minor modifications, and avoids the need for specialized loss functions or additional modules.
3. Empirical results show that TextGraphBART achieves comparable performance on both T2G and G2T tasks with baselines but with fewer parameters.

Weaknesses:
1. The baseline settings are weak. It lacks of comparing with some strong baselines with advanced graph-structured-aware methods [1][2].
2. The model settings are limited. This paper only try one size of model, and further experiments with larger model sizes are needed.
3. The provided ablation study, though useful, could be more detailed, exploring the interactions and contributions of various components more comprehensively.

Reference: 
[1] Stage-wise Fine-tuning for Graph-to-Text Generation
[2] Self-supervised Graph Masking Pre-training for Graph-to-Text Generation

Limitations:
Yes. The limitations are discussed in Discussion section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces TextGraphBART. Its a new method of encoding graph/text-input by using a structure token. This new token should preserve graph structure as opposed graph linearization or cycle training. This should also allow for the generation of graphs, with accompying text tokens, without making architectural changes to the transformer. It is then verified on both Graph2Text and Text2Graph tasks for commonly used benchmarks such as WebNLG and EventNarrative. It is followed by results that show comparable results to previous works, and a short discussion and conclusion.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Paper is generally nice to read
2. The work considers a lot of relevant related work, which they both refer to or base decisions on
3. The build-up of the structure token is well explained

Weaknesses:
1. The work proposes a seamless integration of both text and graph inputs, but only verifies on strictly one-directional modal evaluation: Either text-to-graph or graph-to-text. However, as the authors claim that this is the ""first method that can autoregressively generate sub-graphs with multi-token labels without modifying transformers."", a single text-to-graph benchmark feels like a very shallow evaluation.
2.  Models are evaluated on datasets that already work well. This makes it hard to distinguish the added value of such a structure token. Scenarios are sketched where e.g. graph linearization is treated simply as a sequence of tokens, but then it this work should methodologically be verified on datasets where preserving graph structure should be required. This is now not the case.
3.  This is reflected in the results: they are comparable, so the structure token might aswell not be there. The proclaimed difference that this is done with less parameters I would argue is a very weak claim as the difference is not that big.
4.  Finally, I am not convinced that the proposed ""structure token"" is a truely lossless way of preserving graph structure through-out a transformer. The structure token and its generation is explained in Figure 2, and is claimed to be lossless. However, this is only true in the operations before the embedding leveraging the orthogonal property. After the embedding into a structure embedding, information will (seemingly) be lost about structure in this embedding. From this structure embedding, it cannot be determined what the original structure of the graph was?
5.  In the discussion the authors mention scaling, but there is no clear evidence for this as the authors only tested one model size. This is essentially an unsubstantiated claim.
 6. Why is the domain token needed? This is not further addressed or experimented with.

Limitations:
The authors partially address this limitations by motivating the paper in the introduction. However, I dont feel the discussion (or the rest of the work) addresses the actual limitations of this method. What are pros or cons compared to transformer modifications?

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new unified graph-text generation framework, TextGraphBART, for the large language model. The paper tries to address both the generation and representation of text and graphs. The paper proposes a new structure token to encode text graphs into a set of tokens. The structured token can encode the text graphs and be decoded into text graphs. Specifically, it consists of seven different embeddings, including position, domain, and text information. The paper pretrained the proposed framework over four different tasks, including text2text, graph2text, text2graph, and graph2graph. The model is pretrained on TEKGEN and GenWIKI and tests on the event narrative and WebNLG. The performance is evaluated on BLEU, METEOR< and BERTScore. The paper compares the model with T5, BART, GAP, CYcleGT, ZBT5, and Grapher. The paper also includes an ablation study and analyzes the future directions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The motivation of the paper is clear and the topic of the paper is interesting. The paper investigates an important unified graph-text generation framework for the language model. The idea of structure tokens is interesting and can contribute to future research. Additionally, the model seems to achieve good results while using fewer parameters.
2. The paper tests on both graph-to-text and text-to-graph datasets. The proposed method achieves better results compared to other baselines. The paper also conducts an ablation study to investigate the contribution of each component. It includes a discussion session to explore the paper's current limitations.
3. The paper provides code and implementation details.

Weaknesses:
1. The proposed framework is incremental. Multiple previous papers have used the idea of using different position embeddings to represent the structure information [1,2,3]. The idea of joint text-to-graph and graph-to-text pretraining/generation is also not new [4]. 
2. The experiment is not comprehensive. The paper used eventnarrative for the graph-to-text generation. However, compared to WebNLG  (2020), the EventNarrative has fewer baselines and most of its baselines are outdated. The paper must also add WebNLG(2020) as an additional graph-to-text generation tool to show its contributions. The pretraining datasets and the testing datasets are all Wiki-style datasets. The performance gain may come from pretraining (data leakage) instead of the actual model architecture. The paper needs to include an additional ablation study to show the gain of the eventnarrative/webnlg comes from the model architecture, or also pretraining the proposed baselines on TEKGEN/GenWiki. Otherwise, the scores are not comparable. Additionally, the paper needs to include some of the latest frameworks like [4] since all of the baselines used in the paper are old. The paper also needs to include some human evaluation or qualitative analysis to help readers understand the generation results better. Furthermore, the comparison in Table 4 is not fair, since domain-specific pretraining is more useful than general pretraining [5].
3. The paper puts important information in the Appendix while not reaching the page limit (9 pages). 

[1] Herzig, J., Nowak, P. K., Müller, T., Piccinno, F., & Eisenschlos, J. M. (2020). TaPas: Weakly supervised table parsing via pre-training. ACL 2020.

[2] Wang, Q., Yavuz, S., Lin, V., Ji, H., & Rajani, N. (2021). Stage-wise fine-tuning for graph-to-text generation. arXiv preprint ACL 2021 SRW.

[3] Chen, W., Su, Y., Yan, X., & Wang, W. Y. (2020). KGPT: Knowledge-grounded pre-training for data-to-text generation. EMNLP 2020.

[4] Wang, Z., Collins, M., Vedula, N., Filice, S., Malmasi, S., & Rokhlenko, O. (2023). Faithful low-resource data-to-text generation through cycle training. ACL 2023

[5] Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., & Smith, N. A. (2020). Don't stop pretraining: Adapt language models to domains and tasks. ACL 2020.

Limitations:
The paper includes a limitation section in the Appendix and a Discussion section in the main paper.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
- The paper highlights the limitations of two existing methods for generating text graphs: 1. The multi-stage approach does not consider multi-hop relations and cannot handle the case where two concepts have more than one relation. 2. The graph linearization approach introduces extra complexity to the Language Model (LM) and the predictions are altered if the generated triples are shuffled.
- Building on this, the paper proposes the Structure Token method. Specifically, it identifies a graph element (node or edge) using seven parts. Then, it transforms Structure Tokens into embeddings using OneHot and orthonormal-like vectors, which are then input into a Transformer Encoder-Decoder model.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The method proposed in the paper can avoid extra computation, such as the duplication of concepts.
- The experimental results presented in the paper show that the model performs comparably to models with a larger number of parameters while using fewer parameters, suggesting that the model might perform even better with an increased number of parameters.

Weaknesses:
- The paper's layout is not aesthetically pleasing, particularly the formatting of Tables 1-3.
- The experiments lack error bars, which diminishes the credibility of the results.
- The experimental results are unsatisfactory and fail to demonstrate the superiority of the model, and the experiments are incomplete.
  - The model's results did not achieve state-of-the-art (SOTA) performance in the G2T/T2G tasks.
  - Lines 257-259 state, ""In conclusion, our structure token approach can achieve comparable performance on text-to-graph generation under similar model size without using special training methods or loss functions."" The results in Table 3 are nearly identical to those of Grapher-small (Text) T2G. Combined with the absence of error bars, it is challenging to determine whether the similarities are due to errors or model efficacy, making it difficult to convince.
  - Existing results only may demonstrate that the model performs comparably to models with larger parameters under fewer parameters. This does not prove that increasing the number of parameters will enhance performance. A more direct comparison of experimental results is necessary to substantiate this claim. The explanations in Section 5, ""Scaling Up,"" are overly subjective and unconvincing, making the paper seem like a work in progress.

Limitations:
Author addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Rllg9nOasE;"REVIEW 
Summary:
This paper introduces a novel approach using cross-modal and multi-modal models to align brain activity with naturalistic stimuli, evaluates several unimodal Transformer models, and examines the effects of removing unimodal features from multi-modal representations on brain alignment.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- (S1) The paper presents a thorough imaging data analysis, including a detailed description of the dataset, a comprehensive comparison of methods, and a clear summarization of results, which enhances the robustness and transparency of the study.

- (S2) The paper is well-written and clearly presented, making the complex methodologies and findings accessible and easy to understand for readers.

- (S3) The study provides a rigorous comparison of methods, including various method variations, which highlights the strengths and weaknesses of each approach and demonstrates the thoroughness of the analysis.

Weaknesses:
- (W1) Figure caption should be more demonstrative and detailed. For example for Figure 1's captions, ""Residual analysis"" is too vague for the reader to understand the figure. For Figure 5, if the colorbar range from -0.5 to 0.5, then it does not represetn the percentage, but the proportion instead.

Limitations:
The Limitation section is properly included in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a framework for applying brain encoding models with multimodal stimuli. They apply this to a series of video, audio, and mutlimodal models (cross-modal and jointly embedded models). They introduce a residual analysis to analyze the impact each particular feature had on the corresponding fit in the encoding model. They find that multimodal models significantly out-perform their unimodal counterparts on certain language- and vision-related regions in their fMRI dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Incorporating a new collection of models used for fits to the brain for comparison.
* Expanding to multimodal models, a relatively new space.
* Incorporation of video/speech models, allowing to capture input stimuli over time and removing problems with parsing the stimuli into individual modalities such as ImageBind or VideoMAE.
* Interesting results as seen in Figure 3 showing improvement across several brain regions with multimodal networks. Figure 2 also shows really interesting results with language and visual regions separated.

Weaknesses:
* Clarification on feature removal: I think I found the feature removal description in this paper and prior papers a bit confusing and want to ask for some clarification. I wish more space was spent on that in this paper to provide more intuition. I think some extra descriptions would be useful here. See questions.
* In general, I am quite skeptical of how well the feature removal works. For example, there is no guarantee that the features are completely removed in the residual analysis. I would actually like to see a probing analysis to actually establish that the feature is removed. 
    * Furthermore, the method of projection is rather confusing. The authors use a regression to “project” unimodal video features (referring to figure 1) into the same space as the multimodal feature space. I don’t think this is necessarily wrong but potentially unreliable without any extra metric to establish how well this works. Having some MSE score or pearson correlation (with the averaged embedding) could help understand how well the projection worked. 
    * In my opinion, I wonder why the opposite direction wasn’t taken: instead, project the video features out of the cross-modal/jointly pretrained multimodal representation. You could train a projection matrix to do so using your current vision-language data. To me, this is cleaner and easier to interpret primarily because you aren’t dependent on the quality of your visual representations to capture visual information. 
* The paper compares multimodal and unimodal models to demonstrate improvement in brain alignment. One explanation for this improvement could be an improvement in unimodal processing. For example, one interpretation of the current results is that a multimodal model such as TVLT has better visual processing than ViT-B (as an example).  Is this addressed by feature removal? I’m not sure it is. Some extra text to discuss this would be useful. Some extra discussion on model performance would also be useful. 
* Baselines
    * The paper doesn’t consider the baseline comparison with randomly initialized models. Why? I think this is a very important baseline for characterizing architectural bias. This was also done in prior works.

Limitations:
* I believe these are addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript investigated the process of multi-modal information in human brains through predicting neural responses based on semantic features extracted by existing models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem is interesting.
2. The results show insights into brain region's roles in processing multi-modal information.

Weaknesses:
## Major
1. The method builds ridge regression based on features extracted by pretrained models. However, I am worried that the findings will be affected by choice of pretrained models. It is important to demonstrate the replication of different pretrained models.
2. For some observations in Section 6, the author only presents the observations and does not give insights based on the observations. For example,
    - What does observation i) in lines 311-312 indicate?
    - What does observation ii) in lines 313-314 indicate?
    - Why is AC an exception for observation (1) in lines 316-317?
    - For observation (2) in lines 320 -322, why is TVLT different from IB-concat, given that both of them contains multi-modal information?
3. Why does the author choose ridge regression instead of more complex machine learning models? Is it possible that more intricate interactions of features extracted by pre-trained models are not captured by a ridge regression model, potentially affecting the results? And if you choose a more complex model, the rank of alignment scores of different models could be altered.
4. I do not know if it is too hard or even impossible, but it would be better to check if the results consistent with some existing neuroscientific findings.
5. In section 6.3, why do IB-concat and TVLT act differently given that they are both multi-modal representations.

## Minor
1. There seems to be a trailing 3 in Fig.3's caption.
2. The author moves the results of some brain regions in Figure 3 to the appendix due to the page limit. Since the author refers to those regions from the main text, it would be better to still include those regions in the main text in my opinion.

Limitations:
The authors have adequately addressed most limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses an important question of how accurately multi-modal models can predict brain activity when participants are engaged in multi-modal stimuli. The key challenge is how to integrate or separate the information from different sensory modalities. This work explored two types of models, ie cross-modal and joint pretrained models. Through extensive experiments, this paper found some things that are important to unveil the brain encoding principles, which are important to the AI community.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper writing good, and research problems are well explained.
The encoding pripline is clearly illustrated.  
Experimental designs are insightful.

Weaknesses:
- My major concern is about the train-test settings. There exist `clock’ (temporal) relationship which might lead to information leakage during inference. This paper did not mention how to advoid such an issue. 

- The data collection process should be blocked to aviod inter-data correlation, espeically for joint-modal training. The three settings mentioned in the paper do not really account for the speciality of brain signals.

Limitations:
See above comments

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
RdAfUp4LcD;"REVIEW 
Summary:
This work extrapolates the concept of Linear Mode Connectivity (LMC) modulo model invariances to differentiable tree ensembles (DTE). The authors revealed that, in contrast to neural networks (NNs), permutation invariance is insufficient to provide LMC in DTE and propose two additional tree-specific invariances that enable LMC after taking them into account: subtree flip invariance and splitting order invariance. In addition, they provide a modified DTE architecture that does not posses these additional invariances, however still enjoys LMC with only permutation invariance akin to neural network models. This work proposes two algorithms for building LMC given two independently trained DTEs, based on similar methods from NN LMC literature. The claims are supported by a detailed empirical evaluation.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
Honestly, I enjoyed reading this paper. Although I am not specialized in tree ensembles, I have certain expertise in LMC, and was pleased to find that it is also relevant for DTE models. I think that this contribution is novel and significant.

The paper is very well-structured. It was very easy to follow despite having no significant experience in decision trees, the authors did a good job preparing the reader in Sec. 2. 

Section 3 presents the main contributions of this work, which is done very well using both detailed and intuitive text description and auxiliary images illustrating the main concepts.

Empirical evaluation is excellent, involving multiple datasets, hyperparameter options, and random seeds. The authors tackled many important questions concerning the study of LMC in DTEs and even compared with NN LMC, which I specifically liked.

Weaknesses:
It is hard for me to formulate substantial flaws in this work but a couple of remarks that I put in the next section. 

The main weakness of this work is lack of theoretical support and practical implications. However, I acknowledge that these are the same limitations that are attributed to LMC in neural networks, which is a significantly more broad and well-studied field than LMC in tree ensembles. I hope that future work will address these disadvantages in some way. 

Also, I believe that the text could be slightly polished to eliminate typos and small inaccuracies. For instance, the value $D$ in line 127 is not defined at its first occurrence.

Limitations:
The authors discuss the limitations of their methods in Section 3.2.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an analysis of types of neural networks called soft trees from the linear mode connectivity point of view. The authors enumerate 3 types of invariances inherent to soft trees and study linear mode connectivity between different solutions (by solution they understand a trained ensemble of soft tree models) after weights or activations matching that account for these invariances. They also study linear mode connectivity for a special case of soft trees - decision list-based tree - that has only one type of invariance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written
- Authors claim that it is the first paper to study linear mode connectivity for soft trees

Weaknesses:
## Insufficient contribution
- In my opinion, the main contribution of this paper is a showcase that different architectures need to account for different invariances when LMC is analyzed, e.g. MLP and soft trees have different invariances. I think that this insight alone is not enough for a paper, because it sounds quite obvious even without analysis.

## Questionable results
- It is very important to make sure that interpolation results are not computed between the models which are almost identical (that can happen if there is not enough diversity in training recipes). Could you please provide results with distances (any kind of them, e.g. L2 or cosine similarity) between the solutions in Figure. 5 for ""Naive"", ""Tree Permutation"" and ""Ours"" parameter transformations?
- I would expect decision list trees to be much weaker than soft trees because they have less parameters. Could you please report its performance or show me where I can find it?
- Model merging is mentioned as one of the applications for linear mode connectivity (LMC), however, no results for model merging are provided.
  - line 32: ""In addition, LMC also holds significant practical importance, enabling techniques such as model merging [6, 7] by weight-space parameter averaging.""

## Questionable explanation
- I could not find a related work section.
- What is ""Ours"" in Table 2?
- I did not find in the main text any explanation (even after looking into algorithms in appendix, which I found very confusing) for the operation of weights matching (WM) and activation matching (AM) in case of such invariances as ""Perm"", ""Order"" and ""Flip"" (Notation is from Table 1). Since invariances are the main part of the whole analysis, could you please elaborate more?
- Another important part of parameter transforms includes Linear Assignment Problem (LAP), but I could not find any details for it neither.

Limitations:
- There is no theoretical justification for why and in which scenarios linear mode connectivity exists for soft trees.
- The paper does not propose any practical application for the linear mode connectivity between soft trees. While it can be argued that this paper is an analysis paper, some practical applications can be useful in motivating this kind of analysis.
- I did not find the code of the project while in the survey it is written that code is provided in supplementary material.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper empirically shows that separately trained tree emsemble models can show Linear Mode Connectivity (LMC) when considering tree invariant operations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The exploration of LMC on tree emsemble models is interesting.
- The computational process is clearly stated which makes this paper easy to follow.

------

After reading author rebuttal and discussion with other reviewers, I decide to increase my rating of this paper to borderline reject.

Weaknesses:
This paper does not provide any insights into the question of LMC in neural networks, as it is exploring a totally different model. Although it is always interesting to consider LMC in another senario, I find the contribution of this paper rather insignificant and incremental, since it is basically applying the same idea of [1] to another model. I do not want to deny the author's valueable efforts in exploring symmetry in a new model and using it to achieve LMC, but I just feel that the contribution of this paper may not be sufficient for it to be accepted by this conference.

One possible direction I can suggest for the authors to enhance the current paper is, if any non-trivial theory about LMC can be made on the tree ensemble model setting, then this work will be much more exciting. The underlying reason why neural networks can be made linear connected is not yet clear, and the is hard to study due to the non-linear nature of deep NNs. If the authors can show that the tree ensemble model can be an alternative model to study LMC from a theoretical perspective, then this will make the current work more valuable and intresting.

[1] Git Re-Basin: Merging Models modulo Permutation Symmetries

**Regard writting**
The intro is kind of confusing for readers who are not familiar with the tree ensemble models. It's even unclear whether 1) it is a new model ensembling method for neural networks, or 2) it is a new model, or 3) it is a training method. Although those questions are addressed after reading the detailed definition of tree ensemble in Section 2.2, I think it is better to make it clear in the intro to avoid any confusing.

Limitations:
Authors discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to achieve LMC for soft tree ensembles. Akin to achieve LMC for neural network after accounting for permutation invariance, the authors introduce three different kinds of invariance in soft tree ensembles: tree permutation invariance, subtree flip invariance, splitting order invariance. Additionally, the authors demonstrate that better LMC can be achieved after considering all three kinds of invariance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of extending LMC from neural networks to differentiable tree ensembles is interesting. 
2. Invariances beyond permutation variance are identified for differentiable tree ensembles. The authors demonstrate the effectiveness of accounting for these invariances when doing matching.

Weaknesses:
1. I am not familiar with differentiable tree ensembles, therefore, I would suggest the authors put more efforts on explaining tree ensembles and illustrating the invariances.
2. Another concern is about the motivation. This study is motivated by the question ""Can LMC be achieved for soft tree ensembles?"" but why would we achieve LMC for the tree ensembles? I would expect more elaboration on the motivation.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
RZk2rxJT55;"REVIEW 
Summary:
The paper introduces the so-called ""Leaky ResNet"" ordinary differential equation. Leaky ResNets are a variant of the NeuralODE with an additional vector field that attracts trajectories to the origin, the strength of which is governed by a parameter $\tilde{L}$ that is later shown to correspond to a separation of timescales. 

The authors consider a system of ODEs resulting from the optimization of the empirical risk with a regularization term inversely proportional to $\tilde{L}$. 

They define the ""Cost of Identity"" (COI), a quantity that expresses the coupling of the ODEs and evolves along the trajectories of the ODEs with initial values depending on the training dataset. They then proceed to show that the solutions spend most time in regions where the COI is close to an optimal value.

The final part of the paper proposes three different discretization schemes for the ODE and provides numerical results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The idea of showing that neural networks have a certain property by constructing a model where trajectories spend most of their time in regions with that property is interesting.

* The authors explain their intuition as well as the underlying assumptions of their derivations and highlight the limitations of their analysis.

* The COI seems to be novel and reflect some interesting properties of ODE models for neural-networks.

Weaknesses:
* The main results are not clearly stated in the abstract or introduction. The author's stated goal is to study Leaky ResNets, it would be nice to have a rigorous statement of the results of that study at the beginning of the paper.

* In the abstract, the authors state that the paper explains the emergence of a bottleneck structure in ResNets. It is not clear how this claim can be derived from the results in the paper.

* There is no rigorous justification that the results from the study apply to neural networks with a finite number of layers.

* One of the main ingredients used in the derivations is quoted from another paper in (l.101). It would be nice to have a short proof in the appendix or some explanation to make the paper more self-contained.

* In general the paper lacks rigor, as the authors themselves note in many positions, it is not guaranteed that all the quantities are finite and that the decompositions are justified. It would be welcome if some of the informal discussions could be replaced by rigorous statements and proofs.

* The propositions/theorems in general lack clear statements of which assumptions are used. For example, at the top of page 4 it is stated that the decomposition only holds under a certain assumption (which is rarely satisfied in practice as noted later in the paper). Most of the discussion in the paper relies on this decomposition, but it is not immediately clear whether the formal results require the assumption.

* In general there is a lot of discussion mixing formal definitions and informal arguments, for example reasoning in terms of quantities that don't exist in most cases of interest. This makes the paper somewhat hard to read. I can understand the author's intention of providing some intuition, which might perhaps be better served by a combination of rigorous definitions together with a toy example where all the rigorous quantities simplify.

Limitations:
The authors appropriately discuss the limitations of their results next to their statements. However, the extrapolation to feature learning in ResNets made in the abstract is not justified by the results of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper maps the dynamics of representations across layers of leaky ResNets to a Largrangian and Hamiltonian formulation, giving an intuitive picture of a balance between two terms: a kinetic energy term which favors small layer derivatives and a potential energy
that favors low-dimensional representations. This intuition to explain the emergence of a bottleneck structure, as observed in previous work.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper addresses a timely and important topic: feature learning in DNNs. 

2. The introduction provides a good connection to previous work.

3. The mapping to a Hamiltonian formulation is interesting and provides a valuable intuition. 

4. The propositions and theorems are mostly clearly stated and the proofs seem sound.

Weaknesses:
1. Numerical experiments:
a. Many of the figures are poorly explained and have missing labels etc, e.g. in Figs 1b, 2b what is the color code? 
b. I failed to find a mention of what data the models were trained / evaluated on. 
c. Fig 2c - what is the projection on to? 

2. It is sometimes hard to follow the rationale and motivation for the ""storyline"" of the paper and its different sections could be better connected to each other. 


3. Novelty wrt previous works - in lines 206-208 a difference from similar works is mentioned but this seems is very brief and looks not very significant on the face of it. It would be better to highlight and elaborate on what is new here relative to these previous works. 



Technical comments: 
a. Eqs are not numbered. 
b. I found the notations to be confusing and non standard, e.g. $\alpha_p \in \mathbb{R}^w$ which make it harder to follow the derivations.



Typos:
a. Lines 211-212: ""layers of the layers...""

Limitations:
The authors have adequately addressed the limitations of their results.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies feature learning in Leaky ResNets and shows the emergence of the previously studied Bottleneck structure under certain assumptions. In particular the paper provides a Hamiltonian formulation of the features and their dynamics to show that the ResNet will prefer low dimensional features (low potential energy) when the effective depth of the ResNet is large, which gives the Bottleneck structure. The paper also has a final section on choosing the scales of the residual layers across depths motivated by their theory.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Studies the problem of understanding feature learning in NNs, which is of broader interest in the NeurIPS community.
2. The paper identifies the effect of “effective depth” in Leaky ResNets on the previously observed Bottleneck structure, through a Hamiltonian decomposition into kinetic and potential energy. 
3. In particular, the authors provide a nice intuition that the potential energy is minimised at large effective depths, which corresponds to low rank solutions.

Weaknesses:
1. The paper is unclear in several important moments which compromises readability. For example, is the leakage parameter $\tilde{L}$ suppose to lie in [0,1] (as suggested by line 80) or in [0,\infty] (as is necessary for the “separation of timescales” arguments in section 2.1). Moreover, in line 224 the authors write closed forms for the Hamiltonian but it is not clear how they obtain this object, from the previously stated Hamiltonian on linear 195.
2. Theory seems tied to several simplifying assumptions which reduces its generality in describing/understanding feature learning in standard NNs, e.g. the reliance on ReLU activation, the need for weight decay to minimise parameter norms (though it has been called into question if the role of weight decay is actually to find minimal parameter norms in practice (https://arxiv.org/abs/2310.04415), or also the omission of normalisation layers.
3. On a related note, the theory in the paper seems to have a limited relevance for practice. The one glimmer of this is that the paper suggests changing the weighting on the residual branches in order to evenly balance the difference layers in terms of how much the representations are changing, but this seems underdeveloped at present. It would be worth investigating if this can improve training in practice. Moreover, the paper (and the works on the Bottleneck structure in general) seem to argue that the representations should be changing a lot at late layers because the representation shifts back from being low rank. But this is counter to existing practical works that suggest one can prune late residual layers for improved efficiency https://arxiv.org/abs/2207.07061. This represents a gap between this theory (of Bottleneck structures) and practice to me.
4. The paper studies properties of optimal solutions (e.g. geodesics with minimal parameter norm) in terms of Hamiltonian energies etc (Theorem 4) but does not seem to discuss whether training dynamics will lead to such solutions in practice.

Limitations:
There is no limitations section.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the feature learning dynamics in Leaky ResNets using Hamiltonian mechanics. By introducing the concept of 'representation geodesics',  the authors analyze continuous paths in representation space that minimize the parameter norm of the network. The study provides a Lagrangian and Hamiltonian reformulation that highlights the importance of balancing kinetic and potential energy during feature learning. The potential energy, which favors low-dimensional representations, becomes dominant as the network depth increases, leading to a bottleneck structure. This structure involves rapid transitions from high-dimensional inputs to low-dimensional representations, slow evolution within the low-dimensional space, and eventual rapid transitions back to high-dimensional outputs. The paper also proposes training with adaptive layer step-size to better handle the separation of timescales observed in the representation dynamics. These insights offer a novel perspective on the mechanisms underlying feature learning in deep neural networks and suggest potential improvements in network training methodologies.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This paper offers a novel approach for understanding feature learning by applying Hamiltonian mechanics to Leaky ResNets, bridging a gap between theoretical physics and machine learning.
- This paper conducts experiments to validate the findings. Based on experiments, some interesting observations are obtained, which may give some new insights for future works.
- The insights gained from this study have the potential to influence future research in neural network optimization and feature learning, advancing the state of the art in deep learning theory.

Weaknesses:
1. There are multiple typos in the article, which affect readability. Below are several obvious typos, and it is recommended that the authors carefully polish the language of the article.
	-  The third word in line 24, ""phenomenon""$\rightarrow$ ""phenomena"".
	-   In line 27, ""determines"" $\rightarrow$ ""determine"".
	-   In line 40, ""lead"" $\rightarrow$ ""leads"".
	-   In line 68, the preposition ""in"" should be added after ""interested"".
	-  The formula at the end of line 78  should be$$\alpha_{q}^{'}=\alpha_{q/c}.$$
	-  The formula between lines 78 and 79 is also incorrect. The last term should be $$\frac{1}{c}\partial_{p}\alpha_{q/c}(x).$$
	-  The expression of $K_p$  in line 111 is incorrect, because it should depend on $p$.
	-  The formula between lines 112 and 113 is incorrect. The coefficient of the middle term on the right side of the equation should be 1 instead of $\tilde{L}$.
	- In line 145, "" $||\tilde{L}A_{p}+\partial_{p}A_{p}||_{K_{p}^{+}}^{2}$ ""  $\rightarrow$  ""$||\tilde{L}A_{p}+\partial_{p}A_{p}||_{K_{p}}^{2}$"".
	-  In line 159, ""bound"" $\rightarrow$ ""bounded"".
	-  The expression for $C(A)$ in line 190 is incorrect, which should be $$C(A)=\frac{1}{N}||f^{*}(X)-A||_{F}^{2} .$$
	- In line 282, ""$\rho_{l}L <1$ "" $\rightarrow$ ""$\rho_{l}\tilde{L}<1$"".
	- In line 415, ""cones""  $\rightarrow$  ""cone"".
	- In Theorem 7 and its proof, it seems that all instances of $\sqrt{\gamma c}$ should be changed to $\gamma \sqrt{c}$.
	- In proposition 9, $\tilde{Z}_{q}$ should be $\tilde{A}_{q}$ in the formula above line 485.
2. The formula between lines 101 and 102 is a crucial one, so it is recommended that the authors provide the derivation process for this formula.
3. In line 59 of the paper, the definition of $\sigma$ is given. First, the ""+"" in this formula should be replaced with a comma. Secondly, my question is about the last component ""1"" in the definition of $\sigma$. In the proofs of some propositions, is it necessary that $\sigma$ does not include this last component ""1""?
	- In line 153, why $||A_{p}||_{K_{p}}^{2}=||A_{p}A_{p}^{+}||_{F}^{2}$ for non-negative $A_{p}$ ?
	- In line 156, why $||\sigma(A)||_{F}\le ||A||_{F}$ ?
	- In line 450, why $||A_{p_{0}, \cdot i}||^{2}-||\sigma(A_{p_{0}, \cdot i})||^{2}=c_{0}>0$ for all $\tilde{L}$ ?
4. In the proof of Proposition 8, the authors did not explain why the limit exists. Secondly, the formula above line 454 is missing $O(\epsilon^4)$. Additionally, the formula above line 464 is incorrect.

Limitations:
The theory they proposed aims to provide guidance for better training of networks, and their experimental and implementation details have been documented.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
RTaSjoxmw9;"REVIEW 
Summary:
This paper addresses the challenge of achieving outlier robustness in phase retrieval, specifically focusing on the recovery of real-valued signals from intensity measurements that have been corrupted by adversarial outliers. The contribution of this work is the development of a nearly-linear time algorithm that is nearly sample-optimal and can accurately recover the true vector despite the presence of outliers. This is done through a two-step process that involves robust spectral initialization and robust gradient descent, utilizing recent results in high-dimensional robust statistics.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+A two-stage algorithm achieves nearly-linear time complexity consisting of an initial spectral initialization phase and a gradient descent refinement phase.

+Theoretical analysis showing the algorithm can recover the ground truth signal despite the presence of outliers, while maintaining near-optimal sample efficiency.

Weaknesses:
- The paper's theoretical results assume the corruption level $\epsilon$ as a constant in Theorem 3.1, despite it being initially introduced as a variable in Definition 1.2 to denote the extent of sample corruption. This constant treatment affects the robustness of the results by neglecting the influence of $\epsilon$ on the sample complexity. A thorough analysis that explicitly considers the variability of $\epsilon$ would significantly strengthen the theoretical foundation.
- Lack of numerical validations for the theoretical claims
- The idea and design of the two-stage robust phase retrieval algorithm is not novel, by adapting existing reweighting phase retrieval algorithms (throught different design of the reweights) to handle outliers through robust statistics. The contamination model is also from existing works, which has been recent studied in a number of different contexts; e.g., robust linear/nonlinear estimation by e.g., Diakonikolas and coauthors.
- Lack of numerical validation and comparison of the proposed algorithm with respect to existing (robust) Gaussian phase retrieval algorithms. It is difficult to judge if the proposed algorithm is of practical interest (or only of theoretical interest).
- Quite a lot statements in the paper are rather confusing, e.g., ""we propose the problem of outlier robust phase retrieval""; robust phase retrieval has been long studied in the literature; as far as I understand, the paper studies a new robust phase retrieval problem by considering also adversarial a_i's on top of existing formualtions. ii) ""It is well-known that natural nonconvex formulations of phase retrieval do not have spurious local optima."" which is not precise enough and is true under very stringent assumptions. iii) ""This is first achieved via approaches based on semidefinite programming (SDP) relaxations (see, e.g., Cand`es et al. (2015c))."" I guess the first Gaussian phase retrieval algorithm was the AltMin algorithm (Netrapalli, et al 2013. Phase retrieval using alternating minimization. Advances in Neural Information Processing Systems, 26.) which was interestingly not cited in the submission. iv) ""Similar landscape results are known for other natural nonconvex formulations of phase retrieval as well (e.g., min f(z) = P i( √ yi − |⟨ai, z⟩|)2 (Soltanolkotabi, 2019))."" The first work to study the Gaussian phase retrieval based on the magnitude-based least-squares nonconvex formulation and achieve provable guarantees is (Wang et al (2017). Solving systems of random quadratic equations via truncated amplitude flow. IEEE Transactions on Information Theory. 64(2):773-94.) Please be careful and fair in stating related results.

Limitations:
NA

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper focuses on the problem of outlier robust phase retrieval, whose goal is to recover a vector $x \in \mathbb{R}^d$ from $n$ intensity measurements $y_i = (a_i^\top x)^2$ when a small fraction of the samples are adversarially corrupted. The authors propose and study this problem, providing a nearly sample-optimal and nearly linear-time algorithm to recover the ground-truth vector $x$ in the presence of outliers.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper provides an analysis of the practically interesting problem of outlier robust phase retrieval. The algorithm and framework might have implications for various applications that are relevant to phase retrieval.

Weaknesses:
1. The analysis appears to be more incremental in nature compared to those in previous theoretical works related to phase retrieval. More precisely, the key novelty of the spectral initialization step resides in the assignment of a nonnegative weight to each sample. Regarding the gradient descent step, the problem seems to be simplified to the analysis of robust mean estimation algorithms.

2. Important references are missing. For instance, the authors ought to cite the works related to robust compressed sensing (and it would be better to discuss in more detail the disparities between the analysis for the robust gradient descent step in this work and the analysis in these relevant works), such as

- Liu, Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis. ""High dimensional robust sparse regression."" In International Conference on Artificial Intelligence and Statistics, pp. 411-421. PMLR, 2020.
- Liu, Liu, Tianyang Li, and Constantine Caramanis. ""High Dimensional Robust $ M $-Estimation: Arbitrary Corruption and Heavy Tails."" arXiv preprint arXiv:1901.08237 (2019).

3. The authors solely consider the scenario of noiseless intensity measurements and fail to take into account the noisy case.

4. The paper does not include experimental results, which could limit the confidence in the practical effectiveness of the proposed approach.

Limitations:
No experimental validations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies a classical problem called phase retrieval. The goal is to obtain unknown $d$-dimensional vector $x$ from $n$ datapoints $(a_i, \langle a_i, x \rangle^2)$. This work assumes that $a_i$ are iid Gaussian vectors, but also that a small $\varepsilon$ fraction of the data is corrupted. The authors suggest a two-stage process to identify the vector up to a small error. First, a spectral based algorithm is used to have a small constant error. Further, a robust gradient descent is used to approximate the initial guess up to a small error.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Paper is well-written and provides a good overview of the problem and of the techniques.
2. Phase retrieval is a traditional non-convex problem, which was largely studied before, and understanding how robust algorithms perform on it is important.
3. Paper uses prior technique in a simple way, and it is possible that this two-stage approach can be applied to other problems.

Weaknesses:
1. Results are limited to the Gaussian setting.
2. The method for RME that is used assumes that variance $\sigma$ is known? But in the way it is used here, $\sigma$ depends on the distance between current solution and the true vector. Authors do not comment on this issue.
3. $\tilde O, \tilde \Omega$ notation is not defined.
4.  Intuition in line 98 in my interpretation contradicts more exact version in line 214 (In the end, if I understand correctly, the crucial reason why the spectral initialization algorithm works is that the adversary cannot change the top eigendirection, but can only add new directions).

Limitations:
There are no ethical limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study the phase retrieval problem for retrieving a real signal under the influence of arbitrary corruption. The corruption is allowed to be present in labels or features. They propose a two-step solution. First, they ensure that the initialization is robust to the corruption and second, they show that the gradient descent updates can be made resilient. Authors claim that their method can recover the true signal (with possibly sign mismatch) to an arbitrary precision.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The ideas presented in the paper are certainly interesting. If resilience to corruption can be achieved in individual steps, then it makes sense that it might lead to a good overall recovery.

Weaknesses:
1. The authors claim that corruption level up to some universal constant $\epsilon'$ can be handled through their method. Although, to the best of my understanding, this quantity is not characterized in the main paper. What is the maximum value for $\epsilon'$? 
2. There is no discussion on the dependency of $\epsilon'$ on $n$ or $d$.   
3. The claim of signal recovery to an arbitrary precision puzzles me. It is known that for $\epsilon$-corrupted vectors, the robust mean estimation can only be done up to $\Omega(\sqrt{\epsilon})$ error. Despite that, the authors claim signal recovery (with possibly a flipped sign) to an arbitrary precision. Can the authors comment on how this is achieved?   
4. The claim in line 213 says that $y_i$ is always greater than $0$. Why is that true when the adversary can corrupt $y_i$ arbitrarily? As far as I can tell, the algorithm does not discard negative $y_i$s.

Limitations:
Please see above.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
RNeb41ybNL;"REVIEW 
Summary:
The authors study Langevin dynamics (as well as its annealed counterpart) for gaussian mixtures and sub-gaussian mixtures. In Sec. 4, they prove that Langevin remains stuck in the ""dominant mode"" for an at least exponential time, a claim that is often made in the ML literature but which is never formally proved. In Sec. 5, they provide a sequential method to get rid of this dependence.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
It is healthy to finally have a paper that explicitly prove the claims made in the ML literature and that were known in practice for a long time.  Furthermore, it shows that, unlike what was believed, annealed Langevin also fails.

Weaknesses:
I do not understand why it is sensible to say that initially, $p_0$ should follow $P_0$, one of the component of the mixture, isn't it a rather strong assumption?

Limitations:
It seems like the assumption $p_0 \sim P_0$ is not enough justified. Also, I would have like an insight of the proof of the Theorems in Sec. 4.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
A new algorithm is proposed, called Chained Langevin Dynamics, to improve on the mode-seeking properties of Langevin Dynamics, after annleade Langevin Dynamics had been proposed but did not give significant improvements.
Results about the mode-seeking properties of the three algorithms are obtained.
The results of numerical experiments on synthetic and real image datasets are also shown.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Very inspiring idea on how to improve mode-search for multimodal distributions.
Very clear presentation of premises and of the old and new algorithms.
The new algorithm looks very powerful.

Weaknesses:
No evident connection has been established between experiments and mathematical results.
The description/comment of experiments could have been more accurate (see Questions).

Limitations:
There is a section about limitations in the text.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors consider the Langevin process for sampling from a target distribution $\pi$. This process is known to be slow-converging for multimodal targets: in practice, it has been observed that the process gets ""stuck"" in some modes of the target, and do not ""reach"" other modes of the target. The authors provide theoretical results for this behavior. In Theorem 1, they prove that by evolving a particle with the Langevin process during exponential time (in the dimension), the particle will still be far away (in probability) from some modes. They also prove, in Theorem 2, that this negative result holds even when using the popular heuristic of ""annealing"" the Langevin process using intermediate distributions, obtained by adding different levels of Gaussian noise to the target samples. 

Instead, the authors propose running an alternative sampling process which they call ""Chained Langevin dynamics"". This consists in running ""annealed"" Langevin processes for each component of the target distribution, that is, for each $\pi(x_i | x_{-i})$. The authors estimate the score of each of these conditional targets using a score-matching loss, and empirically demonstrate the ability of their process to reach the different target modes in a limited time. Theoretically, they prove their process approximates the target (in TV divergence) in linear time (in the dimension).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors provide an interesting perspective on popular sampling processes, Langevin and its annealed counterpart. The paper is clearly written and the results are an interesting contribution to the sampling community.

Weaknesses:
See questions.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies Langevin-based algorithms for sampling from multimodal distributions, motivated by generative modeling. The main content of the paper are lower bounds on the convergence of both Langevin and annealed Langevin for mixtures of Gaussian and sub-Gaussian distributions, as well as a proposed modification of the annealed Langevin dynamics to operate on coordinate patches one-at-a-time.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Sampling from multimodal distributions is an importnat problem both theoretically and practically.

- The Chained Langevin Dynamics algorithm that is proposed appears to be novel.

- The empirical results are promising, albeit in a rather contrived setting.

Weaknesses:
- The lower bounds hold only for the distance between the sample and the mean, rather than any standard notion of distance or divergence between probability measures. Moreover, I do not expect that these bounds imply such a quantity is large.

- Related to the above point, it is difficult to appreciate the significance of the lower bound since the lower bound does not depend on the separation between the means. In particular, it seems the lower bounds only show that the iterate remains roughly on the order of the larger variance which is, for example, not surprising in the case where the variances are all of the same order.

- The hidden constants in the $\Omega$ notation are important but difficult to find (as they are suppressed in the main text and some of the appendix). In particular, there should be dependence on the mixture weights but this can't be seen from their result.

- It is unclear if the upper bound in Theorem 5 can be instantiated for their algorithm (see question below).

Limitations:
The limitations of the work have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
RGnjY6l2HT;"REVIEW 
Summary:
The paper proposes UniEdit, a framework that allows the editing of videos. More specifically, UniEdit allows manipulation via text prompts to change the visual style or the motion pattern that is visible in the video. Moreover, it also targeted steering, e.g. via segmentation masks. They achieve this by introducing an additional reconstruction branch and a motion-reference branch into a u-net based diffusion network and share the values of the attention layers which are party designed specifically for this work. The method allows editing videos without retraining and creates very good results.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The core idea is straightforward and well-presented. There are just a few hyperparameters to select. They have a large amount of visual content showing the quality of their contribution. Moreover, they performed extensive human experiments to rate the videos.

Weaknesses:
The implementation may not be entirely reconstructable. Hopefully, this issue will be fixed when they publish the code as promised.

Even if the method description is understandable, the math is sometimes not entirely correct. For example in line 212/212, M is a matrix but the notation says that it is a scalar from a set $\{ -\inf, 1 \} $ or $\{ 0, 1 \} $. The authors should be encouraged to revise the math present in the paper.

**Minor weaknesses:**
Sometimes the English writing is a bit weak. For example: 
 - 100-110: Some parts are not forming complete sentences, e.g. ""Other improvements like efficiency [1], training strategy [19], or additional control signals [16], etc.""
 - 187: I think it should be ""an additional network"" or ""additional networks""

Limitations:
Unfortunately, there is no benchmark to evaluate the method. This is not the author's fault and they tried to do their best to create baselines. However, this makes it harder to rate the results.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper suggests UniEdit, a tuning-free method for editing the motion of a given video. The authors use a pre-trained text-to-video diffusion model and utilize its motion prior, to performing motion editing on a video while keeping the appearance of the original video. During the denoising process, they apply structural/content features injection from the reconstruction branch of the original video to maintain the input video's structure or content. The motion is edited according to a text description used to denoise another reference branch which is then used for injecting features into the editing videos. The results improve over the existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Successfully applying feature injection for video diffusion models. 
* Impressive results.

Weaknesses:
1. Novelty.  Feature injection for image editing is a known technique [56].  Applying injection to video models is important and challenging, but not novel enough in my view.
Showing that the injection of motion features from the reference motion branch into the edited video, constrains the output motion, is important, but not surprising given the observation of [56]. 

2. Given that the main insight of the paper is that “the temporal self-attention layers of the generator encode the inter-frame dependency”, there is not enough analysis of this besides the visual results and Figure 6, which shows the relation between the optical flow magnitude and the temporal attention on one example qualitatively. Showing a quantitative analysis, and analyzing the features during the denoising process for different layers, could support this claim better and show the importance of this insight.

Limitations:
Yes, the authors discuss both, limitations, and broader impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on developing a tuning-free framework capable of editing both the motion and appearance of videos. They introduce UniEdit, an approach designed for text-guided motion editing that maintains the original content of the source video. By utilizing two branches—an auxiliary reconstruction branch and an auxiliary motion-reference branch—they achieve both content preservation and effective motion editing.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper pointing out the problem of existing methods of not being able to keep the non-edited area and propose using spatial self-attention module, spatial cross-attention module and temporal self-attention model to solve the problem. From the experiment results, it shows that the edited results exhibit the editing task correctly while maintaining the unedited area.
2. The paper is overall clear and well-written
3. This paper provides versatile applications like motion editing, stylization, rigid/non-rigid object editing, and background editing.

Weaknesses:
1. The number of the participants in the user study might not be representative enough.

Limitations:
This method is inherently influenced by the T2V model used.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
QsxldAFMFx;"REVIEW 
Summary:
This paper adapts Maia, a group-level model and variant of the AlphaZero model proposed by McIlroy-Young et al. (2020), to function at an individual level. The authors focused on scaling the fit to individual behavior using techniques like LoRA for fine-tuning LLMs. Moreover, the learned embeddings were shown to encode decision-making styles that correspond to individual behaviors. Linear combinations of these embeddings can result in desirable decision-making styles, creating synthetic styles.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This is an interesting paper that treats fitting individual data as a task in a multi-task learning framework. Recent fine-tuning techniques for LLMs have shown promise in adapting a base model, which captures group-level behavior, to individual-level data on a large scale, accommodating hundreds of thousands of individual datasets.

Weaknesses:
The paper lacks a proper benchmark for comparing the accuracy gains from their proposed method. Hypothetically, if the datasets they considered (Chess and Rocket League) exhibit no individual variability in behaviors, meaning all individuals behave identically to the group-level agent, we should expect no accuracy gain whatsoever from adapting a group-level base model to individual-level data. Conversely, if significant variability stems from individual differences, we should expect a substantial gain. Currently, the 0.4% and 4.8% accuracy gains reported in Table 1 are not compared to any benchmark that considers the maximal possible variability that could be extracted from individual-level data. This leaves readers uncertain about the actual gains achieved by the method. It would be beneficial to establish a theoretically optimal benchmark for maximal possible accuracy gains across all datasets.

Limitations:
This paper did not explicitly include a section for Limitation and Future Research.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores modelling user behaviour for chess and rocket league using a PEFT-based method, wherein users are modelled using a composition of MHR adapters. The authors evaluate their approach both for predicting which player played a particular game and for predicting the next move of a given player, and find that their MHR-based approach outperforms prior work in both cases. Further analysis shows that the MHR-based approach further allows steering and combining user vectors to produce new styles, and that the vectors are able to capture a wide diversity of strategies.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The approach is simple but intuitive, applying MHR-style adapters to stylometry and exploring the benefits of this approach. The approach does seem to result in improved performance over prior work, and the analysis of the adapters showing that they can be combined to create new styles, or ‘steer’ a user adapter towards a particular style, is interesting, and highlights that using MHR for learning style vectors allows for interesting analyses/applications. The experimental setup seems reasonable, and the authors show their approach works across more than just chess (although chess is the main focus of the work).

Weaknesses:
- The MHR approach does seem to underperform full-finetuning when there is a large number of games available, as shown in Figure 2, and as expected for a PEFT-based method.
- While older stylometry work [1] was able to produce user vectors by just performing inference (albeit not being generative), this approach requires training, and so might be more computationally expensive as a result (PEFT training is cheap, but still requires computing the full backward pass to compute gradients for the adapters). Grounding some of the discussion in section 5.1 with compute estimates (e.g. estimated FLOPs) would be useful.
- The novelty is somewhat limited, as the primary method (MHR adapters, linearly interpolating between adapters) has been explored in prior work. However, I think that the application of this idea to a new domain (user identification/move generation) is still novel and interesting.

[1] R. McIlroy-Young, Y. Wang, S. Sen, J. Kleinberg, and A. Anderson. Detecting individual decision making style: Exploring behavioral stylometry in chess. Advances in Neural Information Process439 ing Systems, 34:24482–24497, 2021.

Limitations:
The authors address the limitations of their approach reasonably.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Authors found a way to obtain player stylometry using BC on a massive amounts of data. Their idea uses multiple LoRAs and routing matrix to obtain a style vector for each player.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Very interesting technical solution and innovative way to use LoRA and routing matrix to obtain player style vectrors using behavioural cloning. 
- I think there is a large potential in using this technique in analyzing large game play databases. Possibly to glean new insights. 
- Potential also exists to combine these ideas to IRL / adversarial imitation learning schemes. It is not clear that which expert demos we should use to show to the IRL algorithm. Should it be just one pleayer, or maybe k-most similar players to some seed player?

Weaknesses:
- Applications as of now are not very compelling. One option would be to take lichess.org data and ask some interesting research question about the player data that has not seen a sufficient answer. Then use this technique to answer it. 
- Another option would be to integrate this technique to some IRL scheme. 
- Authors cite speaker recognition in the related work, but did not study it more closely. In biometric recognition we have essentially similar task as in the present paper. Idea is to turn observed data (audio in speaker recognition) into fixed length vector and then by comparing these vectors obtain downstream recognition. Where the present paper goes wrong is that evaluation metric is identification accuracy, whereas in biometrics it is known that task is verification task and metric is ROC or DET curve and equal error rate (EER) summary statistic. Authors should use it, in addition to identification. 
- Speaker verification also gives nice possibility for extensions to the current work. One idea back in the day was to do Baysian adaptation from the general speaker model to a specific speaker model. These ideas were also presented in the following paper https://arxiv.org/abs/2012.01244

Limitations:
-

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the authors propose to solve the problem of behavior stylometry, which is to identify the style of a player’s policy in the game, by regarding it as a multi-task learning problem. Each player’s style is a distinct task. Previous methods are either not scalable or not generative, in that they cannot predict the moves of a player given a query set of games played by that player. The authors aim to design a model capable of generating the moves of different players in a scalable manner. To do that, the authors trained a set of Low Rank Adapters (LoRAs) over a base model. The base model is trained with behavior cloning on the whole dataset, and the LoRAs are trained on a specific set of training player dataset separately. Additionally, a routing matrix is trained with LoRAs to specify the distribution over the LoRAs for each player. In this case, the authors claim that the routing matrix is a compact representation of the players’ skills, and encourage the adapters to learn different latent skills while preserved the shared knowledge within the base model. The authors also claim that the routing matrix supports few-shot learning, and induces a series of applicable benefits on stylometry, such as interpreting and manipulating the style of a player.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well written and clearly conveyed.
2. The proposed method achieves convincing results. 
3. Their dataset is partitioned and rotated properly so that they can analyze the inter-player and intra-player consistency of the method, which provides strong convincing for the paper.

Weaknesses:
1. The main concern I have for this paper is the novelty of the proposed method. The base model for Chess is from a previous work called Maia, and trained with existing techniques such as LoRA and Polytropon, either of which is novel to me.
2. The significance of the problem setting, behavior stylometry, is not clear. The authors only have a very short introduction of what it is and its usage in the first section. However, the benefit of modeling accurate behavior stylometry is still vague. For example,  we have very high performance models in chess, even outperforming human players, then why do we need to model the style of human players via behavior stylometry? To me, accurately predict the moves of a player is not the ultimate goal of a game, the ultimate goal is to win the game. How can behavior stylometry help in winning the game? This should be better explained with more introduction and related experiments.

Limitations:
As stated in the Weakness part, the analyze on the significance of behavior stylometry is limited. The authors work on a sub-problem of the game, and didn’t connect it back to the game itself.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
QeebTNgRjn;"REVIEW 
Summary:
The authors introduce the conditional Lagrangian Wasserstein flow method for time series imputation.
The time series imputation task is treated as a conditional data generation problem. The authors use flow matching to learn an ODE sampler for generating the missing time series data. They further propose to enhance the imputation performance via Rao-Blackwellization. The method is tested on the real-world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1. The approach of using Lagrangian Wasserstein flow for time series imputation is interesting and novel.

S2. Compared to other diffusion model-based time series imputation methods, this method seems more efficient.

S3. The experimental results given in the paper are satisfactory.

Weaknesses:
W1. The imputation process is implemented using an ODE sampler. Why not use an SDE sampler?

W2. Why use the Euler method instead of other higher-order solvers, such as the Runge-Kutta methods, for solving the ODE?

W3. Can the proposed method also be applied to time series forecasting tasks?

W4. The dynamic described in Eq (15) is deterministic, which is inconsistent with Eq (1). Why the diffusion term is missing in Eq (15)?

Limitations:
CRPS is not used as the evaluation metric in the experiments.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors introduce a new time series imputation model based on the conditional Lagrangian Wasserstein flow. Different from previous diffusion-based models, the proposed model leverages the optimal transport theory and Lagrangian dynamics to improve the data generation performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The overall presentation of paper is clear. 
2. The idea of using the conditional Lagrangian Wasserstein flow for time series imputation is novel.
3. The proposed method is assessed on the real-world datasets and shows competitive performance compared to the state-of-the-art methods.

Weaknesses:
1. In Sec 3.2, the technique used for projecting the interpolants into the Wasserstein space is unclear. The authors should elaborate more on this.
2. Compared to CSDI and other diffusion-based imputation methods, the proposed method’s data generation process seems deterministic. Hence, it cannot be used to quantify the uncertainty of the prediction.
3. The proposed sampler is implemented through an ODE, which is inconsistent with the Schrodinger Bridge problem in sec 2.3.
4. There are some inconsistencies in the notations, please double-check.

Limitations:
The ablation study on the effectiveness of Rao-Blackwellization should be conducted on more than one dataset (PM 2.5).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Inspired by optimal transport and Lagrangian dynamics, this work proposes to use the conditional Lagrangian Wasserstein flow to impute time series data. The method requires less model evaluation steps to generate high quality samples compared to existing diffusion models. Moreover, a task-specific energy function is used to further improve the model’s performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The contributions of this paper are significant.
- This work is built on a solid theoretical foundation.
- The experimental results and relevant ablation study findings are provided.

Weaknesses:
- How should the values of the hyperparameters (e.g., the variance of the potential function) be chosen? How will these choice affect the model’s performance?
- The reason for choosing the VAE model to construct the potential function is unclear. Are there other functions can also be used as the task-specific potential function?
- The authors proposed to use a VAE model to enhance the data sampling procedure. However, if the VAE model performs poorly, can it still help with the data generation process?

Limitations:
The deterministic sampling process may introduce bias, potentially failing to accurately reflect the target marginal distribution.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work trains a conditional flow model from noise to time series data. A VAE is used to estimate the data density then perform interleaved flow and density gradient ascent steps to generate new time series. This is referred to as a Rao-Blackwellization procedure. It is shown empirically that the model performs well on time series imputation tasks across two datasets, and that the VAE-based guidance helps in one case.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
* Originality: Time series imputation is an important problem and I have not seen any flow matching works in towards this application. Presents a new improvement to the standard flow matching framework with a “Rao-Blackwellization”.

Weaknesses:
- Quality: Experiments are limited in scope and the it is not clear whether the “Rao-Blackwellization” (the main methodological novelty) step reliably improves performance.
    - In order to claim “Less sampling steps” it would be great to understand how performance changes with the number of steps for both diffusion and flow-based models.
    - More than 2 time series datasets would allow an understanding of when this “Rao-Blackwellized” sampler is valid.
    - No error bars (even though this is claimed in the checklist)
    - The empirical support is lacking for the effectiveness of the “Rao-Blackwellization” step. Currently it is shown to slightly benefit (although without error bars this is difficult to tell how much) on a single dataset. In this case, an improvement in RMSE from 18.2 to 18.1 on a single dataset seems insignificant without additional detail. While it is claimed that “the PhysioNet dataset does not have enough non-zero datapoints to train a valid VAE model”, a Rao-Blackwellized sampler should always work regardless of the performance of the estimate? It would be good to see if it helps at all on this dataset and others.
- Clarity: It is not clear to me that this even is a “Rao-Blackwellized” sampler. As far as I can tell this “Rao-Blackwellized” sampler is fundamentally biased (while the original sampler is not), and it therefore cannot possibly be a Rao-Blackwellized sampler. It would be great if the authors could prove that Algorithm 2 is really a Rao-Blackwellization (at least under some conditions).
- Significance: Without further experimental benchmarking especially with regards to the novel sampler, the applicability of this method is very limited.

Limitations:
I'm a bit confused about the limitations. It is claimed that 

> One limitation of CLWF is that the samples obtained are not diverse enough as we use ODE for
> inference, which results in slightly higher test (continuous ranked probability score) CRPS compared
> to previous works, e.g., CSDI. Therefore, for future work, we will seek suitable approaches to
> accurately model the diffusion term in the SDE.

Other ODE models have had no problem with diversity. I would suggest that this is actually due to performing gradient ascent on the density of the VAE.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents Conditional Lagrangian Wasserstein Flow (CLWF), a new method for time series imputation. Using (entropic) optimal transport theory and Lagrangian mechanics, CLWF generates high-quality samples. Enhanced with a Rao-Blackwellized sampler, CLWF incorporates prior information through a variational autoencoder. Experiments on real-world datasets show that CLWF performs competitively.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written, clear, and easy-to-follow.
2. The use of a Rao-Blackwellized sampler and a variational autoencoder to integrate prior information enhances the model's performance, providing a more robust imputation process.

Weaknesses:
1. The conditional generation process is trained through the simulation-free training of the Schrödinger Bridge, which limits the novelty of the CLWF.
2. The paper mentions that the samples obtained using the ODE-based inference method may lack diversity, potentially leading to higher continuous ranked probability scores (CRPS) compared to some previous works.
3. While the method is tested on two real-world datasets, broader evaluation across more diverse and challenging datasets could strengthen the validation of the approach.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
PnSTlFUfcd;"REVIEW 
Summary:
The paper proposes an online shielding approach for safe RL that does not assume prior knowledge of environment dynamics and utilizes finite-horizon model checking with learned approximations of the environment dynamics. It specifically focuses on RL with regular safety properties provided as a PCTL formula. The authors present a framework that dynamically identifies unsafe actions and deploys a safe backup policy when necessary. The main technical contributions of the paper are:

- Definition of a constrained RL problem based on regular safety properties.
- Presentation of model checking algorithms to verify finite-horizon satisfaction probability.
- Development of sample complexity results for statistical model checking procedures.

The novelty of the paper lies in its approach to reinforcement learning with regular safety properties without requiring prior knowledge of environment dynamics. Unlike traditional shielding approaches that need full environment models or simulators, this framework uses learned approximations and finite-horizon model checking. The authors represent the synthesis problem as finding an optimal policy under a constraint that the resultant product Markov chain (from the policy) and the DFA (from the safety property) has probability $\leq p_1$ of violating the safety property in finite horizon $H$. The authors make use of the CMDP formulation to separate the reward and safety considerations by treating the safety property as a cumulative constraint in the CMDP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, this is a very strong submission and very clearly written. The authors present their problem statement precisely and also compare against many other related settings. This is challenging to do in safe RL since it is such a wide field with many parallel approaches, but I feel the authors did a commendable job here, especially in demonstrating how related and alternative formulations can be represented in their setting. 

This is a very interesting combination of shielding from temporal logic specifications with a CMDP formulation. Especially, as the framework allows for regular safety properties instead of just invariant properties, it allows for quite general specifications. 

The authors also do a good job in showing the generality of the work as it pertains to levels of model knowledge.

Weaknesses:
The main weakness I can see in this paper is perhaps a lack of experiments in settings with more complex models and safety specifications. Especially settings in which safety and optimality of the reward are in conflict, it would be interesting to see how the agent is able to achieve a trade off. 

Another consideration is perhaps motivation of using a CMDP as a framework for safe RL. I don't see this as well motivated in the paper. There are several approaches to safe RL that use MDPs and are able to use probabilistic model checking type techniques to give guarantees for cost thresholds.

Limitations:
There is some discussion of limitations in the section before the conclusion. I agree with the authors on the downsides of separating reward and safety.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a new safe RL approach, building on safety shields.
The idea is to leverage model-checking techniques during the RL training to block actions that are identified as unsafe in the shield and use a learned backup policy if this is the case. In contrast to previous approaches, the ""meta-algorithm"" presented does not require an a-priori known model of the safety aspects of the environment. The approach comes with theoretical guarantees on the satisfaction of finite-horizon PCTL specifications and the evaluation assesses the potential of the proposed algorithm.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I'd first like to pinpoint that the paper is quite clear and very well written.

The proposed approach has several key advantages. First, it doesn't necessarily require providing a shield beforehand, in contrast to previous work. Second, the properties the agent needs to enforce during training are specified via PCTL, a well-established specification formalism that is not prone to exponential blow-up in the size of the automaton for translating the formula (as is the case for LTL). Third, the approach comes with guarantees on the shield: (i) when the hyper-parameters of the optimization procedure are well-chose, one can bound the probability of failure of the system from the initial state, (ii) the optimal policy found under the PCTL constraint is ensured to be a feasible policy for standard constrained MDP objectives. Finally, the authors discuss and provide guarantees under different assumptions, namely, the access to a model of the safety-relevant aspect of the environment (as in previous work), under a black-box model, and when one has access to an approximate model such that the total variation between the true and approximate transition probabilities is bounded. Additional statistical guarantees are provided for these last two assumptions. 

Experiments successfully highlight the potential of the approach.

Weaknesses:
**Beyond tabular settings.**
The main concern I have with this paper is the fact that the guarantees seem to solely hold in the tabular setting, i.e., when the state-action space is finite and tractable. Notably, in the second round of experiments, the authors use Dreamer-v3 to learn an approximate model of the environment. Although the resulting method seems to outperform constrained RL methods, the theoretical guarantees do not hold in practice. 

**On the assumptions.**
Assumption 5.2 is confusing. Indeed, when reading it for the first time, I thought one needs to have access to a *generative model*, i.e., a black box model of $\mathcal{P}$ that can be requested at any time, under any state and action, akin to the setting of [1, 2]. Specifically, this would mean that one doesn't necessarily need to sequentially execute the environment to obtain samples (as this is the case in RL), but, at any time, for any given state-action pair $(s, a)$, one could request the model to obtain a finite number of samples from $\mathcal{P}(\cdot \mid s, a)$. Note that this is not compliant with RL. When reading further, it seems that Monte-Carlo model checking only needs to produce episodes, which is fully compliant with RL. Thus, the distinction is really important here.

On another point, Assumption 5.3 looks rather restrictive. How to ensure that the approximate model learned through Algorithm 1 (line 300) yields a bounded total variation? This should be discussed in the main text. Moreover, the linked guarantees (Proposition 5.5) are not evaluated in the experiments. It could be interesting to have an example of how the statistical guarantees can be applied in practice.

[1] Michael J. Kearns, Yishay Mansour, Andrew Y. Ng: A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes. Mach. Learn. 49(2-3): 193-208 (2002)\
[2] Yujia Jin, Aaron Sidford: Towards Tight Bounds on the Sample Complexity of Average-reward MDPs. ICML 2021: 5055-5064

Limitations:
Apart from the points raised above, the limitations of the work have been successfully addressed in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies RL with 'regular' safety properties. The constraint of safe RL is based on the satisfaction of a logic formula in probability. The action from the 'backup' policy will proactively override the potentially unsafe action from RL to ensure/optimize safety, a typical shielding mechanism in formal safe control methods. The authors demonstrated the effectiveness of their approach against CMDP and regular RL (Q-learning) in two examples.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The approach is sound, as a typical shielding method, should work well in a safe RL setting, in 2 examples shown in the paper. 
The problem studied in this paper is important.

Weaknesses:
1. Novelty. Novelty is my biggest concern for this paper. First, Problem 4.1 has already been discussed and solved in [1][2]. Second, the shielding approach is nothing new, a very standard way in formal methods. You can even trace back to simplex architecture with an advanced controller with safety back control. Third, the model-checking approach of this paper is not novel. 
2. Significance. The experiments are weak with only 2 baselines (1 as original RL, 1 as CMDP) on 2 simple examples.

[1] Wang, Yixuan, et al. ""Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments."" International Conference on Machine Learning. PMLR, 2023.
[2] Wachi, Akifumi, et al. ""Safe exploration in reinforcement learning: A generalized formulation and algorithms."" Advances in Neural Information Processing Systems 36 (2024).

You may want to extend the related works part including two papers above and more.

Limitations:
The reviewer would like to know the limitations discussion by the authors in the rebuttal phase.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an approach to online shielding for reinforcement learning agents. Namely, safety is formulated in probabilistic temporal logic with a parametric threshold as an indicator for reachability of the goal state. The proposed algorithm checks the reachability probability threshold in each state of the environment and raises a warning when the threshold is violated. A pre-trained backup policy is then proposed to be deployed which overrides the action of the agent. The approach is evaluated on tabular and visual RL benchmarks.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper addresses an interesting and valuable problem. 

Evaluation includes visual RL benchmarks, which are interesting to provide online safety for.

Weaknesses:
The paper gives a lot of choice to the reader to compose a problem setting of their interest. It does not, however, provide precise enough approach description for each of them. This makes the contributions blurred. Described problem settings have been extensively studied before and the proposed approach does not significantly improve on them. It is also claimed that the approach can be used both during training and deployment, it is, however, not clear if the authors formulate these as two distinct settings and evaluate separately or not. In the latter case, it would be a dangerous simplification. Figures in the evaluation section are not readable.

Presentation: The presentation suffers from imprecise narrative leaving multiple questions until the evaluation section. Assumptions are introduced twice and it is not clear what exact problem the authors propose to address, or to what exact problem setting it generalizes. The use of ""etc."" and ""some other"" give the impression that more problems can be addressed than presented in the evaluation.

Minor:
- p.7: ""if need be we""
- ""don't"" --> do not

Limitations:
The approach is of limited novelty and employs existing components. Technical details in terms of the exact problem setting and guarantees are insufficient to judge the contribution. The approach is not yet placed in the larger context of online safety for RL agents, which would require more precise problem formulation and discussion of limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses safety and constraint compliance in deploying reinforcement learning (RL) systems. Such issues have triggered a vast body of research in the area of safe RL over the last few years. The paper introduces a safe RL framework for so-called regular safety properties, focusing on satisfying these properties with high probability. The paper compares and places this setup to common constrained Markov decision processes (CMDP) settings and presents a meta-algorithm with provable safety guarantees to prevent violations of regular safety properties during training and deployment. The approach is evaluated in both tabular and deep RL settings.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Safe RL is of utmost importance in order to enable RL agents to be successfully deployed in the real world. This paper tackles an important problem: How to ensure safety if the safety criteria go beyond standard reward or simple reachability features. The motivation of the paper is well done, and the placement in the literature is mostly complete and extensive.

Weaknesses:
While this paper addresses an important research question, I feel it is not ready for publication at a major ML conference. I will first summarize the issues that I see and then elaborate in more detail.

1. The claims made in the intro are not properly met. This concerns particularly the claim that the approaches are most permissive in terms of prior knowledge. 

2. The evaluation is not sufficient. The experiments are very sparse and miss important information to assess the quality of the research.

3. The paper is packed with formal definitions, most of them being standard in the formal methods community. Conversely, little space is spent on describing the methods in detail and providing a proper evaluation.

I overall feel that the paper aims too high and wants to solve all kinds of aspects in model-based safe RL with shields. It would have been better to pick certain key aspects more clearly. As an example, the paper claims to 
- not need a model
- to learn a model
- to use most of the available model checking paradigms (and even introduce them)
- evaluate with tabular and deep RL

Many of these aspects are then not properly discussed. 

I will now comment on the previously listed weaknesses.

1. In the introduction, the authors claim to operate in a most permissive setting where environment dynamics are not known. Yet, then, they introduce three model-checking paradigms, which are essentially standard numerical model checking, Monte Carlo-based statistical model checking, or model checking with approximate models. In particular, the last version is shady. What is a guarantee for a learned model? How can a claim be made for a most permissive setting but still obtain hard guarantees when, as a consequence, all guarantees rely effectively on statistics or a learned model? Other approaches are simply very clear about their assumptions, such as knowing a model. The model learning procedure is standard, and if the whole 'permissiveness' of the setting depends on the fact that a model can learned, I do not see a novel approach here. 

Very importantly: While a model is learned, no safety guarantees can be given, defying the notion of shielded RL. This aspect is not discussed in the paper. A step further, even the safe fallback policy that is explained in Section 6 relies on learning. 

2. The evaluation only considers a few environments and compares a simple tabular and deel RL agent. What I would have liked to see is how the different assumptions and model checking paradigms affect the learning, the safety, the performance. I believe, with a thorough evaluation in this direction, the paper would be much stronger. If we can believe that it is feasible to learn a model, then I would like to see a comparison between the strong assumption of knowing the model, and having learned a model, and what the effects on safety guarantees are. 

3. The contributions only start at page 5, and are then still interleaved with standard definitions. It is important to have a paper safe-contained, but, as an example, why is it necessary to introduce both LTL, PCTL, DFA in detail? And then, the evaluation even uses PCTL^*. To me this seems as if the space had been filled up with long definitions, while it should have been used more on the contributions of the paper. 

Generally, I feel the paper follows a great direction, especially investigating Shielded RL with a learned model. With a stronger evaluation and emphasis of this part, the research and contribution could be much improved in my opinion. 

Minor comments:

- proposition 4.2 is obvious and seems like an overformal statement of a simple fact. 

- Def 4.3. Make clear that this is reward engineering

- Non-Markovian cost: Compare to reward machines

- The tradeoff between safety and exploration has (for instance) been investigated in 

Carr et al.: Safe Reinforcement Learning via Shielding under Partial Observability. AAAI 2023.

- Learning the model, proposition 5.5: I think the topology (graph) of the model needs to be known to estimate the probabilities.

Limitations:
The limitations of the work in terms of assumptions and their real-world relation are not properly explained in the paper.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
PlBUSoSUJG;"REVIEW 
Summary:
Disclaimer: I do not have the mathematical background required to check all proofs. It is also my first time reviewing for NeurIPS. However, I have contributions in classical (deep) RL and finishing my PhD.

In this paper, authors use a novel policy parametrization for RL. Namely, the SoftTreeMax policy that replaces the traditional logit values by small horizon trajectories rewards values. The authors claim and prove that SoftTreeMax policy gradient has less variance than traditional policy gradient. They do experiments with PPO on Atari. 

I summarize my review as follows. The idea and work in this paper are novel, strong and well-motivated. However, it is in my opinion poorly presented, and poorly situated compared to existing related work. 

I very slightly lean towards accepting this paper as is but give feedbacks for the authors to increase their score in the following sections.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
Originality: it is the first time that I see someone use look-ahead information in the policy parametrization. I have already seen it done in the critic part of Policy Gradient (PG) methods, e.g. for advantages estimation in PPO using n-step returns with n>1 and generalized advantage estimations.

Contribution: if the proofs of variance reduction as a function of the depth of the tree expansion in the logit values and as a function of the approximation error of the forward models are correct, I believe this work opens a whole new avenue for future work (which, by the way, is missing from the paper).

Weaknesses:
In the following comments, I assume that the techincal results are correct.
 
First weakness: clarity.

The proposed algorithm (PPO with SoftTreeMaxPolicy) makes heavy use of three major branches of reinforcement learning namely policy gradient methods (core of the paper), model-based RL (use of a forward model to compute small trajectories), efficient implementations of tree search (control the exponential cost of look-ahead search). Those components are all central to your work but the paper lacks a summary of how those components work together.  I recommend to summarize those in a schematic (see Questions section).  

Second weakness: baselines and related work.

The related work mentionned in this work does not clearly help to understand the significance of your work. There are well-studied tools reducing the variance in the gradient estimates (n-steps returns and gae-lambda), and some work combining TS and PG. It would have been nice to see comparisons with those work. I recommend the authors to rewrite and extend their related work section (see Questions section). 

REFERENCES.

TS + PG references:
[An Actor-Critic Algorithm Using a Binary Tree Action Selector, Action Guidance with MCTS for Deep Reinforcement Learning, Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov Decision Processes, Policy Gradient Search: Online Planning
and Expert Iteration without Search Trees]

Better sample efficiency and variance reduction:
[Schulman et al., 2015b; Mnih et al., 2016; Munos
et al., 2016; Schulman et al., 2017; Hessel et al., 2018;
Schrittwieser et al., 2020; Wurman et al., 2022; Chebotar
et al., 2023; Schwarzer et al., 2023, Brett Daley, Martha White, 2024]

Limitations:
No limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a model-based online planning method called SoftTreeMax. The method acts as an extension of the softmax, by replacing the logit in softmax with a n-step return. Based on SoftTreeMax, the paper proposes two learning algorithms, C-SoftTreeMax and E-SoftTreeMax. The work includes a mathematical analysis of both algorithms to prove the policy gradient variance is bounded and exponentially decays when the planning horizon becomes longer. Then, the paper provides an empirical test on C-SoftTreeMax to compare the learned policy and the variance with PPO.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* The method is theoretically sound. The paper provides a detailed theoretical analysis to show the advantage of the decayed policy gradient variance of the proposed algorithm. 

* The paper reports the implementation and computation in detail and ensures reproducibility.

* The paper empirically checks C-SoftTreeMax. The empirical results compare between the performance and the change of variance as the agent learns. Curves clearly suggest the inverse relationship between the performance and the variance, thus supporting the importance of having a method with guaranteed decaying variance.

Weaknesses:
* The main concern I have is the difference between the proposed method and the n-step return. 

    In Section 3, the paper defines the SoftTreeMax logit as 

    $$l_{s,a}(d; \theta) = \gamma^{-d} \left[ \sum^{d-1}_{t=0} \gamma^t r_t + \gamma^d \theta(s_d) \right] .$$

    In the Actor-Critic method, it is normal to use $Q(s,a)$ for the scoring function $\theta$, and learn a policy $\pi(a|s) \propto exp(Q(s,a))$, for example, SAC, a commonly known online learning algorithm, with state-of-the-art performance. In this case, $l_{s,a}(d; \theta)$ can be rewritten to 

    $$ \gamma^{-d} \left[ \sum^{d-1}_{t=0} \gamma^{t-1} r_t + \gamma^d E_a Q(s_d, a_d) \right] .$$
 
    This equation is exactly the same as the equation in n-step TD, except the normalizer $\gamma^{-d}. However, the normalizer can be written as part of the temperature in softmax. 

    In C-SoftTreeMax, the learned policy is written as $\pi^C_{d,\theta} (a|s) \propto \exp [\beta E^{\pi_b} l_{s,a}(d; \theta)] $ in formula (3). 

    Replacing $l_{s,a}$ with the equation in (2), we have $\pi^C_{d,\theta} (a|s) \propto \exp [\beta E^{\pi_b} [\gamma^{-d}  \sum^{d-1}_{t=0} \gamma^t r_t + \gamma^d \theta(s_d) ]] $. 

     As the expectation is taken on $\pi_b$, the normalizer $\gamma^{-d}$ does not depend on $\pi_b$, thus could be moved out of the expectation. Then (3) is changed to $$ \pi^C_{d,\theta} (a|s) \propto \exp [(\beta\gamma^{-d}) \mathbb{E}^{\pi_b} [ \sum^{d-1}_{t=0} \gamma^t r_t + \gamma^d \theta(s_d)) ]]  .$$ 

     Now the temperature becomes $\beta\gamma^{-d}$, which is a tunable parameter, and the expectation term becomes n-step TD when using $Q(s,a)$ for $\theta$.
     Similarly, when using the state value $V(s) = \theta$, the normalizer can be written as part of the temperature, and the expectation term becomes n-step TD.
     In this case, using the proposed method seems to be no different from simply changing the value update from TD(0) to n-step TD. 

* The empirical test seems to be incomplete. The paper proposes two algorithms, C-SoftTreeMax and E-SoftTreeMax. But only C-SoftTreeMax was empirically tested. The paper explains the empirical test for E-SoftTreeMax was left for future work on risk-averse RL. But it does not make sense to me that the empirical test is completely omitted while the method is listed as a new algorithm in the paper. 

* There could be a stronger baseline. Given the similarity between SAC policy ($\pi(a|s) \propto exp(Q(s,a))$) and the policy learned by C-SoftTreeMax ($\pi(a|s) \propto exp(W_\theta(s,a))$), it may worth testing SAC, both 1-step TD and n-step TD in critic update.

* The empirical test could include more seeds. Currently there are only five seeds. The variance calculated based on 5 seeds could be highly inaccurate.

* The empirical test is limited to discrete action space. It is acceptable and understandable to leave continuous control experiments to future work, but still, adding continuous control results could make the paper more convincing.

* The new method introduces a new parameter, the planning length $d$. According to the empirical results (Figures 3, 4, and 5), different $d$ gave very different performances. The best setting is different across tasks. In 3 out of 8 tasks, (NameThisGame, Phoenix, and VideoPinball), longer planning hurts the performance. It is acceptable to say the performance is sensitive to the value of $d$ and leave this parameter as a tunable one, but it could be better to give an indication of how to pick this value when the reader would like to apply this method to a new task.


**Typo:**
* Line 307, “one week The” -> “one week. The”

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new policy parameterization called SoftTreeMax, which can reduce the variance of the stochastic policy gradient. The authors consider finite state and action spaces throughout the paper. They start by taking a softmax tabular policy and replacing the logit $\theta(s,a)$ with a score from a trajectory of horizon $d$ sampled from a behavior policy. They then address the normalization accordingly. Because of the randomness of the trajectory of size $d$, the authors take an expectation with respect to the randomness of the trajectory of size $d$ to well-define the policy parameterization. By taking the expectation before or after the exponent operator, the authors propose two policy parameterizations: C-SoftTreeMax and E-SoftTreeMax. It turns out that under either C-SoftTreeMax or E-SoftTreeMax parameterization, the variance of the stochastic gradient of the value function decays exponentially with the planning horizon $d$. The theoretical findings are well supported by simulations. Finally, the authors propose a parallel GPU-based simulator for the practical implementation of SoftTreeMax. By applying the C-SoftTreeMax parameterization to PPO and adapting $\theta(s)$ with a neural network, as well as using the parallel GPU-based implementation, leads to better performance compared to distributed PPO.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The concept of integrating tree search (planning) directly into policy parameterization is both intriguing and innovative. It demonstrates two key advantages: 1) With the SoftTreeMax parameterization, the variance of the stochastic gradient of the value function decreases exponentially with the planning horizon. 2) It can be easily combined with various policy gradient algorithms, such as REINFORCE and PPO, to enhance their performance.
- Although computing the parameterization is computationally intensive, the authors offer a practical solution through parallel GPU-based simulation.
- SoftTreeMax shows significant potential for extension to infinite or large spaces, making it a promising method for addressing complex problems.

Weaknesses:
- Not really a weakness. Since the SoftTreeMax can be beneficial to different policy gradient methods, I expect to see its improvement applied on some (original) policy gradient methods for the experiments, such as REINFORCE and actor-critic, to demonstrate its generality, which is another strength of the paper.
- Since SoftTreeMax is inspired from tree search, the authors can highlight the difference between the ""tree search"" literature and the ""tree expansion"" used in this paper in order to avoid confusion. The tree search method uses a max to find the best policy, while here the authors use a fixed behavior policy.
- For the tree search literature, the authors can cite Efroni et al. [2018] and Protopapas et Barakat [2024]. In particular, Protopapas et Barakat [2024] combine a variant of PG, the policy mirror descent, with tree search.
- For the theoretical PG literature in Line 312, the authors can cite Yuan et al. [2022], where they provide a general analysis of PG, including softmax tabular parameterization as a special case.

Efroni, Y., Dalal, G., Scherrer, B., and Mannor, S. Beyond the one-step greedy approach in reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1387–1396. PMLR, 10–15 Jul 2018.

Kimon Protopapas, Anas Barakat (2024). Policy Mirror Descent with Lookahead.

Rui Yuan, Robert M. Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gradient. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 3332–3380. PMLR, 28–30 Mar 2022.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a model based policy gradient (PG) algorithm that tries to decrease the variance of the gradient updates in PG methods and thus (hopefully) improving their sample complexity. The proposed approach works by modifying the softmax policy to work not just not with simple score functions (mapping states to real numbers), but rather with the truncated expected discounted return from a state. The resulting policy is called ""SoftTreeMax"". As such, this paper seems a little bit similar to decision time planning (or MCTS type) algorithms.

The paper proposes two variants of SoftTreeMax and analyzes their variance properties; in particular, the results show that as the size of truncation increases, the variance decreases exponentially. The paper also has a theorem which shows that SoftTreeMax can estimate the gradients well even with an approximate model. There are also some experiments on certain Atari games which show the efficacy of SoftTreeMax against the softmax policy using the popular PPO algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The proposed algorithm seems novel to me in that I have not seen tree search used with policy gradient. As far as I remember, most tree search methods employ value function based RL algorithms (most popular being AlphaZero). In that way, this paper is quite novel. Further, the modification of logits to instead include the expected return seems novel too. (Although, it might be related to the ""soft-Q learning"" papers a little bit; something which could be mentioned in the paper. For instance, see https://arxiv.org/pdf/1704.06440, Eq. 2.)

Quality and Clarity: The paper is well written and has clear theorem statements that are easy to interpret. The setting considered is the popular discounted finite MDP, making the results very easy to follow. The experiments are also conducted using popular benchmarks and thus help the reader understand and compare the proposed method. Another nice thing is that the experimental results clearly align with the theoretical statements (such as the variance reduction with depth) and the benefits in terms of achieving better sample complexity are quite clear. Overall, the paper seems like a logical next step: using policy gradient methods with model based and tree search based methods. (In particular, for continuous action spaces, this would be highly useful and relevant.)

Significance: The paper combines, a very popular class of RL methods - policy gradients, with tree search like planning algorithms. For many difficult problems, particularly those with very large search spaces, Tree search has been shown to be very effective. On the other hand, policy gradient methods are broadly the most popular way of dealing with continuous action RL problems. Therefore, an algorithm combining these two solution methods can have immense applications, especially as AI methods continue to be adopted for solving more and more real-world decision making tasks (robotics being a good representative application area).

Weaknesses:
- The major weaknesses of this paper are not enough discussion on the exact mechanisms for implementing SoftTreeMax. So SoftTreeMax is akin to a policy parameterization (such as softmax). As such, when implementing an RL agent, we need a large number of other details (some details being how is the exploration policy chosen, what optimizer is used to update the gradients, how are the function approximators initialized, is there any processing of the observation features, etc.). These implementation details are important since they can affect the performance of the RL algorithms more than the core idea itself (for instance, recall RainbowDQN https://arxiv.org/abs/1710.02298 and PPO https://arxiv.org/abs/2005.12729).

- Can you guess how SoftTreeMax run on a continuous action space MDP, such as those from Mujoco?

- Does SoftTreeMax solve the problems of Softmax policies? I know it helps with variance, but what about softmax's dependence on the initialization? Or the possibly slow convergence of sotmax PG in MDPs due to an exponential dependence on the state space size (https://arxiv.org/abs/2102.11270)? Do you have any intuition? From the nice discussion in Appendix C.1, it seems that the answer is no. In particular, choosing the behavior policy while generating the tree could be quite important. Maybe some comments on how this policy is chosen in the implementation could be helpful (I saw the high-level comments about controlling for the eigenvalues of transition matrix; but those seem high-level ideas).

- What does SoftTreeMax really do? Is it just an MCTS like decision time planning algorithm that is helped a tiny bit by the theta values in Eq. 2? For instance, what if I do not update the theta values at all (i.e. set learning rate to zero). In that case, would the performance decrease drastically? Related to that, do you have any comments on how it relates with MCTS? I remember Bertsekas (http://web.mit.edu/dimitrib/www/LessonsfromAlphazero.pdf) said that as the depth increases, the utility / importance of the learned values (in this case theta values) in decision making decreases. Would that apply here as well?

- Once the policy is learned, how do you sample from it? I guess, you still need a model to sample from this policy. This seems like a clear disadvantage (although, this is true for all tree search based / decision time planning methods). Maybe mention this as a limitation.

line 53: ""In all tested Atari games, our results outperform the baseline and obtain up to 5x more reward."" --> this is not true? For instance, results on VideoPinball or Breakout show that PPO is pretty much as good. Also, the paper doesn't specify how the hyperparameters were chosen. So maybe PPO with a better choice of hyper-params might outperform SoftTreeMax?

Limitations:
The major limitation seems to be that the paper doesn't provide

1.  a written algorithm - while I understand that this is almost straightforward, just putting a clear algorithm (specifying order of planning/simulation steps, how to take real world actions with SoftTreeMax policy, computing gradients, and updating the policy) would be highly useful.

2. The paper doesn't run experiments on continuous action space environments, which is arguably the more relevant domain for PG methods. (For discrete environments like Atari, we already have a plethora of value function + tree search based methods - like AlphaZero or MuZero.) Further, there are no hyper-parameter settings specified; it would be difficult to reproduce these results without looking at the codebase.


Further, a limitations section is missing
- if the PG is biased, would that affect convergence? In particular, note that Theorem 4.8 shows that the bias would scale with the state space size S (which is generally huge). Maybe setting d = \log(S) / \log(1 / gamma) helps with this and reduces this error to \log(S). But the computational complexity is probably exponential in d, so each gradient update becomes O(S), which is also bad..

Maybe a discussion between these factors would be helpful
---- how does the increase in bias affect the convergence speed (and the quality of the final policy obtained). What bias does the pruning approach introduce?
---- how does the decrease in the variance of gradient update affect the convergence speed (and the quality of the final policy obtained). Does the additional variance (in computing Lemma 4.6) introduced by pruning remove counteract the variance reduction (as given in Theorem 4.4)?
---- how does changing the bias / variance affect d and thus the computational complexity (edited)

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
PKEmH9ZJfw;"REVIEW 
Summary:
This paper studies weakly supervised causal representation learning with soft interventions.
It assumes that pairs of observations are provided that differ by a soft intervention on one variable in the latent space.
Additionally, the intervention target (and the total number of latent variables) are given.
Identifiability up to element-wise transformations assuming linear mixing and known intervention targets (among other assumptions) is shown.
Experiments on synthetic data compare the proposed method to ILCM, beta-VAE and D-VAE.
An improved D and C score in the DCI framework is shown.
An experiment on semi-synthetic image data is shown.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method extends previous work on weakly supervised CRL and generalizes it from hard to soft interventions, which is an important step for bringing CRL closer to real-world applications, since CRL methods often suffer from restrictive assumptions.
- Also, I appreciate the effort of testing this CRL approach to datasets which are more similar to real-world image datasets.

Weaknesses:
- The mathematical notation is poorly defined and difficult to follow: objects aren't well-defined and notation inconsistently or unusually used. See questions below.
- L136-137: ""a diffeomorphic solution function [...] deterministically maps a value for exogenous variable [...] to a value for causal variable [...]"" That's incorrect. There can be such a map from the set of all exogenous to all endogenous, but not per variable. Consider $X:= U_X; Y:= X+U_Y$, where $U_X$, $U_Y$ are exogenous. In general, there is no deterministic map from $U_Y$ to $Y$.
- Even though the DCI framework for evaluation of the learned representation is cited, only two of the metrics (D and C) are used, whereas Informativeness (I) is omitted without comment. That seems suspicious to me.
- The synthetic experiments test a rather simple setting on few data generating processes. The observation dimension is 4 (same as latent dimension) and the mixing is linear (as far as I can tell). We have seen in previous work (e.g. [2]) that the complexity of the mixing function is a crucial element in the recoverability of the latents. For example, the more nonlinear the function, the harder it is to recover latents. Furthermore, only 10 data generating processes are tested. That seems quite limited, given that the dimensions of the data is so small.


**Minor:**

- L133: ""decoder function"" is a bit of an odd choice for something that relates to the ground-truth data generating process. Usually, this is a part of a model. ""Mixing function"" would be more appropriate.
- There is a period missing in L251.
- L345: There is a follow-up version of DCI that takes more aspects of the learned representations into account [1]. It could be considered for the next iteration if time permits.
- L363: ""As mentioned in [27, 25], causal graphs are sparse and in the G5 case, where the graph is fully connected, the proposed method cannot identify the causal variables well."" The two references **do not say that causal graphs are sparse**. You may refer to the sparse mechanism shift hypothesis, which says that changes between environments are assumed to stem from changes in few mechanisms of the causal graph. This is a statement about sparse changes, not sparse graphs.

Limitations:
- As far as I can tell, no latents were uncovered (theoretically or experimentally) for either nonlinear mixing or unobserved intervention targets. Therefore, it seems to me, it should be mentioned that the results are for the linear case and weakly supervised. The title and abstract make the impression as if the more general CRL problem with paired observations is solved.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new approach **ICRL-SM** that performs implicit causal representation learning (mapping from noise to latent variables) by using causal mechanism switch variable to model the soft intervention effects.

&nbsp;

### References
[1] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning with interventions. arXiv:2301.08230, 2023.

[2] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning: Linear and general transformations. arXiv:2402.00849, 2024.

[3] Julius von Kügelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Kekic ́, Elias Bareinboim, David M Blei, and Bernhard Schölkopf. Nonparametric identifiability of causal rep- resentations from unknown interventions. In Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2023.

[4] J. Brehmer, P. De Haan, P. Lippe, and T. Cohen. Weakly supervised causal representation learning. In Advances in Neural Information Processing Systems, volume 35, pages 38319– 38331, 2022.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of implicitly modeling causal effects using switch variables is very interesting. The experiments have shown promising performance on synthetic and high-dimensional image datasets compared to several baselines.

Weaknesses:
1. My most significant concern is that this paper is generally not well written and, thus, pretty hard to follow. For example, the calligraphic letter $\mathcal{Z}$ was used throughout to denote both causal variables (e.g., line 137) and its domain (e.g., line 139).
2. The assumption of a diffeomorphic causal mechanism is pretty strong. I am aware that [4] made a similar assumption. Yet, it is not a very common assumption for latent causal models in other causal representation learning literature (e.g., [1, 2, 3]).
3. Line 278: The Gaussianity assumption of the causal and exogenous variables might be hard to satisfy in realistic settings.

Limitations:
The main paper did not discuss limitations. The authors pointed them out in the checklist, but I would have appreciated it more if they had adequately addressed the limitations in the main paper.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel approach for learning implicit causal representations through switchable mechanisms, specifically designed to handle soft interventions which are more realistic but challenging compared to hard interventions. The authors introduce a causal mechanism switch variable to model the subtle effects of soft interventions and establish the identifiability of causal models under certain assumptions. Their proposed method, ICRL-SM, demonstrates improved performance in learning identifiable causal representations over baseline methods through experiments on synthetic and real-world datasets. The paper contributes a new perspective to causal representation learning with potential applications in various domains where understanding causal relationships from observational and interventional data is crucial.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A standout feature of this paper is the introduction of a switchable mechanism to model the nuanced effects of soft interventions. Traditional causal inference methods often focus on hard interventions, which are impractical in many real-world scenarios due to the need for stringent control. The paper's approach to incorporate soft interventions through a causal mechanism switch variable is groundbreaking. It allows the model to adapt to changes in causal relationships post-intervention, providing a more realistic and flexible framework for causal representation learning.
2. This paper proposes Augmented Implicit Causal Models, which is an innovative concept that extends the scope of implicit causal representation learning. By integrating the causal mechanism switch variable into the model's solution functions, AICMs can capture the intrinsic characteristics of each causal variable while accounting for intervention effects. This approach sidesteps the need for explicit parameterization of the causal graph, which is a complex and often intractable task. The innovation here lies in the model's ability to implicitly learn the causal structure while directly modeling the effects of interventions.

Weaknesses:
1. The method's effectiveness relies on several key assumptions, including knowing the targets for intervention, interventions being atomic (indivisible), and variables following a multivariate normal distribution. These assumptions might not be realistic in many real-world settings where interventions can be complex, and data may not be normally distributed.
2. While the paper's experiments on synthetic and specific real-world (Causal-Triplet) datasets demonstrate the method's potential, it's unclear how well it generalizes to other data types or different domains.

Limitations:
No

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work is in causal representation learning that utilizes interventional data. It involves two types of common interventions: hard interventions and soft interventions. It’s known that soft intervention is more general since it covers hard intervention. But it is also more challenging since parental relations remain. This work proposed identifiability results using a causal mechanism switch variable designed to change between different causal mechanisms by component-wise transformations. Adequate experiments were provided to verify they claim.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Pros:

1. Overall, the writing is clear and with good context.
2. Adequate experiments were provided, including real-data experiments. Also, they offered good observations of the comparison with baselines. This is a big plus to their contribution.

Weaknesses:
Cons:

1. It's unclear what the unique theoretical contribution is to this switch variable since both types of interventions were proposed before. Could the author clarify the theoretical contribution? Especially regarding the original technical hardness compared to [1] and also [38].

2. In the synthetic experiment, it seems that you use the linear decoder, which loses the purpose of training a neural network to handle non-linear issues. Did you try non-linear functions and see how it goes? Besides the switch variable utilizing interventional data, what is the difference between these settings and those classical linear unmixing methods (like linear ICA or its other assumption variants)?

Some minor issues/recommendations:

Line 161, the $e_i$ is not defined, if you mean causal variable, then you already have defined it as $Z_i$.

Line 187, the ‘\s’ is not defined before but later in line 195.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
OiTr2v90t7;"REVIEW 
Summary:
This article adapts the recently-developed combinatoric concept of the “Pemutree” into a machine learning context, making links with existing methods in Bayesian nonparametrics, and providing a pathway for how to make the abstract mathematical concept relevant to data-driven approaches and inference. The theory is explained, and an example application in phylogenetic analysis is performed.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
This is an ambitious paper! Making links with recent combinatorial research and machine learning is a good thing to do! It has a large vision of building one grand unifying theory of discrete Bayesian nonparametrics, which (it is claimed) can be done with the framework presented. The review of discrete BNP at the start of the article is thorough. 

The potentially difficult subject matter of the abstract mathematical objects is explained fairly clearly through the use of figures and well-chosen notation. The theoretical aspects are explained thoroughly, and the application pursued emerges naturally from the framework developed earlier in the article: the coalescent analysis and the Mondrian process are good results to have. The scientific writing is of a fairly high standard, with only a couple of surprising vocabulary choices.

Weaknesses:
The subsequent developments of the theory and applications don’t quite live up to the grand vision set out earlier in the article. The authors struggle to represent the most widely-used discrete BNP object of the Dirichlet Process in this supposedly all-encompassing framework: perhaps this is doable in the future and represents work yet to be done, but the initial claims about the generality of permutree processes made in the article are not fully followed through on.

The article also (necessarily) spends a lot of time introducing the theoretical framework, quite heavily at the expense of presenting the data applications properly later in the manuscript. Squeezing all of the experimental results into “Demonstration” in half a page is really too brief to be very convincing, although there is a lot of interesting material in Appendix C that would ideally be in the main text. Many of the potentially thorny issues concerning inference and computation are therefore overlooked.

Some of the figures could be better designed: I found the visual interpretation of permutrees key to developing some understanding them, so making Figure 1 bigger and more prominent would be a help (I think figure 1 is more crucial than Figures 2/3/4 in this respect). The representation of the data in greenscale Figures 8 and 14 is very confusing: I don’t think I really learned anything from that representation.

Some of the language choices are a bit strange: line 69: “we dare to pay particular and explicit attention here”, line 869: “Roughly speaking, it is not possible in principle to naively implement a model with infinite parameters on current computers”, 

Line 250: “as an overall trend…” The analysis of the experimental results is not rigorous enough. You have real values and uncertainties for the perplexity. Do some tests or similar to establish more clearly the differences in performances rather than painting broad brushstrokes.

Line 80: some of the symbols used have already established meanings in a machine learning context, i.e. \otimes meaning kronecker product. Maybe make clear that this is a new notation that overrides any previous perceptions.

Limitations:
The practical models that are (presently) successfully captured in this framework are not the most widely used BNP models out there. The whole permutree framework seems best suited to the coalescent-type models pursued, and the other BNP models that have been successfully described, such as Mondrian processes, are interesting but not in widespread use.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
After giving an introduction to permutrees, a stochastic process on them is constructed by sampling the nodes according to an intensity function on the 2d unit interval and uniformly assigning the marks. It is shown how to add the edges to meet the requirements for the object being a labeled permutree. Paths from terminal nodes to other terminal nodes in the permutree can be used to represent sequential data. Finally, the model is used for an inference task involving DNA sequences.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* Sections 1-4 are very concise but nonetheless clear and easy to follow. This paper does a great job explaining the complex concepts of permutrees and permutree processes. 
* Figure 1-3 are very helpful
* the concept generalizes popular processes used in ML, e.g., the Mondrian processes

Weaknesses:
* As a reader unfamiliar with phylogenetic analysis, I did not immediately understand what the task in this setting is (and I am still unsure if I fully got it): Do you have a set of DNA sequences where some of the letters are masked, and you want to predict the masked letters? 
* Besides not really understanding what the goal of this task is, I think Section 5 does not provide enough explanation of how this goal then is achieved, i.e., the length of the paper/ the level of detail in Section 5 is a problem. It's ok to defer details to the appendix as long as one can still follow the main section without them, but I struggle with that. Example given: I find it crucial to know the likelihood function when it comes to a Bayesian inference task, which is not mentioned in the main section.

Limitations:
yes, there is a dedicated paragraph on limitations and I think it captures the limitations of the suggested concept appropriately.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce a prior for Bayesian nonparameterics called the permutree. They apply it to model complex phylogenetic data with both coalescence and recombination, a setting that previous processes such as the Kingman could not model; the model seems to perform state-of-the-art phylogenetic inference. In principle the process could also be used to model other combinatorial objects such as permutations or trees however this is not demonstrated.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
Modeling recombination is challenging and this model proposes a method to do so.

Weaknesses:
The writing is very challenging. There are multiple points where the writing is strange, for example the use of ""dare"" in ""For technical reasons (discussed immediately below), we dare to pay particular and explicit attention here to the set V of the “interior vertices” (i.e., vertices of degree at least 2) other than the terminal nodes."" "". The exposition is also very verbose and proposition is challenging to understand without reading the proof. Figures 3(c) and 1(b)-(d) are never mentioned in the text, the later is quite confusing since it seems to suggest that the permutree can model many combinatorial objects.

It seems that there is a disconnect between the description of the methods in sections 3, 4, and the experiment in section 5. See questions.

The ultimate goal of phylogenetics is to infer ancestry which is not identical to maximizing likelihood. To validate a bone fide phylogenetic inference method that handles recombination, one should show that inferred recombination events are realistic.

Limitations:
Discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors describe the concept of permutrees, how to sample permutrees in a stochastic process, and how to model data with permutrees. They apply it to tracking DNA changes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Interesting new model that unifies permutations, trees, partitions, and binary sequences 

Strong mathematical foundation

Practical applications

well-written

Weaknesses:
the figures are small and hard to read when printed in gray-scale

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
OGaZVSS0Cx;"REVIEW 
Summary:
The authors present the first mini-batch algorithm for kernel k-means. The algorithm itself is simple and works the way one would expect mini-batch kernel k-means to work. The authors improve the running time of an iteration of kernel k-means from $O(n^2)$ to $O(n(k+b))$ for the mini-batch version of the algorithm. Additionally, they show that using a specific learning rate function, there is an upper bound on the number of iterations of the algorithm.

The main challenge in the design of this algorithm is to keep track of the intermediate centers as storing the points in feature space is infeasible, as they are updated iteratively as in Lloyd's algorithm. For this, the authors design a recursive update rule to keep track of the quantity $\| \phi(x) - C_i^j \|^2$ for each iteration $i$ and each center $j$. They show that in a new iteration this quantity can be updated by considering the distance of each point in the dataset to the centers of mass of the clusters in the mini-batch and the previous centers.

Finally, the authors provide an experimental study of the mini-batch algorithm on four datasets and compare it to a non-kernel mini-batch algorithm and the full kernel k-means algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The algorithm offers improved running time bounds that are interesting to practitioners using kernel k-means in practice. It is also the first algorithm for mini-batch kernel k-means.
- The main theorem's bound on the number of iterations is nice to have and a good follow up to paper [26].
- The theoretical analysis is cleanly written and easy to follow.

Weaknesses:
- The techniques, while elegant are not particularly novel in terms of theory. The proofs mostly follow from analyzing the inner product terms in the k-means formulation. 
- A number of the proofs in the main body of the paper could have been moved to the appendix, as they do not give the reader more of an understand of the big picture and are very detail specific.
- There is no discussion of the experimental results.
- While the authors state the paper is mostly theoretical, I believe this algorithm is mostly interesting to practicioners and therefore a more thorough focus on the experimental evaluation with more parameters, additional datasets and thorough discussion would strengthen the paper in my eyes.

Limitations:
The authors included a checklist in the appendix of the paper, but have not discussed practical limitations in the main body of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the first mini-batch kernel $k$-means algorithm, which significantly reduces running time compared to the previous kernel $k$-means methods relying on the full datasets. With the proposed mini-batch kernel $k$-means algorithm, each iteration can be executed in time $O(n(k+b))$, improving the complexity of $O(n^2)$ for fully-batch methods. The authors also provide theoretical guarantees, ensuring that the algorithm can terminate (reach a convergence) within $O(\gamma^2/\epsilon)$ iterations with high probability, where $\gamma$ is the bound on the norm of points in the feature space. When initialized with the $k$-means++ seeding method, the algorithm achieves an $O(logk)$-approximation. Experimental  evaluations confirm that the mini-batch kernel k-means algorithm performs significantly faster than its full-batch counterpart while maintaining solution quality.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed algorithm achieves significant improvements on time complexity compared with full-batch kernel $k$-means methods.

The paper provides theoretical analysis, ensuring the algorithm's termination and performance bounds if initialized with the $k$-means++ seeding method.

Weaknesses:
The techniques used in this paper are largely based on the work of [1]. Mini-batch $k$-means method is not new for clustering problem. The main contribution of this paper is to combine the idea of mini-batch $k$-means with the kernel $k$-means versions. It should be noted that the theoretical bounds given in this paper are not entirely novel.

There are some technical issues in the proofs (details see questions),  potentially undermining the theoretical guarantees.


[1] Gregory Schwartzman. Mini-batch $k$-means terminates within $O(d/\epsilon)$ iterations. ICLR 2024.

Limitations:
Although this paper mainly gives theoretical results for clustering problems, it lacks discussions on broader impact as required by the NeurIPS guidelines.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose the first mini-batch kernel k-means clustering algorithm. It is a variant of Lloyd's algorithm that was introduced by Sculley that takes a batch of random b points instead of the full set of points and a weighted avaerage with the current centers while updating the centers. This paper attempts to translate this idea in the *kernel* k-means setting. The resulting algorithm has the same approximation guarantee as the original k-means but it terminates faster and consumes less time per iteration.

Their analysis follows the recipe of Scwartzman who used an early stopping condition when the improvement on the batch drops below some user-provided parameter. The main challenge in the kernel setting is that the underlying Hilbert space could be large or even infinite-dimensional. This is prohibitive as Scwartzman's bound on the number of iterations depends on the dimension. The authors bypass this by instead giving a bound on the Hilbert norm of the points which can be bounded in practice for example using normalized kernels.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors coduct detailed experiments that compares their algorithm favorably with the prior works. Specfically, the ARI and NMI scores were noticably better across a variety of 4 datasets.

I liked the paper. I think it has a decent theoretical and experimental contribution.

Weaknesses:
New ideas are limited. Mostly an adaptation of Scwartzman's work in the kernel setting

Limitations:
None.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The article presents the first mini-batch kernel k-means algorithm, which significantly improves running time compared to the full batch kernel $k$-means with only a minor negative effect on solution quality. The proposed algorithm runs in $O(n(k+b))$ time per iteration, as opposed to $O(n^2)$ for the full-batch version. The authors provide theoretical guarantees for the algorithm's performance, demonstrating that it terminates within $O(\gamma^2/\epsilon)$ iterations with high probability when the batch size is $\Omega((\gamma/\epsilon)^2 \log(n\gamma/\epsilon))$. Experimental results confirm the efficiency and effectiveness of the algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Improved Efficiency: The mini-batch approach drastically reduces the running time from $O(n^2)$ to $O(n(k+b))$ per iteration, making it feasible to handle large datasets.

Theoretical Guarantees: The algorithm includes a thorough theoretical analysis, ensuring termination within a specific number of iterations and providing an approximation ratio when using $k$-means++ initialization.

Flexibility with Kernels: The algorithm works well with popular normalized kernels (e.g., Gaussian, Laplacian), making it versatile for various applications.

Practical Relevance: Early stopping conditions align with practical machine learning workflows, increasing the algorithm's usability in real-world scenarios.

Weaknesses:
Approximation Quality: While the solution quality is comparable to the full-batch version, the approximation ratio depends on the batch size and initialization, which may not always guarantee optimal clustering.

Parameter Sensitivity: The performance heavily relies on parameters such as batch size and learning rate, which need careful tuning.

Complexity in Implementation: Implementing the recursive distance update and maintaining inner products can be intricate, potentially increasing the implementation complexity.

Potential Issues

Stochastic Nature: The inherent stochasticity of mini-batch algorithms can lead to variations in performance, and convergence to local minima is not guaranteed.

Parameter Initialization: Poor initialization of cluster centers can significantly affect the algorithm's performance and convergence speed.

Data Dependence: The effectiveness of the algorithm may vary depending on the dataset characteristics, such as the distribution and dimensionality of the data points.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
O6YRAOfHGt;"REVIEW 
Summary:
After rebuttal

While I think there are still some problems with this paper, e.g. the short training duration, and the slight exaggeration of claims (that SHED outperforms UED). I think, however, that the idea is nice, and getting RL environment design to work better is a good goal.


-----


This paper aims to improve Unsupervised Environment Design in two ways.
First, it introduces a hierarchical MDP formulation, where the top level corresponds to the teacher, and the lower level corresponds to the learning agent. Each transition in the top-level MDP involves training the lower level agent on generated levels. Related to this, they develop a state representation for the adversary, which is the performance of the agent on a fixed set of diverse levels.

Separately to this, they use a diffusion model to upsample the number of experiences for the teacher, effectively training on synthetic data, to improve sample efficiency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- I think the H-MDP formulation itself is very valuable; it moves away from treating the generation of environments as a black-box, sparse reward, multi-step generation process (as in PAIRED), and towards a more informed process, where the teacher gets feedback in terms of the state (i.e., the performance vector).
- The analysis in the appendix investigating the ability of the model to generate good synthetic trajectories is useful.

Weaknesses:
- Major
	- The results do not very convincingly demonstrate that SHED is better than current SoTA. Looking at figure 3 particularly, I would say that ACCEL has about the same performance as SHED. However, comparing against the RL baseline, SHED does do better.
	- The method is limited in the types of environments it can generate. For instance, mazes are generated using an LLM instead of directly placing blocks. This method therefore is not quite as broad in scope as PAIRED or ACCEL, which can generate arbitrary environments.
	- Relatedly, in the minigrid experiments, do all methods generate the levels in the same way using an LLM, providing the difficulty numbers? It would be good to compare this against the standard version of ACCEL that directly places blocks in the maze level, as it does not have the same restriction as SHED.
- Minor
	- The figures can be improved:
		- Make the alpha value on the error bars a bit less
		- Keep colours consistent across figures, so the same method has the same colour
		- Keep capitalisation consistent across the figure labels.
	- line 80, the period ends on the line after the maths, it should end on the same line.
	- Footnote 1: Jiang et al. (2021) use (amongst others) the positive value loss, which is not quite the GAE, as it clips it at zero before summing.
	- equation one, you use $\beta_t$ but $t$ does not seem to be defined? Should this be $\beta_k$?
	- Line 159, PARIED should be PAIRED
	- There is no reward scale in figure 4
	- Figure 9's caption can be made clearer. I understand it to be the performance of each method in different testing environments. 
	- Line 718 does not link to a figure.
	- Figure 11's caption: zero-shot and not zeros-shot
	- Capitalise the first word in the title of appendix C.2
	- In terms of notation, in line 96, $\pi^*$ usually has a dependence on $\theta$ (e.g. $\pi^*_\theta$) to indicate it is optimal w.r.t. that particular level.
	- Line 217, maybe add a citation to the first sentence, as I thought that is what you do, which confused me for a second.
	- line 237 space after period.
	- Line 241 ""given"" instead of giving?
	- Lines 296 - 297 are a bit confusing, as the word environment is used three times.
	- The assumption in theorem 1 is pretty strong.

Limitations:
I think the authors can list a few more limitations. 
Primarily, the restriction on the type of environment that can be generated, i.e., it needs numerical parameters, and generating a maze outright is challenging. This is quite a large difference to prior settings.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel approach to Unsupervised Environment Design (UED) that addresses the challenges of efficiency by introducing a hierarchical MDP framework and using synthetic data. This framework involves an upper-level RL teacher agent that generates training environments tailored to a lower-level student agent's capabilities. The paper proposes the Synthetically-enhanced Hierarchical Environment Design (SHED) method, which uses generative modeling to create synthetic trajectory datasets, thereby reducing the resource-intensive interactions between agents and environments. The effectiveness of SHED is demonstrated through empirical experiments across various domains, showing superior performance compared to existing UED methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of diffusion models to generate synthetic trajectories is a novel approach that effectively reduces the computational burden of training the teacher agent.
- The paper provides comprehensive experiments across different domains, demonstrating the effectiveness and robustness of the proposed method compared to state-of-the-art UED approaches.

Weaknesses:
- The proposed method introduces significant complexity, particularly in the implementation of the hierarchical MDP and the generative modeling components. This might limit the accessibility and reproducibility of the approach.
- While the empirical results are promising, the evaluation is limited to a few specific domains. It would be beneficial to see broader applicability across more diverse and complex environments.
- Figure 4 is not properly formatted (no values on the axes).

Limitations:
The limitations are discussed in Appendix F.1 but I think the authors should discuss the limitations in the main paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the Unsupervised Environment Design problem, where a teacher agent seeks to design environments to train a student. Methods such as PLR, PAIRED and ACCEL have recently shown promising performance for random, RL and evolutionary generators. This paper proposes a handful of modifications, using RL with a different objective vs. PAIRED (performance on held out set vs. regret) and also proposes to add synthetic data to accelerate the RL process.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This is an interesting method in a relevant area of research. UED seems to be one of the most active areas of research with plenty of opportunities for impact.
* The use of evaluation environments is sensible and novel.
* The idea of combining this with Genie is incredibly exciting. It would be interesting to hear how this could be possible or could work. Is there any way to show a simple proof of concept?

Weaknesses:
* There appear to be two confounding features of the method, the new objective for PAIRED and then the synthetic data. Why do they make sense to combine in this way? It just feels like the authors tried to do ""enough for a paper"" rather than contribute something meaningful that people can build on. I say this because its unclear how these two independent features interact with other existing algorithms. Maybe we should just do ACCEL with synthetic data for instance? Did the authors try that? If it is in the Appendix already and I missed it then I will increase my score.
* The performance gains are fairly minor, and presented in an unclear fashion with just a bunch of curves on a single plot. Can we get some more rigorous analysis for example using the recommendations from Agarwal et al, ""Deep Reinforcement Learning at the Edge of the Statistical Precipice""?
* The Maze experiment seems to have many inductive biases and seems distinct from the diffusion based approach for BipedalWalker and LunarLander. What happens if ACCEL has access to ChatGPT as an editor and then uses replay? This seems like a simpler extension that alone could be a strong paper - although it would resemble ELM (Lehman et al 2022) so it wouldn't be particularly novel.
* The related work is very light. This is disappointing since the paper builds on so many related areas, such as synthetic data, diffusion models, UED, language models for evolution, procedural content generation etc.

Limitations:
Covered in the Appendix.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors of this paper use hierarchical MDP formulation and a teacher agent trained by RL to perform curriculum learning. To address the sparse data available for the teacher agent, this paper uses diffusion models to synthesize datasets for training. This paper performs experiments on lunar lander and bipedal walker environments to validate their claim.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Data sparsity is one of the main limitations of using a teacher agent in curricular RL. This paper uses diffusion models to synthesize a dataset for the teacher agent.

Weaknesses:
- This paper designs the teacher agent via hierarchical MDP to model the learning process of the student agent to perform curricular RL. However, Fingerprint Policy Optimization (Paul et al, 2019) also has a similar idea of modeling the learning process of the student agent. It would be interesting to explain more about how this paper's idea is related and contributes to this line of thought.

- A fully trained algorithm on the BipedalWalker should approach a cumulative reward of 300. Even the modified version used in the ACCEL paper is measured on a scale of 0 out of 300. However, from Figure 3, it appears all baselines perform less than 50 on the BipedalWalker benchmark. It is questionable whether all baselines were fully trained with the right settings. Also, the performance of the proposed algorithm and those of the baselines are statistically too similar to see whether SHED improves over the baselines in Lunar Lander and BipedalWalker benchmarks. Finally, other than the version of ACCEL in this paper not performing as well as the ACCEL in the original paper, I am curious whether ACCEL can be considered state-of-the-art in the benchmarks as written in line 324. Genetic Curriculum (Song et al, 2022) reports higher cumulative reward on the BipedalWalkerHardcore environment.

- Figure 4 has no scale on timestep and reward.

Limitations:
The authors has addressed the limitations of this paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
NhP8MTJzT5;"REVIEW 
Summary:
The paper introduces SyntheOcc, a framework utilizing diffusion models to synthesize photorealistic images for autonomous driving simulations. The proposed method addresses limitations in the existing 2D diffusion model to generate multi-view driving videos by integrating detailed 3D geometric data.
The authors effectively employ 3D semantic multi-plane images (MPIs) for precise geometric control, enhancing the realism and utility of generated images for training perception models. The paper also proposes re-weighting strategies to address the imbalance problem between foreground, background, and object categories. The experiments prove the effectiveness of the proposed MPI encoder and the reweighting strategies.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper introduces an innovative approach by incorporating 3D semantic Multi-Plane Images (MPIs) to capture both geometric and semantic details of a scene. This approach allows for the precise modeling of 3D environments in a 2D image synthesis context, enhancing the photorealism and depth accuracy of the generated images. 
- The design of the MPI encoder is very effective in handling the input conditions with a large number of channels while maintaining spatial consistency to the latent features of diffusion UNet.
- Additionally, SyntheOcc incorporates sophisticated reweighing strategies to address class imbalance and ensure focus on critical features. These include foreground enhancement, depth-aware reweighing, and class-balanced reweighing.
- The paper outlines a comprehensive set of evaluations to demonstrate the effectiveness of the proposed method. Qualitative evaluations visually demonstrate the photorealism and environmental accuracy of the generated images compared to real scenes from the nuScenes dataset. Quantitative analyses leverage metrics such as Frechet Inception Distance (FID) to measure image quality and evaluate perception model performance, offering solid empirical evidence of the framework's effectiveness. Ablation studies further dissect the impact of various components and design choices in the proposed method. Additional robustness tests are conducted to evaluate how changes in the MPI settings (like variations in depth or semantic labeling) affect the output quality and the training effectiveness of perception models.

Weaknesses:
The contributions for reweighing strategies seem to be minor improvements over 
existing methods (Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, and Dit-Yan 
Yeung. Integrating geometric control into text-to-image diffusion models for high-quality 
detection data generation via text prompt. arXiv preprint arXiv:2306.04607, 2023, Benjin Zhu, 
Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and sampling for point cloud 3d object detection (arXiv preprint arXiv:1908.09492), which limits the perceived novelty of the paper's contributions.

The paper focuses on scene editing capabilities, but there is a noticeable underrepresentation of object-level editing in the experiments. 

Magic drive’s data augmentation is evaluated on two perception tasks BEV segmentation and 3D object detection, with CVT (Zhou & Krahenbuhl , 2022) and BEVFusion (Liu et al., 2023a) as perception models, respectively. Hence, evaluations on same downstream tasks are encouraged for better comparisons to the state-of-the-art baseline.

The paper doesn't provide any evaluations comparing the re-weighing solution proposed by GeoDiffusion.

There are also several noticeable view inconsistencies, eg: Fig 14 - row 2 column 2-3 (clouds seem different), row 7 column 4-5 there is a mismatch in building structures, which are not discussed in the paper.

Limitations:
The authors acknowledge some key limitations in the proposed method. First, it relies heavily on existing data for generating scenes, which means it doesn’t create as much variety as it could. This limits how well it can train models to handle different driving conditions. The paper also struggles with complex scenes, like crowds, where it fails to accurately identify individual people. This is a big deal for autonomous driving, where accurate representations of the scene are crucial for predictions. The authors suggest that future improvements could include better methods for creating diverse scenes and making the model more capable of handling dynamic environments, which would help make the system more practical and effective for real-world applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces SytheOcc, a method that employs a diffusion model with 3D occupancy as conditions to generate street view images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Unlike previous methods that use box conditions, this paper proposes the use of 3D occupancy, resulting in finer geometric control ability.
2. The paper further suggests the use of 3D semantic multi-plane images to represent the 3D occupancy.
3. The text and figures are well-presented, and the provided examples are very promising.

Weaknesses:
1. The main concern is the inconsistency between views and frames. Despite using Cross-View and Cross-Frame Attention and 3D occupancy as conditions, the spatial and temporal consistency results are unsatisfactory (e.g., Fig 5 (b) and the video demos). This is not the expected outcome, as incorporating 3D occupancy as a consistent world representation should result in better spatial and temporal consistency. Additionally, it would be preferable to have metric results such as FVD for the temporal experiments.
2. In Table 1, why does SytheOcc-Aug show worse results for certain categories (e.g., bicycle, moto)?
3. Table 1 lacks experiments for ControlNet-Aug or ControlNet+depth-aug.
4. Some discussions regarding 3D occupancy as a 3D geometry condition:
   4.1. The field of view (FOV) for 3D occupancy is limited as it is generally generated using lidar, which leads to inconsistency issues for high-rise buildings when considering cameras of larger FOVs.
   4.2. The current annotation of 3D occupancy has limited category coverage. It would be beneficial to explore open-vocabulary approaches.
   4.3. When using only 2D semantic masks as conditions, the paper mentions the presence of ambiguity (i.e., Fig 6 a0). Can the use of instance-level semantic masks alleviate this problem?

Limitations:
The authors have provided a comprehensive list of limitations and future work of their paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper propose a new 3D semantic multi-plane images (MPIs) based image generation pipeline, which enables finer geometric control for 3D editing, dataset generation, and long-tailed scene generation. Through extensive experiments, the work demonstrates substantial advancement in generation quality and better alignment between condition and synthesized images.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The work explores a new 3D semantic Multi-Plane Images (MPIs) as a condition, which provides better spatial alignment compared with baselines and enables 3D editing.

2. The comparison results are comprehensive and demonstrate the effectiveness the proposed method, the ablation is relatively complete to validate the MPI Encoder and the reweighting strategy.

3. The paper is well-written, and the experimental results are presented clearly.

Weaknesses:
1. The MPI encoder, which is the major contribution, is not novel for me. Although the proposed 3D MPI enables finer control than BEVGen, but the diffusion model also operates on the 2D domain and generates each view and frame separately without strict geometry constraints.

2. The importance of reweighing is tricky and hard to tune, considering many hyperparameters. Are the m and n in Eq. 6 the same for different datasets?

3. It’s hard to decide how the method works without a supplementary video, I doubt the view-consistency of the generated video across frames and views.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors propose a new controllable diffusion-based image generation method named SyntheOcc, which takes an occupancy map as input and generates camera images. SyntheOcc enables the application of scene editing and long-tail corner case generation and shows a strong capability of data augmentation for autonomous driving systems.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Compared with previous controllable image generation methods for traffic scenarios like Panacea or MagicDrive, the occupancy map contains more 3D spatial information than the BEV layout.
2. The paper is well-organized and easy to follow.
3. The extensive experimental results demonstrate the effectiveness of the proposed data generation pipeline.

Weaknesses:
1. The control signal in Panacea or MagicDrive is BEV layout, which only contains lanes and foreground objects and is more easily acquired than occupancy. However, the SyntheOcc relies on sophisticated collected occupancy.

Limitations:
The proposed SyntheOcc faces challenges in real-world application scenarios. For instance, to generate planning-level long-tail corner cases, other methods like Panacea or MagicDrive simply require editing the object's trajectory. However, SyntheOcc demands not only inputting the background occupancy but also constructing a pseudo occupancy for the foreground object. This raises the question of whether using occupancy as a control signal is an advantage or a disadvantage.

The crux of the issue lies in the complexity of this method compared to alternatives. While other approaches need some adjustments to object trajectories, this technique necessitates providing comprehensive background and foreground occupancy data. This additional overhead prompts us to ponder whether occupancy-based control offers tangible benefits or introduces unnecessary complications.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
NMwPKjNTEP;"REVIEW 
Summary:
Unfortunately, the authors begin the manuscript by demonstrating a lack of knowledge about the topic. They claim that deep learning (DL) has been highly successful in the field of brain-computer interfaces (BCI) based on electroencephalogram (EEG) data. However, in reality, the application of deep learning in the BCI or EEG field is limited, and shallow learning with simple hand-engineered features is still the gold standard. Therefore, the paper's claims about the vulnerabilities of machine learning models seem to be more like science fiction and do not meet the standard of the NeurIPS.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Hard to spot any strength as this is an artificial toy example.

Weaknesses:
Lack of connection with real-world problems, especially the BCI and EEG fields, where shallow learning remains gold standards with non-existent vulnerabilities. ML in BCI has been trained for each subject at the bedside.

Limitations:
No application in the real world and a completely trivial problem below conference standards.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents an EEG backdoor for manipulating EEG BCI, called ManiBCI, where the adversary can arbitrarily control the output for any input samples. Experiments conducted on three EEG datasets demonstrate the effectiveness of ManiBCI; which easily bypass existing backdoor defenses.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- A backdoor attack for EEG BCI where the adversary can arbitrarily manipulate which target class the EEG BCI will misclassify without engaging the training stage.
- The use of EEG electrodes and frequencies in EEG backdoor attacks with reinforcement learning.
- Several experiments have been conducted to assess the proposed method.

Weaknesses:
- The proposed methodology is not well described. It mainly based on the application of Fourier transform and reinforcement learning.

Limitations:
Yes. The limitations were addressed in Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents ManiBCI, a novel backdoor attack method targeting EEG-based brain-computer interface (BCI) systems. ManiBCI leverages a three-stage clean label poisoning approach without needing access to the training phase of the target deep learning models. This method optimally selects EEG electrodes and frequency masks for each class using reinforcement learning. The attack involves injecting these learned masks into the EEG data, leading to high misclassification rates while maintaining the original task's accuracy. Extensive experiments on three EEG datasets demonstrate ManiBCI's effectiveness and robustness. The key contributions of this work are: (1) Introducing a new type of stealthy and effective backdoor attack for EEG data. (2) Proposing a method that can manipulate multiple classes simultaneously without requiring control over the model's training process. (3) Providing experimental evidence of the attack's success across various datasets. This research highlights potential vulnerabilities in EEG-based BCI systems, emphasizing the need for robust defense mechanisms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* Introduces a novel and stealthy backdoor attack method for EEG-based BCI systems using frequency transform.
* Demonstrates the ability to manipulate multiple target classes without needing access to the model's training phase.
* Provides strong experimental evidence of the method's effectiveness and robustness across multiple EEG datasets.

Weaknesses:
* Standard baselines (fast gradient sign method and universal adversarial perturbation) are not included for comparison [1][2]
* Limited to the datasets used in the experiments, raising questions about generalizability to other EEG datasets or real-world scenarios.
* The practical implementation of the proposed attack might be complex and computationally intensive due to the need for reinforcement learning optimization.

[1] Xiao Zhang and Dongrui Wu. On the vulnerability of CNN classifiers in EEG-based BCIs. IEEE
Transactions on Neural Systems and Rehabilitation Engineering, 27(5):814–825, 2019.
[2] Zihan Liu, Lubin Meng, Xiao Zhang, Weili Fang, and Dongrui Wu. Universal adversarial
perturbations for CNN classifiers in EEG-based BCIs, 2021.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a backdoor attack strategy for EEG, addressing three inherent issues: low quality, task variances, and morphology variances. The authors introduced a three-stage clean label poisoning attack. The proposed algorithm has been evaluated on three EEG datasets, demonstrating its effectiveness and robustness across datasets. This is an interesting work investigating backdoor attacks on EEG, and the customized strategy shows effectiveness in this particular domain. I believe this contribution will be beneficial to the community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This is a very interesting work, investigating backdoor attacks on EEG, and the customized strategy shows effectiveness in this particular domain.

* The experiments are relatively sufficient and validate the claimed contributions adequately.

Weaknesses:
* I am not the expertise in BA domain. In terms of general EEG analsyis, one of my main concern is the experiment settings. In normal EEG analysis domain, we usually set inter-subejct and intra-subject settings. I failed to see the calrifications of these experiment settings. Whether this strategy can work across subjects, and generalize on the EEG signals collected from new/unseen subject?

Limitations:
The limitations mentioned by the authors are appreciated.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
NC0Bjl4uTf;"REVIEW 
Summary:
The paper presents a novel Chinese inertial generative adversarial network (CI-GAN) designed to generate high-quality training samples for Chinese writing recognition using inertial sensors. The CI-GAN integrates Chinese Glyph Encoding (CGE), Forced Optimal Transport (FOT), and Semantic Relevance Alignment (SRA) to enhance the quality and authenticity of generated inertial signals. The approach addresses the challenge of collecting diverse and extensive training data for Chinese character recognition, showing significant improvements in classifier performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduces innovative methods in the form of CGE, FOT, and SRA, contributing significantly to the field of inertial writing recognition. The release of a new dataset further enriches the community's resources.

Weaknesses:
1.Lack of Detailed Baseline Configuration: The paper compares CI-GAN with a traditional GAN in the appendix, but fails to provide detailed settings for the baseline method. This lack of information hinders the ability to fully understand and replicate the comparative effectiveness reported.
2.Insufficient Comparison with Other Augmentation Techniques: The study does not compare CI-GAN with other data augmentation methods, such as random perturbations. It remains unexplored whether applying random disturbances to the data could also substantially improve classifier performance.

Limitations:
The authors have discussed limitations related to the variability of writing styles and the potential impact of environmental factors on sensor data.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes CI-GAN to acquire unlimited high-quality training samples, alleviating the data scarcity in the inertial signal recognition of Chinese characters. By utilizing these generated data, the performance of recognition models is highly improved.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is easy to follow.
- The proposed methods may help disabled people.

Weaknesses:
- The pipeline lacks novelty. The employed technologies are widely used in CV and NLP, and the proposed pipeline merely reuses them for the inertial signal domain without any innovative design. Furthermore, the author fails to cite relevant studies such as [1][2] and does not discuss their differences.
[1] Wasserstein GAN (WGAN)
[2] Efficient Estimation of Word Representations in Vector Space

- The proposed CGE is simply a learnable embedding to represent Chinese characters, lacking innovative design for glyph information. The author introduces GER to enhance the orthogonality of character embeddings but does not provide an ablation study to verify its effectiveness.

- The author uses Wasserstein distance in GANs. What is the difference between this approach and WGAN [1]? Additionally, the author proposes using FFM to supervise the signal in feature spaces. These measures are also similar to some works, such as perceptual loss using VGG and identity loss using ArcFace, but the author does not cite these and discuss the difference. 

- The dataset used for training and testing is too small, which could not effectively verify the effectiveness of the proposed method.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper address an important probem in human computer interaction: making computers accessible to vision impaired people. The paper address this my collection paired data of text and imu signals. First, the paper address the issues of limited data by training a generative model, to resample/bootstrap more data and then train recognition model on both real and generated data to archive high performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
the paper addresses an important social problem, and accessibility should be focused on all groups. 

The data collected for this paper, the paired data on text and imu is very useful, hope the authors will open-source it. 

paper is well written and the figures are clear and convey the ideas.

Weaknesses:
My main concern is, that it is very unlikely that we get more than we give to the system, the generated samples are a function of real samples. 
I would like to see, a competitive baseline with good data augmentation, and maybe on a low data regime gan generated samples are better than augmentation, but this has to be shown, otherwise, I don't see the value of extra effort to train a generative model to get data augmentation.

Limitations:
I wouldn't say this is a major limitation, but on the scale axis, this problem can be solved by collecting more data. Unlike annotations like explaining an image or video, handwriting signals are more easy to collect on the long term. would be nice if the authors can address this, also please explain the issues with data augmentation.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Mdd3f8Cui8;"REVIEW 
Summary:
This paper proposes a framework to augment latent features from observed features, with the help of LLM. They frame the problem as a text-to-text reasoning problem.  The method can be adapted to different domains easily.  The method is also validated with a real world dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the presentation and logic flow are smooth and clear.  The methodology is also reasonable to me. And their experiments also validate the effectiveness of their method.

Weaknesses:
The key concern for me is that when using the LLM for inference and text generation, I worry about the social bias and fairness of the problem. Some research has shown that LLM is still biased in some sense, can the author conduct some evaluation on whether the latent feature is biased towards some sensitive attributes like race, gender, etc?

The other thing is that I wonder how much human labor effort and expert labor effort will be needed to have the latent features. 

Typo in line 203

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a unique form of LLM data-augmentation that attempts to generate informative latent variables to improve downstream tasks. They do this by transforming the latent feature mining task into a text-to-text propositional reasoning task.
Validation is performed with a case study in the criminal justice system and latent features align well with ground truth labels + significantly enhance downstream classifier performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Clear, well written paper with descriptive diagrams
- Using LLMs to infer latent space in this way seems to be a novel idea

Weaknesses:
- Type on line 203: ""whic serve""
- Potential for LLM biases in the latent variable finding. E.g. Marijuana usage does not necessarily require Substance Abuse Treatment.
- Lack of evaluation on multiple datasets/domains and no publically released code
- Lack of ablations exploring generalizability with x\% features removed

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a framework that uses LLMs to improve predictive modeling by augmenting observed features with inferred latent features. This approach transforms the latent feature mining task into a text-to-text propositional reasoning task, enabling LLMs to infer unobserved yet crucial factors from available data. The framework is tested through a case study in the criminal justice system, demonstrating improved accuracy in scenarios where collected features are weakly correlated with outcomes.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. It addresses the challenge of limited data availability by leveraging LLMs to infer latent features, to improve predictive modeling. 

2. The approach of transforming latent feature mining into a text-to-text propositional reasoning task is interesting. 

3. The validation on criminal justice data, shows potential for broader applications. The method's generalizability across different domains with minimal customization is a significant advantage, and the reduced need for extensive human-annotated training data makes it practical and scalable.

Weaknesses:
1. The paper does not adequately address how to measure the impact of errors introduced by the LLM-based solution on predicted outcomes, nor does it provide uncertainty estimates. This process surely introduces errors, as with any ML-based solution. How do we measure its effect on predicted outcomes, including uncertainty estimates? I suspect more labeled data would be needed to assess this properly (see Egami et al. @ NeurIPS 2023).

2. It is unclear whether the approach should be viewed as a form of dimensionality reduction (based on existing features) or if it extrapolates information not present in the original data. This ambiguity raises concerns about potential bias amplification. For instance, Figure 1 shows deductions made by the model that are not clearly supported by the evidence, suggesting that the method might be amplifying existing biases rather than mitigating them.

3. Connecting to the previous point, the method's ability to learn latent information that is causally predictive of the outcome, as opposed to relying on spurious correlations, remains uncertain. Conducting an out-of-distribution test, where the characteristics of individuals differ from the training data, would be crucial in evaluating the model's generalizability and causal inference capabilities. 

4. The rationale behind not using all available data directly in the LLM for prediction is not well-justified. Directly prompting the LLM with the full data might provide more accurate predictions without the need for dimensionality reduction.

Limitations:
The authors have acknowledged limitations of their work, particularly in addressing the ethical concerns associated with data collection and the need for privacy.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper used large language models to infer latent variables that are important for downstream prediction tasks to augment the existing models. In particular, the author demonstrated the use of the proposal on a criminal justice system use case, in which the LLM-mined-latent features significantly boost the prediction performance.

Overall the paper presents an interesting question and how LLM could help mining for latent features, but I have several questions regarding 1. the generalizability of the proposed method; 2. Appropriate combinations/baseline methods; and 3. The potential ethical implications of this method. I will detail these points in the strength/weakness sections below.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. On the outcome prediction results, using the latent feature seems to have boosted the performance by 7-10%, a big margin.
2. The proposed framework brings a level of formalism to the current crowded LLM for (social) science applications/work, including the text-to-text proposition work.

Weaknesses:
1. Despite the general initial framing in Section 3, it was not very clear how generalizable results from Sections 4-6 are — this includes not only the COT and the prompts used, but more critically the selection of what kind of latent features we are including. 
2. The current work does not seem to go into depth about what kind of latent features are LLM particularly good at constructing and which ones are particularly “bad” (eg subject to the most bias and systematic over-or-under-prediction). I think this is particularly relevant for social science applications where many of the categories and features are more of a “construct” and often qualitative in nature.
3. Have the authors compared the results by using text embeddings of the descriptions as an input feature? It’s interesting that the fine-tuning strategy is necessary for good performance, which seems to suggest that learning the intermediate classification rule is important since direct manipulation of natural language yields less impressive  results.

Limitations:
Mentioned above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
MbbTN9rPzC;"REVIEW 
Summary:
The paper introduces a novel activation function called Quantile Activation (QACT), which aims to improve the robustness of neural networks against various data distortions. The authors propose an end-to-end framework that combines QACT with modified loss functions and quantile classifiers, evaluating their approach on several benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well organized and clearly written.
1. The paper delivers useful empirical and theoretical insights.
1. The experimental results showcase the superiority of the proposed method.

Weaknesses:
1. The proposed method seems a bit complex, which may lead to over-fitting in scenarios with limited training data.
1. Although the experiments are promising, it remains unclear how well the proposed method would scale to larger datasets or more complex tasks beyond those tested in the paper.
1. I feel that the evaluations are somewhat limited as only a few methods are compared against, lacking the most recent SOTAs. This limits the understanding of the real technical contribution of the proposed method.

Limitations:
The limitations should be discussed in the main paper, yet they are provided in the checklist.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Quantile Activation (QACT) to enhance classification model robustness against distributional shifts. Unlike traditional classifiers, QACT outputs the relative quantile of a sample in its context distribution, allowing for context-dependent classification. Validated on datasets like CIFAR10C and MNISTC, QACT improves generalization and robustness, outperforming state-of-the-art models like DINOv2 under large distortions. The paper details QACT's implementation and suggests future research directions, including scaling and exploring theoretical links to biological neurons.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper is notable for its originality in proposing a context-aware activation function, demonstrates high quality through extensive validation, and has significant potential for enhancing generalization in classification models. The innovative use of quantile-based activations opens new opportunities for research and applications in machine learning.

Weaknesses:
- Lack of Clarity on Context Dependency

The concept of context dependency being batch-dependent is not clearly explained until the conclusion of the paper. This crucial detail should be introduced and elaborated on earlier to provide a better understanding of the method.

- Unclear Motivation in Introduction

The motivation and fundamental problem discussed in the introduction are not clearly articulated. The authors mention that, unlike NLP where context is considered, general classification systems do not incorporate context. However, in Vision Transformers (ViTs), image patches are treated similarly to words in NLP ""[...The meaning of a word is dependent on the context of the word. However, to our knowledge, this has not been considered for general classification systems.]"". In ViTs, an image patch is considered like a word, and the context comes from the other image patches.

- Insufficient Related Work on Robustness

The paper lacks a comprehensive review of related work concerning robustness to input distortions. Including a discussion of existing methods and how QACT compares or improves upon them would strengthen the paper.

- Limited Comparative Analysis

The comparison with other methods addressing robustness to input distortions is insufficient. The authors primarily compare QACT with DINOv2-Small, which is not a standard model for robustness. Including comparisons with other state-of-the-art methods specifically designed for robustness would provide a more complete evaluation of QACT's performance.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a new activation function called quantile activation (QACT) which outputs the relative quantile of the sample in the context distribution. Furthermore, the paper validates the proposed activation across several experimental settings, and compare it with conventional techniques. They test robustness against distortions, and find that the proposed activation can achieve a significantly higher generalization across distortions than the conventional classifiers, across different architectures.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
First, the authors develop existing approach in calibrating a pre-trained classifier to the level of a neuron. Thus, suitable forward and backward propagation equations required for learning are derived. Second, the authors also show that the extension can produce context dependent outputs at the level of each neuron of the neural network.

Weaknesses:
The writing of the paper meets the standard, but the notations are confusing. Nevertheless, it would be much better if the authors can polish and clarify them. For instance:
1. in line 107, the authors claim that ‘Assign $\mathbf y=1$ whenever $\mathbf y > (1-\tau)^{th}$ quantile of $\mathbf z$’. It seems that $\mathbf z$ is a vector and is impossible to have a vector be larger than a scalar. 
2. The authors write $z_i$ and $\mathbf z_i$ alternatively to mean the same quantity. Similar situations occur when the authors write $z$ and $\mathbf z$ (see line 119, Eqn. (4)), or QACT$(\textbf z)$ and QACT$(\mathbf z)$ (see lines 119 and 124).
3. The authors use bold lowercase letters to represent vectors (e.g., Eq. (1)) and variable distributions (e.g., lines 105, 106). Also, what is the difference between bold lowercase letters and normal lowercase letters?
Further clarifications can increase the readability of the paper.

Limitations:
Please see the questions and weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
MICrZCQzoN;"REVIEW 
Summary:
The authors establish generalization error bounds for non-iid data based on the online-to-PAC conversion.
In particular, the authors extend the online-to-PAC conversion techinique to non-iid settings utilizing the online learning with delayed feedback. In the paper, the authors illustrate

1. a method of non-iid online-to-PAC conversion,
2. a method of converting online learning algorithms to their delayed counterparts,
3. the resulting non-iid generalization bounds combining 1. and 2., and
4. an extension of 1. to dynamic hypothesis learning.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The well-organized, well-motivated paper on generalization error analysis and online learning.
- The claimed results are novel and help us better understand the generalization in dynamic environments.

Weaknesses:
- Notable logical gap: Lemma 3 only gives a regret bound **independent** of $P^*$, but it seems Corollaries 3 and 4 need $P^*$-dependent regret bound. I believe this is fixable, but still, need some fix.
- More discussion on related work: Are there any results previously not known, but can be proved with the proposed method?

Limitations:
Limitations are not explicitly discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the generalization error of statistical learning in a non-i.i.d setting, where the training data distribution could have temporal dependency. They develop a framework that reduces the generalization error in this case into the regret of an online learning problem with delayed feedback. Then, they present a series of instantiations of their results with different online learning algorithms and assumptions on the data generation process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The propose framework is elegant and wildly applicable to many real-world data generating process.
2. The paper is easy to follow, and the setting is well presented. The introductory section for the reduction in the i.i.d case is very helpful in understanding the context.

Weaknesses:
1. The proposed framework seems to be a straightforward extension of that in the i.i.d setting. Technical novelty of this work seems limited.
2. The instantiation of the framework given in this paper is still very high level and abstract (for example, the algorithm considered is the general follow the leader algorithm). It would be beneficial to have some specific instantiations and show that if the obtained results are comparable to the existing ones, similar to what has been done in Lugosi and Neu (2023).

Limitations:
Nothing necessary stands out.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper extends the Lugosi-Neu(2023) framework for upper bounding the generalization error of statistical learning algorithms to the non-i.i.d. setting, by considering that the training samples are drawn from a suitably mixing stochastic process. They show that the existence of a delayed online learner with bounded regret in the Online-to-Batch game of Lugosi-Neu(2023) against an offline learner implies that the offline learner has low generalization error even when trained on data drawn from a mixing stochastic process. The authors also investigate settings such as FTRL and MWU under this model.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The paper addresses an important question - How to bound generalization error of statistical learning algorithms trained on non-i.i.d. data in a manner which is independent of the complexity of the statistical learner? The paper does a fine job at establishing the notion of such bounds and some conditions under which such bounds are recoverable.

Weaknesses:
The techniques seem to be largely an amalgamation of several papers which have refined the ""blocking technique"" in various settings, and the key observation that the introduction of delay in online games lead to the online cost being a sum of martingale difference sequences, which essentially allows them to use proof techniques of Lugosi-Neu(2003). 

The delayed online learning setting is new to me, and I am not sure how to evaluate its significance versus the standard online learning setting. In fact, it seems like getting similar bounds w.r.t. the standard online setting would involve significant more technical novelty, compared to the current setting. 

The stochastic process also seems to be quite well-behaved in comparison to previous works in the literature such as Mohri and Rostamizadeh (2011), who give generalization bounds (in the pure offline setting) under stochastic processes with weaker notions of convergence.

The authors mention that the results hold for a specific class of bounded loss functions, but I could not find specific details regarding this point afterwards in the paper.

Limitations:
The discussions of limitations in the current work is limited, and lacks discussion as to why certain choices were made (or overlooked).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper focuses on learning from non-i.i.d data. Specifically, the authors develop a framework that derives generalization guarantees through a reduction to an online learning game with delays, where achieving low regret translates to low generalization error. They present specific bounds when using EWA and FTRL as the online learning algorithms. Additionally, the framework is extended to accommodate dynamic hypotheses.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper is well-written and easy to follow. The proposed framework is general, novel, and elegantly designed, facilitating a clear translation between low regret in online learning algorithms and low generalization error in the context of mixing data. I appreciate the simplicity and flexibility of the framework, and overall, it represents a valuable contribution to the field

Weaknesses:
While I did not go over the entire details, I did not find any major weaknesses.  
- A small typo is line 43: ""..we propose propose.. ""

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides a framework for proving generalization bounds for non-i.i.d. data sequences, building upon a recent framework introduced by Lugosi and Neu (2023) that reduces PAC to online learning. This technique recovers some known PAC Bayesian bounds for non-i.i.d. scenarios and various other implications.

The original framework by Lugosi and Neu (2023) introduced an online learning ""generalization game"", where the regret of the online learning algorithm can be translated into a generalization bound in the offline setting. This framework has been shown to recover some important generalization bounds with a clean analysis.

In this paper, the generalization game is extended to a game where the learner gets to see the observation with delay (where there's no delay we are back to the original framework). The regret of the online learner in this game can again be translated into a generalization bound. When the delay is large, it increases the regret of the online learner (one term in the generalization bound) but decreases the term determined by the property of ""how much the sequence is non-i.i.d."".
Online learning with delays has been studied extensively, and so ""off-the-shelf"" algorithms and regret bounds can be used to derive/recover generalization bounds.

One nice application of the technique is that it allows the analysis of stationary mixing processes that have been studied extensively (the assumption on the non-i.i.d. sequences is weaker than known mixing assumptions). Another interesting application is to popular dynamic predictions such as autoregressive models and RNNs.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Deriving generalization bounds for non-i.i.d. settings is a central effort in the machine learning community and is of great interest. The framework suggested in this paper allows us to do so in a very clean way and might be useful for more applications.

Also, the assumption on the non-i.i.d. sequences is quite weak, which is an advantage.

Weaknesses:
The paper heavily builds on the framework of Lugosi and Neu (2023). This is not a weakness, but my question is: besides extending the online game to accommodate delays and using ideas from the online learning literature, what are the technical challenges/contributions in this paper?

Another question - do you know if the paper by Lugosi and Neu (2023) was already published? I'm asking since I didn't go over the proofs in this paper.

Limitations:
Limitations are properly addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
LnJ2EGKTXh;"REVIEW 
Summary:
This paper introduces a framework for generating paired instruction and robot code for further fine-tuning LLMs for robot-specific tasks. A symbolic simulator is used to check the correctness of the generated code and an LLM is prompted with chain-of-thought reasoning to align generated instruction. The resulted dataset was used to fine-tune a robot specific LLM and later tested on benchmark tasks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper tackles two challenges of automated data generation for robot code synthesis, one is to check the correctness of the code by grounding it to logical state of objects and the other is to align the generated the instructions to provided robot capabilities. By designing principled and general modules that tackle each of these problems, RoboInstruct is shown to generate useful data for finetuning general purpose LLM to robot specific code generation applications.

Weaknesses:
RoboSIM can only check for semantically meaningful steps of the code and may not catch lower-level error that requires spatial/geometric reasoning, or even reasoning about physics, including commands that take in numerical parameters e.g. move(0,0.2,0), rotate(0.75). This seem to limit the usefulness of RoboInstruct to certain types of robot APIs.

Limitations:
see weakness

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ROBO-INSTRUCT, a novel framework designed to generate synthetic training data for fine-tuning small language models to create domain-specific robot programs. The framework features two main components: ROBOSIM, an algorithm that validates programs using angelic execution, and INSTALIGN, which aligns instructions with the generated programs.
The key contributions of this work include the development of ROBO-INSTRUCT to enhance the code generation performance of small open-weight language models for domain-specific robot programs. This framework introduces ROBOSIM, which features a dynamic world synthesis and evaluation process for generating relevant world states and performing automated code checks for diverse tasks. Additionally, it includes INSTALIGN, a procedure that refines instruction-code pairs to improve alignment between instructions and the code generated by SELF-INSTRUCT. By fine-tuning the Codellama-Python-7B model using ROBO-INSTRUCT, the model significantly outperforms several other open-source and most proprietary models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper demonstrates strong clarity and organization, making complex concepts accessible to readers. Each section flows logically, and technical details are explained effectively, ensuring that the methodology and findings are easy to follow.
- The paper thoroughly reviews and incorporates current literature.
- The paper meticulously validates all its claims through experimental results and detailed analysis. The effectiveness of ROBO-INSTRUCT, ROBOSIM, and INSTALIGN is demonstrated convincingly through empirical data and comparisons with existing models and benchmarks. This empirical validation ensures that the contributions are not just theoretical but substantiated with practical evidence.

Weaknesses:
- While ROBO-INSTRUCT offers significant advancements for fine-tuning language models in robot programming, there are some weaknesses to consider. Firstly, it heavily relies on SELF-INSTRUCT for generating initial programs, potentially introducing biases from the base model's training data. This could limit the diversity and quality of the generated programs.
- Moreover, while ROBO-INSTRUCT shows promising results on benchmarks like ROBOEVAL, its application to real-world robot programming tasks requires thorough evaluation and validation. Real-world robot environments often present unpredictable challenges that benchmark datasets may not fully capture, necessitating further testing to assess the framework's robustness and generalizability in practical applications.

Limitations:
The limitations of the work are clearly stated

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to improve the performance of small open-sourced LLMs to generate code that can run successfully on a service robot simulator to solve tasks. The idea is to use another small model to generate program data using SELF-INSTRUCT and fine-tune a 7B model for the robotics domain. The authors note that data generated by SELF-INSTRUCT may have good diversity but lack correctness. To this end, they build a simulator RoboSIM that takes the programs generated by SELF-INSTRUCT and verifies the correctness of the execution in addition to syntax errors given a predefined robot API. They further modify the instructions to align with the verified programs better. Overall they show the 7B LLM fine-tuned on this clean data can outperform a GPT3.5-Turbo in the robotics domain.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written and motivated. Figures are easy to understand and helpful in conveying high-level ideas. The idea to build a pseudo-simulator that tracks world states without running the generated programs through an actual simulator is novel. It is amazing that synthetically generated data filtered by simple heuristics such as requiring programs to pass the world state tracking RoboSim is sufficient to help improve the performance of an actual simulator.

Weaknesses:
While the paper is well-written for the scope it sets for itself, I am not sure if the contribution is significant enough. There are many works using LLM to generate data and fine-tune domain-specific models, so the idea behind this paper is not super novel. The performance gain is also limited by the rather heuristic method considering the gap between the best model presented by the paper and GPT-4 it sought out to beat. In fact, given the 17% performance gap, simple baselines could be using GPT-4 to generate the programs and fine-tuning small models or using GPT-4 as the critic to filter programs. These simpler heuristics may yield better results and prove the Robo-Instruct method (which is also rather heuristic) proposed by the paper unnecessary. For reference consider 
[1] Improving Small Language Models on PubMedQA via Generative Data Augmentation
Therefore, I am not sure the contribution of this paper is all that significant.

Limitations:
Building RoboSim to track world states might work only for simple pick-n-place or task-planning problems. How to generalize this heuristic of tracking world states to more complex problems is unclear. Some analysis of the scope (suitable for what kind of problem class) of this approach is needed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces ROBO-INSTRUCT, a novel framework designed to improve the code generation capabilities of smaller open-weight language models (LLMs) for domain-specific robotic tasks. ROBO-INSTRUCT leverages two key components:

1. ROBOSIM with DYNAMICEVAL:  A task-agnostic simulator that dynamically synthesizes a consistent world state based on the robot's actions within the program. This allows ROBOSIM to identify execution errors and validate generated programs even for diverse and complex tasks.
2. INSTALIGN: An instruction-program alignment procedure that utilizes Chain-of-Thought reasoning to refine the generated instructions. This ensures that the instructions better reflect the intent of the generated robot program, improving alignment between the two.

The paper evaluates ROBO-INSTRUCT by fine-tuning a Codellama-Python-7B model and testing its performance on ROBOEVAL, a benchmark for service mobile robots. The results demonstrate that the ROBO-INSTRUCT fine-tuned model significantly outperforms other open-weight models

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novel framework: Introduces ROBO-INSTRUCT, a unique approach to generating training data for fine-tuning smaller LLMs on domain-specific robot tasks.

Dynamic world synthesis: ROBOSIM's ability to dynamically create relevant world states allows it to validate diverse programs generated by SELF-INSTRUCT, overcoming the limitations of traditional simulators.

Instruction-program alignment: INSTALIGN effectively refines instructions to better reflect the program's intent, improving the quality of the training dataset.

Strong empirical results: Demonstrates that ROBO-INSTRUCT significantly improves the performance of small open-weight LLMs, enabling them to surpass even some proprietary LLMs.

Cost-effective and private:  Provides a potential alternative to deploying proprietary LLMs for local robot deployment, offering cost-effectiveness and privacy benefits.

Weaknesses:
Limited novelty: The idea of using a sim/emulator to verify the generated program has already been explored in previous works such as Chain-of-code, which is not mentioned by this work. 

Limited scope: The paper focuses on a specific domain (service mobile robots), and it is unclear how well ROBO-INSTRUCT generalizes to other robot domains.

Lack of real-world evaluation:  The paper only evaluates ROBO-INSTRUCT on a synthetic benchmark. Real-world deployment and testing are required to further assess its practicality.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
LlcygqLdwO;"REVIEW 
Summary:
Saliency methods are among the most popular approaches to explaining an existing black-box image classifier. However, they are limited to localizing class objects in an image. In addition, since they rely on per-pixel importance, they are unable to generalize accross multiple instance to provide a global explanation of the image classifier. To address these limitation, an existing work, Testing with Concept Activation Vectors (TCAV), provides global explanations via learning concept vectors by learning from a set of example images with a known concept. However, TCAV can only provide global explanations, limiting them from providing location information of where the concept is located in the image. Inspired to solve this problem, the authors present Visual-TCAV, an approach that provides both local and global explanations. The authors realize this by learning a Pooled-CAV per concept based on the feature maps of a chosen layer in the network and combining this with the integrated gradients (IG) of the same features maps for a given instance image. The resulting saliency provides a localization of the concept in the instance. To achieve global explanations, they analyze the aggregation of the concept activations across images for a particular class. The authors provide analysis on layer selection, local explanations, and global explanations across several popular CNN-based model pretrained on ImageNet. In addition, they conduct a validation experiment to verify the effectiveness of their method where the ground-truth concept is known.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The methods is able to add localization to the existing TCAV approach increasing is ability to explain black-box classifier CNNs.

The authors address that their approach only considers positive activations in the features and discusses the usefulness of accounting for negatively activated features in the future.

The presentation of the paper is clean and easy to follow. The methods are made simple to understand and are effective. Figure 1 was particularly effective at communicating their method.

The experimentation and analysis was decently extensive. They analyze the effect of choosing shallow, middle, and deep layers for their approach providing interesting findings on where certain concepts are activated. 

The validation experiment shows the faithfulness of their approach's ability to find the targeted concept in a set of example images.

Qualitative results show strong localization ability of their method to identify queried concepts.

Their approach is relatively fast to run for local and global explanations.

Weaknesses:
The authors show the activation of different types of concepts at different layers. It has been shown that certain levels of layers have been associated with different types of concepts such textures, shapes, objects, etc in [1]. This work should be referenced and discussed compared to their findings on activations at different layers.

While the paper analyzes across common CNN models, they do not analyze on ViTs or models trained on other datasets other than ImageNet.

The authors utilize generative models to create certain images containing a concept, but do discuss why this was necessary. It is an interesting avenue, but I'm unsure of it's necessity in this work if no further analysis was done on generative images in particular.


[1] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. “Feature Visualization”.
In: Distill. 2017.

Limitations:
The paper doesn't provide generalization of their method to visual transformer methods which are also becoming popular in explainability and interpretability work.

While this approach is effective at identifying concepts, it requires a manually selected set of example images with a predefined concept. While they mention that generative approaches could help with reducing the number of required examples, the choosing of the correct concepts is still a limitation. This is particularly a problem in more specialized domains such as medical diagnosis where concepts may not be known or are more difficult to explain / generate.

In addition, one has to ablate through several layers for each concept to find which one effectively captures the concept.

The paper only analyzes models pretrained on ImageNet. To convince the generalizability of this approach, analysis on a model trained on other datasets would be necessary.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents a method for combining TCAV with saliency maps to illustrate where feature-related concepts (e.g., “stripes” or “grass”) are activated in an image. The evaluation is largely qualitative, but the method seems to work well on ImageNet classifcation tasks. The method is also validated on a controlled dataset with known ground-truth features, where it performs as expected.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ The method is straightforward and seems to work well.
+ The method is evaluated with a modified dataset where the ground truth importance of concepts is known, or at least well controlled. I haven’t seen this done in many papers in this field and it’s a very nice addition to the work.
+ The paper is well-written and enjoyable to read. The investigation of the proposed method is quite thorough.

Weaknesses:
- The method is not particularly novel – there are other methods which localize CAVs in an image to provide local explanations. I think both of the automatic concept-extraction methods (ACE and ICE) described in Section 2 do this; the recent method CAVLI (https://ieeexplore.ieee.org/document/10208704) also does this.
- This paper does not provide any comparisons to other methods.
- Like other approaches based on TCAV, the method requires the user to identify the concepts of interest and curate training datasets to visually represent each concept. This limits the usefulness of non-automatic TCAV methods for general-purpose model explanations.
- Like most other saliency map approaches, the explanations only focus on how concepts that are present in an image contribute to the decision. However, model decisions may also depend on the absence of features. This approach does not have a way to represent feature-absence in the explanation, which can lead to confusing explanations in some cases, as shown in the validation experiment.

Limitations:
Not applicable

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel technique, Visual TCAV, which unifies concept-based explainability with saliency maps. Visual TCAV produces local explanations in the form of saliency maps, which highlight the pixels in the image that represent a given user-defined concept. The visualization is enriched with an attribution score which represents the importance of a concept in the prediction of a given class. Visual TCAV can also produce global explanations by aggregating attribution scores of multiple images belonging to the same predicted class.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is overall well written and experiments are discussed in detail.
The technique addresses the limitations of TCAV and overcomes them by providing both a visual and quantitative analysis of a concept’s influence in a prediction. It also enriches the ability of TCAV of providing global explanations, allowing it to measure the influence of a concept and not only the sensitivity of a model to it. 
I find the visualization step to be particularly crucial for a fair analysis of convolutional neural networks and for bias detection.

Weaknesses:
Minor remarks: The use of the notation was inconsistent throughout the paper and some of the figures lacked clarity. In particular : 

•	The index referring to the feature maps in the considered layer, k, is not explicitly mentioned in the text. It is understandable from Figure 1 that k refers to the index of such feature maps, but it should be written explicitly for better clarity, as it is frequently used in the formulas.

•	In line 145 the raw concept map is indicated with M_{raw}^c but in equation 1 the notation changes and becomes M^{c, raw}

•	In equation 3, the indices i,j are further introduced when denoting M_{ij}^c. I believe this refers to a pixel-wise notation, with i,j indexing the pixels in the rows and columns of the image, but it is not mentioned explicitly. The non uniformity in the notation is somewhat disturbing.

•	The sentence in line 168-169 seems to imply that additivity holds, while later on in the paragraph it is specified that the measure is concept-wise. I would suggest a rephrasing.

•	The normalized logits presented in line 184-186 could be better expressed with a formula, to avoid misunderstandings on their derivation. Moreover, in line 188, are the normalized attributions the same as the normalised logits? If yes, use a consistent terminology.

•	The normalised attributions are indexed by an index t. It is defined only in line 193 that t represents the target class. It should be clarified before its first appearance. 

•	In equation 4 it appears for the first time the notation p_k^{c, norm}. Is it used to represented the pooled-CAV (p^c) rescaled to [0,1]?

•	There is no legend in Figure 2 to investigate the portrayed degrees of activations. In figure 2d the focus seems to be on the parashoot other than the sky. A legend would help interpret the figures more clearly.

•	I suggest the use of a colour-blind friendly palette for Figure 4 (for instance, figure 5 and 6 are better suited). 

•	Figure 6 caption mentions a statistical significance test: how was the test conducted? Is it the same test described in the TCAV paper?

Limitations:
I find that the main limitation of the method is mainly related to the use of concept-based explainability. It may be possible to craft concepts that incorporate social biases or that may be misleading. Clearly, this is a broader issue, not related directly to this paper, but for which there may be an interesting ground for discussion.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
JkqrZqBO7d;"REVIEW 
Summary:
In pipelined model training, one important issue is to reduce the bubble sizes. 
One stream of work is to use the staleness, where the weight discrepancy is mitigated using stashed weights.
This work tries to reduce the overhead of storing weights with reversible architectures. 
Using the non-stashed updated weights, but with restored inputs to each stage, 
approximated gradients are obtained and parallel training is performed.
This leads to less memory usage on training at the cost of increased communication. 
Training results on resnet variants seem to maintain accuracy.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This is a nice adaptation of reversible architectures to pipelined training. If it works well, there is a potential for becoming a new popular pipelined training method.

- The idea of using reconstructed input instead of stored weights seem to be novel.

Weaknesses:
- Insufficient experiment size: Only compared on three different sizes of resnet. This is far from sufficient, especially with the largest model being resnet50. 

- No comparison on speedup: speedup on the training time is crucial, but the ""memory benefits and training time"" section does not disclose any data. Since the proposed scheme has larger communication, it is crucial to report the number.

- Classification accuracy drop: The final accuracy drops on all three datasets for resnet50. 0.6%p and 0.7%p are huge drops for those models. Given that this is the largest model among the tested ones, it draws a significant concern on whether this technique would work for larger models such as resnet152 or ViTs.

- There is no analysis or proof on why the proposed scheme would work. Why it is a good approximation, or why it is going to converge, etc.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method that combines reversible neural networks and parallel distributed training to enable learning with minimal memory usage, while incurring only slight communication and computation overhead. In this approach, the need for storing intermediate activations in traditional backpropagation is eliminated, thus reducing memory constraints and allowing for higher parallelism on the same device. This new method facilitates efficient learning by providing an innovative solution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The problem setup involving reversible architecture and distributed parallel training is intriguing. High memory consumption is a critical issue in learning, and reversible architecture has been proposed to address this problem. It is anticipated that these advantages can be similarly applied to distributed parallel training. Additionally, the paper is very well-written, making the ideas easy to understand. The figures and tables were also judged to be of high quality and well-prepared.

Weaknesses:
The main drawback of this paper is the insufficient experimentation. Although using reversible architecture in distributed training is a novel concept, it appears to be merely a combination of existing ideas. For this paper to have a significant impact, it must demonstrate the advantages and benefits of the proposed idea in an actual distributed learning environment. However, the experiments were conducted using only a single A100 GPU, and there is no demonstration of the performance improvements or limitations of the proposed idea in a real distributed environment. The values presented in the tables do not clearly differentiate from what can be achieved with existing reversible architectures. To improve the completeness of this paper, it is essential to analyze scenarios that necessitate the use of multiple GPUs, such as video applications, large-resolution diffusion, and large language models. The current data fails to effectively explain the benefits of the proposed idea.

Limitations:
Not relevant.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the author proposes a new alternative algorithm (Parallel End-to-End Training with Reversible Architectures) for regular backpropagation, which significantly enhance the parallelization with a limited overhead compared to regular backpropagation and other alternatives to end-to-end training.
Specifically, the network is split into several stages (one layer or a set of layers) distributed across distinct devices, one batch data is split into several mini-batch data. The first device sequentially accesses the mini-batch data and pass them forward to the next stage until the final stage is reached. The backpropagation is initialized from the final stage to the first stage. It enables a significant parallelization of forward and backward computations across multiple devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This paper is well-organized and easy to follow.

* The background information is very rich and makes it easy for someone who is not familiar with this field to understand the relevant techniques including the technique proposed by this paper.

* The figures about the core technique proposed by authors are very clear, which can help readers understand the technique at a glance.

* The paper evaluates the proposed techniques on multiple datasets and networks.

Weaknesses:
* From the comparison between the proposed method and other techniques from related work, it showcases that the proposed method does not have an overall crushing lead. There exists the method which can achieve higher speed and less time than proposed method with storage increased. 

* The low or even zero storage on proposed method is mainly due to reversible architectures. Maybe authors can extend proposed parallel training method to some non-reversible architectures (need memory storage for intermediate activations), then compare with other SOTA methods.

* It would be great if authors use more distributed devices to get more stages from a network, in this case, the performance of the proposed method is likely to be deeply explored. Because the proposed technique is aimed to deployed on the distributed devices.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose fusing delayed gradient pipeline parallelism with reversible models in order to capture the benefits of the former while mitigating the drawbacks with the latter.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper sets up a pretty compelling combination of ideas. This is a great example of a paper that clearly understands the strengths and weaknesses of two disparate techniques and fits them together like puzzle pieces.

- The paper is clear and methodical in laying out the motivation for the approach. By the time the method is introduced, its seems like the natural and obvious choice. This is good writing.

- The concept is solid. I really *want* to like this idea, since it seems to fit together so well.

Weaknesses:
- While the idea is presented fairly clearly, a lot of the analysis is estimates (S4.2) and generalizations (Tab 1). It's fine for motivating the idea, but not really good enough for proving it works as projected. I'm left wondering how much of this method will actually translate to a scaled-up implementation. (No question that it *was* implemented, but a pipeline-parallel model that doesn't actually pipeline across devices is...not particularly compelling.)

- The paper is a fusion of two ideas, designed to capture the computational performance benefits of pipeline parallelism while using reversible models to mitigate memory scaling. Some estimated results of memory footprint are presented in Table 3. No measured results are presented related to parallelism (timing, utilization, etc.). From this paper, it is not possible to determine whether it has succeeded. This is confused further by the section 4.2: ""Memory benefits and training time"" which does not discuss training time at all. The lack of computational results is fairly damning.

Limitations:
As described in weaknesses. Limitations, like computational performance details, are not well described.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
JWF1dN4TOd;"REVIEW 
Summary:
This paper studies to how to use deep learning to solve large-scale contextual market equilibrium. This paper proposes MarketFCNet, a deep learning method for approximating market equilibrium. The paper propose an unbiased training loss and a metric called Nash Gap to quantify the gap between the learned allocation and the market equilibrium. Experiments on a synthetic game validates its effectiveness.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
Originality: The paper propose a deep learning method to solve large-scale market equilibrium, which represents buyers and goods, and directly outputs the allocation. The application is novel.
Quality: The paper theoretically derives the loss function, and does some experimental analysis to validates the effectiveness of the propose method.
Clarity: The paper clearly defines the contextual market modeling problem.
Significance: Experiments validates that MarketFCNet are competitive with EG and achieve a much lower running time compared with traditional methods.

Weaknesses:
Quality: The paper does not prove the convergence of the training algorithm. The paper either does not show the training curve. The paper does not provide the implementation code of the algorithm. 
Clarity: The paper is hard to follow. It is quite to hard to understand the meaning of each proposition. 
Significance: The paper aims to solve the large scale contextual market equilibrium, and proposes a novel deep learning method  to approximate the equilibrium efficiently. However, the importance of the large scale contextual market equilibrium is not clear. I do not know how to apply the proposed method in real life.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the computation of market equilibrium where there are a large number of buyers and the buyers and goods are represented by their contexts. It proposes a deep-learning method, termed MarketFCNet, to approximate the market equilibrium. The method outputs the good allocation by taking in the context embedding. It is trained on unbiased estimator of the objective function of EG-convex program using ALMM and is evaluated using a metric called Nash Gap. The method is validated by experimental results.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written and easy to understand. The motivation of the paper seems natural. The paper fills the gap of using deep learning for large scale market equilibrium computation, which can be promising for future study.

Weaknesses:
1. The proof of the unbiasedness of $\Delta \lambda_j$ and Lagrangian estimators in Sec 4.2 seems to be a bit hand-wavy. For example, should $b_i$’s be independent of each other? For a fixed $i$, is $b’_i$ an independent copy of $b_i$? It would be great if the authors could provide a formal (and more detailed) proof of the unbiasedness.

2. What is the effect of $k$ on the method performance? For example, if the dimension $k$ is very large, would the method fail to comprehend the context?

3. How to determine the architecture of allocation network? For example, can one use a Transformer or CNN as the allocation network?

Minor issues:

Line 164: It would be better to define $U(B)$ when introducing uniformly sampling to latter use.

Some equations are missing “.” or “,” at the end. Please fix those.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The submission is not in your area and extends beyond my current expertise (from theory and applications to specific tasks and methods).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The submission is not in your area and extends beyond my current expertise (from theory and applications to specific tasks and methods).

Weaknesses:
The submission is not in your area and extends beyond my current expertise (from theory and applications to specific tasks and methods).

Limitations:
The submission is not in your area and extends beyond my current expertise (from theory and applications to specific tasks and methods).

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a deep learning-based method called MarketFCNet to efficiently compute market equilibrium in large-scale contextual markets, where buyers and goods are represented by their contexts. The key idea is to parameterize the allocation of each good to each buyer using a neural network, and optimize the network parameters through an unbiased estimation of the objective function. This approach significantly reduces the computation complexity compared to traditional optimization methods, making it suitable for markets with millions of buyers. Experimental results demonstrate that MarketFCNet delivers competitive performance and much faster running times as the market scale expands, highlighting the potential of deep learning for approximating large-scale contextual market equilibrium.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The deep learning-based approach, MarketFCNet, can efficiently approximate the market equilibrium in large-scale contextual markets by parameterizing the allocation using a neural network. This significantly reduces the computation complexity compared to traditional methods.

The ability to handle large-scale markets with millions of buyers makes this approach highly relevant for real-world scenarios, such as job markets, online shopping platforms, and ad auctions with budget constraints.

The paper introduces a new metric called Nash Gap to quantify the deviation of the computed allocation and price pair from the true market equilibrium, providing a meaningful way to evaluate the approximated solutions.

Weaknesses:
The deep learning-based approach is inherently less interpretable compared to traditional optimization methods. Exploring ways to improve the interpretability of the learned allocation function, such as incorporating domain-specific constraints or incorporating interpretable components, could enhance the practical usability of the method.

The paper does not discuss potential overfitting issues that may arise when training the MarketFCNet model, especially in settings with a large number of parameters. Incorporating appropriate regularization techniques and cross-validation strategies could help mitigate overfitting and improve the generalization performance.

The paper assumes that the contexts of buyers and goods are homogeneous and can be directly used as inputs to the neural network. Extending the approach to handle heterogeneous context representations, potentially by incorporating feature engineering or meta-learning techniques, could increase the applicability to more diverse market scenarios.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
J8m0DEjxEO;"REVIEW 
Summary:
The main claim of this paper is that adversarial suffixes against large language models (LLMs) function by distracting the model from the original harmful goal to the suffix itself. The authors then propose a modification to GCG attack by incorporating a regularization term that increases the attention score on the adversarial suffix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
### 1. Originality and significance

The main claim of the paper is an interesting hypothesis that aims to unfold the inner workings of adversarial attacks on LLMs. This type of question can lead to a nice interpretability tool and/or a potential mitigation. Hence, the significance of this research question is clear. 

While some existing works start to look into “features” or neurons that correspond to these jailbreak attacks, the attention weights have not been deeply studied to the extent of my knowledge so it could be a nice complementary explanation.

### 2. Experiment coverage

The experiments on the attacks are relatively thorough. The authors compare their method against three existing SOTA attacks (GCG, AutoDAN, and ICA) on various open-source models. The transfer attack experiments in Section 3.4 also cover a broad range of closed-source models. The evaluation metrics are also comprehensive, including both the keyword matching and the GPT-4 evaluation.

Weaknesses:
### 1. Attention score measurement and interpretation

I first notice that in Figure 2, the attention scores on all parts (system, goal, suffix) can all go up as the optimization progresses and that the attention scores can be larger than 1 in Table 2 and 3. This suggests that the attention scores are before softmax and hence, not normalized to sum to 1. Please feel free to correct me if I’m mistaken.

1. If this is the case, it makes the score much more difficult to interpret and compare across different attacks. The absolute unnormalized value of the attention scores does not mean much because, for example, even if the score increases for the suffix portion, it may gets smaller relative to the other portions (system or goal). This is major flaw that undermines the main conclusion of the paper.
2. If that authors have not already done so, I would like to ask that all the reported attention scores be normalized (after softmax). The autoregressive generation also contributes to the attention scores, i.e., attention score of the target token $x_{n+2}$ also includes the target token $x_{t+1}$ along with all the prompt tokens $x_{1:n}$. I’m not sure what is the best way to normalize their effect. One way is to simply leave them out of the softmax, but there could be an interesting trend that we fail to capture this way. Another way is to report *difference* between average unnormalized attention score on the goal vs on the suffix portions. This also gives us a relative score but ignores the system portion.
3. In Figure 2 (left), ASR also increases along with the attention score on the goal, contradicting the main claim of the paper that higher attention score on the suffix is better.
4. It is unclear to me how Figure 5 supports the main claim of the paper. The attention pattern on ""Vanilla"" is strikingly similar to that of  ""ICA"" on the goal segment. Based on the color bar, the ICA attention score also seems higher than the Vanilla which contradicts the claim that the attack ""diverts the model’s attention away from the goal towards themselves.”

### 2. Section 3.3: Generalize AttnGCG to other attack methods

1. The purpose of this experiment is unclear to me. If the authors wish to prove their claim that higher attention weight on the suffix leads to a better attack, there should be a better controlled experiments than running GCG or AttnGCG on prompts generated by the other methods. This experiment entangles the initialization method with the attention score.
2. It might be interesting to see AttnGCG with varying values of $w_t$ and $w_a$.
3. I’d suggest an experiment where the attention loss is incorporated into AutoDAN (or other attacks) optimization objective. This would better emphasize the transferability and the usefulness of the attention loss across multiple attack algorithms.

### 3. Limited empirical improvement

While the main idea could help improve interpretability to these adversarial attacks, the attack that is inspired by this observation, AttnGCG, does not lead to significant improvement in the attack success rate, especially in the transfer setting. In the white-box setting, the improvement seems consistent across models, but the small margin suggests that attention score is not the most important factor that determines the success of the attack.

That said, it is sufficiently convincing to me that AttnGCG performs better than GCG and may replace it for evaluating the safety of LLMs.

Limitations:
Limitations and negative societal impact have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a new adversarial attack strategy on LLMs which improves over existing adversarial attacks. For this the authors propose a new regularizer that maximizes the weight of attention corresponding to suffix tokens, which naturally results in minimizing the weight for the other tokens present in the input prompt. Using this additional regularizer with GCG results in improved attack success rate. The authors also show that this attack is transferable to other attack methods like ICA and AutoDAN.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The paper is well motivated and the proposed loss follows well with the reader’s intuition.
2) The results are promising and the gains over the existing GCG attack are significant.
3) The comparison is comprehensive, involving different models.

Weaknesses:
1) It is not clear how the transferability of the same suffix tokens is for different goal prompts. This is important to investigate because GCG shows that the generated attacks are universal and can transfer on different goal prompts. I am currently a bit skeptical that the transferability on using the proposed attack might be limited because the generated suffix tokens might be more specialized for the given goal prompt. This is expected because now the generation of the suffix tokens is largely conditioned on the target target tokens due to the proposed regularizer. 

2) I believe it might be possible that using the proposed attack the model ends up outputting something potentially harmful but completely unrelated with the input prompt. This might be a possibility because the proposed approach inherently minimizes the attention on the goal tokens, which means the context of the input might become less relevant. It would be great if the authors could share some analysis on transferability of adv prompts and also share the generated text for GCG and AttnGCG.

3) It is not clear why maximizing the attention weights for suffix tokens should always lead to a stronger attack? This is also evident from tables 2 and 3 where AutoDAN has a lower value of goal attention score but stil leads to weaker attack as compared to GCG (see Table-4). Thus the argument presented in 162-163 seems questionable. 
In general, it is not clear why authors did not attempt to analyze the defenses like the ones proposed in [1]. Particularly, I believe it is important to analyze if the proposed attacks are able to bypass detection filters based on perplexity [1]. 

[1] Jain, Neel et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” ArXiv abs/2309.00614 (2023)

Limitations:
Yes, the authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a refined GCG method named AttnGCG for Large Language Model jailbreaking attacks. They focus on the attention scores of the input components, refining the loss function by adding an Attention Loss term. The attack success rates are greatly improved. Various experiments are provided to support the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1)	An interesting finding is that as the attention score on the adversarial suffix increases, the effectiveness in safeguarding LLM diminishes.

(2)	The experiments are conducted on various LLMs to prove the effectiveness of the AttnGCG.

Weaknesses:
(1)	It is unclear whether the increased success cases correspond to the 'regret' cases observed in GCG. The authors proposed AttnGCG to address the issue where the model successfully generates target tokens but then rejects the request; however, the results remain ambiguous.

(2)	In the success case illustrated in Figure 4, the attention scores at the boundary between the goal and the suffix are significantly higher than in other regions. Is this a common phenomenon in success cases? If so, why does this occur?

(3)	In Appendix A.3, the table shows that the system prompt for Llama-2 and Llama-3 is set to None, which is different from most jailbreaking papers, including the original GCG. How does this influence the attacking success rate? The authors should also report the success rate under the standard system prompt.

I will reconsider my score if all these problems are adequately addressed.

Limitations:
The authors adequately discussed the limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new jailbreak attack method against LLMs, called AttnGCG. The method integrates a loss of maximizing the attention scores of the adversarial suffix. The paper provides experimental results to show the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to follow.

Weaknesses:
My main concerns are as follows.

- Will increasing the attention scores of adversarial suffixes make the responses focus on the content in adversarial suffixes?  
- The discussion in lines 151-164 is weak. Specifically, in Figure 4, AttnGCG explicitly increases the attention scores of adversarial suffixes, so it is natural to have higher adversarial suffix attention scores. It is not convincing to say ""uncover the underlying reasons for successful attacks within the model’s attention mechanism"".  
- In Table 3, AutoDAN achieves 0.227 goal attention score, while the scores of GCG and AttnGCG are 0.8657 and 0.793. Does the observation mean that AutoDAN is better than AttnGCG?
- Some content seems to be redundant, e.g., Figure 1 and Algorithm 1.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
IfpNsorodK;"REVIEW 
Summary:
This work proposes a training-free time step-skipping method that can be used with existing ODE solvers for reduced NFE. The method was motivated by two observations: 1) a significant similarity in the model's outputs at time step size during the denoising process and 2) a high resemblance between the denoising process and SGD. The proposed method employed gradient replacement from past time steps and rapidly updated intermediate states inspired by Nesterov momentum. The proposed method yielded promising results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- Experimental results look promising with multiple diffusion models on diverse datasets.
- Accelerating diffusion models for sampling is an important issue and this work tried to address it.

Weaknesses:
- There have been a lot of prior works on accelerating diffusion models for sampling. While this manuscript cited many, it still missed important prior works - some of them look quite similar to the proposed method. Thus, the novelty of the proposed method is unclear in the current form of this manuscript. For example, using Nesterov acceleration for fast diffusion models is not really new (e.g., R Li et al., Hessian-Free High-Resolution Nesterov Acceleration For Sampling, ICML 2022). Eq (15) of this work can be seen as a special case of the following prior works such as [R1], DeepCache [28] (using past), [R3] (using three moments or future) or [R2] (using all). Some recent work like [R4] even used partial caching instead of using the whole results. A more theoretically grounded work on using Nesterov momentum for sampling can be found in [R5]. 
[R1] M Xia et al., Towards More Accurate Diffusion Model Acceleration with A Timestep Tuner, CVPR 2024.
[R2] A Pokle et al., Deep Equilibrium Approaches to Diffusion Models, NeurIPS 2022.
[R3] H Guo et al., Gaussian Mixture Solvers for Diffusion Models, NeurIPS 2023.
[R4] F Wimbauer et al., Cache Me if You Can: Accelerating Diffusion Models through Block Caching, CVPR 2023.
[R5] R Li et al., Hessian-Free High-Resolution Nesterov Acceleration For Sampling, ICML 2022.
- A number of acceleration works for diffusion models also investigated the feasibility of the parallel computation. Will the proposed method be parallelized for computation? 
- It is unclear if the proposed method was compared with other methods in terms of computation. Will 1 NFE of the proposed method take the same computation time as 1 NFE of other methods since the proposed method contains multiple evaluations of the neural network as in Eq. (15).
- The notation and explanation are quite confusing, so it is not easy to understand the whole idea as well as the algorithm itself.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new training-free acceleration method for the inference of diffusion probabilistic models. The key components of the presented time-skipping strategy are the use of past and future gradients to eliminate redundant neural function evaluations (NFE). The proposed method is shown effective compared to other training-free acceleration methods, leading to solid performance improvements especially for ODE solvers with less than 10 NFEs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
*    The method is training-free and can complement existing fast ODE solvers
*    The paper is well structured and puts the presented method in the proper context with respect to existing methods
*    The experimental results cover conditional and unconditional settings, showing performance improvements across the board.

Weaknesses:
*    The performance gap compared to training-based methods is still apparent, especially considering the latest distillation techniques resulting in one-step models.
*    The mathematical notations are a bit hard to follow up. I would advise the authors to add a schematic clarifying for a given setting of hyperparameters, which timepoints are being evaluated and which are being skipped.
*    The optimal setting of hyperparameters *k,l* is model/dataset dependent and it is not clear apriori how to set these. Therefore, this requires empirical experimentation which makes it time-consuming to get optimal performance when using the method out-of-the-box.

Limitations:
The authors were upfront about the limitations of their method.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
To accelerate the sampling speed in diffusion models, this paper proposes a training-free denoising method, dubbed PFDiff. 
Concretely, PFDiff employs the gradient from past time steps to update intermediate states, aiming to reduce unnecessary NFEs while correcting for discretization errors.
In this manner, PFDiff enables to improve classic samplers without any training computation.
Importantly, experimental results demonstrate the effectiveness of the proposed PFDiff.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. Reducing discretization errors in diffusion models in a training-free manner is attractive and practical.

2. The motivation of using previous gradients to guide the current sampling direction is intuitively plausible, and the proposed method is technically sound.

3. The presentation is excellent and the figures all are readable.

Weaknesses:
1. In my humble opinion, the theoretical analysis part is naive. Can you provide more explanation about why previous gradients is helpful to guide current sampling direction? Since different noise levels correspond to different gradients, is there any harm in denoising images with the proposed method?

2. Many works investigate using previous gradients to improve sampling speed, so the contribution is limited.

Limitations:
Please see in Weaknesses and Questions. If all of my concerns are addressed, I will improve my score.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes PFDiff, a fast training-free sampler for diffusion models. PFDiff updates the current state with both the past score network evaluation and the future score network evaluation. It can achieve good sample quality with less than 10 NFE. The authors showcase the effectiveness of PFDiff on various pre-trained diffusion models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. With proper tuning, the proposed PFDiff can outperform existing ODE solvers in the low-NFE regime on various datasets. 
2. The authors provide comprehensive technical details about the proposed algorithm.

Weaknesses:
1. Flawed justification for future gradient: The authors' claim that using future gradient information is better than using current gradient information is based on the mean value theorem (lines 164-167 and Appendix B.2). However, this theorem only guarantees the existence of an optimal point within an interval, not its specific location. Therefore, the mean value theorem itself doesn't justify the preference for future gradients. 
2. Missing justification for the approximation: While the authors claim that their approximation is better, there is no theoretical justification for it. The proof in Appendix B.2 assumes that the optimal point is already known, which is not informative. The manuscript will benefit from a further approximation error analysis. 
3. Expensive and case-specific tuning: the proposed method essentially defines a set of candidate points and searches for the optimal point by tuning parameters $k$ and $l$. This tuning process can be computationally expensive and needs to be done for each specific case, limiting its practicality.

Limitations:
The algorithm performance depends heavily on parameters $k$ and $l$ as shown in Table 7.  Optimal values for $k$ and $l$ vary based on the pre-trained model and the number of function evaluations. This necessitates extensive parameter tuning when applying the proposed method in practice.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes PFDiff, a training-free approach for accelerating diffusion models. Motivated by the high similarity of the diffusion network outputs at adjacent timesteps on the sampling trajectory, PFDiff utilizes past and future information for sampling with time-skipping, 
and decreases the number of function evaluations (NFEs) significantly. Experiments on various settings show significant acceleration, especially in the low NFE regime.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- The method is training-free and can be plug into existing solvers.
- The motivation and overall method seem reasonable.
- The improvements is significant especially in the low NFE regime. State-of-the-art diffusion solvers like UniPC and DPM-Solver-v3 are compared.
- The finding that first-order solver (DDIM), along with PFDiff, can outperform high-order solvers, is intriguing.

Weaknesses:
- The highly concise writing and complex notations might be a bit confusing. Additional illustrations for certain local algorithm procedures can be helpful for understanding the overall idea.
- There are fundamental mistakes in the writing. Eqn. (8) (9) are represented as Euler discretizations of the original PF-ODE. However, both DDIM and the series of DPM-Solvers rely on exponential integrators to transform the PF-ODE into other forms, so that the linear term $x_t$ is cancelled. Though this does not mean the method is wrong, such simplified writing can be misleading. The authors are obligated to correct this, or I will be forced to reject this paper.
- It will be more convincing to include experiments on EDM, the SOTA diffusion model on CIFAR-10 and ImageNet 64x64.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
IdEaeCbhUW;"REVIEW 
Summary:
This paper addresses the problem that commonly in RL, small reaction times and high action frequencies are required, which is not the case for computations in the brain. As a more accurate model, the authors propose an RL method that learns an internal model to improve performance in high-latency applications.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper rightfully describes the issue that, usually, RL methods operate under different conditions than the human brain (higher latencies/lack of information). Developing a biologically plausible model that can deal with higher latencies and lower perceptual frequencies would be beneficial to get a better understanding of the human brain and for realizing certain applications in robotics. The approach of learning an internal model guiding the actions, if no observations are present, seems to follow intuition and could be promising. I would therefore rate the problem as relevant and interesting, and the general model (at least on a computational level - this could be stated more clearly in the paper) as ""plausible"".

The problem and approach (besides my points in the Weaknesses/Questions sections) are clearly described. The method is evaluated on a wide range of six control problems, showing that the model can produce reasonable control signals.

Weaknesses:
I found a few parts of the paper difficult to follow (mainly the policy and evaluation section, see also my questions for this), and the paper could benefit from improvements in these parts. In particular, I am uncertain whether the approach introduced in the paper fits the problem description. It seems that the approach targets the setting of high action frequency but not delays of perception.

Furthermore, I had problems understanding how the policy of the proposed approach works. The authors seem to define a probabilistic policy ($\pi_\omega$) defined by parameters $\omega$ with the past previous action as input (line 177). In Eq. 5, which describes the loss for learning the policy, the previous action does not seem to appear. For this loss, they need state-action pairs from a deterministic policy $\pi_\psi$. For a final evaluation, I would need clarification on this (see questions).

Also, the experiment section was not easy to follow. I believe it would be valuable to give an overview at the beginning of the section about how the evaluation is structured and state the goals of each conducted experiment (linked to the motivation of the paper). As an example, I think (please correct me if I am mistaken) that the comparison to SAC was conducted to show that the proposed method can learn a controller that, even with the limitation of a lower frequency, does not compromise much performance. I am a bit unclear as to why the proposed method outperforms SAC (what it does in 4/6 tasks), as to my understanding the strength of the method should be in the specific scenario where hardware is more similar to the human brain. That the proposed method achieves higher performance even in this standard scenario suggests to me rather a lack of appropriate parameter tuning. Also, the setting of the ASL subsection and Online planning section should be stated more clearly (see questions).

In lines 104-105, it is confusing to me to imply that the proposed method in comparison to model-based RL has the advantage that a model is not needed after training. The main purpose of applying model-based RL is to learn a model with the option to replan after training. Model-free RL which could be more similar for this application is not mentioned.

Section 3.2. is about macro actions but it does not even provide in a single sentence an overview of what it is. The authors claim that an advantage of their approach is that it uses the principles of RL. Based on my understanding the concepts of hierarchical reinforcement learning and movement primitives would be very relevant here but are not discussed.


## Minor:
- Line 17 ""not""
- Line 163 $\psi$ should be subscript
- Line 202 Typo in ""Experiemental""
- I find it confusing that parentheses are used for both equations and citations. In most papers, therefore, for citations square brackets are used.
- The plots do not use well the space in the paper (large whitespaces between subplots, the graphs could be made wider), and font sizes between plots differ significantly. Legends are usually integrated into the first subplot only but could be put, e.g., next to the plots to make this information more obvious.
- The Readme of the provided code seems to be incomplete. The code cannot be directly used to reproduce the figures of the paper.

Limitations:
I think so.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the Hindsight-Sequence-Planner (HSP), a reinforcement learning (RL) model inspired by the brain's ability to achieve precise control using slow neurons. The model aims to mimic human-like sensory and reaction times by leveraging an environmental model for sequence learning. HSP demonstrates competitive performance with fewer observations and actor calls compared to faster RL models. The model is evaluated on various continuous control tasks, showing robust performance even with longer action sequences.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The concept of mimicking brain-like conditions in RL models is innovative and offers a fresh perspective on sequence learning. The integration of temporal recall and sequence learning inspired by neural mechanisms is novel.
2. The use of a temporal recall mechanism allows for fine-tuned action sequence learning despite operating on slower hardware.
3. HSP demonstrates competitive performance across various continuous control tasks, showcasing its robustness and adaptability.
4. The experiments cover a range of continuous control tasks and provide comparisons with state-of-the-art models like Soft Actor-Critic (SAC), highlighting HSP’s efficiency.

Weaknesses:
1. The paper lacks specific numerical performance comparisons to quantify improvements over baseline models.
2. Sections like ""Learning the Model"" and ""Learning Critic"" need further elaboration to highlight their specific contributions and novelty.
3. The broader implications and potential real-world applications of HSP are not fully discussed.
4. There is insufficient discussion on how HSP handles situations with highly inaccurate model predictions.
5. The related work section lacks details on ""Macro-Actions,"" the scalability issues of current methods, and the meaning of ""principles.""

Limitations:
1. The authors mention the reliance on an inaccurate model but could provide more in-depth analysis on how this affects different types of tasks?
2. The scalability of HSP to very large action spaces and high-dimensional state spaces is not thoroughly discussed.
3. Typo Issue: In Section Abstract: demonstrating that it not can achieve comparable performance at ‘human-like’ frequencies by relying on significantly fewer observations and actor calls. -->demonstrating that it can achieve comparable performance at ‘human-like’ frequencies by relying on significantly fewer observations and actor calls.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed HSP, a bio-mimic framework for learning-based control. Motivated by human brains, HSP can deal with observation and computation in different frequencies by making the ""actor"" produce action sequences, similar to the functioning pattern of ganglia and the prefrontal cortex in human brains. HSP employs a model-based training approach to achieve model-free control, resulting in precise behavior despite running on slow hardware. The authors demonstrate the performance of HSP on various continuous control tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well structured with clear figures, and the motivation and introduction are interesting, bridging the gap between biological and artificial reinforcement learning systems. The empirical results across various continuous control tasks are impressive as well. Overall, the work presents a promising direction for developing more efficient and adaptable RL algorithms that could have broad implications for robotics and other real-world applications.

Weaknesses:
* One major concern is the performance of the proposed framework. The experiment results do not show a significantly better performance than traditional RL methods, and the performances are worse when $J=16$ in most cases. The latent space variant of HSP shows improvements only in one environment (Walker2d). Further investigation into why this approach doesn't generalize well to other environments would be valuable.
* The novelty of the proposed framework is also questionable. In control literature, especially trajectory optimization control, producing action sequences is commonly used, and a similar technique could be viewed as a variant of model predictive control (MPC). It would be better if the authors could provide a detailed comparison with existing control algorithms (like MPC) in the experiment section. 
* The comparison with model-based online planning is somewhat limited. A more comprehensive comparison with state-of-the-art model-based RL methods would provide better context for HSP's contributions.
* Just a small suggestion - The title of the paper mentions ""hardware,"" so I expect to see some real-world control experiments in the paper. It would be nice if the author could really demonstrate the control performance using ""slow hardware"" like Raspberry Pi or even slower platform.

Limitations:
Limitations are discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
I3kIEjoON6;"REVIEW 
Summary:
This paper presents a framework for crystal structure generation, focusing on polymorphs. The framework utilizes matrix representation of crystals and various generative models used in vision tasks, with specially designed similarity metrics and loss function. It is tested on (1) modification of given structures and (2) generation from scratch within the dataset, as well as finding new structures in the Ta–W–B system.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work develops domain-specific representation, metric, and loss, so that generative models that have proven useful in image generation can apply to crystals. The topic is timely and important.

Weaknesses:
- In the demonstrated use cases, the generation is conditioned on elements, space group, etc., but not materials properties of interest. These show limited usefulness in materials discovery and design.
- The matrix representation does not take physical constraints into account, e.g., space group determines symmetries in the lattice parameters. Besides, related previous works, e.g., [UniMat](https://openreview.net/forum?id=wm4WlHoXpC), should be discussed.
- The clarity and rigorousness need to be improved (see Questions). The mathematical notations are not unified, e.g., $x$ vs $X$.

Limitations:
Discussed in the Conclusion.
Besides, Sec. 9 contains a GitHub link that could break anonymity.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors studied the use of diffusion and flow matching approaches for the generation of crystalline materials. The authors trained UNet models on polymorphs in the AFLOW database (which has a series of DFT-computed properties for these materials) using either simple R3 regression, diffusion/flow matching. The authors then presented inference results on similarity to training structures (Section 3.4, which shows these methods can reproduce training structures to different extent), and showed that a subset of the modified generated crystals (with Ta, W, B) can have a small non-zero formation energy.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
*Originality*: The authors attempted to study the problem of crystal structure generation with no invariances/equivariances other than periodic translation invariance. 
*Quality*: The authors attempted to use DFT to validate some inference results.
*Significance*: Crystal structure generation (especially synthesizable ones) is an important problem. It seems that training from uniform noise distribution works better for CFM than training on Gaussian noise, contrary to the established results in the field.

Weaknesses:
*Originality*. The manuscript lacks originality. The diffusion/flow matching techniques are well-established in inorganic crystal structures (e.g. CDVAE cited here, DiffCSP/FlowMM that's not here). Sure, using a network architecture not designed for materials/crystals and using no invariances/equivalences is new, but it deviates from standard practices in the field without sufficient justification. I believe the implementations shown in the paper are a great exercise for practitioners interested in the field, but unfortunately, I do not see it as a NeurIPS paper.

*Quality*. The manuscript is _very_ bare-boned, making a comprehensive technical critique challenging without appearing disproportionately critical. 
- On the ML side, there are numerous large fallacies/mistakes (e.g. no consideration of bonds between atoms at all, Sec. 3.3 there is no description of the PBC loss, the generation does not consider the unit cell, no generation with atom types, and there is no investigation of any experiments observed e.g. why is uniform noise better for CFM, the result in Table 1 appears to evaluate overfitting rather than novel generation, the list can go on). 
- On the chemistry/validation side, there are again numerous problems (why would formation energy be given during the generation process, what functional did you use in DFT, there are no comparisons against existing structures and hence cannot be claimed as novel, etc.) 
- There is no comparison against _any_ known methodologies. 
- The results overall, are very weak both in ML and in chemistry (e.g. Table 3 shows most if not all materials generated have extremely large positive formation energies despite the simple elemental composition; the remaining few negative ones are at the brink of instability, in any case they likely would not be synthesizable).  

*Clarity*. The manuscript suffers from poor presentation, starting with a promotional-style title that lacks scientific descriptiveness. I unfortunately do not understand the novelty of the paper in comparison to existing methods. The paper consistently fails to provide essential explanations across both machine learning and chemical methodologies. 
- On the ML side, there are numerous things poorly presented (e.g. Figure 1 is just periodic translation invariance and in a typical manuscript would be summarized in one sentence). 
- On the chemistry side, things are greatly exaggerated (e.g. computationally making a few materials with negative formation energy can be done by undergraduate students and certainly does not warrant descriptions such as 'This significant outcome underscores the remarkable potential of our framework in uncovering thermodynamically stable materials)

Limitations:
Partially.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the inverse problem of generating crystal structures based on given properties, thereby avoiding the need for extensive computational resources typically required in traditional methods. The authors utilized the AFLOW materials database, selecting unstable and stable series of structures for two specific tasks: modifying structures to achieve stability and conditional structure generation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors experimented with various generative model approaches and evaluated two tasks in crystal generation. Additionally, they integrated the VASP software for application testing and successfully identified four previously undiscovered stable structures through conditional generation.

Weaknesses:
1. From the perspective of model application, although the authors used the AFLOW database for their study, they did not compare the data range and coverage with other significant databases like the Materials Project. This omission leaves a gap in understanding how the generative models perform across different datasets and whether the results are consistent and generalizable. Comparing the performance of the same generative models on different databases could provide valuable insights into the robustness and applicability of their approach.

2. There is a partial break of anonymity in the GitHub link on Page 9 in this paper.

Limitations:
The authors proposed two major directions: conditional generation and conditional modification. There is room for improvement in both the experimental results and the data used for conditional modification. For example, they could consider recognizing unit cells with translational and rotational transformations and introducing more ways to assess generation results.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper deals with an important application of generative models for science: generation of crystalline structures. However, I have some serious concerns. First, scope. While comparing different methods for the same objective is informative, I am not so sure what is the purpose here. Having so many different methods certainly dilutes the main message in a short paper format like neurips. Second, approach. From what I understand, there are questionable design problems with the technical approach. Third, results. The authors spend most of their space explaining various methods such that there is little room left for explaning the impact of their results, or comparing their results with existing approaches. Finally, presentation. Figure 1 is kind of trivial or at least very simple and I am not sure it is worth a separate figure. The overall typesetting looks not too professional.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The methodology selection is broad and hopefully the audience can benefit from a mini-benchmark of different generative approaches. The overall model architecture is distinctive from what I have seen in the literature.

Weaknesses:
I have a good number of questions on the technical approaches. More specifically, the model takes a specially formulated data structure that does not seem to be obviously invariant or equivariant under permutation, translation, rotation, which is concerning. For example, change the selection or ordering of the unit cell vectors and everything will change in an uncontrolled way.

The conditioning approach seems to be to provide desired properties as inputs to the generative approaches. I am not sure this always make sense. For example, if one desires a certain space group, there is no enforcing compliance with the space group. One can always easily check it. It is perhaps more suitable to enforce space group compliance using a guidance-based conditioning approach.

Limitations:
There is insufficient discussion about the limitations given my concerns shown above.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
Hc2ZwCYgmB;"REVIEW 
Summary:
This work presents a zero-shot face-generation method based on diffusion models. The proposed method first extracts face features using Face2Vec and trains a network to map these features into the textual space (i.e., the prompt embedding space for diffusion’s text condition). The main difference from existing zero-shot face generation approaches is that, instead of adding conditions in the denoising UNet, the presented method incorporates the condition into the textual embedding. To prevent the subject information from overwhelming the generation (e.g., preserving only the subject ID while ignoring other descriptions), the authors propose a Composition Distillation Loss. This contrastive loss encourages the model to generate descriptions other than the subject information.

The main shortcoming of the paper lies in the experimental validation. There are multiple components proposed, but their effectiveness is not adequately demonstrated. Additionally, both qualitative and quantitative evidence fail to show that the proposed method outperforms SoTA methods.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
* The Compositional Distillation Loss is an interesting and intuitive approach to reducing the problem of subject information overwhelming the generation

Weaknesses:
* There is a lack of intuition behind using multi-timestep distillation. This approach can cause accumulated errors, and there is no evidence provided to support the benefits of adopting such a strategy.
* The section on dynamic model expansion is very unclear. It is not specified which part of the model is expanded or if the tokens are simply replicated. While adding Gaussian noise to replicated tokens is mentioned, there is no empirical validation of its effectiveness.
* The qualitative comparison, especially in Figure 7, does not support the claim that the proposed method has advantages over baselines like PuLID. Additionally, the benchmarking results in Table 1 show limited improvement in facial identity preservation, and the text alignment can be inferior to PuLID. Thus, the experiments are not comprehensive enough to convincingly demonstrate that the proposed approach is superior to existing methods like PuLID.

Limitations:
The authors have discussed the limitations.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes AdaFace, a face encoder that maps facial features from the image space to the text space through the AdaFace Prompt Inverter, utilizing the structure and pre-trained weights of the CLIP text encoder for initialization. During the face distillation phase, AdaFace employs random Gaussian face embeddings and multi-timestep distillation, enhancing the model's ability to capture subtle facial details through dynamic model expansion. In the composition distillation phase, AdaFace uses a comparative learning loss, aligning feature increments with orthogonal subtraction, while introducing an elastic face preservation loss to address the misalignment of facial features caused by different prompts.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written and easy to follow
- The proposed method requires fewer training resources
- The approach to constructing contrastive pairs during the composition distillation stage sounds reasonable

Weaknesses:
- I tried the demo provided by the authors, and the ID similarity on a few test images was relatively low; it should be far from the state-of-the-art (SOTA) level of ID similarity claimed in the paper.
- The test dataset consists of celebrities, which does not guarantee whether these IDs have appeared in the training set. Furthermore, the number of test samples is too small to be convincing.
- The upper bound of ID fidelity is constrained by the frozen Face2Image model, in this paper, Arc2Face.
- The proposed improvements like Random Gaussian Face Embeddings, Orthogonal Subtraction, etc. are not effectively validated through ablation study.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes AdaFace, a method for personalizing text-to-image diffusion models for human faces. At its core, it learns a prompt inverter that maps face embeddings from a pretrained face encoder to the text embedding space of diffusion prompts. It leverages various components including face distillation, composition distillation and elastic face preserving loss to preserve subject identity while attaining good compositionality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper designs targeted training losses and regularizations for the task at hand. The explanation of the methods is detailed. The video qualitative results show improvement over ID-Animator.

Weaknesses:
*  The quantitative metrics do not show a clear advantage of AdaFace over other existing personalization methods like PuLID. The number of qualitative examples for comparing with those methods is also limited -- just the 5 images per method in Figure 7, and not sufficient to clearly demonstrate that AdaFace outperforms existing methods. It would be helpful to show a larger number of uncurated examples comparing AdaFace and baselines to get a better comparison of their performance.
* The training of the prompt inverter involves a number of components -- such as model expansion, the inclusion of different feature types in composition distillation, orthogonal subtraction and elastic face preserving loss -- but there are no ablation studies on most of them to demonstrate their effects on the performance.

Limitations:
The authors have discussed limitations and societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes AdaFace, a test-time-tuning-free method for personalized text-to-face-image generation. Previous methods involving face features in the feature space of a face encoder, which is not flexibly composable with natural language for personalized generation. Thus, this paper proposes to map the face features into the features in the text conditioning space. Several techniques are proposed to enhance the performance, like Random Gaussian Face Embeddings, Multi-Timestep Distillation, Dynamic Model Expansion, Composition Distillation, and Elastic Face Preserving Loss. Some experiments demonstrate that the proposed method achieves good visual results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The visual results are satisfactory in general.
2. The proposed method can also be applied for personalized text-to-video generation.

Weaknesses:
1. The overall motivation is not novel enough. Finding ways to convert input images into the textual space is a fundamental goal in text-to-image personalization, which has been emphasized in the very first TextualInversion work. Even in the context of tuning-free based methods, the proposed framework is not so novel compared with ELITE [a], which also involves training a mapper from the image space into the textual space. Similar compositional distillation technique has also been explored in SuTI [b].
2. Lack of detailed studies of the proposed components, either qualitatively or quantitively. The authors propose a bag of techniques to improve the performance. Although their motivation is mentioned in the texts, there is no supportive results to illustrate how these techniques work. 
    * There is only one quantitive study in Tab. 1 regarding the compositional distillation. However, there are actually a lot of technical details in the proposed compositional distillation techniques, like the orthogonal subtraction and compositional delta loss, which lack careful experimental analysis against their alternatives. 
    * The proposed face distillation is not well supported. Can we simply train the face encoder with the simple noise prediction loss of diffusion models?
    * The analysis of the proposed Elastic Face Preserving Loss is also missing.
3. ELITE [a] mentions that using multiple token to represent an image may hurt the textual compatibility. It is necessary for the authors to provide a rationale for doing so.
4. How about the method comparing with the popular IP-Adapter (face version) [c]?
5. The overall training pipeline requires multiple stages of training, which is not so elegant.
6. The authors would like to consider merging multiple figures with similar functionalities and structures to one, like Figs. 2, 3, 4 and 5, to leave enough space for necessary experimental results.

[a] ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation, Wei et al., ICCV 2023.

[b] Subject-driven Text-to-Image Generation via Apprenticeship Learning, Chen et al., NeurIPS 2023.

[c] IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models, Ye et al..

Limitations:
The authors have discussed the limitations of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
HYwfZEhyK4;"REVIEW 
Summary:
Proposed a graph based learnable multi-agent framework. The framework consists of multiple stages : Forwarding: Election (K: Answer agents; R: Reviewer) -> Review -> K Discuss till a final conclusion is reached. Proposed a mechanism to learn the graph connections dynamically. 

The major Contributions Introduced in the paper: (A)  A new swarm intelligence geo-local framework smileGeo; (B) Dynamic learning strategy; (C) A new Geo-dataset (test mainly).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The major strengths of the proposed smileGeo frameworks are: 

 (a) the learnable Graph based communication strategy seems works well empirically. In table 2, authors demonstrated that it helps achieve better acc, but lower average token costs. 

(b) The proposed method is also scalable as shown in table 3. 

(c) Used attention-based GNN to predict optimal connections and optimal election. Also empirically justified the effectiveness of attention based GNN.

(d) Also constructed Simple rules of updating edges(connections) that works well in practice.

Weaknesses:
The major weaknesses are as follows:

(a) Comparisons with baselines seems unfair. 

(b) Missing details of the evaluation setup, metrics, etc.

Limitations:
yes, the authors adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This works proposes a new visual geo-localization framework with multiple LVLM (Large Vision Language Model) agents. The agents communicate with each other to estimate the geo-location of the input image. A dynamic learning strategy is proposed to optimize the communication patterns among agents to improve efficiency. The method is evaluated on the proposed GeoGlobe dataset.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
+ The idea of tacking worldwide city-level geo-localization with multiple LVLM agents is very interesting. 
+ The result is surprisingly good with zero-shot setting, which is even better than powerful close-source models.
+ Detailed comparison with other agent-based methods is provided. The ablation study on the number of agents is also very detailed.
+ The writing is easy to follow.

Weaknesses:
- The authors could make the geo-localization setting more clear in the introduction, for example, the paper focuses on worldwide city-level geo-localization. There are lots of different settings for geo-localization problem and this could be confusing for some researchers.
- This paper provides a comparison with three traditional geo-localization methods, i.e., NetVLAD, GeM, and CosPlace. However, these three methods are either retrieval-based landmark matching methods or fine-grained classification-based place recognition methods. It would be better to provide a direct comparison with worldwide geo-localization method on city-level setting, e.g., [A]. Although I believe LVLM-based method is better at this setting, a comparison can make it more convincing.

[A] Pramanick, Shraman, et al. ""Where in the world is this image? transformer-based geo-localization in the wild."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.
- There are only two qualitative results in the appendix. Given that the accuracy is over 60%, it should be easy to find successful and failed cases to demonstrate the actual output cases of the proposed methods. It can also better illustrate how multiple agents help the geo-localization process.
- There are also some existing worldwide geo-localization datasets that could be used for more comprehensive evaluation, e.g., IM2GPS3K, YFCC4K.

Limitations:
The authors mentioned the limitations in the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces smileGeo, a novel framework for visual geo-localization, which involves identifying the geographic location of an image. The authors argue that while Large Vision-Language Models (LVLMs) show promise in this area, their individual performance is limited. SmileGeo leverages the concept of ""swarm intelligence"" by enabling multiple LVLMs to collaborate and refine their location predictions through a multi-stage review process. To enhance efficiency, the framework incorporates a dynamic learning strategy that optimizes the selection of LVLMs for each image. Furthermore, the paper introduces ""GeoGlobe,"" a new dataset designed to evaluate visual geo-localization models in open-world scenarios where many images depict locations not seen during training. Experimental results demonstrate that smileGeo outperforms existing single LVLMs and image retrieval methods, highlighting the effectiveness of collaborative learning for visual geo-localization.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* The idea of using an ensemble of networks/agents for geolocalization is interesting and novel. The authors propose a graph-based social network to enable collaboration between the agents.
* The ability to search the internet and provide the agents with relevant information is interesting and improves the performance on the task of geolocalization.
* The paper proposes GeoGlobe, a new dataset for benchmarking models on the task of geo-localizing landmarks. The dataset could be utilized in future for other learning based geospatial tasks.

Weaknesses:
* The paper only seems to tackle the problem of geolocalizing **landmark images**. While this is a challenging problem, the current literature [1, 2, 3] has already tried to address the problem of geolocalizing arbitrary ground-level images. The latter problem requires learning sophisticated geographic and visual features. I think even searching the internet cannot effectively solve the geolocalization problem for non-landmark images.
* Limited applicability: The framework is built entirely upon the capabilities of different LVLMs (e.g. GPT4, LLaVA, etc). It seems the framework cannot generalize beyond the training data used for training LLMs.
* The work fails to address the practical applications and real-life use cases of the framework. Why do we require such a framework?
* The limitation and failure cases are not adequately mentioned in the paper.

[1] Vivanco Cepeda, Vicente, Gaurav Kumar Nayak, and Mubarak Shah. ""Geoclip: Clip-inspired alignment between locations and images for effective worldwide geo-localization."" Advances in Neural Information Processing Systems 36 (2023).

[2] Haas, Lukas, Michal Skreta, Silas Alberti, and Chelsea Finn. ""Pigeon: Predicting image geolocations."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12893-12902. 2024.

[3] Berton, Gabriele, Carlo Masone, and Barbara Caputo. ""Rethinking visual geo-localization for large-scale applications."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4878-4888. 2022.

Limitations:
Limitations are insufficiently addressed in the paper. The future works mentioned in the conclusion are vague and fail to specify specific future directions for the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
GxpkSJjbQR;"REVIEW 
Summary:
This work introduces a novel approach to diagnosing Alzheimer's Disease using a decentralized expert system. This system leverages blockchain technology and Federated Learning to enhance data privacy and manage large volumes of MRI data effectively. The key innovation lies in integrating these technologies to address the challenges of traditional diagnostic methods, which often suffer from delays and inaccuracies, especially in the early stages of the disease.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. This work presents a pioneering integration of blockchain technology and Federated Learning to enhance Alzheimer's Disease (AD) diagnostics, addressing privacy concerns and data management challenges.
2. The proposed decentralized expert system architecture, which includes anomaly detection for patient-submitted data, showcases a comprehensive approach to AD diagnostics, emphasizing AI-driven MRI analysis.

Weaknesses:
1. While the system shows promising results, the article does not provide extensive comparative data against traditional centralized systems or other decentralized approaches, which could validate its superiority more robustly. This work lacks of comparative performance data.
2. The complexity of the blockchain and Federated Learning components might pose usability challenges for less technically adept users, potentially affecting the system's adoption.
3. There are no more details of the algorithms this work used, maybe give out more meaningful algorithm design for the specific model you are using.

Limitations:
1. The accuracy of the AI model heavily depends on the quality and consistency of input data, which might vary significantly across different healthcare settings.
2. The use of advanced technologies such as blockchain might limit the accessibility of the system for users not familiar with such technology, potentially restricting its applicability.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assume that applying blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training can improve diagnostic performance. However, the manuscript lacks technical details and experimental evidence. All descriptions are conceptual, making the manuscript a proposal rather than a technical paper.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It is interesting to apply blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training.

Weaknesses:
There are no technical details and no experiments. Details can be found in Questions and Limitations.

Limitations:
1.	No technical details and experiments.
2.	The literature review of Alzheimer’s Disease diagnosis is not complete, especially regarding AI-based approaches.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a decentralized expert system designed to predict early-stage Alzheimer's Disease using AI-driven MRI analysis. The system leverages blockchain technology and Federated Learning to ensure data privacy and security while performing anomaly detection on patient-submitted data. The architecture includes a Web3 application for patients to upload biological information and MRI images securely. The decentralized approach aims to improve early detection and intervention for Alzheimer's Disease, providing a more comprehensive representation of AD patterns and enhancing model performance through data diversity.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper encapsulates a few novel ideas. They can be summarized as follows:

1. Handling the security and sensitivity of patient medical information is of paramount importance. The authors were motivated by a very relevant problem and presented an approach to blockchain technology with stated aim of providing robust data privacy and security. By building on decades on research on this topic, this approach has the potential to be extended in future with the general updates in this domain.
2. While there are some confusion around their use case (see weakness below), the authors leveraging Federated Learning and a decentralized system to mitigate the challenges associated with model training on centralized data repositories, such as data bottlenecks and privacy concerns
3. The system aims to provide early-stage prediction of Alzheimer's Disease, which is crucial for timely intervention and improved patient outcomes.

Weaknesses:
However given the commendable motivations there are several challenges with the current paper,

1. First and perhaps the most important aspect is that the paper fails to present the real-world challenges associated with the adoption of such decentralized approaches, especially as it pertains to patients engaging with blockchain wallets and data submission interfaces. Also, the primary use-case for the decentralized approach is not evident - is model training the prime use-case or is the main use case patients being able to generate inferences on their own medical records. Overall, the usage scenario around the setup needs to be better motivated and established
2. The paper also lacks formalism around the presentation. For example, if the primary contribution is the architecture around the decentralized AI approach, the design principles needs to be better justified and articulated. A system architecture diagrams needs to be established as well. Similarly, the ""proof"" around the decentralized approach is not a rigorous mathematical proof. Rather the logic is derived from a hypothesis that more diverse data should lead to a better model. This is a hypothesis at the best and needs to be experimentally validates
3. Finally, the paper is lacking in experimental validation. For example, the proof needs to be backed by real world experiments. Also, this is not the first paper to posit a federated learning approach to medical AI prediction. Some of the SOTA methods in this space needs to be compared against

Limitations:
Please see the weakness above

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces an innovative decentralized expert system designed for early prediction of Alzheimer's Disease (AD), leveraging blockchain technology and Federated Learning. Traditional diagnostic methods often result in delays and imprecision, particularly in early-stage AD detection, while centralized data repositories face challenges in managing vast volumes of MRI data and maintaining patient privacy. The proposed system addresses these issues by combining blockchain for secure, decentralized data management and Federated Learning for collaborative AI model training across multiple institutions. The system includes robust anomaly detection mechanisms to ensure data quality and integrity, enabling precise early-stage AD predictions. This comprehensive approach aims to revolutionize disease diagnostics by enhancing data privacy, security, and collaborative efforts in the medical community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a novel integration of blockchain technology and Federated Learning for early AD prediction, which is innovative in addressing data privacy, security, and collaborative AI model training.
- The proposed system is well-conceived, with a detailed architecture and implementation strategy. The inclusion of anomaly detection mechanisms to ensure data quality adds robustness to the system.
- The approach has significant potential to improve early-stage AD detection, which is crucial for timely intervention and better patient outcomes. The decentralized nature of the system promotes data privacy and security, addressing major concerns in medical data management.

Weaknesses:
- The integration of blockchain and Federated Learning introduces significant computational complexity and potential delays due to off-chain processing and communication overhead.
- The system's scalability is a concern as the volume of data and the number of users increase, necessitating ongoing optimization to ensure efficient performance.

Limitations:
The authors have addressed several limitations, including data quality and consistency, computational complexity, and model generalizability. However, further discussion may be needed on:
- Ensuring that the AI model is unbiased and fair across different demographic groups can be difficult, especially if the training data is not representative.
- Real-time processing and predictions might be challenging due to the decentralized nature and the need for off-chain processing.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
GP30inajOt;"REVIEW 
Summary:
The authors propose a retraction-free Riemannian optimization scheme on Stiefel and oblique manifolds to perform parameter-efficient fine-tuning (PEFT) in LoRA style. The proposed approach exploits the theory of landing flows on Stiefel manifolds. Theoretical results demonstrating convergence of this iterative scheme are presented, and complemented by numerical experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method combines the advantages of Riemannian methods while avoiding the computational burden of retracting on the Stiefel manifold. The application in the context of parameter-efficient fine-tuning represents a novelty, and enhances the relevance of the numerical results. The work is well-presented, covering both algorithmic aspects and the experimental section effectively.

Weaknesses:
All the presented theory is developed on a function defined on $St(d,r)$, while LoRA fine-tuning gives rise to an objective function $f(B,A) = L(BA)$, where $B \in St(d,r)$ (or $Ob(d,r)$), and $A \in \mathbb{R}^{r \times m}$. This objective function has to be minimized over $St(d,r) \times \mathbb{R}^{r \times m}$, and the advantages of optimizing on a compact manifold are thus lost.

Unfortunately, this makes the presented theoretical results not directly useful for the practical case under consideration. 
To give a more precise statement, for example in Lemma 3, the constant $\widehat{L}$ would depend on $A$. By the mean value theorem, we would get a bound of the kind

$$
||grad_B f(A,B_1) - grad_B f(A,B_2)|| \leq C(A) ||B_1 - B_2||
$$
 (as noted in equation after line 183 for the Euclidean gradient).

 Since the space in which $A$ resides is not compact, one would need at least a uniform control on $||A_k||$ over the iterations to make the theory interesting for LoRA fine-tuning. 
It is interesting to note, however, that the authors observe exact numerical convergence to the constraint in all cases.

Limitations:
As noted in the ""weaknesses"" section, I believe there is a delicate point that is not addressed in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Retraction-free optimization algorithms on the Steifel manifold have been proposed in [1,18,19,41] etc. The motivation is that if the cost of the objective function/gradient evaluation is significantly larger than the evaluation of a retraction, then the retraction-free optimization algorithms show their advantages and efficiency. In particular, for the landing algorithm proposed in [1], the choice of the parameter in the penalty is important and may not be easy to choose. This paper gives an analysis that shows if the parameter is 1/3, the initial point is sufficiently close to the Stiefel manifold, and the step size is chosen sufficiently small, then the algorithm converges linearly to a stationary point. This result gives a concrete value of the parameter. Such a result is further merged into optimization on low-rank matrices and Manifold-LoRA is proposed. Numerical experiments show that the proposed method outperforms the baseline algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper gives a concrete value of $\mu$ and a gap between x_0 and \bar{x}_0 such that the algorithm converges under reasonable assumptions. Numerical experiments show that the proposed method is more effective than the existing approach.

Weaknesses:
(1) Though the value of mu and an upper bound of \|x_0 - \bar{x}_0\| are given concretely, the choice of step size is unknown. Theoretically, the step size needs to be sufficiently small (See Theorem 1). Any theoretical suggestion for the choice of the step size?
(2) Numerical experiments report results of the comparisons. However, the definition of ``result'' is not given. Is the result computational time or classification accuracy or a notion of correctness or something else?
(3) Problem (12) does not remove all the ambiguity. Note that if B \in St(d, r), then B A = B O O^T A = \tilde{B} \tilde{A}, where O is an orthonormal matrix and \tilde{B} = B O is still in St(d, r). Likewise for B \in Ob(d, r). Is it possible to completely remove the ambiguity by considering the quotient manifold? 
(4) Why is the numerical performance of Manifold-LoRA for using Stiefel and Oblique manifold in (12) different? The optimization problem is equivalent in the sense that the local minimizer/stationary point does not change.

Limitations:
The limitations of the paper are discussed in the conclusion section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers solving optimization problems with constraints that have orthonormal columns (i.e. the matrix belongs to the Stiefel manifold). The leading method for solving such problems is the Riemannian optimization. However, Riemannian optimization requires a costly retraction operation. The authors propose to circumvent this by introducing an additional penalty terms that stirs the optimization towards respect the manifold constraints. Indeed, the authors show that with correcting setting of parameters, the optimum will be on the manifold, and the algorithm will find it. The authors advocate that an additional advantage of their algorithm is that we know how to set the parameters for the penalty term, and so their algorithm is parameter-free.

A significant part of the paper is devoted towards motivating the study in terms of low-rank adaption in LLMs, and showing experiments in that vain.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A very elegant method for retraction free optimization on the Stiefel manifold.
- Detailed theoretical analysis showing the algorithm converges to a critical point on the manifold.
- The theoretical analysis gives explicit guidance on how to set the penalty parameter.
- The LLM applicaiton and experiments appear impressive. However, I am not an expert on this subject, so it is hard for me to asses how significant the results and evaluations are.

Weaknesses:
(The following were addressed by the authors in the rebuttal)

Major issues (affecting the recommendation):
1) Novelty: Citation [1] considers optimization with constraints on the orthogonal group (i.e. St(n,n)). It seems that the core idea on how to implement retraction free optimization already appears there. The authors mention this in ""related work"", and say that [1[ does not discuss the r<n case. Inspection of [1] reveals that this is not the entire story. In Sec 3.5 of [1] the case of r<n is discussed briefly, and it is said the results can be extended for that case. However, the authors of [1] are skeptical of the value of this, as they mention that there are fast retraction methods (i.e. Cayley) for the Stiefel manifold. 
-  Setting the penalty parameter: the authors advocate that they give an explicit value for the penalty parameter. And indeed Theorem 1 sets the parameter \mu to 1/3. However, I do not think the situation is so simple. The theorems have the additional assumption that the iterates starts close to the manifold (1/8). This is, of course, easy to achieve - just start on the manifold itself. However, for the proof to work shouldn't all iterates stay inside this bound? This necessitates for the other parameter (step size) to be small enough. And indeed, the theorem requires that the step size be small enough, and does not specify how small. Without looking in detail in the proofs, my guess is that changing the penalty step size (\mu)  affects how close you need to be to the manifold (the value 1/8), which affects how small the step size need to be. In other words, the authors load all the complexity of setting the parameters onto the step size of the main objective. Saying there is  a upper bound on its value , without specifying what that value is. You cannot call this parameter free. 

Another point is that the values of the parameter probably affect convergence rate, though the authors do not discuss this at all.

Minor comments (do not affect the recommendation):
- Line 117: If X is on the manifold, shouldn't \bar{X}, which is the projection of X on the manifold, be exactly X?
- Line 119: ""satisfies the restricted secant""
- Line 131: What is U_St (1/8)? Not defined.  Ditto line 136.
- Line 133: If the condition of twice diff is assumed, then state it earlier. 
- Eq (10): \hat{D}_f is not defined. 
- Table 1: Why metrics are changing between columns?

Limitations:
Nothing to add.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new algorithm, Manifold-LoRA, which incorporates the Stiefel manifold constraint to accelerate low-rank adaptation (LoRA) in fine-tuning LLMs. It also provides theoretical and experimental validation for the retraction-free and penalty parameter-free optimization methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper is highly technical and indicates a strong mathematical background in optimization and manifold theory. Manifold-LoRA leverages manifold geometry to reduce redundancy in LoRA fine-tuning, leading to enhanced performance and faster convergence. Furthermore, it has robust experimental validation across various datasets.

Weaknesses:
W1: Some experimental results are unclear and not well defined.

W2: Lack of the discussion about limitations of your method.

W3: Some findings of the experiments are hard to understand.

Limitations:
Limitation is not enough and is meaningless.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Fm4FkfGTLu;"REVIEW 
Summary:
gRNAde is a graph neural network designed to address the RNA reverse folding problem, a significant challenge due to the potential of RNA as therapeutic modalities and their unique data properties. RNA molecules have lower thermodynamic stability compared to proteins, resulting in fewer training samples, and their increased flexibility means multiple final states are possible. gRNAde addresses these issues by proposing a custom multi-graph representation and extending message passing to operate independently on each conformer while sharing an adjacency graph. The authors thoroughly explore evaluation techniques, comparing their model performance against Rosetta by assessing the percentage of native sequence recovery on held-out sequence families. Additionally, they demonstrate slightly improved performance when utilizing multiple conformers for model training. The authors also conduct an interesting zero-shot ranking analysis on mutation data providing a refreshing evaluation against random baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is exceptionally well written presenting a thorough overview of the challenges within modality, broader field, and the importance of the problem.
- The experimental validation assesses the utility of design choices and although the improved performance in the presence of multiple conformers isn't large (the authors don't report statistical significance), the approach is promising. 
- The authors conduct a variant effect evaluation assessing whether their model is capable of learning impact of single or double mutant sequences demonstrating convincing improvement over random baselines.

Weaknesses:
- I find the arguments regarding gRNAde perplexity being correlated with recovery to have limited support in the current presentation. In figure 2 (b) color denotes perplexity instead of one of the axis making it very challenging to assess the correlation. In addition the authors don't report a correlation value or its significance. 
- The authors only use random baselines for the retrospective variant effect analysis. Including another reverse folding model or a metric from Rosetta similar to gRNAde's perplexity could strengthen the evaluation.

Limitations:
- The authors effectively discuss the current evaluation limitations and the difficulty of assessing the novelty and ground truth recovery of generated sequences.
- There is limited discussion on the data limitations in the current field. With only 4000 sequences, training points are very few, presenting a major challenge.
- The authors could include a brief statement on the broader impacts, such as the potential design of harmful molecules. As these models improve, the dual-use concern becomes legitimate.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a geometric RNA design model. Specifically, it introduces multi-stage GNN to encode multiple conformations and aggregate these candidates, and further feed decoder to predict probabilities of a set of candidate sequences.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	This work creates a new dataset for RNA inverse design, with diverse properties such as sequence length, number of structures, and structural variations.
2.	This work designs a multi-state RNA reverse design model, distinct from existing methods.

Weaknesses:
1.	The technical contribution appears to be somewhat weak. The backbone used is from existing GNN models for equivariant design.
2.	Some technical details are not claimed. For example, are the comparison baselines retained on the new dataset or simply tested on their released model.

Limitations:
This paper has no negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduce gRNAde, a geometric deep learning pipeline for RNA sequence design conditioned on one or more 3D backbone structures. gRNAde is superior to the physically based Rosetta for 3D 320 RNA inverse folding in terms of performance, inference speed, and ease of use. The method demonstrates significant superiority across various experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The authors introduce gRNAde, the first work to consider multi-state biomolecule representation. This study explores the feasibility and specific experimental results of using multi-state biomolecule representation, providing new ideas for researchers in the field.
2. The authors conduct extensive experiments and analyses on multiple datasets and experimental settings, demonstrating the model's effectiveness from various perspectives, especially regarding the ""Zero-shot ranking of RNA fitness landscape"" experiment, which is currently lacking in this field.
3. The authors present various experimental details using numerous visualizations and data tables, making the paper easier for readers to understand.

Weaknesses:
1. The model architecture proposed by the author lacks innovation. The core structure of gRNAde is directly stacked using GVP-GNN, and the handling of multi-state conformations is merely simple stacking. Additionally, the 3-beads representation method is very common in traditional RNA 3D structure modeling, which is also not an innovation by the author. Therefore, I believe the model design is lacking.
2. The baselines compared by the author in various experiments are either outdated or too simple, such as ""Rosetta(2020)"" and the ""random baseline"" in the Zero-shot experiment. This makes it difficult to demonstrate the actual performance of gRNAde. Some recent works using deep learning to model RNA 3D structures can serve as baselines, such as [1-3].
3. The author mentions that gRNAde has a significant speed improvement over Rosetta, but the author did not run the Rosetta code themselves and instead directly cited the original Rosetta paper. I believe this point is debatable because the model's running speed is also limited by GPU computational performance. The author uses an A100, whereas the GPU used by Rosetta four years ago is obviously inferior to the A100. Therefore, the author needs to rerun the Rosetta program on the A100 to provide accurate model inference times.







[1] Geometric deep learning of RNA structure, Science 2021

[2] Physics-aware Graph Neural Network for Accurate RNA 3D Structure Prediction, NIPS workshop 2022

[3] RDesign: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design, ICLR 2024

Limitations:
The authors discuss practical tradeoffs to using gRNAde in real-world RNA design scenarios 330 in Appendix B, including limitations due to the current state of 3D RNA structure prediction tools.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work designed gRNAde, a geometric deep learning pipeline for RNA sequence design conditioned on one or more 3D backbone structures. To achieve this, the authors created single-state and multi-state 3D RNA structure datasets, built a geometric graph representation, and proposed an architecture consisting of a multi-state GNN encoder, a pooling layer, and a autoregressive decoder. The single-state RNA design, multi-state RNA design, and zero-shot rank experiments were conducted and results show that gRNAde outperformed all previous methods including Rosetta.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1.	The datasets were carefully designed. Only structures with high resolution were maintained. Two kinds of clusters were used to split train, validate, and test sets where the hard samples were split into test sets.

2.	The model architecture makes use of information from multiple conformations. This is achieved by sum or average pooling.

3.	The experiments were conducted fairly. Datasets were split carefully. The results were averaged on 16 sampled sequences across 3 random seeds.

4.	The inference speed is much faster than traditional methods. This makes it possible to be used in High throughput screening. The zero-shot ranking ability is also an advantage.

Weaknesses:
1.	The model architecture has no novelty. All components are token from previous work and the overall structure is very similar to that of ProteinMPNN. The multiple conformations are processed independently and the representations are simply averaged or summed, which may not grasp all information.

2.	The model is trained on only about 4 thousand RNA sequences. These sequences are too few to cover the entire space. RNAs with no 3D structures should be exploited, as done in alphafold3.

3.	The results about single-state RNA design were reported on only 14 samples. More samples should be used to test the model. For example, the results on test sets with 100 samples should be reported.

4.	The improvements on multi-state RNA design task are limited. The native sequence recovery is obviously lower than the results of single-state design task.

Limitations:
The representation ability of gRNAde remains to be verified. Usually, the representation is extracted when all nucleotides are known, so the architecture for inverse folding problem may not suitable for representation learning.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduced a multi-state geometric graph neural network for the RNA inverse folding problems. Experiments are conducted on carefully splited structural datasets that avoid data leakage. The results have shown convincing performance improvement over the physics-based methods such has FARFAR and Rosetta which are commonly used for RNAs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is well written and pleasing to read. Explanations on key biological concepts related to RNAs, and how they motivate the model design as well as experiment setup are adequate and above all, clear.
- Evaluation metrics are well considered. The self-consistent scores on the secondary and tertiary levels are meaningful.
    - limitations on self-consistence scores are acknowledged in the main text. For RNAs, many challenges are unique especially when they are compared to proteins. Therefore, clarifications and precautions are particularly needed for RNA related tasks. I appreciate the authors’ effort for stating these limitation clearly in the main text.

Weaknesses:
- Comparison to contemporary deep learning models for RNA inverse folding is limiting
    - RDesign (https://openreview.net/forum?id=RemfXx7ebP) for example is a recent deep learning based method for 3D RNA inverse design
    - For inverse design on the secondary structure level there are many more options — a lot of them are better than RNAinverse from ViennaRNA. I would suggest checking out this survey (Design of RNAs: comparing programs for inverse RNA folding) and include a few other more competitive baselines.
- For the self-consistency scores, I personally doubt if RhoFold (also called e2efold-3d) is reliable software for RNA tertiary structure prediction, since it is from the same group that published e2efold which is a spectacularly awful RNA secondary structure predictor (I personally would avoid using any of their tools; checkout its Github issues, and also followup works on RNA secondary structure predictions that have compared with e2efold). Have the authors used more recent folding softwares such as RosettaFoldNA and AlphaFold3?
    - Would using different structure predictors significantly impact the results? This also includes EternaFold. How would gRNAde hold up against the baselines when RNAfold or LinearFold is used to compute the self-consistency scores on the secondary level?
- Data splits (train, validation and test) are carefully constructed so that the evaluation is not contaminated by data leak. But I wonder if the TM-score cut-off at 0.45 is too lenient? Is it still possible to have similar structures between training and test sets under this threshold?

Line 258 to 259. The argument would be more compelling if the inverse design operates at the quaternary level which would include information about ligand structures.

Limitations:
- Comparison to contemporary models for RNA inverse folding is a bit hollow. It would be more meaningful if some deep learning based baselines can be included into the comparison.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
DsN7tHNo78;"REVIEW 
Summary:
This paper focuses on Zero-Shot Composed Image Retrieval (ZS-CIR), which requires retrieving an image matching a reference image while incorporating specified textual modifications. The authors argue that a key challenge in ZS-CIR is training models on limited intention-relevant datasets to understand human intention implicitly expressed in textual modifications for accurately retrieving target images. Therefore, they introduce an image-text dataset incorporated with pseudo-manipulation intentions to enhance the training of ZS-CIR models in understanding human manipulation intents, based on LLaVA. They also propose to use Q-former to compress the features generated by CLIP for retrieval. The experimental results show the improvements of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors propose a large-scale pretraining dataset for ZS-CIR.
2. The experimental results show the improvements of the proposed method.

Weaknesses:
1. The concept of ""intention"" discussed throughout the whole paper is unclear. Based on Figure 1, the authors haven't explained what's ""intention"" to the MLLM. Moreover, the pseudo-manipulation description is the same as the rewritten caption in semantics. I can't find any ""intention"" added into this pseudo-manipulation description. The key novelty of considering human ""intention"" is farfetched.
2. The proposed model lacks novelty. In the model architecture, the authors just add a Q-Former [1] after the CLIP encoder, which is prevalent in existing research based on CLIP-like models. And the authors even do not cite any relevant work.
3. Existing work on CLIP-based ZS-CIR generally compares the experimental results with different CLIP variants, and different methods may be superior with different CLIP variants. The authors only experimented with one CLIP variant, which is insufficient.
4. In Figure 4, the compared method also accurately captures the ""intention"" in the modification text, which cannot show the superiority of the proposed method in capturing ""intention"".

References:

[1] Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.

Limitations:
The authors addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces De-MINDS, a novel framework for Zero-Shot Composed Image Retrieval (ZS-CIR) that aims to bridge the gap between pre-training and retrieval by incorporating intention-based pseudo-manipulation descriptions. The authors propose intent-CC3M, a dataset featuring these descriptions generated through chain-of-thought prompting by a Multi-modal Large Language Model (MLLM). They also introduce a manipulation intention understanding network that uses learnable queries to enhance the model's ability to understand user intentions from manipulation descriptions. The paper demonstrates significant performance improvements across four ZS-CIR tasks compared to state-of-the-art models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The introduction of intent-CC3M as a dataset for training mapping networks to align intention-relevant visual information is innovative and potentially impactful.
- The proposed De-MINDS framework shows significant performance improvements over state-of-the-art models across multiple ZS-CIR tasks.
- The approach addresses the challenge of understanding manipulation intentions in user descriptions, which is crucial for accurate image retrieval.
- The ablation studies provide insights into the contributions of different components of the proposed method.

Weaknesses:
Major Weaknesses:

1. Experimental Gaps:
   - The paper lacks experimental evidence to support the claim that caption redundancy leads to inaccurate retrieval, as mentioned in the introduction.
   - There's no evaluation of the method's performance with longer text encoders like LongCLIP, which could potentially address some of the stated limitations of CLIP.
   - The comparison with a baseline (other than CIRR and Fashion-IQ) using only f_theta (trained on Intent-CC3M) without De-MINDS (ablation model '4') is missing, which would provide a fairer comparison.

2. Methodological Concerns:
   - The justification for using CC3M as the base dataset for creating intent-CC3M is not clearly explained.
   - There's no exploration of De-MINDS' performance when prompt options are mismatched with their intended tasks or in scenarios where the task is not known in advance.

3. Incomplete Ablation Studies:
   - The ablation study for the T sampling ratios (50%, 30%, 20%) is missing, and there's no explanation why concatenation of them wasn't considered as an alternative.
   - The ablation study lacks an exploration of the impact of the number of learnable queries, despite its apparent significance.

Minor Weaknesses:

1. Presentation Issues:
   - The prompt types (a), (b), and (c) are not clearly explained in the context they are introduced, requiring readers to refer back to previous sections.
   - There are inconsistencies between the notation in the text and figures (e.g., X vs q in Figure 2).

2. Comparative Analysis:
   - The paper doesn't include evaluations on CIRCO and GeneCIS datasets, which were used in baseline studies.

3. Clarity:
   - More details are needed on certain aspects, such as the ""cos distill"" mentioned in ablation model '9'.

Limitations:
The authors acknowledge the computational intensity of generating pseudo-manipulation descriptions using MLLMs and the potential introduction of irrelevant details in these descriptions. However, they could further discuss the implications of these limitations on the practical applicability of their method in real-world scenarios.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an image-text dataset (intent-CC3M) for Zero-Shot Composed Image Retrieval (ZS-CIR) models to make better understanding of human manipulation intentions. Specifically, captions are re-written with LLaVA model to provide more details, and additional manipulation reasoning prompt is applied to make pseudo-manipulation description. With this dataset, the paper proposes De-MINDS framework (unDErstanding of Manipulation INtention from target Description before Searching), which utilizes pseudo-manipulation descriptions. The model training involves reasoning distillation and cross-modal alignment. The method shows state-of-the-art performance with ViT-L backbone comparisons.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes to leverage LLaVA model to elaborate the image caption and further utilize LLaVA's reasoning capability to build a pseudo manipulation. The proposal is intuitive and clear, and the presentation of this paper is also clear. Extensive results including various ablations and qualitative results demonstrate the proposed method.

Weaknesses:
The proposed method of utilizing LLM, referred to as MLLM, is not entirely novel, as it has been previously addressed in works [1, 2] (please also refer to [1]). Furthermore, the evaluation of the proposed method is limited to the ViT-L backbone, which raises concerns about its effectiveness with other, more robust backbones (such as ViT-G).

[1] Jang, et al. Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval, CVPR2024
[2] Karthik, et al, Vision-by-Language for Training-Free Compositional Image Retrieval, ICLR2024

Limitations:
The paper handles possible limitations properly.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
DSVGACQ3sO;"REVIEW 
Summary:
The paper studies the behaviour of amortised (supervised) causal discovery methods based on different training data distributions and its relation to more traditional causal discovery and the related identifiability theory. The authors empirically validate the intuitions about supervised causal discovery and generalisation of supervised learning methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper studies the behaviour of amortised causal discovery methods which have previously been unstudied.
- The empirical insights generally validate the intuition about identifiability and generalisation. Some examples give interesting insights into the identifiability and performance in the case of mixed assumptions.

Weaknesses:
- The paper is a purely empirical study of the generalisation behaviour of supervised causal discovery methods, validating general intuition without thorough novel insights.
- Given the empirical nature of this paper, I'd have expected to see a more thorough comparison, e.g. setting up a leave-one-out generalisation study or more in-depth analyses of the prediction on interesting individual SCMs such as the non-identifiable example or the performance of the prediction from new samples from a training set SCM.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores why causal discovery from observational data, particularly with CSIvA, a transformer-based model, can achieve competitive performance despite seemingly avoiding the explicit assumptions that traditional methods make for identifiability. The authors demonstrate that constraints on the training data distribution implicitly define a prior on the test observations. When this prior is well-suited, the underlying model can be identifiable. In other words, prior knowledge of the test distribution is encoded in the training data through constraints on the structural causal model governing data generation.

Additionally, they provide a theoretical basis for training on observations sampled from multiple classes of identifiable SCMs, a strategy that enhances test generalization to a wide range of causal models. They show that training on mixtures of causal models offers an alternative approach that is less reliant on assumptions about the mechanisms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper bridges the gap between existing theoretical results on identifiability and practical observations. More importantly, it moves away from classical causality settings and quite restricted models, shifting towards more mainstream and modern models like transformers. This opens a pathway for causality research to integrate with large language models (LLMs), which represent the state-of-the-art in a wide range of applications.

Weaknesses:
The presentation can be significantly improved. Since the paper aims to offer novel insights, it is crucial to organize the arguments, theoretical results, and experimental findings effectively to support these insights.

Limitations:
n.a.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the level of generalization achievable when training a predictor to classify “X causes Y” vs. “Y causes X” from observational data. Motivated by recent works performing causal discovery using a pretrained transformer model, the works explores which cases result in predictors that generalize to graph-dataset pairs generated from unseen types of SCM models. This is mostly achieved through a set of empirical experiments on synthetic 2-node SCM data. The work derives a corollary of Hoyer et al [7] to argue why training on multiple identifiable classes of synthetic SCM instances may help generalization amortized causal discovery methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper takes a first step towards analysing why amortized causal discovery performs well in practice and often significantly better than classical approaches. This is generally an important direction and of interest to the field.

Weaknesses:
While the motivation of this work is generally well-grounded, the contribution and argument of the work itself have several weaknesses that, in my opinion, do not justify many of the claims made in the abstract, introduction, and throughout the paper.

First, a major aspect of amortized causal discovery with transformers (referenced in the title) is that of solving structure learning tasks in *high dimensions*. Lopez [10] already provide theoretical and empirical analyses of the bivariate case. Recent work showed that this idea can generalize to (very) large systems -- the literature of works on causal discovery with transformers cited in the paper all study significant large-dimensional problems (ranging from 20-100 variables). Despite this, the present paper limits its entire analysis to the bivariate case. Thus, it is misleading to claim the paper “demystifies amortized causal discovery with transformers”. No part of the analysis concerns multivariate causal discovery or transformers. The paper should be upfront and highlight much more clearly what its contributions are beyond Lopez et al [10], which already study the bivariate amortized causal discovery case.

The paper repeatedly states it analyses CSIvA. However, none of the algorithmic components of CSIvA, such as e.g. the auxiliary loss it is trained on or the architecture of the predictive model, are part of the analysis. The loss function studied here (p.3, l. 133) is the same as e.g. used by [13]. Hence, it would be more truthful to claim the analysis concerns general predictors trained on the classification task of X->Y vs X<-V, as in [10].

A major component of causal discovery performance is not only identifiability of the graph from the observational distribution, but also the intractably large search problem incurred by classical score- and constrained-based methods. The question is: do transformers outperform classical methods in large problem sizes because 1) (parts of) the graphs are identifiable to it, or 2) a prediction-based approach is better at finding the identifiable edges in a large system (as opposed to doing a search)? This question motivates amortizing causal discovery in the first place, but the two-variable special case studied here is ill-suited for answering it. Since the work only studies the bivariate case, the title and claims throughout the paper, as well as their ties to the (large-scale) transformer literature have to be recalibrated.

Section 3.2 seems unnecessary. The section only studies the generalization ability of CSIvA, which is no contribution. The takeaways (lines 195-) that “CSIvA generalizes well to test data generated by the same class of SCMs used for training” and that “it struggles when the test data are [from different SCMs]” are obvious and well-studied by CSIva or related works with the same approach. The same applies to the insight that “training […] exclusively on LiNGAM-generated data is equivalent to learning the distribution p(.|D, LiNGAM)”, implying identifiability.

Limitations:
-	The “theoretical result” (Proposition 1) is a simple corollary of Hoyer et al [7]. The paper makes otherwise no theoretical contribution to the problem underlying amortized causal discovery itself.

-	It is unclear whether “randomly initialized MLPs” are sensible nonlinear functions to use for constructing nonlinear mechanisms and non-Gaussian noise distributions. The fact that a few prior works used it is not a good reason. The shape and scale of randomly initialized neural network functions depends heavily on the activation function and weights distribution. The functions in these experiments could be anything from approximately constant or linear to very jumpy. Please provide additional motivation or evidence for why this is a good choice, and what hyperparameters are used, or consider as an alternative, e.g., samples from a GP, which are smooth and have an interpretable length-scale parameter, also in high dimensions.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper conducts an empirical study of the performance of supervised causal discovery methods, its generality, and the learnability vs. causal structure identifiability. The scope is the bivariate case, and with controlled mechanism and noise to establish the SCM for training and testing data. 

In my opinion, this paper gives two findings:

1) a previous claim (Lopez-Paz et al. [10]) said that, by using the supervised learning based approach, the performance of causal discovery can exceed the boundary of identifiability. which is not true

2) by using diverse training data (diverse = diverse mechanisms + diverse noise), supervised based causal discovery can achieve better OOD performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1 - the study of supervised causal learning, especially the DNN-based approach, is timely and important. 

2 - the experiment setup, is a good starting point. To my knowledge, this is the first paper study the performance, boundary of supervised-based causal discovery methods, setting the bivariate case, with the configuration in terms of mechanism + noise is valid.

3 - some findings are interesting, which can potentially benefit the community for further algorithm design.

Weaknesses:
1 - part of the study can be summarized as learnability vs. identifiability, or in my opinion, one question within this category is ""when and how can learnability exceeds the boundary of identifiability?"". in this regard, the current findings are still very limited, need to be further consolidated.
in this regard, a related work [1] is missing, I think it is helpful for this work.

2 - although not explicitly claimed, this paper suggests that ""CSIvA is capable of in-distribution generalization', is this true? or is this true just for bivariate case or generaly applicable?

3 - I suggest to use the term supervised-based approach, or supervised causal learning (SCL), rather than amortized causal discovery, which is more to the point.

4 -  one claim ""we conclude that the post-ANM is generally identifiable, which suggests that the setting of Example 2 is rather artificial""
I disagree. Although the space of all continuous distributions such that the bivariate post-ANM is non-identifiable is contained in a 2-dimensional space, thus it is a submanifold of the entire distribution space, thus its measure is 0. This is only a mathematical claim but lacks real-world relevance. I would argue that the setting of example 2 is quite valid in real-world setting, or the linear gaussian setting, is also commonly adoped in real-world, but had not been discussed in this work.

5 - potential conflict between section 3.3 and 3.4:
3.3 shows that when mixed two training dataset (different setting) together, would significantly compromise the SCL's performance; however, section 3.4 shows that the more diverse of the training data, the more gain on OOD setting.


[1] Dai, H., Ding, R., Jiang, Y., Han, S., & Zhang, D. (2023). Ml4c: Seeing causality through latent vicinity. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM) (pp. 226-234). Society for Industrial and Applied Mathematics.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
DHVqAQ9DHy;"REVIEW 
Summary:
This paper introduces Posterior Label smoothing (PosteL), an innovative approach to enhance node classification on graph-structured data. PosteL integrates local neighborhood information with global label statistics to generate soft labels, aiming to improve model generalization and mitigate overfitting. The authors demonstrate the effectiveness of PosteL through extensive experiments on various datasets and models, showing significant performance improvements over baseline methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The paper is well-written and easy to follow.
2.	The authors provide a comprehensive set of experiments across different datasets and models, which substantiates the effectiveness of the proposed method.
3.	The figures and tables are well-organized, clear and easy to understand.
4.	The method is relatively lightweight and easy to implement at the technical level.

Weaknesses:
1.	While the paper mentions the computational complexity, a deeper analysis or comparison with existing methods could provide more insight. For example, maybe you could provide some compared experiments with existing methods on time/resource consumption.
2.	The reliance on global label statistics might introduce bias in cases where the dataset has inherent class imbalance or label noise.
3.	The article ""Rethinking the inception architecture for computer vision"" appears twice in your reference list; please consolidate these entries. Carefully review your references to maintain standardization.
4.	The author compares two soft label methods that were proposed quite some time ago (from 2015 and 2016, respectively). Are there any experimental results comparing with more recent methods? Otherwise, the persuasiveness of the experiments might not be so strong.
5.	Please maintain consistent terminology throughout the text. The term ""over-fitting"" in line 46 should be changed to ""overfitting"" to be consistent with the rest of the context.
6.	Authors could provide more details on the sensitivity analysis of the hyperparameters α and β, which are crucial for the method's performance.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed label-smoothing to improve the transductive node classification in GNN.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Label-smoothing and knowledge distillation are applied for node classification performance.

Weaknesses:
1.	The paper could benefit from discussing related works that combine label-smoothing with Graph Neural Networks (GNNs), such as [1] and [2]. Including these would provide a more comprehensive context for the current research.

2.	The proposed method lacks a theoretical motivation or analysis. Providing this would strengthen the paper's scientific rigor and help readers better understand the underlying principles.

3.	The proposed method bears similarities to the approach in [1]. A direct comparison with this work would clarify the novel contributions of the current study and situate it within the existing research landscape.

4.	Iterative pseudo-labeling is a well-established technique in the field. The paper should address this, explaining how the current application differs from or builds upon previous uses of this method.
Addressing these points could significantly enhance the paper's depth and impact. 

---
[1]: Wang, Y., Cai, Y., Liang, Y., Wang, W., Ding, H., Chen, M., ... & Hooi, B. (2021). Structure-aware label smoothing for graph neural networks. arXiv preprint arXiv:2112.00499.

[2]: Zhang, Wentao, et al. ""Node dependent local smoothing for scalable graph learning."" Advances in Neural Information Processing Systems 34 (2021): 20321-20332.

Limitations:
The limitation should be clarified in the paper. The current limitations are not clear.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes PosteL, a label smoothing method utilizing posterior distribution for node classification in graph-structured data. It is basically a preprocessing method for GNNs, generating soft labels based on neighborhood context and global label statistics before the training phase.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is generally well-written and easy to follow. For example, Fig. 1 is clear and intuitive.
2. The method is simple yet effective. PosteL can be combined seamlessly with existing methods.
3. The results are significant. PosteL is tested on seven neural network models across ten datasets, demonstrating significant improvements in classification accuracy.

Weaknesses:
1. The authors select $\alpha$ and $\beta$ from a wide range but did not explore the parameter sensitivity of PosteL. The sensitivity to hyperparameters could be a potential limitation, necessitating careful tuning, which may reduce the credibility of the experiments.
2. The authors do not seem to clarify the difference between PosteL and other label smoothing methods for node classification (or methods that can be adapted to node classification), which makes the novelty of the method unclear. The paper could explore other smoothing techniques or baselines in more depth for a comprehensive comparison.

Limitations:
The authors discuss the case when the prior likely dominates the posterior, which limits the effectiveness of the proposed PosteL.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a preprocessing step to refine labels of nodes in a structured graph that can benefit different graph-related transductive classification tasks. Inspired by the success of label smoothing in other machine learning tasks, the authors propose a label smoothing procedure based on a Bayesian inference that aggregates local and global information to estimate the soft labels. The procedure consists of mixing the soft and hard labels and an iterative regime akin to the Bayes update, which makes the method adaptive to different regularities present in different datasets. Authors conduct experiments applied to various models and datasets to support the efficacy of their methodology. They also provide an ablation study and further analyses of the results that shed light on different aspects of their proposal.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The identified gap is relevant, and applying label smoothing to the context of graph node classification bears novelty in terms of its application in this context. 
2. The empirical results suggest that the proposed solution addresses the research question successfully and merits the attention of the community. 
3. Moreover, the core ideas are communicated clearly and coupled with intuitive illustrations demonstrating the proposed method, which is very well appreciated. 
4. And lastly, the results and analyses are communicated well.

Weaknesses:
1. **Related work**: Currently, the related works seem to provide references to earlier studies that, for the most part, motivate this work and are not methodologically close to it. For example, there is no reference to closely related works that either adopted label smoothing or conducted a very similar procedure in the context of graph data. Most notable is ""[Adaptive Label Smoothing To Regularize Large-Scale Graph Training](https://epubs.siam.org/doi/abs/10.1137/1.9781611977653.ch7)"" which appears to have a very similar procedure but a different approach to obtain the soft labels.

Moreover, the current statements imply that the current work is the first to suggest label smoothing for the graph data. To be more concrete, line 81 needs to be expanded, and more closely related works need to be discussed. For example, to compare the current approach and highlight similarities and key distinctions with earlier works that are closely related to it.

Some other related works could be the following: 
- [Structure-Aware Label Smoothing for Graph Neural Networks](https://arxiv.org/abs/2112.00499)
- [Label Efficient Regularization and Propagation for Graph Node Classification](https://ieeexplore.ieee.org/abstract/document/10234505)
- [Node Dependent Local Smoothing for Scalable Graph Learning](https://proceedings.neurips.cc/paper_files/paper/2021/hash/a9eb812238f753132652ae09963a05e9-Abstract.html)


2. **Design decisions and theory**: besides complexity analysis, the study could have been accompanied by convergence analysis and more theoretically founded justification. However, this does not reduce the value of the work as its empirical results provide a strong signal for the effectiveness of the method, which merits future work toward theoretical assessment and explanation of its success.

3. **Background information**: the classification task that uses the preprocessed smooth labels is not defined explicitly, which makes the work less accessible for the readers without prior knowledge.

4. **The IPL step** is proposed to address the presence of ""unlabeled nodes""; however, it is hard to follow how the varying training size experiment reported in Table 4 is analogous to the unlabeled node scenario. Perhaps it is due to a lack of background information mentioned in point 3. 

5. **Suggestions to rephrase**:
   line 236: ""mitigate the importance"", perhaps some rephrasing is needed.
   line 189: ""learning curve"" -> ""loss curve"" 
   line 209: ""when"", some rephrasing might be needed

Limitations:
Limitations are addressed in the body of the text. It is perhaps preferable to have the important limitations mentioned in a separate section or in the conclusion as well.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
D80tRH9CXF;"REVIEW 
Summary:
The paper examines prediction and estimation risk of ridgeless least squares estimator in the setting of a general error structure. The iid assumption on the error structure is often not valid in settings such as time series data , panel data, grouped data etc. The current paper introduces a theoretical framework which investigates the variance component estimation of both prediction and estimation risks in the above mentioned data settings. The benefits of overparametrization which has been seen in iid context has been shown to exist in the dependent error structure context as well.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Following are the strengths of the paper-

- Investigation of prediction and estimation risk under non i.i.d. regressor errors with specific focus on time series and cluster data

- Explicit quantification of the variance component of both the risks (as mentioned above) which depends on the trace of the error covariance matrix and the trace of a function of design matrix as a separable product.

- Explicit analysis of the variance and bias term of both the risks (as mentioned above) in the high-dimensional asymptotics

- Well constructed numerical experiments to support the theory

Weaknesses:
Following are the weaknesses of the paper

- The theoretical results particularly the bias component analysis section could have been more rigorous and better written. There are some notational discrepancies and theoretical inconsistencies.

- Some remarks following theorem 3.4 and 3.5 where the design matrix $X$ has a known distribution say Gaussian would have been useful  examples to get insight on the results proved in the theorems

- Some notations such as $a(X)$ and $b$ used in theorem 3.4 have been clarified later in the appendix. It would be better to introduce them in the sketch of the proof if you are using them anyway there.

Limitations:
The authors have adequately addressed the limitations of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper explores the prediction risk and estimation risk of the ridgeless least squares estimator under more general assumptions on regression errors. It highlights the benefits of overparameterization in a realistic setting that allows for clustered or serial dependence. The paper establishes that the estimation difficulties associated with the variance components of both risks can be summarized through the trace of the variance-covariance matrix of the regression errors. The findings suggest that the benefits of overparameterization can extend to time series, panel, and grouped data. The paper is a theoretical work that discusses various aspects of linear regression models, providing details on the assumptions and proofs for the theoretical results presented. It also includes information on the experimental setting and provides code and instructions for reproducing the main results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This study addresses an important research gap by considering more realistic assumptions on regression errors. It provides exact finite-sample characterizations of the variance components of prediction and estimation risks, includes numerical experiments that validate the theoretical results, and demonstrates the relationship between the expected variance and the covariance of the regression errors. Additionally, it analyzes the bias components of prediction and estimation risks, offers a comprehensive overview of linear regression models covering various theoretical aspects, and provides detailed proofs for the theoretical results, ensuring the validity of their claims.

Weaknesses:
Is it possible to provide validation on large-scale data?

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the properties of minimum norm (ridgeless) interpolation least squares estimators, analyzing prediction risk and estimation risk under broader regression error assumptions, including clustered or serial dependence. This diverges from the typical assumption of i.i.d. errors with zero mean and common variance. The paper shows that the challenges in estimating the variance components of prediction and estimation risks can be captured by the trace of the variance-covariance matrix of the regression errors.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a more general theoretical analysis of minimum norm interpolation least squares estimators, going beyond the restrictive i.i.d. error assumption.

2. The paper suggests that the benefits of overparameterization can extend to a wider range of regression settings, including time series, panel, and grouped data.

Weaknesses:
While the paper examines broader error structures, it might not fully grasp the complexity of real-world regression challenges, which could involve even more intricate patterns of error dependence.

Limitations:
The authors have addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the ridgeless least-squares estimator, and derives its prediction and estimation risk. One of the assumptions used is that the expectation of the noise variance matrix is finite and positive-definite. This is more general than the assumption that this expectation is some positive multiple of the identity matrix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper has an easy-to-follow introduction that motivates the need to derive theoretical results under general assumptions on regression errors.
- Related works are sufficiently discussed. The most relevant papers are those of Chinot et al. [9] and Chinot and Lerasle [8], which are based on different noise assumptions that this paper makes.
- The technical presentation is clear with examples and figures to help the reader understand the notations and results.

Weaknesses:
The major concern I have is whether the paper makes sufficient technical contributions. Even with the more general assumption on noise (Assumption 2.1), the technical change in the proofs seems very small compared to prior work. For example, the proof of Theorem 3.4 is short and relatively straightforward (and this might further simplify if we make Gaussian assumptions on data rather than left-spherical assumptions. Gaussian assumptions are what I like to make personally). It is always nice to have short and concise proofs whenever possible, but this might also indicate that the paper is not very technically solid.

Limitations:
N.A.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Cth1PyCwZt;"REVIEW 
Summary:
This paper shows the use of psychometric modeling techniques to measure the reasoning ability of LLMs on human exams. Specifically, the author(s) use Item Response Theory (IRT) to evaluate a Brazilian college-entrance exam, and demonstrate that IRT can provide a more informative evaluation of LLMs , including: the ability to distinguish human-like vs non-human-like response patterns, and to determine whether an exam can reliably measure an LLM's abilities. The empirical results suggest that traditional accuracy metrics are insufficient to assess the abilities of LLMs, and advocate for using IRT/psychometric theory to evaluate them.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Provides more comprehensive evaluation methods for LLM performance, I personally agree that accuracy metrics often do not give a complete picture of LLM ability.
2. The results section is methodological, it evaluates not only the IRT scores but how reliable they are based on several metrics (increases reliability of the evaluations)

Weaknesses:
1. The results analysis would benefit from a more detailed and clearer/deeper analysis, some statements made (eg. L293-298) are high level observations based on the results, but lack further insight into why certain LLM behaviors occur. Performing more detailed analyses into the specific subset of questions that contribute to scores could help to further understand the limitations of the LLM (L328-331 alludes to this, but very briefly).

2.  All the evaluations were done on variations of the ENEM exam dataset, showing that these psychometric method would also work on other datasets would make this approach more convincing that it will work for wider applications - I understand that there is limited time to run more experiments, so this is more so just a comment.

Limitations:
As mentioned above, as experiments are done on variations of one dataset, there are doubts about the generalizability of these methods on other datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on evaluating LLM abilities on a dataset of 8 college-entrance exams in Brazil (translated to English) measuring Item Response Theory instead of Accuracy. It highlights how such metric is useful to better understand models' performance.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I have found the work very well structured and appreciated the amount of care the authors have been given to the preparation of the dataset for the experiments (PDF processing, translation to English, use of exams designed for blind people in order to address questions based on images, etc). The experiments and results are discussed in details, with clear comparisons with human performance, discussing clear differences (e.g. in Mathematics).

Weaknesses:
While the paper is well structured, I felt it was missing a ""what now?"" message. The authors wrote a convincing argument in favour of using IRT, how do we convince now the field of ML / AI to use it more extensively? What are its limitations in comparison with accuracy-based metrics (given there are many, for instance you need information on overall human performance) and how do we overcome them?

Limitations:
I think the work should have discussed more about the specificities of ENEM - I agree with the authors that this is a relevant test-bed for this sort of evaluation, but in which ways are they specific / tailored to Brazil? Is there anything researchers should know about ENEM, which would make future testing / applications more challenging? For instance which topics are covered in Humanities or Languages, how specific are they about the country cultural context?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper initiates the empirical study of the performance of LLMs using Item Response Theory (IRT) models from a large college-entrance exam.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The question of construct validity of LLM evaluations (based on scores in exams designed for humans) is very important. This paper addresses this question in the earnest, by leveraging the primary tool developed in the education and psychometrics field, IRT. As far as I know, this is an original contribution as no previous work has used IRT in the same way to tackle the construct validity issue of LLM evaluations.

- The paper is largely well-written and the concepts (e.g. section 3) are explained clearly.

- Relative strength of empirical work. The experiments are well-designed and there is some breadth in the range of hypotheses explored, e.g. English vs. Portuguese effect on performance, Different topics of exams, response patterns in LLMs vs. humans with questions sorted by IRT difficulty. Seven different LLMs were evaluated.

- Significance. The method of this paper (i.e. using IRT in LLM evals) is an important first step to understanding what LLM evals are trying to measure. The paper already observes interesting phenomena, e.g. 
(1) the Fisher information of the math exam for the LLM test response distributions is low compared to other exams (although this is a somewhat obvious corollary of the p_i's being close to random for the LLMs performance on the math exam, the FI is a metric that points in the right direction).
(2) the joint distribution of IRT scores and CTT scores for LLMs is meaningfully different from that of the human test takers.

Weaknesses:
1. Some of the conclusions drawn by the paper appear unscientific/not well-substantiated. To me, the empirical results are subtle and require more thoughtful interpretations. Most of the interpretations of the experiments are confusing to me (i.e. I'm skeptical the conclusions follow), given the actual plots shown. For example, 

(a) What are ""outlier models"" (line 237)? We cannot see from Figure 1 that ""outlier models ... have higher accuracy and/or lower IRT scores..."" - how is this statement supported?

(b) line 223-224. The scale of IRT scores and CTT scores is not comparable. How can you conclude there is ""greater variability"" in the latter than in the IRT score? This is not scientific.

(c) line 264-265. The statement ""...questions that are easy for humans but difficult for LLMs"" is again inaccurate. The questions are relatively easier for humans but may not be ""easier"" than the other questions for humans, if easier means for humans anyway.

(d) Why is the math exam not meaningful for evaluating LLMs? Doesn't it suggest that the models are randomly guessing and therefore bad? I don't agree with this interpretation.

2. A clarity issue with the math writing. Line 154-155: This sentence ""...j has a more likely response vector than indicated by their ability"" is mathematically wrong. It is not possible to have a random draw from a Multinomial distribution that is ""more likely"" (i.e. higher probability) than the expectation vector (which is not even in the space of possible draws).

3. Experiment section writing missing some details and figures are somewhat difficult to interpret (esp Figure 1). I have several unanswered questions. How was the closed curve generated from the 30 points (of random shuffles)? The caption for Figure 1 could be more informative, e.g. was the exam answered in English or Portuguese by the LLM. If English, are the IRT models fit still valid? - I don't think so. 

4. Typo in lines 232-233, ""Natural sciences"" appears twice. and the sentence contradicts the graph.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a fresh perspective to evaluating LLMs by arguing for a stronger emphasis on psychometric methods particularly Item Response Theory (IRT) when evaluating them on exams designed for humans, rather than the reliance on traditional metrics such as accuracy. The authors postulate that IRT provides a more comprehensive evaluation by considering not just the number of correct answers but also the difficulty of the questions and the patterns of responses. The authors utilize the Brazilian college entrance exam ENEM for their case study and compare how various LLMs fare against human test-takers. They show how psychometric methods can be leveraged to distinguish between human like and non-human like responses. Furthermore, they demonstrate how IRT can be used to assess the suitability of an exam for making meaningful measurements of an LLM's abilities in the given area.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is very well written. It provides a comprehensive literature review and does a good job at covering the background information. The experimental analysis is sound with sufficient supporting materials. The paper makes significant novel contributions to evaluation of LLMs. The application of psychometric methods and the insights that can be mined through them when used to compare LLMs can be of significant interest to the research community. The experimental results on assessing whether an exam is a good indicator of an LLM's ability are particularly interesting and open up significant opportunities for future research.

Weaknesses:
The error analysis can be more detailed especially in areas where the results are surprising. This would better help support the conclusions.       For instance for the questions in Math and Natural Sciences wherein the models show fluctuating performance it would be useful to know what those questions aim to test. Are LLMs not able to solve the problems due to calculation errors or do these problems involve more complex multi-step reasoning or is it just linked to knowledge cutoff (e.g questions involving current events)?

Limitations:
The pre-requisite for this type of evaluation seems to be the existence of a strong IRT model which in turn requires the existence of large amount of carefully annotated human data.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
CECVgSZbLW;"REVIEW 
Summary:
The authors explore the use of distributional reinforcement learning within Monte Carlo Tree search. They propose two algorithms CATS and PATS a categorical distribution and particle distribution based approach respectfully. They perform a theoretical analysis of the methods and show analysis of regret. They then evaluate on a synthetic planning tasks and evaluate it in combination with a pre-trained network on the atari benchmark.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- Thorough theoretical analysis
- The authors address distributional RL applied to planning which is a clear important direction of research

Weaknesses:
- Lack of referencing of existing work and novelty relative to existing work
    - Hayes, C.F., Reymond, M., Roijers, D.M., Howley, E. and Mannion, P., 2023. Monte Carlo tree search algorithms for risk-aware and multi-objective reinforcement learning. *Autonomous Agents and Multi-Agent Systems*, *37*(2), p.26.
- Unjustifiable statement: “For example, CATS is significantly better than other methods in Breakout, Enduro” There is no significance testing performed so this statement cannot be made and in fact the Confidence intervals overlap
- Key results in appendix and lack of empirical results in the main paper
- CATS never outperforms fixed depth MCTS on the synthetic tree task
- Unable to find code despite checklist saying it is provided

- Small issues
    - Figure 2 algorithms alignment off
    - Indication of Atari results in section 5 which are not there
    - Adding bold to best performing method in the Atari table would be useful for readability

Limitations:
- Limitations are not included in the main body of the paper which they should be especially considering there is space. The limitations are also not thoroughly discussed for example
    - “faces challenges in managing computational demand” : this does not say anything meaningful
    - “Our approach’s performance is slightly influenced by hyperparameters”, this can be said for essentially any method
- It seems that the distributional approach has an additional memory cost which if correct should be added to the limitations
- Given CATS and PATS do not massively outperform all baselines on the synthetic task I think the limitations should be where this is addressed and perhaps some insight given into why this is and why performance on Atari is also not particular strong relative to methods such as MENTS.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper propose two algorithms, Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS). These algorithms extend Distributional Reinforcement Learning (RL) to Monte-Carlo Tree Search (MCTS) by modeling value functions as categorical and particle distributions, respectively to improve the performance of MCTS in highly stochastic settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- **Originality:** The integration of Distributional RL into MCTS using categorical and particle distributions is innovative and addresses a critical need in handling stochastic environments (Sections 3.1-3.3).
- **Quality:** The theoretical analysis is rigorous, with well-defined proofs and clear methodology (Sections 4.1 and 4.2).

Weaknesses:
1. **Empirical Validation**: While the paper presents a comprehensive set of experiments demonstrating the efficacy of the proposed methods (CATS and PATS) in synthetic scenarios, there is an evident lack of diversity in the benchmarks used. 

2. **Algorithm Complexity and Overhead**: Both CATS and PATS introduce additional complexity by incorporating distributional approaches and Thompson Sampling into MCTS. The paper does not sufficiently address the computational overhead or the scalability of these methods when applied to environments with larger state or action spaces. This could be crucial for understanding the practical deployment of these algorithms in real-world applications.

Limitations:
### Computational Demands
The authors recognize that the Categorical Thompson Sampling (CATS) distributional Monte Carlo Tree Search (MCTS) involves increased complexity due to the management and updating of probability distributions. This acknowledgment is crucial as it highlights a potential scalability issue, especially in environments where computational resources are limited or real-time responses are required.

### Fixed Precision
The approach used in the Particle Thompson Sampling (PATS) to manage the growth in the number of particles by fixing the float precision is a practical solution to prevent computational overload. However, this method may introduce limitations in the precision and adaptiveness of the model, potentially affecting the accuracy of value estimations in environments with high variability.

### Number of Atoms
The performance sensitivity to the number of atoms indicates a hyperparameter dependency, which could impact the effectiveness and robustness of the model. The authors mention that suboptimal choices in this hyperparameter may affect performance, suggesting a need for careful tuning and validation to optimize the model's accuracy and efficiency.

### Addressing Limitations
While the authors have outlined these limitations, the discussion could be expanded to include more detailed strategies for mitigating these issues, particularly the computational demands and fixed precision aspects. For instance, strategies to optimize computational efficiency or adaptive techniques to dynamically adjust precision based on the context could further strengthen the approach.

### Societal Impact
The paper does not explicitly address the potential negative societal impacts of the research. In the realm of reinforcement learning and AI planning, concerns such as the deployment in sensitive or critical environments, where errors may have significant consequences, should be considered. Discussions around ethical implications, misuse, and long-term effects would be beneficial.

### Suggestions for Improvement
1. **Enhanced Computational Strategies**: The authors could explore methods to reduce computational overhead, such as parallel processing or optimizing algorithmic efficiency, to make the model more practical for real-time applications.
   
2. **Dynamic Precision Adjustment**: Introducing mechanisms to adjust the precision of particle distributions dynamically based on the observed variability in the environment could help maintain balance between computational efficiency and model accuracy.

Overall, the authors should be commended for their upfront discussion of the limitations, but there is room for deeper analysis and additional strategies to address these limitations comprehensively.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces distributional return estimates to MCTS-based planning. For this the authors borrow from work on distributional Q-Learning and show how to adapt the MCTS value back-up and action selection steps to compute and utilise these distributions. They formulate two approaches based on different distribution representations (quantile and particle based) for which they provide some theoretical convergence analysis as well as first experimental results.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
The paper combines two well-established ideas in MCTS and distributional value approximation resulting in a new algorithm with better theoretical guarantees. The overall approach and implementation of this combination makes sense and should at least in theory overcome limitations in stochastic environments.

Though I was unable to verify all proofs in detail, the theoretical analysis seems to make sense and establish the advantages of the proposed methods.

Weaknesses:
Despite the soundness of the overall proposed method, I found the paper very hard to follow and felt details were missing due to an overall lack of focus. Contributing to this were the following issues:

1. Empirical Evaluation
Experiments are limited to a toy domain and results on the Atari benchmark reported in the appendix.
The toy domain is a tabular environment that is being generated randomly and contains stochasticity in both the final reward and transitions.
For an illustrating example this makes it hard to judge the combined effect on the overall return distribution to be approximated. 

How the combinations of branching factor and depths that were plotted were chosen is unclear to me. Beyond this I am not sure how meaningful these plots are. In the right most plot it appears as if the PATS approximates the root value almost correctly in under 100 simulations - at which point it could not even have tried all k = 200 actions available to it.

Also CATS appears to be doing consistently worse than some of the other methods despite having the same theoretical properties as PATS.

For the Atari baseline the authors make use of Q-networks and point to a related paper. However, the exact implementation details and hyperparameters are not discussed making it hard to reproduce this work based on the paper alone.

While stochasiticity and the exploration challenges this causes form one of the main motivations for this paper, no further ablations how the proposed methods improve here are presented.

2. Content division
The author devote a significant amount of space to the summarisation of MCTS and distributional RL. While the theoretical analysis is arguably the strongest part of the presented work only the main theorems are found in the main body of the paper with very little contextualization.

3. Overall presentation
There are several presentation issues in overall formatting, grammar and spelling. The former includes, but is not limited to overlapping lines, inconsistent / in-text section headers and wrong section references.

Limitations:
The discussion of limitations is restricted to a short paragraph in the appendix listing generic points such as increased computational demand and sensitivity to hyperparameter choices. However, no further investigation or explanation as to their severity is provided.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS) algorithms, which incorporate distributional reinforcement learning into Monte Carlo Tree Search (MCTS) to handle value estimation in stochastic settings. By modeling value functions as categorical and particle-based distributions and applying Thompson Sampling for action selection, the proposed algorithms aim to improve the robustness and accuracy of value estimates. The paper proves the theoretical effectiveness of these methods by achieving a non-asymptotic problem-dependent upper bound on simple regret of $O(n^{−1})$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea is interesting and original and the non-asymptotic problem-dependent upper bound on simple regret of $O(n^{−1})$ significantly advances the state-of-the-art from the previous $O(n^{−1/2})$.

Weaknesses:
1- While using distributional RL in MCTS to do Thompson sampling is interesting, it introduces much computation complexity hindering the applicability of the proposed algorithms.

2- The numerical experiments for the stochastic environments that are the main motivation of this work are done on a toy problem.


Minor comments

1- The presentation of the paper can be improved, specifically the parentheses () citation style can be confused with equations reference. 

2- Line 42, the authors mention V node for the first time without properly defining what is a V node.

Limitations:
1- The added high computational complexity from maintaining a distribution for each node in the MCTS.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ASA2jdKtf3;"REVIEW 
Summary:
This paper extends the framework of multi-agent influence diagrams (MAIDs) to explicitly capture complex forms of reasoning corresponding to Theory of Mind (ToM) as required for the interaction of Multi-Agent Systems with human users.  It introduces the framework of incomplete information MAIDs (II-MAIDs) for explicitly modeling higher-order beliefs in multi-agent interactions alongside probabilistic and causal dependencies between variables. Using results connecting EFGs to MAIDs, the authors demonstrate a natural mapping between strategies in the two frameworks that preserves expected utilities according to the agents’ subjective models.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The approach is well situated within the state-of-the-art of related work in agent models with a game theory component and, as far as one can judge, appears technically sound within the broad remit of causal and influence diagrams (IDs). 
The paper is very well structured and the authors did their utmost to keep it relatively accessible by alternating formal sections with intuitive descriptive summaries. It remains somehow tedious to read, owing to the large number of definitions, whose numbering alternate with that of theorems. 
The rationale for building a framework on top of MAIDs rather than EFGs is well introduced, together with the mapping between strategies in MAIDs and EFGs and the choice of working at the interim stage. This culminates with Theorem 20, until section 5.1 raises some issues around the relevance of Nash Equilibria.

Weaknesses:
The major issue I would raise for this paper is one of relevance to NeurIPS, even in the extended sense. While a major rationale for the paper appears to be its potential application to AI Safety, in the NeurIPS context there does not seem to be enough outreach to current AI models, at least in a way in which they could be interfaced to the proposed ID model. This means some consideration of how current models may form ‘beliefs’, and this was not entirely obvious from the paper’s Title and Abstract. Perhaps my expectation was unrealistic, but I had imagined an attempt to unify formal ToM issues with ToM properties that are known to be associated to LLM, under a framework where this approach would federate or wrap formal agentic methods around, say Agentic LLM. With this comment I am not criticising the authors for not having written another sort of paper, I am simply pointing the perceived gap that may exist between this approach and the NeurIPS constituency. Further evidence would be the absence of references to NeurIPS paper and the relative dearth of mainstream AI venues in the references (to the notable exception of AIJ). Overall, it appears that AAMAS might be a better venue to host this type of paper. 

The paper does not really clarify its ToM framework which references both “multi-agent interactions” as well as “higher-order intentional states” but these aspects are not part of further formal developments. It also mentions “belief hierarchies of arbitrary and infinite depth” and this raises the issue of whether such a formal approach is realistic when it comes to ToM, in particular in the interactions between agents and human users. 

Despite an early reference to AI Safety and a mention in the paper’s abstract, there is little in the paper that actually progresses the discussion on AI Safety, which is only used marginally through ID examples, such as the one of Figure 2.

Limitations:
The limitation section begins with a number of upbeat statements that would better be placed in the conclusion or parts of the abstract. The main identified limitation, which echoes the discussion of section 5.1 is verbatim: “The main limitation of our work is the lack of a useful solution concept.” appears a quite severe restriction. While not affecting the solid grounding of the approach it considerably restricts its impact at its current stage of development.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework Incomplete Information Multi-Agent Influence Diagrams (II-MAIDs) for modeling complex multi-agent interactions involving theory of mind (ToM) and higher-order beliefs. The authors prove the equivalence between II-MAIDs and Incomplete Information Extensive Form Games (II-EFGs) at the interim stage. The paper also shows the existence of Nash equilibria in II-MAIDs under certain conditions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The II-MAID framework fills a gap in existing game-theoretic models by allowing for inconsistent beliefs and higher-order reasoning. The paper is built on solid mathematical foundation with formal definitions and proofs.

Weaknesses:
* From my perspective, the proposed II-MAID framework appears overly complicated for modeling Theory of Mind (ToM), which is fundamentally a straightforward psychological mechanism observed in daily human interactions. The paper's approach may overcomplicate a concept that should be more intuitively represented.
* The paper introduces numerous assumptions and definitions without clear explanations which hinders the readability. As a non-expert in the field, some details in the paper are difficult to read. 
* It is unclear whether the model can be scaled and applied to larger, more realistic scenarios, where ToM takes place more frequently.
* The paper lacks experiments that validates the model.

Limitations:
N/A, see weaknesses.

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work extends the theoretical framework of multi-agent influence diagrams (MAIDs) with incomplete information (II-MAIDs) to explicitly capture this complex form of reasoning. The primary theoretical contribution is the proof of the existence of Nash equilibria, although, in general, these equilibria are impossible for agents to identify.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This work is game-theoretic in nature, and overall, the presentation quality is good and smooth to the best of my knowledge.

2. Although I think the assumption made in this work generally makes sense to me: agents have consistent beliefs as part of our commonsense, which can be derived from a common prior distribution, I agree that there are settings with no common prior available. The setup is a less constrained setup.

Weaknesses:
1. One of my major concerns is the audience of this work. Given that this work is submitted to the safe ML track of NeurIPS, I expect more discussion on the relevance of this framework to AI safety. The author should elaborate on what they imply by “safety” rather than making a very brief claim about its relevance in the related work and conclusions sections.

2. The discussion of theory of mind is also lacking, given that this is well-motivated. There have been extensive studies on machine theory of mind, ranging from early studies [1-2] to recent studies on LLMs [3-4]. There has also been research connecting Theory of Mind to Game theory [5] and Interactive POMDP [6]. See the survey [7] for details. Overall, this work needs significant improvement in discussing related work for readers to evaluate its contribution and relevance to NeurIPS.

[1] Rabinowitz, Neil, et al. ""Machine theory of mind."" International conference on machine learning. PMLR, 2018.

[2] Jara-Ettinger, Julian. ""Theory of mind as inverse reinforcement learning."" Current Opinion in Behavioral Sciences 29 (2019): 105-110.

[3] Sap, Maarten, et al. ""Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs."" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.

[4] Ma, Ziqiao, et al. ""Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models."" Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.

[5] Yoshida, Wako, Ray J. Dolan, and Karl J. Friston. ""Game theory of mind."" PLoS computational biology 4.12 (2008): e1000254.

[6] Çelikok, Mustafa Mert, et al. ""Interactive AI with a Theory of Mind."" Computational Modeling in Human-Computer Interaction. 2019.

[7] Albrecht, Stefano V., and Peter Stone. ""Autonomous agents modelling other agents: A comprehensive survey and open problems."" Artificial Intelligence 258 (2018): 66-95.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
C3t6GMPnC5;"REVIEW 
Summary:
This paper explores the capabilities of Mamba state-space models (SSMs) in comparison to Transformer large language models (LLMs) in various downstream learning tasks. Despite Mamba's success in some areas, the paper identifies challenges and limitations in achieving performance parity with Transformers on standard benchmarks, particularly in in-context learning (ICL), mixed-precision fine-tuning (MPFT), and parameter-efficient fine-tuning (PEFT). The study demonstrates that while Mamba models have robust recurrent dynamics and can achieve significant speed and memory efficiency gains through fine-tuning techniques, their downstream learning improvements still lag behind those of Transformers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to read.

2. The study shows that Mamba’s recurrent dynamics are robust to small input changes, which is validated both theoretically and empirically. This robustness ensures stability in training and fine-tuning processes.

3. Despite initial shortcomings in ICL performance, extensive experiments demonstrate that mamba models exhibit strong potential for improvement through efficient fine-tuning. The models can achieve up to 81.5% of the ICL performance improvement, highlighting their adaptability with appropriate tuning methods.

Weaknesses:
I appreciate the authors for providing a theoretical analysis to demonstrate the controllability of implementing AMP on Mamba blocks, and the experiments indicate that PEFT is also suitable for Mamba. However, I have several concerns:

1. The authors define the Mamba process as a generalized operation: $x_t=F_{\theta}(x_{t-1},u_t)$, but the actual output of Mamba is $y_t = \bar{C_t}x_t$.Therefore, the theoretical analysis provided in the paper pertains to the stability of the hidden state under small perturbations. Is it possible to extend this analysis directly to the output $y_t$? Since the stability of the hidden state does not necessarily imply the stability of the output.
2. Theorem 1 ensures the feasibility of implementing LoRA on Mamba blocks but focuses on the $W$ matrix, neglecting the consideration of the most crucial transition matrix $\bar{A_t}$ in Mamba. Does this mean that the $\bar{A_t}$ matrix was not subjected to LoRA during fine-tuning? If so, is it possible to consider applying PEFT to the $\bar{A_t}$ matrix as well?
3. As an empirically-driven paper, would it be possible to include more backbones for comparison in future versions? Currently, the only baseline for comparison is Pythia.

Limitations:
The authors raise some limitaions, for example, the lower-precision method is not explored in this paper, but the authors claime to solve them in the future.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores Mamba's downstream learning capabilities through two primary aspects: (i) fine-tuning and (ii) in-context learning. Specifically, it examines the training stability and robustness of fine-tuning when mixed precision is applied, as well as Mamba's ability to perform in-context learning. The contributions of this paper include:

- Theoretical analysis of the stable dynamics of Mamba.
- The theoretical analysis is corroborated by the experiments.
- Experimental demonstration of Mamba's limitations on real datasets in terms of ICL.
- ICL performance improvement of Mamba through fine-tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The theoretical analysis section, although I did not verify the proofs, offers valuable insights and paves the way for more complex analyses in future work.
- This paper is well-motivated.

Weaknesses:
The weakness is majorly from the ICL part. 

In fact, the authors show that pretrained Mamba cannot learn well via ICL, but can learn well after fine-tuning. This fact indicates that this limitation does not come from Mamba architecture itself, which is also consistent with the observation of other works such as [1]. Therefore, the limitations observed in this paper is just a general limitation caused by training recipes, which is not Mamba-specific and has been studied in many works. The solution is also standard, and the improvements are also expected since once trains well, Mamba should be able to perform in-context learning as shown in [1]. 

Moreover, many questions are still unclear. For instance, why does Mamba suffer from such limitations? 

Therefore, the study in terms of the ICL part is lack of depth, novelty, and technical contribution. 

In terms of the mixed precision part, there are also many places that are unclear to me. I suggest the authors to use more space in discussing the speciality of Mamba compared to Transformers, and what structure of Mamba caused this problem. If recurrence is the main cause of the problem, having more experiments of similar models such as GLA, linear attention, etc would also help readers to understand more about the phenomenon. 

---

References

[1] Park, Jongho, et al. ""Can mamba learn how to learn? a comparative study on in-context learning tasks."" ICML 2024.

Limitations:
The work is well-motivated, but the study lacks depth.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper looks at improving start space models or Mamba by enabling mixed precision handling to improve inference and fine-tuning. The results show similar performance with a significantly reduced memory requirement

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
There are extensive results compared to full-precision models
The authors provide a proof of the theorem to back up their claim
The change to the mamba block is clear and easy to implement by others

Weaknesses:
The actual change is relatively minor in quantity but does deliver the author's required memory reductions.
The works don't use the larger models available due to limitations on memory requirements still

Limitations:
the limitations are well discussed at the end of the paper and generally relate to LLM or transformers in general too

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
BQEOJZ0aSD;"REVIEW 
Summary:
This paper presents a new method which directly encourages ensemble diversification on selected ID datapoints without the need for a separate OOD dataset. They also introduce a new measure of epistemic uncertainty which measures the diversity of the final predictions of each model, and suggest a speedup of comparing pairwise disagreement via random sampling.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written, and the presentation of the method is easy to understand. 
- The experiments cover a wide range of OOD datasets.
- The methods are intuitive and can be inexpensively applied to existing ensemble diversification algorithms.
- SED-A2D outperforms other baselines when using uniform soup or prediction ensembles for OOD generalization, and also achieves the highest AUROC for OOD detection.

Weaknesses:
- It appears that utilizing this new training objective leads to a loss in ID accuracy, since it encourages members of the ensembles to diverge. This tradeoff between ID accuracy and OOD accuracy may not be desirable in many settings. Overall, the paper emphasizes the improved OOD performance but does not show its impact on ID data for many experiments, such as the ablation studies for OOD detection, model diversity, etc.
- The stochastic computation of pairwise disagreement seems incremental, and there is no work comparing this stochastic implementation with the traditional expensive one. It would be helpful to include an ablation study to understand the accuracy vs performance tradeoff.
- There are many other methods for OOD detection beyond MSP with BMA (eg [1]). How PDS does compare against other baselines?
- Deep ensembles remain competitive in many settings, and the best values for C-1 and C-5 OOD generalization are still achieved using ensembles. 

[1] Xia and Bouganis: On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution Detection https://arxiv.org/abs/2207.07517

Limitations:
- The approach sacrifices ID accuracy for OOD generalization/detection.
- The experiments only showed the result of finetuning the last two layers.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper aims to train a diverse ensemble of models via a framework called Scalable Ensemble Diversification. This framework does not require an additional dataset of OOD inputs, as it identifies OOD samples from a given ID dataset. It then encourages the ensemble to return diverse predictions (disagreement) on these OOD samples. Furthermore, the framework makes use of stochastic summation to speed up the disagreement computation. Results are shown for different tasks like generalisation, OOD detection on different OOD datasets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The high level idea of removing the need for a separate OOD dataset and speeding up the diversification computation can be useful in practice.
- The writing is clear and easy to follow.

Weaknesses:
1. It is not clear why the method works, additional ablations studies would be useful.   
    - Naive A2D (and DivDis) uses IN-R data to compute the disagreement loss, which could give the method an advantage as it has access to the OOD data. However, it has a lower accuracy on IN-R. There seems to be two methodological differences between A2D and SED-A2D, the OOD data and use of stochastic sum. Given that A2D computes the full pairwise disagreement and stochastic sum is meant to reduce cost rather than improve performance, why does SED-A2D perform better? It would have been useful to compare two methods that only differs on the OOD data used. E.g., SED-A2D without the stochastic sum.   
    - From eqn 6, it looks like the two terms have contradicting objectives. For a “OOD” point, the first term encourages all models to classify the point correctly, but the second term encourages models to have different predictions on the same point. These objectives can be challenging to balance.   
2. The writing clearly explains the method or setup, but sometimes stops short of giving further insights. For example,  
    - Further analysis of experimental results   
        - Table 2 why does having more ensemble component (5→50) make the SED-A2D results worse? Similar trends can also be seen in Tab 4 for C-1 or C-5. Could it be because the stochastic sum does not scale with more models?   
        - Why was #unique used in Tab 1 to measure diversity when the Predictive Diversity Score was just introduced?  
        - Why does oracle selection perform worse compared to simple average in Tab 2? I would expect otherwise given that there is privilege information.   
    - Components of the method can be better motivated   
        - Why was the A2D loss chosen instead of other losses e.g. DivDis?   
        - Why is optimizing Eqn 6 preferable to e.g., forming an OOD dataset from the ID data based on the errors of DeiT or even from the errors from an ensemble of models, similar to imagnet-a, and using existing techniques like [23,28].   
    - “collecting a separate OOD dataset can be very costly, if not impossible”. 
        - There are cheap ways to introduce OOD samples to an ID dataset, e.g., simple augmentations/transformations to the input. Why are these methods not preferable?  
4. One of the main contributions involves speeding up the disagreement computation. There does not seem to be experimental details or results on this. E.g., a subset of models is chosen, what is the size of this subset? How does performance for generalization/detection change with and without this speedup?

Limitations:
Yes the limitations were adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents SED, a method for scaling up existing diversification methods to large-scale datasets and tasks. SED identifies the OOD-like samples from a single dataset, bypassing the need to prepare a separate OOD dataset. Experimental results demonstrated good performances by SED on the OOD generalization and detection tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. SED scales up existing ensemble methods.
2. According to the author's experimental results, SED demonstrates its application to OOD generalization and detection at ImageNet level.
3. Simple method, and easy to understand.

Weaknesses:
1. I am very confused about the dataset division for OOD detection task in the paper. The distribution should refer to “label distribution” in OOD detection [1], which means that OOD samples should not have overlapping labels w.r.t. training data. In the paper, the ID dataset is ImageNet-1K, while the OOD dataset for the OOD detection task includes ImageNet-C (Table 3). Their label spaces overlap, which is clearly incorrect. I don't believe the experiments conducted in this paper fall under the category of OOD detection. I suggest the authors refer to relevant literature on OOD detection, such as OpenOOD [1].

2. The ablation studies are insufficient. For instance, the number of layers being diversified is a hyperparameter. I believe conducting ablation experiments on this would make the paper more solid.

3. I think the experiments in the paper are not comprehensive enough. For example, how does it perform on small-scale datasets? Although it may not be fair to compare with methods using real OOD datasets, this could provide insights into SED's performance from multiple perspectives.

4. The comparative methods in the paper are not comprehensive enough. How does it perform compared to existing OOD generalization and OOD detection methods? If SED is complementary to existing methods, how much improvement can it bring?

5. The paper claims to speed up pairwise divergence computation, but no results are shown. Could authors demonstrate specifically how much speedup was achieved?

6. typos:
6.1 Line 61: ""We verify that SEDdiversifies a model...""
6.2 Line 68: ""In all three cases, SEDachieves a superior generalization...""

[1] Yang et al, OpenOOD: Benchmarking Generalized Out-of-Distribution Detection, IJCV 2024.

Limitations:
I think the author should discuss more about the limitations of the method proposed in the article, such as the computational time required for the method proposed in the article compared with other methods.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Ensembles of diverse models have shown promising signs for out-of-distribution (OOD) generalization.
To boost diversity, some methods require a set of OOD examples for measuring the disagreement among models.
The desired OOD examples, however, can be difficult to obtain in practice.
This paper proposes to dynamically draw OOD samples from the training data during training.
This is done by assigning a higher OOD score to examples with a greater loss in each mini-batch.
To make the diversification process across multiple models more efficient, the authors propose a stochastic approach that only diversifies a small sample of models at each iteration.
The resulting diversified models give rise to the notion of a diversity score for uncertainty estimation and can be used for OOD detection.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper introduces several reasonable improvements to a state-of-the-art method, A2D, making it more scalable and practically feasible.
- The empirical performance looks good. It is a bit surprising that the proposed method can outperform A2D which has access to “true” OOD datasets.

Weaknesses:
- The notion of “OOD samples” in an ID dataset is confusing. The actual implementation, i.e. assigning a higher “OOD-ness” weight to training examples with a greater loss, is more like identifying “hard” training examples rather than just OOD samples. Calling them “OOD samples” somewhat obfuscate their nature. They are not arbitrary OOD samples but hard samples within the support of the ID dataset. It is not obvious why diversifying models’ predictions on such samples would help. Is such prediction diversification always conducive to OOD generalization? If not, when would the proposed method work or break? These relevant theoretical questions are not answered satisfactorily in the current manuscript.
- The connection between SED and PDS is weak; PDS is not well justified. The A2D diversification loss can also be seen as a measure for prediction diversity, like PDS. Why choose PDS instead for OOD detection? Furthermore, is PDS really a good measure for epistemic uncertainty? Imagine two cases. In the first case, two models confidently (with probability 1) predict the same class for an input example, while in the second case, the two models assign uniform probability to all classes for another example. The PDS for these two examples are exactly the same, yet the models are much less confident (or more uncertain) in the second case. Meanwhile, BMA does not have this issue.
- The baselines are relatively limited. There are many other diversification methods which do not require a separate OOD dataset [1, 2, 3]. How does the proposed method compare with these methods? Can the authors also comment on why BMS is the only considered baseline for OOD detection?
- The definition of #unique values is not very clear. Table 1 shows SED-A2D has extremely large #unique values. On C-1 dataset, the value is 5, the maximum possible value. If my understanding is correct, does this suggest that all the 5 models disagree with each other on every C-1 example? If so, this suggests that for many examples, 4 out of 5 models are probably wrong. Why is this more of a good sign than a bad one?

[1] Rame, Alexandre, et al. ""Diverse weight averaging for out-of-distribution generalization."" Advances in Neural Information Processing Systems 35 (2022): 10821-10836.  
[2] Chu, Xu, et al. ""Dna: Domain generalization with diversified neural averaging."" International conference on machine learning. PMLR, 2022.  
[3] Lin, Yong, et al. ""Spurious feature diversification improves out-of-distribution generalization."" arXiv preprint arXiv:2309.17230 (2023).

Limitations:
The authors only briefly mentioned two limitations of the work. I don't notice any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes Scalable Ensemble Diversification (SED) to extend existing diversification methods to large-scale datasets and tasks where ID-OOD separation may not be possible, and also propose Predictive Diversity Score (PDS) as a novel measure for epistemic uncertainty. Extensive analysis and experiments support the effectiveness of the proposed modules.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The logic of this paper is very clear, the motivation is reasonable, and the proposed method has been proven to be effective in analysis and experiments. The figures and tables in the paper are also relatively clear.

Weaknesses:
1. Although the experiments are diverse, I am not sure if the comparison is comprehensive. Can more explanation and discussion be added?

2. The feature extractor used is frozen. Is the proposed method robust enough to different feature extractors? What will the performance be if the feature extractor is also involved in the training?

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
B3jt0Ran2t;"REVIEW 
Summary:
The paper describes the problem of matching students to daycare centers, with each family allowed to express preferences about the joint allocation of all siblings within the family. The authors present a modified notion of a stable matching, in which a family may choose to withdraw one of its children from a daycare in favor of a different child, so long as the daycare still prefers this assignment among alternatives with the first child removed. Under this stronger notion of stability, they show that the existing SDA algorithm may produce unstable outputs. They present an extension to the algorithm ESDA, whose successful outputs meet the new stability condition. They also show that the algorithm will be successful with high probability for a particular distribution over problem instances. In this distribution, children populate a daycare preference order by selecting from a fixed distribution over the daycares, and families aggregate these preferences into preferences over joint allocations, using an arbitrary aggregation function. Daycares sample a preference order over children from a Mallows model, with low dispersion. 
Finally, the authors show some empirical results from real Japanese municipalities in which ESDA produces stable matchings in all cases.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The algorithm is a heuristic (for good reasons of computational hardness of the problem). Hence, I characterize the contributions as follows:
* The authors define the problem, generalizing from couple-matching instances that may be expressed as families of size at most 2
* The authors present a new notion of stability in the matching which seems to be justified, given that families are generally empowered to prevent one of their children from attending a particular daycare.
* The authors present the ESDA algorithm, which internalizes the new stability notion in the details of the algorithm execution, and produces stable matchings whenever the algorithm succeeds
* Empirical analysis shows the algorithm succeeding on real-world instances
* Finally, the authors show that real-world instances have some strong properties in terms of the similarity across daycares of the preference ordering for students. They also incorporate this observation into the algorithm design, and are able to show that under a certain random model of problem instances, the algorithm succeeds with high probability. In the real-world examples, the preferences of the daycare are largely provided by the municipality, so the assumption is very likely to hold.
* As a smaller point, I appreciate that the authors presented some analysis of the behavior of the algorithm when the dispersion of the mallows models becomes high.
* An additional smaller point: earlier results that operate with a vanishing fraction of couples in the population seem unsatisfying. The theoretical results in this paper instead allow a constant fraction of the families to have siblings, but place stronger constraints on the similarity of orderings of the daycares, which seems better justified.

Weaknesses:
My first question is about goodness of fit of the paper to NeurIPS. The best fit from the CFP is:
* Social and economic aspects of machine learning (e.g., fairness, interpretability, human-AI interaction, privacy, safety, strategic behavior)
specifically for strategic behavior. However, I'm not sure this should be called economic aspects *of machine learning* specifically. I'll leave this issue with the area chair---my personal view is that it's not a great match. There are some related papers that have appeared in past conference instances (for instance, on deferred acceptance variants, but with more of a focus on computational complexity of an algorithmic approach, such as https://papers.nips.cc/paper_files/paper/2019/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html).

Second, I'm concerned about the family preference model in section 4.1. In particular, the title of the paper says ""Large Markets,"" and as the size of the market grows beyond a small geographic area, it seems like that geographic preferences (for nearby daycares) will play a role. However, the random model is based on a single global distribution of preferences that applies to all families across all locations. This distribution is then further constrained to place similar probabilities on all daycares. There is no analysis of the empirical data to justify this uniformity assumption. Additionally, the model assigns independent preferences to two siblings of the same family, which seems to miss a) the fact that a family may have certain specific desires, and b) the family's geo preferences will apply similarly to all children, and c) sending multiple siblings to the same daycare may provide complementarities such as less logistic overhead for transport.

Limitations:
I think the authors have done a good job here.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study a variant of the many-to-one matching problem called the stable matching problem with siblings, which generalizes the stable matching problem with couples. In this problem, some families $f \in F$ may have more than one and at most $k$ siblings, ordered by age $(c_1,\dots,c_k)$. Each family $f = (c_1,\dots,c_k)$ expresses a joint linear preference order for daycares, denoted as 
$>_f \subseteq D \times D \dots \times D$,

 where $>_{f,j} = (d_1,\dots,d_k)$ represents the $j$th preference of family $f$, and $d_i$ corresponds to the preference for child $c_i$. Note that $>_f$ is an ordered set—a tuple. Each daycare $d \in D$ expresses a linear preference order $>_d \subseteq C$ for a subset of children and a maximum capacity $Q(d)$.

The objective is to find a (stable) matching such that no blocked pair exists. In essence, a blocked pair is a tuple (of edges) $(x_1,\dots,x_\ell)$ and $(y_1,\dots,y_\ell)$ such that swapping $x_i$ with $y_i$ results in a new matching that assigns children to daycares with higher priority for at least one family while not negatively impacting the assignment of any other family or daycare preferences.

A stable matching might not exist for restrictive settings of the problem, as the authors illustrate with a simple example in Appendix B.3. My understanding is that if the preferences form cycles, it becomes impossible to find a feasible solution that satisfies these preference constraints. However, in a daycare market where priorities are generated from a specific distribution, particularly random, the authors demonstrate that the probability of a stable matching existing converges to $1$ as the number of children $n$ approaches infinity.They present algorithms to solve the problem and conduct experiments on synthetic and real-world datasets, demonstrating that they can find feasible solutions in most instances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses a challenging and relevant variant of the many-to-one matching problem, the stable matching problem with siblings. The authors provide a comprehensive approach by defining the problem, presenting algorithms to solve it, and conducting thorough experiments on both synthetic and real-world datasets. Their work not only demonstrates the feasibility of finding stable matchings under specific conditions but also highlights the practical applications and implications for real-world daycare allocation scenarios.

Weaknesses:
While the paper makes significant contributions, there are some areas that could be improved. The writing is occasionally imprecise, making it challenging to follow the arguments and understand the definitions clearly. In particular, the choice of notation can be confusing (see detailed comments and questions). The structure of the paper is somewhat disorganized, with most of the proofs deferred to the appendix. Considering the strict page limits, this may be reasonable. However, Sections 3 and 4 could be compressed and written more concisely, and some proofs (or at least proof sketches) can be included in the main paper. I have only reviewed the proofs at a high level and have not verified the claims in sufficient detail. Given the strict reviewing timeline, this is the best I can do.

Limitations:
The authors do not discuss limitations and potential negative social impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the problem of daycare matching with siblings, an extension of matching with couples. Here, children in families (of size 1 or larger) are matched to daycares. Families have ranked preferences over the tuples of daycares their children end up at (since their preference for one child at one daycare may affect their preference of another child at some daycare), and daycares have preferences over children. In most cases, daycares do not differentiate between children within a family. This is an important problem to solve in Japan, and the authors actually worked with the Japanese daycare matching market in order to produce this work.

Their contributions are: 1) introduce the problem along with notions of rationality/stability/assumptions/etc, 2) propose an extended sorted deferred acceptance algorithm and prove that it will only return stable matchings and will fail to recognize a possible stable matching with probability approaching 1 as the problem grows, and 3) run experiments on their algorithm.

Their model is defined in a pretty standard way according to stable matching literature. The novelty, of course, is the introduction of families generalizing the size of couples. Their stability definition uniquely allows children in the same family to pass along seats to each other, so that a family may use that to their advantage in forming a blocking coalition. They assume that daycares have similar rankings over children and that they are drawn according to the Mallows Model, and that families only have few daycares they are interested in.

The algorithm itself works much like deferred acceptance. First, single children can propose to daycares per usual. Then, families with multiple children begin proposing, presumably according to their full ranking of matching tuples. When a single child is unseated from a daycare, they can simply propose to their next choice. When a family f has an unseated child when family f' is processed, the algorithm attempts again under a new order where f' goes before f. This can cause many iterations.

In the experiments, they use real datasets from Japan as well as larger synthetically-generated datasets. They compare their algorithm to a baseline constraint programming solution, showing that their algorithm returns the same solution faster.

Quick note: diameter is introduced in the main body but only used in the appendix. Perhaps move it to the appendix.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Stable matching is a very well-respected area of research, and this seems like a very natural formulation of the problem. It is particularly interesting that the authors are working directly with the market in need and they seem to have been given positive feedback about their work, so this work will almost definitely have a valuable use case. For the most part, the paper is written very well and it is very easy to get a high level understanding of most aspects of the project. Overall, I am very pleased with this paper and would be excited to see it at NeurIPS.

Weaknesses:
I am a bit concerned about the literature review provided. I am aware there is much more research that has been conducted on matching markets with complementaries (I am not knowledgeable enough to know what papers would be most useful), and I know there are various papers in this field. However, very few previous works are cited in this paper. It would be great if the authors could clarify the place of their work in the context of current literature and give confidence that this problem or a generalization of it has not already been studied. In fact, this is very important to motivate the paper.

Otherwise, there are a few points in the paper that are unclear. Much of it is very high level and lacks details, which is okay because it writes a narrative, but it comes at a cost of understanding the details of the proofs. More notably, I think the authors didn't spend enough time explaining their algorithm. I found it somewhat vague and I was uncertain about how it worked, and yet it is an integral part of the paper. This definitely needs to be improved.

Limitations:
Everything seems adequate.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the existence of a stable daycare-children matching in the presence of siblings from the same families with same preferences over the daycares. The authors particularly study the case when the daycares have similar preferences over the set of children, and the market size is large. They propose a variant of the Sorted Deferred Acceptance algorithm to compute the stable matchings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The problem is well motivated by the real-world observation that stable matchings exist in the markets as opposed to what the theory suggests. This observation allowed the authors to make necessary adjustments to the assumptions that are sufficient for the theory to work out.
2. The authors take a systematic approach to the problem. They first define a new notion of stability that takes siblings into consideration and show that stable matchings may not exist in the presence of siblings and that the previous algorithms do not work for this new notion of stability. They then consider a specific random daycare market, mention the drawbacks of the existing methods of computing stable matchings, and then prove that a modification to the existing algorithm can find stable matchings with the new definition of stability.
3. The results by themselves are quite interesting; that stable matchings exist even in the presence of siblings with complementaries.
4. The analogy is drawn between the related work in stable matchings with couples and stable matchings with siblings

Weaknesses:
1. The assumption that day cares have similar priorities over children is slightly unrealistic.
2. The random daycare market for which the results are derived is somewhat restrictive.

Limitations:
Limitations sufficiently addressed by the authors.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
B1tCaKP5nB;"REVIEW 
Summary:
The authors proposes a method for testing conditional independence in presence of discretisation.
They assume the variables to be jointly Gaussian, and that some of them are accessible only after discretisation; thus the data contain a mix of discrete and continuous variables.
Discretization might remove some conditional independencies. Assume X1 to be independent from X2 given X3.
It might be that X1 becomes instead dependent on X2 given \tilde{X3}, where \tilde{X3} is the discretised  X3.
The authors develop a way to infer the latent correlation on the real value variables and they propose a novel test for conditional independence for  the setting mixed continuous and discrete variables.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Testing conditional independence with mixed types of variables is an interesting topic and the work is original. 
The presentation is good, even though I could not follow the development of the bridge equations (this might be because I am not familiar with the adopted techniques).

Weaknesses:
* I am skeptical about the specific research question addressed.
X1 and X3 are independent given X2; yet they might be not independent given the discretised version of X2.

For instance, with ref to  Fig 1a,   X1 and X3 are formally dependent given \tilde{X2}; yet the induced dependence might be very weak.
I argue that the strength of the induced dependence depends on how the discretisation is done.
Example: X2 is human height, discretized into bins of few centimeters; then \tilde{X2}  is practically as informative as X2 and the induced dependence is likely to be negligible, in which case it might be sensible not rejecting H0.

The author did not discuss the impact  of the adopted discretization approach on the induced dependence.
Also, there is no compelling example in which discretisation induces a strong dependence.


* In the first set of experiments the test is better calibrated than the competitors, but it has by far less power. Overall, these results are not very strong.


* The competitor tests (Z-test and chi-square) are not modern.
There is no comparison against  existing tests for mixed variables; I can cite for instance Bayesian Independence Test with Mixed-type Variables, Benavoli et al. 2021.
Another simple baseline which I think should be present: test conditional independence having discretized all variables and use modern test for discrete variables (as a starting point, I suggest  those available in bnlearn https://www.bnlearn.com/documentation/man/conditional.independence.tests.html )

Limitations:
No potential negative societal impact.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Authors propose a test for conditional independence in case of discretized variables, i.e. variables that originally were defined over a continuous domain and are then mapped to a discrete domain. In this case, a binary domain. Authors propose to bridge the unobserved continuous variables with the observed discretized variables with equations modeling the original covariance/precision matrix coefficients. Both theoretical and experimental evidence support the proposed testing  methods. 

Typo at page 13, line 491, equation 17, missing closed bracket.

Citation 3, 4 are the same reference.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper contributes in a significant way on a topic which is crucial in the context of structure learning/causal discovery. Specifically, the proposed theoretical framework is solid and sound, explaining the logical steps that lead to the conditional independence test. The major strength points of this contribution are:

- The self-contained graphical representation of the discretized variables and their original continuous ones,
- The flexibility of the bridge equations, that can be adapted to specific cases without compromising the theoretical soundness.
- The performance of both the unconditional and the conditional independence test.

Weaknesses:
The only weakness is that I would do more experiments.

Limitations:
The only limitation, that is also discussed by authors, is that the ""discretized"" variables are in fact ""binary"" variables, which limits the applicability of the proposed test.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a novel statistical method for testing conditional independence (CI) when some of the data is discretized. Initially, the authors introduce bridge equations to estimate covariance and establish asymptotic normality, facilitating an unconditional independence test. For the conditional independence test, they employ nodewise regression to recover precision coefficients. Theoretical analysis and empirical validation are provided to showcase the method’s effectiveness.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper introduces a conditional independence test tailored for scenarios with discretized data, which often encountered in financial analysis and healthcare due to data collection or measurement constraints. The CI test is highly adaptable, capable of handling situations where both variables are discretized, both are continuous, or one is discretized. Numerical experiments on both synthetic and real-world datasets demonstrate superior performance in various scenarios.

Weaknesses:
The development relies on the assumption of a multivariate Gaussian distribution, which is rather stringent.

Limitations:
The development relies on the assumption of a multivariate Gaussian distribution, which is rather stringent.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses a critical issue in Conditional Independence (CI) testing methods, specifically when the available data is a discretized version of the original continuous data. Traditional CI testing methods often assume that discretized observations can directly substitute for continuous variables, leading to erroneous conclusions. To overcome this limitation, the authors introduce a novel CI test tailored for discretized data. The key innovation lies in using a bridge equation and nodewise regression to estimate the precision coefficients that reflect CI relationships among latent continuous variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper tackles a highly relevant and important problem within the realm of statistical analysis and CI testing.
The paper is well-written and presents the concepts clearly.
The proposed method is novel, and the theoretical contributions are solid, providing a robust foundation for CI testing in discretized data settings.

Weaknesses:
Assumption of Multivariate Normality: A primary limitation is the assumption that the data follow a multivariate normal distribution. This assumption simplifies the derivation of bridge equations for unconditional independence testing and the use of nodewise regression for the CI test. However, it restricts the applicability of the method to this specific class of variables. It is unclear how the method would perform with unknown or non-normal variables.

Discretization Modeling: The paper models discretization as a binarization operation applied to observed variables. This assumption may not hold in all practical scenarios. The performance of the proposed method on datasets with different types of discretization (beyond binarization) remains unexamined and is an important consideration for real-world applications.

Empirical Results: According to the empirical results, the proposed Discretized CI Test (DCT) shows smaller power compared to baseline methods. This indicates that while the method is innovative, its practical effectiveness in terms of power may be limited in some scenarios.

Limitations:
See above comments.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
AthZ2g6VE2;"REVIEW 
Summary:
This paper proposed LoCoDL, an algorithm than combines communication compression with local training. The authors proved the convergence results under regular assumptions, achieving comparable rate with existing SOTA algorithms. The experimental results also show that LoCoDL behaves best among tested algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The combination of communication compression and local training is novel.
2. The convergence results achieves SOTA for large $n$ and nearly SOTA for small $n$.
3. The algorithm behaves empirically better than ADIANA, an existing theoretically SOTA algorithm.
4. The algorithm is simple.
5. The target problem setting is novel and general.

Weaknesses:
1. Throughout the four experimental settings, the number of nodes, $n$, is comparable to the feature dimension $d$. As the convergence rate of LoCoDL is suboptimal when $n$ is small, I believe it important to compare LoCoDL with ADIANA when $n$ is at least 10$\times$ or 100$\times$ smaller than $d$ to see whether LoCoDL beats ADIANA in these scenarios.
2. The experimental datasets are relatively small. It's recommended to conduct experiments on MNIST or larger datasets.
3. It is not easy to capture the intuition behind each algorithm line. It's recommended to give more detailed explanations on how the algorithm is developed.

Limitations:
As stated in the conclusion part, the algorithm is limited to single-directional, deterministic setting without partial participation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes LoCoDL, a new GD-based distributed training algorithm that employs both communication compression (CC) and local training (LT). It achieves double acceleration and a SOTA convergence rate for strongly convex problems.

A crux of the algorithmic improvement is maintaining two local estimates, intuitively enabling efficient LT (similarly to SCAFFOLD) and efficient CC (i.e., compressing values' differences instead of values themselves).

The paper offers a thorough theoretical analysis that proves the main claim and conducts some experiments that show LoCoDL's benefits compared to previous algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is timely and important, and I enjoyed reading it. It appears to set a new bar for distributed communication complexity in the strongly convex case (and with full participation?).

While some works assume similarity between local client functions, this work allows these functions to be arbitrarily different.
Also, interpreting the added term $g$ is intuitive and compelling (viewpoints 1-4). 

The theoretical claims are rigorously proven, and some experiments demonstrate the efficiency of LoCoDL compared to previous techniques.

Weaknesses:
The practical applicability of LoCoDL is unclear. Namely, it does not apply to NNs and possibly is less efficient for partial participation use cases (this part is unclear). Either providing concrete evidence of why this contribution is important for modern practical use cases or slightly rephrasing the paper as a theoretical (and important) contribution would strengthen the claim.   

Strengthening the evaluation section is also advised. The submission would be strengthened if the author provided an experiment other than logistic regression to demonstrate the efficiency of LoCoDL in another task.

Additional points: 

1.	“are smooth,  so their gradients will be called. “ This sentence is unclear.

2.	“is slower than  broadcasting the same message to an arbitrary number of clients.” Are there any real FL systems that employ broadcasting? Or do you mean sending the same message? (The term “broadcast” may be confusing here.)

3.	“In this work, we focus on the  uplink communication complexity, which is the bottleneck in practice.“ The second part of the sentence should be softened or extended with real evidence that this is the case. 

4.	“No other compressor can be used, which notably rules out any type of quantization.” Why is this the case? Why quantization cannot be applied according to the selected pattern? 

5.	“Instead of the cumbersome permutation-based compressor of the latter.” is there a specific challenge in implementing permutation-based compressors? Maybe it's worth specifying specific setups where this is insufficient or cannot be applied.

6.	“Thus, LoCoDL sets new standards in terms of communication efficiency. “ Do you mean: our experiments indicate that...?

7.	What is the communication complexity with partial participation (PP) (i.e., $\rho=1$)? When considering PP, is LoCoDL the current SOTA, or are there better alternatives? 

8. It seems that some elements of LoCoDL have some similarities to DoCoFL [1], which also uses an anchor for the model that allows clients to obtain only a compressed correction to that anchor. Can the authors shed light on this similarity?

[1] Dorfman, Ron, et al. ""DoCoFL: Downlink compression for cross-device federated learning."" International Conference on Machine Learning. PMLR, 2023.

Limitations:
The paper does not have a dedicated limitation section. The conclusions section discusses potential future extensions. Outlining the limitations clearly is advised. For example, is the PP use case relevant here?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an algorithm (LoCoDL) that leverages two well-known methods of local training. It reduces the communication load in distributed learning.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper addresses the interesting problem of distributed learning.

Weaknesses:
1. Generally, compressing the error and feeding it back to the updates is a well-known technique to reduce the variance in distributed learning. The idea of the algorithm is marginal with respect to the previous known algorithms (feeding back the error and aggregating with proper coefficients). 
2. Also, the experiments should include the accuracy versus iteration (or time) to see after how many iterations (or how much time), the performance shown in Figure 1 is achieved. So, there are lots of work to improve the experimental part.

Limitations:
justification, experiments

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
AZuONuYzKl;"REVIEW 
Summary:
The paper addresses a major challenge in biology: identifying evolutionary traits, which are features common to a group of species with a shared ancestor in the phylogenetic tree. Compared to the existing works, this submission proposes new architectures and loss to avoid the over-specification problems. In the experiments, the authors demonstrate that the proposed method improves existing works and set up ablation studies to show the impact of different components of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
[+] The paper introduces HComP-Net, a new architecture designed to discover evolutionary traits from images in a hierarchical manner. This addresses the limitations of current prototype-based methods that operate over a flat structure of classes.
[+] Together with the architecture, the paper proposes contrastive loss and several additional losses to improve the performance.
[+] The inclusion of a novel masking module allows for the exclusion of over-specific prototypes at higher levels of the tree without compromising classification performance. This helps maintain the accuracy and effectiveness of the model.
[+] The proposed method not only improves the accuracy and other metrics, but also shows the generalizability to unseen species.

Weaknesses:
[-] More background: For most of the machine learning conference readers, I guess the proposed problem background is required. Therefore, more related work and background sections should be useful. 
[-] I wonder whether the proposed framework can address ""Convergent evolution"" and other similarity cases. Since these species can have similar features but should not be very close in the evolutionary trees. I suggest the authors to include more details and discussions about the background knowledge.
[-] While the framework has shown promising results on datasets of birds and other animals, I wonder whether the method can show its scalability to larger and more diverse datasets.

Limitations:
I do not think this work has potential negative social impact. The problems sounds very interesting.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a novel deep learning based algorithm named HComP-Net that can detect evolutionary traits common to groups of species with shared ancestors. Based on earlier studies, they aim to build a model that can accurately isolate common traits of specific species and reject over-specific features.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors presented their aims and methods quite clearly. While inspired by the earlier studies, they point out how their study is different from the earlier studies. To identify common visual features (i.e., evolutionary traits), they 1) combined two novel loss functions with a previously proposed loss and 2) used a novel masking module. Their results are compelling, which suggest the learning power of HComp-Net and its utility in detecting evolutionary traits. As HComp-Net may be used in other domains, this study can be of interest to other researchers.

Weaknesses:
HComp-Net was tested with only 3 datasets, which is understandable, as proper datasets may not be readily available. Still, a more thorough evaluation is desirable in the future.

Limitations:
The authors provided the limitations in the appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a method that automatically learns multiple orthogonal embeddings to act as prototypes. This approach helps the discovery of hierarchical similarities by representing data in a structured space.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The writing is clear and easy to follow.   
The authors conduct experiments that with other methods, and the visualization of prototypes in feature maps, They also perform an ablation study on different parts of the loss functions.

Weaknesses:
In the comparison with HPNet, the authors modify HComP-Net by removing the final two max pooling layers, resulting in a more detailed 26x26 feature map. In contrast, HPNet produces only a 7x7 feature map as shown in figure 4(a). Since the architecture and effectiveness of these networks heavily depend on the resolution of feature maps, this discrepancy raises concerns about the fairness of the comparison. To ensure a fair comparison:
  * HPNet should also be adjusted to generate a larger feature map. 
  * This adjustment and its impact on performance should also be included in the ablation study section.

In the generalizing to unseen species section, the evaluation method used by the authors could be extended to include comparisons with non-hierarchical methods. This would provide a more comprehensive evaluation of the method's effectiveness across different types of classification challenges.

Limitations:
There are several limitations concerning the comparison with other methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors investigate the use of prototype-based explainability (as in ProtoPNet) for the visual discovery of evolutionary traits in biology image repositories.
In particular, the authors aim to find traits that apply to group of species in a hierarchical fashion, according ot the tree-of-life hierarchy. 
The authors identify three challenges with state-of-the-art prototype methods such as learning over-specific prototypes that do not apply to all species in a given group, and prototypes that do not descriminate between the group and other groups of species in the hierarchy. Ther main contribution is the design of a loss and a masking mechanism to mitigate those issues.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ Interesting application and qualitative results in evolutionary biology.
+ Dedicated focus on learning discriminative hierarchy-level features in an interpretable way.
+ The authors provide their source code.

Weaknesses:
- The results are limited to three relatively small datasets. Why are there no result on the iNaturalist dataset which is at least 57x larger than CUB-200- 2011?
- It was hard to judge the effectiveness of the approach from the provided figures. The images are quite small.
- It was also hard to assess the effectiveness of masking. The figures did not illustrate how it helps.

Minor: I encountered several language issues. Below are ones I noted:
- hiearchy
- seperation
- Futhermore
- scenarious => scenarios 
- overlayed => overlaid
- indicating to difference => to a difference
- hasn’t => has not [avoid abbreviations in a scientific text]

Limitations:
A fundamental limitation in the application domain of interpretable biological traits is discussed in section I. Beyond a few ablation studies focusing on the introduced losses, I missed a discussion on the limitations of the approach, in particular the effectiveness of masking.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
AO5MjDuHpr;"REVIEW 
Summary:
This paper proposes Tree of Attributes Prompt learning (TAP). Unlike previous works that rely on unstructured class descriptions, this approach distillates structured knowledge graphs associated with class names from LLMs. Text/vision prompts and vision-conditional pooling module are designed to extract instance-specific text features. Extensive experimental results demosntrate its improved performances.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Overall, the idea of distillating structured knowledge from LLMs in the task of prompt learning is new and interesting.

- The paper designed an effective prompt learning framework to capture fine-grained attributes, using vision expert tokens and vision-conditional pooling layer.

- The illustrated way to generate structure tree of attribute from LLMs can also be used in other tasks.

- From the experiments, using structured knowledge leads to better performances than unstructured descriptions in base-to-novel and few-shot classification tasks.

- The visualization of class activation maps and attention weights look good. The paper is well written and easy to follow.

Weaknesses:
- Apart from the new framework, the method highly relies on the quality of tree of attribute generated with GPT-3.5-turbo. There is no study on the robustness aganist different LLMs, different generation prompts, or varying attribute sets.

- The loss includes a model regularization and its effectiveness is not discussed.

- In Figure 2, it is not too clear to me about $I_1 T_1$, $I_2 T_2$,etc. They seem not be discussed in the text parts.

Limitations:
One limitation is its reliance on LLMs (GPT) to generate the tree of attribute. When generating more complex responses, it is challening to ensure the quality and variances. How to keep a balance between the diversity of attribute sets and relevancy of attributes to classification is important.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new method called ""Attribute Prompt Learning Tree (TAP)"" to improve the performance of CLIP on zero-shot and few-shot classification tasks. The authors leverage large language models (LLMs) to generate more descriptive text prompts and introduce a hierarchical tree-like structure to systematically generate and integrate these descriptions, ensuring a layered and comprehensive understanding of the visual content. The method also learns specialized ""domain expert"" prompt tokens that focus on different visual attributes and uses a vision-based pooling module to extract text features for specific instances. Extensive experiments show that TAP outperforms state-of-the-art methods on zero-shot and few-shot classification tasks across multiple datasets

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1), The idea that utilizing LLM to generate tree-like prompts makes sense. This structured description approach is significantly different from the existing simple text prompt methods and provides an efficient way to improve VLMs.

2), The image-conditional pooling module looks like good for capturing instance-specific features.

3), Experiments and visualization demonstrate the effectiveness of the proposed model.

Weaknesses:
1), TAP introduces many textual and visual prompts, which leads to high computing and time costs. This may limit its applications.

2), TAP first generates hierarchical token prompts, while it seems like TAP does not use such a hierarchical structure to integrate the output of the text encoder. It only uses a pooling strategy to update the text encoder output with the visual feature. That is, TAP also does not utilize these relationships in the prompt graph.

3), TAP can be viewed as a multimodal prompt tuning method. What is the main difference between TAP and MAPLE,  ALIGN.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a method that aiming to align the vision modality with not only the category name but also the whole concept subgraph the noun represents in the knowledge graph. This is achieved by adding a bunch of attributes branches attached to this concept. The authors argue that this integration of attribute knowledge will make the alignment more transferrable and thus result in a good performance boost in terms of zero/few shot results.
Basically, this work focusing on the topic of textual prompt enrichment task that is investigated before but implement in a different manner. Additionally, the proposed method use seperate tokens to learn different aspectrs of attributes of given images, working as 'domain expert'.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Might be the first work trying to align the vision image with structured data. It is quite interesting considering that most text prompts now are less organized and noisy. And structured data, as pointed out in the recent research of LLM, may lead to better reasoning skill for a foundation model.
2. The proposed vision-conditional pooling can help the model filter out descriptions that are not direct appeared in the image.
3. Recieve good results on different classification datasets with the model trained with this method.

Weaknesses:
1. The attributes description is generated by the LLM, which could contain hallucinated content. While there are many reliable sources of knowledge such as wikipedia or conceptNet, this paper seems skip these sources to obtain some accurate attributes.
2. Though this paper decide to use a tree structure to represent the concept. The built tree is not encoded in a structure-awared manner. They are still feed as langauge tokens to the LLMs. 
3.  in equation (5), what is $v_y^a$ stands for? 
4. The author argued that the vision-conditional pooling, which is bascially a cross attention layer between the visual and language modal. The authors believe this this design will make the model filter out non-exisiting material in the text description. However, we know that due to the quirk of softmax function. You can never make some tokens attention to be '0'. Thus, the model is learning some spurious correlation aftertall.

Limitations:
Not applicable.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The TAP method structures textual descriptions in a hierarchical “concept-attribute-description” format, effectively creating a knowledge graph from large language models (LLMs) for each category name. This structure allows for a more comprehensive and detailed understanding of the visual content. The paper reimagines learnable prompt tokens as ""domain experts,"" each specializing in different aspects of the image, supplemented by a global perspective provided by the CLS token. To address potential misalignment between general descriptions and specific image content, the paper introduces a vision-conditional pooling module. This module extracts instance-specific text features, ensuring optimal image-text alignment.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method incorporates structured tree of attribute into prompt tuning that provide richer supervisory information compared to unstructured attribute information. A set of experiments has been conducted, and the results look promising.

Weaknesses:
One major limitation of the method is that it requires human review to ""ensure the quality of the example"" (L175). Recall that one major advantage of prompt tuning is that it can adapt large models quickly to specific tasks. However, the requirement of human reviewing in the proposed method is not consistent with this goal. In addition, it is not clear how many human efforts are needed here, and how to handle the potential human bias in quality evaluation. 

The paper lacks cross-dataset experiments, which is typically provided in existing PT papers. The results are important to examine the domain generalization capability of the method. 

For training details, different learning rates were used for different datasets, however, existing methods typically use a same LR for all datasets. From this point, the comparison is somewhat unfair.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new prompt tuning method for adapting the vision-language model.  The authors design the tree of attribute prompt learning to substitute the categorical description for adapting the vision-language model. A vision-conditional pooling module is proposed to extract instance-specific text features. Extensive experimental results demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. A tree of attribute prompt learning method is proposed to guide the adatpation of VLM with the hierarchical semantic information.

2. This paper is well-written and easy to follow.

Weaknesses:
1. According to the experiment, the performance improvement of TAP is marginal, e.g., the few-shot performance on most of datasets.  Although the visualization results of VCP layer are impressive, the improvement of this module is also very slight compared to average pooling. 

2. The core motivation of this method is learning fine-grained attributes to adapt VLMs. However, similar ideas have been explored in previous works , e.g., APPL[1], MAP[2].  Please discuss the differences.

[1] AAPL: Adding Attributes to Prompt Learning for Vision-Language Models

[2] Multi-modal Attribute Prompting for Vision-Language Models

3. The construction of ToA depends heavily on the prior information on the category of attributes suitable for the dataset. However, one of the most capability of VLM is its zero-shot ability in the open-vocabulary context. What's the performance of the proposed method in the domain generalization setting?


4. The model details in Figure 2 are not presented very clear, especially the input & output streams. This figure should be refined for better clarity. 


5. The mechanism behind Equation (5) and the function of VCP needs more clarification. Why conduct constrastive learning between expert token P_a^v and attribute embedding v_c^a generated from P_a^v itself, instead of P_a^v and the embedding of attribute descriptions D?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
AM1znjeo9l;"REVIEW 
Summary:
The paper studies the effect of rescaling symmetry in SGD and shows SGD tends to favor solutions with balanced gradient noises. The authors then derive an exact solution of the stationary distribution of a toy model trained by SGD.  The derived solution shed lights on problems observed in deep learning such as fluctuation inversion and edge of stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper contributes to the understanding of SGD properties. The noise balance theorem is novel and important. The analytical solution as well as the interpretation is interesting and insightful.

Weaknesses:
The results of the paper are interesting and important, the writing needs refinement to improve clarity and precision. The conditions under which the results hold is sometimes omitted, leading to confusion. The language should also be made more precise.

Minor points:
1.	The first paragraph in related works appears to overstate the novelty of the results. Specifically, our result is the first to derive an exact solution to the stationary distribution of SGD without any approximation. (Line 55-56) This is a strong claim, but it seems inaccurate. There are previous results showing exact solution of stationary distribution of SGD (e. g. Liu Ziyin 2021). Corollary I.1 in arXiv:2306.04251 (2023) also states the stationary distribution on a deep learning setup similar to the D=1 model discussed in this paper. Also, the solution given in the paper is for a specific model. These should be made clear.
2.	It seems that eq. 15 takes D=1, which has not been stated and thus is confusing.
3.	It is unclear why the left figure of Fig. 5 has only two theory lines instead of three.

Major points:
1.	The related works on symmetry and SGD dynamics are insufficient. There are a few related works that are missing, e. g. arXiv:2309.16932 (2023).
2.	The paper has not discussed convergence to the stationary distribution. The authors seem to assume convergence to stationary distribution and use interchangeably the SGD properties and the stationary solution properties (e. g. line 97-98). However, the properties of SGD can be very different from the properties of stationary solutions unless convergence to the stationary solutions is guaranteed. The authors should clarify this.
3.	The authors fail to discuss uniqueness of the stationary solutions. For example, it is unclear to me why eq (3) is a necessary and efficient condition for stationarity. Eq (3) is a critical result in the paper, and it would be better to make it a theorem or corollary. However, since eq (2) cannot be interpreted as a deterministic ODE. The unique condition for a stationary distribution should be justified, especially considering that C1 and C2 are not constant but depend on u and w.
4.	The equivalence of SGD bias and weight decay is not rigorous. (line 155-158) The C0 term is not constant but depends on u and w, while the weight decay rate is constant.

Limitations:
The authors have listed limitations at the end. The major limitations are the simplicity of the model and lack of experiments on deep neural networks.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
For ReLU networks trained by gradient flows, it is classical that a type of Minkowski inner product between the coefficients of consecutive layers is preserved. The authors demonstrate a monotonicity of the same quantity for stochastic gradient descent in continuous time. They use this to study the invariant distribution of parameters trained by (continuous time) SGD.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The topic is well-chosen and the results - if correct - are very interesting.

Weaknesses:
* In its current form, I find the article a bit unpolished and the results not easy to access. Many questions remained unanswered when I tried reading the article (see questions).

* Important quantities are defined throughout the plain text. I understand that reading as a reviewer under time pressure is different from normal reading, but for instance in Theorem 3.1, I would have hoped for a more self-contained statement on relations and properties $L, C, \ell$ and the distribution of $x$ have to satisfy. As far as I can tell, the statement is fairly general and not specific to machine learning.

* I have serious doubts about Theorem 3.1. It is derived in Appendix A from Itô's Lemma without the diffusion term. This is valid *in expectation over $\theta$*, but not pointwise in $\theta$. Pointwise in $\theta$, there should be white noise in the 'time derivatives', i.e. the ODE identity should be written as an SDE. In the proof, equations (27) and (28) appear to be wrong.

* The authors do not pay any attention to whether solutions to the evolution equations exist (or are unique). Problems with regularity can sometimes be alleviated if the distribution in $x$ is sufficiently regular, but I would appreciate a short discussion.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to analyze the specific features that carry the noise of SGD (through a continuous model). The authors show that there is a certain 'law of balance' across the layers when some invariance is assumed. Going further, they derive a toy model to push their study, showing that there is an analytic stationary solution to it. They finally propose a phenomenology related to the role of the noise of SGD when analyzing this precise stationary distribution.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea that a conservation law for the gradient flow implies an asymptotic balancedness condition for the stochastic flow is a good and striking idea.

The one-dimensional examples that are given in the text are very pleasant to follow and they are good exercices to display the ability of the stochastic flow to diverge from the gradient flow.

The example given in Eq.(13) is thoroughly analyzed.

Weaknesses:
The paper present the following weaknesses:

- The law of balance is an interesting phenomenon, yet considering it with a closer look, it seems that not much can be said generally and that one has to understand it case by case. In one dimension, sure, it is possible to conclude that balancedness will occur at exponential speed, yet in dimension more than $2$, it seems impossible to predict it surely.

- I have to say that I was a bit bothered by the general overselling of the paper : 
     - As said before the law of balance is truly valid asymptotically in one-dimension
     - The stationary distribution that the authors claim to be the first to derive is for a very specific model, which is not standard and does not resemble a diagonal network! 
     - The fact that the stationary distribution can be computed is also very inherent to $1d$ calculation and is simply a recognition of a Pearson diffusion that already made in way in ML (at least in https://arxiv.org/pdf/2402.01382 and https://arxiv.org/pdf/2407.02322).

Minor typos/flaws:

- l.41: Fokker Planck is not inherently high-dimensional
- l.44: Go to the line for new paragraph 
- l.165: The law of balance is not strictly applicable here since $\ell$ is not scale invariant because of the regularization.
- Section **4.1 Depth - 0**: I think that $\Delta > 0$ is not currently the ""most practical example"" since it corresponds to a underparametrized model.

Limitations:
As said before, all conclusion are drawn for models that live intrinsically in one dimension.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
AAN46kUPXM;"REVIEW 
Summary:
In this paper, the author proposes ""Expressiveness,"" a metric that measures the dissimilarity of feature maps produced by different filters. Subsequently, the author introduces NEXP, a technique to prune filters based on their expressiveness. The proposed method is tested on tasks such as image classification and object detection.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The overall structure of the paper is clean and easy to follow.
2. Although I am not very familiar with the structured pruning literature, the application of the concept of representation power of neural networks in this field appears novel.
3. The experiments are comprehensive, including pruning at initialization (PaI), pruning after training, and other tasks like object detection.

Weaknesses:
1. The notation is very confusing. For example, in the section 'Generalization of concepts at a structural level,' $\ell$ is the index of a certain layer, but the upper-case K is the total number of layers, and the lower-case k is the index of a filter/channel in a certain layer.
2. The concept of expressiveness is not new in the context of pruning [1] and neural architecture search [2,3,4].
3. The performance improvement by NEXP is not prominent. For example, in Tables 1 and 2, NEXP often shows a higher compression ratio but lower accuracy. This makes it unclear if NEXP offers a significant advantage over other methods.

***
**Minor Mistakes:**

Line 163: ""where k the is"" should be corrected.

[1] Tanaka, Hidenori, et al. ""Pruning neural networks without any data by iteratively conserving synaptic flow."" Advances in Neural Information Processing Systems 33 (2020): 6377-6389.

[2] Lin, Ming, et al. ""Zen-NAS: A zero-shot NAS for high-performance image recognition."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.

[3] Wang, Haoxiang, et al. ""Global convergence of MAML and theory-inspired neural architecture search for few-shot learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.

[4] Chen, Wuyang, Xinyu Gong, and Zhangyang Wang. ""Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective."" arXiv preprint arXiv:2102.11535 (2021).

Limitations:
The authors have discussed limitation in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper works on weight pruning for CNNs. It proposes an evaluation metric, i.e., ""expressiveness"", to evaluate whether a neuron/groups of neurons should be pruned or not. The metric focuses on the neurons' ability to redistribute informational resources. As the evaluation of expressiveness requires data samples, the paper includes studies on arbitrary data or limited dataset’s representative samples. The experiments are conducted for image classification tasks on ResNet architectures, and the object detection task on YOLOv8m.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Weight pruning is an effective manner in reducing the redundancies in DNNs. Instead of focusing on weight importance, this paper considers the expressiveness of neurons in the information flow within a network. The proposed evaluation metric can also be combined with existing strategies with importance evaluation metrics as a hybrid pruning approach.

Weaknesses:
1. Limited practicality of the approach. The method is mainly focused on removing redundant filters from CNNs. Though CNN is one category of DNNs, recent works have shifted to more advanced model architectures such as transformers and Mamba, which are mainly composed of FC layers instead of CNNs. The practicality of the approach is highly restricted to SOTA model architectures for image classification tasks.   

2. Limited performance improvements. This is a major concern. The performance gain of the proposed method is not obvious compared with baselines. For instance. on CIFAR-10 VGG-16, SCP and reduce the parameters 15.28$\times$ with a 93.85\% accuracy while the proposed method can only reduce the parameters 5.62$\times$ with a slightly better accuracy 93.87\%. HRank also provides better performance than the proposed method with higher accuracy 93.96\% (0.09\% higher than NEXP) and higher reductions of FLOPs (4.26$\times$ (HRank) v.s. 4.01$\times$ (NEXP) ). On DenseNet-40, Hrank also shows better performance across all metrics. Hrank v.s. NEXP: Accuracy 95.05\% v.s. 94.64\%, parameter reduction 3.31$\times$ v.s.3.12$\times$, FLOPs reduction 3.38$\times$ v.s. 2.51$\times$.

Limitations:
Please refer to weaknesses and questions. The major concern is that the method does not show better performance than baselines, and is also limited in model architectures.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a new structured pruning approach NEXP. It works by computing the dissimilarity score of the feature activations across samples and removing those filters with smaller variances. Experimental results on several models and datasets demonstrate the effectiveness of the proposed approaches.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written with clear motivation and many of the important technical details are included.  
2. The authors have presented the experimental results well and the additional discussion provides deeper insights on the effectiveness of the proposed methods.  
3. The authors evaluated models beyond image classification, i.e., the proposed methods work well on YOLOv8 object detectors.

Weaknesses:
1. The conclusion section is too short and fails to characterize the main contribution of this paper. What does it mean as to “when” and “how” to prune? The authors should elaborate on these further.   

2. I think that this is not a scalable approach for computing the pruning metrics. The pruning metric proposed in the paper requires computing a N by N matrix for each filter in the network, where N is the number of samples. This could grow quite computationally infeasible for large networks and batch sizes. The authors also fails to discuss this aspect on the pruning efficiency in the paper.  

3. In terms of experiments, I am not sure why the authors compare each methods under different compression ratio? If the pruned models in each method have different parameters, it can be hard to compare the accuracy numbers.  

4. I feel a lot of the content in section 3 is not necessary and they can could go into a separate preliminary section. Section 3.1 and early parts of Section 3.2 takes up a lot of space and in the meantime do not provides us with the motivation and insights for the later introduced methods.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to handle network pruning problem. Specifically, it proposes to use a new importance measurement, called expressiveness, to decide the pruning process. It jointly considers the model state to leverage on the proposed measurement. In addition, it can also combined with typical importance based pruning methods to improve the model efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Network pruning is a valuable research direction to study on, espeicailly for current large-scale era where efficiency matters a lot.
2. The proposed new metric to measure the network importance is interesting.
3.  Empirical results show the method superiority.

Weaknesses:
1. Adding more discussion in the conclusion part helps to improve the paper readability.
2. Since this paper proposes a new pruning metric, it is better to show more visualization and network behavior analysis to illustrate the intuition.
3. The compared baseline models are relatively old, adding more recent publications helps to support this paper.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
93qSRpucpN;"REVIEW 
Summary:
The paper proposed RGD, a novel method for integrating classifier guidance into classifier-free guidance diffusion models for solving offline MBO problems. Experiment results and ablation studies validate that the method outperforms state-of-the-art baselines and each proposed component is resonable.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Idea is intuitive and easy to follow

- Motivating example in the introduction makes the reader easy to understand the limitations of the prior method and the advantages of the proposed method

- Strong experiment results and detailed ablation studies make the proposed method more convincing

Weaknesses:
- For the diffusion-based proxy refinement part, it seems that there are several estimations to compute the distance between $p_{\phi}(y\vert \hat{x})$ and $p_{\theta}(y\vert \hat{x})$. Furthermore, it incurs additional hyperparameter $\alpha$, which should be carefully tuned.

Limitations:
There are a few minor comments on the manuscript.

- For figure 2, it seems that $\tilde{s}(x_T, y, \omega)$ should be written as $\tilde{s}(x_T, y, \hat{\omega})$. Furthermore, at first, it makes me confusion that RGD conducts classifer-guidance. However, that misleading part has been resolved after reading the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper,  the authors proposed to combine both classifier guidance and classifier-free guidance for offline black-box optimization.  In addition, the authors propose a Proxy Refinement procedure by minimizing KL divergence between the Proxy distribution and diffusion distribution regarding $y$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.  The paper is well-written and well-organized. 



2.  The paper introduces several refinement procedures to boost the offline optimization performance.  The proposed Diffusion-based Proxy Refinement procedure is interesting.

Weaknesses:
1.  **Technical contribution seems to be incremental**

Employing diffusion models for offline black-box optimization is not new.  The technical contribution of this paper seems to be incremental. The draft extends the paper ""Diffusion Models for Black-Box Optimization"" [1]. However,   detailed discussions about the relationship between the proposed method and the paper [1] are missing.

[1] Siddarth Krishnamoorthy, Satvik Mashkaria, and Aditya Grover. ""Diffusion Models for Black-Box Optimization."" ICML 2023. 


2.  **Part of the technical details are not clear.**

(a) In Equation (12),  the concrete computation procedure of $p_\theta (\hat{\boldsymbol{x}} | y)$ and $p_\theta (\hat{\boldsymbol{x}})$  via diffusion model is not clear. 

(b) The derivation of Equation (10) is not given.   It seems that Equation (10) is from the forward pass of the diffusion model.  However,  the forward pass  (Eq.32-32 in [10]) is regarding the distribution.  And the concrete  $\boldsymbol{x} _ t $ is constructed via the backward pass with  $s_\theta(\boldsymbol{x}_k,k)$ for $k \in T,\cdots, t+1$. 
 In addition,  how to choose $\mu(t)$ and $\sigma(t)$ in Equation (10)  is not clear.  

3.  **The additional proxy training, sample refinement procedure and proxy refinement procedure  increase the computation cost**

The additional proxy training, sample refinement procedure and proxy refinement procedure increase the computation cost. However, the time comparison with baselines is missing. 

4.  **The additional proxy training, sample refinement procedure and proxy refinement procedure bring many additional hyperparameters, which may overfit the offline BBO task**

In the offline BBO tasks,  the offline dataset is provided.  The evaluation is the black-box function value at the generated query at one time. 
The long-term convergence properties and exploration/exploitation balance are not considered.  As a result, there are risks that overfit the evaluation metric for the offline tasks.  The paper Introduces lots of additional hyperparameters, which increases the overfitting risks.

Limitations:
Additional computation cost and overfitting risk may be additional limitations besides the limitations discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework called Robust Guided Diffusion for the problem of Offline Black-box Optimization. The key idea is to formulate the solution as conditional generation of high-performance designs using a diffusion model which has explicit guidance from a proxy (surrogate) model. This proxy model is also refined/updated via a proxy-free diffusion procedure. Experimental analysis is shown on multiple tasks from design-bench benchmark.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Overall, I like the paper because it includes two simple changes to an existing approach (DDOM) that shows improved performance and the changes are validated by ablation choices.

Weaknesses:
- One major premise (repeated multiple times in the paper) in the paper is that proxy guidance conditional generation is more robust than updating the design with standard gradient ascent on the proxy. However, it is not immediately clear why this should be true and the justification for this key point is somewhat limited. If true, this will be much bigger insight going beyond black-box optimization. If it is only about the exploration/exploitation balance driven by w, we could also make standard gradient have this property by optimizing a upper/lower confidence bound on the objective. Please describe why this is the case either via some empirical experiment or theoretical insight. Also, in equation 11, we might evaluate the proxy far away from the training data depending on the values of s_\theta(x_t), \sigma(t), \mu(t).

- The related work coverage and corresponding experimental analysis of the paper can be improved. This problem has seen an extensive body of work recently. Please see the references below and discuss/compare them appropriately. Some of them are included in references but not compared in the experiments ([1], [2], [3]):

- [1] Yuan, Ye, et al. ""Importance-aware co-teaching for offline model-based optimization."" Advances in Neural Information Processing Systems 36 (2023).
- [2] Kim, Minsu, et al. ""Bootstrapped training of score-conditioned generator for offline design of biological sequences."" Advances in Neural Information Processing Systems 36 (2023).
- [3] Nguyen, Tung, Sudhanshu Agrawal, and Aditya Grover. ""ExPT: Synthetic pretraining for few-shot experimental design."" Advances in Neural Information Processing Systems 36 (2023).
- [4] Chemingui, Yassine, et al. ""Offline model-based optimization via policy-guided gradient search."" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 38. No. 10. 2024.
- [5] Yao, Michael S., et al. ""Generative Adversarial Bayesian Optimization for Surrogate Objectives."" arXiv preprint arXiv:2402.06532 (2024).

Limitations:
Please see weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a robust guided diffusion framework for offline black-box optimization, combining proxy and proxy-free diffusion for conditional generation. Key improvements include proxy-enhanced sampling and diffusion-based proxy refinement to address out-of-distribution issues. Experiments on the Design-Bench benchmark show the method outperforms existing techniques, validated by ablation studies.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The regularization of the proxy using the diffusion model is interesting. Additionally, optimizing the alpha parameter in an offline manner aligns well with the offline setup, enhancing the method's consistency and applicability.
- Experiments and ablations on four continuous and three discrete tasks validate the effectiveness of the proposed RGD method, showing improved performance and robustness.

Weaknesses:
- The paper lacks comparison with relevant approaches like ICT [1] and TRI-mentoring [2]. Despite referencing the latter in the related work section, it’s overlooked in the results.
- It is unclear why the results without proxy-enhanced sampling still achieve competitive outcomes, surpassing the dataset y_max. This contradicts the claims in lines 40-46. Where does the out-of-distribution (OOD) problem arise then? What is the distribution of the generated 128 candidates with and without the sampling? 
- The BDI reported results are significantly lower than in the original paper, especially for the ANT and TFBIND8 tasks. This also seems to be the case for BONET results. Did the authors change the evaluation setup?


[1]: Importance-aware Co-teaching for Offline Model-based Optimization, https://arxiv.org/abs/2309.11600

[2]: Parallel-mentoring for Offline Model-based Optimization, https://arxiv.org/abs/2309.11592

Limitations:
The authors address the limitations and potential negative impacts in their paper.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method, named RGD, for Offline Black-box Optimization (BBO). RGD incorporates an improved proxy to guide the previous proxy-free method (i.e. DDOM[4]). Key technical innovations includes (1) improving the robustness of the proxy function against adversarial samples by consistency regularization with the diffusion process; (2) dynamic per-sample reweighting between proxy-guided and proxy-free sampling. Compared to previous approaches, RGD demonstrates superior performance on Design-Bench [3].

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Methodology: RGD integrates forward and reverse approaches for BBO, in a way that they can help with each other (e.g. using forward proxy to guide the reverse sampling and using the diffusion process to improve the forward proxy), which is technically sound and interesting.

Experiment: RGD demonstrates superior performance on Design-Bench, compared to the baselines.

Ablation: Ablations on different components of RGD are provided.

Weaknesses:
The reviewer would prefer some clarifications on the method and the experiments

i) Algorithm 1, Line 4, how to identify the adversarial examples? From Line 187-188, it looks like gradient ascent is utilized to find the x that maximize y, it is unclear to the reviewer that how to determine if the obtained x is an adversarial example

ii) Algorithm 1, Line 7, refine the proxy function via eq 15. It would be best if the author could provide further details on how to optimize eq (15), e.g. number of validation and adversarial samples, number of iterations for the bi-level optimization discussed in Appendix B.

iii) Algorithm 1 Line 13, optimizing \omega. Again, it would be best if the author could provide extra info on how to optimize \omega. From Algorithm 1, it looks like \omega is time dependent and optimized for each time step. How many training iterations are required for each time step. The reviewer also wonder if the obtained \omega are dramatically different between different time steps. 

iv) From Line 257-258, it looks like the baselines shown in Table 1 & 2 were re-implemented. If this is the case, the authors are encouraged to include more implementation details, e.g. the model architecture for the score function, etc. This could help follow-up works to reproduce the reported results. The reviewer also wonders if the source code will be made public.

Limitations:
Limitations have been discussed in the appendix

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
8bExkmfLCr;"REVIEW 
Summary:
The paper introduces the JOBCD (J-Orthogonal Block Coordinate Descent) algorithm, a novel method designed to tackle optimization problems under J-orthogonality constraints. JOBCD includes two variants: GS-JOBCD (Gauss-Seidel strategy) and VR-J-JOBCD (Jacobi strategy with variance reduction). Theoretical analyses establish the algorithms' complexity and convergence, while extensive experiments show JOBCD's superior performance compared to state-of-the-art methods in various applications.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The strengths of this paper are listed as follows:

Originality: This paper introduces JOBCD as a novel approach to handling J-orthogonality constraints. It offers GS-JOBCD and VR-J-JOBCD, showcasing flexibility and innovation in optimization strategies.

Quality: This paper provides comprehensive complexity and convergence analyses. Extensive experiments demonstrate superior performance on real-world and synthetic data.

Clarity: The structure is logical.

Significance: This work is relevant to various statistical learning and data science fields.

Weaknesses:
Some proofs for Section 4 are hard to follow.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes two Block Coordinate gradient descent methods(BCD) for solving J-orthogonal constrained problem. One is Gauss-Seidel type, the other one is Jocobi type as well as addressing finite sum problem using variance reduction strategies. Convergence guarantees are proved with KL conditions. Numerical experiments show the advantages of the  proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed algorithm decomposes the matrix variable into row block structure, yielding a block coordinate descent algorithm with a small size subproblem. The numerical performance is very impressive.

Weaknesses:
1. This paper is based on the paper "" [51] Ganzhao Yuan. A block coordinate descent method for nonsmooth composite optimization
under orthogonality constraints. ArXiv, abs/2304.03641, 2023.""
The main difference is the constraint in this paper becomes J-orthogonality constraint. However, the framework follows almost the same as [51]. The authors should highlight the novelty of the algorithm or difficulty in the extension.

2. The authors of the reference [31] may be wrong. Besides, the UMCM algorithm in [31] solves orthogonal constrained problem. Is there any difference in implmenting in solving J-orthogonality problem? The objective value of UMCM is far from the JOBCD method. I'm curious about the reasons.

3. In numerical experiment, how do you select subset from the dataset, see line 309.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes two block coordinate descent methods for minimization of a finite-sum subject to the J-Orthogonality constraints — one based on Gauss-Seidel strategy, the other based on variance reduction and Jacobi strategy. The convergence is proved, with a global convergence rate of O(N/\epsilon) and O(\sqrt{N}/\epsilon) respectively, and a local convergence rate that depends on the desingularization in the KL-condition assumption.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The algorithms proposed are novel and might be useful in practice: the update rules involve solving a small size problem thereby is very simple, and the convergence is proved theoretically under reasonable assumptions.

Weaknesses:
The paper is relatively dense, and I find it a bit hard to keep track of all the terms introduced. For instance, the parameter theta is used in the algorithms but I’m not sure where it is introduced; in Assumption 4.8 KL function is mentioned but it’s not defined…

Limitations:
Yes, the paper discusses the assumptions of the theorems.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a block coordinate descent method for solving optimization problems with J-orthogonality constraints. Several variants of the method are introduced within this framework, and convergence results are established. Extensive numerical results are also presented to demonstrate the efficiency of the proposed methods. However, I have some concerns regarding the novelty of this paper as well as the numerical results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
It appears that optimization with J-orthogonality constraints has not been thoroughly studied in the literature. This paper proposes an efficient method for addressing this problem.

Weaknesses:
1. My major concern is that the novelty of this paper might be insufficient since the row-based approach is very similar to that in [51], even though the two papers tackle different problems.

2.  For the numerical results shown in Table 1, the proposed method fails to return a feasible solution for some instances, such as randn(10-10-5) and w1a (2470-290-145), as well as some other instances in the appendix. This is strange since the paper describes a BCD-type method, which should always return a feasible solution.

3. The information of the reference [31] might be incorrect. 

4. For the GS-JOBCD method, there are two options for choosing $Q$, whereas J-JOBCD only has one option. The authors should provide an explanation for this difference.

5. The presentation could be further improved. Here are a few examples: the formulation of $P_i$  after equation (12) could be simplified by removing the notation $\mathrm{mat}$; it is unclear if the requirement on $\underline{Q}$ in equation (4) is sufficient to guarantee convergence (probably not, since $\underline{Q} = 0$  also satisfies this condition).

Limitations:
At the beginning of the paper, the authors claim that equation (2) can imply 
$\\|\nabla f_i(X) - \nabla_i f(X^+)\\| \leq L_f \\|X - X^+\\|$, which is incorrect. Note that the converse is correct. The other assumptions in Assumptions 4.1 and 4.2 essentially assume the compactness of the iterates.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
8JmUmTgKiY;"REVIEW 
Summary:
This paper first generalizes the Kolmogorov–Smirnov (KS) distance from one-dimensional spaces to multidimensional spaces and proposes the Kolmogorov-Smirnov GAN, which formulates the generative model by minimizing the Kolmogorov-Smirnov (KS) distance. Theoretical results are also given in this paper and the experiments also show the superiority of stability during training, resistance to mode dropping and collapse, and tolerance to variations in hyperparameter settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I like the idea of generalizing the one-dimensional KS distance to multi-dimensional, which I had thought about before but failed to achieve. The motivation and writing are good, and the experiments of KSGAN seem to have achieved good results.

Weaknesses:
1. I'm skeptical about some parts of the theory. (See questions for details)
2. It seems that the advantages of KS distance over JS divergence and Wasserstein Distance are not explained.
3. The idea of reformulating a distance between distributions to a GAN model seems to be old and is now unlikely to attract readers' interest.
4. The experimental setup is relatively simple, only comparing with vanilla GAN and WGAN on Synthetic, MNIST, and CIFAR10 datasets
5. According to the experimental results, the advantages of KSGAN lie in the stability of training and resistance to mode dropping. A significant issue is that with current network architectures and training techniques, these two problems are rarely encountered.

Limitations:
The authors have discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel variant of the generative adversarial network that uses the Kolmogorov-Smirnov distance to align the generated distribution with the target distribution. This distance is calculated using the quantile function, which acts as the critic in the adversarial training process. Experiments are conducted on synthetic distributions and small image datasets to show that the proposed KSGAN performs on par with the existing adversarial methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The paper is well-presented and easy to follow.

2. The claims and methodology designs are well supported by theoretical analysis.

Weaknesses:
1. It is still unclear why we need another adversarial design based on KS distance. The vanilla GAN paper shows that the designed bi-level optimization process can already be seen as optimizing the distance between the generated and the target distribution. Then, what are the specific advantages KS distance can bring within the adversarial framework?

2. The experiments are merely conducted on synthetic datasets and small image datasets. It is unclear whether the proposed method can be adapted to larger-scale datasets or incorporated into more advanced frameworks like StyleGAN. Moreover, the compared baselines are limited to early works, and the experimental results of KSGAN are worse than those of WGAN-GP. Thus, I do not see many advantages of KSGAN in terms of the presented experiments.

Limitations:
Please see the discussions above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a generalized KS distance applicable to high dimensional spaces, formulate the corresponding dual problem, and use adversarial training to construct a generative model that minimizes the GKS between data and generated distributions.

The paper is well presented and appears technically correct through what I've seen, though I didn't check the proofs in details.

The main problem is that there's no clear motivation for why using the GKS is beneficial at all (either theoretically or practically). As such, despite being novel, I don't see any clear impact from the paper. Furthermore, the final algorithm is quite complicated and the results are fairly underwhelming, so at the end of the day the cons dramatically outweigh the pros of the newly introduced algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is well written. It reads easily and it is clear what they want to do. The contribution of a generalized KS distance to multidimensional spaces and the algorithm to approximate it are to the best of my knowledge, novel.

Weaknesses:
The main problem I have with the paper is that I don't see any clear advantages of using the KS distance (gan) as a replacement of other distances like the Wasserstein one, or their GAN equivalent. The only mention of this, which should arguably be the most important thing in a paper introducing a new GAN, is in lines 224-228 of page 7. The authors claim there that they don't need to maximize the supremum in (5) which is false depending on how to interpret it, if you just take any set C in (5) you end up with |P_F(C) - P_G(C)| which is just measuring one moment for a given characteristic function, and far from being anything meaningful (and the same holding true for most IPMs). The results are also not particularly interesting to merit the claim that there's anything particularly different or benefitial on using this new formulation.

Limitations:
No clear motivation or benefit from using their algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed a new kind of GAN training method called KS-GAN. The method is based on minimizing the Kolmogorov–Smirnov distance. The KSGAN updates the generator by minimizing an upper bound of the generalized KS distance. It updates the discriminator (or the critic network) by using energy-based model training with regularization terms. The KSGAN is a novel attempt to explore new approaches to train generative adversarial networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* (1) The paper studied using generalized KS distance to train GAN generators is a novel attempt to extend the GAN literature. 

* (2) Some theoretical arguments about implementing the empirical KS distance using neural networks is novel yet constructive.

Weaknesses:
My main concern about the paper is its weak evaluation baselines and questionable practical usage:

* (1) Though the idea of using new objectives for training GAN generators is attractive, the practical usage of KSGAN seems questionable, especially for high-dimensional data. For instance, in the CIFAR10 generation experiment, the author compares KSGAN with WGAN-GP and Vanilla GAN, which have shown weak empirical performances. However, it is well-known that, for CIFAR10 data, the StyleGAN2-ADA[1] model is a strong baseline GAN model. I think it would strengthen the paper a lot if the authors could somehow show strong performances of KSGAN using StyleGAN2's architectures and implementation techniques. However, I do admit that such a requirement may be too tough for new methods.

* (2) The KSGAN's critic function is constructed with EBMs. However, even with regularization terms, energy-based models are well-known for poor scaling ability to high-dimensional data. This may prevent the practical usage of KSGAN for real-world high-dimensional data.

Limitations:
The author has addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
8Hy3KMZTL5;"REVIEW 
Summary:
The paper presents H-CLIP, a novel framework for open-vocabulary semantic segmentation using the CLIP model. The framework addresses three key challenges: high computational cost, misalignment between CLIP's image and text modalities, and degraded generalization ability on unseen categories when fine-tuning for pixel-level predictions. H-CLIP employs a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both CLIP modalities. This strategy uses efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module to mitigate misalignment issues. Additionally, an orthogonality constraint based on the hyperspherical energy principle is applied to the text encoder to preserve the generalization ability of the pre-trained model.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The introduction of the H-CLIP framework for open-vocabulary semantic segmentation represents a significant innovation. The use of a symmetrical parameter-efficient fine-tuning (PEFT) strategy in hyperspherical space is a unique approach to addressing the challenges associated with fine-tuning vision-language models.

The paper provides extensive experimental results across multiple benchmarks, including ADE20K, PASCAL VOC, and PASCAL-Context. These experiments validate the effectiveness of H-CLIP, showing its superior performance compared to state-of-the-art methods.

Weaknesses:
1. Consider that the expression in formula 5 does not specify how to interact with the \boldsymbol{R} matrix.

2. The paper states that current fine-tuning strategies are usually asymmetrical, but it does not provide enough evidence or references to support this claim. The authors should provide empirical evidence or references to support the claim of asymmetry.

3. While the paper extensively discusses the orthogonality constraint in the CLIP image encoder, it lacks an in-depth analysis of how the misalignment problem impacts segmentation performance. The authors should discuss the specific effects of misalignment on segmentation.

4. The paper should mention SAM (Segment Anything) and how the current work is still significant

Limitations:
The work is on semantic segmentation and there is no qualitative comparison shown in the main paper. There are some visuals in the supplementary, but most of those are from test set and there is no comparison shown with the baselines and existing methods, so it is not clear where the improvement is coming from. It will be good to see how the results improve with and without alignment.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents H-CLIP, a novel approach for parameter-efficient fine-tuning of the CLIP model in hyperspherical space, specifically for open-vocabulary semantic segmentation. H-CLIP includes the introduction of a symmetrical parameter-efficient fine-tuning strategy, leveraging hyperspherical energy principles. And a dual cross-relation communication module is utilized to enhance cross-modal and cross-layer alignment.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper is well-motivated. The proposed H-CLIP effectively addresses common issues in fine-tuning CLIP.
- The paper effectively argues that maintaining the hyperspherical energy helps preserve the model's generalization ability, a critical factor in multi-modal tasks.
- The ablation experiments are thorough and effectively support the arguments.

Weaknesses:
- The writing needs improvement. The introduction lacks transitions from existing problems to the approach of this paper, such as introducing the advantages of Hyperspherical Space. 
- Some formula descriptions can be optimized, for example, explaining the meaning of * in Formula 9. 
- Details about comparison methods are needed. In Table 1, the compared method SAN includes an additional backbone.

Limitations:
The authors provide no analysis of the limitations and broader impact. The author can analyze the limitations of this fine-tuning strategy in the field of OVS.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes H-CLIP, a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both of the two CLIP modalities. The PEFT strategy is achieved by a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. Extensive evaluations across various benchmarks show that H-CLIP achieves new SOTA open-vocabulary semantic segmentation results while only requiring updating approximately 4% of the total parameters of CLIP.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper achieves SOTA performance.
The Parameter-efficient Fine-tuning is explained by tensor computation.

Weaknesses:
1.The novelty is limited.  Partial orthogonal fine-tuning (POF) doesn't directly address the challenges of OVSS but rather offers a generic PEFT approach, so what is the difference between POF and OFT[1]? In Equ.5, which module's weights are used by the pre-trained weight matrix, Q(K or V)’s projection layer or FFN? Method details need detailed explanation.
2.Some concerns about DCRC. In this section, the author discusses the use of two k layers deep neural network to update the fourth-order tensor in Equ. 7, and provides some mathematical proof. However, these proofs only show that reversible transformations S(·) can be replaced by reversible matrices S (as shown in Equ. 11,12,14,15), and the authors use k layers deep neural network to replace such reversible matrices S, which cannot explain the meaning of reversible transformations. In other words, why adopting reversible transformations to update the fourth-order tensor in Equ. 7, and what is the role of reversible transformations? Is this approach also work in other fields other than semantic segmentation tasks? In addition, If the block diagonal structure is not adopted, Equ. 16 seems to require only one reversible matrices S4 for mapping. Does this reduce the number of parameters?
3.Insufficient experimental analysis. 
1)The decoder of HCLIP seems to be learnable as well. Does the param in Table 2 calculate the decoder part? And, Is the proposed PEFT method applicable to various decoders? If it is replaced with linear probe, is the proposed method still effective? Need further exploration.
2)If a different VFM is adopted (not CLIP), is the proposed method still valid?
3)The proposed method should be compared with more PEFT methods such as VPT, Adapter, LST, SSF [1-5] .

[1] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information Processing Systems, 36:79320–79362, 2023.
[2] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709–727. Springer, 2022.
[3] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790–2799. PMLR, 2019.
[4] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:12991–13005, 2022.
[5] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. Advances in Neural Information Processing Systems, 35:109–123, 2022.

Limitations:
The limitation of the proposed method should be discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel method called Parameter-Efficient Fine-Tuning in Hyperspherical Space for efficiently solving the open-vocabulary semantic segmentation problem. The method introduces a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. To maintain the generalization ability offered by the CLIP text encoder, the authors designed a constraint to PEFT based on Hyperspherical Energy. Comprehensive results on open-vocabulary semantic segmentation benchmarks demonstrate the strong performance of this PEFT method by training only 4% of the total parameters of CLIP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The idea of introducing hyperspherical space to achieve parameter-efficient training is interesting. This approach attains state-of-the-art performance on current open-vocabulary semantic segmentation benchmarks with fewer learnable parameters. Additionally, it demonstrates better parameter efficiency than LORA on open-vocabulary semantic segmentation tasks, as shown in Table 3.

Weaknesses:
I do not see any clear weaknesses. However, I acknowledge that I am not familiar with hyperspherical theorems.

Limitations:
See in questions. No other clear limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
7yqjVgWWxx;"REVIEW 
Summary:
This paper proves that the probability ratio that appears when computing the time reverse rate matrix for an absorbing state diffusion model has a simple form composed of the conditional distributions of clean data given partial masking scaled by an analytic time dependent weighting. They exploit this form to simplify the parametrization of absorbing state diffusion models and show this improves performance and sampling speed on text datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work is clearly written and I think theorem 1 will be genuinely useful for future work in absorbing state diffusion models. The fact that the time reversal of the rate matrix has a simple relation to the conditional distributions of clean data that is independent of time makes the target of optimization much clearer. This removes needless complexity when trying to condition models on time, even though the relationship with time is known analytically. This also helps model convergence since the scale factor of the target is known allowing the network to be targeting normalized quantities which is highly desirable for neural net training.

The removal of the time conditioning also has a significant benefit with respect to model speed ups. It is quite surprising that the absorbing state literature does not use this trick where at most L neural network evaluations are required for L length data. This paper should significantly help existing implementations in this regard by removing needless calls to the network.

Weaknesses:
Theorem 2 is wrong. Line C.14 in the proof is incorrect, it's not an equality but a lower bound. The correct version should read

$q\_\theta(x\_0) = \sum\_{\pi} U(\pi) q\_\theta(x\_0 | \pi)$

$q\_\theta(x_0) = \sum\_{\pi} U(\pi) \prod\_{l=1}^d q\_\theta(x\_0^{\pi(l)} | x\_0^{\pi( <l )} )$

$q\_\theta(x_0) = \mathbb{E}\_{\pi \sim U(S\_d) } [ \prod\_{l=1}^d q\_\theta (x\_0^{\pi(l)} | x\_0^{\pi (<l)}) ] $

$ \log q\_\theta(x\_0) = \log ( \mathbb{E}\_{\pi \sim U(S\_d)} [ \prod\_{l=1}^d q\_\theta(x\_0^{\pi(l)} | x\_0^{\pi(<l)}) ] )$

$ \log q\_\theta(x\_0) \geq \mathbb{E}\_{\pi \sim U(S\_d)} [ \sum\_{l=1}^d \log q\_\theta (x\_0^{\pi(l)} | x\_0^{\pi(<l)} ) ]$

When your generative model is a mixture of different generation paths (which an absorbing state diffusion model is), then you need to apply Jensen's inequality. See https://arxiv.org/pdf/2110.02037 equation (2).

Therefore, the authors should remove Section 3.3. I think this section should be replaced with discussion of Autoregressive Diffusion Models https://arxiv.org/pdf/2110.02037 which the author's model has basically reduced to. Autoregressive diffusion models randomly sample a generation order and gradually infill tokens with no dependence on time. The link between Autoregressive Diffusion Models and absorbing state diffusion should be clearly discussed in this paper and this reference is glaringly missing.

The authors should also remove line SEDD-S* in Table 2 since it is based on Theorem 2. Your models are then really not doing favourably compared to standard SEDD. What is your explanation for this and new narrative for Table 2?

In the paper's current state I cannot recommend acceptance since a large part of the narrative is based around Theorem 2. However, I believe the contributions surrounding Theorem 1 with regards to making models simpler and achieve good speed up stand alone as a worthy contribution. Therefore, if the authors clearly describe how they will adjust the narrative under this new information I will be happy to raise my score.

I think it would also be good to include a baseline against autoregressive diffusion models since they propose additional tricks relating to picking how many tokens to reveal. However, I appreciate this would be difficult in the limited time of the rebuttal period and is not required for an increase in score.

Limitations:
The authors adequately discuss the limitations in Section 6.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a simplified discrete diffusion model to improve upon prior language diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The method is simple and scalable. It is overall a nice insight, and the authors do a good job in extracting the relevant and impactful applications of this.

* The method seems to improve upon previous results, in particular resulting in better/faster sample quality.

* The presentation is pretty clear and direct.

Weaknesses:
* Although the method speeds up sampling, especially in the large sample step regime, this is a bit misleading/irrelevant. In particular, under more standard sampling practices, the gain is naturally not as big, so the claim of 3.5x improvement is a bit misleading. Furthermore, this does not really improve the sample quality at a smaller number of steps, which is the critical question. As a comparison, this would be like sampling from a standard diffusion model with 4096 timesteps, showing that you can speed it up in that regime, and then claiming a general improvement.

* The results are ultimately a bit marginal. The improvements on sample quality are nice, but I think there is a mistranslation between figure 2 and table 1 (there is no 15 generative perplexity for RADD in that table). Until this is clarified, I'm trusting the results of table 1 more. Table 2 also shows a slight improvement.

* The exact likelihood computation is never applied. I want table 2 to showcase this exact likelihood instead of just a bound.

* The model size is only small. I want to see a similar improvement for the medium quality.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work derives a new interesting connection between the concrete score and conditional target densities in absorbing diffusion models, which decomposes the time-dependent ratio between marginal probabilities (of two transitive states) as a conditional distribution on clean data scaled by an analytic time-dependent scalar, and hence inspires the commonly-used scaling trick and new re-parameterizations. In addition, it also simplifies the original complicated loss objective (denoising score entropy; DSE) as a more straightforward denoising cross-entropy loss (DCE) that enables the exact log-likelihood computation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is generally well-written and easy to follow.
2. This work proposes valuable insights of decoupled model parameterizations and simplified learning objectives, whose effectivenesses are theoretically grounded.
3. The proposed methods are also numerically verified, which advances the development of (absorbing) discrete diffusion models.

Weaknesses:
1. Despite that the overall presentation is good, it can be further improved by interpreting or illustrating more about the problem formulation. For example, what is the intuition behind the absorbing matrix $Q^{\text{absorb}}$ (eq. (2.4))? Why do we require a more complicated DSE loss instead of usual score-matching objectives (e.g. MSE)? Note that in eq. (2.6), the score network must be *additionally* positive. 
2. Although the proposed method (RADD) is reported to be superior for efficient sampling, the performance of RADD need further verifications on language modeling tasks (Table 2). The hyper-parameters should be fine-tuned to better demonstrate the capability of RADD.

Limitations:
As is stated by authors, future explorations include flexible variable-length texts generation and applications to models with larger scales.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
7ug4oSmN7l;"REVIEW 
Summary:
This paper presents a novel neural network-based CARP solver that uses a direction-aware attention model to incorporate directionality into the embedding process. It then applies supervised reinforcement learning for subsequent fine-tuning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A learning-based CARP solver is proposed.  
2. The performance of the proposed solver on large-scale data is discussed.

Weaknesses:
1. The comparison algorithms were published five years ago, and there is no discussion of existing methods aimed at big data.  
2. The experiments only tested the self-constructed dataset and did not evaluate on public datasets.

Limitations:
The amount of data required for algorithm training needs to be discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a learning-based method to address the Capacitated Arc Routing Problem (CARP). It involves breaking undirected edges into directed arcs and utilizing a graph attention network to build a Direction-aware Attention Model. In the training process, supervised learning is used to create the initial policy, followed by reinforcement learning based on policy gradients using Proximal Policy Optimization (PPO) to refine strategies. Lastly, dynamic programming is applied to optimize depot placements for path enhancement. Experimental outcomes show notable benefits of this algorithm in evaluation criteria.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
In general, the paper exhibits a well-organized structure with detailed experimental outcomes showcase through graphs and tables, facilitating readers in comprehending and visualizing the results effortlessly. The dataset employed comprises real-world scenarios, thereby boosting its practical relevance.

Weaknesses:
Converting the graph G from arcs to nodes represents a common approach in many heuristics for addressing CARP. This process adds complexity to the problem and increases its scale. The proposed method appears to lack enough novelty, with most components bearing resemblance to neural models designed for CVRP.

Limitations:
It appears that the paper focuses on an unlimited number of vehicles. How would the approach adapt to a specific set of vehicles?

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors skillfully address challenges posed by non-Euclidean graphs, traversal direction, and capacity constraints with their novel NN-based solver in solving capacitated arc routing problem. The introduction of the direction-aware attention model and a supervised reinforcement learning scheme is particularly commendable. These innovations significantly narrow the gap with advanced metaheuristics, achieving superior efficiency and competitive decision quality.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The manuscript employs numerous innovative methods to solve the capacitated arc routing problem, achieving impressive results. 
2. It also shows promising performance in generalizing to larger problem instances.
3. The combination of supervised and reinforcement learning is quite interesting. Using supervised learning for pre-training followed by fine-tuning with reinforcement learning is a noteworthy approach.
4. The qualitative comparisons in real street scenes presented in Figure 4 are particularly interesting.

Weaknesses:
1. It's better to redraw the first part of Figure 1 to enhance its aesthetic quality.
2. The baseline is not very recent. After S2V-DQN and S2V-DQN, there are still some excellent works that can be used to address the CARP problem.
3. Some writing errors have been identified, such as in line 2 of Algorithm 1. Please review the entire manuscript to check.
4. The completeness of the manuscript still requires supplementation and refinement.

Limitations:
The approach of decomposing undirected edges into directed ones introduces additional decision elements, which complicates the problem. It's better that the authors can find a more efficient graph processing method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new learning-based constructive heuristic for capacitated arc routing problems. In contrast to node routing problems such as the TSP and VRP, arc routing problems received comparably little attentition. To address the specific
challenges in the capacitated arc routing problems, the authors propose a Neural Network-based approach that uses a graph attention
model considering arc directionality, a reinforcement learning approach with supervised pre-training and PPO-based fine-tuning. In
order to improve solutions obtained by an RL-based construction approach, they propose a beam search approach for path optimization which, after turning the set of routes into a giant tour, splits the tour into routes by adding returns to the depot. A set of experiments
shows that the proposed approach consistently yields better results than traditional hand-crafted constructive heuristics, and that their solutions almost match the quality of a time-consuming memetic algorithm that is only capable of solving small instance in a reaonable amount of time.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors propose one of the first learning-based approaches for the capacitated arc routing problem (CARP). Their approach, in particular their graph embedding, explicitly addresses one of the challenges of learning-based construction algorithms for this problem by explicitly replacing the undirected edges by directed arcs. This idea is original and turns out to be helpful to create a well-performing heuristic. 

The online performance of the approach surpasses hand-crafted constructive heuristics both in terms of runtime and efficiency. While for small instances, other metaheuristic approaches are better, it can be assumed that for large-scale instances, the proposed approach surpasses the state-of-the art of heuristic approaches. This is a significant result, since for node routing problems such as the CVRP, researcher have been struggling for years to design learning-based heuristics that achieve a performace that is comparable to hand-crafted heuristics. It should be mentioned, though, that in general, arc routing problems receive much less attention than node routing problems in the literature.

The paper comprises several insightful and, as far as I can tell, reasonably designed experiments, in particular showing the generalization capability to larger instances. 

The paper provides both code and instances.

Weaknesses:
The presentation of the CARP routing problem, the solutions approaches and related work lacks clarity in many places.
As an example, in the abstract, we find that the CARP consists in finding ""the minimum-cost tour""hat covers all required edges on a graph, while within capacity constraints"". This is a a bit misleading description since we look for a set of routes instead of a tour.

The paper distinguishes ""heuristics"" and ""metaheuristics"", while clearly metaheuristics are a type of heuristics. Actually, what the authors appear to have in mind is ""constructive heuristics"" which sequentially construct a solution by adding edges to form routes. I suggest to formulate more precisely here.

Similarly, it would enhance the understanding of the paper to introduce the notion of ""route-first, cluster second"" and the related
notion of a ""giant tour"" which is commonplace in routing applications, to characterize respective existing work. It would even facilitate the
presented path optimization which actually turns the presented approach into a route-first, cluster second approach. 


The computational results are convincing, but the discussion should emphasize that a fair comparison can only be made between their
approach wihout path optimization and the other constructive heuristics. It would indeed be interesting to see how the far the path
optimization is able to improve the results of the other constructive heuristics.

The claims ""NN-based approaches tend to lag behind advanced metaheuristics"" (abstract) and ""NN-based methods usually lags far
behind the traditional ones in solving CARP"", ""they still lag significantly behind traditional methods"" are not valid. Actually, (Rahmamoorty et. al 2024) (reference 20) report that on average, they improve upon the memetic algorithm by 11% on average.

When it comes to the evaluation of the path scanning approaches in the experiments, it is unclear how they are parameterized. From reading the paper (Aarakaki 2019) one sees that the parameter alpha and the number of iteration have a considerable impact both on solution time and solution quality, and (albeit on different instances), the average gaps for the path scanning approaches to the optimal (and to the memetic algrithm) reported in (Aarakaki 2019) are smaller than those found in the submission.

The description of the path improvement is not very clear; in particular the definition of the state used in the Dynamic Programming
algorithm. Is it a path? Is it the length of a path? Also, the statement ""f(*) denotes a state featuring dynamic programming"" is hard
to decipher.

Training time is not discussed at all.

Limitations:
Limitations are mostly addressed in a reasonable way. I suggest to add the following aspects:

I think that for small instances, the approach by (Rahmamoorty et. al 2024) may surpass the results reported here, which should be mentioned. 

Also, you should at least briefly mention the training time, since this makes it easier to assess the trade-off between offline effort and online performace.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
7nbAots3f8;"REVIEW 
Summary:
This paper introduces a new method to learning unsigned distance functions. Specifically, the method leverages local shape priors which brings geometry priors and is also able to handle noises and outliers. The results demonstrate that the proposed method outperforms previous baselines, especially in the corrupted situations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The idea of learning local shape functions is widely explored in the SDF or occupancy based methods, but is not yet introduced to the field of UDF-based reconstruction. This paper combines the strengths of local functions and UDF which can represent shapes with arbitrary typologies.
2. The paper is overall easy to follow.
3. The comparison are comprehensive by introducing recent works as the baselines. 
4. The method can better handle the noises and outliers compared to the previous prior-free methods.

Weaknesses:
1. The idea is straightforward to learn local shape functions as the prior for global shape reconstruction. Similar ideas are proposed in SDF-based methods. I would like to see more insight in the differences to the previous methods and also the importance of introducing the local geometric priors for UDF learning. 
Local Implicit Grid Representations for 3D Scenes (CVPR 2020)
Surface Reconstruction from Point Clouds by Learning Predictive Context Priors (CVPR 2022)
Deep local shapes: Learning local sdf priors for detailed 3d reconstruction (ECCV 2020)
2. The visualization is not quite convincing in the geometry details. For example, the reconstructions of real-scans in Fig.6 seems worse than the results of other methods in their papers. The local shape functions is expected to produce reconstruction with more details and sharper edges, but the scene reconstructions lack details. 
3. Also, some reconstructions are too fat, which in my opinion, is caused by the inaccurate UDF near the zero-level set, since the method use DCUDF for UDF meshing.

Limitations:
Please refer to the strengths and weaknesses above.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an approach to reconstruct 3D surfaces from point clouds, using unsigned distance fields. The proposed approach consists in training a specific neural network architecture to predict UDF values from local point cloud patches, which can be triangulated using UDF meshing methods. The paper mathematically analyses the possible local patches that can appear, smooth and sharp, and trains the proposed architecture with them. Once trained, the neural network is queried for UDF values from a point cloud, and it computes them by extracting a local point cloud patch for each query point and applying the information learned during training. A denoising module is employed to reduce the impact of noise and outliers. Experiments are carried out on ShapeNet cars and DeepFashion3D, synthetically sampling point clouds from the dataset meshes and reconstructing surfaces using a number of baselines, an extracting meshes using DCUDF. Results show that the performance of the proposed method is not the strongest on clean data, but it is so in the presence of noise and/or outliers. Experiments on real-world point clouds are shown qualitatively.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
3: good

Strengths:
The analysis of the possible kinds of local surface patches is well thought out and significative, enabling the possibility to treat the problem of surface reconstruction locally instead of globally, with an extensive training set that covers virtually all possible cases. I believe this is a good contribution, and the main strength in the paper.
The performance of the method on noisy data, albeit the noise was introduced synthetically, is good; the performance with synthetic outliers looks impressive.
The writing quality is generally good, with some exceptions on clarity (see weaknesses).

Weaknesses:
I believe there are a few weaknesses in the paper, mostly regarding clarity, architecture validation and experiments. I list them in no particular order.
1) The paper proposes a complex architecture, consisting of two branches, a cross-attention module, fully connected layers and a denoising module. The latter is only qualitatively justified in Fig.8, but the rest of the architecture is not validated in any way. A (quantitative) experimental evaluation of why this architecture is suitable for the task would be needed, in my opinion, as well as the intuition that led the researchers to arrive to this final architecture.
2) The cross-attention mechanism is unclear to me: the two branches (points and vectors net) output a latent code. What is the meaning of a cross-attention module on a single input token (actually one as K-V, one as Q)?
3) In the experiments, a few details are not specified, for example the query grid resolution and the number of points used for the Chamfer distance.
4) A time evaluation is completely missing. The paper claims ""Our method is computationally efficient"" in the introduction, which is partially justified by the comparatively better training times and storage requirements with respect to GeoUDF, but there is no evaluation of the time required by the method to reconstruct a surface, compared to the other methods. Notice also that DCUDF is an extremely slow meshing algorithm, so the evaluation should be performed both with it and without it.
5) The paper claims ""superior performance in surface reconstruction from both synthetic point clouds and real scans, even in the presence of noise and outliers"". The experiments however show no superiority in the absence of noise and outliers. Moreover, there are no quantitative experiments on real scans, making it impossible to evaluate whether the noise robustness on synthetic noise also translates to real data. Additionally, the qualitative experiments shown on real data lack comparison with the baselines. Thus, in general, I find the experimental section incomplete and not fully convincing.

Limitations:
A limitation of the method on completing incomplete surfaces has been correctly addressed and disclosed. 
The limited performance on clean data is shown in the experiments, but not acknowledged in the claims in the introduction.
A quantitative assessment of the performance on real scans and of the time required to reconstruct the surface are missing, making it harder to fully evaluate the possible limitations of the method with respect to the claims.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel training strategy to learn Unsigned Distance Fields from local shapes. The idea is to train the model on a dataset of point cloud patches characterized by mathematical functions representing a continuum from smooth surfaces to sharp edges and corners. Although trained only on synthetic surfaces, it demonstrates a remarkable capability in predicting UDFs for a wide range of  surface types. The method is evaluated on “Car” category of ShapeNet dataset in addition to DeepFashion3D, and ScanNet datasets. Furthermore, the paper shows results  on scans from real range scan dataset , in addition to ablation studies highlighting the robustness of the method to noise and outliers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written and easy to follow.
- **Novelty**: While training on local patches is not new, the design of the local shapes and network architecture is novel and effective.
- **Performance**: The paper shows good generalization results while being only trained on local synthetic patches and more robustness to noise and outliers in the input pointcloud.

Weaknesses:
- **Inference time**: While the method is better in terms of data storage space, data preparation time and training time, no comparison regarding the  inference time is provided.
- **Patch radius**: The patch radius used at test time is a crucial hyper-parameter for the method. A discussion about how to set this parameter and how it depends on the point cloud density/size would strengthen the paper.

Limitations:
The authors adequately addressed the limitations of the method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method for open surface reconstruction from 3D point clouds. They train a network to predict unsigned distance functions (UDFs) from point cloud patches using only synthetic data of quadratic surfaces. Evaluation shows that the trained network generalizes well to other complex patterns and is more resilient to noise when reconstructing 3D surfaces from point clouds.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea of training a UDF regression network using only synthetic data of quadratic surfaces is quite intriguing. This approach allows for a controlled and systematic way to generate training data, which can be more consistent and free from the imperfections and variability found in real-world data. 

Using quadratic surfaces as a basis for synthetic data is quite interesting where the authors argue that quadratic surfaces can approximate various local geometries. I have some doubts about this but it is reasonable and novel to a certain extend.

Weaknesses:
I have three main concerns: potential biases with the synthetic training data, the evaluation scheme, and the applicability in practice due to the patch radius.


## Potential Biases with Synthetic Training Data: 

The use of primitive geometrical patches as training data for a UDF regressor might introduce biases. The observation that any local geometries can be approximated by quadratic surfaces is only valid at a very fine resolution, which requires dense point clouds to observe reliably. It is unclear how many patches have been synthesized and whether they provide a good approximation of universal geometrical primitives. Some analyses would be helpful here: for example, using the ShapeNet car dataset, cropping all local patches from each car, and finding the closest synthesized one to check for approximation errors. Are there any patterns with sufficiently high approximation errors? Can we perform these analyses at different resolutions and see how they correlate with surface reconstruction?

## Evaluation Scheme

The testing data is simulated to match the scenarios the method is designed for: the point cloud is quite dense, and artificial noise is added similarly to the training data. There is no quantitative evaluation for real-world scanned data. 

## How sensitive is the method to different patch radii?

The value of 0.018 is oddly specific. I suspect that with a larger value of r, the method will generate overly smooth surfaces (as shown in Figure 6 - right), and with a smaller value of r, it will generate holes due to the point cloud not being dense enough. Overall, there is an inherent issue with this trade-off that may not be resolvable with this approach. Detailed experiments varying the patch radius and analyzing the impact on reconstruction quality would be helpful.

## Additional Concerns

- Ablation studies on the network architecture are missing. I am unsure about the roles of the two branches and the cross-attention mechanism. There is a potential issue with the reliance on Point-Net for embedding computation. Is this network pre-trained on other datasets? If so, we should be careful with the claim of using only synthetic data, as pre-training on real data could influence the results.

- It also seems that the method could be quite slow. The authors should include a speed test to provide insights into the computational efficiency of the proposed approach. Evaluating the method's runtime on different hardware setups and for varying point cloud sizes would give a clearer picture of its practical applicability.

Limitations:
The authors acknowledged that the proposed method cannot handle incomplete point cloud. However, it is unclear what how resilient it is when dealing with this.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
7g8WSOHJtP;"REVIEW 
Summary:
1. The paper unifies existing heterophilous graph neural networks (HTGNNs) into a Heterophilous Message-Passing (HTMP) mechanism.
2. The authors reveal that the effectiveness of HTMP is due to increasing differences among node representations belonging to different classes.
3. Guided by this revelation, the paper then introduces Compatibility Matrix-aware Graph Neural Network (CMGNN) to further enhance HTGNNs.
4. The authors conduct fair evaluations and comparative analysis on multiple benchmark datasets, highlighting the superior performance of the HTMP mechanism and the proposed CMGNN method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The claims are supported empirically by a detailed comparison across multiple benchmark datasets.
2. The paper is well-written and clearly structured, with each section logically building on the previous ones.

Weaknesses:
1. Several research publications [1, 2, 3] have used compatible matrices to boost the effectiveness of GNNs on heterophilic graphs. In-depth qualitative and quantitative comparisons are missing from this submission. Including such analyses would significantly increase the importance of the contributions.
2. Some claims made in the paper, such as Observation 1 and Observation 2 in Section 4, would benefit from additional analysis. For instance, including theoretical analysis with formal notations would provide more rigorous support for these claims.
3. Existing survey articles have unified and categorised message passing on heterophilic graphs [4,5,6]. This submission should compare and position the proposed HTMP unification against these categorisations.
4. The experiments lack a comparison of training times with baseline models. Including an analysis of the tradeoff between accuracy and training time would greatly enhance the results.




References:
1. Simplifying Node Classification on Heterophilous Graphs with Compatible Label Propagation, In TMLR'22,
2. Explicit pairwise factorized graph neural network for semi-supervised node classification, In UAI'21,
3. Graph Neural Networks with Heterophily, In AAAI'21,
4. Graph Neural Networks for Graphs with Heterophily: A Survey,
5. Learning from Graphs with Heterophily: Progress and Future,
6. Heterophily and Graph Neural Networks: Past, Present and Future.



**Edit post Rebuttal:**
The authors have promised to include detailed comparisons in a future revised version. Since these details cannot be verified within the review period, I will lower my confidence from 4 to 3. However, given that other major concerns have been addressed, I will raise my rating by one point from 4 to 5.

Limitations:
The authors have provided some discussion on the limitations of their work (for instance, see section 7 on Page 9).

Potential negative societal impacts are not relevant to this study.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work revisits the message-passing mechanisms in existing HTGNNs and reformulates them into a unified heterophilous message-passing (HTMP) mechanism. Based on HTMP, the authors propose a new framework named CMGNN. Experiments on 10 datasets with 13 different baseline models demonstrate the effectiveness of the proposed framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This work proposes a unified heterophilous message-passing (HTMP) mechanism, which could be a guideline for further research on heterophilous GNN. 
2. Based on the HTMP mechanism, this work proposes a new framework named CMGNN, which is novel and has basic value.
3. The effectiveness of the HTMP mechanism and CMGNN framework is well supported by experiment results.

Weaknesses:
1. Paper presentation could be further improved. For example, the conception of ""good"" heterophily and ""bad"" homophily deserves further explanation. There are some spelling mistakes, such as ""heterophilious"" in line 12.
2. If space permits, I feel like moving experiments in Appendix C to the main body would be better for the introduction of *Observation 1*. 
3. It might be hard to follow as this paper has so many equations, especially those about CMGNN. So I suggest providing a flow chart or a pseudo algorithm for better understanding.
4. Conclusions in this paper are mainly based on experiment results. It would be better if corresponding theoretical analysis or proofs are provided.

Limitations:
As the authors mentioned, this work mainly focuses on semi-supervised settings, which could be further generalized.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to address the question of ""why does message passing remain effective on heterophilous graphs"" and proposes a unified framework called heterophilous message-passing (HTMP) mechanism. It extensively reviews the architecture of existing heterophilous GNNs under this framework. It then moves on to discuss the empirical observation that the success of message passing in existing heterophilous GNNs is attributed to their implicitly enhancement of the compatibility matrix among classes, and proposed a new GNN approach called CMGNN to further enhance the separability of the compatibility matrix for different classes in the message passing process. The paper includes an extensive empirical analysis involving 10 benchmark datasets and 13 well-established baseline GNNs, and show that the proposed CMGNN approach has the best overall performance against the baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The writing is clear and well-organized for most parts of the paper; 
- The paper gives an extensive survey of existing message-passing GNNs under the HTMP mechanism in Table 1 and Appendix A. 
- The experiments are well-thought and extensive: it addresses the drawbacks of the previous homophilous and heterophilous node classification benchmarks identified in previous works by using more recent benchmark datasets, and include 13 baselines for a comprehensive evaluation of the proposed method.
- The proposed approach, CMGNN, has the best overall performance against 13 baselines on 10 benchmark datasets.

Weaknesses:
- This work builds upon the findings of several previous works regarding the effective designs for GNNs under heterophily and when is heterophily challenging (or in other words, ""bad"") for GNNs. While the authors cited these works in some parts of the paper ([6,9,12,18] in References), I feel that **some of the observations in the paper overlapped with the findings in previous works, and their connections and differences are not clearly stated in the paper**. 
  - For example, Observation 1 seems to overlap with the previous observations made in [6] (""to ensure that the neighborhood patterns for nodes with different labels are distinguishable, the inter- class similarity should be low"") and [9] (""two key factors, low-degree nodes and complex compatibility matrices, deteriorate the distinguishability of the neighborhood label distributions when coupled with heterophily, thus making heterophily a unique challenge for GNNs in most cases""). 
  - Given this, I also think that the claim in the related work section (line 732-734) that ""these reviews ... not exploring the reason behind the effectiveness of message passing in heterophilous graphs"" is inaccurate, as this paper is in fact built upon these analyses regarding the effectiveness of message passing in heterophilous graphs. 

- Section 5 (method) is too condensed to present a clear picture of how the proposed Compatibility Matrix-Aware GNN (CMGNN) works for the readers. For example, it is unclear what ""topology structure"" that the authors are considering as ""additional available node features"", and the term in Eq. 7 is not well explained. The authors also didn't explain clearly in the main paper how is the ""soft pseudo labels"" being generated for the model. It will help with the understanding if the authors can include a figure showing the architecture of the proposed CMGNN model. I feel the ""method"" section is the most novel part in the paper and deserves more length in the paper. 

- It would be good to analyze the computational complexity and/or compare the empirical runtime of the model with the baselines. 

- As a minor point, the ""Norm"" term in Eq. 3 should be explained as ""L1 normalization for matrix row vectors"" to avoid the confusion that the normalization is done with the L1 norm for *matrix* (instead of for vectors).

Limitations:
The authors acknowledged the limitation that the proposed HTMP framework is only applicable to GNNs following the message-passing mechanism. One additional limitation is that the paper is mostly empirical and does not give theoretical underpinnings.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
7VNvM9SnRE;"REVIEW 
Summary:
This paper considers the Adaptive Optimal Assortment (AOA) problem a.k.a. Utility Maximization with Subset Choices. The goal of this problem is to find the optimal profit-maximizing subset of size up to m (Top-m-Objective) or its weighted variant (Wtd-Top-m-Objective). Given a selected subset, the feedback follows the Plackett-Luce (PL) choice model that returns an item from the subset or a ""no-choice"" option. The probability of choosing each item is proportional to their underlying score/utility values.

The paper proposes a new algorithm, AOA-RB, that is claimed to be practical, efficient, and optimal. Compared to previous works, this algorithm does not require sampling the same subset repeatedly nor assumes a strongest default item. Later, the authors extend this algorithm with adaptive pivots that further improves performance.

The theoretical analysis shows that AOA-RB obtains regret guarantees that build on a novel ""Rank-Breaking"" parameter estimation technique for the discrete choice model.

The performance of AOA-RB is further demonstrated in numerical experiments using synthetic datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Problem Statement
- Clear presentation and motivation. Easy-to-follow section.

Algorithm
- Clear strengths are the relaxation of previous assumptions, e.g., repeated sampling of the same subset or the assumption of a strong default item
- The algorithm is well-presented and easy-to-follow.
- The adaptive pivot extension of the AOA-RB is a clear improvement that provides significant improvements

Theoretical Analysis
- The new concentration lemmas in Section 3.2. are claimed to be novel by the authors.
- Regret guarantees are provided for both objectives. The main strength is Theorem 6 which analyses the regret of the adaptive pivot version of the algorithm and shows a regret bound that does not blow to $\infty$ in corner cases.

Experiments
- The numerical experiments section further demonstrates the performance improvement of AOA-RB over the state-of-the-art MNL-UCB algorithm. It highlights especially the benefits of the adaptive pivots.

Weaknesses:
Introduction, Related Works, and Contribution
- Certain claims are not supported, e.g., Line 21 ""Studies have shown that it is often easier..."" but it lacks citation which studies the authors refer to.
- I found some citations to be misplaced or non-supportive of the claims it is used for, e.g., [11] is used in Line 62 as a reference for Battling Bandits while it is a survey of dueling bandits. Similarly, citations [45, 46] are used for dueling bandits while they are only two examples from the literature. It would be great if authors could use consistent citations, e.g., surveys when they refer to broader literature and individual publications when specifics are important.
- Table 1 is provided for the comparison of regret guarantees but the authors do not describe it. It would be great if they could comment on the differences between the algorithms.

Problem Setting
- Limitations are not mentioned in the problem statement. For example, how restrictive is the Plackett-Luce model, and whether the approach could be extended to other models? I see that it is mentioned in Remark 1 but could be commented on in Section 2 as well.
- Both Top-m and Wtd-Top-m consider the (weighted) utility optimization problem. However, for most of the applications used as motivation, e.g., assortment optimization and recommender systems, the utility of the user which dictates the selected feedback, and the utility/profit of the subset selection (platform) are misaligned. Could the authors comment on how to formulate these problems in their setting?

Algorithm
- The $argmax_{S\subseteq [K], |S|\leq m}$ optimization is non-trivial and could be computationally expensive for large values for $K$.
- The authors claim that AOA-RB is practical, efficient, and optimal. While the theoretical analysis supports the last two claims, I struggle to find the intuition behind the algorithm. Could the authors elaborate further on this point?

Experiments
- Numerical experiments demonstrate performance only in synthetic data. Given the clear application and motivation of the paper, I would like to see experiments that reflect these problems.
- I recommend the authors to use larger figures. Axes and titles are hardly visible in the printed version.
- Only one baseline is considered. It would be appreciated if the authors could include the other algorithms mentioned in Table 1 for numerical comparison besides the theoretical one.

While the paper is easy to read and follow even for readers not familiar with all the works in the area, the inconsistent citations and unsupported claims have to be addressed before the paper would reach publication standards.

Limitations:
Limitations are mentioned in the paper, however, it is often not directly connected, e.g., the assumption of the PL model is only addressed in Remark 1. I would suggest the authors address limitations more clearly when they appear for easier readability.
The work is mainly theoretical without any immediate direct societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the online MNL assortment optimization problem, where the goal is to learn MNL parameters while suggesting assortments, with the goal of either learning the top-m highest utility items or learning the maximum revenue set with m items. They use a UCB-based approach on pairwise win rates to get a UCB for utilities, which can then be fed into a traditional assortment optimization algorithm. The authors show this approach achieves asymptotically optimal regret and does not require assumptions used by previous approached. The basic algorithm relies on comparisons between each item and the no-choice option, but they also introduce a more sophisticated adaptive pivot approach that works better when the no-choice option is rarely selected. In experiments on synthetic data, their assortment optimization approach performs significantly better than the previous state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem studied is natural and important. 
2. The presentation is generally clear.
3. The technical quality seems good, although I cannot attest to the correctness of all the proofs in the appendix.
4. The UCB approach on pairwise win rates is clever and appears original.

Weaknesses:
1. The algorithms and proofs could use some additional description/intuition. Some of the steps in the proofs take rather large leaps.

Limitations:
I think the limitations of the paper were adequately stated

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the problem of active online assortment optimization problem with preference feedback, which has been extensively studied. The paper argues that the previous studies have some unrealistic assumptions such as: there is a ‘strong reference’ which is always included in the choice sets; the same assortments can be repeatedly selected. Without these assumptions, they propose some efficient algorithms for the problem of regret minimization in assortment selection with Plackett Luce (PL) based user choices.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proves the regret bounds of the proposed online learning algorithms. The regret bounds are proved based on some concentration guarantee for estimating the score parameters of the PL model using ‘Pairwise Rank-Breaking’.

Weaknesses:
1.  I cannot fully understand the motivation of the paper. The paper says that two major drawbacks of the previous studies include: the existing algorithms assume that the ``no-choice’’ option is stronger than the other choices, and they may query the same set of items for multiple times. It seems that the focus of the paper is to address these drawbacks. However, I think that these ``drawbacks’’ may not be real. First, it is natural that most of the customers will not choose any product, so it is very reasonable to assume that no-choice option is stronger. Second, in the typical assortment optimization scenario where customers arrive online one by one, showing the same set of items to different customers for multiple times absolutely will not cause any problem.  So I think that addressing these ``drawbacks’’ has very limited value.
2.  The regret bounds proposed by the paper is actually K\sqrt{T}\log T. It seems that this regret bound is weaker than those of the previous studies such as [2] (at least by log factors on T). The authors may argue that their bounds are better when \theta_{max}\rightarrow \infty, but this depends on the assumptions made on specific application scenarios, which is questionable as explained in my last comment.
3.  The experiments are conducted using some specific values of \theta and hence are not very convincing. I think that more experiments on more applications are necessary to demonstrate the superiority of the paper.

Limitations:
see the above

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the problem of active assortment optimization in MNL model.

In the problem of assortment optimization, we have  a large universe of products i=1,2,\dots N, each of which generates a given revenue r_i for the seller.  In MNL model  each product i has a value \theta_i to the customers and when customers are offered a subset of products they choose each item (including the no-choice option) with a probability proportional to their value. We also assume there is a no-choice option with revenue 0. The seller’s objective is to identify the assortment of products which generates maximum expected revenue. 

In the active version of the problems, the values of items  \theta_1, \theta_2,\dots , \theta_N, are not known to the seller. Thus, the seller  shows a  subset of items from the universe to the customers at  rounds 1,2, \dots ,T  and estimates \theta_i s  based on the observations. After approximating these values, the seller may solve the problem in static setting and find the optimal assortment.  This strategy is known as exploration and exploitation. 

In active assortment optimization, the objective is to minimize the regret of the algorithm which is defined as the summation over rounds t=1,2,\dots T the difference of the expected revenue in each round from the optimal revenue.

Prior works for instance [2] provided an algorithm for this problem by estimating at each round a high probability upper bound for the values \theta_i,  and then solve the static problem using the upper-bounds. In [2] the authors assume that \theta_0 (the value assigned to no-choice option and thus its probability ) is the highest among all items. 

The submitted manuscript claims that they provide an algorithm with a similar regret bound to [2] which does not have the restriction of assuming the no-choice option has the highest value. Their suggested approach is similar to that of [2] (finding high probability upper bounds for the parameters) but it is hard to follow all details of obtaining the upper bound and how it removes the restriction imposed on the value of the no-choice option. 

The result, if true, is interesting but I found the paper hard to read and got lost in section 3.1. I think that the paper will benefit greatly from rewriting and improving the presentation. 

I will detail my confusions as follows: 

- In Equation (3) on line 173 there is a variable x which is not defined up to this point. I understand that x appears to bound the probability of error in Lemma 1. But you have to introduce it before you use it the first time. 
- Between line 176 and 177 what is the + sign on the denominator of the equation? you use this notation again in another equation between lines 252 and 253.
- In equation 3 you show an upper bound on \hat{p_ijt} which then turns to a bound on \theta_i s. But in Lemma 1 you have shown a different upper bound for \theta_i. Can you explain the connection of these two bounds. 

A few minor typos:

Lemma 1. atleast-> at least
^ucb is sometimes with roman font and sometimes normal font. 




[2] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5):1453–1485, 2019.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The problem of active assortment optimization is a fundamental problem in revenue management 
- The result is interesting if correct, as it removes an important restriction from prior algorithms.

Weaknesses:
- The results are poorly presented and it is hard to follow the paper. The paper lacks an explanation of main intuitions . 
- The technique seems to be similar to [2] as both papers obtain high probability upper bounds for the parameters and then solve it in an static setting. An intuitive explanation of how the given different upper bound is obtained, why it is correct, and how it removed the restriction on no-choice option is not provided.

Limitations:
limitations are not discussed but there are several future directions that have been discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
7Su7gAei1l;"REVIEW 
Summary:
This paper introduces SymmetricDiffusers, a new approach to learning complex distributions. It works by breaking down the problem into simpler steps: learning how to reverse a transformation using deep neural networks. The authors identify a particularly effective method for this reversal step (the riffle shuffle) and provide guidance on choosing the right length for the process based on mathematical properties. Additionally, they propose a more powerful alternative to a common distribution (the generalized Plackett-Luce distribution) and a theoretically sound strategy for improving efficiency (the denoising schedule). Experiments show that SymmetricDiffusers performs extremely well on various tasks, including sorting images, solving puzzles, and optimizing routes.

Soundness:
1: poor

Presentation:
3: good

Contribution:
1: poor

Strengths:
The proposed method in general is interesting and seems effective in the examples studied in this paper.

Also, the problem studied is interesting.

Weaknesses:
My main concern is the proposed method is not the state-of-the-art. A clear approach is to learn invariant features and equivariant group actions using some existing methods like that proposed in Robin Winter, et al. Unsupervised Learning of Group Invariant and
Equivariant Representations, NeurIPS 2022. Such an approach. 

In Robin Winter et al.'s paper, the authors have studied the symmetric group and my understanding is disentangling invariant features and equivariant groups enables the design of flexible diffusion models since you only need to perform diffusion modeling in the invariant latent space. 

I want to see the comparison of the approach proposed in Robin Winter et al's paper. My understanding is that Robin Winter's approach enables building state-of-the-art diffusion models for molecular generation.

Limitations:
My main concern is the proposed method is not the state-of-the-art. A clear approach is to learn invariant features and equivariant group actions using some existing methods like that proposed in Robin Winter, et al. Unsupervised Learning of Group Invariant and
Equivariant Representations, NeurIPS 2022. Such an approach. 

In Robin Winter et al.'s paper, the authors have studied the symmetric group and my understanding is disentangling invariant features and equivariant groups enables the design of flexible diffusion models since you only need to perform diffusion modeling in the invariant latent space. 

I want to see the comparison of the approach proposed in Robin Winter et al's paper. My understanding is that Robin Winter's approach enables building state-of-the-art diffusion models for molecular generation.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
I am unable to review this paper as it lies outside my area of expertise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I am unable to review this paper as it lies outside my area of expertise.

Weaknesses:
I am unable to review this paper as it lies outside my area of expertise.

Limitations:
I am unable to review this paper as it lies outside my area of expertise.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors aim to create a discrete diffusion model that generates permutations. This model can then be used to solve combinatorial problems including jigsaws and travelling salesman problems. To formulate their model they cover a range of forward shuffling strategies and discuss how to parametrize the reverse transition. During sampling, they also use beam search to find high probability samples. They find their method performs competitively on computational experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea of using riffle shuffles to create a corruption process over permutations and then parametrizing the time reversal of this process is novel and interesting. I enjoyed reading the paper. I believe further work will build on this as there are many instances in machine learning where needing to learn permutations crops up.

The paper is quite well written and easy to understand. It is not overloaded with mathematical equations and intuition is given for some concepts.

The experimental results seem promising as it performs on par or better (especially in high dimensions) than previous methods for learning permutations. I appreciate the ablation studies into the greedy search vs beam search and types of shuffle (in the appendix).

Weaknesses:
I think some further clarification is required for the weaknesses of the random transposition and random insertion style of card shuffling. You later say that you can merge steps of the forward process if each individual step does not induce enough mixing and so the stated weakness that these styles of shuffle have slow mixing seems moot.

You mention that you do not have access to $q(X_t | X_0)$, and I think it should also be discussed that $q(X_{t-1} | X_t, X_0)$ is also unavailable since this distribution is used in standard diffusion models to re-write the variational bound in a lower variance form, see Appendix A in https://arxiv.org/pdf/2006.11239 .

You dedicate a lot of space to discussing the various forward noising processes with different shuffling methods, which is quite interesting. However, the ablations with these different styles of shuffle are in the appendix and I think it should be in the main since they have been given such prominence earlier on in the discussion of the method, it is strange they are not included in the main experiments.

I find it difficult to follow the description of the inverse transposition parametrization, there is no intuition given for the functions $\phi$ and $\psi$ nor the functional form of $p_{IT}(\sigma)$. Perhaps this is due to space limitations but since inverse transposition is not in the main experiments (see above point), I think you should either relegate a lot of this to the appendix if you only use the riffle shuffle in practice, or try and shift the wording to properly explain these types of forward and inverse process and have experiments for them in the main.

Limitations:
The authors do a good job of discussing the limitations of various parametrizations of their method.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a discrete diffusion model to learn distribution over the finite symmetric group $S_n$. The forward process is built off of random walks on finite groups (in this case, card shuffles), and the paper learns to reverse this diffusion process with standard discrete diffusion arguments.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, I really like the paper's contributions and presentation.

* The idea is a neat application of the discrete diffusion ideas to an important area. In particular, the structure of $S_n$ is sufficiently different from standard image/text datasets as to necessitate this paper.

* The presentation is very good and the contributions are numerous.

* For the experiments listed, the method seems to provide a very strong improvement over baseline methods. In particular, these other methods are based on fundamentally different technology, so this highlights that discrete diffusion can become a very promising direction here.

Weaknesses:
There are three primary weaknesses. These should all be addressable to some degree, and I'll take any response into consideration when recalibrating my final score.

1. The model proposes to directly learn the reverse transition densities $p_\theta(X_{t - 1}, X_t)$. The issue with doing this for standard diffusion models is that this seems to hurt model training since it increases the variance of training (as, in particular, one must sample the two $X_{t - 1}, X_t$ for training instead of just one $X_t$). As such, most works use the (ultimately equivalent) mean/score-parameterizations [1, 2]. I would want to hear a bit more about if this would be applicable in the $S_n$ case (and training with this parameterization might improve the model) or if this is not possible.

2. (Related to the above). Since most modern discrete diffusion methods are formulated in continuous time, I think the paper would benefit greatly with a discussion about potentially extending the current methods to this realm. In particular, works like [3, 4, 5] have established a working theory for discrete diffusion in continuous time, so it would be beneficial to discuss how the proposed framework might fit into the established theory.

3. The experiments, while showing good results, do not show that the method is particularly scalable, which seems to be a fundamental problem in prior work that was explicitly mentioned in this paper. In particular, it seems that the maximum value of $n$ in $S_n$ is 100. While some discussion is made here that talks about transformer layers, I think large values of $n$ aren't that big of an issue in transformers due to systems like Flash Attention. So, it should be made more clear if this is a fundamental problem with the existing method, or a larger scale example (even toy) should be presented.

[1] https://arxiv.org/abs/2006.11239

[2] https://arxiv.org/abs/2011.13456

[3] https://arxiv.org/abs/2205.14987

[4] https://arxiv.org/abs/2211.16750

[5] https://arxiv.org/abs/2310.16834

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
6qJKrOulTr;"REVIEW 
Summary:
The authors propose a mathematically rigorous methodology based on Riemannian geometry for attributing network importance of tokens in a transformer models input space (e.g. image patches, or ~words in the textual domain). The proposed methodology—whilst based on sound theory—translates into an intuitive algorithm involving what appears to be a relatively inexpensive eigendecomposition. Experiments on 3 datasets across both the image and NLP domains explore how the features correlate with ground-truth inputs in the text domain, in addition to first steps towards exploring how the features affect the networks’ output logits.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A major strength of the paper is the mathematically solid approach in attempting to identify regions of the **input** space that explain transformers’ model decisions. This is an important area of study: in contrast to many recent mechanistic interpretability methods finding latent network representations (that are intrinsically hard for humans to interpret by default), salient features in the pixel/text input space are much more readily interpreted by humans.
- Whilst I am unfamiliar with geometric deep learning, the authors do a fantastic job of presenting the technical content in a digestible manner without sacrificing depth or rigor.

Weaknesses:
# [W1] Feature importance comparisons

Feature importance-based explanations are motivated on [L303] as quantifying the contribution of features `""to a model prediction""`. More concretely, around [L180], the authors motivate the eigenvalues of the pullback metric found using their method as ultimately deducing the importance of each segment (e.g. image patch)  `“with respect to the the final prediction”`. 

Consequently, a major weakness of the paper is how there is no comparison with related work for how well the proposed method’s identified important features alter the **output** logits (e.g. upon ablation).

I am slightly confused by why the authors did not adopt the established “perturbation test” experimental protocol in the baseline [1] against which they compared, to provide experimental evidence in favor of this. Currently, the only comparisons made around [L274] measure the features’ importance as they correlate to the *input’s* labels.

Concretely, the authors could, for example, ablate particular patches of MNIST and observe that the resulting performance drops correlate with the pullback metric’s eigenvalues. This would provide stronger evidence of the authors’ claims about the features affecting the networks’ output, and (crucially) ground the results in contrast to those achievable by existing methods.

# [W2] Limited experimental results & improvements

There is a lot of interesting theory here, but ultimately this is a paper with a concrete applied goal of feature attribution in transformer models. With such a new methodology with many technical details, I believe there is an extra burden of proof on the authors to demonstrate this somehow leads to additional insights / practical gains. As such, it is a relative weakness of the paper that so few experiments are performed to justify the methodology.

Beyond toy datasets, it would be interesting to see how the method performs on more complex ones (not necessarily larger ones), such as TinyImageNET. Here, we could visualize much more easily if the method helps identify salient features of animals’ body parts (for example) as being important features for classification. MNIST experiments alone in the image domain are hard to interpret given the similarity of all the input data. 

Furthermore, the method provides an almost insignificant increase of just `0.07` cosine similarity (over the baseline in [1]), on just a single dataset (and with just two baselines—for example, how does GradCAM perform here?). This is not sufficient evidence to convince me as a reader that the proposed methodology should be adopted. 

---

- [1]: Chefer, Hila et al. “Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021): 387-396.

Limitations:
Some limitations are indeed addressed throughout. However, (unless I have missed something, in which case I apologise!) I can only find the limitations of the small number of datasets used stated in the NeurIPS checklist. This needs to be stated explicitly in the main paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work attempts to find the set of inputs that generate the same neural network predictions. To this end, the authors interpret the layers of the network as transformations of the input manifold. This interpretation is used to defined equivalence classes over the inputs and to define feature importance. Finally, the tools are used to identify equivalence classes for MNIST digits and for hate speech detection with BERT.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
Section 2 does a thorough job of introducing a manifold interpretation to neural networks.  This introduction is then used to motivate multiple algorithms for finding equivalence classes of the inputs---or the setup of inputs that result in the same prediction---and identify features that are important.

Weaknesses:
The main contribution of this work is to introduce a tool for analyzing which set of inputs produce the same output. However, this is exactly the Fisher information matrix (with respect to the inputs) and has also been introduced in prior work (https://arxiv.org/abs/2104.13289). Could the authors clarify what the differences are and what the additional novelty. If the ""local data matrix"" introduced in https://arxiv.org/abs/2104.13289 is identical to the tools in this work, I think it severely diminishes the contributions of this work. Furthermore the experiments are extremely similar (such as Figure 1). 

The second weakness is the limited number of experiments. The work does not show any quantitative results: Figure 1, Figure 2 and Figure 3 is just 1 example and is not indicative of why the tools are useful. The experiments in section 4 primarily discuss wall-clock time. It would significantly help if claims such as ""(Line 266) we notice that the perturbation-based algorithm ends up producing monochrome ..."" are substantiated quantitatively. Overall, the work doesn't provide novel tools and the experiments lack a novel usage of these tools and do not reveal any new insights.

Limitations:
The authors address limitations of their work but it can be expanded upon. For example, the authors can discuss the time required to compute Eigenvalues, and other limitations such as not having any Eigenvalues to be 0 in Algorithms 3 / 4. Furthermore, their algorithm should work (in theory) for infinitesimal steps in the input manifold.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a method for exploring equivalence classes in the input space of Transformer models using a solid mathematical theory. By analyzing the Jacobian of the model, the method reconstructs and navigates these classes, offering a powerful tool for understanding Transformer interpretations and enhancing explainability. The proposed method is expected to solve problems in Computer Vision and Natural Language Processing tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
I write both strength and weakness. 

First, I must disclose that I have no prior study or background in both Transformers and manifolds. While I am conceptually aware of them, my knowledge is limited to that extent, and I lack confidence in reviewing the technical details. Therefore, please consider my review comments as feedback from a layperson in this field, focusing on the overall mathematical consistency and readability of the paper.

This paper describes mathematics in a clear and understandable manner that even a layperson like myself can grasp. Each definition and theorem is stated accurately, and I believe that the general concepts can be understood with basic knowledge.

I personally feel that the objective of this paper is not clearly conveyed. While the paper claims to contribute to explainability and sensitivity analysis through the analysis of input manifolds, the logic behind this was not clear to me in the Introduction and Preliminaries. Although the concepts of explainability and sensitivity analysis become clearer in the later chapters, it might be beneficial to provide a bit more explanation in the Introduction.

Additionally, it might be helpful to clearly define the equivalence class mathematically.

Since I am not familiar with the existing literature, I was unable to judge the novelty of this work.

Weaknesses:
See above.

Limitations:
N/A.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper develops a novel theoretical framework grounded in Riemannian geometry for analyzing the input space of Transformer models, and introduce two algorithms, SiMEC and SiMExp, which facilitate the exploration and interpretation of equivalence classes within this input space. These methods offer new insights the internal mechanisms of Transformers, and provide new understanding of how these models perceive and process input data which can be very useful in the field of explainable AI.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1 novelty: This paper provide an innovative application of Riemannian geometry to analyze the input spaces of Transformer models, which is very novel in the area.

2 Theory: This paper establishes a solid mathematical theory on how Riemannian geometry is applied to Transformer models. Based on this theory, SiMEC and SiMExp are developed to explore the input spaces of Transformer models.

Weaknesses:
In experiment, the MNIST dataset is a little bit trivial, as the pixels of the background is essentially zero. It is nice to see the application of the proposed algorithm on natural images like CIFAR.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
6Wm4202Wvc;"REVIEW 
Summary:
The paper revisited the problem of label leakage in split learning in the context of fine-tuning large models with parameter-efficient training. Based on modern use cases, they proposed two privacy-preserving protections for gradients and activations during split learning. The proposed methods are evaluated on several large models including Llama2-7B, fine-tuned using LoRA and full model fine-tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper moved forward a step on the urgent need to privacy-preserving split learning over large models and fine-tuning with LoRA.
2. The writing is generally good despite some minor issues. The flow of ideas is clear.
3. The proposed method is evaluated over different pre-trained large models, conforming to current real-world use cases of LLMs.

Weaknesses:
1. There exist several related works discussing the attacks and defense regarding the label leakage in split learning. The authors may need to compare the differences between the proposed methods and previous literature. The evaluation part lacks the comparison to existing privacy-preserving solutions over label leakage and some trivial solutions such as directly applying differential privacy, which is easy to implement.
2. In modern use cases of API fine-tuning, apart from the applications of classification, text generation with LLMs and image generation with multimodal models and diffusion are more common cases. And it is very critical to protect labels in these applications. For example, labels in text generation can contain answers to private questions in the private dataset. However, the leakage study and proposed privacy-preserving methods do not apply to these applications.
3. There are some minor writing issues that could be improved. For example, content introducing API fine-tuning and potential privacy concerns can be shortened in the introduction. The paragraph from line 53 to 58 can be reorganized so that it won't leave '[18]' for a whole line. Same thing for line 209. On line 28, write the full name of LoRA before using acronym. The authors are suggested to talk about split learning and no need to raise extra efforts for readers to understand what vertical federated learning is.

[1] Wan, Xinwei, Jiankai Sun, Shengjie Wang, Lei Chen, Zhenzhe Zheng, Fan Wu, and Guihai Chen. ""PSLF: Defending Against Label Leakage in Split Learning."" In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pp. 2492-2501. 2023.   
[2] Kariyappa, Sanjay, and Moinuddin K. Qureshi. ""ExPLoit: Extracting private labels in split learning."" In 2023 IEEE conference on secure and trustworthy machine learning (SaTML), pp. 165-175. IEEE, 2023.     
[3] Erdoğan, Ege, Alptekin Küpçü, and A. Ercüment Çiçek. ""Unsplit: Data-oblivious model inversion, model stealing, and label inference attacks against split learning."" In Proceedings of the 21st Workshop on Privacy in the Electronic Society, pp. 115-124. 2022.      
[4] Xu, Hengyuan, Liyao Xiang, Hangyu Ye, Dixi Yao, Pengzhi Chu, and Baochun Li. ""Permutation Equivariance of Transformers and Its Applications."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5987-5996. 2024.

Limitations:
Discussed in the last section.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This study addresses the privacy concerns associated with the fine-tuning of Large Language Models (LLMs), focusing on SplitNN. It explores how gradients and activations can leak data, potentially allowing attackers to reconstruct original data sets. In experiments, the proposed method reduces label leakage while maintaining minimal utility loss.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
S1. The manuscript highlights significant privacy issues in LLM fine-tuning, specifically the potential for data leakage through gradients and activations in SplitNN.

S2. Experimental results show that the proposed method significantly mitigates label leakage with minimal impact on utility.

Weaknesses:
W1. The claim that backpropagation is ""conditionally linear"" is not sufficiently rigorous. The manuscript suggests that $\text{backprop}(x, \theta, g_h+z)+\text{backprop}(x, \theta, g_h−z) = \text{backprop}(x, \theta, g_h)$ under the assumption that $\theta$ is constant. However, $\theta$ updates during each backpropagation, invalidating this assumption. Moreover, swapping the order of $\text{backprop}(x, \theta, g_h+z)$ and $\text{backprop}(x, \theta, g_h−z)$ could lead to different outcomes. Formal proof and a clearer statement of assumptions are needed to substantiate this claim.

W2. Section 3.4 describes a method to protect activations that resembles secure multi-party computation [1], lacking novelty. Its effectiveness is also questionable when only one adapter is present.

W3. The proposed protection mainly focuses on labels. In practice, data such as personal identifiers may pose a greater risk than labels. For example, knowing (a) Alice's salary (label) is included in the database is considered a more serious leakage than knowing (b) someone earns a salary of 3.2k. The manuscript should explore if the proposed method can also protect other sensitive features.

**References**

[1] Du, Wenliang, and Mikhail J. Atallah. ""Secure multi-party computation problems and their applications: a review and open problems."" Proceedings of the 2001 workshop on New security paradigms. 2001.

Limitations:
There elaboration on limitations is insufficient.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses privacy leakage during API-based Parameter Efficient Fine-Tuning (PEFT). Their designed P3EFT is a multi-party split learning algorithm that leverages PEFT adjustments to uphold privacy with minimal performance overhead. Their method proves competitive in both multi-party and two-party setups while achieving higher accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Researching API-based fine-tuning for large models is an intriguing topic, especially considering that many clients face challenges loading such large models due to size and computational constraints. In this scenario, privacy concerns regarding client data become paramount. This paper aims to mitigate potential privacy leakage by obfuscating gradients and parameters communicated during transmissions.

Weaknesses:
Their approach shows limited privacy improvement compared to the scenario  Without LoRAs, as indicated in Tables 1, 2, and 3, thereby restricting the overall benefits.

Limitations:
The paper addresses limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an algorithm to preserve the label privacy while achieve good accuracy in the split learning regime. The algorithm is used in parameter-efficient fine-tuning and empirically tested on some language models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper clearly presents its motivation and contribution. The modification on the back-propagation is reasonable and empirically effective across three models and different attacks that are tested

Weaknesses:
The main concern is the scalability of this method. For one iteration, the number of backpropagation is m (at least 2), which is too slow even for PEFT. The computation cost of PEFT is 2/3 of full training so if m=2, the total cost of this method is 4/3 of full training.

Looking at the code, there are 7 hyperparameters introduced by this method, which may be hard to use in practice. I would suggest the authors fix some hyperparamters that the algorithm is robust to, to reduce the number of tunable hyperparameters.

Also the experiment results on SST2 show a severe leakage around 10% compared to without LoRA (even though this is relatively weaker than other methods).

Limitations:
As in its current presentation, the method is limited to language models, split learning (two parties), and label privacy. The empirical evidence is limited to text classification (specifically, this method does not apply to natural language generation where LLAMA is originally trained for) and LoRA. Each limitation can be relaxed, e.g. extending to vision models, data reconstruction, additional PEFT, etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
5tOVh81aze;"REVIEW 
Summary:
While existing scaling law studies look at compute-optimal pretraining, this paper considers scaling laws in the context of both pretraining and downstream performance. They perform scaling experiments and find that performance is predictable even in overtraining, and average downstream performance is also predictable.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I think this is a solid paper that attempts to answer an important question. While I’m concerned about the lack of novelty (see the weaknesses), I overall lean towards accepting the paper. I think that the fact the paper reproduces findings from other papers using a different methodology is a good sign that the overall results are correct, and is valuable information (e.g. [Owen 2024](https://arxiv.org/pdf/2401.04757) also finds that average downstream performance is more predictable than for individual downstream tasks). To me, this is the primary contribution of the paper, and an important one as such.

Weaknesses:
To my mind, the largest weakness of this paper is the lack of novelty. In particular, my understanding is that the most important findings are that (1) performance is predictable in overtraining, and (2) average downstream performance is predictable. I’m not sure why (1) should be surprising – doesn’t a parametric scaling law, such as method 3 from the Chinchilla paper, also give the ability to predict loss when overtraining? I think the authors could improve the motivation for this consideration by providing a back of the envelope calculation: for instance, I plugged in the model size and dataset size for Gopher 280B into the Chinchilla scaling law and got a predicted test ppl of ~7.3 on MassiveText. However, Gopher actually had a (validation) perplexity of ~8.1, this constitutes a relative error of around 10% – substantially larger than the relative errors obtained by the authors of this paper. If the authors can provide an argument of this sort I'd find that helpful. 

I thought (2) was a more interesting claim, but I’ve seen this analyzed in Owen 2024 (https://arxiv.org/pdf/2401.04757), albeit with a different methodology. As such, I felt that the core results of the paper weren’t very novel. However, I’d be happy to update my assessment if the authors can provide evidence that my understanding is incorrect. 

One interesting point that the authors mentioned is that performance on individual tasks is less predictable. But this is only mentioned in passing, and I felt that it could be expanded upon a fair bit. What are the implications of this observation? Are there any patterns for which individual tasks are or aren’t predictable? 

I’m slightly concerned about data leakage being an issue for the downstream tasks, given that the training data (The Pile and RedPajama) covers a wide swath of the internet, and some of the downstream benchmarks have been criticized for data leakage or having label errors. 

Minor comment: The last paragraph of section 5 is a bit confusing: “There has been a rise in over-trained models [113, 114] and accompanying massive datasets [112, 82, 104, 3]. For example, Chinchilla 70B [45] is trained with a token multiplier of 20, while LLaMA-2 7B [114] uses a token multiplier of 290.” This makes it sound a bit like Chinchilla is overtrained, which I don’t think the authors are trying to say, so I’d suggest something like the following instead: “For example, while Chinchilla 70B is trained compute-optimally with a token multiplier of 20, LLaMA-2 7B…”

Limitations:
I felt that the authors did a good job describing some of the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a scaling law for the “Chinchilla over-trained” regime where models are trained on many more tokens (in this paper, up to 30x) than Chinchilla-optimal. They motivate a scaling law relating pre-training compute and “over-training” to validation loss. They empirically demonstrate that the proposed scaling law accurately predicts the validation loss of a 1.4B 32x over-trained model and a 6.9B Chinchilla-optimal model. They then study a simple scaling law relating perplexity to downstream benchmark error. They select a subset of 17 benchmarks for which a 154M parameter models performs 10% above random chance accuracy, and show show that average downstream error of the 1.4B and 6.9B models is predictable.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The setting considered by the paper is very relevant at the moment, as major model releases in the past year fall precisely in the “Chinchilla over-trained” regime. The work is also novel, as I am not aware of prior work that proposes and empirically validates scaling laws tailored for the Chinchilla over-trained regime.

The proposed scaling law in the over-trained regime is well-motivated both by prior scaling laws and by further empirical observations in the over-trained regime (Figure 2). While the models trained (5-8 1e21 FLOPs) are at least two orders of magnitude smaller than the latest open models (e.g., Gemma 2, Llama 3, Qwen 2), the experiments are of a large enough scale for a proof of concept.

The paper is clear and it gives sufficient details on the experimental set-up.

Weaknesses:
My main concerns are twofold: authors do not compare with the standard scaling laws of Kaplan et al. (fitted on their own testbed), and it is unclear how authors choose the models used to fit their scaling laws (Table 1).

Authors do not compare their over-trained scaling law with that proposed by Kaplan et al. The authors could fit this scaling law as in Hoffman et al. , Section 3.3, without any additional model training. Specifically, how well can the standard Kaplan et al. law predict the validation loss of the 1.4B over-trained model, when fitted on the model testbed with N < 1B described in Section 3.2?

Regarding the claim that validation loss (resp accuracy) is predictable with 300x (resp. 20x) less compute, I find this misleading, since authors train and evaluate a reasonably large model testbed, but only report the compute required for the 5 (resp. 6) models that they ultimately choose for the fit.  The authors do not discuss how this “train”/“test” split was chosen. Clearly, the train/test split should be chosen before seeing the evaluation results for any of the models, rather than including models until the fit seems ""good enough"", or choosing the smallest subset for which the fit is “good enough”. Otherwise, both the claim of 300x/20x compute as well as Figure 5 are misleading. Similarly, Figure 1 would be misleading, and it should include all models with N < 1.4B. 

The authors consider token multiplier M <= 640, however current models are even more overtrained. If am not mistaken, for Llama 3 8B, M ~= 2000. Demonstrating the validity of the proposed scaling laws for the amount of over-training of current models would have been ideal, even at smaller model scales.

Lastly, the proposed scaling laws are validated at substantially lower compute scales than current models with publicly available weights. It would have been ideal to at least see results for over-trained 7B models. I understand that the experiments presented in the paper already require a substantially amount of compute, and more closely matching the compute scales of recent models would be unfeasible for most research labs — therefore I am not taking this point into consideration when scoring the paper.

Limitations:
Limitations are adequately discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the power laws (scaling laws) of neural language models, particularly from the perspective of over-training and the relationship between validation loss (perplexity) and NLP downstream tasks. The authors define over-training as the situation where runs consume large amounts of computational resources, and they introduce a token multiplier, M. It is computed by D / N, where D is the number of training data tokens and N is the number of parameters. Through various model training setups and three different training corpora, the authors demonstrate that the validation loss can be computed and predicted using an equation that includes M. They also introduce another equation that illustrates the relationship between validation loss and downstream task error.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution of this paper is the exploration of the scaling laws in language models concerning over-training and NLP downstream tasks. These results, including equations and practical outcomes, are beneficial for researchers and engineers developing large language models. As highlighted by the authors, these insights are valuable for researchers in their future work.

Weaknesses:
The authors mentioned several limitations and future work in the paper. I agree with them, and especially the ‘scaling up’ part is the primary concern of this paper. The model sizes range from 0.011B to 6.9B, but open-source models are larger than these sizes - for instance, Llama 2 starts at 7B, and Llama 3 starts at 8B [1]. Furthermore, model size is crucial for techniques such as CoT [2]. I hope to hear the authors' opinions on this concern.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
5p57uCUH8k;"REVIEW 
Summary:
This paper formulates the higher-order curve estimation problem as a NODE problem, enabling effective and accurate solutions with standard ODE solvers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well-written and structurally organised.

Weaknesses:
Reference formats are not consistent.

Limitations:
As described above

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper mainly addressed the problem of insufficient data for low-light enhancement. Specifically, it proposed CLODE , which employs Neural Ordinary Differential Equations to learn the continuous dynamics of the latent image for the first time. The experiments demonstrate the CLODE performs better than other unsupervised learning methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ This is the first attempt to formulate the higher-order curve estimation problem as a NODE problem.
+ CLODE can offer user controllability, enabling users to manually adjust exposure.

Weaknesses:
- Details of User Controllable Design. Despite the better result with use control, detail of the users is missing. For example, the number of volunteers, and whether they are banned from the ground truth image before they adjust the output image. Also, involving human feedback bring much more time in the inference stage.
- In Sec. 3.3 Inference Process, the relationship between the output image IT and noise-free image is questioned. Each iteration includes a noise removal module, yet the output image still contains some noise, contradicting the expectation of a noise-free result in the model.
- Experimental Setup: The experimental setting described in [1] seems more suitable for unsupervised methods. Using only a single dataset for training in this study does not adequately reflect the advantages of the proposed method. A specific analysis comparing and justifying the differences in experimental setups is necessary.
- Model Iteration Selection in ""CLODE+"" (Table 2): The manual operation required to select the iteration step raises concerns. How is this value determined to ensure suitable results? This approach appears more suited to image retouching tasks than enhancement.
- Concern about the fair comparison with previous methods. This paper uses 5 different losses. I wonder whether only part of them is used in previous methods, are the proposed method align with previous methods? For example, some Retinex-based method does not explicitly consider the impact of noise, and they do not have Noise Removal process. Does CLODE still outperform other methods without noise removal? More ablation experiments are needed for thorough explanation.
- Effectiveness of Noise Removal Module: In the first toy scene in Figure 4, as well as Figures 7 and 8, there is noticeable noise residue and some degree of color distortion, which casts doubt on the effectiveness of CLODE and its noise removal module for low-light enhancement.
- More explanation of the superiority of CLODE. Can author provide clearer explanation of the mechanism? For example, in Figure 9 of Supp material, is the better results comes from the more iterations, or more iterations at the early stage, where the estimation is harder?

[1] Learning a Simple Low-light Image Enhancer from Paired Low-light Instances

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript introduces CLODE, which learns low-light image enhancement using neural ordinary differential equations (NODE). The key innovation lies in formulating the higher-order curve estimation problem as a NODE problem. Experimental results show that the proposed approach outperforms state-of-the-art unsupervised counterparts across several benchmarks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	The paper is easy to follow.
2.	Using neural ordinary differential equations to address the iterative curve-adjustment update process shows better performance.

Weaknesses:
1.	The novelty is limited, and the technical contribution is incremental. Apart from formulating the curve estimation as a NODE problem, the paper lacks innovation，which is the main reason why I gave this paper a lower score.
2.	More strong supervised baselines should be included for reference. Comparing only a few relatively weak baselines can lead to a misunderstanding of the current gap between supervised and unsupervised methods. 
3.	Additionally, the authors should report some perceptual metrics for better comparison.
4.	The writing and the presentation need improvement.

Limitations:
The authors have discussed the limitations of the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper This paper proposes an ODE-based  method to tackle low-light image enhancement problems. The motivation of the paper is inspired by the observation that the conventional discrete iterative approaches set fixed update steps. It does not only miss the optimal solution and also does not guarantee the convergence.  Hence, the proposed method takes the iterative curve-adjustment approach and formulates them into solving neural ordinary differential equations. This method is used to work with unsupervised learning to estimate the higher order curve parameters to reconstruct image structure details. Comprehensive experiments demonstrate that the proposed method outperforms the baseline methods on LOL and SICE  benchmarking datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths

1. This paper proposes a novel method that integrates the neural networks into an ODE optimization framework. The neural network is playing an adaptive set of updatable parameters. 
2. Comprehensive experiments show that the proposed method outperforms the baseline methods in the task of low-light image enhancement. 
3. The motivation of this paper is strong and solid. It is inspired by the drawbacks of the existing methods and tackle the problems directly in the proposed method.
4. This paper addresses the limitation of the proposed method.

Weaknesses:
Weakness
1. Based on the visual comparison in Figure 4, the proposed method tends to produce over-exposed areas for highlight regions.  
2. The processing speed of the proposed method is one of the limitations.

Limitations:
The limitation is included in the main manuscript. The processing speed (inference speed) of the proposed method is slow compared to dedicated supervised DL methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a Neural ODE method for curve-adjustment-based low light image enhancement methods to achieve better results which are often sub-optimal for fixed discrete step methods. Specifically, the proposed method reformulates the curve-adjustment-based from the discrete version into the ODE problem by introducing a continuous state. An ODE solver is adopted for the optimization to find the optimal step for the enhancement. Additionally, a simple denosier and a curve parameter estimation module are proposed for noise removal and parameter estimation, respectively. Extensive experiments are conducted to show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Turn the discrete curve-adjustment method into a NODE problem, benefited from the optimization to search for the optimal step.
2. User control support during inference is good for the application of the proposed method.
3. The proposed method seems to have good performance over other competitors.

Weaknesses:
1. The proposed method faces color casts, which is obvious in almost all qualitative results, even with a color constraint in loss functions.
2. The proposed method proposes to denoiser and curve parameter estimator in the NODE framework, however, generalize the method to existing curve-ajustment-based method seems to be a more attractive solution.
3. The denoiser seems to be weak since there is so much noise left for the qualitative results.

Limitations:
Yes, it is discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
5KEb1mqZRl;"REVIEW 
Summary:
The authors proposed a novel compression strategy of transformer based trackers. Unlike previous works, it divides the teacher network into multiple segments, each segment corresponds a single transformer layer of student network, then train each student layer separately. It also introduced some training strategies to enhance performance including (progressive) replacement training, prediction guidance and feature mimicking. Such compression framework is insensitive to the change of architecture of teacher network.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Effectiveness. The experiment results clearly demonstrated significant improvement of inference speed while reserving the majority of tracking accuracy.
2. Flexibility. The proposed compression strategy is insensitive to the change of architecture of tracking models, making it easy to apply on almost any transformer based trackers. The segmentation strategy and the size of student network also supports user customization, which enables the user to design student network according to their unique demands. Such flexibility shows excellent application prospects in end-side scenarios.

Weaknesses:
The detailed strategy of dividing the teacher network is not stated clearly in the paper. Base on the pseudo code provided in page 13, it seems that the segmentation strategy is simply mapping the list of transformer blocks of student network to that of the teacher network base on the lengths of the two lists. This could be too simple.

For example, assume teacher network has 8 transformer blocks in module 1 and 2 blocks in module 2, while student network consists of 2 blocks, then the second student block would have to emulate the last 3 blocks of module 1 and the 2 blocks of module 2, while module 1 and  module 2 might have been trained separately and possess different knowledge. Empirically, this would result in sub-optical performance.

A brief discuss on the divide strategy could help this paper become more informative.

Limitations:
The paper clearly addressed its limitations including inefficient training process and the performance gap between teacher and student network.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to  distill knowledge from larger teacher models into more compact student trackers. Three techniques are proposed: A stage division strategy that segments the transformer layers of the teacher model. Replacement training technique. Prediction guidance and stage-wise feature mimicking. Experiment verifys the effectiveness of the method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The proposed techniques are comprehensive and include a bunch of methods to improve the performance and efficiency of the trackers, 
2.	The experiments are extensive which includes 5 VOT benchmarks. 
3.	The speed is fast when applying the 2 layer tracker variants.

Weaknesses:
1.	The most obvious weakness is that the whole method consists of many distilling techniques, including training strategies, feature mimicking, and loss guidance. It is hard to see the inherent consistency between those techniques. This may harm the generalization ability and transferability of the proposed framework, as the author claims the framework is general. 
2.	The overall method is complex. I am worried about its application to other researchers.
3.	When applied to the Mixformer v2, which has only 2 layers, performance can be improved marginally while speed is unchanged. This may indicate the method's shortcomings. Complex techniques only bring a little improvement.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces CompressTracker, a novel general model compression framework that enhances the efficiency of transformer-based object tracking models. It innovatively segments transformer layers into stages, enabling a more effective emulation of complex teacher models by lightweight student models. The framework incorporates a unique replacement training technique, prediction guidance, and feature mimicking to refine the student model's performance. Extensive experiments demonstrate CompressTracker's effectiveness in significantly speeding up tracking models with minimal loss of accuracy, showcasing its potential for real-time applications on resource-constrained devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1）	Innovative Approach: The paper presents a novel compression framework, CompressTracker, which innovatively addresses the challenge of deploying transformer-based trackers on resource-limited devices by significantly reducing model size and computation cost without substantial loss of accuracy.

2）Structural Flexibility: A key advantage of the proposed framework is its structural agnosticism, allowing it to be compatible with any transformer architecture. This flexibility enables the adaptation of CompressTracker to various student model configurations, catering to diverse deployment environments and computational constraints.

3）Efficiency and Performance: The paper demonstrates through extensive experiments that CompressTracker achieves a remarkable balance between inference speed and tracking accuracy. It notably accelerates the tracking process while maintaining high performance levels, as evidenced by the nearly 96% retention of original accuracy with a 2.17× speedup.

Weaknesses:
1）The concept of ""prediction guidance and stage-wise feature mimicking"" and the idea of BEVDistill [1] seem somewhat similar.

2）Despite the model's efficiency in inference, the training process for CompressTracker is relatively inefficient.

3）While the paper shows promising results on certain benchmarks, there may be concerns about how well these findings generalize across different types of tracking tasks and real-world scenarios.

4）The paper does not compare with other model compression techniques, such as knowledge distillation, model quantization, and pruning.

5）According to the results in Table 3, I observed that the outcomes of CompressTracker-2 are inferior to those of MixFormerV2-S. What could be the reason for this?

6）It is necessary to apply compression to other tracking models in order to further validate the efficacy of the CompressTracker presented in this paper.

7）The authors lack a sufficiently comprehensive review of the related work. The authors should give more reasonable related work by carefully introducing the recent approaches to tracking with compression, such as [2].

[1] BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection, ICLR 2023.

[2] Distilled Siamese Networks for Visual Tracking, TPAMI 2021.

Limitations:
Please refer to weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors proposed a general model compression framework for efficient Transformer object tracking, named CompressTracker. The method adopts a novel stage partitioning strategy to divide the Transformer layers of the teacher model into different stages, enabling the student model to more effectively simulate each corresponding teacher stage. The authors also designed a unique replacement training technique, which involves randomly replacing specific stages in the student model with specific stages in the teacher model. Replacement training enhances the student model's ability to replicate the behavior of the teacher model. To further force the student model to simulate the teacher model, we combine predictive guidance and staged feature imitation to provide additional supervision during the compression process of the teacher model. The authors conducted a series of experiments to verify the effectiveness and generality of CompressTracker.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The author has clear ideas and the article is easy to understand. He proposes a general compression framework for single object tracking. This method can efficiently compress large object tracking models into small models. The author has conducted a large number of experiments to prove the effectiveness of this method.

Weaknesses:
The font size of the pictures in the article is too small. The author can adjust the font size appropriately to facilitate reading. The training time line in Figure 1a is blocked, resulting in incomplete display. The font size of the tables is inconsistent, for example, the font size of Tables 5, 6, 7, and 8 is too large. The abstract is redundant and can be appropriately deleted.

Limitations:
For lightweight tracking models, the training time is too long. The author can try to find new ways to reduce the time spent on training.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces CompressTracker, a general model compression framework for efficient transformer-based object tracking. CompressTracker divides the teacher model into stages corresponding to student model layers and randomly replaces student stages with teacher stages during training. It also aligns the teacher and student models using prediction guidance and feature mimicking. The framework gradually increases the probability of using student stages throughout training. CompressTracker achieves significant speed improvements while maintaining high accuracy. For example, CompressTracker-4 accelerates OSTrack by 2.17x while preserving 96% of its accuracy on LaSOT.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Versatility: Compatible with various transformer architectures for student models.
- Efficiency: Achieves a good balance between inference speed and tracking accuracy.
- Streamlined training: Offers a single-step, end-to-end training process, simplifying the compression pipeline.

Weaknesses:
- Limited theoretical analysis: The paper focuses on empirical results without providing much theoretical justification for the proposed methods.
- Lack of ablation on some components: Some components of the framework are not thoroughly explored. For instance, the impact of different feature mimicking strategies is not extensively analyzed.
- Performance and Efficiency Trade-off: While CompressTracker maintains high accuracy, there's a slight performance drop compared to the original model. Training time for CompressTracker-4 (with only 4 blocks) exceeds that of the original OSTrack. This trade-off between training efficiency, inference speed, and model performance requires further optimization.
- The core idea of reducing the number of Transformer blocks is not new. Similar approaches have been used in other models like TinyViT[1] and MiniViT[2].


[1] Wu K, Zhang J, Peng H, et al. Tinyvit: Fast pretraining distillation for small vision transformers[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 68-85.
[2] Zhang J, Peng H, Wu K, et al. Minivit: Compressing vision transformers with weight multiplexing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 12145-12154.

Limitations:
No

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
5ClpGA0u9K;"REVIEW 
Summary:
The paper proposes a new method, called Energy Rank Alignment (ERA) to finetune large language models (LLMs) for molecular generation in a similar fashion to Reinforcement Learning from Human Feedback (RLHF). The paper first introduces how the alignment task  in LLMs is very similar to creating property-conditioned molecules from SMILES strings, which are token-based generation techniques. In the introduction, the paper distinguishes ERA from common RLHF methods, such as PPO and DPO, by stating that it has a minimization objectives and leverages a reward function. Next, the paper describes related work for using LLMs for molecular generation and RLHF for language models and reiterates the differences of ERA compared to PPO and DPO.

In Section 2, the paper outlines the definition of ERA which mostly center on the derivation of relevant loss functions that the algorithm aims to minimize. In its definition, the ERA loss makes use of the KL divergence to arrive at the final formulation at the end of Section 2 leading up to the on-policy loss formulation for ERA. Section 3 provides a theoretical analysis of the ERA loss and its gradients, as well as its connections to the regularized entropy objective.

Section 4 describes the experiments for molecular generation using ERA, including unprompted and prompted generation. The paper also includes a sub-section on general alignment settings of LLMs related to IMDB movie reviews. The results generally show a distribution shift between models finetuned with ERA and those that were not. The paper subsequently ends with a conclusion and discussion of limitations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The provides proposed an interesting method and finetuning objectives that is useful for conditioned molecular generation and LLM alignment. The strengths include:
* A novel method for designing property conditioned molecules that is also applicable to LLM alignment. [Originality, Significance]
* A detailed derivation of the ERA loss, as well as a theoretical analysis on relevant properties. [Quality, Clarity]
* Experiments that generally support the distribution shift induced by the ERA method.

Weaknesses:
The weaknesses of the paper mostly center on expanding relevant related work and baselines for experiments:
* The authors do not discussion related work to training of transformer models and LLMs using reinforcement learning to arrive at molecules with desired properties. Some examples include [1] [2] 
* The experiments do not include baseline evaluation of DPO and PPO, which would have provided relevant details for how ERA performs compared to established baselines. 
* The paper could be strengthened by providing additional details related to experimental settings (see questions)


[1] Ghugare, Raj, Santiago Miret, Adriana Hugessen, Mariano Phielipp, and Glen Berseth. ""Searching for High-Value Molecules Using Reinforcement Learning and Transformers."" In The Twelfth International Conference on Learning Representations.

[2] Blaschke, Thomas, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. ""REINVENT 2.0: an AI tool for de novo drug design."" Journal of chemical information and modeling 60, no. 12 (2020): 5918-5922.

Limitations:
The authors briefly discuss limitations at the end of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce “Energy Rank Alignment”, a novel alternative to PPO and DPO for policy optimization when an explicit reward model is available. ERA is shown to work for enriching chemical libraries for proxy objectives that are fast and easy to compute, and has clear benefits in the simplicity of tuning the strength of regularization to a reference and entropy of samples with two decoupled parameters. This controllability allows ERA to avoid greedy policies and the sort of mode collapse often observed using DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The ERA approach is interesting and clearly defined. It is well-suited for many preference optimization settings, where an explicit reward model is available and alternative methods do not take advantage of this. The authors show results on multi-objective optimization to illustrate that the approach is not limited to greedy optimization of single objectives.

Weaknesses:
The main weakness of the paper is the evaluation with respect to lead optimization of small molecules. This is a notoriously difficult kind of evaluation to make meaningful with purely in silico experiments. One clear opportunity for the authors to improve their evals, while respecting the constraints imposed by easily-computable reward functions, is to incorporate some kind of online evaluation. Comparing DPO and ERA in an online setting would be informative and more relevant for the chemistry community.

Limitations:
Partially

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study an important problem about searching through chemical space, where the number of possible molecules grows combinatorially with the number of atoms. They focus on aligning large autoregressive models trained on chemical compound databases to generate molecules. The energy rank alignment (ERA) algorithm is proposed to use an explicit reward function to produce a gradient-based objective for optimizing autoregressive policies.  The authors offer theoretical insights into the relationship between energy rank alignment (ERA) and proximal policy optimization (PPO), direct preference optimization (DPO). Their experiments show that ERA is scalable, does not require reinforcement learning, and performs well compared to DPO when preference observations per pairing are limited.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors study a significant problem about generating molecules with desired properties based on autoregressive models by proposing the energy rank alignment (ERA) algorithm. 

2. This paper is well written.

3. The proposed methods work reasonably well.

Weaknesses:
1. Diversity, novelty and uniqueness are all important properties for drug discovery as discussed in previous works. To verify whether the models can be used to improve the process of drug discovery, the paper may benefit from comparing the aligned models with the reference model based on these metrics.

2. Missing the discussion of the related works which also focus on molecule optimization and drug discovery for both traditional and state-of-the-art methods, such as [1] [2] and so on.

3. The authors propose using reinforcement learning for drug optimization, a well-established method frequently employed in prior works, such as [3,4]. Additionally, advantage-based and multi-objective policy optimization are well-known in the reinforcement learning literature. A more comprehensive analysis of the limitations of this approach, along with a comparison to other existing methods, would have been beneficial.

[1] Drugassist: A large language model for molecule optimization.

[2] Automatic chemical design using a data-driven continuous representation of molecules.

[3] Optimization of molecules via deep reinforcement learning. Scientific Reports. 2019. 

[4] Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Nature Machine Intelligence. 2021.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
4kCr61XYQJ;"REVIEW 
Summary:
This work extends Poisson-Gamma Dynamical Systems (PGDSs) by considering non-stationary transition dynamics to effectively capture the evolving dynamics of observed count sequences.

The authors propose a model where the underlying transition matrices evolve over time, based on three (gradually more complex and flexible) Dirichlet Markov chains.

For inference of the model, the authors make use of the Dirichlet-Multinomial-Beta data augmentation to derive a fully-conjugate Gibbs sampler.

Experiments showcase improved data-smoothing and forecasting performance of the proposed method across several real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Extending PGDS models to accommodate time-varying transition dynamics is of interest and significant

- The proposed variations of Dirichlet-Markov chains provide flexibility in capturing different modeling assumptions

- Devising a closed-form Gibbs sampler for posterior inference of this model is significant.
    - The attained expressions seem correct to the best of my knowledge, although I did not carefully double-check the mathematical details of the derivation.

Weaknesses:
- The main limitation of this work is the assumption that the transition kernel is static within each sub-interval: i.e., the authors consider that the kernel can only change at discrete instants, while is constant within each sub-interval.

Limitations:
The authors address the main limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Existing PGDS models struggle with capturing the time-varying transition dynamics seen in real-world data. To address this, the submission proposed a non-stationary PGDS, allowing the transition matrices to evolve over time, modeled by Dirichlet Markov chains. Using Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed for posterior simulation. Experiments demonstrate that the proposed non-stationary PGDS achieves improved predictive performance compared to related models.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed non-stationary Poisson-Gamma Dynamical System offers several notable advantages. 

Firstly, its ability to allow transition matrices to evolve over time addresses the limitation of state-of-the-art PGDS models in capturing time-varying transition dynamics, making it more suitable for real-world count time series. 

Secondly, the use of specifically-designed Dirichlet Markov chains to model the evolving transition matrices enhances the model’s capacity to learn non-stationary dependency structures. 

Thirdly, the application of Dirichlet-Multinomial-Beta data augmentation techniques facilitates the development of a fully-conjugate and efficient Gibbs sampler for posterior simulation.

Weaknesses:
I did not find any obvious weaknesses.

Limitations:
The authors discussed some future work directions in the conclusion.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work extends  Poisson-Gamma Dynamical systems (PGDS) to model non-stationary dynamics by replacing the constant transition matrix $\Pi$ with a time dependent one $\Pi^{(t)}$ and the original Dirichlet prior on the columns with three different Dirichlet Markov chain constructions. The manuscript describes am efficient Gibbs-sampler for inference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The work addresses a relevant problem of modeling non-stationary dynamics in count time series. The provided extension relative to the original PGDS is sufficiently novel. I lack deep enough understanding of some parts related to the sampler, therefore I cannot assess if the construction of the sampler required new ideas or was a mechanical extension of the sampler for PGDS  (this being the main reason for my lower confidence score.). I tend to assume new ideas were necessary.

Weaknesses:
My major problem is the experiment evaluation. In Table 1 in the NIPS dataset we can see results like $14.014 \pm 4.387$ bolded, over values like $14.706 \pm 4.414$, or $17.105 \pm 6.449$. 
In ICEWS values like $0.214 \pm 0.008$ over $0.215 \pm 0.007$ , in USEI $4.596 \pm 0.562$ over $4.703  \pm 0.538$, in COVID $6.969 \pm 1.107$ over $7.566 \pm 1.095$. These are mainly smoothing results. In the light of this I am not confident in the statement “As the experiment results shown in Table 1, the NS-PGDS exhibits improved performance in both data smoothing and forecasting tasks.”.  We do not know how the confidence interval was computed, or how many repeats were made. The lack of statistical rigor in the evaluation stands in striking contrast with the sophisticated Bayesian model presented. 

Besides this, other possible problem with the evaluation is that the manuscript states that default paramerters were used for the benchmark methods “GP-DPFA, PGDS, GMC-RATE, GMC-HIER, BGAR”  while the present method used specific K based on the dataset. It is very hard to tell if this is a fair comparison or not.

Limitations:
No specific limitation section was provided. The part on future work in the Conclusion can be interpreted as pointing out some limitations of the current model, but a specific limitation statement would be preferable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces non-stationary Poisson-Gamma dynamical systems, an extension of Poisson Gamma dynamical systems with a dynamic transition matrix. Decomposing the time steps into equally spaced subintervals, the transition matrices evolve between sub-intervals, remaining static within sub-intervals. The authors introduce three options for transitions to occur. The authors derive a Gibbs sampling scheme for exact posterior inference using data augmentation techniques and showcase the effectiveness of their method through a series of predictive and qualitative results.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This is a well-written, organized paper that is easy to read. The proposed method allows for exact posterior inference through Gibbs sampling. The authors exhibit extensive predictive results across 4 datasets, although their method only exhibits marginal improvement as compared to Poisson Gamma dynamical systems.

Weaknesses:
I'm not convinced that the magnitude of the author's contribution, nor the significance of the paper is strong enough to warrant acceptance, and the methods produce only marginally better results than that of Poisson Gamma dynamical systems. The qualitative results are not groundbreaking.

Limitations:
The authors address the limitations of their work, stating intention to address these limitations (e.g. constant sub-interval lengths) in future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
4dmwvbs4Ea;"REVIEW 
Summary:
This paper studies the offline policy optimization problem, i.e. to find a policy whose value function is close to the optimal value function using offline samples.  Under the assumption of linear MDP, they proposed a gradient ascent algorithm. 

The sample complexity of the algorithm only depends on the feature coverage of the best policy and does not require coverage over any other policies.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well written. The algorithm, theorems and lemmas presented in this paper are all very clear.

The problem studied in this paper is offline policy optimization, which has significant value to the community, both empirically and theoretically. 

The algorithm in this paper is simple and computationally tractable.

In contrast to other offline RL paper, this paper does not require that the offline data is sampled i.i.d. or to be admissible, and they can handle arbitrary offline data as long as the data has sufficient coverage to the best policy.

Weaknesses:
Compared to Zanette (2021), the algorithm idea is somehow similar. Specifically, both algorithms use the actor-critic update, and the optimization in the algorithm of this paper is similar to the pessimism estimation in Zanette (2021).

The assumption made in this paper is the linear MDP assumption, which is stronger than the assumption in Zanette (2021).

Limitations:
Yes. The authors addressed all the limitations listed in the guidelines.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a new algorithm for offline reinforcement learning in linear infinite-horizon discounted MDP, which achieves a strong sample complexity under the weakest data coverage assumption. Moreover, their algorithm is easy to implement and computationally efficient. Their algorithm design is based on a reduced version of linear programming formulation of MDP, which approximately transforms the original problem into a unconstrained saddle-point optimization problem.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The algorithm proposed in this paper has many nice properties: it is computationally efficient, easy to implement, and works under the weakest data coverage assumption.
2. The developed techniques and the observations on transforming the optimization problem is insightful.
3. The paper is also well-written, with clear proof sketch and is well-positioned among related work.

Weaknesses:
The paper is pretty notation heavy and a bit hard to follow. I would suggest including a table of notations with descriptions. The choices of notation can also be optimized. For example, I found the mixed use of $D_{\pi}$ and $D_{\theta}$ confusing, and they looks like they are dependent on the value of $\pi$ and $\theta$.

Limitations:
Nothing necessary stands out.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an approach for solving Offline RL problems in domains where the underlying MDP problem has reward and transition models that are linearly realisable under a known feature map. The paper is well presented.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Some of the tricks employed in the approach are quite interesting. 
2. The analytical results are good.

Weaknesses:
1. This work is only for MDPs where rewards and transitions are linear. Many of the moderately interesting problems are not linearly realisable, so it is quite important that authors provided a detailed discussion on how it can be addressed. 
2. I am not entirely confident about this, so will wait for inputs from authors. The approach seems to be built based on works by Hong and Tiwari, and stabilisation trick [Neu and okolo, Jacobson et al.]. I was not sure on the key significant contributions of this paper on top of those works. 
3. For me the biggest concern is that there are no experimental results. How would such an approach work for an MDP where the transition and reward models are not linear? Also, the approach is still approximate (given the bound), so would have been important to show the real results.

Limitations:
There is no limitations mentioned.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
4FwlejUlg5;"REVIEW 
Summary:
This paper investigates the problem of latent post-treatment bias in causal models where there exists some proxy variables of the latent confounder and post-treatment variables. The authors first derive a general form of latent post-treatment bias which is intractable in most situations (except in special cases such as linear SCM). The authors state that the latent post-treatment bias can be arbitrarily bad for existing proxy-based causal inference methods. They then propose an identifiable VAE-based causal inference algorithm under the assumption that at least one dimension of each sufficient statistic of the latent prior is invertible. The proposed method is evaluated on both synthetic and real-world datasets to demonstrate its causal effect estimation capability with the presence of both latent confounders and post-treatment variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
• Causal reasoning in the context of latent confounder and post-treatment variables is an important topic especially with observational data.

• The authors clearly state the necessary assumptions for the identifiability of true latent variables, and the logic of determining the dimensions of $\boldsymbol{C}$ and $\boldsymbol{M}$ is well presented.

• The paper has a well-established theoretical basis.

Weaknesses:
•	For the illustrative example in the introduction, it might be better to explicitly specify what the post-treatment variable is.

•	Other existing works [1-3] on identifying latent confounder/mediators based on the iVAE architecture should also be included in the related work.

•	The role of post-treatment variables $\boldsymbol{M}$ seems to be a bit ambiguous. To be specific, is Theorem 4.1 valid for all types of relationships between $\boldsymbol{M}$ and $Y$?

•	The illustration of (iv) in Assumption 3 is a little confusing, as it assumes one extra degree of freedom on the prior parameters of $\boldsymbol{Z}$ and is critical to the identifiability of $\boldsymbol{Z}$ from $\boldsymbol{X}$. More explanation on this point will be appreciated.

•	The empirical evaluation consists of only one real-world dataset, which somehow limits the applicability of the proposed method.

References:

[1]. Zhou, D., & Wei, X. X. (2020). Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE. Advances in Neural Information Processing Systems, 33, 7234-7247.

[2]. Sorrenson, P., Rother, C., & Köthe, U. (2020). Disentanglement by nonlinear ica with general incompressible-flow networks (gin). arXiv preprint arXiv:2001.04872.

[3]. Jiang, Z., Liu, Y., Klein, M. H., Aloui, A., Ren, Y., Li, K., ... & Carlson, D. (2023). Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators. arXiv preprint arXiv:2306.07918.

Limitations:
The authors do not include a paragraph discussing the limitations and potential societal impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors deal with latent post-treatment bias for proxy-based methods which are employed for causal effect estimation.
They show that post-treatment variables can be latent and mixed into the observed covariates along with the latent confounders.
The authors transform the confounder-identifiability problem into a tractable pair-wise conditional independence test problem.
They prove that the latent confounders and latent post-treatment variables can be identified up to bijective transformations. Finally, they provide experimental analysis for their approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper deals with a very interesting problem.	 The proposed method appeared to be theoretically robust. The method is evaluated with proper experimental analysis on synthetic and real-world datasets and compared with multiple benchmarks.

Weaknesses:
Here I provide some weaknesses of the paper:
* Bi-directed edges in Figure 1 are not defined properly.
* Do-operator in equation 3 is not defined in detail.
* Assumptions in Assumption 2 should be described in more detail.
* The proposed method seems to depend on a lot of assumptions. Assumptions 1,2,3 each contain multiple assumptions. The authors should explain how their assumptions hold for the real-world scenarios they considered in their experiment section.

Limitations:
The authors discussed a very few limitations of their paper but more discussion should be done.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of causal inference with observational data, particularly when direct measurement of confounders is infeasible. The authors propose a new method, Confounder-identifiable Variational Autoencoder (CiVAE), to mitigate post-treatment bias using observed proxies for both latent confounders and latent post-treatment variables. The paper provides a theoretical analysis under specific assumptions and validates the proposed approach through experiments on both simulated and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper investigates a critical question concerning the mitigation of post-treatment bias, which is essential in various practical scenarios.
* The ideas presented in the paper are clear and easy to follow, and the theoretical analysis is well-established.

Weaknesses:
* In practical scenarios, interactions among latent factors are often present and can significantly impact the estimation. It would be beneficial if the authors could elaborate on how their method addresses these interactions and whether there are any theoretical guarantees regarding their handling in the proposed approach.

* The theoretical guarantees rely on strong assumptions, and the assumptions are hard to verify in practice. In assumption 1, the paper assumes an injective function of latent confounders and latent post-treatment variables into the observed proxy. This is a strong assumption,  and it will be much harder to meet the assumption in general when the function is nonlinear. The specific setup with strong assumptions limits the practical applicability of the proposed approach. It would be helpful if the authors could provide examples where these assumptions hold and demonstrate how they can be verified.

* The experiment lacks sufficient details on setup and implementation. Could the authors provide more specific information to enhance understanding of the empirical results?

Limitations:
* The proposed method relies on very strong assumptions to ensure identifiability, which can be challenging to verify in practical applications.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors investigated the issue of latent post-treatment bias in causal inference from observational data. They showed that estimator of existing proxy-of-confounder-based methd, i.e., DEV (f(X)), is an arbitrarily biased estimator of the Average Treatment Effect (ATE), when the selected proxy of confounders X accidentally mixes in latent post-treatment variables (Theorem 3.2). To address this issue, they proposed the Confounder-identifiable VAE (CiVAE), which identifies latent confounders up to bijective transformations under a mild assumption regarding the prior of latent factors. They showed that controlling for latent confounders inferred by CiVAE can provide an unbiased estimation of the ATE. Experiments on both simulated and real-world datasets demonstrate that CiVAE exhibits superior robustness to latent post-treatment bias compared to state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Being able to recover latent variables (cofounders, post-treatment variables, or others) from observations is challenging and important. Ignoring latent variables or assuming non-existence of latent variables is unrealistic and can lead to the wrong conclusion and decisions. The authors further motivated the importance of recovering latent cofounders, post-treatment variables and the consequence of not doing so  (Theorem 3.2). The solution provided shows originality and quality.

Weaknesses:
The presentation can be improved.

Limitations:
n.a.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
47CdPNiWUB;"REVIEW 
Summary:
This paper proposes a methodology called Rockafellian Relaxation (RR) to mitigate the impact of labeling errors in neural network training. The method is architecture-independent and integrates concepts from adversarial training to address dataset imperfections robustly. Through theoretical justifications and a series of experiments on standard datasets like MNIST and Toxic Comments, the paper demonstrates that RR can significantly improve the performance of neural networks trained under various corruption levels. The paper’s contributions are particularly valuable as they provide a new tool for improving training accuracy in the presence of label noise, enhancing the robustness and applicability of machine learning models in diverse and error-prone real-world settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper's approach to using Rockafellian Relaxation for addressing labeling errors is innovative, especially the combination with adversarial training concepts.

Quality: The method is grounded in solid theoretical justification, and the empirical results show marked improvements over existing methods.

Clarity: The explanations of the methodologies and the algorithms are clear and detailed, making it easier to understand the operational aspects of the proposed solution.

Significance: The significance of this work lies in its potential to improve training robustness across various domains and dataset imperfections, which is highly relevant for deploying machine learning models in error-prone real-world environments.

Weaknesses:
Computational Complexity: The added complexity might limit the practical application of the method in scenarios with constrained computational resources.

Limitations:
Generalization to Different Noise Types: While the method is tested against uniform label noise, its effectiveness against other types of noise is not thoroughly investigated.

Dependence on Hyperparameter Tuning: The effectiveness of RRM is likely sensitive to the choice of hyperparameters, such as the regularization term and the parameters controlling the adversarial component. The paper does not provide extensive guidance on hyperparameter selection, which could affect the reproducibility and ease of application in different scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a loss reweighting scheme to train models in the presence of label errors. When training an NN with empirical risk minimization in this setting, one would want to assign a weight of zero to all datapoints that are mislabeled and a weight of one to all datapoints that are correctly labeled. This paper presents an automated method for accomplishing this weighting, called the Rockafellian Relaxation Method (RRM). It is noted in Theorem 3.1 that the inner minimization objective of RRM reduces to a linear programming problem, despite RRM being non convex in general. After relating RRM to distributionally robust optimization techniques, the adversarial variant of RRM is introduced (A-RRM), which includes adversarial perturbations to induce adversarial training as well as loss reweighting. Experiments on four datasets show that RRM and A-RRM outperform other methods in both adversarial settings and settings with high proportions of noisy labels.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work addresses two different types of robustness: robustness to label noise and robustness to adversarial feature perturbation. It should be of interest to those who are generally interested in robust and trustworthy machine learning. Furthermore, the proposed training method has strong theoretical foundations, and its relation to other optimization formulations is discussed in detail. The theoretical results are validated in experiments that cover different datasets and types of data corruption.

Weaknesses:
The experimental section lacks a relevant baseline for comparison. As it stands, it is unclear how this compares to other noise-reduction techniques. The relationship to other techniques is discussed in the related work section, it would be nice if the purported benefits of this approach were borne out empirically.

The introduction of adversarial training in section 3.5 is under-motivated. Based on the earlier sections, it is unclear how label and adversarial feature corruptions are related to each other, why we would want to achieve robustness to both, and whether previous approaches have attempted this before. I would suggest explicitly motivating this earlier in the paper.

Limitations:
The limitations are briefly discussed in the paper. As noted above, one main limitation is that it only studies $\ell_\infty$ bounded FGSM attacks. Furthermore, this paper only considers the uniform label noise model, and does not consider the case when label corruption might be correlated with features.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents Rockafellian Relaxation (RR), a new method to address labeling errors in machine learning datasets. RR is a loss reweighting technique that enhances neural network robustness against labeling errors and adversarial attacks, working across various data domains and model architectures. The key contribution is an approach that mitigates label corruption and class imbalance without needing clean validation sets, offering a practical solution for training robust models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper introduces Rockafellian Relaxation (RR), a novel loss reweighting methodology that addresses learning with noisy label problems

- The authors provide a solid theoretical basis for RR, relating it to optimistic and robust distributional optimization formulations. RR is also designed to be architecture-independent, making it a versatile tool applicable across different neural network architectures.

- The method does not rely on having clean validation data, which is of advantage in many real-world applications.

Weaknesses:
- While not explicitly mentioned, the iterative nature of the RR algorithm could potentially be computationally intensive, especially for large datasets.

- The method assumes a specific model of label noise (e.g., uniform label noise), which may not hold in all real-world scenarios. 

- The paper could benefit from a more comprehensive comparison with other state-of-the-art methods for handling noisy labels, such as GCE [1], ELR[2], to better position RR in the existing literature.

[R1] Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels

[R2] Early-Learning Regularization Prevents Memorization of Noisy Labels

Limitations:
Authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
476zUsqFZB;"REVIEW 
Summary:
This paper attempts to address the problem of low interpretability of reaction prediction methods by proposing modeling step-wise polar reactions. To model such mechanisms it uses an existing dataset PMechDB. The authors propose an approach to model such reaction by first selecting the right atoms to react from the input molecules using learned models and then react them.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- Several existing chemistry reaction prediction models are benchmarked on the PMechDB dataset.
 - A way to integrate reaction mechanism information is introduced.

Weaknesses:
- The paper has substantial clarity problems: 
  - Table captions are insufficiently informative, requires going deeper into the text to understand what results are actually presented (e.g. 'Table 3: Top-N Accuracy of Trained Models').
  - Figures 5 and 6 are formatted inconsistently with the rest of the file.
- Citation quality is poor:
  - Could provide more references to prior work overall. e.g. section 3.3 describes prior work on sequence to sequence modelling without any references. 
  - PMechDB is introduced in a way that makes it unclear, whether the database is a contribution of this work or not.
- Novelty is not prominent. Method in [18] (OrbChain) is already working with similar task on a similar dataset.
- Evaluation is insufficient:
  - Source code for reproduction has not been provided.
  - The resulting models have not been evaluated on the global datasets, making it unclear whether the fine tuning as specified in this work improves the performance in general rather than on the test set of PMechDB.
  - Error bars are not provided.
  - Not benchmarked against a comparable method, referenced in [18].

Limitations:
The authors address some of the limitations of their work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Current reaction prediction models lack interpretability for chemical reaction prediction. This paper evaluates the various machine learning models on the PMechDB dataset which contains polar elementary steps. Besides, this paper proposes a new system: PMechRP, which achieves the highest top-5 accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths:  
1. A new benchmark has been introduced, which improves the interpretability and causality of a chemical reaction.

2. Several methods are evaluated.

Weaknesses:
Weaknesses:

1. This paper seems like a technique report.

2. The main conference track is not suitable for this paper. I think the dataset & benchmark track is more suitable.

3. Writing is poor.

Limitations:
N/A

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Previous reaction prediction models formulate the forward chemical reactions in an end-to-end manner, which only considers the input state and output state while ignoring intermediate states describing the electron redistribution changes. This work tries different models on a new benchmark dataset PMechDB. Experimental results demonstrate the effectiveness of the transition state information in the new benchmark dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The motivation is quite clear. This reviewer agrees with the importance of the exploration of intermediate electron transfer. This is particularly important for the chemical reaction simulation, benefitting the understanding of reaction mechanisms.

Weaknesses:
(1) The technical contribution of this work is very limited. This reviewer does not see enough improvements from the algorithm side. Also, it seems the dataset is not proposed by this work. The contribution of this work is overall limited.

(2) If this work intends to propose a new benchmark, then much more comprehensive reaction models should be covered. Currently, two important reaction models are not discussed: ""non-autoregressive electron redistribution modeling for reaction predictions"" and ""A Generative Model For Electron Paths."" In addition, the evaluation metric and the new task are not clearly described. More detailed descriptions should be provided for clarity.

(3) The presentation of this work is not very clear. This reviewer does not fully understand how the multi-step information helps the reaction modeling. A good example illustrating the significance of the intermediate step information is required. At this stage, this reviewer thinks the multi-step transition information can be easily captured by recursive modeling of single-step reaction models. Currently, this reviewer does not see what new challenges are brought by the intermediate step.

(4) This work is very similar to the published paper ""AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning."" This reviewer does not see many differences between the submitted work and this prior work.

Limitations:
Constructing the benchmark dataset with ground-truth multi-step electron transition states is very hard. This may hinder the further development of this direction.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
the paper describes a new approach to predict polar reaction mechanisms, which is the most important class of chemical reaction mechanisms. this can be quite useful for chemical reaction prediction.


this reviewers rating is based on the current presentation of the manuscript, if the authors are willing to enhance the clarity of the manuscript, this reviewer is willing to increase their score.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Addressing an underexplored but important problem
- decent results
- interesting results with pre-trained methods, that in some cases are surprising (T5 seem to work not as well despite multi-task pretraining)

Weaknesses:
- Model and data processing descriptions are quite short and should be expanded, and presented coherently in one location in the manuscript. From the description in the manuscript I would likely not be able to re-implement the method
- It is not immediately clear which ensemble is shown in table 4
- maybe not so much innovation from the ML side?

Limitations:
ok

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
3zYmlmkIuK;"REVIEW 
Summary:
In this paper, the authors study multi-agent reinforcement learning where agents cooperate through asynchronous communications with a central server to learn a shared environment. They consider the following two settings: multi-agent contextual bandits with general function approximation, and multi-agent RL with general function approximation. For both settings, they propose provably efficient algorithms with low regret and low communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem of asynchronous MARL with general function approximation is interesting and important.

2. This paper is the first to consider the setting with general function approximation. The results are solid and the proof looks good to me.

3. For both settings, the authors propose provably efficient algorithms. The results generalize previous results under the linear setting.

Weaknesses:
1. It seems that part of the techniques is from previous results, such as the bonus function oracle. It will be helpful if there is a section discussing technical novelty.

2. It seems that the setting is closely related to low switching RL and RL with delayed feedback. It will be interesting if the authors could briefly discuss about the connections.

3. For the communication complexity bound in theorem 5.1, should it be $/\alpha$ instead of $\alpha$? In addition, why not choose $\alpha=1/M$ in both theorems? In this way, the communication cost can be improved. (Please correct me if I misunderstood anything) 

4. Line ?? in line 214 of page 6. Please correct it.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose two algorithms for asynchronous communication in multi-agent reinforcement learning with generalized value function approximation: Asynchronous-NLin-UCB for context bandit scenarios and Asynchronous-NLSVI-UCB for episodic MDP scenarios. These algorithms achieve near-optimal regret with low communication complexity. The authors theoretically show the trade-off between regret and communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors provide a detailed background on the related literature concerning regret, communication complexity, and the presence of asynchronous update, which is greatly helpful in understanding the contributions of the proposed algorithms. 
- The theoretical foundations and proofs regarding the communication criterion are important and interesting. Also, the trade-off between regret and communication complexity via parameter $\alpha$ offers valuable insights. 
- The approach of receiving decisions and bonus functions from a central server instead of historical data is intuitive and appears crucial from a privacy perspective.

Weaknesses:
While I have studied general value function approximation, I do not have research background in this field for multi-agent scenarios. Therefore, my critique may not have captured the weaknesses of this paper. 
I’m open to revising my score based on the authors' responses.

As far as I know, MARL often adopts the Centralized Training Decentralized Execution(CTDE) framework to avoid the action space growing exponentially with the number of users. However, it is unclear whether the proposed scenario follows ""decentralized execution"". Agents are supposed to execute based on partial observations in a decentralized manner, but the proposed approach appears to involve a central server consistently during execution. If the proposed scenario is inconsistent with CTDE, I would be interested to hear from the authors what the distinct advantages or necessity of this scenario is.

Typos:

- Line 214: Reference to the label is not correctly written.
- Theorems 4.3 and 5.1: $\tilde{\beta}$ is not properly defined in the statement, and $\beta_t$ should be fixed to $\tilde{\beta}$.
- Theorem 5.1: Total communication complexity should be fixed to $O((1+M\alpha)^2 / \alpha)$.

Limitations:
The requirement for global state instead of partial observation may limit the practical applicability of the proposed methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the asynchronous multi-agent bandit and RL problem with general function approximation (measured by Elude dimension). The main contribution is to establish $\tilde{O}(\sqrt{\text{dim} T})$ regret bound with $\tilde{O}(M^2 \text{dim})$ communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well written and the contribution is solid.

Weaknesses:
I think the major concern is non-optimal complexity bounds. Although it seems unreasonable to ask for a matching upper\&lower regret bound for the contextual bandit problem, the part about RL could be possibly improved (at least, the dependence on $H$ is not tight). Also I am curious that what is the current best lower bound for the communication cost to reach an $\sqrt{T}$ regret bound. It would be an interesting problem to study the exact trade-off between the communication cost and regret.

A minor concern might be about the technical novelty given previous methods on measuring the uncertainty.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studied the distributed federated contextual bandit and federated reinforcement learning (FRL) in the presence of a trusted server. In both problems, nonlinearity and asynchronous communications are explored. Similar algorithms for contextual bandit and FRL that encourage exploration via bonus functions are proposed. Finite-time convergence results in terms of regrets are established for both algorithms and communication complexities are also characterized.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This paper studied the asynchronous federated learning problem where only one agent is activated to sample data and infrequently communicate with server.
* The trigger-based communication is an interesting approach in multi-agent or multi-learner problems.

Weaknesses:
* The clarity of some of the important quantities are not well defined or explained. For example, 
1)	In the sample complexity result of Theorem 4.3, $\tilde{\beta}_1$ is used. However, it was not defined. It’s unclear what this notation is referring to. Similarly, in Theorem 5.1, $\tilde{\beta}_2$ is used.
2)	The oracle for to compute bonus term bk+1,h is crucial in understanding the algorithms. However, it was not very well-explained or shown anywhere in the main paper.	
3)	The two sentences from Line 300 to Line 302 are confusing. Please clarify them.
* Typos:
1)	An extra closing parenthesis appeared in Line 141.
2)	Line 214, ?? -> 12
3)	In Line 1 of algorithm 3, $k=[K]$ -> $k\in [K]$.
4)	In Line 154, the trajectory should be $(s_h, a_h, \cdots, s_H, a_H)$.

Limitations:
Please see weaknesses and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
3sWghzJvGd;"REVIEW 
Summary:
This paper investigates the generalization capabilities of world models in RL, particularly with respect to latent representation errors, which arise when observations are encoded into a low-dimensional latent space. The authors provide a bound on latent representation error when using CNN encoder-decoder architectures. The world model is framed as a stochastic differential equation to characterize the impact of latent representation errors on generalization in terms of either zero or non-zero drift. The authors provide theoretical analysis which shows that these errors can result in implicit regularization in the zero drift case, and propose a Jacobian regularization scheme to tackle the unwanted bias term in the non-zero drift case. Finally, when performing model rollouts for learning a policy, the authors study the effect of these errors on the value function. Experiments on Mujoco tasks demonstrate that the proposed Jacobian regularization enhances robustness to noisy states, reduces the detrimental impact of latent representation errors, and improves convergence speed for longer horizon tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- World models are a popular area of research in the RL community, but there is a lack of theoretical understanding. This paper takes one step towards theoretically analyzing the generalization capabilities of world models.
- The analysis of the effect of latent representation error is a novel theoretical contribution, to the best of my knowledge.
- The results in the paper seem mathematically sound and provide useful insights. The empirical results demonstrate that the Jacobian regularization, which naturally arises from the theoretical analysis, is helpful in improving robustness.
- As a very theory-heavy paper, the authors structured the writing such that it makes it easy to follow each individual result (though there is some room for improvement here, see weaknesses).

Weaknesses:
While the paper studies a previously unexplored problem, there are some questions about the significance of these findings and the use of drift and diffusion terms to represent the error. Other areas for improvement include explaining the insights from the theoretical analysis more clearly, describing the experimental settings in more detail, and supporting certain claims with more evidence.

- Studying the effect of latent representation error is certainly useful, however, with recent advances in representation learning approaches, one can learn reasonably good representations such that the reconstruction error is negligible. When it comes to model-based RL, a much bigger issue is the compounding model error, which is a result of error in the latent/state dynamics model predictions. A comment from the authors on this aspect would be helpful.
- The decomposition of latent error into drift and diffusion terms seems a bit contrived. It is not clear how the error can be expressed in this form, and what defines the scenarios of zero versus non-zero drift.
- The interpretation that propagation of latent error leads to the model exploring novel states seems somewhat questionable. My understanding is that the erroneous states improve robustness similar to noise injection, but will most likely not be valid states belonging to the state space of the MDP. Some reasonable evidence is required to support this statement.
- The paper presents several results and including some intuitive or low-level explanation for each of those results would greatly improve readability. Additionally, due to the large amount of mathematical notation used throughout the paper, it would be helpful to include a notation table in the appendix for easy reference.
- The experimental setting is not sufficiently clear, especially in the introduction when the authors refer to Table 1. With regards to the perturbations - are they applied to every state in the trajectory? For masking, is the same mask used for every state, or is the mask also sampled randomly? With regards to injecting encoder error - how to interpret the $\mu_t$ and $\sigma_t$ values?

Limitations:
There is little discussion on the limitations of the analysis. Some points worth discussing could be the impact of various assumptions when deriving the results, the fact that the analysis is mostly focused on a specific setting - learning from pixels using a CNN encoder and an RNN latent dynamics model, and further investigation of the compounding model error problem.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the generalization capability of world models via a stochastic differential equation formulation. They try to understand latent representation errors on generalization, with both zero-drift representation errors and non-zero-drift representation errors. They found that zero drift latent representation errors are implicit regularization and thus bring generalization gain. Jacobian regularization is proposed to enhance training stability and generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ A deep understanding of the generalization of world models via stochastic differential equation formulation;
+ A careful study of the different effects of zero drift and non-zero drift on gn

Weaknesses:
+ The unseen images are produced via global/partial Gaussian noises and rotation, which seems more on the robustness side rather than the generalization of unseen images;

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the generalization capability of world models in reinforcement learning. In particular, they investigate the latent representation error in world models. They show that zero-drift representation error is inherently a regularizer for the learned model functions. On the other hand, they show that the non-zero-drift representation error accumulates errors and Jacobian regularization can be used to alleviate the issue. They demonstrate their proposed approach improves stability, convergence, and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work investigates an interesting aspect, the generalization of world models that learn the dynamics of the environment. Very limited work has been done in this facet of RL, thus it will share significant insights with the DRL research community.

2. The paper followed a structured methodology to analyze the world model and its representation errors. They interpret the learned model function as stochastic differential equations (SDEs) and model the variation as Brownian motions. 

3. I liked the way they theoretically analyzed it case-by-case and established connections with prior findings. 

4. The paper articulately presents the findings of zero-drift error as a regularizer and the Jacobian correction term for non-zero-drift representation error.  It systematically proves its hypotheses and shows evidence against the claims. They presented corresponding formulas and interpretations.

Weaknesses:
1. The paper is very thorough in terms of theoretical derivation. However, in my opinion, the experimental section of the paper is somewhat lacking. It utilizes only two tasks from Mujoco to prove the efficacy of the approach. More diverse tasks from other benchmarks and robust perturbations will certainly improve the paper. 

2. The experimental evaluation is limited to reward comparison. However, it would be interesting to see some visualization of how the trajectories unfold in the case of both types of errors and with Jacobian regularization.

Limitations:
While the paper discusses the potential social impact of the work, it doesn’t discuss any limitations. I believe the characterization of the models as SDE and the use of Brownian motion as variation have certain contributions to the identified claims. Other interpretations may alter the findings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
3mzFmBPFIX;"REVIEW 
Summary:
This work presents a method for learning metriplectic systems from data. Metriplectic systems are a model which conserve energy and produce entropy, two desirable features. Their method, termed “neural metriplectic systems” (NMS), is based on a more efficient system parametrization. The authors also prove universal approximation results on non-degenerate systems, and generalization error bounds. They verify that their method outperforms other metriplectic-learning baselines, GNODE and GFINN, on two physical experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality: Although I am not at all an expert in this field and therefore cannot properly judge, it seems that the main theorem (Theorem 3.4) is novel and non-trivial.

Quality: The proposed method outperforms baselines in two experimental settings, verifying the expected gain from having a more efficient parametrization. The corresponding theoretical results, on universality and generalization, provide a fairly thorough picture of the method. 

Significance: Within the field of learning metriplectic systems, this paper seems to make a valuable contribution and improve on prior work.

Clarity: The paper is very well-written, and mathematically rigorous. Although the details are not accessible to someone without a background that matches the subject material rather closely, the high-level ideas about the benefits of metriplectic systems, what past work has done, and the advantages of their new method, are conveyed well.

Weaknesses:
Clarity:
1. Although well-written, the paper is not accessible to most machine learning audiences, and seemingly requires the reader to already have a physics background in phenomenological modeling, or exterior algebra.

2. Mathematical terms such as algebraic brackets, Poisson brackets, degenerate metric brackets, etc. should be defined in the beginning of the paper, or with a reference to a textbook or other paper defining them. The “exterior algebra” background is suitable for only those with a strong mathematical background already, using terms like “wedge product” and “graded algebra” without definition. (Admittedly, it would be impossible to fully explain all of these concepts in only 9 pages — perhaps a citation to a textbook would be helpful here, but in practice if the reader needs to understand the decomposition result properly to grasp the contribution, then this work may be more suitable for a venue other than a machine learning conference.)

Quality: The baselines in experiments, as well as the methods discussed in the exposition, are all metriplectic. However, it seems like other methods (e.g. which preserve energy but do not increase entropy, or even those which are not physics-informed at all), should be included too.

Significance: I am not sure how widely applicable metriplectic learning systems are, or what alternative (non-metriplectic) methods can be used for the same problems. The paper would be improved by providing more of this background/motivation.

Overall, as a non-expert, my main concern is with the suitability of this work for a machine learning conference - I defer to the AC on this point. It seems that the machine learning techniques used within NMS are fairly straightforward, while it is the parametrization in Theorem 3.4 that seemingly constitutes the crux of the method. However, the statement and proof of Theorem 3.4 would be more accessible to a physics or math audience, than an ML audience.

Limitations:
Limitations are discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new parameterization for neural metriplectic systems which explicitly incorporates structural information about the degeneracy conditions $\{ S, \cdot \} = 0, [E,\cdot ] = 0$ into the model. The model requires $\sim O(n^2)$ learnable parameters for a problem with $n$ state variables instead of some prior methods which need $O(n^3)$. Further, it also encodes this degeneracy condition in a hard constraint, leading to models which will by construction respect these desired physical conditions. The authors provide a deep learning implementation scheme for their method which involves learning $E(x), S(x)$ and using $\nabla E, \nabla S$ to construct the matrices $L, M$ needed for the bracket from observed trajectories of the physical system. The gradients $\nabla E, \nabla S$ needed for the brackets are computed with autodifferentiation. The authors show that this system is trained end-to-end on simple physical systems including a two-gas system and a thermo-elastic pendulum and can outperform existing methods on these benchmarks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper motivates the need for metriplectic systems which can be efficiently implemented and that incorporate the dynamical constraints required for these systems (energy conservation and entropy production). This captures a potentially interesting class of physical systems that could be modeled by machine learning methods like those employed in the present work. The method that they present as Algorithm 1 is straightforward and improves upon the cubic time complexity of GNODE or GFINN. The authors also provide an approximation result for their algorithm and support their claims with some experiments.

Weaknesses:
While the authors improve the scaling from cubic to quadratic in the number of state variables, the total complexity (quadratic) still scales poorly with size of the problem (number of state variables / dimensions). Further, the current experiments and comparisons were performed on small benchmarks. However, since this paper is the first to point out that the cubic scaling can be improved by reflecting constraints due to degeneracy, I think the experimental component of the contribution is not the most important.

Limitations:
The authors do mention the primary limitations of this present work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a parameter efficient parameterization for neural networks simulating metripletic systems, which are differential equations that have both an energy dissipative piece and an energy conservative piece. The method works by learning several of the required quantities (L and M, which trade off dissipation and conservation, I believe), while also using a small neural network to estimate the dissipation and conservation pieces (E(x) and S(x) ). As not all quantities in the state, x, can be observed, they use a time based diffusion model to emulate the hidden states (e.g. entropy) to develop initial conditions for these. Experiments are performed on two systems of this class, where it seems like the method performs better (probably due to having better inductive biases).


Unfortunately, due to not having a strong physics background, I feel somewhat unqualified to judge many of the technical strengths although things seem reasonable from a skim. I don’t know if I can properly assess novelty and significance as a result.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Significance:
-	Building better emulators of physical systems that are complicated is a good first step in what the authors term “phenomenonological” understanding of these systems.

-	Even quicker training time (and demonstrated both practically and theoretically) is quite helpful. I remember one of the original issues with NODE was that it took a very long time to converge.

Clarity: 
-	Overall, the paper is pretty well written, even if quite dense, and okay to follow for a non expert physicist. I was able to follow at least the ML pieces and the experiments section quite well.

-	The relevant literature is reasonably well signposted; I learned a fair bit about the state of this field by checking the references.

Novelty:

-	The approach seems to have a clear inductive bias win over the prior works GFINN and GNODE due to better parameterization of the system.

Weaknesses:
Unfortunately, the writing ends up being quite dense and technical with minimal outside applications. 

Sure, emulating these physical systems in the experiments is quite nice, but what types of applications does this lead to? This is more of a writing based thing and the paper could be refactored around one of these applications if possible.

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
3gZBGBglBf;"REVIEW 
Summary:
The paper highlights how temporal autocorrelations in EEG data can lead to misleadingly high decoding accuracy in brain-computer interface (BCI) tasks. Using a novel approach with a ""watermelon EEG dataset,"" the authors demonstrate that many reported high performances may exploit these autocorrelations rather than genuine neural activity. They propose a unified framework to address this issue across various EEG tasks and recommend improved experimental designs and data splitting strategies to ensure more accurate and reliable results in BCI research.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Novel Problem Formulation: The paper introduces a novel problem formulation by addressing the potential overestimation of decoding performance in EEG-based brain-computer interfaces (BCIs) due to temporal autocorrelations. This is an innovative perspective that has not been extensively explored in prior research.

2. Creative Use of Non-Human Subjects: The use of watermelons as a model to eliminate stimulus-driven neural responses is highly original. This approach allows for the isolation of temporal autocorrelation effects in EEG signals, providing a unique method to investigate the problem.

3. Impact on BCI Research: The findings have significant implications for BCI research, highlighting a critical issue that could affect the validity of many existing studies. By identifying and addressing this pitfall, the paper provides good insight for more accurate and reliable BCI systems.

Weaknesses:
Plz go and check questions.

Limitations:
While the authors recommend avoiding certain data splitting strategies, the practical implications and feasibility of implementing alternative strategies in real-world BCI applications are not fully explored.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper investigates the potential overestimation of decoding accuracy in brain-computer interface (BCI) tasks that utilize EEG signals. The authors address concerns that high reported decoding accuracies may be attributed to the inherent temporal autocorrelation present in EEG signals rather than the actual decoding of neural responses to stimuli. It contributes to the field of BCI by identifying a potential source of bias in decoding performance, providing a novel dataset to study this issue, and emphasizing the need for careful experimental design to ensure the robustness and reliability of BCI systems.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This article explores the issue of Overestimated Decoding Performance Arising from Temporal Autocorrelations and verifies it through experiments, with both the expressed viewpoint and experimental process offering high enlightenment value to the BCI community.

2. The self-collected Watermelon EEG is interesting. The use of Watermelon EEG dataset to simulate EEG data without neural activity is a good method to isolate the effects of temporal autocorrelations. 

3. The paper provides empirical evidence through experiments that show high decoding accuracies can be achieved even with non-neural datasets, suggesting that reported accuracies in BCI might be influenced by factors other than the models' ability to interpret neural information. The experiment is solid.

Weaknesses:
1. This article only covers image decoding, emotion recognition, and ASAD tasks, and to further substantiate the viewpoint presented in this paper, the use of more other tasks or datasets is recommended.

2. The presentation still needs improvement, such as Figures 1 and 2. Some technical terms may be ambiguous, such as “domain”, and should be given more rigorous and clear definitions.

3. The paper only uses a simple CNN (or some parts of this CNN) for EEG classification. A broader range of model testing (e.g. EEGNet and EEG Conformer) would contribute to enhancing the reliability of the research presented in this paper.

Limitations:
The authors have addressed some limitations.But there are still some questions. Please see the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors have correctly identified a significant issue of numerous hyperbolic or irreproducible results in EEG decoding or classification tasks. However, their evaluation approach of recording signals from electrodes placed on a watermelon needs correction. The authors are advised to consult the definition of EEG, as a watermelon is not a brain and does not generate any electrical signals. Therefore, the recorded electrical noises, even when amplified using equipment typically used for EEG, do not constitute EEG data. In summary, while the authors' intentions were good, the numerous errors in their approach make it unacceptable for publication at a top conference such as NeurIPS.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
An excellent intention to discuss problems with many overblown EEG decoding publications. Yet the conclusions are obvious and many reputable researchers defend their approaches with leave-one-out-subject evaluations to avoid the obvious issues in training and testing data splitting identified by the authors.

Weaknesses:
There were unacceptable errors in using EEG terms since instead some environmental or amplifier Brownian noises were recorded after placing electrodes on a watermelon, which probably acted as an electromagnetic antenna capturing all possible low-frequency noises in a room. The CNN application with data splitting issues is too basic for NeurIPS.

Limitations:
Watermelon cannot produce EEG, even if an EEG amplifier records some electrical noise.
The presented study thus hardly relates to EEG decoding problems but seems to report on obvious issues in machine learning due to erroneous data splitting into training and testing sets, thus making it too trivial for NeurIPS.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Authors hypothesise that the high temporal correlation of EEG data contributes to the high BCI decoding accuracies reported in some prior BCI studies. Specifically, the highly questionable data partitioning practice of splitting continuous EEG data with the same label (or subject) across train/test sets. They present a framework to assess the impact of temporal correlation of EEG features on three different BCI decoding tasks applied to independent datasets, human and watermelon (phantom) EEG data. The inclusion of watermelon dataset is to separate the influence of stimulus-driven responses from highly correlated temporal EEG features that is not fully eliminated when using human EEG data. Results based on the standard data partitioning show high BCI decoding performance for the various tasks even when using watermelon EEG data, and performance is significantly reduced to around chance level when the impact of temporal autocorrelation is mitigated with alternative data partitioning schemes.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality**

-	The inclusion of “phantom EEG” recorded from watermelon to disambiguate stimulus-driven neural responses and temporal autocorrelation during the data analysis, which is not fully eliminated with human EEG recordings. 

**Quality**

-	Authors provide a theoretical basis to justify their hypotheses and experiment design plan. 
-	Analysis plan includes data partitioning used in different BCI decoding tasks (image classification, emotion recognition, auditory spatial attention) applied to a different BCI task (speech evoked response).  

**Clarity**

-	The paper is generally well-written. Problem well illustrated in Figure 1.
-	Some areas require clarity (maybe figures?) to better illustrate the different analyses in the framework (for other applications) and results.

**Significance**

-	Highlights the need for more robust experimental design and data partitioning practices in BCI decoding tasks to minimise the impact of inherent temporal correlations of EEG data on performance.
-	The paper demonstrates a limitation of deep learning models (“black box”) in relation to correlation vs. causation.

Weaknesses:
•	Adding the performance of the current framework on the actual datasets (CVPR, DEAP, KUL, if publicly available), as well as additional independent BCI datasets would provide other benchmarks for comparison. 

•	Not sure why there is a need to match number of the subjects in the SparrKULee dataset to that of the WM “subjects”. The objective of the study is to provide a framework for exploring the impact of temporal correlation of EEG features on BCI decoding performance, not directly comparing both datasets. So, it is fine to include data from all subjects in SparrKULee database. 
> To match the number of subjects in the Watermelon EEG Dataset, EEG data from 10 subjects… from the SparrKULee Dataset were used.

Limitations:
Authors acknowledge limitations of their work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
3Rtn1OMTC4;"REVIEW 
Summary:
This paper studies how to extract useful visual features from out-of-domain and action-free human videos to enhance robotic visualmotor control. Specifically, the authors argure that naively extracting spatial features via MAE is insufficient for robotics control, in contrast, jointly captureing spatial control and temporal movement will be more effective. To do so, the authors propose STP, a new self-supervised learning method, that simutaneously performs MAE on current frame to extract spatial information and predict furture frames to extract temporal motion clues. The overall motivation, idea and method are straightforward and reasonable. The authors evaluate STP on diverse benchmarks including 21 tasks spanning from simulation to real world tasks using imitation learning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-motivated, highlighting the importance of pre-training visual features for robotic foundation models.

2. The logic in the paper is clear and easy to follow.


3. The proposed method is straightforward and simple to implement.

Weaknesses:
1. The high costs associated with evaluating real-world tasks using different random seeds make it challenging to report variances. However, assessing the impact of multiple random seeds in simulated tasks could provide more reliable statistical insights. As shown in Table 1, STP's performance improvement over baselines is marginal (STP 63.7 vs. VC-1 61.8, and STP-L/16(Post PT) 78.4 vs. MAE-L/16(Post PT) 76.7). Given the inherent stochastic nature of imitation learning and reinforcement learning, evaluations across multiple episodes and various random seeds are crucial to validate the proposed methods effectively.

2. Some previous methods also consider the temporal movements when extracting the visual features. For instance, the video-language alignment loss in R3M [1] tries to align language with correct visual transitions, which can extract semantic informations about visual movements. Voltron[2] and DecisionNCE [3] also try to extract the semantic features of the temporal movements between two frames. VIP[3] and LIV[4] use RL to extract visual features, which may also capture long-term movements via bootstrapping. Therefore, the authors could strengthen their paper by highlighting these related works, demonstrating awareness of existing methods, situating their contributions and highlighting the differences between STP and these baselines.

[1] R3M: A Universal Visual Representation for Robot Manipulation. CoRL 2023

[2] Language-Driven Representation Learning for Robotics. RSS 2023.

[3] DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning. ICML 2024.

[4] VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. ICLR 2023.

[5] LIV: Language-Image Representations and Rewards for Robotic Control. ICML 2023

Limitations:
The authors have properly discussed the limiations in the Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a new spatio-temporal pretraining algorithm for representation learning for robotics. The authors propose using masked autoencoding for reconstructing the current frame (for spatial reasoning) and a future frame (for temporal reasoning). The authors provide extensive experimentation across simulated and real-world settings and provide ablation studies to justify their design choices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper addresses the important topic of including temporal dynamics in video data for pretraining robot representations.
- The paper does a good job of explaining the method and detailing the various experimental settings.
- The authors provide policy performance using both the pre-trained representations and post-pre-trained representations which helps assess both the quality of representations learned from internet data as well as the advantage of finetuning representations on the task-specific data. Overall, the proposed method has been extensively evaluated over varied settings across a variety of simulated settings.
- The authors provide an insightful ablation study to justify their design choices.

Weaknesses:
- It is unclear where the diverse image data for STP trained with Ego+I in Table 1 is obtained from. Some information about this would be helpful.
- The real-world experiments seem limited with only two real-world tasks where the MAE also performs reasonably well.
- The authors must include comparisons with prior works using MAE for spatiotemporal learning [1].

[1] Feichtenhofer, Christoph, Yanghao Li, and Kaiming He. ""Masked autoencoders as spatiotemporal learners."" Advances in neural information processing systems 35 (2022): 35946-35958.

Limitations:
The limitations have been addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes STP, a visual representation learning method for robotic motor control. Trained on human videos, STP uses masked auto-encoders for spatial-temporal prediction. The spatial decoder predicts the current frame from its representation with 75% of patches masked. The temporal decoder predicts the future frame using the representations of 75%-masked current frame and the 95%-masked future frame. Experiments on various simulation and real-world tasks show the effectiveness of STP compared with baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed method is simple yet effective, utilizing a masked spatial-temporal prediction objective to learn visual representations for robotics.
2. The paper presents extensive experimental results in both simulation and real-world settings, comparing with proper visual representation baselines.

Weaknesses:
1. Many works have considered temporal information for robot visual representation learning. This paper should mention these and highlight the differences. For example, R3M [1] uses temporal contrastive learning, while VIP [2] and V-PTR [3] use temporal difference.
2. Though STP outperforms the baselines in many benchmarks, the performance gap is not significant (Table 1). The slight performance difference may be due to hyperparameter selection and randomness, as the paper did not provide error bars over multiple seeds.

[1] R3m: A universal visual representation for robot manipulation, 2023
[2] Vip: Towards universal visual reward and representation via value-implicit pre-training, 2022
[3] Robotic Offline RL from Internet Videos via Value-Function Pre-Training, 2023

Limitations:
The authors have discussed the limitations. These cannot be addressed within the scope of this paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, we present a self-supervised pre-trained visual representation in robotic motor control, with spatiotemporal prediction with dual decoders, utilizing large-scale video data. The spatial prediction follows a standard MAE pipeline, and the temporal prediction tries to predict the future based on the current frame. The trained encoder is applied to downstream tasks and real-world robot task for better sample efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper adopts actionless human video data for representation learning, which can be easily obtained. The learned representation can be adapted to downstream robotics tasks. 

2. The experiments contain several real-world tasks, which could be more valuable for applying a pre-trained visual encoder to real-world domains that lack data.

Weaknesses:
1. The major concern is the novelty of the previous methods, considering several related papers that leverage human data and visuals pertaining to downstream tasks have been proposed [1-3].

2. The experiment only contains imitation learning experiments in downstream tasks, while the reinforcement learning framework with sub-optimal data is not considered. 

[1] Learning Manipulation by Predicting Interaction. RSS 2024

[2] Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning. https://arxiv.org/html/2402.14407

[3] Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation. https://arxiv.org/abs/2312.13139

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
34dHGTri2w;"REVIEW 
Summary:
This paper presents a novel parallel sampling method named ""Follow Hamiltonian Leader"" (FHL) designed to address sampling challenges by leveraging zeroth-order information, particularly when first-order data is unreliable or unavailable. The method incorporates a leader-guiding mechanism to enhance the efficiency and effectiveness of the sampling process. Experimental results indicate that FHL significantly improves the exploration of target distributions and outperforms traditional sampling techniques, especially in scenarios involving corrupted gradients.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Innovative combination of zeroth and first-order information.
2. The effectiveness of the method is demonstrated in multiple task scenarios.
3. Theoretical analysis and prove are sufficient.

Weaknesses:
1. Is there any quantitative experiments like evaluating FID and IS on cifar10 datasets and I think it's more compelling whether a novel sampling methods combined with generative models can be used on image datasets with more complex distributions.
2. Lack of experiment of OOD in combination with EBMs or score-based models to valid the stability during sampling with proposed method.

Limitations:
1. Limited exploration of integration with other advanced MCMC methods.
2. Lack of quantitative experiments to demonstrate the advantage of proposed sampling method compared with other methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an interesting parallel sampling method that leverages zeroth-order information to address challenges in sampling from probability distributions, particularly when first-order data is unreliable or unavailable. The method incorporates a leader-guiding mechanism, enhancing efficiency and effectiveness by connecting multiple sampling instances through a selected leader. The proposed method, named Follow Hamiltonian Leader (FHL), extends the Hamiltonian Monte Carlo (HMC) framework by concurrently running multiple replicas at different energy levels and combining both zeroth and first-order information from various chains. Experimental results demonstrate that FHL significantly improves the exploration of target distributions and produces higher-quality outcomes compared to traditional sampling techniques, showing resilience against corrupted gradients and excelling in scenarios characterized by instability, metastability, and pseudo-stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed Follow Hamiltonian Leader (FHL) method markedly improves the efficiency and effectiveness of sampling processes, significantly expediting the exploration of target distributions and producing superior quality outcomes compared to traditional sampling techniques.
- FHL demonstrates greater resilience against the detrimental impacts of corrupted gradients by incorporating zeroth-order information. This robustness makes the method particularly valuable in scenarios where first-order information is compromised, ensuring more reliable and accurate sampling.

Weaknesses:
- The proposed FHL method involves intricate modifications to the traditional Hamiltonian Monte Carlo framework, such as the leader-guiding mechanism and elastic leapfrog technique, which may increase the complexity of implementation and require significant computational resources.
- The effectiveness of the FHL method heavily relies on the appropriate selection of the leader particle. If the leader is not accurately chosen, it could lead to suboptimal sampling performance, potentially compromising the overall efficiency and accuracy of the method.
- While the paper presents experimental results to demonstrate the efficacy of the FHL method, there is a lack of in-depth theoretical analysis to rigorously establish the convergence properties and performance guarantees of the proposed approach.
- The method’s scalability to high-dimensional problems or extremely large datasets is not thoroughly addressed. The parallel sampling approach may encounter challenges in maintaining efficiency and effectiveness as the dimensionality and size of the data increase.

Limitations:
The authors have not adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes to incorporate the energy $U$ into the gradient-based sampling techniques. In particular, it proposes to choose the lowest energy particle as the leader and then add an extra elastic tension between the leader and followers in the Hamiltonian Monte Carlo method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea is simple and clear, the toy examples are easy to understand and demonstrate the benefit of the proposed method well. In addition, the authors conduct experiments for each of the three challenging sampling scenarios identified by the authors.

Weaknesses:
It might worth including the overhead of the proposed method, how much slower the algorithm is per iteration compared to HMC for instance.

The tension coefficient $\lambda$ is critical, setting it to 0 recovers the baseline. But I did not find an ablation over the $\lambda$, is it hard to choose? From my understanding, if you set $\lambda$ pretty large it might recover something like gradient descent and the sampling will collapse.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an interesting new approach for improving sampling methods for energy-based generative models and score-matching models. The key idea is to incorporate zeroth-order information (energy values) in addition to the typical first-order gradient information used by most sampling algorithms like Hamiltonian Monte Carlo (HMC).

The authors identify several challenging scenarios where relying solely on gradients can be problematic - cases of instability, metastability, and pseudo-stability. They argue that incorporating energy values can help mitigate issues in these situations and improve sampling efficiency and quality.

Overall, the core idea of leveraging zeroth-order information in addition to gradients is quite novel and the FHL algorithm is an elegant way to implement this for improving sampling efficiency and quality. The paper is well-motivated, the method is clearly explained, and the empirical results are compelling.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Novel idea of incorporating zeroth-order energy information into sampling algorithms like HMC, which typically only use gradients. This can help address issues like instability and metastability.

2. The Follow Hamiltonian Leader (FHL) algorithm is an elegant way to exchange both energy and gradient information across parallel sampling chains in a principled manner.

3. Thorough experimental evaluation across synthetic examples illustrating the identified challenging scenarios of instability, metastability, and pseudo-stability.

4. Promising results showing improved sampling quality over baselines for energy-based generative models on real datasets like CLEVR.

5. Clear motivation and well-explained methodology.

Weaknesses:
1. It would be better to show exploration of the sensitivity to key hyperparameters like the number of parallel sampling chains.

2. Discussion of computational cost/overhead compared to baseline sampling methods are missing in the manuscript.

Limitations:
Based on the provided paper, the authors do not appear to have explicitly discussed the limitations or potential negative societal impacts of their work. The paper is primarily focused on presenting the technical details of the proposed Follow Hamiltonian Leader (FHL) sampling algorithm and its empirical evaluation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
2vhkjOdlc8;"REVIEW 
Summary:
The paper ""Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection"" introduces a minimalist reconstruction-based framework for unsupervised anomaly detection (UAD) in multi-class settings. The framework focuses on four main components: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets show that Dinomaly achieves superior performance compared to state-of-the-art multi-class and even some class-separated UAD methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Using simple components like Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction to achieve superior performance is highly original. This is a significant departure from traditional methods that rely on complex designs and multiple modules. It challenges the conventional views: more complex architectures are necessary for better performance in anomaly detection tasks.
2. The methodology is well-detailed, and the experimental design is robust. The authors conduct extensive experiments on three well-known datasets (MVTec-AD, VisA, and Real-IAD), providing comprehensive performance metrics and comparisons with SOTA methods. The result is convincing, showing that Dinomaly not only outperforms existing MUAD methods but also surpasses some of the best class-separated UAD methods. 
3. The paper is generally clear, well-organized, and relatively reproducible.
4. The significance of this work is substantial, and makes a valuable contribution to anomaly detection, as it addresses a major challenge in UAD—achieving high performance in multi-class settings without resorting to complex, specialized architectures, and is potentially scalable.

Weaknesses:
1. The paper provides a detailed explanation of the proposed framework but lacks important justification and discussion, it is difficult for readers to realize the novelty and improvements brought by Dinomaly. The author may need to compare Dinomaly to specific previous methods, highlighting the differences and improvements. Discuss how the minimalist approach contrasts with more complex architectures and why this improvement is significant.
2. The motivations for choosing Noisy Bottleneck and Loose Reconstruction are not deeply explored. For instance, explain in more detail why Noisy Bottleneck helps prevent identity mapping.
3. The paper claims simplicity but there was no discussion of parameter number, computational complexity, or time complexity in the experiment. 
4. In Loose Constraint, the author claims that 1-group LC mixes low-level and high-level features which is harmful for anomaly localization. How to group the features into the low-semantic-level group and high-semantic-level group in 2-group LC?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the Multi-class Unsupervised Anomaly Detection task and proposes a minimalistic reconstruction-based anomaly detection framework — Dinomaly that consists of only vanilla Transformer blocks. In this framework, four key components (Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction) are introduced to alleviate the performance gap between multi-class and class-separated models. The paper conducts extensive experiments on three major datasets: MVTec-AD, VisA, and Real-IAD. Results show that Dinomaly outperforms current state-of-the-art methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
(1)The paper is well-written and has clear statements which make it easy to understand. 

(2)The design of Dinomaly is straightforward but innovative. The use of foundation transformers, noisy bottleneck, linear attention, and loose reconstruction is well-justified. 

(3)The paper generally outperformed existing SOTA methods and did enough experiments and comparisons.

Weaknesses:
(1)The method relies heavily on transformer architectures, which might limit its applicability to other types of models.

(2)Transformers can be resource-intensive, and the paper does not fully address the computational cost of training and inference.

(3)The method's generalization to other domains or types of anomaly detection is not fully explored.

Limitations:
Limitations are discussed in Supplementary Sec. A.4.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Dinomaly, a simple yet effective anomaly detection framework using pure Transformer architectures. It identifies four key components essential for multi-class anomaly detection: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets show that Dinomaly achieves superior performance, surpassing both state-of-the-art multi-class and class-separated anomaly detection methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors have conducted extensive experiments to validate the effectiveness of their method across multiple anomaly detection tasks.
2. The authors have proposed a simple yet effective framework that approaches and even surpasses the results of state-of-the-art methods in single-class anomaly detection tasks.

Weaknesses:
1. L53-55 'In addition, previous...': The authors should provide relevant evidence rather than subjective assumptions.
2. L77: Placing the Related Works section in the appendix is unconventional.
3. The first component proposed by the authors, Foundation Transformers, was already introduced in the ViTAD paper, which diminishes the overall contribution of the paper. 
4. The input resolution used in the authors' experiments is 448x448, while other comparison methods use 256x256 or 224x224. This is an extremely unfair comparison. Please include results with a 256 resolution in table for a fair comparison.
5. In the proposed Loose Loss, 90% of the feature points were selected. How was this 90% hyperparameter determined? Please provide ablation study results.
6. The authors have employed Linear Attention to reduce computational load while maintaining similar performance. It is recommended that the authors compare the parameter count and FLOPs of their method with those of the baseline methods to demonstrate its efficiency. Additionally, it is suggested to conduct ablation studies to verify the computational efficiency of Linear Attention. 
7. Other methods, such as RD4AD, SimpleNet, and UniAD, perform under the proposed settings. The authors can conduct a more equitable comparison.

Limitations:
It is recommended that the authors evaluate the performance of different methods under fair settings.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces Dinomaly, a minimalistic unsupervised anomaly detection (UAD) method designed to bridge the performance gap between multi-class UAD and class-separated UAD. Utilizing pure Transformer architectures with key components such as Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction, Dinomaly achieves superior performance on MVTec-AD, VisA, and Real-IAD benchmarks, surpassing state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Dinomaly effectively bridges the performance gap between multi-class and class-separated UAD, achieving superior results on popular benchmarks such as MVTec-AD, VisA, and Real-IAD. 
2.  It utilizes a simple, straightforward approach with pure Transformer architectures, avoiding complex modules or specialized tricks.
3. The detailed ablation study demonstrates the effectiveness of each component—Noisy Bottleneck, Linear Attention, Loose Constraint, and Loose Loss—in enhancing anomaly detection.

Weaknesses:
1. The method might be perceived as too application-oriented, lacking broader theoretical contributions.

Limitations:
See the weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Dinomaly simplifies the anomaly detection process by eliminating the need for complex designs, additional modules, or specialized techniques. It relies solely on basic Transformer components such as self attention mechanisms and multi-layer perceptrons (MLPs) to perform anomaly detection for multi class images.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper has a clear motivation and contribution. The paper effectively proposes the viewpoint of ""less is more"" in multi class unsupervised anomaly detection, emphasizing how the simplicity of model architecture can achieve or surpass the performance of more complex systems.

Weaknesses:
I hope to provide a specific explanation of the information provided by the decision-making process for identifying anomalies in the model.

Limitations:
The author candidly acknowledged the limitations of the work and provided the problems that need to be addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
2Bef9YxSJc;"REVIEW 
Summary:
The paper studies an important and open question, how much user behavior knowledge (generally captured by collaborative filtering models) are present in large language models. This has been a topic attracting significant research interest in recent years. The authors propose that simple linear mappings done on top of LM encoder representations are sufficient to capture collaborative filtering signals in recommendations, and propose a new recommendation method, AlphaRec, which takes pretrained language model content embeddings as input, transforms them via MLPs and lightweight graph convolutions, followed by a contrastive loss. The authors conduct experimental analysis for AlphaRec in both standard settings and zero-shot settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
**S1**: the topic studied is important. It is generally believed that language model and collaborative filtering (recommendations) learn different representation spaces, and methods to bring the two spaces closer is of significant interest to the large community working on search, recommendations, ads, and related topics.

**S2**: the particular approach proposed (linear mapping from textual space to collaborative filtering space) is understudied in prior work on LLM and (Generative) CF, despite numerous papers in recent years.

Weaknesses:
**W1**: the writing in this paper, esp. recommendation system paradigm related discussions, misrepresents (or ignores) significant prior work done in the field. e.g., ""AlphaRec follows a new CF paradigm, which we term the language-representation-based paradigm."" and related writings. 

- Content signals and/or embeddings have been used as the dominant recommendation paradigm in the field, even well before the seminal YouTube DNN paper [1] was published (see e.g., Pazzani and Billsus, 2007 [12]). For recent examples of related work, see e.g., [10, 11] from Pinterest and Meta in KDD'22 (but one should be able to easily find similar papers in WWW KDD etc in prior years as well). 
- Replacing ResNet/ViT- or GPT-/BERT- generated embedding with LLaMa- or Mixtral- generated embeddings cannot and should not be viewed as a paradigm shift, especially given the core architecture of AlphaRec is not substationally different from prior work.

**W2**: AlphaRec needs to be compared with stronger baselines. This applies to many major experiments in the paper. Here are some examples of baselines missing, which may significantly change conclusions obtained and discussions etc:

- vs ID-based recommenders (Table 3).
    - Equation (2) and line 171-173 for $N_u$ already captures set of items that a user is related to (""user interaction history"" / ""user engagement history"") to a large extent. Thus, the authors should compare AlphaRec with at least some SotA sequential/generative recommenders, such as SASRec, BERT4Rec, TIGER, HSTU [3, 4, 6, 7]. All of them are missing in the current paper.
    - Given AlphaRec uses the transposed item id representation - the one layer $N_i$ formulation (equation (2)), relevant work in recent years include Dual contrastive network [8] and User-centric ranking [9]. The authors should compare with or at least discuss some work in this category as related work. 
- Zero-shot performance. (Table 4)
    - ""Book Crossing"" is not a commonly used benchmark dataset. The ""Industrial"" dataset (per citation [1] on line 273) seems to be a small-scale ""Yelp"" dataset, and should be renamed to avoid confusions.
    - For ML-1M, the SotA approach one year ago (LLMRank [14]) already achieved 53.73 NDCG@20, significantly higher compared with 32.15  (AlphaRec) in this work. 

**W3**: many other formulations/experiments/writings could be significantly improved. Examples include:

- The proposed task formulation does not reflect how recommendation systems work in practice. e.g., ""Line 97-99. Personalized item recommendation with implicit feedback aims to select items i ∈ I that best match user u’s preferences based on binary interaction data Y = [yui], where yui = 1 (yui = 0) indicates user u ∈ U has (has not) interacted with item i [58]."" -- here ""selecting the item that user will interact with"" is not the same as ""selecting the item with the highest reward"", as the interaction itself can be negative (e.g., disliking a recommendation, abandoning session, etc.). See [1, 2] for references.

- A key contribution of this work should be the linear mapping finding. But Table 1 uses a questionable set of baselines for both LMs and CF baselines, which weakens linear mapping related claims.
    - To claim ""Moreover, with the advances in LMs, the performance of item representation linearly mapped from LMs exhibits a rising trend, gradually surpassing traditional ID-based CF models"" -- I would expect the authors to compare with a single set of models (e.g., LLaMa-2 7B 13B 70B or GPT-3 1.3b 2.7b 6.7b 13b 175b) trained on identical data. As it stands, all models are trained and/or finetuned with different data, so a simpler hypothesis explaining the LM (Linear Mapping) trend is that people are including more and more data into LLM pretraining/finetuning stages, which happen to capture more and more aspects relevant to recommendations. 
    - On the CF side, ""MF"" ""MultiVAE"" and ""LightGCN"" do not represent SotA baselines on Amazon Review datasets (see W2). 

- Table 1. Please highlight the particular K used for Recall and HR metrics (hard to find in the paper, applies to other tables too).   Most work on recommendation models also report HR/NDCG/etc. over at least 2-3 Ks to help readers understand how metrics vary with different approaches.


- Table 4. [1] should not be labeled as an ""Industrial"" dataset. The cited paper (per line 273) is Ni et al. ""Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"" which in turn seems to be an publicly available review dataset provided by Yelp. Please use appropriate language as the current writing leads readers to think that AlphaRec is an industrially deployed system. Please refer to industrial papers (e.g., KDD ADS track papers [2, 9, 11, 12]) for how to describe testings done on publicly available industrial sampled datasets (like Yelp), vs real deployments. 

- Misc: Contrastive loss is widely used and should not be viewed as a contribution of AlphaRec. See [15, 11] etc.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper states that LLM encodes collaborative signals that make it easy to connect language representation space with an effective recommendation space. Thus, it proposes an effective collaborative filtering model AlphaRec that takes as input only the transformed LLM representations of textual descriptions of items and is trained by InfoNCE loss and graph neural networks. The proposed method outperforms traditional ID-based models and other LM-enhanced methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-written and easy to follow. 
2. The paper conducts extensive experiments validating the effectiveness of the methods and proves the validity of the design through ablation study and anlysis.
3. The proposed method exhibits significantly good zero-shot recommendation performance

Weaknesses:
1. One of the most important motivations of the work is that the paper declares large language models encode collaborative signals which indicates the advantage of using representations of large language models for recommendations compared to id embeddings. However, how the preliminary experiments prove this point is insufficiently discussed in the paper. Advanced large language encodes more semantics of the textual descriptions and thus yields better performance. Why this alone doesn't fully explain the performance gain of LLMs should be more explicitly discussed in the main paper.
2. The novelty is limited. Using semantic embeddings of items has been widely used in recommendations. The novelty mostly lies in using the representations of large language models and the implementation details of how to make it effective when combined with traditional recommendation frameworks like non-linear transformations.
3. The paper states that language representation-based methods have low training costs. Still, if taking into account the costs of generating language representations, the computational cost is much higher than ID-based methods.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes AlphaRec, a novel method to incorporate both knowledge from pre-trained language models and collaborative signals. Authors firstly reveal the advantages brought from pre-trained embedding model, and then propose three modules within AlphaRec. An MLP layer to transform pre-trained embedding to item-representation. A graph convolution to aggregate neighbor’s information, and the InfoNCE loss to train introduced parameters within the MLP for each dataset. Overall, the novelty of this paper lies within exploration of NLP encoded embedding on RecSys. The graph convolution and InfoNCE loss are already widely used techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. A good exploration on new direction (language-representation-based) RecSys
2. Experiments are conducted from different angles for analyzing their model.

Weaknesses:
1. Insufficient baselines.
2. Uncleared model name definition.

Limitations:
See Weakness.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes AlphaRec, an LLM-based recommender system that utilizes language representations of item textual data for recommendations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ Investigating ID paradigm and LLM paradigm is important.
+ The method is simple but seems to be effective.

Weaknesses:
- In this paper, what most confuses me is the usage of the terminology ""collaborative filtering"" throughout the paper. In traditional recommender system, collaborative filtering information means the interactions among users and items. The authors find that using LM as feature extractors to get user/item embeddings from meta-data can achieve similar results as if CF is used for recommendation. However, this seems to be fundamentally different than LM has the ""collaborative information"", as for most online service platforms, the interaction data should be confident and open source LMs won't be able to train on that data. Therefore, the main claim in the paper seems questionable.

- It would be beneficial if we could have results on more diverse datasets.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
2BOb4SvDFr;"REVIEW 
Summary:
The paper proposes to use a new distance, Min-Max-Jump, which is the minimum largest distance on any path between two points, to be used in k-means clustering to learn clusters.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The distance can overcomes some demerits of the convex (""spherical"" in the paper) clusters.

Weaknesses:
The distance in the paper is, in fact, related to single linkage clustering that assign give a pair of points a distance at which the pair is joint to one cluster. This need to be analyzed to relate to previous work as well as to compute pairwise distances efficiently. 

Theoretical property of the distance is poor. The paper should review many other density-based distance functions to put this work into the correct context. 

There would be a lot of problems using this distance as many of pairs of nodes would share the same distance. There is no analysis on the  metric property of this distance. 

On evaluation, the methods need to compare to single-linkage clustering (SSL) at the very least as all the advantages of using this distance with k-means are available in SLL in its simplest form.

Presentation-wise, it is hardly up the standard. There are methods/algorithms/concepts that are mentioned as ""a is like b with a difference"" without a formal definition. This mixes up definitions and properties.

Limitations:
The paper uses a new distance without theoretical justifications. It learns nonconvex clusters by using a nonconvex clustering-based distance (without explicitly mentioning it), which is hardly a novelty.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new metric, min-max jump distance. Effectively, say we are given a complete graph with vertex set $\Omega$ and edge weights $d(x,y)$ denoting the distance between $x$ and $y$, where $d$ is a metric. Then $MMJ(x,y|\Omega')$ is the minimum, over all paths between $x$ and $y$, of the maximum weight edge between $x$ and $y$ on the subgraph induced on the vertices $\Omega' \subseteq \Omega$. Explained nicely in the paper, if you started at vertex $x$ and wanted to get to $y$ and $d(\cdot,\cdot)$ denoted the distance required to ``jump'' from one vertex to another, what is the minimum distance you need to be able to jump to somehow traverse from $x$ to $y$?

This is a nice intuitive metric, and has strong connections to the minimum spanning tree. In fact, I suspect there is more literature to draw from minimum spanning tree research that could yield conclusions about MMJ. The MST is also nice in clustering since oddly shaped clusters (non-convex, for instance) can have small MSTs. This is the idea of MMJ: use it as a metric for K-means so that it can identify non-spherical clusters.

The paper proves some notable theory about the properties of MMJ. Mostly, they show how: 1) When adding a new vertex $p$ to a set $\Omega$, $p$'s MMJ within the context of $\Omega+p$ can be computed knowing all pairwise MMJ's within $\Omega$ within the context of $\Omega$. This effectively adds a new point and evaluates it within the complete, updated context. 2) Given the MMJs for this additional point $p$ within the context of $\Omega +p$, expand the MMJ context of all other pairs in $\Omega$ to the context of $\Omega+p$.

This can then be used in a very dynamic programming-like manner to start with just two points, add new points $p$ and find the context of $p$ and all other points, and then update all known existing MMJs to the new context. This is their algorithm 1, requiring $O(n^3)$ time. They also use properties of the MST to bypass unnecessary calculation to yield algorithm 2, which takes only $O(n^2)$ time (to find the MST).

They then evaluate the performance of algorithms using the MMJ measure on irregular shaped clusters to verify that MMJ helps identify these. This makes sense, since they likely have small MSTs, but not small average/sum/max/etc distances within clusters. Notably, they show how MMJ improves K-means.

Soundness:
4: excellent

Presentation:
1: poor

Contribution:
3: good

Strengths:
I think MMJ is a very cool metric with nice properties and intuition, particularly that related to the MST (I wish the authors had spent more time discussing this!). Their findings are nice and relatively simple to understand (in spite of the presentation). They show that it helps K-means expand to more complicated cluster shapes, and overall it is a very nice, NeurIPS-worthy result. However, as I will explain in the weaknesses section, I do not think this paper is in an acceptable state for NeurIPS.

Weaknesses:
There are a few notable downsides. In terms of the result, I'm not entirely convinced of its novelty. How much of this is actually a re-iteration of MST-based methods already understood? Is this really better than other MST-based algorithms on irregular clusters (think single linkage)? I know that there is a lot of literature that explores irregular shaped clusters, but I am not an expert in this area and so I cannot place this work in the context of existing results. I wish the authors would explain that. Though even if these algorithms aren't entirely better than state of the art, the novelty of the nice formulation of MMJ is certainly appreciated.

However, the biggest flaw in the paper is the writing quality. There are places where the paper is nice and concise, but most of the time it just lacks exposition to understand the higher level of things or adequate details to fully understand what is happening. Formal proofs are contained in the paper, but the jumps in some of the proofs are too large. Theorems and proofs are placed back to back with no high level explanation. Algorithms are written and pseudocode with only the briefest justifications, and no thorough explanations. This is not an acceptable paper for NeurIPS, and I think these issues are too extensive to simply ask the authors to revise. Though if other commenters disagree, I am amenable to changing my opinion.

And one of the disappointing things about this is how natural this work is and how much it lends itself to nice intuitive explanations and visual depictions! For instance, you could do some very nice visualizations of Algorithm 1, where you depict a matrix and show which indices have been calculated to what context $\Omega_n$ at each time point. This clarifies the different purpose of the two loops.

I hope to see this paper submitted again later in a more cleaned up state!

Limitations:
None notable

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents the Min-Max-Jump (MMJ) distance concept and two calculation methods, focusing on path optimization in data analysis and clustering. The contributions include introducing MMJ distance, proposing efficient calculation methods, discussing its properties and applications, and offering a user-friendly approach for practical implementation. Overall, the paper introduces a new distance metric for path optimization and data analysis, providing useful tools and insights for various applications in the field.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1: The paper demonstrates strength through its meticulous use of theorems and proofs, enhancing the credibility and robustness of the research findings.

S2: Clear visualization of results in the paper aids in effectively conveying complex information to the readers, improving understanding and interpretation.

S3: Extensive literature citations throughout the paper showcase a strong foundation of existing knowledge and research, adding depth and scholarly rigor to the study's presentation.

Weaknesses:
W1: The paper's writing style deviates from academic norms, indicating a need for improvement in writing proficiency.

W2: The extremely brief Introduction lacks a detailed definition of the problem, its significance, and challenges. Moreover, it lacks citation support for the points presented. While Section 2.1 mentions methods like k-NN, UMAP, HDBSCAN, it fails to provide corresponding references, lacking essential academic backing.

W3: The overall structure of the paper lacks clarity, as it introduces different distance metrics in Section 2.1 but introduces a new distance measurement approach in Section 2.4, leading to disjointed logic.

W4: The presentation of various distance metrics in Section 2.1 appears disorganized and lacks coherence.

W5: The extensive definition provided towards the end of Section 6.3 disrupts the logical flow of the paper, suggesting a need to adjust the paper's structural coherence.

Limitations:
The paper does not discuss limitations. The authors seem to perceive their work as solely testing models with datasets without considering the shortcomings of the algorithms themselves.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Different distance metrics have been introduced in the literature for data analysis. In this paper the authors consider the min-max-jump distance and apply it in the context two applications, namely, k-means clustering and as an internal clustering evaluation index. They also present two algorithms for computing the min-max-jump distance.

Experimental comparisons reveal that min-max-jump based k-means clustering is better than standard k-means clustering. Also, the min-max-jump distance is shown to be a better internal clustering evaluation index.

This referee feels that this work is rather incremental. Also, experiments have been conducted only on very small datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors demonstrate the efficacy of the min-max-jump distance on two different applications.

Experimental results have also been supplied to support their assertions.

Weaknesses:
The work done is incremental with very limited novelty.
Extensive experiments are called for.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
1URMG6B3WW;"REVIEW 
Summary:
This paper introduces the KrwEmd algorithm, a novel approach to hand abstraction in Texas Hold’em-style games. The main contribution is the integration of historical information using K-recall winrate features and earth mover’s distance, addressing the limitations of previous imperfect recall abstraction methods. The algorithm demonstrates significant performance improvements over state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The integration of historical information via K-recall win rate features enhances the accuracy and reliability of hand abstraction.
2. KrwEmd significantly outperforms existing methods, showcasing its practical value and potential for real-world applications.
3. The algorithm is technically sound, with strong experimental validation supporting its claims.

Weaknesses:
Scalability: While the paper demonstrates KrwEmd's effectiveness in Texas Hold’em-style games, its scalability to more complex and diverse game scenarios remains to be explored.

Comparative Analysis: The paper could benefit from a more detailed comparison with existing hand abstraction methods to better highlight KrwEmd's unique advantages and limitations.

Limitations:
The paper primarily focuses on Texas Hold’em-style games. While this is a significant achievement, a discussion on the model's scalability and generalization to more complex and varied game scenarios would be beneficial. Including potential strategies to address these challenges would strengthen the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach to hand abstraction in Texas Hold'em-style poker games, addressing the limitations of current methods that often disregard historical information. The authors make two primary contributions: First, they develop KRWI (K-Recall Winrate Isomorphism), a new abstraction method that incorporates historical information from previous game phases. Second, they present KrwEmd, the first hand abstraction algorithm to effectively combine K-recall win rate features with earth mover's distance for hand classification. Through experiments conducted in the Numeral211 Hold'em environment, the authors demonstrate that KrwEmd significantly outperforms state-of-the-art algorithms such as Ehs and PaEmd in terms of exploitability, while maintaining the same number of abstracted information sets. This work shows that incorporating historical information can substantially enhance the performance of hand abstraction algorithms, potentially leading to more advanced strategic computation in large-scale adversarial games and stronger poker AI systems.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
* Overall, the paper provides a new technique that is promising for an important area of research
* The results indicate strong improvements over alternative methods

Weaknesses:
For me, the paper's primary weakness is the presentation method. I had trouble understanding the significance and nature of the contribution from the current submission. In general, a clearer description of this area of research for people who, e.g., work on games but don't focus on poker would be quite helpful. Some specific suggestions/areas for improvement are:

* Clearer introduction of key concepts: The paper jumps into technical terms like 'imperfect recall abstraction' and 'hand abstraction' without adequately explaining them for a broader audience. A brief explanation of why these concepts are important in poker AI would be beneficial.
* More intuitive explanations of the algorithms: The descriptions of PWI, KRWI, and KrwEmd are highly technical. Including some simple examples or diagrams to illustrate how these algorithms work could greatly improve understanding.
* Better contextualization of the contribution: While the paper claims to outperform existing methods, it's not clear how significant this improvement is in the broader context of poker AI. A discussion of the practical implications of this improvement would be valuable.
* Clarification of experimental setup: The Numeral211 Hold'em environment is not well-known. A clearer explanation of how this relates to standard poker variants would help readers understand the relevance of the results.
* More accessible presentation of results: The graphs and tables are dense with information but lack clear explanations. Simplifying these visualizations or providing more guidance on how to interpret them would be helpful.
* Glossary of terms: Given the many technical terms used (e.g., 'earth mover's distance', 'K-recall winrate feature'), a glossary could be a valuable addition to help readers keep track of these concepts.

Limitations:
There's limited discussion of limitations. It would be good to include an explicit limitations section. In particular, it would be good to discuss potential computational challenges in more detail, any limitations on the scope of the evaluation, and future work that might be planned.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on the problem of hand abstraction for Texas Hold-Em style poker games. Hand abstraction is the process of partitioning game histories into infosets which still contain enough information to make strategically advantageous decisions. Previous approaches have focused on abstractions that primarily focus on the future outcomes from each hand, but the authors suggest that it may instead be beneficial to also include past information. They design a hand abstraction algorithm called KwrEmd that outperforms previous work by incorporating historical information.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The results seem to show that KrwEmd outperforms other imperfect recall hand abstraction algorithms in terms of exploitability.

Weaknesses:
As somebody who is unfamiliar with the subfield of imperfect recall abstraction, I found the paper to be quite confusing throughout. The authors do not often provide intuition or examples for their method, and I found it difficult to tell exactly which contributions were novel compared to Fu et al. While it's reasonable for a paper to use technical language at times, well-written papers are usually understandable by a broader set of readers than just those in the specific subfield.

Limitations:
Not sure.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes KrwEmd, a novel hand abstraction algorithm for imperfect recall settings in Texas Hold'em poker. The algorithm leverages K-recall winrate features, incorporating historical information in addition to future information for constructing hand abstractions. The authors introduce two new isomorphism frameworks: Potential Winrate Isomorphism (PWI) and K-recall Winrate Isomorphism (KRWI). They demonstrate that KRWI outperforms existing methods like POI in identifying distinct infosets. KrwEmd, which combines KRWI with Earth Mover's Distance (EMD) for hand classification, shows superior performance compared to POI, Ehs, and PaEmd in the Numeral211 Hold'em environment.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
*Originality*: The paper presents a novel combination of K-recall winrate features and EMD for hand abstraction in imperfect recall settings, addressing a critical limitation of current approaches that solely rely on future information.
The introduction of KRWI and PWI provides valuable new tools for understanding and constructing hand abstractions in poker AI.

*Quality*: The experimental results in the Numeral211 environment demonstrate a clear improvement over existing methods, supporting the claims of the paper. The paper includes an appendix with algorithm details and supplementary experimental data.

*Significance*: The proposed KrwEmd algorithm advances the state-of-the-art in hand abstraction for imperfect recall settings, offering a potentially significant improvement for developing stronger poker AI agents.
The incorporation of historical information is a valuable contribution that benefit positively future research in poker and other imperfect information games.

Weaknesses:
I found the paper  challenging to understand, though this may be due to my limited background knowledge in poker AI and game theory.  While the authors provide a background section, the density of the technical content and the numerous specialized terms make comprehension difficult.

The description of the accelerated algorithm in Appendix A.3 could be expanded for better understanding. Additionally, a clear discussion of the limitations of the accelerated algorithm would be beneficial.

The paper provides limited information about the proposed algorithms, particularly KrwEmd. While the core concepts are presented, the details regarding implementation and specific design choices are limited. More in-depth explanation and pseudocode would enhance the paper's quality.

Limitations:
The authors acknowledge the computational complexity of clustering with EMD and introduce an accelerated algorithm. However, the paper lacks a dedicated section addressing the limitations of the proposed methods and the accelerated algorithm.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper develops new hand abstraction techniques for Texas Hold'em-style games (in general: games with ordered signals), which fare better than previous methods in both the number of hands identified, and performance (exploitability) in a simplified version of the game. 

Hand abstraction is a technique aiding the strategy construction in a Texas Hold'em, where concrete hands, or rather concrete signal infosets (i.e. ""possible words"" according to the information revealed so far), are replaced by abstract infosets, represented in an abstract feature space (here $\mathbb{R}^n$).

The core idea of the paper is to use the features of hands from previous rounds in the construction of the current round feature. More precisely, the paper investigates a simple method (KRWI = *k-recall winrate isomorphism*) of maintaining, at a given round, the collection of all potential-winrate isomorphims (PWI) features from previous $k$ rounds by concatenating them all together. PWI for an $n$-player game is a categorical probability distribution over $n+1$ events of a form *""this player outperformed exactly $l-1$ other players while losing to none""* for $l = 1, 2, \ldots n$ and *""this player lost to at least one player""*. Those distributions can be computed by a dynamic programming method. To reduce the cardinality of the space, the paper later clusters KRWI features with k-means using the Wasserstein distance, naming it the KrwEmd method.

All of the methods are benchmarked against currently used techniques that do not use historical information. Experiments find that KRWI identifies similar proportion of signal infosets as the previously used KROI. Using the metric of exploitability of the equilibrium (how it deviates from Nash equilibrium) of the strategy found by an imperfect-information game solver, authors find that 2-RWI-based approach performs almost the same as 2-ROI, and that KrwEmd outpefrorms previously used Ehs and PaEmd by a relatively large margin.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proposes a reasonable extension of the currently used techniques for hand abstraction in Texas Hold'e, and shows that the new idea beats SOTA. It does not shy away from introducing the reader to the relevant background in a rigorous way (which is what made it possible for me to even start reading it). Experiments show a meaningful improvement, and provide additional insights (such as the decreasing worth of historical information).

Weaknesses:
From the perspective of someone not at all acquainted with the field of imperfect information games/games with ordered signals, the paper was quite hard to read and understand - even though (assuming that the authors agree with my summary), the contribution is a relatively straightforward idea.

The introduction was uninformative and confusing (I would recommend rewriting the whole second paragraph); the preliminaries, although presented in-depth and trying to be formal, also posed quite a few questions; sections 4 and 5 describing the main contribution lacked detail and justification (i.e. ideally I would like to see definition/theorem/proof style - otherwise the text is impossible to read for someone unfamiliar with the field), and the experimental setup is assuming a lot of background knowledge that was not explained neither in the main paper nor in the appendix (it was also difficult to gauge if the comparison between SOTA and the new approach was fair from the resources pov - the paper reports some numbers, but never an aggregated ""memory/time used"" for all methods).

Please see Questions below for a detailed explanation of what I found lacking or hard to understand.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
1MQXBnEbE8;"REVIEW 
Summary:
This paper presents a novel imputation method, based on a bipartite graph constructed from the data and the missing-data patterns, and two components which allow to measure similarities between the features and samples.

The method shows very good results in terms of MAE on several datasets for MCAR, MAR and MNAR data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written. 

- Experiments are well conducted, on several datasets, with different missing-data ratios and considering MCAR, MAR or MNAR data. The authors have made an effort to compare themselves with many other imputation methods. 

- There is a true discussion on the parameters to choose in the experiments. The authors are honest about the performance of their method, and give explanations when another method is better.

Weaknesses:
- Although well presented, the method is complicated to understand. 

- The methods uses 8 MLPs and one GNN. The authors discuss in Appendix the computational resources, but do not compare other methods on this point.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces M3-Impute, a mask-guided representation learning method for missing value imputation. The core idea of M3-Impute is to leverage missingness information as an explicit input to the model through masking schemes. This approach allows M3-Impute to effectively learn both feature-wise and sample-wise correlations, accommodating various types of data missingness. The model employs a variant of GraphSAGE for graph representation learning, incorporating edge embeddings via neighborhood aggregation. It outperforms traditional tabular data models in various benchmark datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper presents a novel approach to missing value imputation through the introduction of a mask-guided representation learning method (M3-Impute). The originality of the work lies in its utilization of missingness information as a model input, employing innovative masking schemes. This allows M3-Impute to accurately capture feature-wise and sample-wise correlations despite varying types of missing data (MCAR, MAR, MNAR). The use of GraphSAGE for graph representation learning, combined with edge embeddings via neighborhood aggregation, further distinguishes this work from traditional tabular data models.

2. The quality of the research is demonstrated through comprehensive experiments across multiple datasets and missing data mechanisms. The empirical results show that M3-Impute consistently outperforms baseline methods. The authors include a code package and datasets with the submission.

Weaknesses:
1. The paper evaluates the sensitivity of the M3-Impute model to the initialization parameter ϵ (Table 3), demonstrating that a non-zero value of ϵ improves imputation accuracy. However, the lack of detailed sensitivity analysis for other critical hyperparameters, such as the learning rate, batch size, number of GNN layers, and the dropout rate, represents a weakness.

2. The paper also has notable limitations in its contextualization relative to prior work. While it effectively presents M3-Impute and compares it against several baseline models, it lacks a deeper analysis of how these baseline models have evolved and the specific innovations they have introduced over time. For instance, the paper mentions GRAPE and IGRM as key prior graph-based imputation methods but does not adequately explore their strengths and weaknesses or how M3-Impute directly addresses the limitations of these methods. This omission makes it challenging to understand the novelty and improvements offered by M3-Impute.

3. Relying solely on MAE to evaluate the performance of imputation models has several limitations. MAE measures the average magnitude of errors but does not account for the variance or distribution of those errors, making it insensitive to outliers and providing no insight into model bias. This can result in an incomplete understanding of a model's performance, particularly in contexts where large errors or systematic biases are important considerations. To address these limitations, incorporating RMSE alongside MAE would be beneficial. RMSE penalizes larger errors more heavily, offering additional insight into the presence and impact of significant errors in the model's predictions.

Limitations:
The authors should consider summarizing the limitations of their method in the conclusion to provide a comprehensive overview of their work. Specifically, in Section 4.2, the authors discuss the cases of MAE degradation for the Kin8nm and Naval datasets. They attribute this to the independence of features in Kin8nm, which prevents observed features from aiding in the imputation of missing values, and the strong linear correlations between nearly all features in the Naval dataset. Summarizing these points in the conclusion would give readers a clear understanding of the method's limitations and the contexts in which it performs best.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes a missing value imputation method. The proposed method tackles the missing value imputation problem as a link prediction task on the bipartite graph. It represents a data matrix with missing values as a bipartite graph, then uses a graph neural network on the bipartite graph to learn the embeddings of samples and features. Next, a feature correlation unit and a sample correlation unit are employed to obtain feature-wise and sample-wise correlations, which are then fed into an MLP to obtain imputed values.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- An advanced and novel approach to missing value imputation. The idea is intuitive.
- Experimental comparison is done with various existing methods, including recent ones.
- Significant performance improvements achieved.

Weaknesses:
- The size of the bipartite graph may drastically increase with data size and dimensionality.
- No investigation on various missingness scenarios and missing rates.

Limitations:
-

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of missing values in data analysis and machine learning by proposing M3-Impute, a novel imputation method. Traditional imputation techniques often neglect 'missingness' information and fail to explicitly model feature and sample correlations, leading to suboptimal results. M3-Impute innovatively incorporates missingness information and correlations through advanced masking schemes. It represents data as a bipartite graph and utilizes a graph neural network with a refined initialization process to learn node embeddings. These embeddings are further optimized using feature correlation and sample correlation units, which explicitly consider the correlations during imputation. The method's effectiveness is demonstrated through experiments on 15 benchmark datasets with three different missing patterns.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The feature correlation unit (FCU) and sample correlation unit (SCU) are particularly compelling. The FCU learns correlations between the target missing feature and observed features within each sample, refined by a soft mask on missingness information. Similarly, the SCU computes sample-wise correlations, enhanced by another soft mask on missingness information for pairs of samples.

Integrating FCU and SCU outputs to estimate missing values is methodologically sound. Extensive experiments on 15 open datasets show M3-Impute's superior performance in 13 out of 15 cases under various missing value patterns. The reported improvements in mean absolute error (MAE), up to 11.47% over the second-best method, underscore M3-Impute's practical relevance and robustness.

Weaknesses:
M3-Impute demonstrates strong performance across many datasets but shows limitations in handling datasets with highly independent features or strong linear correlations, such as the KIN8NM and NAVAL datasets.

The model's robustness in general scenarios may degrade when confronted with datasets containing extreme correlation structures.

Future improvements could concentrate on enhancing M3-Impute's adaptability to these challenging cases to broaden its applicability and robustness across diverse datasets.

Limitations:
Although M3-Impute outperforms other baselines on most datasets, two cases (KIN8NM, NAVAL dataset) highlight its limitations in handling datasets with either highly independent features or strongly linear correlations. This suggests that while M3-Impute is robust in general scenarios, its performance may degrade in datasets with extreme correlation structures.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposed a new imputations method called M3-impute. M3-impute follows the basic structure of some recent imputation methods: a undirected bipartite graph is constructed with nodes for features and samples, where edge weights correspond to observed data at the given feature-sample pair. Previous approaches use Graph Neural Networks (GNNs) to impute missing values via edge weight prediction. M3-impute improves these approaches by adding two new components on top of an initial GNN to model feature-wise and sample-wise correlations respectively. Empirical results show that M3-impute achieves competitive performance in terms of MAE for imputation across several tabular datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well written.
- Empirical results are extensive. Many other imputations methods are included for comparison, providing a good representation for the state-of-the-art for tabular data imputation. Ablation studies and robustness studies also further strengthen the credibility of the methodology.

Weaknesses:
- The paper does not support categorical features. This is a big weakness compared to other imputation methods that can handle categorical features such as iterative approaches like hyperimpute. 
- The paper does not discuss the impact of missing value imputation on downstream tasks. Imputation is usually a preprocessing step, and thus assessing the impact on possible downstream tasks is paramount. For example, in supervised learning, some recent evidence suggests that mean/zero imputation is as good as more complex imputations [1, 2]. 

[1] Le Morvan, Marine, et al. ""What’s a good imputation to predict with missing values?."" Advances in Neural Information Processing Systems 34 (2021): 11530-11540.
[2] Van Ness, Mike, and Madeleine Udell. ""In defense of zero imputation for tabular deep learning."" NeurIPS 2023 Second Table Representation Learning Workshop. 2023.

Limitations:
See weaknesses. In particular, not handling categorical features is not mentioned in the paper anywhere as a limitation of the method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes M^3-Impute, a missing value imputation method that utilizes GNNs to learn embeddings of samples and features. By incorporating feature correlation unit and sample correlation unit, M^3-Impute effectively captures correlations between features and samples for accurate imputation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
S1. This paper introduces a novel masking scheme that effectively utilizes missing information for modeling.

S2. This paper proposes the feature correlation unit (FCU) and sample correlation unit (SCU), which help to consider feature and sample correlations during imputation.

S3. Experimental evaluations on various datasets compare the proposed method with state-of-the-art approaches, demonstrating good imputation performance.

Weaknesses:
W1. SCU takes into account the pairwise similarity of \mathcal{P} during its construction, which subsequently determines the scalar parameter \alpha during imputation. The initialization of \mathcal{P} seems to directly impact the model's performance and remains unchanged once set. It would be beneficial for the authors to discuss this aspect and, if possible, provide some experimental evidence to support their approach.

W2. In Section 4.4, the paper mentions different sampling strategies for SCU and uses a new strategy in the ablation study (Table 2), which is different from the strategy mentioned in Section 3.4. The authors claim that this strategy leads to inferior performance compared to previous strategies, thereby highlighting the superiority of the ablation study results. This lacks experimental evidence and results in inconsistency in the experimental setup.

W3. As a crucial parameter, the size of \mathcal{P} directly affects the construction of SCU. Table 3 presents experimental results with different sizes, but the differences are not significant, which is somewhat counterintuitive. Although the authors discuss the experimental results in Section 4.4, the performance fluctuation of 0.01 to 0.02 does not clearly reflect the ""decrease then increase"" trend mentioned by the authors. Exploring larger peer values and providing more detailed analysis and guidance on parameter selection might be beneficial.

W4. The authors emphasize the importance of specific missingness information throughout the paper. Intuitively, different types of missing data (MAR, MNAR, MCAR) might offer varying types of missingness information, potentially impacting the model's performance. While the paper experiments with data of different missingness types, a more thorough discussion of the results could enhance the motivation and clarity of the paper.

W5. In Section 4.3, the caption of Table 2 references a concept, the uniform sampling strategy, which is introduced for the first time in a later subsection. The authors might consider adjusting the structure of the paper for better clarity.

W6. In Figure 3, in the subfigure with the missing ratio of 0.7, two bars exceed the upper boundary and need adjustment.

W7. It would be helpful to add independent labels to all subfigures, such as (a), (b), etc., to facilitate referencing.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This is a novel approach for imputing missing data using mask-guided representation learning. The main contributions include the development of an imputation model that leverages both feature and sample correlations. This model improves imputation accuracy and robustness compared to existing methods. The paper also provides comprehensive experiments and ablation studies to validate the effectiveness of the proposed approach across various datasets and missing data scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Novelty: A unique mask-guided representation learning method that effectively combines feature-wise and sample-wise correlations.
- Comprehensive experiments
- Strong empirical performance

Weaknesses:
- Computational complexity 

## Minor Points

l4: ""Existing imputation methods, however, fall short of considering the ‘missingness’ information in the data  during initialization and modeling the entangled feature and sample correlations  explicitly during the learning process,""
-> This is not true. Many existing methods consider missingness patterns.

The distinction between ""statistical"" and ""learning based"" methods seems off. Certainly most learning based methods are statistical, and vice versa.

l. 38 ""struggles"" -> struggle

Limitations:
- Gives little insight into why it works better on some datasets than others.
- Would be interesting to understand better how robust results are under systematic changes in datasets, e.g., different types of missingness.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
1CssOqRYyz;"REVIEW 
Summary:
The paper proposes the first diffusion-based point cloud compression method called Diff-PCC.  
A dual-space latent representation is devised in this paper, where a compressor composed of two independent encoding backbones is used to extract expressive shape latents from different latent spaces.    
At the decoding side, a diffusion-based generator is devised to produce high-quality reconstructions by considering the shape latents as guidance to stochastically denoise the noisy point clouds.    
Experiments demonstrate that the proposed Diff-PCC achieves state-of-the-art compression performance (e.g., 7.711 14 dB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while attaining superior subjective quality.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Novelty is a strength. To my knowledge, diffusion model is used in point cloud compression for the first time. And the dual-latent design is also novel for learned point cloud compression.   

The manuscript is well written and easy to follow. Especially, the author did a good job in introducing related works on image compression, point cloud compression, point cloud analysis and diffusion model.

Weaknesses:
More work on diffusion model for data compression could be discussed, like ‘Idempotence and Perceptual Image Compression, ICLR 2024’. In addition, although this paper focuses on point cloud compression, the way of applying diffusion model should be compared with those learned image compression works in the related work part. From my impression, the method in this paper is still novel compared with those learned image compression paper using diffusion model.    

More recent learned point cloud compression method [30][14] should be compared in Table 1, Figure 3 and Figure 4, regarding rate distortion and encoding/decoding speed. Besides, only object point cloud is considered currently, large scale point cloud like SemanticKITTI could be compared [30][14].  
 
It is not clear how the speed is measured in Table 1. The hardware and commend line shoud be provided in the supplementary material.  

Minor:  
L86, Point·E[] is a typo.  
[30] and [31] are the same.   
L202, the reference should be fixed.   
What is the FPS in eq 14? farthest point sampling?

Limitations:
The limitation is addressed in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors propose a diffusion-based point cloud compression framework. Low frequency and high frequency features are extracted via PointNet and PointPN from input point clouds, which are quantized and encoded for compression. During decompression, the quantized features would be decoded to condition a diffusion model to construct the decompressed results. The experiments on 15K points data from ShapeNet, ModelNet10, and ModelNet40 show superiority over compared methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea to introduce diffusion models for point cloud compression is different with former works;

2. The paper is easy to follow, while the disgrams are also good;

3. The performances show improvements on sparse point clouds.

Weaknesses:
1. The comparison is not convincing enough. Some commonly used compression methods are not compared, while the evaluation is limited to sparse point clouds with relatively simple structures from ShapeNet, ModelNet;

2. The motivation of using diffusion model for compression is questionable. As a sampling-based framework, diffusion models may construct different results during decompression from variant sampled noises each time. I am not so sure if the diffusion model is more appropriate than existing AE or VAE-based frameworks for the compression task, which may need decompression as accurate as possible;
Please check the questions for more details, thanks.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, they introduce the diffusion-based point  cloud compression method, dubbed Diff-PCC, to leverage the expressive power of the diffusion model for generative and aesthetically superior decoding. They get better performance than G-PCC and two deep learning methods.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Encoding point clouds using diffusion models is a good idea. The article is easy to understand.

Weaknesses:
Firstly, how do we obtain a point cloud with added noise in the decoder? We have no knowledge of any other information about the original point cloud, except for the information in the bitstream. This will result in the inability to decode.
This manuscript claims to achieve state-of-the-art compression performance, but it only compares with two deep learning methods from the past two years. It does not compare with the most advanced methods such as CNet, SparsePCGC, and so on.

Limitations:
The decorder will not work

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
08oUnmtj8Q;"REVIEW 
Summary:
The authors developed a few-shot evolutionary optimization framework to effectively solve the multi-objective EOPs and constrained EOPs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed method can solve the multi-objective EOPs and constrained EOPs with little data, especially for the engineering problems.

Weaknesses:
The learning results may rely on the relation degree of different tasks.

Limitations:
The overhead of the algorithm need to be further decreased.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new surrogate-assistant evolutionary algorithm that utilizes a Gaussian process with Deep Kernel Learning as the surrogate model. The method employs few-shot meta-learning to learn from multiple tasks to construct the surrogate. It is then integrated with the existing MOEA/D-EGO algorithm to create a new approach. Experiments are conducted on the DTLZ benchmark problems and a gasoline motor engine calibration problem to evaluate the performance of the proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is instantiated and tested in expensive multi-objective optimization and constrained optimization scenarios.
2. A real-world problem is considered in the experiments.

Weaknesses:
1. Many important algorithmic details are unclear. For instance, the main distinction between the proposed MDKL and the existing DKL algorithms is its ability to learn from a set of related tasks, yet its implementation is not clearly explained. How parameters from different source tasks collectively form the experience, and how parameters from both source and target tasks jointly create this experience, are not clearly addressed.
2. The effectiveness of the algorithm is primarily tested on expensive multi-objective optimization problems, but state-of-the-art algorithms in this field were not selected for comparison.

Limitations:
Even if a clear definition of task similarity cannot be provided, it is recommended to offer some hints to help users apply the algorithm.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduce a meta-learning framework into few-shot optimization to assist the surrogate modelling in expensive evaluation setting. The authors parameterize a mapping function to get the hidden feature of the solution space and then integrate such mapping into a gaussian kernel function as a deep kernel. They then facilitate meta-training of the proposed deep kernel over a group of related tasks to attain an experience model, by maximizing the posterior likelihood. During the online optimziation of the target task, the experience model is firstly adpated to the new task in the same way above and then updated acoording to its accuracy in terms of the predicted objective value. The experimental results show that the proposed framework achieves competitive performance against some strong baselines over EMOPs and ECOPs benchmarks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The idea of integrating meta-learning into the kernel-learning based surrotgate methods is novel, and might improves the surrogate-based optimziation towards generalizable setting.

2. The expriments result show that the proposed FSEO framework is at least competitive with the existing baselines, which is acceptable and should be encouraged for further development.

3. The overall writing is clear.

Weaknesses:
Before the next round of author-reviewer rebuttal, following concerns exist:
1. Given that the likelihhod-based loss function (Eq. 4) should be maximized to fit the samples from all of the related tasks, why its update should follow a gradient descent rather a gradient ascent? Correct me if I was wrong.

2.  line 144 ~ 146, the authors state that the U update interations roots from the smaller number of available related tasks. I can not understand the reason behind, can you explain it more?

3. The neural network $\phi$ used in the deep kernel function is a 2-layer MLP, which limits the FSEO to meta-learn surrogate function among the related tasks with the same slution dimension. However, in practice, related tasks might not share the same dimension. I would appreciate the authors to provide realistic scenarios where FSEO is eefective. Besides, the effectiveness of the FSEO on traditional single-objective tasks should also be verified to make it more convincing that FSEO is a general framework.

4. Although the overall writing of this paper is not bad, it is still difficult for less-skilled readers to understand the whole picture. In particular, the content in Section 3.2 and Section 3.3 should be carefully refined to make sure the potential readers fully understand how the DKL and MDKL operate. For now, it is too simple and ambiguous.

Limitations:
The limitations listed in Conclusion are quite brief. I would appreciate the authors to explain more about the two limitations listed here.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes Meta Deep Kernel Learning (MDKL), a new surrogate for SAEAs. MDKL consists of a deep kernel with meta-learning. Empirical studies demonstrate its effectiveness in expensive multi-objective optimization and constrained optimization.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. This paper is well-written and easy to follow. The technical details are well presented.
2. This paper extends deep kernel and meta-learning-based surrogates into evolutionary algorithms.
3. This paper investigated multi-objective optimization and constrained optimization.

Weaknesses:
Meta-learned deep kernel surrogates have already been well-studied in Bayesian Optimization [1]. The authors are also aware of this as they mentioned in Related Work. I think this paper does not present significant new advancements based on the previous work.

First, the authors claim that MDKL is specially designed for optimization, while the previous work is not. In this regard, I do not see many differences between MDKL and previous meta-learned deep kernels. The authors claim that the advantage of MDKL lies in continuous adaptation; however, most models support parameter updates or fine-tuning. The authors also did not sufficiently explain the relationship between continuous adaptation and optimization problems, or what significance it has for optimization problems.

Second, the authors propose that one of the novelties of this paper is taking expensive multi-objective optimization problems (EMOPs) and expensive constrained optimization problems (ECOPs) into account. MDKL, as a surrogate, can be integrated into almost any expensive optimization algorithm. It seems to be able to cooperate with Bayesian optimization as well. The authors simply replaced the surrogate in a multi-objective optimization algorithm with MDKL and conducted some experiments, without providing any new analysis, insights, or proposing any new methods specifically for MOPs or COPs. Therefore, I believe this paper does not make a significant contribution to solving EMOPs and ECOPs.

[1] Martin Wistuba and Josif Grabocka. Few-shot Bayesian optimization with deep kernel surrogates. ICLR 2021.

Limitations:
No concerns.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
