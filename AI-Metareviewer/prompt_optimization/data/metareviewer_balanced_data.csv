id;text;label
zzOOqD6R1b;"REVIEW 
Summary:
The paper studies whether fine-tuning can elicit the hidden capabilities of LLMs, especially motivated by the setting of dangerous capabilities evaluations. 

To provide a specific experimental setup, the paper considers password-locked models, which responds with weak answers unless provided with a fixed password in the prompts. By giving strong demonstrations generated from a strong base model to the locked model, they show that SFT typically can recover most of the capabilities hidden in the locked model, achieving unlocking. 

The paper continues to study RL (with reward but not explicit demonstrations) and a toy setting where the password locking is achieved from scratch.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
In my view, the paper studies a well motivated concern in current safety research agenda on dangerous capability eval. The password-locking scenario is a novel instantiation that we can concretely study capability elicitation. I find that simply making this formal and executable is already a meaningful contribution.

The paper is nicely written and well structured. 

The two main lines of the experiments on SFT and RL are solid, and the results are novel.

Weaknesses:
For section 5, a minor concern I have is that the password-locking is achieved by SFT as well. This introduces this  weird ""causal structure"" in the experimental setup: specifically, the capability elicitation (in section 5) and the password-locking are both via SFT. I can imagine that we get the results in section 5, not (only) because SFT is good in capability elicitation, but because it's relatively weak in robustly password-locking the model. This might happen, for example, if password-locking a kind of an unnatural task, and a base LM trained on natural language corpus won't robustly lock itself via SFT. To be fair, I think this is not the most likely explanation of the results in section 5. But I am wondering if the author(s) has any take on the relative power of SFT. (Also, I think section 7 partially addresses this concern anyway, though I look forward to seeing more evidence against this weird hypothesis.)

The paper would generally be stronger if there is a straighforward baseline on capability elicitation via few-shot/many-shot prompting only (no messing with the model weights). Similarly, I'd love to see how RepE / steering vector perform on password-locked models; i.e., can you find a direction in the activation space to unlock the model? (I would guess you can.)

Limitations:
The paper addresses its limitations in section 8.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To determine whether models are safe to deploy, it's important for labs and regulators to be able to ""elicit"" model capabilities, including ones that are somehow ""hidden"" (whether by a malicious deployer, by some unforeseen property of the training data, or by an unaligned model that is able to plan). One way to evaluate for the presence of some capability is to fine-tune the model to perform the capability and check whether the model is able to do it. Does this work? The authors study this question by introducing and training ""password-locked"" models that are trained to only perform particular tasks well if a password is present in the prompt. The authors find that given sufficiently good demonstrations, fine-tuning and RL succeed at eliciting capabilities, but very poor demonstrations are insufficient.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The strongest contribution of this paper is the framing of the problem---motivating the problem of capability elicitation and showing some preliminary experiments in the space. The problem is pretty similar to backdoor detection but the new framing does provide different constraints which I find interesting: (1) the test model (in this case a password-locked model) doesn't necessarily have to be poisoned in a sneaky way like in backdoor attacks, instead the goal is to produce a test model in any way that somehow hides capabilities in a way that realistically imitates ""scheming"" or ""sandbagging""; (2) the attacker is not necessarily given access to the training data or high-quality data.
- Though I think it's unsurprising that fine-tuning successfully unlocks these password-locked models, I think the finding that fine-tuning on lower quality data doesn't work as well is an interesting contribution (a priori I could imagine that fine-tuning on lower quality data would still work to fully unlock the model if the password-locking were implemented internally as a simple conditional). The result in Fig 10 where domain generalization still works when each domain has a different password is also slightly surprising.
- Overall I like the experimental methodology (testing both supervised fine-tuning and RL, modulating the quality of the fine-tuning data, testing generalization across domains---all choices motivated by their problem framing).

Weaknesses:
The technical contributions here are somewhat limited. I'd say password-locking is a fairly simple way of modeling hidden capabilities and could be quite unlike the threat models that the authors are actually interested in (situationally-aware LLMs or AI labs deliberately hiding capabilities from auditors; in the case of situationally-aware models, presumably the model does some kind of complex internal reasoning to determine their next action, whereas these password-locked models seem likely to just learn some kind of simple conditional to output X or Y). Indeed, the authors acknowledge this briefly in the limitations section.  To some extent I think this is a result of the these threats not being understood well enough for us to know how to model them in a more realistic way than these password-locked models. Given the simplicity of password-locking I'd say that the success of the authors' elicitation methods are mostly unsurprising and also may not generalize to their real threat models.

Limitations:
The authors' limitations section is well written, and I can't think of any direct negative societal impacts.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study the problem of testing advanced LLMs for non-obvious capabilities in a toy setting, where a model has been fine-tuned to only perform well when presented with a fixed password, and to perform poorly otherwise. This is meant to simulate the difficulty of finding the correct prompt for eliciting a particular capability among many other suboptimal prompts. The authors then study various methods of fine-tuning to trigger the strong performance even without the right password, such as supervised fine-tuning on high/low-quality demonstrations as well as various forms of RL which do not require demonstrations at all. They find that performance on par with original with-password performance can be easily achieved, suggesting that prompt fragility and variability may not be difficult problems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The problem is interesting, and the authors present a creative reduction to the toy problem of password-locked models
* A wide array of elicitation methods is considered, from many different settings of SFT as well as various demonstration-free methods using reinforcement learning to explore the more difficult where we do not have access to high quality demonstrations. Editorially, I do not think this is a very important setting, since it seems unlikely that we would ever be testing models for capabilities for which we have no idea what successful behaviors look like. And the dependence on RL approaches, which are in general much flakier than supervised learning, casts a lot of doubt on whether results in this setting would generalize in the real world. But the authors think this is important and study it carefully.
* Lots of analysis on a diverse set of task settings

Weaknesses:
* The authors explain the definition and process of password-locking models quite thoroughly, but their paper would be better served by a clearer and earlier motivation for studying password-locked models.
* In general, the authors lean heavily on references to prior work to explain various decisions in their experimental design, or different analyses of model performance. It would be helpful if these choices were just explained directly, and then separately credited prior work for motivating these decisions.
* There are various alternative approaches to eliciting stronger performance without the correct password, such as prompt optimization (e.g. with GCG or even gradient-free search) or few-shot prompting, which are not explored in this paper
* The nomenclature throughout the paper is a little confusing. The pi^weak_base notation was hard to keep track of, as well as the locked with password/locked without password/unlocked distinctions
* Given the close connection to neural backdoors/trojans, the authors should explain more clearly why or why not backdoor removal methods are not applicable here

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of eliciting hidden capabilities from language models, through fine-tuning. In order to plant 'locked' capabilities and then do controlled test of elicitation, the authors lock the capabilities using passwords they bake into the prompts at fine-tuning. Then, they try to elicit the capability without the password in a zero shot way, and then after fine-tuning with a few good samples, or using RL and existing judgment labels. They find that using a few good FT samples the capabilities are almost completely attainable.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. I liked the idea of using passwords to control for the model behavior, and lock the capabilities.

Weaknesses:
1. I am not sure about the motivation of the paper. Is model locking something that people really do in practice?  Using a set of words in a prompt is pretty weak in practice, there might be stronger ways to lock a model, such as posthoc methods at decoding time. Also the findings and insights are not that surprising. Many safety alignment and jailbreaking papers show that alignment is 'shallow' and can be easily reversed [1,2]

2. Using fine-tuning and RL at decoding time is a pretty strong assumption, as having access to model parameters, training a model and also having access to high quality data is not that realistic.

[1] Patil, Vaidehi et al. “Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks.” ArXiv abs/2309.17410 (2023): n. pag.

[2] Yang, Xianjun, et al. ""Shadow alignment: The ease of subverting safely-aligned language models."" arXiv preprint arXiv:2310.02949 (2023).

Limitations:
The paper discusses limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zxSWIdyW3A;"REVIEW 
Summary:
The authors present a Federated Hardware-Prompt learning (FedHP) framework to address the fact that compressive snapshot spectral imaging devices may not be easily tuneable against changes in the coded aperture, and that in fact the said access to coded apertures may not be possible due to privacy reasons. The authors solve this by hardware prompt learning, which essentially learns from observing diverse coded aperture samples of all clients, regularizing the input data space and achieving the goal of coping with heterogeneity sourcing from hardware. The results show on a specific dataset improvement across all 10 samples in terms of spectral reconstruction quality. The comparison is primarily in the sense of federated learning approaches.

Typo: figure 3  caption -> colorblue shouldn’t be there

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The presentation is somewhat accessible to a generally knowledgeable non-expert in federated learning, in that the purposes are clear.

Weaknesses:
The biggest weakness is arguably that the paper covers a somewhat very niche topic, which is the application of a federated learning scheme to compressive snapshot spectral imaging. To some extent one would expect the technique to abstract away from the specific case of CASSI, as the solution does not particularly pertain to CASSI.

In addition, due to limited data available in this setup and to very limited size datasets, it is difficult to ascertain the significance of the findings.

Limitations:
The addressed setup assumes that the problem the authors propose to tackle is meaningfully posed, I.e., that federated learning in the chosen formulation is practically meaningful. The reviewer is not sure whether this is a practically relevant problem considering that CASSI systems are arguably scientific instrumentation/experimental devices whose calibration is likely done per case anyways.

In addition the topic would appear to be more meaningful for publications that cover CASSI systems such as IEEE TGARRS or the like. It is hard for this reviewer to disentangle the margin of novelty of this paper in terms of federated learning approach vs. impact on the target application.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the challenges faced in snapshot compressive imaging (SCI) systems due to hardware shifts and the need for adaptability across multiple hardware configurations. By introducing a hardware-prompt network and leveraging federated learning, the framework enhances the adaptability and performance of SCI models across different hardware configurations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The manuscript is well-organized with a clear and logical structure that enhances the readability of the content.
2. The paper provides a detailed background on SCI and FL. The planned release of the Snapshot Spectral Heterogeneous Dataset (SSHD) will significantly aid future research.
3. Using different coded apertures for different clients closely mirrors real-world scenarios, adding significant practical relevance to the study.

Weaknesses:
1. The literature review on federated learning (FL) heterogeneity in the Introduction section lacks comprehensiveness. There are numerous recent papers addressing heterogeneity in FL that are not cited here. Additionally, the references included are somewhat outdated. Including more current and diverse references would strengthen the review and provide a more accurate context for the study.
2. the manuscript explains that the coded apertures for each client follow a specific distribution Pc, it does not provide further details about the exact nature or type of this distribution.
3. There are many ways to partition data to construct heterogeneous scenarios, such as practical and pathological methods. The approach of equally splitting the training dataset according to the number of clients is not very convincing. The authors should try different partitioning methods.
4. It is unclear which datasets were used to obtain the experimental results in Tables 1 and 2. The authors did not specify this, which creates confusion in the experimental analysis.

Limitations:
1. In the ""Discussion of the client number"" section, the number of clients increases very little, and the metrics slightly decline. However, the authors conclude that the performance is stable with the change in the number of clients. The small variation in the number of clients is unconvincing. A larger difference in the number of clients should be set to demonstrate this more effectively.
2. The authors mention the ""presence of data privacy"" in the contributions, but there is no further discussion or experimental comparison regarding data privacy in the subsequent sections. This makes it difficult to validate their contribution to data privacy protection.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Most existing reconstruction models in snapshot compressive imaging systems are trained using a single hardware configuration, making them highly susceptible to hardware variations. Previous approaches attempted to address this issue by centralizing data from multiple hardware configurations for training, but this proved difficult due to hardware heterogeneity across different platforms and privacy concerns. This paper proposes a Federated Hardware-Prompt Learning (FedHP) framework, which aligns data distributions across different hardware configurations by correcting the data distribution at the source, thereby enabling the trained model to adapt to multiple hardware configurations. The performance on existing datasets shows an improvement compared to previous popular training frameworks. Additionally, the authors have released their own created dataset and code.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.Previous work focused on the data itself, directly correcting various types of data through network models. In contrast, the authors of this paper focus on the root cause of the differences—hardware. They address the issue from the perspective of learning the differences in hardware.
2.The method proposed by the authors has achieved excellent performance compared to existing mainstream methods, and the average performance has also improved.

Weaknesses:
1.The number of clients used in the experiments is still relatively small. Although a simple comparison of the impact of different numbers of clients was made, there is not much difference in performance compared to other methods when the number of clients is larger.
2.Although good results were reported on simulated data, more results on real data should be included to evaluate the effectiveness of the proosed method.

Limitations:
The generalization to different hardware systems is crucial for deep learning based methods. The current form of this manuscript only reported on a small scale real dataset captured by several systems. A larger dataset captured by more systems is necessary to evaluate the method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces FedHP, a reconstruction method for snapshot compressive imaging systems, which addresses the challenge of cross-hardware learning by proposing a federated learning approach. The key contribution lies in using a hardware-conditioned prompter to align data distributions across different hardware configurations, thereby enhancing the adaptability of pre-trained models without compromising data privacy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The writing of the paper is good, making it easy to read and follow with clear arguments.
2. The problem defined in the paper is novel with a clear motivation, providing good inspiration for solving the issue of inconsistent device configurations in snapshot compressive imaging.
3. The proposed method is clear and the conclusions are relatively convincing. Overall, it is an interesting work.

Weaknesses:
1. There are some typos in the writing. For example, the caption of Figure 3 and the bold parts in the second row of Table 1 and the eighth row of Table 2 are confusing.
2. The proposed FedHP method is relatively straightforward and lacks deeper insights. Moreover, it does not show a significant performance improvement compared to FedAvg.
3. The experiments are not comprehensive enough. Given that this work aims to address the snapshot compressive imaging (SCI) problem, I suggest adding experiments to test the applicability of other SCI systems, such as Coded Aperture Compressive Temporal Imaging (CACTI).
4. There is a lack of sufficient real-world experiments. It would be beneficial to set up multiple independent SCI systems to test the algorithm's performance. Including reconstruction results obtained from these real-world systems is recommended.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zw2K6LfFI9;"REVIEW 
Summary:
The paper proposes a framework that integrates large multimodal language models (MLLMs) and diffusion models to enable holistic language planning and vision planning for long-horizon robotic manipulation tasks with complex instructions. The authors jointly train the MLLM and diffusion model for language reasoning and visual imagination through latent image token generation. An explicit consistency loss aligns the reasoned instructions with the imagined subgoal images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel motivation for integrating of multiple modalities for providing better guidance.

2. Principled design of the framework components like the encoding-side alignment and the latent image token generation approach.

Weaknesses:
1. Weak experimental evaluation (see below questions).

Limitations:
Yes, the authors address the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tackles the problem of long-horizon task planning on pick-and-place tasks in the Ravens domain. Given a dataset of trajectories, it first learns the projection to align the vision and language encoder for a multimodal LLM. Then it finetunes both the multimodal LLM and a diffusion model to generate a step action in language, where the diffusion model is used to generate a conditioning subgoal image, which is proposed as an intermediate step that helps with the step action generation in language.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is overall well-written and the figures are helpful for understanding the method.

Weaknesses:
- It is unclear, at least from the experiments in the paper, that the diffusion model is actually useful, especially when the output is still in language space. For example, it seems that the tasks studied in the paper can be easily tackled by a modern multimodal language model (likely even the open-sourced ones), by simply providing the the initial image and appropriate prompting. However, this is missing as an important baseline in the paper (and this does not require additional training data). Furthermore, to demonstrate the effectiveness of an image subgoal in addition to a language subgoal, the evaluation would have to be done on tasks that have subgoals that are difficult to describe in language but easy to describe in visual space, but all the evaluated tasks are the contrary.
- A related work “Video Language Planning” also seems to be missing from the paper, despite it might involve closed-sourced models. However, the idea seems quite relevant and it’s unclear if the paper provides additional insights for the community.

Limitations:
The limitations are described in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a holistic vision-language planning method for long-horizon robot manipulation, by learning a multi-modal large language model (MLLM). The MLLM generates interleaved language actions and keyframe images based on language goal and the initial image. Each pair of generated language and keyframe image is used as conditioning of a learned motion policy for robot manipulation.

Based on a pretrained MLLM model, the paper first learns a projector to align visual encoding to with language on image captioning tasks tailored to robot manipulation. Then it applies instruction tuning to fine-tune the MLLM, an output projector, and a diffusion model to generate interleaved language and images. Additional, the authors propose another training objective to align the generated language and images. All large models are fine-tuned with LoRA.

On simulated robot manipulatio benchmarks, the proposed method outperforms imitation learning, language planning, and vision planning methods. The paper also systematically evaluates capabilities of the MLLM along different axes, and justifies the benefits introduced by each loss design via ablation studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper tackles the important challenge of robot long-horizon planning. The proposed method plans jointly in the language and image space, providing rich information for the low-level policy to condition on.
- The paper exploits the capabilities of MLLM to generate language and images for robot manipulation, used with a separate low-level policy. I think this is good practice as MLLM is not naturally suitable to generate robot motion.
- The experiments are comprehensive and provide useful information on understanding the capability of the trained MLLM.
- The paper is in general well-written and easy to follow.

Weaknesses:
- The explanation of low-level policy is missing from the main paper. This part is very important - the MLLM outputs language and images only, and it's not clear how these modalities are bridged with robot motion.
- The contribution of the alignment loss between generated image and language is not sufficiently justified in the experiment. It will be helpful if the authors can provide the task success rate when the loss is absent.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on robotic manipulation with complex instructions. It proposes PERIA, a framework that integrates MLLM and diffusion models to incorporate both language planning and visual planning for long-horizon language-instructed manipulation tasks. Specifically, PERIA first performs a lightweight multi-modal alignment to consolidate the multi-modal perception capabilities. Then, PERIA performs multi-modal instruction tuning, where it outputs both subgoal language descriptions and visual tokens, both of which are fed to a diffusion model to generate subgoal images. PERIA introduces an additional consistency loss between and generated subgoal image and language descriptions. Experimental results demonstrate that PERIA significantly outperforms competitive baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
•	This work follows a natural and reasonable pipeline to tackle the manipulation tasks with complex language instructions. Combining language planning and visual generation for manipulation is a sound approach.

•	The alignment stage empowers the overall capabilities, as demonstrated in the experimental part.

•	PERIA achieves convincing experimental results compared with previous works. The authors also conduct extensive ablative study to mine more insights.

Weaknesses:
•	End-to-end learning for such a large system requires considerable cost. Such a comprehensive framework may lead to powerful performances but the resources may be a limitation. This paper does not present how much resources PERIA uses or related experiments to address such potential concerns.

•	One of my concerns is that the consistency objective, which forces the MLLM to output subgoal language descriptions, may suffer from accumulative error. This is because when the generated subgoal image is not the desired image but is a natural image that can be reached within one-step action, the MLLM would learn an incorrect subgoal description.

•	More literature references and related baselines should be incorporated.

•	The ablation in visual planning lacks an experiment where PERIA generates subgoal images with either subgoal descriptions or generated visual tokens, which should reveal more insights into what leads to the improvements in visual planning.

Limitations:
Yes, the authors address the limitations at the end of the conclusion.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
zv9gYC3xgF;"REVIEW 
Summary:
The paper studies the convergence of EM for learning mixtures of Gaussians.  Specifically, they consider a simplified setting where the Gaussians are in $d$-dimensions and all have covariance $I_d$.  They consider an overparameterized version of the problem where they parametrize the mixture they are trying to learn by a mixture of $n$ Gaussians with means $\mu_1, \dots , \mu_n$ and the ground truth distribution generating the data just consists of a single Gaussian $N(\mu^* , I_d)$.  The paper analyzes the dynamics of gradient EM for this problem.  The main result of the paper is proving that for this overparametrized variant, gradient EM converges to the true distribution at a rate of $1/\sqrt{t}$ with additional constants depending exponentially on the distance between the initialized means and the true mean, which they show is necessary.

There has been a long line of work on understanding the convergence of EM or gradient EM for learning mixtures of Gaussians.  Without overparametrization, provable convergence is known for mixtures of two Gaussians and it is also known that convergence fails in general for mixtures of three or more components.  For overparamterized settings, a previous work [Dwivedi et. al. 2018] shows that if we parametrize a mixture of two Gaussians and try to learn a ground truth distribution consisting of a single Gaussian, then EM converges at a $1/\sqrt{t}$ rate (as long as the mixing weights are set to be different).  This is in contrast to when we parametrize with only a single Gaussian and EM converges exponentially fast.  The results of the current paper can be seen as generalizing the results of [Dwivedi et. al. 2018] to more than two components.  The paper empirically validates their theoretical results with experiments on simple synthetic datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper makes progress on a well-studied problem of understanding convergence of EM for learning GMMs.  They give the first global convergence results for mixtures with more than two components.

The paper overcomes nontrivial technical barriers to extend previous results to more than two components.

Weaknesses:
The results of the paper only work when the ground truth is ""trivial"" i.e. a single Gaussian.

The results are qualitatively similar to previous work on overparametrized mixtures of two Gaussians.  The contributions of the paper are mostly technical and it is a bit difficult to find a nice conceptual takeaway \--- the previous work for two components already showed that overparametrization can lead to drastically slower convergence.  It would be much more exciting and novel, say, if we could prove something when the ground truth were not just a single Gaussian.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper talks about the gradient-EM algorithm for over-parameterized GMM. The paper mostly shows the GLOBAL convergence and its rate when using this model to learn a single Gaussian.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
I believe any non-convex global convergence optimization problem is valuable. It is an extension of Dwivedi et al. 2019.

Weaknesses:
1. The over-parametrized model may have severe overfitting problem. 
2. The based distribution is quite easy: a single normal, with known variance. In the paper, the covariance is fixed as the identity, which simplifies the problem in a deep way. Actually for symmetric 2-GMM, there are already faster algorithms to learn both mean and cov. 
3. I feel confused about the consistency and convergence in the paper. In Line 96, the convergence of KL divergence also contains the convergence of MLE, ie consistency. The convergence to the MLE is another loss function. Also in Remark 6, the convergence when sample size to infinity seems more easily ensured by WLLN.

Limitations:
Besides above, 
7. the citation format is not uniform.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper focuses on the setting of a Gaussian Mixture Model with several summands and an input vector produced by one Gaussian distribution, where it employs the Expectation-Maximization rule to infer the model's parameters. Since the problem of having arbitrary number of summands has been unsolved, the paper provides an innovative scheme which includes the computation of the likelihood function and shows that the EM algorithm converges with sublinear complexity. 

The authors also show that there exist neighborhoods of slow convergence rates.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper is well written, the theorems, lemmata and algorithmic steps are described gradually.
- From a first overview of the literature, the result about global convergence seems novel. 
- Across section 4, there is intuition and remarks provided about the necessity of the steps.

Weaknesses:
- The experimental evaluation is used as a proof of concept and thus is limited. The authors could have (potentially) experimented with several datasets, with varying weights in the GMM, and try to benchmark their algorithm to compare the emergent convergence rates.

Limitations:
NA.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers fitting a single Gaussian with multiple-component Gaussian mixture models (GMM) through the Gradient EM algorithm. While the two balanced over-specified Gaussian setting has been widely studied in the previous work, generalizing it to multiple-component GMM requires significant algebraic efforts. The entirety of the paper is to show the $1/\sqrt{t}$ convergence rate of the population EM algorithm. In particular, the paper characterizes the explicit convergence rate of $1/\sqrt{T}$ with constants exponential in the number of components, the phenomenon that coincides with the exponential lower bound for the parameter estimation of general GMMs with no separation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
-	Extending some existing two-component results to general multiple-component GMM is non-trivial and significant. The paper nicely characterizes the convergence rate that captures some important properties of learning GMM that can be achieved by GMM. 

-	The paper is well-written, emphasizing important aspects of the results and well-contrasting their techniques to existing results. 

-	Proof sketch is nicely written to help readers understand their key results.

Weaknesses:
-	While the lower bound result (Theorem 7) is a nice addition to the literature, I believe that the gap between this lower bound and the upper bound is large, since the upper bound is exponentially slow in the number of components. 

-	One important result from two specified GMM is the $n^{-1/4}$ (n is the number of samples here) statistical rate after convergence. I would like to see $n^{-1/2k}$ style results in general k-component GMM settings. At least, the authors should have discussed this aspect of previous work and contrasted the implications to k-GMM settings. 

-	The experiment would have been nicer if the final statistical rates were compared.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zv4UISZzp5;"REVIEW 
Summary:
This paper proposes a method of generating prompts for evaluating large language models such that the prompts are dynamic and allow for showing meaningful performance gaps between different language models.The authors show that the generated data is more-challenging and discriminative than prior datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Work is very timely and addresses a major issue in how we can better evaluate LLMs which are continuously improving and saturating existing benchmarks.
- Good to see that the generated prompts are indeed harder than baseline datasets - this should indicate that the prompts are challenging enough to provide decent signal on a language model's capabilities.
- Experimented with many SOTA models and compared with several baseline datasets.

Weaknesses:
The main weakness of this work is that much of the pipeline relies prompting language models to modify seed data. This means that the performance of the language model plays a huge role in the quality of the resulting data. Given that the pipeline seems to have many different steps, each of these steps can introduce errors since LLMs are not fully reliable. It then becomes crucial to have a way of verifying that the generated questions are of high quality. There's also a concern that the ground truth answers might not be entirely accurate. The authors mention both of these issues as limitations.

Limitations:
The authors mention the most-important limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a prompt synthesis framework for evaluating LLMs to accurately reflect different Large Language Model abilities. The authors develop two models to measure LLMs’ question discriminative power and difficulty. This study presents “instruction gradient” and “response gradient” methods to exploit rule sets to generalize questions.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper focuses on the generation of a large number of queries and corresponding answers on general language and mathematical topics. They have released a set of over 3000 questions for LLM evaluation. Their proposed metrics (discrimination index and difficulty score) show significant improvement in the quality of the benchmark datasets.

Weaknesses:
Although the paper tries to solve a crucial research area in the scope of LLM evaluation, the study lacks in many different ways. The textual flow is difficult to follow. Many of the concepts introduced were not properly described or not cited with previous work’s references. These issues restricted the reviewability of this study.

Limitations:
While this proposed method is understood to work on general text questions fairly well, mathematical questions are the weakest part of this study.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel framework for evaluating Large Language Models LLMs) based on Item Discrimination ID theory, which generates adaptive, high- quality prompts to effectively differentiate model performance. Key contributions include a dynamic evaluation set that evolves with LLM advancements, a self- correct mechanism for prompt precision, and models to estimate prompt discrimination and difficulty. The authors validate their framework by testing it on five state-of-the-art models and release a dataset of over 3,000 prompts to aid further research, demonstrating enhanced challenge and discrimination over previous methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper proposes a novel prompt generation method to produce more challenging evaluation data.
The paper is well-structured and clearly written. The methodology and evaluation criteria are explained clearly, making the paper accessible to a broad audience.

Weaknesses:
The paper only used one LLM Hunyuan) to generalize data and did not verify whether the proposed method can generalize to other LLMs.
It is debatable whether using test data generated by an LLM to evaluate the performance of LLMs has practical value. The paper lacks validation of the effectiveness of the machine-generated test set, such as comparing its metrics with those of other human-annotated datasets.
The paper lacks an analysis of the diversity of the data used to produce the test set.

Limitations:
The authors have identified some limitations; however, there are additional ones that I have raised in the weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zuwpeRkJNH;"REVIEW 
Summary:
The paper addresses challenges in surgical video-language pretraining (VLP) due to the knowledge domain gap and scarcity of multi-modal data. It proposes a hierarchical knowledge augmentation approach and the Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP) framework. This approach enhances data efficacy and tackles spatial-temporal challenges by combining language supervision with visual self-supervision. Extensive experiments demonstrate significant improvements in zero-shot transferring performance and the generalist visual representation for surgical scene understanding.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper presents a unique approach to surgical video-language pretraining by employing hierarchical knowledge augmentation using LLMs, significantly improving textual data quality and diversity. The PeskaVLP framework innovatively integrates visual and language supervision, addressing the spatial-temporal challenges in surgical scene understanding. The methodology is meticulously validated through extensive zero-shot and linear-probing evaluations on datasets such as Cholec80 and AutoLaparo, demonstrating substantial performance improvements. The clarity of the presentation, with well-organized sections and effective visual aids, facilitates comprehension. The significant contribution lies in enhancing surgical scene understanding and cross-modal retrieval, making it highly valuable for the NeurIPS community. The paper's originality in using hierarchical pretraining and the detailed discussion on model architectures and initialization underscore its quality and significance in advancing surgical data science.

Weaknesses:
Firstly, the dataset size is relatively small, with 1,007 videos for phase-level pretraining and 920 for video-level pretraining, which may limit the generalizability of the findings (as mentioned in the supplementary material). I know the difficulty in collecting medical data, but we must be sure that the presented approach can be generalized to different domains and hospitals. Furthermore, I doubt the methodology's potential to process ""noisy"" videos.    
Expanding the dataset and including more diverse surgical procedures would improve robustness. 

Secondly, while the paper mentions ASR errors in transcriptions, it does not provide a detailed methodology for handling them. Providing specific techniques for improving transcription accuracy would strengthen the study. 

Additionally, the practical implementation of the PeskaVLP framework in real-world surgical contexts is not thoroughly discussed. Detailing strategies for integration into clinical workflows and addressing potential technological barriers would be beneficial.

Limitations:
The authors have acknowledged the limitations related to dataset size and ASR errors but could elaborate on strategies to mitigate these issues. Specifically, they should discuss plans for expanding the dataset, incorporating more diverse samples, and improving transcription accuracy.

The positive societal impacts, such as enhancing surgical training and assistance, are well-discussed. However, the authors should address potential negative impacts, such as data privacy and ethical concerns. A detailed discussion on data security measures, user consent protocols, and ethical safeguards is needed.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel approach for enhancing surgical video analysis by incorporating procedural awareness. The authors propose a system that integrates knowledge of surgical procedures to improve the identification, segmentation, and annotation of surgical activities in video footage. This approach aims to address challenges such as the variability of surgical techniques and the complexity of visual data in operating rooms. The contributions of the paper include the development of a procedural model that can be aligned with video data, the creation of annotated datasets for training and evaluation, and the demonstration of improved performance over traditional video analysis methods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.The integration of procedural knowledge into surgical video analysis is a highly original concept. This approach not only enhances the accuracy of video analysis but also opens new avenues for improving surgical training and documentation.

2.Introduces a novel hierarchical knowledge augmentation technique using large language models to refine surgical concepts. Employs a Dynamic Time Warping-based loss function for effective cross-modal procedural alignment. Demonstrates significant improvements in zero-shot transfer performance across multiple surgical datasets. Provides a robust general visual representation beneficial for various surgical scene understanding tasks.
Weaknesses:

3.The potential applications of this research in surgical training, intraoperative assistance, and postoperative review are significant. The approach addresses a critical need in medical video analysis, making it highly relevant and impactful.

Weaknesses:
Dataset Limitations: The annotated datasets used for training and evaluation are crucial for the model's success. Expanding the diversity and volume of these datasets would enhance the generalizability of the findings.

Limitations:
The paper does not adequately address potential limitations and negative societal impacts.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP) method that enriches language supervision with LLM-refined surgical concepts. It further constructs hard negative samples by reversing the text orders at the phase and video levels and employs a Dynamic Time Warping (DTW) based loss to align multimodal procedures. Extensive experiments on multiple surgical procedures and comprehensive evaluations demonstrate the effectiveness of this framework.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is overall well-written, with the background and motivation well-stated.
- Using LLM to augment surgical video text descriptions is a good idea to enhance the quality of surgical text narration. It establishes a good baseline and guideline for future works that aim to apply LLM in surgical narratives.
- A more comprehensive parent-child level cross-modal correspondence was designed using DTW than existing works.
- Demonstration of the proposed method can close the representation gap for different modality, and analysed both successful and complicated examples.

Weaknesses:
- By reading the enriched dataset by LLM in Appendix H, I am concerning that the variation and diversity of narration will be removed by the augmentation. Will that cause any problems?
- In my opinion, using LLM to refine the text description of surgical videos is the most important contribution of this paper. It would be interesting to see if other components are also effective enough without the knowledge augmentation.

Limitations:
The authors adequately addressed the limitations. Since the proposed method is tailored for surgical data and applications, it is strongly suggested that the authors include a discussion on the potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new framework called PeskaVLP for surgical video-language pretraining. A hierarchical knowledge augmentation approach is used for enriching text information. The pretraining is implemented with the proposed language supervision and visual self-supervision. A new training objective is proposed for surgical procedural understanding. Extensive experiments are conducted to demonstrate the effectiveness on the surgical phase recognition task and cross-modal retrieval task on multiple downstream dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper addresses the problem of VLP in the surgical scene. A hierarchical knowledge augmentation is proposed to tackle the problem of lack of textual information in the surgical field.
2. The paper is generally well-written and easy to follow.

Weaknesses:
1. The explanation of method details is not clear enough, and there is a lack of discussion on some experimental results
2. The proposed method is based on certain assumptions but lacks a comprehensive consideration of applicability.

Limitations:
Authors discussed it briefly in the appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zuwLGhgxtQ;"REVIEW 
Summary:
The paper investigates the complexity of sampling from heavy-tailed distributions and presents a distinction between obtaining high-accuracy and low-accuracy guarantees. It analyzes two types of proximal samplers: those based on Gaussian oracles and those based on stable oracles. The main findings are that Gaussian oracle-based samplers can only achieve low-accuracy guarantees when sampling from heavy-tailed distributions, while stable oracle-based samplers can achieve high-accuracy guarantees. Additionally, the paper establishes lower bounds for samplers using the stable oracle, indicating that the presented upper bounds are optimal and cannot be fundamentally improved.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The problem is well-motivated and interesting. 
2. Designed the algorithms and derived the upper bounds and lower bounds for different settings. 
3. The authors also provided insightful discussion.
4. The authors provided solid theoretical proof for the results.

Weaknesses:
There is no experiment to verify the theoretical findings.

Limitations:
There is no experiment.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of heavy-tailed sampling. First, the paper shows that while the gaussian proximal samplers are efficient for light-tailed targets, they are not accurate for heavy-tailed ones; the paper develops a lower bounds for the Gaussian proximal samplers, which reveals a fundamental challenge in heavy-tailed settings.

Then, the paper proceeds to develop a novel samplers based on restricted alpha-stable oracle; the insight is to replace the standard heat equation in gaussian oracle with a fractional heat flow. The paper proves that under suitable conditions the proposed sampler is efficient for heavy-tailed targets. Additionally, the paper proposes a practical implementation for a particular case of alpha=1.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Novel theoretical analysis for the gaussian oracle sampler, which provides a new insight to developing sampling algorithms

- A novel methodology for heavy-tailed sampling

Weaknesses:
- The paper is purely theoretical and lacks experimental evaluation; it would be nice to at least have a toy illustration for the implementable algorithm 2+3 in the alpha=1 case.

- As the authors discussed in Sec5, the current paper does not present implementable algorithms for general alpha values in (0,2).

Limitations:
Most of the limitations have been touched upon in sec 5. Otherwise see the weakness comments.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper focus on studying the complexity of heavy-tailed sampling and present a separation result in terms of obtaining high-accuracy versus low-accuracy guarantees. Their results are presented for proximal samplers that are based on Gaussian versus stable oracles. Authors show that proximal samplers based on the Gaussian oracle have a fundamental barrier in that they necessarily achieve only low-accuracy guarantees when sampling from a class of heavy-tailed targets. In contrast, proximal samplers based on the stable oracle exhibit high-accuracy guarantees, thereby overcoming the aforementioned limitation. They also prove lower bounds for samplers under the stable oracle and show that our upper bounds cannot be fundamentally improved.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
Although I am not an expert in this field, I find this work quite interesting. The authors provide new material and support their statements with proofs.

Weaknesses:
The paper is not tested in any way on a numerical experiment. I am convinced that a paper presented at this type of conference should be both motivated by a real-world application and tested numerically, e.g., on a near-real-world formulation of the problem.

**After a rebuttal process**, the authors agreed with this weakness and promised to add the experiments to the final version of the paper.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors provide a lower bound for sampling from heavy tailed distributions under the Gaussian oracle of order $O(\textup{poly}(1/\varepsilon))$. They then propose an alternative proximal sampling algorithm using the $\alpha$-stable oracle that achieves a convergence rate of $O(\log(1/\varepsilon))$ for heavy-tailed distributions satisfying a fractional Poincare inequality. They then provide a practical implementation of the stable proximal sampler, and lower bounds on its convergence rate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This work presents a very nice combination of results showing a separation in the performance of stable and Gaussian proximal samplers. The combination of lower and upper bounds separating the two methods makes the work a particularly interesting contribution.

- The addition of a practical implementation of the stable proximal sampler is nice to have, demonstrating that it is viable in practice.

- The work is generally clearly presented and the authors are clear about their contributions.

- Overall, I consider this to be a very sound piece of theoretical work.

Weaknesses:
I have no major concerns about this paper. The presentation is somewhat dense in places, though this is mostly just a consequence of it being a very technical paper and not a flaw as such. If the authors want to make the claim that practicioners should use the stable proximal sampler in applied settings, then they may want to provide empirical evidence of its performance compared to the Gaussian proximal sampler. However, I understand that this is not the main purpose of this theoretical paper.

Limitations:
The authors provide an adequate discussion of the limitations of their methods in the final section, and I foresee no additional negative impacts of their work.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the complexity of sampling heavy-tailed distributions. It provides lower bounds on the complexity of Gaussian-based samplers for a class of heavy-tailed targets. Then, the paper constructs proximal samplers based on stable oracles, which improve the sampling complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This paper is well-written. The background of sampling and the research problems regarding sampling complexity are clearly introduced. The contributions of the lower bound on Gaussian-based samplers for heavy-tailed targets and the improved complexity using stable oracles are clearly presented.
* The paper is technically sound. The definitions and assumptions are discussed clearly, and the theoretical results are supported by proof sketches.

Weaknesses:
The contribution of the paper could be improved with empirical experiments to evaluate the sampling algorithms and their complexity.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ztwl4ubnXV;"REVIEW 
Summary:
The paper introduces ""AnonFair,"" a toolkit designed to enforce algorithmic fairness across various domains, including NLP, computer vision, and traditional tabular data. It is compatible with popular machine learning frameworks like sklearn, AutoGluon, and PyTorch. Unlike well-established fairness tools like FairLearn and AIF360, AnonFair extends to different types of data, including NLP and vision.

Other tools offer many methods but limited control over them, while AnonFair uses a single, highly customizable method that allows for per-group thresholding.

It specifically addresses the issue of overfitting by utilizing validation data, making it more reliable when traditional methods might fail.

Empirical evidence presented shows that AnonFair performs well, often matching or surpassing other methods in fairness benchmarks without being specifically optimized for complex or high-dimensional scenarios.

AnonFair seems to provide a robust and adaptable solution for implementing fairness in machine learning, in ways that other tools do not currently offer.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper does well in positioning AnonFair against competing tools by demonstrating its performance on standard fairness metrics and its versatility across a variety of use cases.
- AnonFair supports NLP and computer vision classification tasks, allowing broader applicability.
- The toolkit uses validation data to combat overfitting, ensuring that fairness measures remain robust across both training and unseen data.

- The toolkit not only competes well in terms of accuracy and fairness metrics but also offers significant advantages in computational efficiency.

Weaknesses:
- Some sections are overly detailed, such as the introduction, while others are missing necessary depth:
    - Section 3 could use a clearer structure, possibly with a diagram, to help readers understand how to interact with the toolkit.
    - The section on toolkit expressiveness needs more detailed examples and explanations of how the supported fairness measures are implemented. 
    - Results discussion is kept very brief and could benefit from specific numerical examples, like percentage improvements compared to other methods.m actual numbers, such as how much % improvement in comparison to method XY and such.

- The paper assumes readers are familiar with fairness terminology and metrics without adequate explanations or definitions for some acronyms (e.g., DEO in Table 3 and 4).
    - Subsection 4.3 lists supported fairness measures but fails to provide examples or brief explanations, making it less informative for those not familiar with these terms.

- Lack of consistency in terminology usage; for example, ""EOp"" in Figure 1 (top right) vs. ""EO"" in Section 5.2, “AnonFair” missing before ""Frontier"" in Figure 1 (left), and inconsistent references like ""See Figure"" vs. ""See fig..""

- A stronger call to action for community engagement, such as through open-source collaboration or empirical validation studies, could significantly enhance the broader impact and encourage more widespread adoption and refinement of AnonFair.

- The paper would benefit from a summary of explicit cases and recommendations advising users on the best scenarios for using the tool.

- Figure 2 is not referred to in the paper, or did I miss this part.

Limitations:
Some of the limitations are acknowledged, but could be expanded with more actionable insights. 
A call to action for community engagement, such as through open-source collaboration would also encourage broader impact and adoption of AnonFair against its competitors. 
It would be beneficial if the authors suggested potential improvements or future research directions for the suboptimal fairness metrics and data scarcity issues mentioned.
The broader impact section identifies ethical concerns well. However, detailing the intended applications and scenarios where AnonFair might be most effective, or where it could fail, would provide readers and users with clearer guidance on its practical use and limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper describes a new toolkit for algorithmic fairness, enabling the optimization of any fairness measure that is a function of the confusion matrix. Experiments on vision and NLP demonstrated the effectiveness of the proposed toolkit.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
An easy-to-use toolkit for enforcing algorithmic fairness.

Weaknesses:
Presentation could be made more self-contained, e.g. a table listing the supported fairness metrics, as functions of the confusion matrix. This would help readers not familiar with the field.

It seems that only binary classification is supported. How can such metrics be extended to other tasks?

Some minimal code snippets for the interface could be shown as examples.

Limitations:
The authors adequately discussed the limitations of their toolkit.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new toolkit designed to enhance algorithmic fairness with greater expressiveness. Unlike existing toolkits, this one offers more customization options to optimize user-defined objectives and fairness constraints. Although the proposed toolkit currently includes only one method, it supports both computer vision and natural language processing (NLP) tasks. The authors compare the efficiency of this method, finding that the toolkit is relatively more efficient than Fairlearn. Comprehensive experiments were conducted on various datasets, and the results were compared with those from other popular toolkits.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces a versatile toolkit that supports both NLP and computer vision tasks, unlike existing toolkits which lack this capability.
- The proposed toolkit employs efficient optimization techniques that accelerate the evaluation process.

Weaknesses:
- The formulation presented in Subsection 4.2 of the paper is limited to a single-layer model, which restricts its applicability across different machine learning models. To enhance the flexibility of the method, I recommend adopting a more generic notation, particularly if we aim to incorporate pretrained language models.
- The abstract is quite unclear, especially the part that mentions ""9/9 and 10/10 of the group metrics of two popular review papers."" I suggest rephrasing the abstract for better clarity and comprehension.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper describes details of a fairness toolkit (""AnonFair""), which confers fairness to any given machine learning classifier by exploring a wide range of prediction thresholds for different groups (which are either provided upfront or inferred through an auxiliary classifier). The toolkit is designed to be quite expressive, as it can optimize several different metrics, e.g., false positives/negatives, true positives, etc. The toolkit can work across all classifiers (which can output class probabilities), including ones trained on vision and NLP tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper introduces and describes a toolkit that implements several fairness strategies and can support any fairness measure that can be expressed in terms of true positives, false positives, true negatives and false negatives. These techniques primarily rest upon adjusting the classification thresholds of different groups, and the paper also incorporates tricks to speed up their computations of precision and recall across different thresholds. The fairness techniques that this paper implements are (largely) classifier agnostic, and can be applied to a wide range of classifiers including NLP and vision classifiers (as this paper shows). Overall, I appreciate that expressivity and broad applicability of their toolkit.

Weaknesses:
While the toolkit might turn out to be useful for some practitioners, it is a relatively straightforward implementation of well-known (and simple) technique of adjusting prediction thresholds across groups. Exploring different thresholds can be computationally prohibitive, for which the authors use a standard trick to speed up their explorations (which I appreciate). The paper acknowledges and cites relevant papers/techniques that they implement.   Overall, the originality and novelty of their work is significantly limited, as the toolkit is an implementation of known and simple fairness techniques. Further, the underlying fairness techniques (not from the authors) are themselves applicable to most classifiers, so any implementation of the same could work for NLP and vision tasks—which is claimed to be one of the major contributions of this work.

Limitations:
I believe the paper adequately communicates their shortcomings and cites past references when using them. However, I think it might help to also acknowledge that the underlying fairness techniques broadly apply to a wide range of classifiers, and naturally extend to classifiers in computer vision and NLP domains. Reading parts of the paper felt like that there are significant challenges in adoption of fairness techniques to NLP and CV, and this paper overcomes them through novel solutions—which is not the case.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents AnonFair, a cutting-edge open-source toolkit designed to promote algorithmic fairness. Authors claim the following contributions:
(1) Comprehensive support for NLP and Computer Vision classification, as well as standard tabular problems.
(2) Enhanced robustness against overfitting challenges through the ability to enforce fairness on validation data.
(3) Versatility in optimizing any measure that is a function of True Positives, False Positives, False Negatives, and True Negatives, making it easily adaptable and more expressive than other toolkits.
(4) Seamless integration with popular ML toolkits such as sklearn, Autogluon, and pytorch.
(5) AnonFair supports 9/9 and 10/10 of the group metrics of two prominent review papers and is accessible online at no cost.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This toolkit progresses in algorithmic fairness and enhances multidisciplinary collaborations, it is design to integrate the intervention of policy-makers.

The paper includes a complete section of experiments and comparison with existing toolkits. 

AnonFair key contributions include support to popular and relevant NLP and Computer vision areas.

Weaknesses:
* Lack of clarity in some reported experiments, e.g. results tables are not cited in the text, metrics are not well-contextualized (e.g. larger or lower scores are better?)

* Lack of analysis, examples or human evaluation to better understand contributions and limitations of the method in each of the experiments.

Limitations:
Authors report some limitations, but further analysis on the experiments could raise more limitations that may be currently ignored.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zb8jLAh2VN;"REVIEW 
Summary:
This paper develops a switching RNN (SRNN) framework to model neural activity. It builds up on switching linear dynamical system models that are used in neuroscience to segment and extract underlying dynamics of observed neural activity. The different segments corresponding to unique dynamics often reflect distinct behavioral states. The crucial novelty of this work is that they allow the dynamics to be non-linear, unlike SLDS and rSLDS, making the model more expressive. They fit these models using VI using an inference network. Finally, they apply SRNN to synthetic data, as well as 3 distinct neural datasets and show that it outperforms SLDS and rSLDS on segmenting activity into behavioral modules where each module corresponds to distinct dynamics. They visualize these underlying dynamics, and also evaluate their fitted model on predicting future neural activity.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. As we move towards large-scale neural datasets, it is crucial to scale model complexity in order to fully harness these datasets. This paper makes a step in that direction by allowing for non-linear dynamics, while also providing an appropriate fitting approach.
2. The experiment section is extensive, and I appreciate the application to multiple neural datasets. I particularly found the results on the decision-making dataset to be most impressive. 
3. The literature review is thorough, and the authors do a good job of situating their work in the context of other related studies.

Weaknesses:
1. The authors mention switching nonlinear dynamical systems (Dong et al. 2020), and discuss how their work differs from Dong et al. I think it is important to either provide an experimental comparison to SNLDS or a justification for why these existing models are insufficient to explain neural datasets, as the main novelty/motivation for SRNN and SNLDS is very much related (also noted by the authors in the paper). More on this in the  question section.

2. Behavioral segmentations are somewhat subjective in nature, and while I can see that in the experiments shown here they make sense, in a real world setup we may want to infer the number of such segmentations from the data. Here the authors set the number of discrete states to the # of true behavioral states, however this might not be known in practice. Furthermore, there might be distinct sets of dynamics within one behavioral state due to other reasons not totally explicit from behavior. From the current set of results, it is not clear if SRNN is capable of inferring the # of underlying states. I will elaborate more in the questions section on this as well. 

3. I also think the paper will benefit from some editing by the authors. The references are not formatted properly, commas are missing. The referencing to supplementary figures doesn't seem to be working, it links back to figures in the main text. I also think the authors can trim some of the background, such as the section on VI, in favor of explaining some of the experiments such as the Lorenz attractor setup in more detail.

4. While I appreciate the extensive experiments, I find it hard to reconcile some of the results. It seems like in some of the plots (Fig 3C/D, Fig 5C/D) prediction + reconstruction performance across all models is similar. However, the discrete states being inferred look hugely inaccurate for SLDS and rSLDS. I wonder if the authors have thoughts on why this happens.

Limitations:
The authors have addressed limitations in the last section of the paper, and I do not envision any societal impact of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a new class of probabilistic nonlinear state space models called switching RNNs. In essence, this extends the well-known switching linear dynamical system (SLDS) model to switch between nonlinear dynamics governed by a stochastic RNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The results shown in panels A of Figs 3, 4, and 5 are nice and convincing.

Weaknesses:
* Like many other deep learning based approaches, the model is not particularly interpretable. For example, panel F in Figs 3, 4, and 5 shows 2D flow fields for the different hidden states, but the RNN hidden state is 16-dimensional. Here the authors have used PCA to attempt to find a reasonable 2D flow field, but I know from experience that this has the potential to very poorly capture the true dynamics of the system. Intuitively, even small variance dimensions can matter a lot if the flow field changes rapidly along that dimension.

* There are many tunable parameters in this model (e.g. number of continuous and number of discrete states). It is unclear how to choose these on datasets without ground truth, or at least good educated guesses.

* Related to above, I worry a lot about the identifiability of this model. A nonlinear RNN without discrete switching can already model any flow field if given enough units. Thus a model with many continuous states (e.g. $P=128$) but zero discrete states may perform equally well to a model with few continuous states (e.g. $P=16$ or $P=8$) but a handful of discrete states. How would one then go about choosing between these models? Adding discussion or ideally some sort of mathematical analysis regarding the statistical identifiability of the model would be very helpful.

Limitations:
The discussion adequately acknowledges limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose to model time series neural population activity using switching recurrent neural networks. The generative model includes discrete latent states

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed method does appear to outperform related switching linear dynamical systems approaches in certain contexts.

Weaknesses:
High-level:
- The contribution beyond other switching nonlinear dynamical systems models is not clear. Such models include the cited Dong et al., 2020, as well as Karniol-Tambour et al., ICLR 2024. If there is a contribution beyond these works, the authors should compare against those existing related methods.
- The authors do not demonstrate an ability to automatically determine the appropriate number of discrete states. One approach to this might be ""co-smoothing"" (see Yu et al., Gaussian Process Factor Analysis, 2009).

Details:
- The mathematical details and notation are often unclear. For example, equation 2 does not appear to be a valid probability distribution, given the description that f(.) = tanh(.). Shouldn't this instead be a categorical distribution or similar? Relatedly, f is also used in equation 8, but from the context it appears to denote something entirely different.
- The authors should more clearly describe the cross-validation techniques for used for each dataset. The blanket statement in the intro to Section 4 (""On each dataset, we do N-fold cross-validation, where N equals to the number of conditions, sessions, or subjects in the dataset"") obscures how cross-validation was actually applied in each instance.

Limitations:
The authors address several limitations, including their need to manually set the number of discrete states, their need for good parameter initializations, and the heavy computational requirements for fitting their models.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes switching recurrent neural networks (SRNN), which allow the RNN weights to switch across time. The RNN weights switch based on a latent Markovian process of discrete states. The authors apply SRNN to a simulated dataset following the Lorenz attractor and three real-world neural recordings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Clarity: The authors clearly explain the problem, related work, and methodology with well-written equations and easy-to-understand figures. 

- Extensive use of datasets: The paper applies SRNN to numerous real-world neural datasets, illustrating the effectiveness of SRNN in accurately segmenting different datasets in an unsupervised fashion.

Weaknesses:
- Lack of comparison with other methods:
The paper compares SRNN to (r)SLDS models. However, there exist many other models for unsupervised segmentation. For example, ARHMMs and their extensions are simple yet powerful and interpretable models for segmentation [1, 2]. The authors should cite and consider comparisons with multiple model classes.
In addition, the paper notes in line 103 that SRNNs have the most comparable structure to SNLDS, but the authors do not make comparisons. The authors should also cite and compare with [3], which has switching nonlinear dynamics.

[1] Wiltschko, A. B., Johnson, M. J., Iurilli, G., Peterson, R. E., Katon, J. M., Pashkovski, S. L., ... & Datta, S. R. (2015). Mapping sub-second structure in mouse behavior. Neuron, 88(6), 1121-1135.

[2] Lee, H. D., Warrington, A., Glaser, J., & Linderman, S. (2023). Switching autoregressive low-rank tensor models. Advances in Neural Information Processing Systems, 36, 57976-58010.

[3] Karniol-Tambour, O., Zoltowski, D. M., Diamanti, E. M., Pinto, L., Tank, D. W., Brody, C. D., & Pillow, J. W. (2022). Modeling communication and switching nonlinear dynamics in multi-region neural activity. bioRxiv, 2022-09.

- Experiments:
The simulated experiment with the Lorenz attractor shows that SRNN does well when it has access to noiseless observations with known state dimensions. In order to have a more convincing simulated experiment, the authors could consider the following. First project the Lorenz attractor to a higher dimensional space and add additive Gaussian noise. Then fit SRNN (and other compared models) to the dataset to see if it can recover the Lorenz attractor and true latent state dimension (using some metric on held-out data). Another simulated experiment could be done with a dataset that simulates the NASCAR track [1,2].

[1] Linderman, S. W., Miller, A. C., Adams, R. P., Blei, D. M., Paninski, L., & Johnson, M. J. (2016). Recurrent switching linear dynamical systems. arXiv preprint arXiv:1610.08466.

[2] Lee, H. D., Warrington, A., Glaser, J., & Linderman, S. (2023). Switching autoregressive low-rank tensor models. Advances in Neural Information Processing Systems, 36, 57976-58010.

Limitations:
As the authors noted, some limitations of the model are that the model needs good initialization and that the model takes considerably more amount of time to train than simpler models such as SLDSs.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
KrHFICMPjm;"REVIEW 
Summary:
The paper introduce GUIDE, a RLHF framework for real-time RLHF with online and continous human feedback. GUIDE translates the human feedback to dense reward. Addtionally, GUIDE includes a parallel training model that learns a simulated human feedback. By involving 50 participants annotation, GUIDE solves three typical challenging sparse reward environments.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper is easy to read.
2. GUIDE firstly proposed novel continous human feedback and is also evaluated human annotators.
3. GUIDE demonstrated improvement compared to baselines in 3 environments, and analyzed cognitive tests and analyses.

Weaknesses:
1. My biggest concern comes from the practicality of GUIDE. From both theoretical and experimental perspectives, I find it hard to believe that such a simple continuous feedback model can be applied to real-world scenarios. For example, the paper states in line 38 that ""Current research has demonstrated success primarily in simple, low-dimensional tasks with limited solution spaces."" However, the experiments conducted in the paper also involve environments where baseline algorithms like DDPG or SAC can converge with a good reward function after only about **10 minutes of training**. Moreover, according to the experimental results, GUIDE, which incurs a high cost of human feedback, does not outperform manually designed simple rewards (such as the distance to the target, I think it is not hard to design it). Therefore, despite the fact that the environments used do have continuous actions and image inputs, I believe these environments are not suitable for validating RLHF algorithms because the reward functions are easy to design and the tasks themselves are simple.
2. The core argument of the paper is that continuous real-time feedback is extremely difficult to implement in practice. It requires annotators to constantly provide scalar rewards without pause, and such absolute value annotations are more susceptible to biases from different individuals. Pair-wise annotation is much easier than absolute value annotation and can be conducted asynchronously with the training process. If an AI agent needs to be trained for several days, the cost will be unacceptable.
3. Although the paper suggests using model predictions to synthesize feedback, such a simple supervised learning regression objective is unlikely to accurately model the complex reward distribution. My reasoning is that predicting the relative goodness of A and B is easier than predicting scalar reward values, but there will still be many prediction biases.
4. The definitions of various symbols in the paper are imprecise and confusing, for example:
- What is the meaning of A(s, a) in Equation 1? Also, A(s) = a in the line 123, is it the same?
- difference of Q(s, a) and q?
- How to get the r^hf?
These typos make it very difficult to understand the details of the paper.
5. There is a lack of discussion on recent related works in RLHF, such as:
- [1] White D, Wu M, Novoseller E, et al. Rating-based reinforcement learning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(9): 10207-10215.  
- [2] Yuan Y, Hao J, Ma Y, et al. Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback[J]. ICLR2024.
- [3] Guan L, Verma M, Guo S S, et al. Widening the pipeline in human-guided reinforcement learning with explanation and context-aware data augmentation[J]. Advances in Neural Information Processing Systems, 2021, 34: 21885-21897.
- [4] Guan L, Valmeekam K, Kambhampati S. Relative behavioral attributes: Filling the gap between symbolic goal specification and reward learning from human preferences[J]. ICLR2023.

Limitations:
Yes, and yes.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new approach to reinforcement learning with human feedback in simple video games. The method relies on continuous human feedback that is provided by the human observer hovering their mouse over a window with a spectrum of positive and negative rewards. Unlike prior approaches, this method converts human feedback directly into a reinforcement learning reward with an added time delay. Moreover, the method includes a model that regresses states into observed human feedback, which allows for simulated human feedback. The effectiveness of the method is demonstrated in three simple games in a human study with 50 participants.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The authors propose a simple way to incorporate continuous human feedback as a reinforcement learning reward with a constant time delay. The environment reward and human feedback are simply added to form the final reward function.

2. It is demonstrated that human preference can be directly regressed from states and actions to provide simulated human feedback.

3. The authors perform an extensive human study showing the effectiveness of their method. They also correlate the subject’s performance in a cognitive test with their ability to guide an RL agent.

Weaknesses:
There are three major unstated assumptions:

1. The delay between an event appearing on the screen and the change in human feedback is constant (Question 1). The authors tune this constant for each environment. But, more complex environments might induce different delays as the human observer might need to think about what they saw.

2. People are able to provide constant feedback (Question 2). This might not be true for more complex environments where certain states might have ambiguous values.

3. The human feedback is Markov (Question 3). This might not be true in more complex games.

## Detailed comments:

* Equation 1 should have Q instead of A. Unless you want to define an advantage function A.
* Equation 2 should have an upper-case Q.
* The term $R_{t+k+1}$ in Equation 2 is not very clear.
* The meaning of “We follow recent advancements in neural architectures and hyperparameter designs” on line 125 is not clear.
* The rest of the paragraph on lines 125 - 127 is superfluous.

## Minor comments:

* Inconsistent spacing between text and citations.
* Calling this approach a “computational framework” might be a bit redundant given the context of the conference.

Limitations:
Limitations are partially addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new framework for human-in-the-loop reinforcement learning, where the human and provide real-time and continuous feedback, and an algorithm where the learning agents uses the human feedback to accelerate policy learning. The paper conducted a user study of 50 subjects to demonstrate the effectiveness of the proposed framework in accelerating policy learning and improving success rates over RL and human-in-the-loop RL baselines. Optionally, a human feedback simulator can also be trained to mimic human feedback after a certain amount of time, reducing the amount of human input.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper proposes human-in-the-loop RL where the human can provide real-time continuous feedback, which is a novel paradigm compared to mainstream existing work which focus on discrete feedback signals.
- This work conducted a user study of 50 subjects, which is the largest among relevant works. This is a great contribution in assessing the effectiveness of human-in-the-loop RL.
- The evaluation is done on three challenging tasks, and GUIDE outperform all the baselines by a large margin on the ""find treasure"" and ""high and seek"" tasks.
- The paper provides a detailed individual difference characterization by conducting a series of human cognitive tests. Analysis of the human cognitive test data provides meaningful insights. These data can also be very useful in future work.

Weaknesses:
- The baselines are generally quite weak. Based on the experiment results, it is unclear whether real-time continuous feedback is necessarily the best way for humans to guide the policy learning. There might be intermediate points on the spectrum of conventional discrete feedback and full continuous feedback that provides the best tradeoff between amount of human input and effectiveness of guiding policy learning.
- Whether the simulated human feedback is helpful is unclear. In both the ""bowling"" the ""find treasure"" task, the score does not increase much after switching to simulated feedback. It might be the case that the simulated human feedback only works for tasks where it is straightforward to model the reward.

Limitations:
The paper focused on the final success rate of policy learning, but did not provide sufficient data from the user's perspective. For example, the user study failed to include a survey regarding whether the real-time feedback system feels easier to use than the discrete feedback system.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new framework, GUIDE, for learning from continuous human feedback in complex decision making domains with continuous action spaces. By framing human feedback as a state-action value function, the framework proposes to learn this function and to combine it additively with the (generally sparse) reward coming from the environment. The feedback is collected in a continuous fashion by asking participants to move their mouse up or down to indicate higher or lower feedback values. In a user study, the paper finds that training agents with this type of feedback yields better performing agents than baselines. After assessing participants in a suite of cognitive tests, it finds that participants that score higher on the cognitive tests trained better agents.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper introduces an interesting new way of collecting continuous human feedback, and shows significant improvement in two out of three tasks considered. The use of cognitive tests as part of the user study is interesting, and uncovers insightful correlation between subject performance and their cognitive test scores.

Weaknesses:
The assumptions regarding what human feedback represents do not seem consistent between section 3 and 4 (see Questions). Further, the treatment of the feedback collection is rather simple (added to environment reward function) and, especially if it does represent a signal regarding the future value of state-action pairs, heuristic. Relative to Tamer and Deep Tamer, which treated human feedback more consistently by using it directly as a proper state-action value function, this paper feels like a regression on that front. 

The implementation of the c-DeepTamer baseline raise a number of questions (see Questions), which shake my confidence in it as a baseline, or as a proper representative for how well Deep Tamer should perform here.

Limitations:
Limitations are discussed in the conclusion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
FCsEvaMorw;"REVIEW 
Summary:
The paper addresses automatic red teaming of large language models through open-ended generation of jailbreaks. The key components of their methodology are 1) a categorization of different jailbreak categories to create a diverse archive of possible jailbreaks, 2) a strategy to evolve and mutate jailbreaks, 3) and a selection process to keep the jailbreaks with the highest quality.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper presents a considerable contribution towards practical automatic redteaming of large language models and address multiple weaknesses of prior work (such as attack diversity). The authors provide an exhaustive empirical evaluation of the proposed method with extensive hyperparameter descriptions.

Weaknesses:
W1: Despite the extensive lists of hyperparameters and explanations, the results are not realistically reproducible without the dataset of generated jailbreaks or the trained model. As adversarial robustness has shown to be brittle in the past, I strongly recommend any safety paper to make it as easy as possible to verify their defense. A considerable amount of powerful open-source large language models is available and there is no particular reason why releasing the trained robust model would provide any additional safety concern to the community. (Note that this is more of a personal concern and will not influence my score as I understand that it might have not been possible for the authors to release the model / code / data)

W2: The robustness evaluation of the the model trained with rainbow teaming data is insufficient. I would argue that safety assessments can never be “fixed” and need to be adaptive and designed for the model at hand. The authors performed a train test split to evaluate the robustness of the model, which is non-adaptive. At least any evaluation with one of the many adversarial attacks proposed for LLMs in the last year would have put the robustness into better perspective. Most defenses proposed in the robustness domain have later been shown to be ineffective and “offline” adversarial training (generating the attacks prior to training) does not yield robustness for image models against stronger attacks in my own experiments. Thus, I am a bit sceptic if rainbow teaming actually improves worst-case adversarial robustness.

W3: Evaluations are limited to the Llama series of models. Experiments on non-aligned models or models trained with less safety fine-tuning would have been interesting. E.g., ""How does rainbow-teaming compare to standard model alignment?"". (relatively minor concern)

I am very likely to raise my score if the authors provide more results regarding the worst-case adversarial robustness of models trained with rainbow teaming or provide a good reason why this is not necessary / out of scope.

Limitations:
The authors provide an extensive list of limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces RAINBOW TEAMING, an approach for generating diverse adversarial prompts to test and improve the robustness of LLMs) The method uses quality-diversity search to produce a wide range of effective adversarial prompts across different categories. The authors demonstrate its effectiveness on state-of-the-art models like Llama 2 and Llama 3.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
RAINBOW TEAMING offers a new perspective on adversarial prompt generation by framing it as a quality-diversity problem.

Weaknesses:
1. The study focuses primarily on Llama 2 and Llama 3 models, citing licensing constraints for not including other major models like GPT-4 or Claude. This focus limits the generalizability of the findings. It would have been more convincing to see results across a wider range of models from different providers, especially given the importance of the topic. 

2. While the authors report high inter-evaluator agreement between GPT-4, Llama Guard, and humans on a small sample, the study relies heavily on automated metrics for evaluating the safety of responses. 

3. While the paper mentions that fine-tuning with RAINBOW TEAMING-generated data improves model robustness, it lacks a detailed analysis of potential effects on the model's general performance or capabilities.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present Rainbow Teaming, a structured approach to automated redteaming of large language models. Based on a user-specified set of strategies and risk categories, Rainbow Teaming uses a mutator LLM to rewrite several variations of existing prompts, then compares the resulting outputs from the target model with a judge LLM against the existing prompt. If a more effective prompt is found, it replaces the existing prompt in the ""archive"" of prompts found so far. The authors conduct experiments redteaming various open-weight models such as Llama and Mistral, showing that their method achieves a high success rate on various risk categories. They also explore the use of the generated prompts in supervised finetuning, showing that robustness to Rainbow Teaming can be improved by training against Rainbow Teaming.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
* Exposition of proposed method is very clear and effective
* Lots of helpful figures and diagrams to illustrate the various components of the entire pipeline, such as the concept of the ""archive""
* Method appears quite effective at least against smaller, open-weight models

Weaknesses:
* Contribution and novelty seems very marginal. The difference from methods such as PAIR and TAP appears to come down to 1) presenting the attack/mutate LLM with high level categories instead of specific behaviors and 2) specifying several concrete strategies instead of relying on the attack/mutate LLM to come up with them on the spot
* Lack of comparisons to prior work. The authors offer various criticisms of PAIR, TAP, and the approaches studied by Perez et al. but do not show any evidence that their method outperforms or finds substantively different prompts from these approaches. Table 1 presents some results which I do not understand. Evaluations on common benchmarks such as AdvBench and HarmBench are missing.
* Lack of experiments on bigger models. The authors only run Rainbow Teaming against 7B models. They claim they are unable to evaluate against more powerful models such as GPT and Claude because of legal constraints, but there are plenty of larger and more powerful models for which this is not a concern, such as Llama-3 70B and many others.
* Fig 4 shows that Rainbow Teaming only improves performance by about 10% beyond the simple baseline of sampling lots of candidates from scratch, suggesting that the whole evolutionary framing of elites, mutations, etc is not so critical.
* The experiments on improving robustness with training on Rainbow Teaming prompts in Sec. 5 are not convincing. The authors generate 15 sets of prompts targeted against Llama-2 7B, train on 12, and then show near-perfect performance on the held out 3 sets. How different are these 3 sets from the 12 training sets if they are generated with the same algorithm? But when running the Rainbow Teaming pipeline against the fine-tuned model, they still find a nearly 40% attack success rate which is not robust at all. And this is in spite of the fact that they do not appear to be using any holdout behaviors for validation.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel method Rainbow Teaming for the automatic generation of diverse adversarial prompts aimed at large language models (LLMs). The goal is to identify and enhance the robustness of LLMs to various user inputs, which is crucial as these models are increasingly used in safety-critical environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method holds significant importance in the current AI red teaming study for large language models (LLMs).

2. The proposed automatic method is straightforward to follow and appears to be effective on different open-source models.

Weaknesses:
Lack of baseline comparisons in the safety evaluation for LLMs.

Limitations:
I'm not sure if the authors plan to release their code or model checkpoint to facilitate further advancements in this field. Automatic red teaming is indeed crucial, but some methods can be challenging to reproduce since they often involve large amount of engineering work, which somehow is more likely an engineering work rather than a research study.

Furthermore, while this paper primarily focuses on diverse risks, it would be beneficial to include a comparison with existing red teaming approaches, as they can also be utilized to evaluate and enhance the safety of LLMs.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
I'd like to thank the authors for submitting their work for review. I found the work insightful, inspiring, and high-quality. In short, the work has been a pleasure to review as a last-minute reviewer. 

The manuscript's primary contributions include:
- **Rainbow Teaming Method.**  A new methodology for automatically generating adversarial prompts through the lens of a quality-diversity problem.
- **Demonstrative Evaluation for Safety.** A demonstrative evaluation of the Rainbow Teaming methodology's utility application to the task of identifying prompt vulnerabilities that exist in a series of generative models. This demonstration also validates the underlying components of the Rainbow Teaming approach (e.g., the choice and design of the Preference Model).
- **Fine-Tuning Evaluation for Safety.** A fine-tuning experiment that illustrates how fine-tuning Llama-2-chat 7B on a dataset of 1,500 synthetically generated adversarial prompts with SFT reduced the attack success rate from 92% / 95% to 0.3% / 0.7%. 
- **Post-SFT Evaluation for Safety.** The authors re-apply Rainbow Teaming to the fine-tuned model produced from the Fine-Tuning Evaluation and report that the model is ""substantially more robust to our approach, with a final ASR of 39% (down from 92%"".
- **Non-Safety Evaluations**. The authors contribute two additional experiments that illustrate the method's efficacy for the Question-Answering and Cybersecurity settings, each of which contribute a set of concise, abbreviated findings in their own right.

The manuscript has several other minor contributions that aren't explicitly referenced as contributions, but should not go unnoticed (e.g., the extended taxonomy of safety risk categories that was previously defined by Inan et al.)

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Generally speaking, I find the work to be strong in its contribution, and I have no issue in acknowledging the paper's strengths -- because there are many!

### 1. Originality
* Rainbow Teaming can be categorized as a synthetic data generation method for adversarial settings. Synthetic data generation methods that are similar in nature (e.g. PAIR, MAP-Elites) are recognized by the authors. 
* Despite bearing similarity in the fundamental approach, there are certainly aspects of originality that enable the method to distinguish itself from those that come before it.
* Irrelevant of the methodology's originality, it can be argued that aspects of the evaluations are themselves original. 

### 2.Quality
* I view the quality of the work is high, and the conducted experiments sufficiently support the recognized the work as such. 
* The manuscript experiments related to safety progressively build on one another, providing incremental validation at for each of the steps that educated readers might expect to see when replicating the methodology on their own. The experiments use appropriate metrics and are accompanied with conclusive statements that are, for the most part, reasonable and believable.

### 3. Clarity
* Given page limit requirements, I find the paper's presentation to be exceptional. The writing is crisp, clear, and to the point. 
* The authors have given clear time and attention to ensuring their work is digestible to readers. 
* The figures are also well-designed and make it quite easy to understand how the Rainbow Teaming aims to provide a more holistic evaluation of safety. One could argue that Figure 1's visual representation may be appropriate for adoption as model providers continue to champion safety as an area of investment.

### 4. Significance
* I view the work as an amalgamation of several existing concepts that, when stitched together as a collective, can be viewed as a significant contribution.
* Rainbow Teaming's applicability to safety is clear and obvious, and the secondary evaluations on Question-Answering and Cybersecurity elevate the work's significance.
* It's easy to imagine a generalization of the Rainbow Teaming methodology being applied to settings that aren't adversarial in their nature.

The Appendix should be recognized as a strength in itself. The quality of depth and thoroughness is appreciated, even if some sections may not be as open as I'd like.

Weaknesses:
The work has several weaknesses that should be taken seriously, but not viewed as disqualifying. I view each of the following weaknesses as nothing more than ""expected"".

The weaknesses are as follows:

1. **Longitudinal Practicality.** The authors make a number of claims about the Rainbow Teaming method's ability to improve the robustness of generative models. While this is clearly demonstrated in the manuscript's family of experiments, the claim is weakened by the notion that the experiments do not provide information about how the Rainbow Teaming method may operate over time (i.e., in which adversarial methods evolve in new and unexpected ways). 

2. **Attack Styles and Risk Categories.** The paper's contributions are bound by a static set of attack styles and risk categories. It remains unclear if the methodology would perform similarly with other styles or categories.

3. **Minor Weaknesses.** There are two minor weaknesses:
 - **Model Choice.** Conducted experiments are performed with models that are now viewed as potentially being dated (i.e., and are no longer state-of-the-art). This weakness is stated out of recognition that the model choice itself is a *potential* weakness. Regardless of whether this be recognized more formally as a weakness, I strongly believe that reviewers refrain from scrutinizing the choice of models as the work simply uses them as a vehicle for demonstrating their methodology.
- **Diversity Metrics.** Diversity is primarily measured via BLEU, which one flavor of measurable diversity. Common practice is increasingly gravitating toward the measurement of multiple metrics that are reported as a collective, e.g. https://arxiv.org/html/2403.00553v1.

Limitations:
The authors acknowledge several key limitations of the Rainbow Teaming approach in Appendix A. Generally speaking, they are sufficient, but are not comprehensive or clear as I note in describing weaknesses and questions.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zsXbGJJ7Oo;"REVIEW 
Summary:
This paper proposes G2D, a novel vision-language pre-training (VLP) framework for medical imaging that aims to learn both global and dense visual representations from radiography images and their associated radiology reports. The key innovation is a pretext task called Pseudo Segmentation (PS), which uses a pseudo mask derived from attention maps to guide the learning of dense visual features during pre-training. The authors demonstrate that G2D outperforms existing medical VLP approaches on various downstream tasks including classification, segmentation, object detection, and zero-shot visual grounding across multiple medical imaging datasets. Notably, G2D shows strong performance on segmentation tasks even when fine-tuned on very limited data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novel approach: The paper introduces an innovative method for learning dense visual representations in medical VLP without requiring pixel-level annotations, addressing a key limitation of existing approaches.

Well-motivated: The authors provide a clear rationale for why learning dense representations is important for medical imaging tasks and why existing VLP methods struggle with this.

Comprehensive evaluation: The method is evaluated on a wide range of downstream tasks and datasets, demonstrating its versatility and effectiveness across different medical imaging applications.

Strong results: G2D consistently outperforms existing methods, especially on segmentation tasks where it achieves impressive results with very limited fine-tuning data.

Ablation studies: The paper includes thorough ablation experiments to validate key design choices and components of the method.

Potential impact: The proposed approach could significantly reduce the need for large annotated datasets in medical imaging, which is a major bottleneck in the field.

Weaknesses:
Limited theoretical analysis: While the method is empirically strong, there is little theoretical justification for why the pseudo segmentation task leads to improved dense representations.

Complexity of the approach: The method involves several components and processing steps, which may make it challenging to implement and potentially limit its adoption.

Computational resources: The pre-training process appears to be computationally intensive (16 A100 GPUs), which could be a barrier for researchers with limited resources.

Generalization to other domains: While the focus on medical imaging is valuable, it's unclear how well this approach would generalize to other vision-language domains.

Comparison to more recent baselines: Some of the baselines used for comparison (e.g., ConVIRT, GLoRIA) are somewhat older.

Comparison to more recent medical VLP methods would strengthen the evaluation.

Limitations:
The authors provide a brief discussion of limitations in the appendix, acknowledging potential issues with the weak supervision signal from pseudo masks and the need for further research on regional visual representations. They also touch on broader impacts, mentioning both potential benefits for healthcare and risks associated with sensitive medical data. While these discussions are valuable, they could be expanded to provide more specific insights into the limitations of the current approach and potential mitigation strategies for the identified risks.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript describes a medical vision-language pre-training framework called Global to Dense level representation learning (G2D), that learns global and dense visual features simultaneously with only image-text pairs, by exploiting the aggregated attention map from the vision encoder for a pseudo segmentation pretext task. The improved (frozen) vision encoder is then utilized as part of the model pipeline for a number of downstream tasks (e.g. segmentation, classification)

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Pseudo segmentation pretext task enables dense segmentation during pre-training, and avoids external resources as for alignment-based methods, and limitations on high-level semantic representations in reconstruction-based methods
 - Importance of associating semantic meaning verified via experiment

Weaknesses:
- Unclear if specific sentence/phrase to individual image region alignment is achieved, for dense learning
 - Lack of fine-grained pixel-level evaluation of masks

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes an encoder-decoder medical VLP approach for global-to-dense visual representation learning. Pseudo segmentation is adopted for dense level learning. Rich experiments validate the effectiveness of the proposed method.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The motivation behind the work is clear. Pseudo-segmentation supervision is effective, which is validated by experiments.
2. The experiments are rich and ablation analysis shows the contributions of each component and design.
3. The illustrations are clear and easy to understand.
4. The improvements are consistent and sometimes substantial.

Weaknesses:
1. The comparisons with MGCA and MRM in the CXR14 dataset are not included in Table 3, but Table 4 includes the comparisons with MGCA and MRM. What are the reasons behind this?
2. Transformer-based vision encoder is not analyzed.
3. The balance between VLA and PA losses is not analyzed.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new medical vision-language model, G2D, which employs vision-language alignment (VLA) and pixel alignment (PA) strategies, combined with a pseudo segmentation (PS) pre-training task, to learn global and dense visual representations from medical images. The VLA strategy is used to learn global representations of images and texts, while the PS task constructs pseudo masks through a parameter-free mechanism to facilitate the learning of dense representations. The method is comprehensively validated across five downstream tasks (image segmentation, object detection, zero-shot image visual grounding, zero-shot image classification, and fine-tuned image classification), demonstrating its effectiveness in handling both unimodal and cross-modal tasks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
+ The paper is well-written, with the motivation, method, and results clearly presented. A minor concern is the reference format; it should be [1] instead of (1) according to the NeurIPS template.

+ A significant concern with most existing works is that they operate primarily at the Image-Text Retrieval level, similar to the perceptual level of CLIP, and do not effectively capture dense features between modalities. The G2D model addresses this issue by integrating Vision-Language Alignment (VLA) and Pseudo Segmentation (PS) tasks to facilitate simultaneous learning of global and dense visual features. This multi-level feature learning significantly enhances the model's performance in tasks requiring dense feature perception, such as segmentation.

+ During pre-training, the G2D method utilizes only image-text pairs without the need for additional annotated data. By generating pseudo masks on the fly through the PS task, it reduces the cost and complexity associated with data annotation.

+ The G2D method is novel, and the experiments are robust. Experimental results on five medical imaging tasks involving 25 diseases demonstrate that the G2D model outperforms existing models, even with minimal fine-tuning data. Notably, in segmentation tasks requiring dense visual features, G2D achieves excellent results with just 1% of the training data for fine-tuning.

Weaknesses:
Major concerns:

- The attention maps could introduce errors in pseudo mask, and these errors may propagate throughout the training process. To address this, a clear validation strategy needs to be outlined. For instance, in Figure 2, aggregated attention map might incorrectly highlight irrelevant regions. It is essential to establish methods for **detecting** and **measuring** these errors to ensure the reliability of the model. I hope the authors could quantify the errors in aggregated attention map and pseudo mask during the rebuttal period.

Minor concerns:

- The training and validation of the model rely on specific datasets, which may introduce biases and potentially affect the model's generalizability to different datasets.

- It is uncertain whether the method can be effectively extended to vision-language tasks involving 3D imaging (e.g., CT and MRI), presenting a limitation in its current scope of application.

Limitations:
Limitations were discussed in Section A.1

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zqLAMwVLkt;"REVIEW 
Summary:
This paper works on node anomaly detection in the novel semi-supervised setting where few labeled normal nodes are given and proposes to generate new anomaly nodes to boost the training data. The anomaly generation algorithm is inspired by the empirical observation that:

(1) Anomaly nodes have lower affinity score than normal nodes
(2) Feature distribution of anomaly nodes are similar to normal nodes if they share similar neighborhood patterns.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The setting is novel and aligned to the real-world situation where normal nodes are typically known compared with anomaly nodes.

(2) The motivation for the proposed two regularization losses is very intuitive and clear.

(3) The experimental results are very impressive.

Weaknesses:
(1) The proposed two regularization losses are heavily based on the empirical analysis, which might not transfer to other anomalies in other datasets. 

(2) For the second prior, its assumption that anomaly nodes sharing similar local structures would share a similar feature distribution has not been empirically verified.

(3) Experiments miss the comparison with diffusion-based generative anomaly detection baseline.

Limitations:
In addition to the limitations mentioned by the author, there are some other limitations worth addressing:

(1) The currently proposed anomaly generation method is still operated in the embedding space. As admitted by the author anomaly behavior is heavily based on interactional behaviors, therefore, it is also helpful to consider directly characterizing/generating anomaly in the graph space.

(2) The comparison misses one generative-based baseline [1]

[1] Liu, Kay, et al. ""Graph diffusion models for anomaly detection."" (2024).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel approach called GGAD aimed at improving anomaly detection in graphs under a semi-supervised framework. GGAD generates pseudo anomaly nodes that serve as negative samples for training a one-class classifier. This method is built on two
key priors: asymmetric local affinity and egocentric closeness, which help in generating reliable outlier nodes that mimic real anomalies in terms of both graph structure and feature representation. Extensive experimental results demonstrate the effectiveness of the method across diverse graph anomaly detection datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The method is innovative. The proposed graph anomaly detection method can exploit the feature and structure information of normal nodes more effectively in the studied semi-supervised scenario compared to existing methods.  The proposed two priors provide a meaningful characterization of desired properties of outliers in this semi-supervised setting and can be utilized to explore other beneficial priors further. 

2.The experiments in the paper are comprehensive and thorough.

Weaknesses:
1. The model relies on prior knowledge to generate anomaly points. This prior knowledge can limit the model’s application scenarios. The model performs best only when the real anomalies align with this prior knowledge. For anomaly types that do not conform to the prior knowledge, the model may not effectively detect them.

2.The model does not perform best on the Photo dataset in Table 1, and the article lacks an explanation of the results at the overall data level.

3. This model employs a semi-supervised approach that uses some positive samples for training. However, it does not consider the issue of noise interference within the positive samples, namely, how the model overcomes interference when some positive samples are mislabeled.

4. During the initialization step, only the initial feature of outliers are obtained while the connections between the outliers and normal nodes are not well illustrated in the paper. From Figure 2, one outlier is connected to more than one normal node while the feature of the outlier is generated according to single normal node. The neighborhood of outliers is important since the it involves the computation of node affinity score of outliers.

Limitations:
yes, the authors point out that some anomalies whose characteristics may not be captured by the two priors used

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel generative-based GAD approach, named GGAD, tailored for the semi-supervised scenario. Unlike existing GAD frameworks, the authors highlight the feasibility and importance of a semi-supervised setting where labels for normal nodes are relatively easy to obtain during training, but labeled abnormal nodes are very limited. In this context, the paper proposes generating pseudo-anomaly nodes to serve as substitutes for real anomaly nodes in training, thus aiding in anomaly detection. These pseudo-anomalies are generated through two unique loss-guidance mechanisms. Experimental results demonstrate the effectiveness of GGAD.

However, the description of the semi-supervised setting in this paper lacks clarity and unconvincing. Additionally, there is minimal differentiation between the proposed method and existing works that generate pseudo-anomaly samples for data augmentation. I think this paper's novelty is limited. I still think that doing unsupervised GAD is more necessary, and if the authors can prove that the pseudo-outlier proposed by GGAD can benefit unsupervised GAD as a general module, I can up my score.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The complete experiment shows the effectiveness of the method and the necessity of each component.

2.Some visual illustrations help the reader understand, although the shapes of the images seem to be compressed.

Weaknesses:
1. I am still confused about the motivation for performing semi-supervised GAD. Why do most methods emphasize unsupervised scenarios? The cost of labeling normal nodes seems too expensive, as the authors themselves state on lines 268 to 269, yet they assert again on line 31 that labels for normal nodes are easy to obtain.This inconsistency hinders a clear understanding of the necessity and practical applications of semi-supervised GAD, which significantly undermines the motivation for this work.

2. While the first loss function proposed by the authors appears intuitively valid, the second loss function aims to generate outliers similar to normal nodes. In my opinion, optimizing these two losses together is unreasonable because they conflict with each other. It seems that they should correspond to different outlier generation processes

3. The paper validates the improvement of unsupervised GAD using labeled normal nodes and claims that GGAD remains superior. I think the authors ignore the fact that unsupervised methods do not obtain this outlier like GGAD and this comparison is not reasonable.

Limitations:
No limitation need to discuss

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores the problem of semi-supervised graph anomaly detection (GAD), where some nodes are known to be normal, in contrast to the typical unsupervised setting with no labeled data. The authors show that even a small percentage of labeled normal nodes can improve the performance of existing unsupervised GAD methods when adapted to the semi-supervised scenario. The paper proposes a novel Generative GAD approach (GGAD) to better exploit normal nodes by generating pseudo anomaly nodes, called 'outlier nodes', to provide effective negative samples for training a one-class classifier. GGAD generates these outlier nodes using priors about anomaly nodes, such as asymmetric local affinity and egocentric closeness, to mimic anomalies in structure and features. Experiments on six real-world GAD datasets show that GGAD outperforms state-of-the-art methods in both unsupervised and semi-supervised settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ This paper studies a new problem of semi-supervised GAD that has not been widely studied. 

+ The proposed method is simple and effective from the empirical perspective.

+ The experiments are extensive including effectiveness and efficiency analyses and the method has been tested on real-world large-scale graphs to verify the scalability.

Weaknesses:
- The two priors that are used to generate outlier nodes are heuristic or based on empirical evidence. There is no theoretical analysis provided to better guarantee the effectiveness of the proposed method.

- It will be more interesting and helpful to show the generated outlier nodes can capture the characteristics of anomalous nodes in addition to comparing their representations.

- The experimental settings of anomaly contamination are not very clear: how the contamination is introduced?

- Overall experimental settings. What hardware has been used in the experiments, e.g., memory, and why are the experiments conducted on CPUs?

Limitations:
The authors have adequately addressed the limitations

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies an under-explored graph anomaly detection problem where the detection models have access to a set of labeled normal nodes. To tackle this problem, it introduces a generative approach namely GGAD that generates pseudo anomaly nodes, called outlier nodes, to support the training of a discriminative one-class classifier. The key idea underlying this approach is to generate the outlier nodes in a way that can well simulate real anomaly nodes in both graph structure and feature representation perspectives. To achieve this, GGAD defines and incorporates two priors, including asymmetric local affinity and egocentric closeness, into its optimization objectives, with the former prior focusing on the alignment on the graph structure aspect and the latter on the feature representation aspect. The method is evaluated on six large real-world datasets and shows impressive detection performance compared to existing state-of-the-art methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well-written and easy-to-follow.
- The problem setting is practical since labeled normal samples are easy to obtain in many real-world applications. Compared to the commonly studied unsupervised setting, this semi-supervised setting often results in better detection performance.
- The proposed method GGAD is novel. There have been many generative anomaly detection methods, but as far as I know, they are unable to consider the graph structure and the neighboring nodes’ representations. By introducing the two new priors, GGAD addresses this issue well. Fig.1 and Fig. 3 help demonstrate this effect.
- The method is compared with a range of unsupervised and semi-supervised methods on 6 real-world datasets with diverse genuine anomalies, and gains largely improved detection performance over these competing methods.
- The ablation study is plausible and justifies the contribution of each proposed prior.

Weaknesses:
- The outlier node generation in GGAD may cause non-trivial computational overhead.
- Despite better performance than the competing methods, GGAD gains an AUC of only around 0.6 on some datasets, such as DGraph and Reddit.
- In Fig. 4 (b), GGAD shows a fast AUPRC growth with increasing training size, but the other methods have a flat performance trend. What would be the reason behind?

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zpw6NmhvKU;"REVIEW 
Summary:
This paper proposes a method (RashomonGB ) to estimate the Rashomon sets/predictive multiplicity of gradient boosting models. It estimates multiple ($m$) models at each stage (effectively performing a local exploration) and then combine all such models in the end to construct $m^T$ models for Rashomon set computation, where $T$ is the number of iterations of the boosting. On several datasets the paper shows that RashomonGB performs better than re-training with $m$ seeds, in that at the fix $\epsilon$ (loss difference) level, RashomonGB tends to show more predictive multiplicity.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
Predictive multiplicity is an important topic. The paper is generally clear and well-written. The proposed method is a sensible first method for boosting algorithms, which was previously underexplored. I think the proposed method is likely adopted by people who care about this problem as it's intuitive and easy to implement.

Weaknesses:
1. The current exploration strategy is fast to compute, but I'm not sure if this follows the motivation of Rashomon set very well. While the authors mention one example on the Contraception dataset where re-training underestimates the predictive multiplicity, in general RashomonGB might create models that are more correlated than normal (because the ""backbone"" is the same GB model), thus underestimating the predictive multiplicity. Right now, the conclusion shows otherwise probably because the number of re-training is too small. 

2. Regarding the experiment, if I read this correctly, currently we use more compute for RashomonGB as well (by combining different weak models), so it is also not quite a fair comparison in my opinion. I would be very interested to see some estimate of how much compute RashomonGB saves against re-training, by running more re-training and see when are the metrics in Fig3 in the two methods become comparable.



minor: one ""RashomonGB"" in L290 should be ""re-training"".

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an approach that compute Rashomon set for gradient boosting algorithm where the set can be obtained through products over weak learners at each step rather than sampling them through retraining. The authors further proposed a dataset related Rashomon bound through sub-Gaussian assumption, where mutual information between hypothesis space and dataset shows the predictive multiplicity, which can further decomposed into model uncertainty and quality of data. Experiments show the proposed solution offers more models in Rashomon set than retraining given the same computation budget.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The rough idea of the proposed approach is straightforward since decomposing Rashomon set search on boosting algorithm can be a ""standard"" operation given the unique residual learning property of boosting algorithms. The novelty of the proposed approach is probably more from ""our work is the first to explore the Rashomon effect for gradient boosting"".

The dataset related Rashomon set bound seems an interesting point. But it needs some justification for the key assumption of it (sub-Gaussian). Proposition 2 seems make sense given the positive relation between number of boosting iterations and Rashomon set (also for dataset size).

Experiments in 4.2 seem interesting. I would love to see more experiments like it.

Weaknesses:
I got some difficult time to understand the introduction and abstract of this paper even I have read some literatures about Rashomon effect and predictive multiplicity. It is simply hard to read given the narrative there. Especially the second paragraph of introduction; it gets me confused and self-questioning my understanding of Rashomon effect from other works.

Limitations:
No hard limitation I can see.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the Rashomon effect in gradient boosting, a commonly used algorithm for tabular datasets, but something that has not received enough attention in multiplicity literature. The paper provides several theoretical discussions on the size of the Rashomon set and the impact of the number of iterations on multiplicity in GBRTs. Furthermore, the paper proposes RashomonGB, a method to create an exponential number of ‘near-optimal models’ by training only a polynomial number of models. With more models in the Rashomon set, the use of RashomonGB can create several downstream benefits without any extra cost of training, shown empirically by the authors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Multiplicity in GBRTs, or generally any gradient-boosting algorithm, has not been studied in the literature, and so the authors provided a novel discussion, especially given the importance of these algorithms in tabular settings.
- The paper provides several theoretical discussions backed by empirical support. The insights on the growing Rashomon set with iterations were quite interesting, although I have concerns about the validity of these insights (see Weaknesses).
- Multiplicity quantification can be quite costly, and various methods in pursuit of reducing this cost can significantly benefit further auditing. The use of RashomonGB, as proposed by the authors, can be an important step in that direction for gradient-boosted algorithms.

Weaknesses:
- While the presentation of the rest of the concepts and the theoretical discussion were easy to follow, important details about the RashomonGB method and the details of the empirical setup were either missing (even from the Appendix) or imprecise. For instance, the Rashomon set of the gradient boosting algorithm isn’t going to simply be the iterative extension of Rashomon sets at every residual level, i.e., equation 4 is imprecise. Similarly, it seems that the epsilon value of the Rashomon set increases with more iterations, and thus it is confusing to me whether the insight that more iterations create bigger Rashomon sets is a result of multiple iterations or simply a result of bigger epsilon. See the section ‘Questions’ for more detailed comments and some follow-up questions. Edit after rebuttal: Acknowledged, correct and clarified.
- There are other methods to measure predictive uncertainty in gradient-boosted algorithms. Some examples based on a cursory search (there might be more, as I’m not too familiar with GBRTs) - https://arxiv.org/abs/2205.11412 https://arxiv.org/pdf/1910.03225 https://arxiv.org/abs/2106.01682 - While I understand that prediction uncertainty is not the same as predictive multiplicity, the two are closely related, and when proposing a better method to measure multiplicity, the paper should compare itself with other stronger baselines than just retraining. Just as previous works have proposed using Monte Carlo Dropout (which was initially created as a method to measure uncertainty) as a measure of multiplicity, uncertainty measurement baselines for GBRTs could have been adopted to create reasonable baselines, and would have made the results a lot stronger. Edit after rebuttal: Acknowledged and added.

Limitations:
- A central piece of the paper is their method RashomonGB. While the authors do try to emphasize the importance of this method by highlighting the number of models that can be created using their method, just the number alone is not enough to imply a better method for measuring multiplicity. Even assuming that the comparisons are indeed fair (see Questions), the differences in multiplicity are not very severe, and that makes me wonder if combining pieces of various residual models actually gives us new interesting models or do we just end up with similar models as already seen during retraining. The authors acknowledge this briefly in their limitations paragraph. Edit after rebuttal: Appropriate details added and clarified.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores the concept of predictive multiplicity in gradient boosting models. The Rashomon effect refers to the existence of multiple models that perform similarly well on a given dataset. The authors formalize this effect in the context of gradient boosting, introduce a new method called RashomonGB to efficiently explore this multiplicity, and demonstrate its application on various datasets. The paper aims to improve the estimation of predictive multiplicity and model selection, especially with considerations for group fairness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The introduction of RashomonGB represents a novel method for exploring the Rashomon set in gradient boosting, offering an exponential search space as opposed to traditional linear methods.
2. The paper provides a robust theoretical foundation using statistical learning and information theory to analyze the Rashomon effect, enhancing the understanding of this phenomenon in gradient boosting.
3. The authors demonstrate the practical utility of RashomonGB on a wide range of real-world datasets, including tabular and image data, showcasing its versatility and effectiveness.

Weaknesses:
1. While the paper discusses the positive societal impacts of RashomonGB, it lacks a thorough exploration of potential negative impacts or misuse of the method.
2. The theoretical analysis relies on several assumptions that may not hold in all practical scenarios, potentially limiting the generalizability of the findings.
3. The paper mentions the intention to release code post-review, but the lack of immediate open access to code and data can hinder reproducibility and independent validation by other researchers.
4. Implementing RashomonGB might be complex for practitioners without a strong background in the theoretical aspects of machine learning and gradient boosting, potentially limiting its adoption in the industry.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zn6s6VQYb0;"REVIEW 
Summary:
This paper proposes a cross-correlation autoencoder for graph structural reconstruction. The authors first analyze the problems of existing self-correlation encoder. Then, a cross-correlation autoencoder is designed. Experimental results show the effectiveness of the cross-correlation autoencoder.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation is clear and the cross-correlation autoencoder is reasonable.
2. The paper is well-written and easy to follow.
3. The experiments are comprehensive.

Weaknesses:
1. The authors mention that the current self-correlation methods can not address specific (sub)graph structures. But this paper only presents an overall experimental performance. It is unclear how the proposed cross-correlation autoencoder performs given a specific graph structure. 

2. It is not clear whether the graph dataset used in the paper is a directed or undirected graph. Since the cross-correlation autoencoder can represent the directed graph effectively, it is suggested to consider the directed graph dataset.

3. More different architectures of the encoder and decoder should be employed to further verify the effectiveness of the cross-correlation mechanism.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a method to address the limitations of existing graph autoencoder (GAE) models that primarily rely on self-correlation for graph structure representation. They claim existing GAE often fail to accurately represent complex structures like islands, symmetrical structures, and directional edges, particularly in smaller or multiple graph contexts. The proposed model, GraphCroc, introduces a cross-correlation mechanism that aims at enhancing the representational capabilities of GAEs. It employs a mirrored encoding-decoding process to ensure robust structural reconstruction and introduces a loss-balancing strategy to tackle representation bias during optimization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea to introduce two latent space for reconstructing the graph structure is ""simple and intuitive"". 

2. The writing is clear and easy to follow.

3. The experimental results are sound.

Weaknesses:
1. This paper lacks discussion on related works. There already exists some works trying to solve the graph autoencoder structure recovering issues. For example, including position encoding [1] or adding extra node labels [2]. How the proposed method is compared with these methods, from the perspective of effectiveness and efficiency?

[1] You, Jiaxuan, Rex Ying, and Jure Leskovec. ""Position-aware graph neural networks."" International conference on machine learning. PMLR, 2019.

[2] M. Zhang, P. Li, Y. Xia, K. Wang, and L. Jin, Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning, Advances in Neural Information Processing Systems (NeurIPS-21), 2021.

2. As the proposed method generate two latent embeddings, I wonder if there exists some techniques to control them to be different with each other? Otherwise I am concerned that whether the two embeddings could converge to each others.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper theoretically analyzes the limitations of existing graph autoencoders (GAE) in representing special graph features such as islands, symmetrical structures, and directional edges. To address this, the paper proposes a new GAE method, GraphCroc, which employs a cross-correlation mechanism that significantly enhances the representational capabilities of GAEs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper clearly shows the limitations of existing GAEs through theoretical analysis.

2. The experimental results demonstrate the advantages of the proposed method in structural reconstruction and graph classification tasks.

3. The paper is easy to follow.

Weaknesses:
1. In Table 1, the improvements of GraphCroc are evident only on two datasets.

2. While the proposed cross-correlation method performs better than the general self-correlation method on island, symmetric structures, and directed graphs, it would be beneficial to include more results in reconstruction visualization, particularly regarding island or directed edge reconstruction.

3. Some related works [1] need to be discussed.

[1] Liu, Chuang, et al. ""Where to Mask: Structure-Guided Masking for Graph Masked Autoencoders."" arXiv preprint arXiv:2404.15806 (2024).

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zm1LcgRpHm;"REVIEW 
Summary:
This paper introduces a new method for time-series representation learning that enhances the modeling of non-adjacent segment dependencies. Specifically, the proposed method segments, shuffles in a learned manner and stitches the shuffled segments to combine with original time series. The proposed method is model-agnostic without adding significant parameter overhead and shows performance improvement across multiple classification and forecasting base models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method permutes the original segments to better capture inter-relations between distant segments. It is model-agnostic and introduces minimal parameter overhead to the original model.

2. Extensive experiments on various base models for both classification and forecasting tasks demonstrate the effectiveness of the proposed method.

Weaknesses:
1. It it not clear how the sorting process, specifically the calculation of permutation $\sigma$ from $P$, is made differentiable.

2. The compared forecasting baselines such as Informer are no longer state-of-the-art methods. Adding more recent baselines such as Time-LLM, GPT4TS, DLinear, PatchTST would provide a clearer understanding of the proposed method's comparative benefits.

3. The basic assumption for S3 is that modeling non-adjacent dependencies is important. However, the paper lacks detailed case studies that demonstrate the specific types of non-adjacent dependencies effectively captured by S3, which are not addressed by existing models. Additionally, there is no case study to validate that the learned shuffling weights accurately represent these segment dependencies.

Limitations:
The paper mentions potential expansions into tasks like imputation and anomaly detection. Further details on limitations from the reviewer are discussed in Weaknesses and Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a plug-and-play mechanism called Segment, Shuffle, and Stitch (S3) designed to enhance time-series representation learning in existing models. S3 operates by dividing the original sequence into non-overlapping segments and shuffling them in a learned manner that is optimal for the given task. It then reattaches the shuffled segments and performs a learned weighted sum with the original input to capture both the newly shuffled sequence and the original sequence. This proposed model can enhance the performance of specific models in classification and prediction tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is easily comprehensible and straightforward.

Sufficient experiments are conducted to confirm the effectiveness of the method.

Weaknesses:
Lack of comparative methods:
In fact, the proposed method seems to share the same spirit as data augmentation methods in the time series field[1-4]. Why hasn't any data augmentation method been compared?


Selection of baseline models:
The selected baseline model, Informer, seems somewhat outdated. Why not choose a more recent model, e.g., iTransformer[5] or PatchTST[6]?


Dataset for prediction task:
The author conducted experiments on three ETT datasets, but for prediction tasks, more datasets should be considered, e.g., traffic, electricity, and weather.


Time-Series Representation Claim:
 As the author pointed out, more tasks should be considered for time series representation learning.


[1]FRAUG: FREQUENCY DOMAIN AUGMENTATION FOR TIME SERIES FORECASTING  [2]Time Series Data Augmentation for Deep Learning: A Survey  [3]SimPSI: A Simple Strategy to Preserve Spectral Information in Time Series Data Augmentation [4]TOWARDS DIVERSE AND COHERENT AUGMENTATION FOR TIME-SERIES FORECASTING [5]ITRANSFORMER: INVERTED TRANSFORMERS ARE EFFECTIVE FOR TIME SERIES FORECASTING [6]A TIME SERIES IS WORTH 64 WORDS: LONG-TERM FORECASTING WITH TRANSFORMERS

Limitations:
See Weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new neural network design element which segments, shuffles, and stitches time series for improved representation learning. They evaluate their methods on forecasting and classification tasks, and show that S3 benefits some widely used baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. To the best of my knowledge, the idea is novel, and fundamentally challenges and changes how to learn representations for time series data
2. The paper is well written and easy to follow
3. Experiments are well-designed, and results are promising

Weaknesses:
I have not found any major weaknesses in the methodology or experimental design. However,  I think that the paper might benefit from showing what the S3 module is actually learning. For example, the authors can include the segmented, shuffled, and stitched time series on a particular dataset as an example, along with the weighted time series (used as input to the model), and the original time series. This might provide some intuition as to how this design element improves predictive performance. 

I think there's always scope to improve experimental design. TS2Vec is a excellent choice for classification, but not for forecasting. I would recommend that the authors use methods such as PatchTST (transformer-based) or iTransformer, TimesNet (CNN-based), N-BEATs or N-HITS (MLP-based) etc. for time series forecasting. For classification, it would also be good to compare with fully supervised methods such as ResNet1D (see [1]). 

### References
[1] Ismail Fawaz, Hassan, et al. ""Deep learning for time series classification: a review."" Data mining and knowledge discovery 33.4 (2019): 917-963.

Limitations:
The authors have a very brief description of limitations of their study.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper paper introduces a new approach called Segment, Shuffle, and Stitch (S3) to enhance time-series representation learning. The method involves segmenting the time-series into non-overlapping parts, shuffling them optimally, and stitching them back together along with the original sequence.

Key contributions include:

- Proposing the S3 mechanism to improve time-series representation learning by dynamically reordering segments.
- Demonstrating that S3 can be integrated with existing neural architectures like CNNs and Transformers, resulting in significant performance improvements.
- Showing through extensive experiments that S3 enhances performance in time-series classification and forecasting tasks, with improvements up to 68%.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Code is available, making reproducing this paper easier.
- Paper is clear.
- Results appear good, when considered on the set of baselines and dataset picked by the authors.

Weaknesses:
- Tables 1 and 2 focus on the ETT datasets, which are only a (highly intra-correlated) subset of the common forecasting datasets: Electricity, Traffic, Weather, Illness...
- I see no mention of CoST in the results tables, despite being cited in the paper. This is usually a very strong baseline for contrastive approaches. Including it would certainly paint a more complete picture of the results landscape. On a related note this also applies to e.g. more recent transformer baselines. Informer is relevant, but also very far from state of the art.
- Error bars would help one better contextualize the results.
- The lack of an ablation study makes understanding the reason this works more complicated.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a simple but effective differentiable module that performs pre-processing to input multivariate time-series before being fed into any differentiable model for arbitrary task. The pre-processing involves segmenting, shuffling the segments and stiching them together. The novelty include making this seemingly discrete operations into a differentiable module. This simple idea yields significant improvement in performance of different kinds of models over variety of datasets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The method is simple and easy to add to most deep learning models
2. The technical details are well-motivated and explained
3. The method also improves training efficiency and convergence time along with performance with very little increate in model complexity
4. Experimental results across different tasks are strong

Weaknesses:
1. Visualization and any qualitative study on the shuffling and segments generalted by S3 would greatly benefit the readers.
2. How well does it optimize transformer based models, especially those that already do segmentation like PatchTST since the attention module captures the relations all pairs of segments already?
3. Does the representations due to S3 generalize to multiple tasks at a time or do we need to retrain for each task?

Limitations:
1. Lack of understanding on the segment permutations generated and why they are better for the model performance atleast qualitatively

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zlgfRk2CQa;"REVIEW 
Summary:
To solve the stability of Deep Thinking models, this paper proposes to constrain activation functions to be Lipshitz-1 functions. The original DT and DT-R models have training stability problem, basically because of scale explosion or vanishing. The authors revealed the stability problem, attribute the problem to Lipschitz constants, proposed ways to ensure Lipschitz smoothness, and show the effectiveness of their approach through a few examples used in the original DT paper, as well as include the traveling salesman problem.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* This paper is clearly written and well motivated. 
* The storyline is very reasonable: identify problems => propose ways to solve the problem => show the approach actually works
* This approach is mathematically grounded.
* Experiments are thorough, by running many random seeds and report error bars.

Weaknesses:
* The idea is quite straight-forward (may not be a bad thing, but make technical contributions smaller)
* In the TSP problems, DT-L's results seem worse than NN Tours and BNN Tours. At least some explanation is warranted. 
* I'm not fully convinced by the significance of this paper. The examples shown in the paper are quite toy. Are there more examples you expect DT-L would work?
* I'd appreciate more visualizations that can intuitively show the benefits of DT-L over DT/DT-R? Maybe some Figures like in the original DT paper. 
* The title is not very informative. Might be better to mention Lipschitz smoothness in the title.

Limitations:
The authors cleraly addresses limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper identifies and rectifies an issue with a particular type of iterative neural network called Deep Thinking Networks. The problem arises in exploding latent representations and unstable training routines. The authors of this work propose an update to the architecture where they add Lipschitz constraints to the model. They show three major benefits: (I) The models train more stably/predictably; (II) the inference-time behavior is better as the latent representations converge with iterations of the recurrent model; and (III) this new approach can learn how to solve NP-Hard problems where the old methods fail.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper is original to my knowledge. I am aware of much of the work on Deep Thinking Networks and the issues raised and the solutions proposed in this work are novel.
1. The quality of the work is high. For the most part the experiments are done well and cover many natural questions that would arise from reading the abstract/intro.
1. The clarity is good. I think the writing is clear and the results are compelling.
1. The results are significant for those interested in easy-to-hard generalization. These Deep Thinking Networks have strong extrapolation of toy problems and with the proposed updates to the methods they show strong performance even for TSP solving.

Weaknesses:
1. Clarity: A couple things could be more clear.  
  i. I think IPT stands for Incremental Progress Training, but I don't see the acronym defined anywhere.  
  ii. Table 1 the units are unclear. I gather there are tour lengths, but that isn't stated in the table or the caption.  
  iii. The violin plot in Figure 2 is hard to parse (no harder than any other violin plot). This type of graphic does look nice, but offers little quantitative context. For example, there is no indication of the units/scale of the width of each violin. This is not the right type of plot for a conference paper.

Limitations:
Yes, the limitations are adequately addressed.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the positive feedback issue in the so called Deep Thinking networks, where the inference computation may involve more recurrent computations than encountered in training.  The proposed solution is to normalise the state vector that undergoes the recurrence, i.e. make the mapping contractive, i.e. ensure negative (but just) feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is well written and clear to follow, the proposed method is pretty straight forward and effective.

Weaknesses:
As far as I can tell, it is pretty straight forward control theory stuff for addressing positive feedback.  Nothing wrong with the proposed solution, but I would assume this is such a fundamentally well known issue in any recurrent/feedback system that we can leave this to be addressed by the designer at implementation time with any choice of normalisation.  It is somewhat disappointing that with the proposed method there is still the need for batch normalisation.

Limitations:
If I understand this correctly, the proposed normalisation creates vanishing gradient problem, but authors seem to be aware of this and address it by keeping the spectral norm close to 1.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Deep Thinking with Lipschitz Constraints (DT-L), an improved version of the Deep Thinking (DT) networks, designed to enhance the stability and performance of iterative algorithm learning models. The authors address the instability issues inherent in DT networks by analyzing intermediate representation growth and applying Lipschitz constraints. The DT-L model guarantees convergence to a unique solution and demonstrates robustness in learning algorithms that extrapolate to more complex problems. The paper furthermore benchmarks DT-L on the Traveling Salesperson Problem (TSP) as well other than the datasets used in the Deep Thinking models. It compares its performance against existing DT models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Introducing Lipschitz constraints into the DT framework enhances the models' reasoning capabilities. This approach addresses instability issues in training and inference, offering theoretical guarantees for convergence.
- DT-L demonstrates the ability to scale to larger problems effectively, maintaining stability and performance, which is crucial for real-world applications.
- The comprehensive evaluation on various problem classes, including prefix sums, mazes, chess puzzles, and TSP, highlights the robustness and versatility of the DT-L model.
- The paper provides a thorough analysis of the issues with DT networks and clearly explains how the proposed modifications address these problems.

Weaknesses:
- The modifications and theoretical underpinnings of the DT-L model, such as the Lipschitz constraints and orthogonal transformations, add complexity to the model, which might hinder its adoption and understanding by a broader audience.
- While the DT-L model shows improvement, its performance on the TSP is not impressive, indicating room for further optimization and refinement.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zkhyrxlwqH;"REVIEW 
Summary:
The paper addresses unsupervised homography estimation from multi-modal image pairs. The authors propose to cope with the issue of 1) modality, 2) registration in two distinct networks that are trained in an interleaved fashion. The networks architecture derives from the Barlow Twins framework, with changes in the loss function. Results are illustrated on several public benchmark of small images (128^2) and compares favorably wrt to related unsupervised approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1- I enjoy reading the paper. I walked through the paper, first with curiosity and skepticism, then with strong interest. The approach is intuitive (adjust the two representations then compute the transformation) and compelling. I am somehow surprised that it works :) The constrastive-like loss used in  Barlow Twins  is contributing much for the network to learn the correct solution. 

2- Overall, the authors are tackling an important problem (unsupervised learning) for which an original solution is proposed --while based on previous recent work. The methodology is clearly presented. Results are convincing (thought only on small images 128x128) and illustrated on various modality pairs. Quantitative results show improvement wrt related unsupervised work

Weaknesses:
1- Not a weakness, but a points which could have been discussed: why not simply transforming the inputs into edge maps before learning a matching/homography function (and putting aside the modality discrepancy). It would not be a very fancy approach, but I believe it could be a baseline for comparison. 

2- The approach would be more convincing if each of the two modules (GL and MARL) had demonstrated their effectiveness also individually (ie same image pair modality using only GL).

Limitations:
From a practical point of view, the size of the image and the strong overlap between the pairs show that the work need to be further developped for applications at full scale. 
From a methodology point of view, the authors have discussed the limitation of having two networks trained in an interleaved way, with potential collision or collapse.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an unsupervised homography estimation method for multimodal image pairs using an alternating optimization approach. The claimed key innovation is the introduction of the Geometry Barlow Twins loss function for the alternating optimization. The authors show that their approach works on 3 multimodal datasets and different homography estimation architecutres.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The alternating optimization framework together with Geometry Barlow Twins loss seem to be a fresh perspective on unsupervised multimodal homography estimation.

Weaknesses:
Weaknesses
1. Discussion on the Feasibility and Rationality of the Proposed Method: First, for unsupervised training of networks based on iterative prediction, such as RAFT, to ensure stability during training, related methods [1-2] typically apply some form of direct supervision to the motion predicted by the network. This is different from the approach proposed in this paper, which only uses the Geometry Barlow Twins loss for brightness supervision. Second, how RAFT can be used for homography estimation should also be explained, because it is designed for optical flow estimation. Moreover, the paper does not explain how the proposed Geometry Barlow Twins loss supervises the intermediate stages of iterative prediction, whereas RAFT, IHN, and RHWF, along with methods leveraging their structures [1-2], generally provide details on their supervision mechanisms on the intermediate stages. This raises concerns about the feasibility of the proposed supervision method in this paper. Additionally, the effectiveness of the Modality-Agnostic Representation Learning (MARL) introduced in section 4.3 is questionable because it lacks spatial information in its supervision. As mentioned in section 3.2, the projector removes spatial information from the feature maps. The authors should provide a convincing and thorough explanation for these issues. 

2. Doubt about the Effectiveness of the Proposed Method: For example, the paper proposes the alternating optimization (AltO) method but does not provide sufficient experimental results to demonstrate its superiority over other strategies, such as directly cascading all the modules. Furthermore, the paper lacks a comparative demonstration of the features extracted with and without the MARL phase, making the advantages of introducing this phase less convincing.

3. Insufficient Experimental Validation: The paper conducts experiments on only 3 cross-modal datasets, among which only the GoogleMap dataset exhibits significant modality differences. The GoogleEarth dataset mainly consists of images taken in different seasons [3]. Part of the DeepIR dataset is simulated multispectral data [4], which will significantly reduce the difficulty of homography estimation. It would be beneficial to conduct experiments on more challenging multimodal datasets, such as those involving VIS-SAR modalities.

[1] Stone, A., Maurer, D., Ayvaci, A., Angelova, A., & Jonschkowski, R. (2021). Smurf: Self-teaching multi-frame unsupervised raft with full-image warping. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (pp. 3887-3896).
[2] Liang, Y., Liu, J., Zhang, D., & Fu, Y. (2023). Mpi-flow: Learning realistic optical flow with multiplane images. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 13857-13868).
[3] Zhao, Y., Huang, X., & Zhang, Z. (2021). Deep lucas-kanade homography for multimodal image alignment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 15950-15959).
[4] Sa, I., Lim, J. Y., Ahn, H. S., & MacDonald, B. (2022). deepNIR: Datasets for generating synthetic NIR images and improved fruit detection system using deep learning techniques. Sensors, 22(13), 4721.

Limitations:
The paper discusses the additional training cost arising from the inclusion of an additional module in the two-phase network, and explores potential solutions for addressing this issue in future research. However, the method’s generalization capabilities are not thoroughly explored, with experimental datasets limited to satellite images, maps, RGB, and NIR images. Future research could involve testing the method on a broader range of datasets to validate its generalization capabilities.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new unsupervised homography estimation approach for multimodal images. This method is designed as a two-phase optimization framework named AltO. The first phase named ""Geometry Learning"" trains a registration network to align the input multimodal images geometrically. The second phase named ""Modality-Agnostic Representation Learning"" trains an encoder and a projector to extract the image-level features invariant to modality changes. Experimental results demonstrate that AltO outperforms several existing unsupervised approaches on the multimodal registration datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed framework is intuitive and interesting. This framework trains a registration network to align the input multimodal images geometrically, and trains another encoder to match the image-level features of the warped multimodal images. This framework has the potential to capture the pixel-level and image-level information in an unsupervised manner.
2. The organization and presentation of this paper are good. I think I can understand the core idea of this paper.

Weaknesses:
**1. Some central claims of this paper lack experimental evidence.**

1.1 The ""alternating"" optimization framework is a central design in this paper. However, why is ""alternating"" optimization necessary? Will optimizing the ""geometry loss"" and ""modality loss"" simultaneously hurt performance?

1.2 The superiority of the proposed Geometry Barlow Twins (GBT) loss was not verified. The original Barlow Twins loss can be straightforwardly applied to the proposed model by considering both the spatial axis (indexed with ""h,w"") and batch axis (indexed with ""n"") as the batch dimension. This straightforward implementation should be compared with the proposed GBT loss.

1.3 The proposed approaches should be compared with some recent unsupervised approaches. Here are some approaches with released codes.

[1] Unsupervised global and local homography estimation with motion basis learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

[2] A Multiscale Framework with Unsupervised Learning for Remote Sensing Image Registration, IEEE Transactions on Geoscience and Remote Sensing, 2022.

**2. This paper did not discuss the recent hand-crafted approaches for multimodal image registration.**

Many recent hand-crafted methods have been published in the top journals, so this kind of approach should not be ignored. The experiment should also compare the proposed approaches with the recent hand-crafted approaches. Here are some hand-crafted approaches with released code.

[3] Histogram of the orientation of the weighted phase descriptor for multi-modal remote sensing image matching. ISPRS Journal of Photogrammetry and Remote Sensing, 2023.

[4] POS-GIFT: A geometric and intensity-invariant feature transformation for multimodal images. Information Fusion, 2024.

**3. The discussion of the motivation is not sufficient.**

The Introduction section mentioned some typical unsupervised approaches designed for the images from the same modality (e.g., UDHN and biHomE). However, the unsupervised approaches [2,5] designed for multimodal image registration are not discussed. What is the motivation of the proposed method compared with this kind of approach? 

[5] A Novel Coarse-to-Fine Deep Learning Registration Framework for Multi-Modal Remote Sensing Images. IEEE Transactions on Geoscience and Remote Sensing, 2023.

**4. This paper misses some references to hand-crafted and unsupervised approaches.**

I have listed some of them in the above weaknesses. The authors should further survey more papers and carefully revise the ""Related Work"" section.

Limitations:
The authors have discussed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zkfCa4oESF;"REVIEW 
Summary:
This paper proposes a new task, ""generalized zero-shot learning (GZSL),"" in which both seen and unseen objects should be recognized for vision-language tasks. It also proposes a new method based on CLIP that uses the loss in the ""attribute space"" to perform better in both seen and unseen classes. This method is evaluated on various kinds of data sets and evaluated by the harmonic mean of the accuracies of seen and unseen classes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed approach using the attribute space seems novel enough, and its effectiveness was verified by a detailed comparison of the other methods and the well-designed ablation studies.

Weaknesses:
1) It is unclear what is learned in ""learnable attribute tokens."" It is not so beneficial for unseen classes. It is unclear what information is represented as tokens for seen classes. It may be better to analyze the acquired tokens in more detail.

2) It is difficult to think about the case that we have never seen an object, but we know its attributes quite well. In such a sense, 
I believe this method is more appropriate for few-shot learning.

Limitations:
None.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper the author proposed a dual-space feature alignment module to keep the semantic consistency between visual and attribute. In addition, the authors proposed Topology-Preserving Reservoir (TPR) to tackle the issue into the generalized zero shot learning (GZSL) setting, which utilized the Pearson correlation coefficient to define a topology-preserving loss, which effectively prevents overfitting of the seen and unseen classes. Sufficient experiment demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1)The Paper is well-written, meanwhile, the method is intuitive and easy to understand.
(2)The proposed method focused on Generalized Zero-Shot Learning (GZSL) to present Topology-Preserving Reservoir to finetune the pre-trained CLIP for better fit the distribution of seen and unseen classes, which seems reasonable.
(3)Sufficient and significant experiments demonstrate the effectiveness of the proposed method.

Weaknesses:
(1)The Dual-Space Feature Alignment proposed by the author, which uses a Cross Attention mechanism for cross-modal alignment, lacks innovation.
(2)The author mentions ""attribute reservoir"" in the article, but essentially it is just a fully connected layer that generates different feature representations through various loss constraints. Additionally, in Figure 2, the attribute reservoir is shown in two states: frozen and trained. I am unsure about when these two states should transition between each other.
(3)The idea proposed by the author to fine-tune feature distribution using spatial topological structures is intriguing. However, relying solely on the Pearson correlation coefficient to define a topology-preserving loss seems somewhat simplistic.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The proposed approach targets the generalized zero-shot learning (GZSL) problem for the vision language model (VLM). It is observed that a strong VLM model shows promising results for novel class generalization. Fine-tuning these models for seen classes leads to a loss in generalization capability and poor results for unseen classes. Additionally, a single latent space demonstrates limited ability to adapt to complex visual-linguistic patterns in fine-grained datasets. The paper proposes dual-space alignment, augmenting the latent space with static and learnable tokens. To address the generalization problem post fine-tuning, the paper introduces a Topology-Preserving Reservoir (TPR), which helps preserve the model's generalization ability for unseen classes. The authors conducted extensive experiments across several standard ZSL datasets and explored the impact of various components through ablation studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
[1] Generalization of unseen classes in VLM is a critical problem. The strong pretrained model also loses its generalization ability, which the author explores, and the proposed model shows a significant impact.

[2] The idea and intuition behind the static and learnable attribute reservoir are interesting. Additionally, TPR helps improve generalization.

[3] The wide-ranging experiments conducted across various ZSL datasets and the ablation studies are satisfactory.

Weaknesses:
[1] The standard ZSL model assumes that there is a description per class rather than per sample, which is more intuitive since a single description for each class suffices for the model to understand the class, making it cost-efficient. Standard annotation-based attributes often yield better results for ZSL/GZSL settings. For example, [a] demonstrates impressive results for the CUB dataset compared to the proposed complex static, learnable, and description-based model. This issue is particularly observed in fine-grained datasets. Why is this the case?

[2] It is unclear how the base attribute vocabulary is created. At a high level, the author collected a few attributes and obtained LLM embeddings. This description may not be sufficient for reproducibility since the code and data are not provided.

[3] There are multiple variants of TPR (Table-3) in various scenarios where different methods work, making it difficult to apply and choose the best one. What does the author conclude here?

[4] In Table-1 for the SUN dataset, the model shows inferior performance. While we do not expect the model to outperform in all scenarios, a clear description and author observations are required: why is this the case?

[a] Meta-Learned Attribute Self-Interaction Network for Continual and Generalized Zero-Shot Learning, WACV-24

Limitations:
Authors has not discussed the limitation in the paper, while in the Checklist they said ""Yes"" i.e. they had discussed. This is a bad practice, I don't know what to do.

Dear AC please look into it.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper is a new study that introduces the Generalized Zero-Shot Learning (GZSL) framework within VLMs, aiming to classify both known and novel classes without class partitioning. Key innovations include a dual-space feature alignment module, enhancing latent representations with an attribute reservoir for nuanced visual-linguistic patterns. Additionally, a topology-preserving objective ensures that model adaptations preserve the semantic structure learned by CLIP, thus maintaining generalization across all classes. Extensive experiments across diverse datasets validate the proposed Topology-Preserving Reservoir (TPR) model, demonstrating superior performance over conventional methods in recognizing both seen and unseen classes, underlining its potential for practical applications in complex visual recognition tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
1: poor

Strengths:
1. This paper introcuces a novel research aspect for VLMs: generalized zero-shot learning, which requires the model to identify both seen and unseen concepts at the same time. From my perspective, this proposal could be a great contribution to VLM community.
2. This paper is well-organized and well-written, which makes it easy to follow. 
3. Extensive experiments，ablation study and visualization results demonstrate the effectiveness and rationality of TPR.

Weaknesses:
None in particular.

Limitations:
None in particular.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
KppBAWJbry;"REVIEW 
Summary:
The paper proposes a new attack called ""privacy backdoors"", which introduces backdoors into foundation models, making them more prone to leak fine-tuning data of a victim who is adapting the foundation model for their task. For this attack, the attacker collects a set of possible data points that might be used to fine-tune the model. The attacker then tries to inconspicuously alter the loss for the target data points such that they have an anomalous loss value. After the victim fine-tunes the model with private data, the loss of the target data points used for fine-tuning will be anomalous, allowing the attacker to tell whether they were used for training or not. The proposed approach is evaluated on vision models (CLIP) and on LLMs (GPT-Neo). In an ablation study, the paper shows that the attack is robust against different parameter-efficient fine-tuning and inference methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to understand
- The paper is well organized, and a reader who is not an expert in privacy attacks can follow the paper easily
- The proposed approach is novel
- The approach is technically sound, and extensive experiments were conducted to show that the proposed approach is working with different models, fine-tuning methods, and inference methods.

Weaknesses:
- My main concern is that the assumption that the attacker already has part of the training data (i.e., the target data points) is quite unrealistic. This setting assumes that the attacker has way more knowledge than in traditional membership inference attacks, where usually only similar but not the exact same data points are available. If the fine-tuning data is assumed to be the victim's private data, then it is not really private in the first place if the attacker can collect parts of this data set. As a result, it is not very surprising to me that after introducing the ""backdoor"", the model leaks more information about these data points the attacker had in the first place.
- I am skeptical that the proposed approach is a ""backdoor"". Usually, backdoors in machine learning have a trigger and produce a predefined output. However, with the proposed approach, we are basically just changing the loss of specific samples. The way it is done is ""stealthy"", but the methodology does not fully align with the definition of a backdoor as it is currently defined in the literature.
- I am not quite sure what the intention of section 2.2 is. At the beginning of the second paragraph, it is said that the presented method shares similarities with federated learning. But then only differences are brought up. So, for the reader, it is not clear what the similarities are to federated learning.
- It is a bit hard to judge the performance of the LLMs based only on the log perplexity loss. It would be nice to have some kind of benchmark similar to the accuracy in the vision model experiments.
- The authors state that maximizing the loss of target data points does not work for LLMs; however, no explanation or experimental results are given, which shows that it does not work.

Limitations:
The limitations are addressed. However, I would encourage the authors to discuss the potential impact of different data distributions on the proposed approach.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors focus on a new vulnerability that concerns pre-trained models which relies on an adversary modifying the pre-trained model in a way that increases the models vulnerability to membership inference attacks (MIAs). The attack is thoroughly evaluated on both vision-language models and LLMs when fine-tuning using different strategies and on different fine-tuning data sets. Furthermore, the authors conduct different ablations of the attack.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Originality: The work shows that SoTA MIAs (like LiRA) can yield better performance with an attacker that has not unrealistic amount of extra information or power. While it builds on-top of LiRA, it is quite clear that (loss-based) MIAs should benefit from this approach. Related work is appropriately discussed and cited.
- Quality: The paper is technically sound and shows through an appropriate amount of experiments that their proposed attack works on multiple SoTA models. The threat model is carefully described and provides the reader with enough information about potential weaknesses of the method. Some minor points I'll mention in the Weaknesses.
- Clarity: The paper is clearly written and I only have some minor suggestions under weaknesses that the authors can easily fix before a potential camera-ready version.
- Significance: The work is highly significant as nearly all current SoTA approaches rely on fine-tuning pre-trained models. As mentioned by the authors, libraries like Huggingface make pre-trained models more accessible and lower the threshold for downloading pre-trained models. At the same time it seems very likely that malicious models could be downloaded (e.g., when searching for a CLIP like architecture). I think this work makes very apparent that the community must defend against these types of attacks.

Weaknesses:
Major:
- Stealthiness of the attack: The paper does not look at how, e.g. the zero-shot performance on unrelated data sets (e.g., NOT the target data set) changes once the model is poisoned. The original CLIP paper (Radford et al., 2019) considers many different data sets. I understood that the authors argue that poisoning for better MIA on CIFAR-10 is stealthy because the performance on CIFAR-10 doesn't change much. But how well is the model still performing on other data sets such as CLEVER, FGVC Aircraft or others? Is there forgetting regarding that data? I see that the scenario breaks a bit down if that is the case because the adversary would need to poison specifically for one victim while hoping that nobody else uses the model and wonders why it performs poor in zero-shot. Eventually this would lead to the model being flagged and the model being taken down. I am willing to increase my score if this point has been addressed by the authors or if they clarify why this is not relevant.

Minor:
- Defense and Detection: The paper would be better if it could provide some ideas regarding potential defenses against this attack or methods to detect that the model includes a backdoor. I don't expect experiments but it would be great to elaborate a bit more than ""In the future, it may be necessary for those who make use of pre-trained models to perform as much (or more) validation of the pre-trained models that are being used as any other aspect of the training pipeline.""
- Table 3: It would be great to specify which model is being used in a separate column. It can be quite confusing as apparently the Linear Probing is only used with CLIP.
- Tables: It would be great if the caption could be elaborating a bit more than just a heading. E.g., by mentioning the model that is under attack.
- Pre-training data: It would be good to mention what pre-training data has been used for the pre-trained models. This can make quite a difference when replicating the results.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a so-called “privacy backdoor” attack. The attacker poisons a pre-trained model to make it susceptible to membership inference attacks (MIA) on an apriori known set of target examples. This poisoning is carried out by continually training the pre-trained model on the target examples and an auxiliary dataset (needed to preserve the base performance of the model), employing loss terms that later cause significant loss-contrast between target examples that are included in fine-tuning and that are not. The attack is empirically tested on vision and large language models, using (on a low level) opposite attack strategies. The evaluation presented in the paper shows that this privacy backdoor attack is effective at enhancing the performance of a prior MIA on both domains, and across various models, fine-tuning methods, and inference strategies.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper studies an important and timely threat. The practice of downloading and fine-tuning open-sourced pre-trained models is currently wide-spread and understanding the associated safety and privacy risks is crucial.

The poisoning attack appears to be highly effective in enhancing membership inference success under the examined setting.

The experiments extend to various fine-tuning and inference schemes, which could be employed by the victim and cannot be influenced by the attacker. The attack is robust in the provided MIA improvement across these scenarios, highlighting the severity of the posed threat.

Appendix C collecting negative results of failed attempts at constructing the attack is highly insightful and a refreshing sight given current publication practices.

Weaknesses:
**Novelty**

The paper claims in several places to introduce a “new” threat model and privacy attack, however, closely related [1] and virtually identical [2] settings have been proposed by other works. While the paper already briefly discusses [1], classifying it as concurrent work (available for slightly less than 2 months at submission time), it omits [2] (available for slightly more than 2 months at submission time). I believe that due to the large similarities in settings between these works, they warrant a longer discussion in similarities, differences, and concurrence in the paper; and the claim to unveiling a “new vulnerability” reassessed and tamed down in light of this discussion.

**Strong assumptions**

The paper reads currently as if the presented attack would be just a nice addition on top of any black-box MIA setting, and the presented game in Threat Model 2 makes it seem like it seamlessly integrates with the traditional MIA framework. However, I believe that this is misleading as the benefit from the introduced privacy backdoor attack is tied to assumptions that are stronger than those usually found in MIA literature.

In MIA, online and offline attacks are usually distinguished [3]. In online MIA (weak setting), the attacker is assumed to be able to adjust the computationally heavy part of their attack (e.g., retrain their shadow models) when the challenger reveals a new target data point to them. In offline MIA (strong setting), the attacker prepares an attack once, before knowing specific target data points, and then this attack is employed (sometimes with minor adjustments without virtually any additional compute costs) once the challenger presents target data points. Also note that another standard assumption is that the challenger is allowed to continuously present new target data points to the attacker as long as they are samples from a distribution that is also available for the attacker for sampling. The paper currently does not introduce these standard elements and assumptions of MIA.

Instead, the implicitly induced setting is in fact weaker than that of online MIA; the entirety of $D_{\text{target}}$ has to be known to the attacker when preparing the privacy backdoor. As such, if at MIA time the challenger presents a new target data point (which is allowed under usual MIA assumptions) the privacy backdoor has no “support” for it and adjusting the backdoor is not possible anymore, as the model on which the MIA is done is already released from the hands of the attacker and the fine-tuning has already happened.

Further, the attack requires that the attacker possesses a dataset $D_{\text{aux}}$ that is disjoint from the target data points, which, while in many cases may be realistic, is a non-standard assumption in MIA once again. Standard MIA does not require that the dataset available to the attacker and the set of all potential target data points is disjoint.

In summary, the paper makes several non-standard restrictive assumptions to the MIA setting, without discussing or motivating them explicitly and clearly; the assumptions are only stated in scattered places and not positioned in the context of MIA.

**Only partially follows best practices in MIA result reporting**

TPR@1%FPR is reporting at an order(s) of magnitude(s) higher FPR than suggested best reporting practices [3].

ROC-AUC score is included, while logarithmic full ROC curves are omitted, in stark contrast to suggested best practices [3].

As such, it is currently unclear if the attack provides as large benefits as currently perceived also at more relevant FPR regimes.

**Concerns over the employed MIA**

In the evaluation, the authors use the LiRA [3] attack with 16 shadow models. However, as it has been already shown in [3] and especially reinforced in [4], the standard LiRA attack is particularly weak for a low number of shadow models. While the version using global variance estimators is better in this regime, it is unclear which one was used for evaluation in this paper. As such, for this potentially weak attack the privacy backdoor provides a large benefit. However, it remains to be seen if the benefit is equally as large for stronger attacks, specifically tailored to perform well with just a low number of shadow models, such as [4] (available since 6th Dec 2023).

**Weak justification of the different attack strategy choices**

The paper currently employs two contrasting attack strategies for vision models and large language models. For vision models the loss of the target data points is increased in the backdooring phase (“maximization”), while for LLMs the target data points are encouraged to be heavily memorized (“minimization”). While both of these strategies are clear how they would encourage contrast at fine-tuning time between member and non-member target data points, the use of different strategies is currently weakly motivated, impacting the convincingness of the paper’s technical contribution.

The differing choices could be better motivated by showing an experiment how the alternative strategies perform on the other domain, i.e., showing how minimization performs for vision, and how maximization performs for LLMs. In particular, the attacks on LLMs seem to be much stronger, which is currently unclear if this is due to the chosen attack strategy, the evaluation protocol and datasets, or due to some other factor.

**Presentation, writing, and clarity**

In several places there are certain inconsistencies, writing is not clear, or small errors in citation formatting or similar. This gives the overall impression that the paper was written hastily.

In Threat model 2, point 3; to match with the generic setup of MIA, I assume that the idea what this point should stand for is (correct me if I am wrong) that the challenger randomly selects if it present a target data point from the target set which was also included in the fine-tuning set or one that was not. However, I think that the used notation currently does not reflect this:  1. “[i]f $c=\text{head}$, they randomly select a target data point $(x,y)$ from $D_{\text{target}}$” — this $(x,y)$ could still be both in the training set or not in it, this does not tell us anything about that; 2. “if $c=$tail, a target data point $(x,y)$ is randomly sampled from $(D_{\text{target}} \setminus D_{\text{train}})$” — this is confusing, as this would imply that $D_{\text{target}}$ is a superset of $D_{\text{train}}$, which is not only not stated anywhere nor followed in the experimental section, but would also mean another highly unrealistic assumption. I believe, sampling from $D_{\text{target}} \cap D_{\text{train}}$ when the coin is head, and sampling from $D_{\text{target}} \setminus (D_{\text{target}} \cap D_{\text{train}})$ when the coin is tail would be a correct notation/presentation.

Section 3.2. is very confusing at the moment, as it gives a clear motivation for the maximization attack, but then presents the exact opposite of this idea for LLMs, solely because ‘maximization did not work’. In conjunction with the experiment justifying this choice and an adjustment in writing would make the presentation of the attack more compelling.

While, as I already elaborated above, the assumptions made by the attack are rather strong, they could be justified by presenting a clear real-world example for the attack (but still have to be explicitly compared to the standard assumptions of MIA). While there is an attempt on this in l159-l162, I suggest to elaborate on this and present it more convincingly, given that the assumptions made are non-standard for MIA.

I struggle to understand the paragraph from l267 to l271. I especially do not get the causal link between observed memorization and what is meant by similar format and similar types of personal information. What the supposed link between the results on Simple PII and MIMIC-IV is, is also unclear. I do not understand why these two results are compared, as to me, the outlier seems to be the result on Simple PII, with a base attack success of 0.242 TPR@1%FPR, compared to an order of magnitude lower TPRs on both ai4Privacy and MIMIC-IV. Further, if the statement is supposed to be that PII is memorized better by the LLMs than images by the vision models, then for this the experiment setup in my view is unfit, as the MIA scores on the unattacked models are comparable in each case (each time two datasets produce around 0.0X TPRs and once 0.1X-0.2X TPR), and in case of the attacked models, as the attacks are different, we cannot know if the difference stems from the data domain or the attack itself.

As already elaborated in its own point, the setting of MIA and how the presented attack relates to it have to be presented clearer, as currently in parts it seems like the attack enables a more powerful setting; being an additive improvement over any MIA scenario.

Citep and citet are mixed up in certain places, e.g., l68 or l115.

**References**

[1] S Feng and F Tramèr, Privacy Backdoors: Stealing Data with Corrupted Pretrained Models. http://www.arxiv.org/abs/2404.00473. 

[2] R Liu et al., PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps. https://arxiv.org/abs/2403.09562. 

[3] N Carlini et al., Membership Inference Attacks From First Principles. https://arxiv.org/abs/2112.03570.

[4] S Zarifzadeh et al., Low-Cost High-Power Membership Inference by Boosting Relativity. https://arxiv.org/abs/2312.03262v3.

Limitations:
Limitations are not prominently and explicitly discussed, only recognized in the checklist. In my view, the strong assumptions made in the threat model have to be discussed in the main paper. Note also other weaknesses pointed out in my review.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a new privacy attack for foundational models like CLIP and large language models (LLMs). The attack's key idea is to ''poison'' the target data (e.g., maximize loss) point into the pretrained models so that the victim's finetuned models uploaded on the open-source platform can reveal what target data points have been used for finetuning.
In the end, the evaluation on both vision and language foundational models validate the attack effectiveness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ Important research topic: data privacy for foundational models
+ New attack setting and method

Weaknesses:
My main concerns are about the threat model and the evaluation of LLMs.

- Although the paper studies a new threat setting, existing model ecosystem may not work like the described manner. Specifically, the paper assumes the adversary can firstly finetune (i.e., poison) the publicly available pretrained model $F_{pre}$ to $F_{adv}$, and release the F_adv to the platform. Then the victim downloads and finetunes the $F_{adv}$ with D_train to $F_{adv, ft}$, and releases $F_{adv, ft}$ to the platform so the adversary can infer membership privacy from $F_{adv, ft}$. But why would the victim finetunes $F_{adv}$ instead of $F_{pre}$? Typically the victim would choose the $F_{pre}$ for finetuning, just like the authors choose the CLIP for evaluation instead of a random finetuned CLIP  on the Hugging Face.

- Besides, the attack goal of maintaining a comparable level of performance on downstream tasks does not persuade the victim to use $F_{adv}$ instead of $F_{pre}$. Let's assume $F_{pre}$ and $F_{adv}$ have similar performance. As $F_{pre}$ is shared by organizations (e.g., Meta, OpenAI, etc.) that can pay the training cost, the downloads and likes of $F_{pre}$ is certainly high as what we have witnessed in the era of LLMs, and $F_{adv}$ can be just one of hundreds of finetuned $F_{pre}$, why would the victim prefer $F_{adv}$, with potentially not much downloads and likes?

- The impact of finetuning to the LLM performance is also questionable. The results reported in Table 2 is lower validation loss after finetuning, but the loss cannot tell the model performance (i.e., low loss is not necessarily better). I would suggest the authors to provide more concrete LLM evaluation.

- The evaluation on large language models is not thorough. The largest evaluated LLMs in this paper is GPT-Neo-2.7B while widely studied LLMs are above 7B such as LLaMA, Mistral, etc.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
B7S4jJGlvl;"REVIEW 
Summary:
This paper proposes a way to incorporate LLM prompting to improve symbolic regression. They uses PySR, a standard symbolic regression library, as their base SR algorithm. Then they add LLM prompts to different SR algorithm steps: population mutation, crossover, and initialization. They replace the PySR implementation with LLM prompted implementations 1% of the time. The LLM prompts are based on identifying high level concepts and prompting the LLM to do its operation using the concept as a suggestion to follow. High level concepts are tracked using an abstraction prompt given high performing expressions, and are evolved using LLM prompting as well. The authors show that augmenting PySR with LLM concept-based prompting solves around 7/100 addition tasks from the AI Feynman SR benchmark, and show that PySR + LLM (LaSR) performs better on a synthetic task designed to test for data leakage.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Well written, great figures
- The framework for integrating LLM-based SR into PySR is well-designed, and could in principle work for other prompting approaches besides the concept-based SR. The LLM-mutation, crossover, and initialization steps could be replaced with other LLM based techniques. Cool!
- Using LLM's to learn concepts to guide SR is a well-motivated choice of prompting technique, given the importance of high level concepts for human equation discovery
- Based on the literature review comparing LaSR to two other LLM SR tools, LaSR seems like an original contribution.
- Given existing literature on program synthesis with library learning, LaSR is a great approach that bridges the gap a bit between SR and program synthesis, as done with modern tools.
- LaSR is also a good contribution to the growing body of work on library learning and its benefits for search. It is very similar to LiLO, which is built off DreamCoder, but applied to symbolic regression.
- The authors have a strong analysis of LLM incorporation based on (1) algorithmic cost in terms of millions of tokens per iteration, and (2) comparing GPT 3.5 with open source llama 8b on the results.

Weaknesses:
- It's not clear how valuable the concept abstraction approach is compared to some ""baseline"" of simple LLM prompting. For example, one baseline could just use a single concept ""This expression is a good formula for symbolic regression"" or something like that, and see how it compares. This could perhaps be a direction for future work: try a bunch of simple prompting strategies for combining LLM's with PySR, and report how well each of them work. 
- I'm not sure what to make of the synthetic dataset. In particular, PySR works so well on AI Feynman alone, but works very poorly without the LLM addition on this synthetic dataset. Why does LaSR work so much better than PySR here, but not help as much on AI Feynman? One hypothesis is that AI Feynman has a lot of easy tasks, and the synthetic benchmark only has tasks right on the edge of what PySR can discover, which LLM incorporation helps push over the edge. Another pessimistic take is that the synthetic benchmark is designed with LaSR in mind. While this still eliminates worries on data leakage, an explanation here would help understand better how LaSR is being helpful. 

I'd like to emphasize that including answers to these questions (both of which could suggest negative results) in the paper would not decrease my review score.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a method that learns a library of concepts (natural language description of helpful and unhelpful concepts) as a means of guiding genetic search for symbolic regression. The core idea is that such concepts can be used to bias genetic operations through an LLM.

The method was evaluated on the 100 Feynman equations and on a synthetic dataset. The paper also includes ablation studies on the various components of the system.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents a creative form of using LLM to speed up the search of genetic algorithms for symbolic regression. Instead of simply storing a library of programs, as GA algorithms do, the algorithm also stores a library of natural language concepts. Such concepts can be seen as abstractions of the population of programs encountered in search. Given in natural language, the abstractions can be used to drive the search as an LLM can be used to generate programs based on the description. Such a creative approach!

The idea presented in this paper is general and can be more broadly applied to other problems. I can already see how I could use a similar approach in my own research!

Another strength of the approach is the author's care with data contamination. Initially, I was skeptical of the approach as it uses LLMs to solve problems whose solutions are available online. The authors then explain that the way the LLM is used is unlikely to allow it to simply retrieve the solution from its training data. The explanation makes perfect sense since the LLM is used to extract concepts from programs the GA generates, and they can't encode the solution available online. In addition to this explanation, the authors also included an experiment on synthetic data showing the advantages of the learned concepts over the search alone. Nicely done!

I also enjoyed the fact that the system is built on top of PySR, which is a very efficient system for symbolic regression. This eliminates the possibility that all the gains the LLM provides could be easily washed with clever engineering. The current results already show that clever engineering alone is outperformed by the system.

Weaknesses:
The paper also has a few weaknesses.

**Claims that need to be fixed**

Some of the claims in the paper are a bit strong and I suggest toning them down. While the leakage explanation the authors provided is reasonable, I would be careful in claiming state-of-the-art performance. When writing ""LASR achieves a higher exact solve rate than all other baselines,"" it is worth mentioning the possibility of leakage.

Another claim that seems to be incorrect is the following: ""LaSR's increasing the backbone model size and the mixture probability significantly enhances."" I think the authors meant to write ""substantially"" and not ""significantly"" as there is no statistical test involved. I would also explain why these results are substantially better as the number of problems solved isn't much larger. The explanation I gave to myself is that solving each of these equations is very difficult, so solving one new equation is already quite an achievement.

Another claim that needs to be adjusted: ""demonstrating that LaSR's performance gains are not rooted in memorized responses."" The experiments on the synthetic dataset do not demonstrate this. The experiment with the synthetic dataset is almost independent of the experiment with the 100 Feynman equations. What the experiment demonstrates is that LaSR can outperform PySR even when data leakage is not possible.

**Discovering what is known**

Perhaps an unfair criticism of the paper is that the method it introduces is used to discover things we already know. I understand that the bar would be way too high and it would be unhealthy to the research area if we required the discovery of new things with the presentation of novel approaches to scientific discovery. So I do not make this criticism as a means of arguing for rejecting the paper (I think the paper should be accepted), but more as a reflection of what the community has been pursuing. The hope is that systems such as PySR and LaSR will eventually be used to make actual discoveries.

**Lack of Running Time**

I missed the running time of LaSR and PySR in the paper. How do they compare with LaSR making calls to an LLM?

Limitations:
The paper lists all limitations I could think of, either in the last section ""limitations"" or throughout the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work focus on symbolic regression. They enhaned the traditional method like genetic algorithms by inducting a library of abstract textual concepts. The algorithm, called LASR, uses zero-shot queries to a large language model to discover and evolve concepts occurring in known high-performing model to discover and evolve concepts occurring in known high-performing hypotheses. The algorithm can be seen as a kind of hybird of evolutionary algorithm and LLMs. Through experiments, LASR substantially outperforms a variety of state-of-the-art SR approaches.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This work proposed to introduce a concept library in symbolic regression, which is really similar to how human works, so the idea make sense. For the introduction of the library, this work also leverage abstrction or understanding ability of LLMs to design three phrases process, concept evolution, hypothesis evolution and concept abstraction. The design mix the strenghes of evolutionary algorithms and LLMs. 
2. The experimental results are good comparing to those other baselines of learning-based or evolutionary-based.

Weaknesses:
1. Introdution of LLMs would inevitably raise the cost for the task comparing those traditional algorithms.
2. (this could be a question) The introdution of concept library seems to not making sense in all cases, imaging that we are facing some totally unknown black-box environment, the backbone function could be something random or out of the knowledge we have. In this setting, the traditional evolutionary algorithms might perform better because they do not have this kind of knowledge as the constrant.

Limitations:
See weakness and question parts.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces LASR, a symbolic regression framework that enhances PySR by incorporating Large Language Models (LLMs) to discover and evolve ""concepts"" from high-performing equations. These concepts are then used to guide the search process. LASR is evaluated on the Feynman equations dataset and a set of synthetic tasks, showing improved performance over existing symbolic regression baselines.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea of integrating LLMs into symbolic regression to learn and use abstract concepts in terms of natural language is interesting. The methodology is well-structured  with clear explanations of the algorithm components.

Weaknesses:
My main concerns are regarding the evaluation. Experimental design and analysis are insufficient to convincingly demonstrate the method's advantages over existing approaches; specifically: There are serious concerns about potential data leakage and unfair advantage when using LLMs on well-known, simple physics equations that may be part of LLM training data. While there is a section on the data leakage validation, its limited evaluation scope and lack of comprehensive analysis on more complex and real-world datasets, makes it difficult to assess the true capabilities and generalizability of the method.

Limitations:
See above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
ziehA15y8k;"REVIEW 
Summary:
This work studies the problem of reconstructing attack policies using collected adversarial samples to enhance the robustness of GNN-based models in social network tasks, specifically rumor detection. The authors propose the MoE-BiEntIRL framework, which employs a mixture-of-experts approach to learn optimal policies from diverse adversaries, and provides feature-level explanations by estimating interpretable linear reward functions. Experiments on two real-world rumor detection datasets validate the effectiveness of MoE-BiEntIRL.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The authors investigate the rumor detection problem from the novel perspective of reconstructing attack policies.

2. The paper is well-written and well-organized, with motivating illustrations of the problem.

Weaknesses:
While the proposed problem and approach are generally novel and intriguing, the following issues regarding experiments require further clarification:

1. **Table 2:** What makes the policies on Pheme significantly harder to recover than the policies on Weibo?

2. **Table 3:** The results are not clearly illustrated and explained.
    -  For instance, it appears that the column under ""w/o Att."" reflects test accuracy (%), while results under other columns reflect accuracy decline in actual numbers. Please align the representations for consistency.
    -  If ""w/o Att."" refers to GCN's rumor detection performance without adversarial attacks, it is surprising to see that GCN only achieves ~70% test accuracy on the Weibo dataset with binary rumor / non-rumor labels. The authors claim that the Weibo dataset is adopted from existing work [1], which reported over 80% test accuracy on Weibo even using simple models such as TF-IDF or GRU. Please elaborate on the causes for this significant performance discrepancy, e.g, data differences and model structure differences.

3. **Computational Efficiency:** Given the complexity of the model structure illustrated in Figure 2, it would be beneficial to benchmark the computational efficiency of the proposed approach against the baselines in Table 3.

[1] Changhe Song, Cheng Yang, Huimin Chen, Cunchao Tu, Zhiyuan Liu, and Maosong Sun. Ced: Credible early detection of social media rumors. TKDE, 33(8):3035–3047, 2021.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel approach to enhancing the robustness of Graph Neural Networks (GNNs) against adversarial attacks, specifically in social media contexts such as rumor detection. The authors propose an enhanced maximum entropy inverse reinforcement learning (IRL) method with a mixture-of-experts approach to tackle multi-source graph adversarial attacks. This method aims to reconstruct attack policies, integrate various attack models, and generate additional adversarial samples to improve the robustness of GNN-based detection models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The application of inverse reinforcement learning to reconstruct adversarial attack policies is novel and offers a highly interesting perspective on enhancing GNN robustness.

Combined with the Mixture-of-Experts, the method allows for the integration of various attack models, providing comprehensive feature-level explanations and robust adversarial samples for use in adversarial training.

The generation of good additional adversarial samples for training improves the GNN’s resilience to attacks, which is a significant step towards robust social media analysis. 

The authors use real-world social media datasets to validate the proposed method.

Weaknesses:
The proposed method involves multiple components (IRL, mixture-of-experts, bidirectional updates), which can increase the computational complexity and may not be easily scalable.

The focus is primarily on rumor detection in social media, which, while important, might limit the generalizability of the method to other types of graphs and applications.

Some sections, particularly those involving the theoretical underpinnings of IRL and mixture-of-experts, could be more clearly explained to enhance understanding and accessibility.

No code is provided. This hinders the exact reproduction of results.

I think the authors use the term ""Threat model"" in an incorrect or at least unorthodox way that will likely be misunderstood in the security community and potentially beyond. Specifically, in line 229, the authors start a paragraph called “Threat model” and they proceed to describe that they use, GCN, number of hidden dimensions, optimizer, etc. This is not what is typically understood as a threat model in literature: a model of a threat actor's capabilities, possible courses of action they may take and how it will impact the operation of a computer system [1]. Speaking of it, including an actual threat model (or rather making the implicitly exiting model explicit) would certainly strengthen the paper and increase acceptance in the security community.

Minor issues:
Typos/grammar in lines 13, 69, 109, 111

[1] https://www.sciencedirect.com/science/article/abs/pii/S0167404818307478

Limitations:
The method assumes that the perturbations captured during training are representative of real-world adversarial attacks.

Further testing on different graph types and applications is required to confirm the broader applicability of the proposed method.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of adversarial attacks on Graph Neural Networks (GNNs) employed in social media tasks, such as rumor detection. The authors introduce MoE-BiEntIRL, a method that leverages a mixture-of-experts approach combined with inverse reinforcement learning (IRL) to reconstruct and explain adversarial attack policies. The objective of this method is to enhance the robustness of GNNs by generating additional adversarial samples for training, thereby improving resilience against attacks. MoE-BiEnt\IRL incorporates mechanisms for precise sample guidance and bidirectional updates, which are designed to optimize both the accuracy and the speed of policy learning.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Innovative Approach: The introduction of MoE-BiEntIRL represents a significant innovation, particularly through its application of a mixture-of-experts approach to manage diverse adversaries and provide detailed feature-level explanations. 
2. Real-world Validation: The method is validated on actual datasets from Weibo and Pheme, demonstrating its practical applicability for improving the robustness of GNNs in social media rumor detection scenarios. 
3. Experimental results, focusing on policy reconstruction and adversarial training, effectively illustrate the method’s robustness and efficacy.
4. The approach facilitates a deeper understanding of attack behaviors through feature-level explanations, aiding platform operators in enhancing system defenses.

Weaknesses:
1. The proposed method involves multiple stages and sophisticated mechanisms, potentially complicating its implementation. 
2. Scalability Discussion: The paper would benefit from a more extensive discussion on the scalability of the method, particularly concerning its applicability to large social media graphs. 
3. Experimental Setup Details: Enhancing the description of the experimental setup would significantly improve the reproducibility of the study and aid other researchers in replicating the results.
4. Typos and grammar errors could be avoided.

Limitations:
The paper's limitations are adequately addressed in the supplementary material. There are no further suggestions at this time.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel method, MoE-BiEntIRL, which combines a mixture-of-experts approach with inverse reinforcement learning to enhance the robustness and explainability of adversarial attacks on GNNs. The method addresses the critical issue of stabilizing GNNs used in social media for rumor detection, demonstrating significant practical relevance. Strengths include its innovative approach, comprehensive mechanisms for improving attack policy accuracy, and robust evaluation results. However, the paper could benefit from clearer explanations of the method, detailed parameter sensitivity analysis, enhanced experimental reproducibility, expanded comparative baselines. Despite these minor weaknesses, the overall contribution and practical importance of the research are compelling.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The MoE-BiEntIRL method presents a highly novel application of a mixture-of-experts approach combined with inverse reinforcement learning to address adversarial attacks on Graph Neural Networks (GNNs). This innovative approach stands out in its ability to not only enhance the robustness of GNNs but also to provide explainability to the attack policies.
2. The inclusion of precise sample guidance mechanisms and a bidirectional update mechanism demonstrates thoroughness in approach, aiming to improve both the accuracy of attack policy reconstruction and the speed of policy learning. This comprehensive approach adds substantial value to the proposed solution.
3. The evaluation methods employed in this study are robust, validating the effectiveness of the proposed method. The results are compelling, showing notable improvements in the robustness of GNNs.

Weaknesses:
1. Although the proposed method is innovative, some aspects of the algorithm could benefit from clearer explanations. 
2. A minor issue is that the sensitivity of the model to various parameters is not thoroughly explored. A brief analysis or guidance on parameter selection could aid in the practical application of the method.
3. While the method is novel, there is little discussion on its computational complexity. Including an analysis of the computational cost and suggesting optimizations could enhance the practical feasibility of the approach.

Limitations:
The authors have listed the limitations in appendix.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ziYC4FHRNr;"REVIEW 
Summary:
This paper is first to establish entrywise guarantees for low rank approximation of kernel matrices when kernel eigenvalues satisfy either polynomial or exponential decay. More specifically, in the $\alpha$-polynomial decay setting, entrywise error scales as $O(n^{-\\frac{\alpha-1}{\\alpha}} \\log n)$ for rank $d = \Omega(n^{1/\\alpha})$, while for $(\\beta,\\gamma)$-exponential decay error scales like $O(1/n)$ for $d > \\log^{1/\\gamma}(n^{1/\\beta})$. In order to establish such results, authors prove that eigenvectors corresponding to small eigenvalues are completely incoherent/delocalized i.e. have bounded entries of size $O(1/\\sqrt{n})$. Technical novelty stems from the fact that entries of the kernel matrix are dependent and have non-zero mean.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) This is a first result showing entrywise error guarantees for low rank approximation of kernel matrices. 

2) Proof sketches of two main theorems are clear and easy to follow.

3) Strongest technical contribution of this paper is proof given in Appendix D that, simply speaking, shows that the norm of projection of vector 1 on the subspace spanned by $n-d'$ eigenvectors with smallest eigenvalues is vanishing sufficiently fast.

4) Experiments are complementing theoretical results well.

Weaknesses:
1) Although authors claim that Lemma 1 is a novel concentration result, it seems to be only a slight generalization of Lemma 68 in Tao and Vu [2011], and is proved essentially using the same argument as that in the proof of Lemma 68. 

2) Although I appreciate proof sketches of Theorems 1 and 2 in the main text, I believe it would be more useful to add more information about the proof deferred to Appendix D since this is the most novel and interesting part of the proof.

3) It is not clear whether assumption (R) is necessary and how general it is apart from the two special cases given in Section 3.1.

Limitations:
The authors have addressed limitations adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper focuses on deriving entrywise error bounds for low-rank approximations of kernel matrices using truncated eigen-decomposition. It addresses the statistical behavior of individual entries in such approximations under assumptions of polynomial eigenvalue decay or exponential decay. The authors also provide empirical studies on synthetic and real-world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is clear and well written. The proof seems to be solid.
2. The entrywise error bound is new to the community. 
3. The assumptions on polynomial/exponential eigenvalue decay seem general and cover lots of common kernels.
4. Some statements about random matrix theory and concentration inequalities are provided (e.g., Lemma 1), which could be independently useful to the community.

Weaknesses:
1. The assumptions on the eigenfunctions corresponding to the assumptions of eigenvalue decay are hard to verify for general kernels, especially the part on the rate of decay ($\alpha >2r+1,\beta> 2s$). Moreover, I wonder if these inequalities are required to guanrantee the uniform convergence of the kernel (I note that $k(x,y)=\sum_{i=1}^{\infty}\lambda_i u_i(x)u_i(y)$ converges uniformly under these assumptions).  But in the proof I see these assumptions are used in a way like $\beta-s\ge \beta/2$ (e.g., Line 590). Thus, I am not sure if these asssumptions are necessary for derivation.
2. Assumption (R) seems not natural (why is $1\le a < b/16$ is needed?) and also I do not know how to verify this. Could you provide some examples with $\Gamma_i \neq 0$ under Assumption (R)?
3. The contributions are undetermined. The proof of the main theorem seems to heavily rely on past random matrix theory works (Tao and Vu [2011], Erdős et al. [2009 a,b]). With assumptions (E)/(P) and (R) and the previous works, the proof is straightfoward. And I am not sure about the importance of entrywise error bound.

Minor typos: 
1. Line 578/588 hypotheses-> hypothesis
2. Line 539/581 miss a period

Limitations:
There is no negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the kernel matrices, formed by $n$ vectors i.i.d. drawn from a $p$-dimensional probability distribution $\rho$. Under several assumptions on the associated kernel operator on $L^2_{\rho}$, including the positive definiteness of the kernel and decay condition on the eigenvalues of the kernel, the authors prove an estimate on individual entries of the matrix kernel and those of the low-rank approximation of the kernel. Numerical experiments on the estimate error are done with both synthetic datasets and real-world datasets.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The problem is a very fundamental one and it is considered both analytically and numerically.
- The writing is very clear and easy to read.

Weaknesses:
- Lemma 1 is wrong, and thus the proofs of the main results do not work. 
Consider an extreme case where $a=0$ with probability $1$. Then, since $\pi$ is an orthogonal projection, $\| \pi_H(a) \| = 0$ and thus Lemma 1 fails. The main issue is that in the proof of Lemma 1, if $S_1 = \sum p_{ii} (\xi_i^2 - 1)$, then $E[S_1^2] = \sum_{i, j} p_{ii} p_{jj} E[\xi_i^2 - 1] E[\xi_j^2 - 1]$, which is different from $\sum_i p_{ii}^2 E[(\xi_i^2 - 1)^2]$ in (17), unless $E[\xi^2]=1$. As a result, (17) and the estimates on $P(E_+)$ and $P(E_-)$ fail.
-> The proofs of the main results would work after modifying Lemma 1 as suggested by the authors.

Limitations:
The work does not seem to have potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zgh0ChWocO;"REVIEW 
Summary:
This paper introduces a new way to balance multiple rewards with some long-term rewards potentially missing. It does so by using Pareto Policy Learning of optimizing each reward subject up to the tradeoff frontier. This can be more practical than simple linear weighting since the linear weighting strategy applies the constant weight regardless of the amount of conflict between pairs of objectives. Empirically, the papers show that the approach is superior to linear on two synthetic tasks with some real data. Overall I think the paper is promising and adding more realistic empirical evaluation can add values to the current state of the paper.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Learning to combine multiple rewards is an important and well-motivated question, and has wide ranging implications.
- The method proposed is mathematically sound. The paper shows theoretically that the input parameters can be interpreted as a form of worst case value on each objective. 
- The paper explains how the most popular approach of linear weighting can fall short, derives the method through first principles, and empirically demonstrates that the proposed method is superior.

Weaknesses:
- The main weakness of the paper is that the experimentation is rather limited. The experiment uses partial real data with synthetic generation of short-term and long-term rewards. For example, in robotic planning, the authors could show how their approach helps balance the long-term reward (e.g. goal reaching) / short-term reward (e.g. minimizing jerk). This is just an example, but including other more real-world planning and RL problems would seem beneficial.
- It seems that compared to linear weighting, the proposed method seeks more short-term reward but is not necessarily better in terms of long-term reward. It may not be a weakness but reading the table does strike me that the method is more “short-sighted.”

Limitations:
- The limitation of the overall framework are mentioned but there is no much detail perhaps due to space constraint.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper attempts to address the challenge of learning the optimal policy for balancing multiple long-term and short-term rewards. The authors point out that the existing linear weighting method leads to a sub-optimal policy. To address this limitation, the authors propose formulating formulate the problem as a multi-objective optimization problem. They utilize the Lagrange algorithm to use preference vectors to solve the formulated multi-objective optimization problem and aim to learn the policy to meet Pareto optimization. In order to decide the preference vectors, the authors propose establishing the connection between the optimization problems and the ε-constraint problem. Experiments on IHDP and JOBS demonstrate the efficacy of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The multi-object problem is practical in both reinforcement learning and other optimization scenarios. The paper provides a good summary of the limitations of the existing linear weighting method and introduces a novel perspective on solving the problem by resorting to the Lagrange algorithm and Pareto optimization.
2.	The author has a solid mathematical foundation and is able to provide detailed mathematical descriptions and solutions to the proposed optimization problems.

Weaknesses:
1. The authors point out that the linear weighting method is suboptimal. However, there is no explanation in the method section or corresponding experiments to demonstrate that the proposed method (i.e. DPPL) is optimal.

2. In line 38, the authors claim that when some of the rewards are interrelated, the linear weighting method can only achieve a suboptimal solution. The claim may not be rigorous as the linear weighting method might be able to model the relationship among the rewards. More explanation and experiments are required.

3. In line 95, the definition of Pareto optimality, the condition for Pareto optimality by the author is to find the $\theta$ that makes all $\bar{\mathcal{V}}$ optimal. However, is it possible that the $\theta$ is not optimal for some $\bar{\mathcal{V}}$ but is optimal for the overall $\bar{\mathcal{V}}$?

4. Some mathematical symbols and proprietary terms in the paper are not explained clearly. For example, what does the $e$ in line 110 mean? What does MOP represent? Does MOP represent multi-objective problems? What do $v$ and $R_{+}$ mean in line 171? What does the KKT condition mean? Is it the KKT condition in the Lagrange algorithm? What is the difference between the two descent directions $d_{rt}$ and $d_t$? There are many similar situations in the paper. I suggest providing necessary explanations for each noun and symbol that appears for the first time.

5. In section Simulating Output and section Experimental Details, many parameters are defined by the authors themselves, but most of them do not have reasons or ablation experiments. For example, why is the number of preference vectors 10? In Line 253 to Line 254, why are some parameters truncated normal distributions and some Gaussian distributions?

6. In Table 1 on the L-REWARDS metric, the proposed method is comparable to the linear weighting method. However, the authors claim that for most of the preference vectors, DPPL's solutions have better performance.

7. In Figure 1, it seems that the effect on the $\delta{w}$ from the missing rate and T is not obvious for either the proposed method or LW. More explanation is needed.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the tradeoff between short-term and long-term rewards. The authors formulate the policy learning problem as a multi-objective optimization problem and propose a decomposition-based Pareto policy learning method. I only had experience in reinforcement learning in robotics five years ago. I tried my best to understand the paper, but I am not sure about my rating and comments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper studies a quite interesting and important problem, and the proposed methods seem effective on these two benchmarks.
- The paper is well-organized, the division is relatively easy to follow, and the proposed method is well-motivated.

Weaknesses:
- Only the linear weighting method is used as the baseline. I am wondering if there are any other methods that can be used for comparison. If not, why? Since both IHDP and JOBS are widely used.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a framework for solving multi-objective optimization problems: multi-objective optimization problems are divided into sub-problems in different regions by setting different preference vectors. The parameter optimization direction of the sub-problem can be easily solved by transforming it into a dual problem through the KKT condition, and a Pareto optimal solution of the original problem can be obtained by solving the sub-problem. This paper uses this framework to balance the optimal strategy learning under multiple short-term rewards and long-term rewards and achieves better and more stable performance than the traditional linear weighted method in the constructed experimental environment.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper reveals in detail the connection between the proposed method and the linear weighted method and the epsilon-constrained optimization method. Based on this connection, the epsilon-constrained optimization method can provide interpretability for the method in this paper.
2. The method in this paper theoretically overcomes the suboptimality problem of the linear weighted method and avoids the situation where the epsilon-constrained optimization method does not have a feasible solution.
3. This paper obtains better and more stable results than the epsilon-constrained optimization method in the optimal strategy learning problem under multiple short-term rewards and long-term rewards constructed by the author.

Weaknesses:
1. This paper mainly proposes an important multi-objective optimization algorithm and compares it with two existing algorithms in theory. However, the title of this paper seems to be just a specific application scenario of the algorithm. In what other scenarios can this algorithm be applied?
2. The experimental part is mainly conducted in a constructed environment, and it is unclear how difficult it is in the field of causal inference.
3. The v in line 171 is missing \bar. In Appendix B, t in line 5 of Algorithm 1 should start from 0.

Limitations:
Have addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zeaBrGv7Ll;"REVIEW 
Summary:
This paper presents a diffusion-based video super-resolution method, and proposes Instance-Centric Alignment Module and Channel-wise Texture Aggregation Memory. The former leverages a pre-trained open-vocabulary segmentation model (i.e., OpenSeeD), which is utilized to perform alignment within video clips by modulating the spatial and temporal features. The latter leverages channel-wise attention and memory mechnism to better super-resolve the video frames. The results on publich benchmarks indicate that the proposed method achieves state-of-the-art perceptual performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method achieves state-of-the-art perceptual results on REDS4 and Vid4.

Weaknesses:
Although the proposed method achieves promising results on the public benchmarks, there are some concerns that greatly affect the rating of this paper.

1. The presentation of the method needs to be improved. The readability of the paper is unsatisfactory. The technical details and the rationale behind it is not clearly described and explained.
(a) The main figure (Figure 1) is ambiguous. It is hard to understand the workflow of the framework based on this figure. It is also hard to see the connection among different modules.
(b) In the abstract, what is the ""conditional video generation"" (L6)? I do not see any pre-trained conditional video generation module in the described method. Maybe it should be rephrased.
(c) In L206-207, what is the role of ""randomly initialized tokens""? And what is specific role of the encoder-decoder module?
(d) In L187-188, are the ""semantic tokens"" actually text embeddings ? What is the difference?
(e) In L223, how to divide channels into different groups and what is rationale behind it?
(f) It is hard to understand Eq. (16), (17) and (18). From (17) and (18), it seems T_j is used to calculate itself, which is confusing.
(g) The choice of mathematical notations is sub-optimal and confusing.
(h) In L149, I think the ""belta_t"" should be ""yita_t"".

2. The novelty of this paper is limited.
(a) Some of the modules are based on existing methods. For example, the way of introducing semantic features is similar to SFT (but no comparison in the paper); the multi-frame self attention is from [21].
(b) The proposed blurring ResShift is a modification version based on ResShift, but the rationale behind it is not fully explained. Also, there is no direct ablation.

3. The comparison with other related methods are not thorough. 
(a) The authors should explicitly compare with ResShift [33], since residual shifting technique is also exploited (but no citation in L48). Also, there is no comparison with it in Sec. 2.2.
(b) The authors should compare with Upscale-A-Video [36], another diffusion-based video super-resolution method. Also, it is recommended to compare the performance of [36].
(c) The authors should compare with SFT[28], another method also leveraging semantic segmentation information.

4. The proposed method is not fully ablated. There is no direct ablation for exploitation of DWT and blurring resshift.

5. Some of the statements could be inappropriate. 
(a) In L35-36, I think it is hard to reach the given conclusion from [8]. Please elaborate.
(b) The naming of ""semantic distiller"" could be inappropriate. The pre-trained semantic segmentation model is directly leveraged and frozen. I don't see any distillation.

Limitations:
The authors have adequately addressed the limitations in Section D.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose SeeClear for Video Super-Resolution (VSR). SeeClear is a diffusion-based method that improves restoration performance by introducing semantic priors. The authors design an Instance-Centric Alignment Module (InCAM) and Channel-wise Texture Aggregation Memory (CaTeGory) to utilize semantic information effectively. Comparisons on multiple datasets demonstrate that the proposed method achieves state-of-the-art performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper introduces semantic priors to achieve spatial modulation and temporal correlation, improving diffusion-based VSR performance. This idea is both reasonable and effective.
2. The authors design the Instance-Centric Alignment Module (InCAM) to align using semantic information, avoiding pixel inconsistencies and being well-suited for diffusion models.
3. Additionally, the authors propose the Channel-wise Texture Aggregation Memory (CaTeGory) to transfer semantic information between different frames.
4. Comparisons with state-of-the-art methods demonstrate the effectiveness of the proposed method.
5. The paper is well-organized, with clear and aesthetically pleasing layouts, figures, and tables.

Weaknesses:
1. The method uses pre-trained models to extract semantic information, introducing significant additional computation, which limits the method's applicability. Meanwhile, the paper lacks comparisons of complexity and parameter counts.
2. The method lacks experimental support for some critical hyperparameters, such as the choice of k in InCAM and the number of frames used in SR.
3. The paper proposes using wavelet transform to improve UNet but lacks experimental justification for why simple downsampling and upsampling wouldn't be more efficient.
4. Figure 1, while aesthetically pleasing, is challenging to understand. It would be better to clearly explain the network structure (e.g., Figure 8) and the inference process.

Limitations:
The authors discuss the method's limitations and societal impacts.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel video super-resolution framework leveraging semantic distillation to enhance pixel condensation in diffusion-based models. SeeClear addresses stochastic fluctuations by using a Semantic Distiller and a Pixel Condenser to extract and upscale semantic details from LR frames. The framework includes an Instance-Centric Alignment Module and a Channel-wise Texture Aggregation Memory to improve temporal consistency and visual quality. Experimental results demonstrate SeeClear's superiority over state-of-the-art diffusion-based VSR techniques.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The combination of semantic distillation and pixel condensation is novel and effectively addresses the challenges of maintaining detail consistency across frames in diffusion-based VSR.
- The Instance-Centric Alignment Module (InCAM) and Channel-wise Texture Aggregation Memory (CaTeGory) significantly improve short-term and long-term temporal coherence.
- The paper provides extensive experiments to demonstrate SeeClear's advantages over sotas across multiple benchmarks.

Weaknesses:
- Lack of computation analysis. Diffusion-based methods are often criticized for unbearable inference time, so it would be better to list params, runtime, and FLOPs/MACs for a fair comparison.
- Lack of an ablation study on the wavelet transform which is introduced in Section 3.1.
- Table 2 is incomplete, making it difficult to assess the effect of the CaTeGory.
- The Other baselines such as VRT and IconVSR are also evaluated on Vimeo-90K-T and UDM10 datasets. Could you complete it for a fair comparison?
- Figure 7 needs more explanation.

Limitations:
The authors have largely addressed their limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a framework for video super-resolution (VSR) that improves temporal coherence and high-resolution detail generation. The proposed method, SeeClear, integrates a Semantic Distiller and a Pixel Condenser to extract and upscale semantic details from low-resolution frames. The framework employs an Instance-Centric Alignment Module (InCAM) and Channel-wise Texture Aggregation Memory (CaTeGory) to enhance inter-frame coherence and incorporate long-standing semantic textures. The methodology also introduces a blurring diffusion process with the ResShift mechanism to balance sharpness and diffusion effects. Experimental results show that SeeClear outperforms state-of-the-art diffusion-based VSR techniques in terms of perceptual quality and temporal consistency.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The SeeClear framework introduces a combination of semantic distillation and pixel condensation, which significantly enhances video super-resolution.
2. The Instance-Centric Alignment Module (InCAM) and Channel-wise Texture Aggregation Memory (CaTeGory) improve the temporal coherence of the generated high-resolution videos.
3. The integration of blurring diffusion with the ResShift mechanism effectively balances sharpness and diffusion, leading to high-quality detail generation.

Weaknesses:
1. While the method demonstrates robust restoration capabilities, it may still struggle with accurately restoring tiny objects or intricate structures, especially under severe degradation conditions.
2. The method has been tested primarily on specific benchmark datasets. Its performance in real-world applications, where video degradation processes are more varied and unpredictable, remains to be thoroughly evaluated.
3. The experiments are not sufficient and should be improved.

Limitations:
Please refer to the details above.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
zeYyq0GpXO;"REVIEW 
Summary:
This paper disentangles positional vectors from the hidden states of a pretrained Transformer language model to facilitate the understanding of length extrapolation. After a series of analyses, this paper proposes two context extending techniques. Experiments show that the proposed methods lower the perplexity on the task of language modeling.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
It's always good to have a mechanistic interpretability view of the hidden states of language models. The findings presented in this paper might inspire follow-up work along this direction.

Weaknesses:
The experiments presented in the current draft are not convincing enough to me. See questions below.

Limitations:
See above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a mean-based decomposition technique to analyze the formation and effect of positional encodings in LLMs. It then uses these results to propose methods to extend the context window, resulting in models that generalize better to longer texts.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper is very well-written, and the main findings are properly highlighted.

2. This paper not only explains how positional vectors are formed, but also introduces methods to interpolate them based on the findings.

3. Experiments are performed to show that the new methods result in better perplexity scores beyond the context window.

Weaknesses:
I believe this contribution is novel and insightful enough, and there is no apparent weakness.

Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper dives into the inner workings of how transformer-based language models handle positional information. By decomposing hidden states into semantic and positional vectors, the authors give a series of analysis about how the positional information are encoded and propagated through layers. I believe this work offers valuable insights for understanding the positional information within the transformer architecture.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Very detailed and clear analysis about how the positional information is encoded and propagated within the transformer architecture, and to the best of my knowledge, I have not seen similar work before. I particularly enjoyed reading Figure 2 and 3, which shows how positional information is propagated through layers and goes beyond the window size, and shows how the manipulation of the positional embedding causally influence the attention patterns, particularly removing the attention sink.

Weaknesses:
There are few points that I would like to suggest here to make the paper even stronger.

- Section 4 feels weak and unnecessary. The performance of replacing the positional vector, if my understanding is correct, seems to be much worse than Dynamic NTK. Given the current mainstream approach is modifying the base of Rope (like YaRN), which is much easier than the approach proposed by this work, I do not think this work’s proposed context extension will be accepted by mainstream model builder.
- That being said, I think the in-depth analysis of the positional embeddings are strong enough for me to give an acceptance (I learned a lot from it), so **I would strongly suggest removing the content of section 4, and use its space for more experimental analysis of the positional vectors**

There are a few important problems that I believe will receive the communities’ attention and worth being addressed:

- Although this paper shows the positional information can propagate through layers (Figure 2), in practice, many work found that models with window attention cannot pass the needle in a haystack test, and this is why Mistral 1.5 changed its attention back to full attention. It would be insightful if the authors can discuss the relationships between positional information and needle-in-a-haystack performance (because needle in haystack is what makes long-context models useful), i.e., why window attention cannot pass needle in haystack even it does have the correct positional information?
- This paper’s analysis is restricted on TinyLLaMA, but TinyLLaMA is not a widely used open-source model, thus casting the doubt whether this discovery of this paper will hold for other model families, particularly mainstream open-weight models like LLaMA 3, Mistral, QWen or Yi. I would strongly suggest the authors verify the behavior of positional embedding on either LLaMA 3, Mistral, QWen, or Yi.

Currently I’m given a borderline accept, and I will consider increasing my scores if the authors could either (1) discuss the relationship between positional vectors v.s. needle-in-a-haystack or (2) verify that the properties of positional vectors hold for LLaMA 3, Mistral, QWen or Yi (any 2 out of the 4).

Limitations:
see the above weakness section

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zcEPOB9rCR;"REVIEW 
Summary:
The paper introduces the Geometric Diffusion Bridge (GDB), a novel framework designed to generate the evolution of geometric states in geometric (coordinate) systems. GDB uses a diffusion bridge connecting initial and target geometric states with equivariant transition kernels, preserving symmetry and joint state distributions. Furthermore, GDB can use a chain of equivariant diffusion bridges to leverage trajectory data for more accurate dynamic modeling.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The presentation of theorems in Section 3.1 is clear and straightforward, establishing a solid theoretical foundation for GDB. The authors effectively derive theorems and integrate them with point cloud states.
- GDB demonstrates strong performance across various tasks, including QM9, Molecule3D, and OpenCatalyst IS2RS.

Weaknesses:
I have no complaints regarding the technical and experimental sections, as they are well-written. However, I wonder existing works, such as [1] and [2], also use diffusion bridges over molecular data. What advantages does your approach have over theirs?

[1] Diffusion-based Molecule Generation with Informative Prior Bridges. Lemeng Wu, et al. NeurIPS 2022.

[2] DiSCO: Diffusion Schrödinger Bridge for Molecular Conformer Optimization. Danyeong Lee, et al. AAAI 2024.

Limitations:
The authors note the need of exploring better implementation strategies for their framework to enhance performance.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a generative model for bridging initial and target geometric states using diffusion bridge. This work introduces an equivariant diffusion bridge based on equivariant transition kernels for symmetry constraints. The proposed method was validated on diverse settings including simple molecules and adsorbate-catalyst complex, outperforming previous MLFF baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The motivation of using diffusion bridge to bridge initial and target geometrical states is reasonable. 
- Using diffusion bridge model for equilibrium state prediction and structure relaxation is novel to the best of my knowledge, and the paper shows that GDB significantly outperforms previous methods with diverse datasets.
- Equivariant design of bridge process is based on solid theory.
- The paper is well written except for some missing relevant works on diffusion bridge.

Weaknesses:
- Related works on diffusion bridges or diffusion mixtures were not discussed. Diffusion bridges has been studied in [1,2,3,4] with applications to molecules, graphs, point clouds, and images, and more recent works have studied general framework for diffusion bridges [5, 6] which is worth discussing. While GDB has a contribution for using diffusion bridges in new tasks, discussing related works and clarifying the novel contributions is necessary in particular for strengthening the contribution of this work.
- Contribution seems limited as using diffusion bridge as generative modeling was already studied [1,2,3,4], in particular deriving diffusion bridges using Doob's h-transform. Designing an equivariant diffusion process (not necessarily bridge) specifically in SE(3) group has been covered in [7,8, 9]. What is the difference of designing equivariant diffusion bridges compared to equivariant diffusion processes?

[1] Peluchetti, Diffusion Bridge Mixture Transports, Schrodinger Bridge Problems and Generative Modeling, JMLR 2023
[2] Liu et al., Learning Diffusion Bridges on Constrained Domains, ICLR 2023
[3] Wu et al., Diffusion-based Molecule Generation with Informative Prior Bridges, NeurIPS 2022
[4] Jo et al., Graph Generation with Destination-Predicting Diffusion Mixture, arXiv 2023 
[5] Albergo et al., Stochastic Interpolants: A Unifying Framework for Flows and Diffusions, arXiv 2023
[6] Shi et al., Diffusion Schrodinger Bridge Matching, NeurIPS 2023
[7] Xu et al., GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation, ICLR 2022
[8] Xu et al., Geometric Latent Diffusion Models for 3D Molecule Generation, ICML 2023
[9] Yim et al., SE(3) diffusion model with application to protein backbone generation, ICML 2023

Limitations:
While the paper discusses future direction for the proposed method, specific limitations of the work is not specified. One potential issue might the scalability of GDB as the model has transformer architecture, and other issue could be long inference time which is a typical problem of diffusion models.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a type of diffusion model that captures the evolution of geometric states. The model is characterized by a diffusion SDE that couples the initial state with the target state, in the middle of which trajectory guidance is enabled when such data present. The framework is designed to yield equivariant density similar to other geometric diffusion models. Experiments on equilibrium state prediction with or without trajectory data have been performed to verify the applicability of the proposed approach.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The distinction between existing works has been elaborated in Table 1, which is clear.

2. The method is designed with an option to leverage additional trajectory data, which is quite interesting.

Weaknesses:
1. The experimental setup and comparison with baselines on equilibrium state prediction is a bit troublesome which requires more clarification or additional comparisons. Please refer to Q1.

2. The presentation is a bit unclear. Please refer to Q2.

3. Additional baselines may be considered. The baselines selected in the paper are not closely connected to the proposed approach. See Q3.

4. Missing ablation studies. In the current shape it is unclear where the performance gain comes from. See Q4.

Limitations:
There seem to be no discussions on limitations in the paper. It would be better to discuss potential limitations from perspectives such as scalability and sampling time.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors introduce a Geometric Diffusion Bridge (GDB) framework, which aims to predict the evolution of geometric states in complex systems accurately, crucial for fields such as quantum chemistry and material modeling. Traditional methods face computational challenges, while deep learning approaches lack precision and generality. The authors use Doob’s h-transform to construct an equivariant diffusion bridge. By applying Doob’s h-transform, the authors adjust the SDE to ensure that the process starts from an initial geometric state and is conditioned to reach a target geometric state. This ensures that the transformed process respects the symmetry constraints of the geometric states, leading to more accurate and physically meaningful predictions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ The framework utilizes an equivariant diffusion bridge derived from a modified Doob’s h-transform. This ensures that the diffusion process respects symmetry constraints, making the predictions more robust and reliable.
+ The paper provides a theoretical framework analysis about preserving symmetries and accurately modeling evolution dynamics.
+ Experimental evaluations show that GDB is better than state-of-the-art approaches in various real-world scenarios, including equilibrium state prediction and structure relaxation tasks.
+ The framework achieves significant error reduction compared to strong baseline models, particularly in challenging tasks such as structure relaxation in the Open Catalyst 2022 dataset

Weaknesses:
- The framework, especially when leveraging trajectory data, might introduce significant computational overhead. The simulation-free matching objective is designed to be efficient, but the overall framework’s computational demands might still be high
- Some mathematical notations and definitions in the paper could be made clearer. For instance, explicitly defining all variables and functions used in the modified Doob’s h-transform and constructing equivariant diffusion bridges would improve readability and understanding.

Limitations:
No limitations are addressed in the paper by the authors

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zaXuMqOAF4;"REVIEW 
Summary:
This paper introduces a new LLM length extrapolation method, called Mesa-extrapolation, which utilizes a chunk-based triangular attention matrix and applies stair PE. The proposed method is based on theoretical analysis. The paper conducts extensive experiments on passkey, PPL, summarization to demonstrate the effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a theoretical analysis to prove the effectiveness of meticulous weave position with PE for length extrapolation.
2. The proposed method is efficient and is proved to be effective through extensive experiments.

Weaknesses:
1. The passkey retrieval experiment is simple, good performance on the passkey is far from a real usable context window. Please consider to add evaluations on Ruler[1] and RepoQA[2]

2. The achieved context window is limited. 



[1] https://arxiv.org/abs/2404.06654

[2] https://evalplus.github.io/repoqa.html

Limitations:
See the weakness and question sections.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper conducts a theoretical analysis to help understand the No Position Encoding. Also, the paper proposes weave position encoding to achieve improved extrapolation performance without additional cost. Also, the paper introduces the weave PE method, Mesa-Extrapotion, which recalculates the position ID to reduce the gap between training and inference. Finally, the paper conducts experiments to prove the effectiveness of Mesa-Extropoation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The presentation of wave PE, Star PE, and Mesa-Extrapolation is clear. The author also provides the details of wave PE, Star PE and Mesa-Extrapolation to help understand the concepts
* The author conducts experiments to prove the effectiveness of the proposed Mesa-Extrapolation.
* The author also further analyzes the Latency & Memory Usage of the proposed Mesa-Extropoaltion.
* The paper discusses the limitations for further discussion.

Weaknesses:
**Major Concerns**: It seems that the proposed method Star PE is the same as Self-Extend LLM [1]. If possible, I sincerely hope that the author can address the following concerns:
* **Concern 1**: The Figure 1 Star PE implementation result does not match the Equation proposed in Section 4.1 Page 5. In Figure 5, when t-i is 5, the implementation result of Star PE is 4. However, according to the Equation proposed in Section 4.1 Page 5, the implementation result should be N+ $\lceil (t-i-N)/E \rceil$=4+$\lceil (5-4)/2 \rceil$=5. Hence, to match the implementation result of Figure 1, the Star PE calculation equation should be N+ $\lfloor (t-i-N)/2 \rfloor$.

* **Concern 2**: The Equation of Star PE is almost the same as Self-Extend LLM. When t-i is small than N, both Star PE and Self-Extend LLM employ normal relative distance. When t-i is larger or equal to N, we discuss it below.
   * The Equation of Star PE is N+ $\lfloor (t-i-N)/E \rfloor$ (as shown in Figure 1), while N is called the extrapolated position and E is called the extrapolated width, and t-i is the relative distance. 
   * The Equation of Self-Extend LLM is $(t-i)//W + (W- W//G)$=$W+ (t-i)//G - W//G$, while W is called neighbor window size and G is called group size. Apparently, when W%G==0, the Equation of Self-Exntend LLM becomes $W+ (t-i)//G - W//G$=$W+ (t-i-W)//G$= $W+\lfloor (t-i-W)/G \rfloor$. Then, we change the notation W to N and the notation G to E, we have N+ $\lfloor (t-i-N)/E \rfloor$, which is the same as Star PE.
* **Concern 3**: If possible, could the author compare the performance between Mesa-Extropolation and Self-Extend LLM?
* **Concern 4**: when the output sequence length $L_{generate} \gg L_{input}$, will the time cost also becomes O($L_{generate}^2$)?

Based on the above concerns, the paper may need to rethink the major contribution. The proposed Mesa-Extrapolation seems to make sense and may benefit society, while the paper should clarify its original contribution.


Reference:

[1] Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C. Y., ... & Hu, X. (2024). Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325.

Limitations:
Yes, the authors have addressed the limitations. We may further discuss and analyze the Mesa-Extropolation in other areas.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the length extrapolation of LLMs.
1. It provides a theoretical analysis of why NoPE and PE fail to extrapolate beyond a certain length. Previous work has shown that this failure is related to the explosion of hidden states as positions increase. This paper demonstrates that both NoPE and PE suffer from this hidden state explosion, using a constructive approach to illustrate the existence of Transformer weights.
2. It proposes weave PE, a simple adaptation of PE that theoretically addresses the extrapolation issue. It also provides a simple implementation of weave PE, using a chunk-based triangular attention matrix. Then, it demonstrates that the proposed extrapolation scheme matches the performance of prior length extrapolation methods, such as Dynamic-NTK.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- Great theory explains the failure of NoPE and PE in length extrapolation.
- Proposes weave PE, derived from the theoretical analysis, which also works well in practice.
- Shows good empirical results in passkey retrieval, language modeling, and summarization.

Weaknesses:
1. Methodological comparison with $\Lambda$-Attention

The proposed Stair PE resembles the $\Lambda$-attention of LM-Infinite & Streaming-LLM, yet with differences in 1) the additional attention at the bottom, and 2) a different length extrapolation scheme, Meta-Extrapolation. In the experiments, Meta-Extrapolation significantly outperforms LM-Infinite & Streaming-LLM. Could the authors provide the intuition behind these empirical gains?

---
2. Empirical comparison with Dynamic-NTK

Dynamic-NTK outperforms Meta-Extrapolation on the summarization task for mid-lengths of 7-11k, while Meta-Extrapolation shows better performance on summarization for shorter lengths of 4-6k and better language modeling fluency for lengths greater than 11k. Could the authors provide the intuition behind these results?

---
3. Relation between input sequence length $T$ and effective length $M$

The theorems only show the existence of an effective length $M$, but do not provide intuition on the scale of $M$, such as the ratio over the input length $M / T$. Could the authors provide some intuition on this? If I understand correctly, $M$ is set from the construction of the Transformer weights, so can it be controlled to an arbitrarily large number?

---
Editorial comments

- The fonts of the figures and tables are too small. Please make them more readable.
- Some parts of the writing are mechanical. For example, lines 116-120 do not provide meaningful information. It would be great to discuss the implications of the theorems in natural language. For instance, both theorems state the failure of length extrapolation in NoPE and PE, rather than just ""revealing the internal mechanism of extrapolation.""

Limitations:
Well discussed. Extending this approach to fine-tuning would be an interesting next step.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a positional embedding scheme to address the extrapolation issue: train on short sequences, evaluate on longer sequences. Authors propose a theoretical framing of the positional embeddings contribution to attention. They apply their analysis to NoPE (No Positional Embedding) and to standard PE, and RoPE. They propose the Mesa-Extrapolation idea where input tokens are organized so that attention is paid to nearby tokens and those at other key positions. Authors validate their findings with empirical evidence on several benchmarks and applications.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is about a very relevant topic which has attracted a lot of attention lately. The paper proposes a simple approach to solve the problem which seems to be easy to adapt to different positional embedding models. Some of the numerical experiments are encouraging.

Weaknesses:
The theory part of the paper is hard to read and I am not sure about its usefulness. Result appear hand-wave-y and vaguely stated. For example the definition of the threshold H in the Assumption is surprising (see questions). Numerically, experiments on language modeling and Summary of Tasks do not seem to show the method's claims.

Limitations:
--

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a weave position encoding method to enhance LLMs’ inference performance when the input context window exceeds the training context window. This method can be integrated into existing pretrained LLMs without additional finetuning. To support their findings, the authors conducted theoretical analyses on the failure reasons of various position encoding methods, including those without position encodings. They demonstrate that the significant shift in the hidden state’s value range, when input token positions exceed the maximum context length, is the cause of this phenomenon.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
One of the strengths of the proposed method is that it can be integrated into existing pretrained LLMs without requiring any additional finetuning. This makes the method highly practical and easy to implement, saving both time and computational resources.

The method has demonstrated excellent performance in pass key retrieval tasks, showcasing its effectiveness in real-world applications. This indicates that the proposed approach not only works in theory but also delivers tangible improvements in practical scenarios.

The authors have conducted comprehensive theoretical analyses to understand the failure reasons of various position encoding methods, including those without position encodings. This thorough investigation provides a solid foundation for the proposed method and enhances its credibility

Weaknesses:
The proposed position encoding method, while promising, does not consistently improve performance across different tasks. This inconsistency suggests that the method may not be universally applicable or reliable in every context, potentially limiting its overall utility. 

Additionally, the main narrative of the paper emphasizes the method’s ability to handle extrapolation beyond the training context window. However, given the observed variability in improvements, it would be more accurate to adjust the claims to better reflect the method’s performance, providing a more balanced and realistic presentation of the work.

Limitations:
The authors have included a limitations section; however, it reads more like a discussion of future work rather than addressing the actual limitations of the current study.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
za9Jx8yqUA;"REVIEW 
Summary:
In this work, the authors propose learning a pixel-based reconstructive world model, and then separately learn networks to convert the representations of a pretrained VLM into the learned world model latent space.  By using a VLM trained via contrastive alignment, this essentially enables the projection of both image as well as text inputs into the latent space of the world model, and therefore simple similarity can be used to provide rewards for downstream policy learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This reviewer is a supporter of the idea of unifying the representation spaces of a large-scale pretrained VLM and that of a world model.  This author appreciates the benefits: matching behavior of a world model with natural language can enable text-conditioned generalization.  The preliminary experiments show promise.

**Originality**: This work appears decently original.

**Quality**: This works quality is acceptable.

**Clarity**: The clarity of the work is acceptable, the core ideas are communicated clearly.  However, there are lots of open questions surrounding this work that could be elaborated upon further.

**Significance**: This work appears to be decently significant, as a preliminary investigation in this space.

Weaknesses:
Chief amongst the weaknesses of this work is the limited environments applied to, and also the limited baselines (essentially, the only existing work the authors compare against is VLM-RM).  The authors can consider comparing against other forms of text-conditioned policy learning, such as LIV for the kitchen setting, or Text2Reward and similar approaches for the general case.  It also seems a strange setup to normalize in-between expert and random, and report results in this way.  This reviewer is unaware of prior work that performs evaluations in this way.  What is the rationale behind this evaluation strategy compared to what is used in prior work?

Details about certain components of the model and how they are implemented are sparse.  For example, is the aligner a video generative model (text-to-video model)?  How is it implemented?

It is a bit dissatisfying to rely on a corrupted version of vision as a language embedding.  It seems strange that the aligner should on one hand be learning to bring language embeddings meaningfully across modalities to the image/video space, which the authors motivate is necessary because of the multimodality gap.  However, the authors then treat language embeddings as a noisy corruption of a video embedding - so essentially the training objective for the aligner is essentially a denoising?  And rather than bridging a modality gap, the aligner is essentially a denoiser?

Why do we not learn the reverse direction, where we optimize a world model's latent space that projects into existing VLM space?  This design decision is not elaborated upon, but seems more intuitive to this reviewer.

From the video demonstrations, on the associated project website, it is rather unclear what is happening.  Are Behavior Retrieval videos from expert policies in an offline dataset that are matched with a particular text prompt/input video?  What are those text prompts/input videos?  It's not clear what the retrieval setup is.  For Multitask Generalization, it is also not obvious what the corresponding prompts are.  Furthermore, the results for multitask generalization do not seem smooth and natural, despite being simplistic DM_Control environments (especially the case for their proposed simplified Stickman environment) and they are missing Kitchen environments.  In the end, it appears that their method is still good as a retrieval technique (retrieving already-achieved expert behaviors in ""Behavior retrieval"") due to the underlying VLM, and is decent at reconstructing video prompts, but still suffers in terms of learning coherent policies (e.g. what is visualized in ""Multitask generalization""), which is ultimately what is of interest.

For the video prompts that are decoded, it appears as if almost all of them are rather stationary (with the exception of the cheetah/dog example and the human dancing example) - they collapse to a stationary goal pose.  Perhaps this is because the clips are so short (8 frames) that essentially it boils down to pose-matching.  It is not obvious that this is that beneficial in supervising motion; so why does this improve upon static image supervision?  Indeed, many of the results that are shown learned by the policy are rather stationary and do not have much movement (most are just jitters around a stationary pose).  It then begs the question how this approach improves upon just a static goal supervision.  However, the authors simultaneously find that in static tasks other methods outperform the authors' approach.

This reviewer pushes back on the term ""data-free RL"", as there still needs data (and interaction data) to learn their method.  This is a very confusing terminology, and honestly the generalization comes from the large-scale pretrained VLM - it would be more appropriate to reuse the terminology of zero-shot reward models or zero-shot policy learning used in prior works across alignment methods (vision-language models are zero-shot reward models for reinforcement learning, [Rocamonde, '23]) and diffusion (text-aware diffusion for policy learning, [Luo, '24]).

This reviewer really enjoys the work but believes there are many open questions that warrant further explanation.  Furthermore, the evaluation suite (environments) and comparison suite (benchmarks) is rather weak.  The idea is indeed neat, but the execution leaves much to be desired, and therefore this reviewer believes the work is of borderline quality.

Limitations:
The limitations section seems acceptable.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper looks at a method for leveraging foundation multimodal models for learning world models in RL. They do so by aligning the latent space of a video language model with that of a generative model that can be used for learning in imagination. This is done by training connector-and-aligner networks . The rewards for a task can then be derived by measuring the cosine sim between representations of the states visited by a policy and the states generated by the connector aligner network when it is conditioned on a language-based task prompt. A policy can be optimised to maximise this alignment based reward.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
Transferring foundation model knowledge to improve policy learning is an open problem of interest to the community. 

The paper provides a successful recipe for aligning a foundation model with the world model for a specific domain that we want to do policy learning in. 

The paper is written well.

I'm currently being conservative in giving a borderline accept score, since some aspects of the method are not clear to me (I have addressed this in my questions below) - but I will be happy to raise my score after engaging with the authors once they have addressed these questions.

Weaknesses:
1. I would have expected that simple tasks with clearly distinguishable static end states (such as standing) should have worked equally well with CLIP rewards, however the table shows a big difference between the proposed method and the image-language reward baselines even on those tasks, which leads me to think that the baselines may be missing out on some component that the proposed method has. What could be missing, or is this intuition wrong? 
2. The generations in Fig 6a are actually not accurate at all - many of the poses don’t correspond to the humanoid pose if you look closely and would actually optimize learning to strike the wrong pose if a policy is trained with it.

Limitations:
The paper includes a brief discussion on limitations of their method.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to combine a DreamerV3-style world model with a pretrained vision language model (VLM). By training two small adaptors to align the latent space of the VLM with that of the world model, the aligned representations from the VLM can be used as a reward signal to train agents in the world model.

The training process consists of two main parts. 

1) There is a large offline dataset needed in the environments of interest (prompt and trajectories of states and actions), generated by expert RL agents and a random policy. This trains the world model and the adaptors. Each environment (domain) uses a separate world model.

2) Actor-critic agents are trained purely within this world model’s imagination, a separate policy for each task. The paper shows these agents outperform standard model-free offline RL methods trained on only the large offline dataset. It also shows some effectiveness at generalizing to new tasks within an environment, specified with a new text prompt.

Soundness:
1: poor

Presentation:
3: good

Contribution:
3: good

Strengths:
- The core idea of the paper is very nice. There is a lot of interest from the community in working out how to get value from the broad general knowledge locked away in LLMs and VLMs, into RL agents. This paper offers a novel way to attack this – to my knowledge world models have not been used in this context before. 

- The results are not dazzling, but they indicate the approach works and it outperforms standard (though perhaps weak) offline-RL baselines. Section 4.2 shows promise in generalizing to knew text prompts in an existing environment.

Weaknesses:
My main criticism of the paper is that the narrative oversells what the core work actually supports. I detail examples below. Overall I’d suggest either presenting the work that has been done comprehensively in a more reserved manner, or adding the required work to support the broader claims and experiments. Either way, I think changes would be large enough to require a resubmission.  I’m disappointed to not be able to give the paper a higher score as I liked the main idea. 
- The capability of the model to condition on visual goals is presented as a main functionality of the model – featuring in the first figure, the abstract, and throughout the paper. But the only evidence to support this is a very brief and qualitative experiment (Figure 6a). Everything else is conditioned on text. I am of the opinion that conditioning on visuals would likely work, but the paper must present good evidence to support this.
- Several aspects of the title ‘Multimodal foundation world models for generalist embodied agents’ are misleading. 1) Only one modality is really tested (as in prior point). 2) ‘Foundation world models’ suggested I'd see a single very general world model. But in Appendix D is an important detail -- each environment learns a separate world model, so they are only general or foundational within a specific Mujoco embodiment. This kind of detail is important and should be honestly discussed in the main paper. 3) A ‘generalist agent’ is referred to, but every agent in the paper only performs a single specialist task, there is nothing general about the agent’s themselves.
- The method is reported as needing ‘no language annotations’ (line 42). This is not true. The large offline dataset requires text prompts accompanying each trajectory.
- The paper claims to be ‘the first large-scale study of multitask generalization from language in RL’ (line 165), but I can think of others. Language table is the first that comes to mind. 
- One of the motivations for the work is that reward functions can be hard to specify, while language is a more natural form. However, the large offline dataset is generated by using multiple expert agents which need reward functions.
- ‘Data free RL’ is suggested as a new paradigm for foundation models in RL. I’d argue that this is simply know as zero-shot generalization to most in the community.
- Main experiments are presented in Table 1. Whilst the offline-RL methods are one comparison point, I’m not sure how comparable they are, since they are all model-free while GenRL is model based. Are there any model-based variants that would be easily considered as baselines? The differences are reflected in the different compute times required – GenRL takes 5 days for world model training +5 hours per policy, while the baselines take 7 hours per policy. This seems like an unfair comparison, especially to withhold the detail to the appendix.
- Results in Minecraft are briefly mentioned in Section 5. But so few details are given that I am lost as to what it is showing. This should either be removed or full details added. 
- The paper presents a new stickman environment. But details are sparse. The authors have failed to correctly identify this in Checklist Section 13.

Limitations:
Fine.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper wants to leverage the large-scale pre-training of foundation models trained on internet data to train a world model for embodied agents that generalizes across tasks and domains. This is done by training a world model in the standard way, but in addition training aligner and connector networks that (1) map language embeddings to video embeddings and (2) map video embeddings to world model latent states. At inference time, this allows conditioning the world model on a task language prompt and then training in imagination to learn policies.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
- On the website, the reconstruction results from language and video are nice and quite unexpected (I'm unsure why the aligner and connector networks are able to generalize to new prompts) 
- The problem the paper is trying to solve is relevant, especially given the mismatch in data availability between embodied and vision / language settings

Weaknesses:
- The main claim of the paper is strong generalization performance, leveraging the internet scale pre-training of video-language models. The bottleneck is the generalization ability of the networks which map embeddings from the video-language model to the world model latent states, and on the quality of the world model itself. I don't see why the aligner and connector should generalize.
- Given the main claim, I would like stronger baselines / ablations in the generalization and data-free settings. Currently, there are no baselines in the data-free case which makes it impossible to assess how well the method generalizes.  
- Many of the experimental details are unclear in the paper (please see my questions). I encourage the authors to explain these better in the rebuttal and camera-ready, and also provide some intuition for why their method is better than the baselines.  
- In the single task, offline RL case, all the baselines are model-free, whereas the proposed method utilizes a model. I would have liked to see at least one model-based baseline to confirm that the improvement is because of the better reward signal and not because of the model-based optimization.
- In the single task, offline RL case, reward is computed by looking at the similarity between the representations of the task prompt and the image / video. In the case of the base lines, these representations are fixed (eg. CLIP / Intern2Video representations), whereas for the proposed method these are taken from the last layer of the model learnt on the data itself. This is also reflected in the compute budget - the model takes 5 days to train (in addition to the 5 hours of training in imagination).

Limitations:
Yes, the authors adequately assessed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zZVqZRXSao;"REVIEW 
Summary:
This paper introduces the problem of Universal Unsupervised Cross-Domain Retrieval (U2CDR) and proposes a two-stage semantic feature learning framework to address it. The framework includes a cross-domain unified prototypical structure established through an instance-prototype-mixed contrastive loss and a semantic-enhanced loss in the first stage, and a modified adversarial training mechanism to ensure minimal changes during domain alignment in the second stage. Extensive experiments demonstrate that this approach significantly outperforms existing state-of-the-art CDR methods in solving U2CDR challenges.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper addresses a new problem, namely Universal Unsupervised Cross-Domain Retrieval, and proposes an initial solution.
2. The paper first formulates the problem and then introduces the proposed method in a hierarchical manner, which is clear and well-structured.
3. The ability to perform U2CDR has broad implications for various applications, such as image search, product recommendations, and artistic creation.

Weaknesses:
1. The main effort of the paper seems to be on designing an optimization method. However, the optimization methods involved appear to be mostly existing ones. The authors should enhance the description of the novelty.
2. Although the paper uses $L_{SPR}$ to maintain the semantic structure within domains, how to maintain the relationship between the positive pairs across domains should be emphasized.
3. The analysis related to the Ablation Study seems insufficient. It would be beneficial to analyze the reasons for the experimental results in Table 4.

Limitations:
No limitations or negative impacts have been identified in this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tackles the problem of unsupervised cross-domain retrieval. This is the problem where the query and retrieval domains are distinct. For example, in sketch to real retrieval, the system must retrieve the most relevant real images to a query sketch. ""Unsupervised"" refers to the fact that no labels are available during training, but the images from both domains are available. The authors claim to be the first to investigate the ""universal"" version of this problem, where the query and retrieval domains are allowed to have disjoint labels spaces. For this problem, the authors propose a two-stage optimization procedure. In the first stage, three losses are used: (1) an instance-wise contrastive loss (2) a cluster-wise contrastive loss and (3) a semantic enhanced loss. In the second stage, the embeddings between domains are aligned with three losses: (1) an adversarial domain alignment loss (2) a contrastive loss and (3) a nearest neighbor matching loss.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The method is theoretically motivated.

(2) The paper follows a logical orders.

(3) Experiments appear to be complete.

Weaknesses:
(1) The method is clearly described and seems to be theoretically motivated. However, it is hard to understand intuitively why each loss is necessary. In particular, why we must use six different versions of the contrastive loss across two stages? (IPM, INCE, PNCE, SEL, SPR, SN2M). The theory only seems to justify the IPM loss. 

(2) In my opinion, even for someone well versed in metric learning, this method is hard to grasp. Some examples:

 - In line 148, the method applies k-means with a variable number of clusters determined by the ""Elbow approach"" and a contrastive loss on top of the cluster centroids. Just this one paragraph requires the person implementing the algorithm to reference another paper and implement a clustering algorithm.

- The argument, starting at line 152, explaining the IPM loss is hard to understand, mostly because of the unusual notation (arrows and xor symbols).

- The argument for the SN2M loss, starting at line 235 is unclear to me. 

(3) Overall, the method reads like a series of steps that do not follow one central motivation.

Limitations:
Adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes Universal Unsupervised Cross-Domain Retrieval for the first time and designs a two-stage semantic feature learning framework to address it.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper proposes a new approach in universal unsupervised domain adaptation, with sufficient experiments to verify its motivation.

Weaknesses:
1. In unified unsupervised domain adaptation, there is no handling of instances that are not common categories. Isn't this necessary?

2. From the perspective of innovation, the proposed unified prototype structure is interesting, and the rest is mostly incremental work, such as semantic structure preservation and adjacent feature matching in domain adaptation. From the visualization results, the author failed to prove the above contributions.

3. This paper should reflect the difference between universal domain adaptation and unsupervised domain adaptation.

4. This article does not have a better way to state the method, especially in cross-domain prototype conversion and close neighbor matching.

Limitations:
no

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zXfhHJnMB2;"REVIEW 
Summary:
This paper proposes Neural Conditional Probability (NCP), a novel operator-theoretic approach for learning conditional probability distributions. Extensive theoretical results are provided to support the optimization consistency and statistical accuracy of NCP. NCP can be used to extract conditional density and compute statistical measures such as conditional mean, variance, moments and CDF once it is trained. Experiments on a collection of conditional density estimation datasets are conducted to highlight the efficacy of NCP.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is mathematically solid and well-organized.
- This paper focuses on a fundamental problem of learning conditional distribution in statistical learning and introduces an effective and simplistic approach that outperforms baselines with more complex architectures.

Weaknesses:
- The proposed NCP method is not clearly motivated or introduced. In Line 49-50, the authors mention that NCP does not belong to any of the four aforementioned approaches. But how is NCP in contrast with them and in what aspects does NCP make improvements? I believe adding some intuitive explanations accompanying theoretical analysis would help improve the readability.
- Some key concepts or methods are not clearly explained, which makes it hard to understand the contributions of this work. For example, why is learning *conditional expectation operator* considered useful? Are there any baseline methods that also learn expectation operators?

Limitations:
The authors have discussed the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
I am not qualified to review this paper

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I am not qualified to review this paper

Weaknesses:
I am not qualified to review this paper

Limitations:
I am not qualified to review this paper

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a method (Neural Conditional Probability, NCP) for learning a conditional distribution P(Y | X) from a finite sample from a distribution. The method is based on following observations: (1) it is sufficient to learn the conditional expectation operator E_{Y | X}[f](x) = E[f(Y) | X = x]; (2) the conditional expectation operator can be written as (an infinite) SVD decomposition which could be truncated at some point, so the problem reduced to learning the finite number of functions in the SVD decomposition; (3) the joint distribution density can be written using the functions from the SVD decomposition of the conditional expectation operator, which gives an optimisation objective for fitting the functions from the SVD decomposition using the sample from a joint distribution. The authors provide an extensive theoretical analysis of the proposed method as well as a simulation study on a few synthetic datasets.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
+ An interesting, novel and theoretically well-motivated method addressing an important problem of conditional distribution estimation
+ The method uses a fairly simple neural network (MLP) but achieves the competitive to the methods using much more complex architectures
+ Thorough theoretical analysis on statistical properties of the proposed estimator

Weaknesses:
- Limited experiments restricted to synthetic data making it difficult to judge the potential applicability of this method
- It would be nice to have a short summary on the main properties of operators, their SVD decompositions, etc. I could generally follow the presentation without major problems, but having a such an operators summary would have made it easier to read the paper

Limitations:
The limitations are sufficiently addressed (NeurIPS Paper Checklist)

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes Neural Conditional Probability, a novel operator-theoretic approach to learning conditional probability distributions by learning parameters of the truncated SVD of the conditional expectation operator with a neural network. The authors provide a rigorous mathematical derivation and argue for statistical guarantees of their method. The empirical evaluations require major improvements to an otherwise solid paper.

**As a general note:** I do not consider myself an expert on the theoretical aspects of learning theory. My background is in Bayesian deep learning and simulation-based Bayesian inference. As such, my confidence regarding sections 3 and 5 is rather low, and my review shall mainly consult on the remaining sections that focus on presentation, embedding into other literature, and empirical evaluations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The introduction is excellent, with a high degree of accessibility for the broader NeurIPS community and sound motivation of the proposed method.
- The method seems mathematically rigorous, well-motivated, and sound.
- The authors compare their method against a high number of competing algorithms in the numerical experiments.

Weaknesses:
## Major
- The Related Work section does a good job of acknowledging related works that aim to learn conditional distributions. However, it utterly fails to embed the current paper into this research landscape. I recommend the authors elaborate on the precise similarities and differences between the referenced papers and their methods in the rebuttal period.
- The empirical evaluations are limited to low-dimensional toy problems. This is a stark contrast to the introduction of the method, where the authors repeatedly list the curse of dimensionality as a drawback of other methods. While I acknowledge that the paper is situated in the area of operator learning and ML theory, the quality standard of NeurIPS is not met by the authors’ experiments. This weak evaluation does not do the remainder of the paper justice and I strongly recommend the authors overhaul the experiments to feature high-dimensional tasks that cannot be solved with other state-of-the-art methods. This constitutes a major revision, and this is the main reason why I cannot recommend acceptance to NeurIPS 2024.


## Minor

- The empirical evaluation is missing some important information for real-world applications: What are the approximate wall-clock times for (1) training and (2) inference of the competing methods? Further, the authors mention the large required training set size, which might also influence the practically expected training duration in real-world tasks.
- Please fix the citations throughout the manuscript: Most citations are ‘text citations’ even if their embedding in the sentence warrants parenthesized citations (Author, 1976).
- This is just a personal preference, no need to address it: The ‘paper organization’ paragraph at the end of the introduction does not add value and the space could be used more efficiently elsewhere in the manuscript.
- The first sentence in the conclusion is incomplete.

Limitations:
- The authors mention limitations throughout the manuscript, which I appreciate. However, I would recommend adding a dedicated **Limitations** section in the conclusion to give a compact overview for readers who don’t engage with the entire paper in detail.
- The performance of NCP in high dimensions might be a limitation, but the authors do not study this crucial setting.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zWuHSIALBh;"REVIEW 
Summary:
This work studies how to do alignment for large language models to improve their factuality. The focus of this work is on SFT and DPO. The motivation behind this work is a pilot study which shows more factual data does not always lead to a more factual model. To resolve this issue, the proposed Flame framework (1) handles fact-based and non-fact-based examples differently; (2) uses few-shot generated examples from the model itself for fact-based SFT; (3) builds a reward model specifically for factuality (via atomic fact decomposition, retrieval augmented claim verification, etc.) Experiments on multiple datasets demonstrate that Flame can improve the model's factuality without hurting other capabilities (e.g., instruction following). Ablations are also conducted to measure the gain from each individual step.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation is clear and reasonable. I like using a simple and quick pilot experiment to demonstrate the main motivation of this paper.

2. The idea is straightforward and effective. The high level framework can applied to many different systems. 

3. Ablation experiments are conducted to show the gain from each step. The effectiveness for both SFT and DPO are clear.

Weaknesses:
1. No external baselines are used in the comparison. It would be great to compare the flame model with other related approaches (e.g., few-shot prompting, sampling multiple responses, and reranking using FactScore or the reward model). I know these approaches are not directly comparable, however, it will still be valuable to understand the relative trends, especially since approaches such as few-shot prompting are used in data generation.

2.  It will be great to conduct human evaluations even just on a few examples.

3. The whole pipeline involves a number of components. While many details are presented in the appendix, low-level details like few-shot prompts, and implementation of fact decomposition are omitted. Adding these details will be super valuable for future work to build similar systems. It would be even better if the authors decide to release the code.

Limitations:
Limitations are discussed in Sec. A.6 in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper shows that training on new or unfamiliar knowledge can promote hallucination and that reward functions in standard RL often inadequately capture factuality. The authors propose a factuality-aware alignment method that first identifies instructions as fact-based or non-fact-based. For fact-based instructions, they employ adapted techniques in respective SFT and RL to generate additional training data, thereby reducing the hallucination of the model's responses.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
* The paper conducts a pilot study that highlights the limitations of SFT and RL in capturing factual knowledge. This study provides valuable insights into data selection for LLM alignment training.
* The proposed dual-stage factuality-aware method improves factuality without compromising the instruction-following capabilities for both SFT and RL stages.

Weaknesses:
* The proposed strategy to create SFT and DPO training data using the generated responses from the LLM itself is limited to the knowledge learned within the original model. This approach may struggle with instructions that the original model cannot generate factual answers for.
* The proposed strategy relies on accurately identifying the instruction type initially, which is limited by the model's ability to correctly classify the instruction type. 
* In the pilot study, it is unclear whether the $PT$ and $PT^{RAG}$ are evaluated using the same protocol as other methods. If they are, the FS score decreases after both SFT and DPO, which contradicts the claim that ""fine-tuning LLMs on their own generations appears to be crucial for factual alignment.""
* While the results in Table 2 and 3 indicate that eliciting knowledge from the model itself can enhance factuality compared to introducing more factual but unknown knowledge, it does not improve the FS of the $PT$, which achieves a score of 53.1 on the Biography task with just 5-shot demonstrations.
* As discussed in Sec 5.5, conducting fact checks and computing factuality rewards solely for fact-based sentences can lead to more factuality errors. Clarification is needed on how FS is calculated for the experiments in Sec 5.2 and 5.3.

Limitations:
Yes, the authors have discussed the limitations of the metric for evaluating factuality.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the issue of factual inaccuracy, or ""hallucination,"" in Large Language Models (LLMs). The authors identify factors that lead to the generation of false facts during supervised fine-tuning (SFT) and reinforcement learning (RL). They propose FLAME, a novel alignment method that incorporates factuality-aware SFT and direct preference optimization (DPO) to guide LLMs towards more factual responses without compromising their ability to follow instructions. Experiments demonstrate FLAME's effectiveness in enhancing factuality while maintaining helpfulness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The ablation experiments provides comprehensive insights into the effectiveness of DPO and SFT in mitigating hallucination.
2. The method proposed in this paper attempts to balance instruction following and factuality. It relies on model self-construction data, and does not depend on external proprietary models.

Weaknesses:
1. The baselines compared in this work are limited to different settings of SFT and DPO only. The baselines in the paper should at least include the work [1]. This prior work also uses DPO and algorithms, and the only difference seems to be data construction. The paper should compare with this work to demonstrate that its algorithm truly achieves a balance between instruction following and factuality.
2. In addition to the works listed in the related work, there are some works whose methods are somewhat similar to this paper, such as [2] [3], etc. The paper may need to add explanations of the differences between these methods to clarify its own novelty.

[1] Fine-tuning Language Models for Factuality. https://arxiv.org/abs/2311.08401

[2] Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation. https://arxiv.org/abs/2402.09267

[3] GRATH: Gradual Self-Truthifying for Large Language Models. https://arxiv.org/pdf/2401.12292

Limitations:
The authors have addressed some limitations of their work in the Appendix, which is commendable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper discusses a novel alignment method to enhance the factual accuracy of LLMs. The authors observe that conventional alignment processes, which include SFT and RL, often result in the generation of false facts or 'hallucinations'. To address this, they introduce factuality-aware alignment (FLAME), which includes factuality-aware SFT and RL through direct preference optimization. FLAME identifies factors leading to hallucination and adapts the training process to reduce the generation of false claims. Experiments demonstrate that FLAME guides LLMs to produce more factual responses without compromising their ability to follow instructions. The paper contributes to the field by tackling the issue of maintaining helpfulness while improving the factuality of AI-generated content.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Clear and Logical Structure: This paper is well-organized and presents its findings with a logical flow, making it easy to follow.
- In-depth Analysis of Hallucination: The paper thoroughly analyzes the factors contributing to hallucination during the SFT and RL phases of language model alignment. It identifies key issues: training on unfamiliar data can reduce factual accuracy, and standard RL reward functions often prioritize longer, more detailed responses, potentially encouraging the model to fabricate information.
- Innovative Solution:  The proposed FLAME is a novel alignment approach that effectively addresses hallucination without compromising the model's ability to follow instructions. By extending both SFT and RL, FLAME tackles a critical issue in LLMs, ensuring more accurate and reliable information generation.
- Comprehensive Evaluation: The paper thoroughly evaluates FLAME's effectiveness in improving both factuality and instruction-following abilities. Experiments demonstrate that models aligned using FLAME achieve significantly higher FactScore compared to standard alignment methods, without sacrificing their helpfulness.

Weaknesses:
This paper is well-written and makes a valuable contribution to the LLM alignments. I only have several minor concerns as follows:
- Model Size and Generalizability: The paper focuses solely on the LLaMA2-70B model. It would be beneficial to investigate whether FLAME's effectiveness extends to smaller models, such as 7B or even smaller, given that the factuality-aware SFT relies on self-supervision through few-shot prompting. 
- Evaluation Metrics and Human Assessment: While FactScore is a valuable metric, it has limitations. It assumes Wikipedia as the definitive source of truth and may not be suitable for broader domains. Using a more comprehensive metric like Veriscore [1] could provide a more nuanced evaluation (I understand that Veriscore is a recently released method, so this is a suggestion for the future version of this paper). Additionally, incorporating human evaluation would strengthen the analysis. A manual assessment of factuality and helpfulness would provide valuable insights and increase the persuasiveness of the findings.
- Multi-faceted Evaluation: The paper primarily focuses on instruction following and factuality. However, other crucial aspects of LLM capabilities, including knowledge, reasoning, and code generation, should also be considered. It would be insightful to evaluate the performance of FLAME-trained models on standard benchmarks like MMLU, GSM8K, and HumanEval to assess potential trade-offs in these areas.

Limitations:
The authors adequately addressed the limitations and broader impacts.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zWnW4zqkuM;"REVIEW 
Summary:
The authors propose an approach to enhance image synthesis using multimodal attributed graphs, adopting a strategy to condition image generation via a tokenization scheme on graph structure.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper studies an intersectional topic: leveraging graph learning techniques for image generation, which is a creative application and an area which deserves more focus.
- The authors' use of qualitative examples (e.g. Figure 5 and 6) is commendable and helps articulate visual improvements.

Weaknesses:
Please see questions and concerns below.  My general feeling is the paper is fairly incremental in its introduction of a mechanism to encode graph condition into the conditioning for generation.  Many design choices for graph conditioning are not discussed well and the quantitative results for some of these choices are missing which hurts the overall impact of the work.

Limitations:
Yes, Appendix A.1

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on the problem of image synthesis on multimodal attributed graphs (MMAGs) and proposes a graph context-conditioned diffusion model, INSTRUCTG2I, to address the challenge in this setting. In particular, it proposes a semantic personalized PageRank-based method to sample related neighbors in the graph.  Then, the INSTRUCTG2I can effectively encode graph conditional information as graph prompts with Graph-QFormer. Systematic experiments on MMAGs demonstrate the effectiveness of the methods proposed in this paper compared to competitive baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.  This paper studies an interesting and meaningful question. It investigates the graph-structured relationships of real-world entities for image generation on MMAGs, a task well-grounded in practical applications. 
2. This paper is well-structured and easy to understand.
3. The graph context-conditioned diffusion model proposed in this paper is reasonable in solving image generation problems on MMAGs.

Weaknesses:
1. The description in eq.10 may be incorrect. Please check more carefully.
2. Subsection 3.4 is more challenging to understand when reading. The authors' descriptions of some symbols in Eq. 10 and Eq. 11 are not exhaustive.
3. The results of the ablation experiments in Table 2 indicate that using a GNN such as GAT or GraphSAGE to aggregate graph information seems to be worse than the straightforward approach in Eq.7. Authors are requested to give a more detailed discussion with a reasonable explanation.
4. The images sampled by the semantic PPR-based sampling shown in Figure 5 appear to have the same image as the ground truth. Does this indicate that the proposed method suffers from label leakage?

Limitations:
This paper has reasonably discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new task graph2image which is to generate images conditioned on both text descriptions and graph information, which improves consistency of generated images compared to conditioned only on texts or images. To address combinatorial complexity of graphs and dependencies among graph entities, the paper proposes a graph context-conditioned diffusion model InstructG2I for generating images from multimodal attributed graph.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- To the best of my knowledge, graph2image is a novel task, and the motivation to use the rich and high-dimensional information of graphs for image generation seems reasonable and interesting. 
- The proposed approach to incorporate graph information into pre-trained text-to-image is new, in particular introducing graph conditioning token and considering scalability of graph size. 
- The generated samples show that using graph information results in better consistency with the ground truth compared to methods that use only text prompts or images.
- Examples of controllable generation with both text and graph show the ability to balance content and style in a simple manner.

Weaknesses:
While I do not have a major concern, an ablation study on scalability to graph size seems to be missing. How large graphs is the method able to be applied?

Limitations:
Yes the paper addresses the limitation in Appendix A.1.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach for controllable image generation using both graph and text conditions. The authors propose that additional context information from multimodal attributed graphs (MMAGs) can enhance the performance of diffusion models. Specifically, they formulate the Graph2Image problem and develop the INSTRUCTG2I model to incorporate contextual information during the generation process. Empirical evaluations demonstrate the strong performance of the model.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is easy to follow.
2. The intuition behind the approach is clear.

Weaknesses:
1. The overall setting is questionable. The authors integrate graph information using a Graph-QFormer and context information such as artists and genres, stored in graph prompt tokens. Given the large graph size, they only use subgraph structures. Consequently, the Stable Diffusion (SD) model absorbs additional information from similar artworks, which could be derived from image or text prompts alone. This raises the question of whether an additional condition structure is necessary. I suggest the authors demonstrate a unique application where standard models with text and image prompting capabilities are insufficient.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zVrQeoPIoQ;"REVIEW 
Summary:
This paper proposes a new no-reference image exposure assessment method, Pixel-level IEA Network (P-IEANet), which analyzes and evaluates image exposure from the perspectives of brightness and structure using discrete wavelet transform (Haar DWT). Also, a dataset exclusively tailored for IEA, called IEA40K, is constructed. According to a comprehensive evaluation of methods on the IEA40K dataset, the proposed method achieves SOTA performance and offers advantages for the exposure enhancement community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper demonstrates very good originality as it is the first realization of pixel-level image exposure assessment. The authors have designed corresponding methods specifically addressing the characteristics of this problem and achieved satisfying results. Detailed explanations of the motivation and the current state of research are provided. Both the principles and the implementation of the method are clearly presented. The experimental results effectively demonstrate the performance of the proposed method. This paper not only proposes a new IEA method but also contributes a new dataset and benchmark, providing a significant boost to the IEA and exposure-related community.

Weaknesses:
Haar DWT is used to decompose an image into components with different frequencies, but the advantages of this method compared to other similar methods are not adequately explained. In the method section of this paper, some operations lack clear motivation or principles. For example, the reason for applying the DWT^{-1} and the choice of l1 norm as the loss function are not well explained. In the experiments section, SSIM and MAE are adopted to measure the structure and lightness similarity between the ground truth and predicted exposure residual. However, as a perceptual IQA metric, SSIM may not be suitable for evaluating the prediction accuracy of exposure residuals. The paper claims that the proposed method has improved adaptability across varying criteria and scenarios, but this is not well demonstrated in the experiments.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel paradigm that extends Image Exposure Assessment (IEA) from an image-level to a pixel-level framework. This paradigm comprises three components: model, dataset, and benchmark. Concerning the model, the study introduces the Pixel-level IEA Network (P-IEANet). This network processes images of varying exposures, separates them into low and high-frequency components via a discrete wavelet transform, assesses brightness with the low-frequency component, and evaluates structure with the high-frequency component, ultimately delivering pixel-level assessment results. Regarding the dataset, the authors have developed a new dataset, IEA40K, which includes 40,000 images featuring diverse exposures and corresponding pixel-level annotations. Finally, the paper presents comprehensive experiments on both holistic and pixel-level assessments, yielding promising results.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper initially proposes a pixel-level image exposure assessment paradigm, significantly enhancing precision in the field of image exposure assessment.

2. The paper introduces an assessment network that employs discrete wavelet transform, an intriguing choice supported by several ablation studies.

3. The paper proposes a large-scale, multi-exposure dataset with pixel-wise annotations derived from an automatic multi-exposure fusion technique, subsequently refined by human experts.

4. The paper also demonstrates that the P-IEANet can potentially improve the performance of low-light image enhancement methods.

5. The paper is well-composed, demonstrating a clear structure, precise language, and a logical flow of ideas.

Weaknesses:
The main weakness is that the paper lacks a well-defined definition for pixel-level image exposure assessment. For other details, please refer to the ""Questions"" part.

Limitations:
The paper adequately discusses the limitations of moving objects and image size while training. No negative social impact is present in this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work tackles the challenges in image exposure assessment from three aspects: models, datasets, and benchmarks. Specifically, A P-IEANet model based on DWT is proposed, which can generate pixel-level assessment results in a no-reference manner. An exposure-oriented dataset IEA40K is collected to cover various lighting scenarios, devices, and scenes, which are annotated by more than 10 experts with pixel-level labels. A comprehensive benchmark of 19 methods is conducted on the collected IEA40K dataset, where the proposed P-IEANet delivers the best performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ Decomposing images into lightness features and structure components using Haar DWT is theoretically reasonable and empirically effective as presented in this work.
+ The dataset construction strategies described in Sec. 4.1 and Sec. 4.2 provide valuable insights to the related community.
+ The proposed model delivers good performance, even outperforming the LMM-based model Q-align.

Weaknesses:
- Holistic level assessment is performed on SPAQ. It should be straightforward to convert the pixel-level annotations to holistic level annotations in the proposed IEA40K dataset because the pixel-level annotations contain more information than the holistic level annotations.
- Would the performance of IEA models be boosted by jointly training (like the practices used in UNIQUE, LIQE, etc.) the model on the combination of IEA dataset and general-purpose IQA datasets?

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes an innovative no-reference image exposure assessment method, transitioning from traditional holistic image evaluation to fine-grained pixel-level assessment. This approach effectively addresses the shortcomings of existing techniques in terms of accuracy and generalization. Researchers have developed P-IEANet, a pixel-level evaluation network that utilizes Haar discrete wavelet transform to analyze image brightness and structural information, enabling exposure assessment without reference images. Additionally, to support this method, the researchers have constructed the IEA40K dataset, which contains 40,000 images with detailed pixel-level annotations, covering diverse lighting conditions and devices. Using this dataset, they established a comprehensive benchmark including 19 methods, demonstrating that P-IEANet achieves state-of-the-art performance across multiple evaluation metrics. This work not only enhances the accuracy of no-reference IEA tasks but also provides valuable resources and new research directions for the image exposure research community. Future work will focus on optimizing the framework to support multimodal outputs and enhancing exposure perception in AI-generated content.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Pixel-level Evaluation: The P-IEANet proposed in the article is capable of conducting pixel-level image exposure assessment, which offers a more refined analysis and more accurate results compared to traditional overall image assessment.
- Innovative Model Architecture: By integrating the Haar Discrete Wavelet Transform with specific feature extraction modules, P-IEANet is able to analyze images from both the brightness and structural perspectives, providing a more comprehensive exposure assessment.
- Large-scale Dataset: The article has constructed the IEA40K dataset, which is a large-scale, diverse image dataset that provides rich resources for evaluation and training.

Weaknesses:
- The author mentions in the abstract that the code and dataset can be found in the supplementary materials, but there is no relevant section in the supplementary materials.
- There is no explanation as to why the Haar wavelet was chosen over other wavelets.
- The aesthetic quality of Figure 4 needs to be improved.

Limitations:
Not applicable

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
IIoH8bf5BA;"REVIEW 
Summary:
This paper proposes a new generative model on continuous random variables similar to diffusion-based generative models, but this method uses a different family of perturbation schemes. In particular, for the forward processes, which transform the clean data to a stationary distribution, the authors propose using piecewise deterministic Markov processes (PDMPs), which consist of a combination of discrete diffusions and ODEs. Specifically, the initial value, i.e., data, is transformed via simple ODEs, whose velocity term is often chosen to be a constant. However, the velocity term (also called vector fields) is updated at any random time with a constant rate $\lambda$. The occurrence of a new event is independent, and the time interval of the new event since the last one is exponentially distributed. Moreover, the new velocity will be assigned by user-chosen schemes; for example, one can flip the sign of the velocity, or another one can sample from the standard normal distribution. Thus, while the forward process transforms the data with a simple ODE, it randomly changes the ODEs, hence, the title of the paper.

In addition, the paper shows that under some mild assumptions, the time-reversed processes of the PDMPs are also PDMPs, and their transition probability distribution and update rate can be written in terms of the forward processes. Therefore, we can consider modeling such terms with parametric models similar to popular diffusion-based generative models.
The paper proposes the explicit ratio matching objective function, as in Equation 6, to train such terms.

Moreover, the authors suggest using three popular PDMPs for the forward process: Zig-Zag process (ZZP), bouncy particle sampler (BPS), and randomized Hamiltonian Monte Carlo (RHMC). Consequently, the paper shows the time-reversed PDMPs and potential parameterizations that fit each case.

Finally, the paper demonstrates the efficacy of the proposed method through a few toy experiments.

----------------------------------
Update the rating from 5 to 7 after the authors' rebuttal.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
One of the paper's key contributions is the introduction of a novel generative model that uses PDMPs as the forward processes. Moreover, the authors propose a training method for the time-reversed PDMPs.

Weaknesses:
The proposed method is novel and interesting. However, the paper didn't characterize some potential drawbacks well. For example, due to the nature of PDMPs, the training of the proposed method inherits the problem of discrete diffusion models. In particular, the ratio matching should be done for each dimension independently, and this costs a lot for high-dimensional data compared to the denoising score matching and others for previous methods. This problem could be the reason that the current submission didn't include the experiments on popular high-dimensional datasets. In this regard, the paper suggests that Monte Carlo estimates can reduce the computational cost, but it trades off the variance of the learning signals, which would be critical for large-scale experiments.

I consider that a new method doesn't always need to be better than previous methods. Nevertheless, proper information about the proposed method in practice would be important for potential readers to evaluate the significance of the paper. Thus, some additional discussions related to this problem need to be added.

Moreover, given that many alternative choices exist other than the proposed method, in-depth discussions on the proposed method's merits would be appreciated for evaluating its importance.

In addition, I find that the presentation should be improved. For example, many statements contain multiple prepositional phrases, which are difficult to parse and understand.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores generative models utilizing PDMPs, a type of stochastic process characterized by deterministic motion interspersed with random jumps at random times. These models offer an alternative approach to diffusion-based generative models, which have become very popular in the AI community in recent years. The authors focus on three specific PDMPs: the Zig-Zag process (ZZP), the Bouncy Particle Sampler (BPS), and Randomised Hamiltonian Monte Carlo (RHMC). The authors leverage the existing literature on the time reversals of Markov jump processes, characterizing the time reversal of any PDMP under appropriate conditions. They show that the time reversal of a PDMP is itself a PDMP with characteristics related to the original process. The authors also specifically outline the characteristics of the time-reversal processes for ZZP, BPS, and RHMC. The jump rates and kernels of these time reversals admit explicit expressions based on the conditional densities of the PDMPs before and after jumps.

The authors provide bounds on the total variation distance between the data distribution and the distribution of their generative models, considering errors from the approximation of the backward PDMP’s characteristics and its initialization from the forward process’s limiting distribution. Some initial but promising numerical simulations on simple toy distributions are presented, showcasing the potential of PDMP-based generative models. The results support further investigation into this class of models on more challenging data structures.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Originality: The paper introduces a class of generative models based on piecewise deterministic Markov processes (PDMPs). This is a very novel idea which departs from the widely used diffusion-based models. In my opinion, this is a fresh perspective on generative modelling and opens up new avenues for exploration and potential improvements in various application areas. The key idea behind diffusion-based models is the derivation of the reverse time and process, and following a similiar line of thinking, the authors of this paper characterize the time reversal of PDMPs, which is particularly original. While time reversal in diffusion processes is well-studied, applying these concepts to PDMPs and providing explicit expressions for their jump rates and kernels is a significant contribution.

Quality: The paper presents a comprehensive theoretical framework for PDMP-based generative models. The mathematical rigour in characterizing time reversals, deriving error bounds, and proposing training procedures leads to a complete and high-quality piece of work. The thorough analysis of the three specific PDMPs (ZZP, BPS, RHMC) and the detailed exploration of their time-reversal characteristics demonstrate a deep understanding and careful consideration of the underlying processes. The numerical simulations provide supporting empirical validation of the proposed models, adding credibility to the theoretical claims. The use of multiple toy distributions (extras are in the appendix) to test the models in the paper strengthens the evidence for their potential efficacy in practice.

Clarity: The paper is very well-structured, with a logical flow from the introduction of PDMPs to the detailed theoretical contributions and empirical results. Each section builds on the previous one, making it easier to follow the progression of ideas. The explanations of the PDMPs, their time reversals, and the derivation of jump rates and kernels are clear and very detailed. The inclusion of propositions and their proofs in the appendices provides a robust foundation for the proposed algorithms and establishes their convergence properties. The use of toy distributions to demonstrate the numerical simulations helps illustrate the practical application of the models, aiding in the reader’s understanding and also provides some nice opportunities for the authors to clearly articulate the advantages of their approach over the diffusion-based alternative approach.

Significance: The proposed PDMP-based generative models have the potential to be applied in a wide range of fields, from machine learning and statistics to physics and biology. This broad applicability enhances the significance of the work. By offering an alternative to diffusion-based models, this paper advances the field of generative modelling. The potential advantages of PDMPs, such as better scalability and reduced computational complexity, could lead to significant improvements in high-dimensional data generation. The theoretical insights and empirical results lay a strong foundation for future research in this area.

Weaknesses:
Originality: While the introduction of PDMP-based generative models is novel, the paper primarily focuses on theoretical aspects and toy datasets. There is a limited exploration of how these models can be applied to more complex, real-world problems, which could showcase their true originality and practical utility. Although PDMPs offer a new approach, the paper does not extensively compare these models with a wide variety of existing generative models, even on the simple toy examples that were considered here. This makes it difficult to fully appreciate the originality and benefits of PDMPs over other state-of-the-art methods.

Quality: The paper relies on several technical assumptions and conditions (e.g., H3, H4). While these are necessary for the theoretical results, their practical applicability might be limited and the authors do not discuss whether or not these assumptions hold for the toy examples which they consider. The paper could be strengthened by discussing the feasibility of these assumptions in real-world scenarios. The empirical validation is primarily limited to simple toy distributions. While these are useful for initial validation, the lack of experiments on more complex datasets (e.g., image or text data) reduces the overall impact and persuasiveness of the empirical results. It would be beneficial to include comparisons on more challenging benchmarks.

Clarity: The paper uses dense mathematical notation and detailed proofs, which may be challenging for readers who are not specialists in stochastic processes or PDMPs. This is going to be challenging for the authors as the high-level of technical detail provided in the paper does lead to a very robust paper. However, perhaps more intuitive explanations or visual aids could help make the content more accessible, if not in the main paper then in the appendix. The practical implementation details, particularly regarding the training procedures and simulation methods, are somewhat sparse (even though there are more details in the appendix). Providing a step-by-step guide or pseudocode could help practitioners better understand how to apply the proposed methods. Given the space constraints, this would have to be added to the appendix. This is covered in the case of splitting schemes, but could perhaps be modified to be more user-friendly to people new to this area of research. 

Significance: The paper’s significance is somewhat limited by the focus on theoretical and synthetic examples. Without demonstrating the effectiveness of PDMP-based models on real-world data, it is challenging to gauge their practical significance and potential impact in applied settings. While the paper claims that PDMPs offer better scalability and reduced computational complexity, there is limited empirical evidence to support these claims. Benchmarking the computational performance against existing generative models would provide a clearer picture of the advantages and limitations in terms of scalability and efficiency.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This interesting paper on the popular topic of generative models introduce a new family of generative models which builds on the so-called piecewise deterministic Markov process (Zig-Zag process, Bouncy Particle Sampler, Randomised Hamiltonian Monte Carlo). In contrast to many of the existing models this family is not based on diffusion models. The paper includes a through analysis of the construction 
and it propose training procedures and methods for approximate simulation of the reverse process.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
* A new family of generative models is proposed. 
* Thorough analysis of the properties of the proposed construction is provided.
* Simple examples provided.

Weaknesses:
* Missing real-world examples
* The is a big jump in  the style of writing between Section 1 and 2. Do not get me wrong here, the technical developments are most interesting, but many readers would be helped by a more gentle transition between these sections. Space for this can be created by moving more of the technical details into the supplemental material.

Limitations:
Real-world examples missing.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers the development of generative models based on piecewise deterministic Markov processes. The key idea proposed in the paper is to use piecewise deterministic Markov processes instead of diffusions as the ""noising process"" of the generative model. This relies on the fact that time reversals of PDMPs are themselves PDMPs. Three specific instances of PDMPs are considered. The authors also derive a bound (in total variation distance) between the data distribution and the distribution of the generative model. The methodology is illustrated with some simple experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I enjoyed reading the paper and I like the idea of considering alternative noising processes in the context of generative models. The paper covers both theory and provides an example showing the viability of these methods. The examples are sufficient and certainly the area seems worthy of further investigation.

Weaknesses:
To me, the descriptions of approximating the process characteristics with normalizing flows are unclear. This part should be written more with more details, perhaps in the supplement, as this is core to being able to reproduce the results. I do appreciate that the authors provided a description of the experiment in E.1, but it is not enough to put things together. I would be interested in replicating at least the simple experiment, but I don't think I can do it as the paper stands.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes using Piecewise Deterministic Markov Processes (PDMPs) for generative modelling applications, by using the property that PDMPs also admit time reversals that themselves are PDMPs. There are three major contributions in my understanding - 

By characterizing certain families of PDMPs, i.e. Zig-Zag processes (ZZP, the Bouncy Particle Sampler (BPS), and the Randomised Hamiltonian Monte Carlo (RHMC), in terms of their jump rates and kernels, this paper shows how to obtain tractable closed-form and approximations for the jump rates and kernels of the time reversed PDMPs.
Theoretically, this paper then proposes a total variation bound between the “learnt” data distribution and the true data distribution when the base distribution is a Gaussian. This is a useful property, quite similar to bounds that have been proposed before in the literature (for example, for the Ornstein-Uhlenbeck process in [1, Theorem 5.2.]).
Finally, the paper proposes two empirical techniques to learn the time reversals akin to score-matching. First, for the ZZP process, inspired by score-matching techniques, the authors propose a ratio-matching technique. Secondly, for the BPS and RHMC processes, the authors learn normalising flows for the time reversal. They then show promising results in low-dimensional and MNIST generative modelling applications as a proof-of-concept.

[1] Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. Analysis and geometry of Markov diffusion operators, volume 103. Springer, 2014.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
I quite like the structure and formulation of the paper. The main goals and approach is elucidated quite clearly, and the mathematical preliminaries, while dense, seem correct for me. 

In my understanding, it is a known fact that all PDMPs admit an equivalent time-reversal, but these are quite hard to calculate in general. In this paper, building on theory involving jump Markov Processes in [2], the authors derive expressions for time-reversal jumps and kernels for 3 different PDMPs. In general, deriving the backwards time reversal and then also designing an empirical scheme with a neural network architecture and loss function would be a substantial contribution, but this paper has many additional contributions on top of that. The numerical experiments seem compelling, even if a little small scale. However, this paper seems like a proof-of-concept on the use of time-reversed PDMPs for generative modelling, and I think the theoretical contributions along with the design of the loss functions and training paradigms are a pretty significant contribution already.

[2] Giovanni Conforti and Christian Léonard.   Time reversal of markov processes with jumps under a finite entropy condition. Stochastic Processes and their Applications, 144:85–124, 2022.

Weaknesses:
Fundamentally, I think the paper lacks a convincing argument about why generative modelling with PDMPs would fundamentally be more useful than traditional generative modelling. I understand that there were some arguments made in the introduction of the paper, namely Lines 35-36 (“such as better scalability and reduced computational complexity in high-dimensional settings”). However, it is really unclear to me how this argument actually translates to the empirical score-matching (or normalising flow training) objectives that the authors formulate, vs an approach like DDPM. The experimental section is quite lacking in details and comparisons about how the PDMP approach improves along any number of axes, beyond the qualitative plots. For example, I can think of many axes of improvement that could be discussed - 
sample efficiency (how many training datapoints are needed to learn the time-reversal given that the process is partly deterministic), 
mixing rates towards the Gaussian for their time reversal. Usually, SDEs such as the Ornstein-Uhlenbeck process are quite quick at mixing towards a Gaussian, making them quite nice to use when reversing a Gaussian distribution as the base. For partly deterministic processes, is this easier or harder to do?
Are there any comparisons to regular Markov process methods that can show that having an irreversible Markov process is beneficial here? I believe that this is a big factor in why PDMPs are alluring, and their irreversibility makes them mix faster and use less data [3]. Any experiments showing sample efficiency and mixing rates would be really beneficial here.

I am worried that there are many subtleties in the training and sampling procedures of diffusion models, and indeed there are many papers focusing solely on the empirical training tricks that can improve generative modelling, and comparing to a vanilla DDPM model doesn’t properly ablate the technique. I would be hesitant to rely on these empirical results as a surefire sign of improved modeling, which is frustrating, as theoretically, the paper does seem to point to this being the case, and I really do want to believe. 

Furthermore, I think the paper could benefit from being a lot clearer about the specific advantages of PDMPs vs other stochastic processes for generative modeling. This is barely mentioned, but does form the crux of the empirical results. This made it difficult for me to read through the theoretical developments, proofs and theorems without knowing the reason why we would really want to do this in the first place.

I also think the paper can also benefit from being more explicit in how their developed score matching and normalizing flow training differs from traditional methods (maybe an algorithm block), as this would be something really interesting to practitioners looking to adopt existing codebases to using PDMPs instead.

[3] Bierkens, J., Fearnhead, P., and Roberts, G. (2019), “The zig-zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data,” The Annals of Statistics, 47, 1288–1320. DOI: 10.1214/18-AOS1715

Limitations:
Yes, they have addressed any potential limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zV2GDsZb5a;"REVIEW 
Summary:
This paper presents a method for relighting objects observed from a single image. While existing approaches rely on specific capture condition using flashlight illumination or portrait captures, or require to explicitly decompose the scene into geometry and reflectance, the proposed method aims to generate images of a given objects under novel illumination conditions for arbitrary environmental lighting conditions. The authors show that this is possible by relying on a generative diffusion method that is conditioned on the environmental map. The method relies on a pre-trained diffusion model that is fine-tuned on a synthetic relighting dataset to learn the conditioning. The approach is evaluated qualitatively and quantitatively on single-object images. Relying on a conditional diffusion model for relighting, the authors also show additional conditioning on text for relighting.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work presents a simple (this is a good thing) and effective method for relighting from a single image. The method relies on synthetic supervision with a novel Blender-rendered dataset that uses Objeverse as input model source. The authors went a long way by collecting diverse HDR environment maps from the Internet that were augmented to produce a large synthetic relighting dataset of almost 20M rendered images with ground truth lighting maps. Overall, the method offers a number of intriguing benefits listed as follows:

* Conditional image-to-image diffusion model: The method inherits a conditional Zero-1-to-3 model that is extended in its input latents to a rotated environment map with the camera coordinate frame, allowing for image-to-image relighting in a consistent frame. While, given enough training data, the method is effective in relighting, the approach also enjoys the benefits of existing diffusion architectures with various types of conditioning. The authors demonstrate this effectively with their image conditioning. 

* Relighting 3D radiance fields: The proposed method is evaluated as a prior for 3D relighting of a neural radiance field. Specifically, the authors propose to use diffusion-based relighting as a coarse reconstruction loss (predicting a coarse relit scene during the NeRF optimization) and a detail refinement loss where the NeRF appearance is further refined.

* Qualitative evaluation: The evaluations presented qualitatively in the main manuscript and the supplemental material in the form of supplemental videos are visually plausible and convincing. 

* Quantitative evaluations: The method is adequately ablated and quantitatively compared to single image relighting methods, 3D radiance field relighting with reasonable margins on the test sets. This validates the method as an effective approach.

Weaknesses:
What makes the method exciting, at first glance, is also one of the major weaknesses: the technical novelty. The paper piggy-backs on an existing generative method, the Zero-1-to-3 model, that is with a few variations used for relighting. While the simplicity is something that is desired, it also makes it challenging for the reader to derive deeper insights from this work. We learn that pre-trained diffusion-models, when just given enough and the right synthetic data, can allow for plausible novel view synthesis with artefacts that are improved over existing methods. However, the recent work by 

Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Fine-grained lighting control for diffusion-based image generation, 2024.

in a way also does show the exactly same, although the technical approach is different. Overall, the technical contribution of the approach is rather incremental (although the method is effective). As such, I am torn on this work. While the technical contribution is not near other work at NeurIPS, the method is effective and likely of high impact. 

A further qualm I have is regarding the results compared to NVDIFFREC. While the margins are not substantially different, the results in Fig. 6 seem to indicate differently. It seems as if these results are cherry-picked.

Limitations:
All major limitations are addressed. The only open limitation not addressed in the manuscript is the runtime. The authors should address and comment on the runtime for their diffusion model.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Neural Gaffer, an end-to-end 2D relighting diffusion model designed for single-image relighting without the need for explicit scene decomposition. Neural Gaffer can synthesize high-quality relit images of any object under novel environmental lighting conditions by conditioning on a target environment map. The model builds on a pre-trained diffusion model, fine-tuning it on a synthetic relighting dataset. The advantages in generalization and accuracy through evaluations on both synthetic and in-the-wild Internet imagery are shown in the paper. Neural Gaffer can be combined with other generative methods for various downstream 2D tasks like objection insertion. The video results presented in the paper are of high quality.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1)  Neural Gaffer performs single-image relighting without the need for explicit scene decomposition into intrinsic components like normals and BRDFs. This provides an avenue for relighting without collecting expensive relighting real-world datasets.

2)  The model can generate relit images of various objects under different environmental lighting conditions based on a target environment map. The method takes a single image as an input.

3) The method can be applied to real-world objects with high-quality relighting results and perform various downstream tasks such as object insertion.

Weaknesses:
1) In case of the real-world object scenarios, the object may not be always centred and may have complex backgrounds and lighting to start with. The paper does not demonstrate how would the method behave in such cases. How about the objects with high-frequency texture details?

2) Related to 1) there might be multiple objects in a scene. From the results, it seems that the method cannot handle multiple objects from a single image.

3)  The real-world object examples shown in the paper and the video are good but not impressive. It would be more compelling to show faces, humans, animals etc under the lighting conditions to show the generalizability of the method.

Limitations:
The authors have discussed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Neural Gaffer presents an approach to object-centric image relighting using diffusion models. The method adapts a pre-trained diffusion model and fine-tunes it on a synthetic dataset designed for relighting tasks. The main feature is its ability to condition the diffusion process on target environment maps, allowing for control over lighting effects.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1) Simple yet effective approach: The paper presents a straightforward fine-tuning method for object relighting, similar to zero-1-2-3 shot learning. This simplicity is a strength, demonstrating that complex relighting can be achieved without overly complicated techniques.

2) Powerful data-driven learning: The supervised conditional diffusion model effectively learns to relight objects, highlighting the potential of data-driven approaches in capturing intricate lighting interactions.

3) Competitive results: Based on the presented figures, the method appears to outperform the recent DiLightNet in some aspects. However, this comparison raises some evaluation questions (see questions section for details).

Weaknesses:
1) Real-world evaluation: The model is fine-tuned on a synthetic relighting dataset, which might not fully capture the complexity of real-world lighting scenarios. Real-world evaluation is necessary, and there are datasets capturing these effects. The paper is currently missing this evaluation, and there are datasets available for such evaluation [1] OpenIllumination [2] Objects with Lighting or [3] Stanford ORB dataset. These papers have been cited but it is surprising to not see an evaluation of these datasets.

2) Reliance of Environment map: Do you need to supply the environment map for relighting? There is a missing baseline that shows what happens if you condition the target lighting image without a full environment map (only image crops). The Diffusion Light Probe (CVPR 2024) paper indicates that diffusion models are capable of inpainting reliable environment maps and they seem to be implicitly encoded within the model. This baseline will justify why a full environment map is required or necessary for this task.

3) Generalization to scenes: The extent to which the method generalizes to scenes -- not just objects -- is unclear. Evaluating the MIT-multi illumination dataset could shed light on this. The current reliance on explicit environment maps makes it harder to perform on these scenes, but it would be interesting to see if, without explicit environment maps (like suggested above), can you learn to relight and compare on scenes.

4) Evaluation metrics: Recent studies show that PSNR, SSIM, etc. are not consistent with human evaluation. See ""Towards a Perceptual Evaluation Framework for Lighting Estimation"" (CVPR 2024). These metrics don't tell us much about whether the method is promising as such. A thorough evaluation via user studies or the metrics as defined in the recent paper is currently missing from the paper.

5) Unrealistic results and missing comparisons: The object insertion results look unrealistic, with incorrect shadows that don't match the lighting conditions. Several relevant lighting-aware compositing methods are missing from the comparisons, such as ControlCom [Zhang et al., arXiv 2023], Intrinsic Harmonization [Carega et al., SIGGRAPH 2023], Reshading [Bhattad and Forsyth, 3DV 2022], and ARShadowGAN [CVPR 2020]. The comparison to AnyDoor doesn't make sense as it's not lighting-aware. Including these comparisons would provide a better evaluation of the method's performance against current state-of-the-art techniques.

6) Further, as the papers use off-the-shelf methods to estimate environmental maps (text2light), why not compare with existing 3D object compositing with lighting estimation methods to get a sense of how the proposed methods compare to these tasks -- see Garon et al (CVPR 2019), StyleLight (Wang et al; ECCV 2022) and similar papers? Rendering objaverse objects using lighting estimated from the mentioned or similar methods would help understand the gaps between explicit environment map prediction methods.

7) 3D relighting evaluation: For the 3D relighting setting, according to the Objects with Lighting 3DV 2024 paper, Mitsuba + NeuS is a stronger baseline compared to TensorIR, which is currently missing in the paper.

8) Failure analysis: The paper mentions in the limitations section that the approach might not work for portrait relighting, but it would be interesting to see the kind of failures the diffusion model makes. The current setup lacks experiments in this direction to see what are these failures to encourage future research. Further, the current papers also do not provide any failure examples from Objaverse instances. Is the method perfect on all unseen objects -- detailed analysis is missing as to what objects the proposed methods perform best or worse on. Such analysis helps scope out limitations of the current methods instead of shallow limitations provided in Appendix D.

9) Lack of comparison with simple color matching baselines: The paper doesn't include a comparison with straightforward color adjustment techniques, such as RGB histogram matching between the inserted object and the target scene. This omission raises questions about how much of the method's perceived success in relighting is due to sophisticated light interaction modeling versus simple color transformations. A comparison with such a baseline would help quantify the added value of the diffusion model approach over a simpler method.

Limitations:
Somewhat but not fully. See my weakness 8.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel method for single-image relighting, which takes an image of an object and a target environmental map as inputs. The authors fine-tune Stable Diffusion on a synthetic relighting dataset to output relit images, conditioning on both the input object image and the target environmental map. The authors show their method outperforms existing baselines. Additionally, the trained relighting model can be applied to downstream tasks such as relighting a neural radiance field and object insertion.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- I check the video results in the supplementary video. The visual results are impressive.
- The authors have shown several downstream applications using their trained relighting model, including text-based relighting and object insertion.
- The authors have conducted extensive ablation studies to prove the effectiveness of their proposed method.

Weaknesses:
I don’t have many complaints about the paper. I list several potential improvements below:

- In the 3D relighting experiments, it seems unfair to compare with inverse rendering methods such as Nvdiffrec-mc and TensoIR, as they can apply any lighting to the object once the material is recovered, while Neural Gaffer needs optimize for every lighting. On the other hand, I think Neural Gaffer should be combined with these inverse rendering methods and provide priors when recovering material and lighting.
- The extrinsic information is injected by rotating the environmental map. However, it seems intrinsic information is not considered, which means there is an assumed fixed FOV. This could introduce biases in downstream applications and limit the input views in 3D relighting.
- The quantitive comparison with IC-Light is missing.
- The generated image resolution is limited to 256x256.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zTu0QEpvtZ;"REVIEW 
Summary:
This paper aims to understand two mechanisms of diffusion models. First, the denoising process is analyzed, and it is found that shapes in an image are constructed in the beginning of the denoising process, while textures and details are filled in later. This empirical observation is justified with a mathematical frequency analysis. Second, the role of text conditioning is analyzed and it is found that the [EOS] token, which captures global information of the prompt, is relied on more heavily by the diffusion model. It is also observed that the text prompt is utilized more in the earlier stages of the denoising process. This finding is utilized to speed up diffusion sampling by ~25% while maintaining the image quality and prompt alignment. This is done by only injecting conditional information in the beginning of the denoising process.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Although the finding that shape is constructed in the first few timesteps has been observed many times before, it is nice to have a more principled study with various experiments and mathematical justification. 
* The finding that the special [EOS] token is the most relied upon during generation rather than the prompt tokens is an interesting finding that can be used in later studies. For instance, improving prompt alignment, attribute binding, etc. 
* The observation that the text prompt is used more in the early denoising process lends itself to a practical application of speeding up inference. 
* Multiple architectures and samplers are used in this study, suggesting the generality of these findings.

Weaknesses:
* As mentioned in the Strengths section above, the findings are not completely surprising (for instance, the shape reconstruction or reliance on text in the early denoising steps, then detail-filling in the later steps). However, this work takes a principled approach in studying these phenomena which have largely been used in diffusion application literature (e.g., [1, 2])
* Limited to no mention of broader impact or limitations. Furthermore, the Conclusion section is just a summary of the paper but does not discuss the implications of these findings. 

[1] @inproceedings{mengsdedit,
  title={SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations},
  author={Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
  booktitle={International Conference on Learning Representations}
}
[2] @inproceedings{hertzprompt,
  title={Prompt-to-Prompt Image Editing with Cross-Attention Control},
  author={Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-or, Daniel},
  booktitle={International Conference on Learning Representations}
}

Limitations:
Although there are no societal implications, a discussion of limitations is lacking.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores the mechanism in the text-to-image diffusion model, including the generation order of image components, the influence of various tokens, and the steps in which tokens work.
These observations bring some insight into understanding the diffusion model.
Besides, the authors also design a sampling strategy that accelerates the sampling of the denoising process by 25%+.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The conclusion of the [EOS] token is interesting and has been rarely investigated in previous papers.
2. The analytical experiments in this article are sufficient and strongly support its conclusion.
3. The writing expression of this article is very clear.

Weaknesses:
1. The other conclusions in this paper, e.g., shape first then details, have been discussed in previous works.
2. The sampling strategy is more like a sample trick than a method.

Limitations:
Suggest the author to discuss the applicability and limitations of the proposed sampling scheme.
For example, can it be applied to human face generation without losing human identity?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper investigates the denoising process in DPM, identifying that the overall shape of the image is formed early in the process while details are added later. It further examines the influence of different text prompt tokens, finding that the end-of-sequence token [EOS] plays a crucial role in shaping the initial stages of image generation. The authors propose a method to speed up the generation process by removing text guidance after the initial stages, achieving a significant reduction in computational cost.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Comprehensive analysis of the denoising process stages in DPM.
- Detailed exploration of the influence of different tokens in the text prompt.
- Practical application of findings to accelerate the T2I generation process.
- Empirical and theoretical support for the proposed acceleration method.

Weaknesses:
- The paper might lack clarity in explaining the theoretical aspects of frequency signal analysis.
- Limited exploration of potential biases introduced by the dominance of the [EOS] token.
- The study may benefit from a broader range of experiments to validate the generalizability of the findings.

Limitations:
- Authors should discuss the robustness of their findings and the need for further experiments across various models and more complex or diverse text prompts to validate their conclusions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper study how the EOS token plays a role in the generation process of diffusion model. In particular, this paper finds that diffusion models tend to first generate low frequency part of the image at the beginning of the generation process, then gradually add high frequency signal to it. Experiments show that the low frequency signal is conditional on the EOS token while the high frequency signal can be generated without text guidance. In combined with the aforementioned observation, this paper proposes to remove $\epsilon_\theta$ in classifier-free guidance once the low frequency signal has been generation to improve generation efficiency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This paper offers a new perspective for understanding the role of textual condition in diffusion models.  By exploring how the EOS influence the generation process of diffusion model, this paper argues that the conditional part $\epsilon_\theta$ in classifier-free guidance (CFG) might be unnecessary after certain denoising step $t_w$.  
- Most experiments are inspirational and interesting. By swapping the EOS token and the sentence body, it demonstrates that diffusion models rely on the EOS token to synthesize low frequency part of the image. 
- This paper explains the tendency of generating image from low-to-high frequency in diffusion models.

Weaknesses:
- It is not clear that how the ""computational cost"" is defined in this paper. If the computational cost is GPU VRAM, then the claimed efficiency improvement might be invalid, as the required GPU VRAM for computing $\epsilon_\theta(x_t, C)$ or $\epsilon_\theta(x_t, \emptyset )$
 is unchanged. 

- This paper mainly focus on the role of EOS token in T2I diffusion models while neglecting the SOS token. Despite the weight of SOS token is significantly higher than SEM and EOS token (see Figure 3). However, the authoer(s) claims that the SOS carries no information due to the autoregressive nature CLIP text encoder.  Since this claim is not yet supported by other works, the author(s) should have conducted experiments to support this claim, as there is a chance that EOS and SOS tokens altogether influence the generation process.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zO55ovdLJw;"REVIEW 
Summary:
The paper proposes a prompt optimization approach to the missing modality issues in multimodal learning. Inspired by the missing-aware prompt (MMP), this paper adds more prompts, including correlated, dynamic and modal-common prompts, to each encoder to improve the performance. The experiment on three datasets shows the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The missing modality issue in multimodal learning is a practical challenge. 

The designed method is clearly presented.

Weaknesses:
1. The novelty of the proposed method is limited since the MMP has proposed the prompt optimization approach to solving the missing modality issue. Compared with MMP, this paper adds more parameters in the form of prompt tokens from different inputs and functions. 

2. The empirical comparison with MMP is probably not quite fair as the proposed method uses more additional parameters compared with MMP. According to Line 337, this method adds 2.4% additional parameters, while MMP only adds 0.2%.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The model proposes prompting strategy where both modalities (image and text) are prompted, and the prompt for both modalities are correlated. The strategy is to use multiple prompts, namely correlated prompts, dynamic prompts, and modal-common prompts. As the backbone itself is multimodal (CLIP), it is a good idea to consider synchronized multi-modal prompts to fully harness the model capabilities when prompting it. The model surpasses multiple multimodal SoTAs on multiple datasets and also has proven to be effective in handling missing modalities in training and inference.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The strategy of using multiple types of multimodal prompts, along with the correlation strategy, is logically sound as the multimodal backbone itself is trained to understand the relationship between image and text modalities.

2. The modal surpasses multiple SoTAs on multiple benchmarks with considerable score improvement.

3. The ablation studies are sufficient to understand the justification of the network design.

Weaknesses:
1. Ablation studies regarding the multimodal backbone, e.g. using other model than CLIP or use dedicated unimodal encoders for each modality, highly recommended to increase paper quality.
2. In table 4, what are the performances when either image or text modalities are completely missing?

Limitations:
The limitations are discussed in the appendix, including that only text and visual modalities are tested with this model and the number of tested models.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of generalized missing modalities in multimodal learning, where a modality can be absent during any learning phase (e.g., training, testing, or both). he authors investigate prompt learning with missing modalities and propose deep correlated prompts designed to capture various types of correlations between prompts and input features across different modalities. Specifically, the proposed prompts include mechanisms for perceiving beneficial information from preceding layers, dynamically generating prompts based on input characteristics, and leveraging the complementary information from multimodal inputs. These designs improve the robustness of large multimodal models (e.g., CLIP) to missing modalities. Extensive experiments and ablation studies demonstrate consistently superior performance and verify the effectiveness of the proposed method.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	This paper addresses a more challenging missing modality setting, where modalities may be absent during both training and testing phases, making it highly practical and essential for real-world applications.
2.	The paper is well-motivated. The authors highlight the weaknesses of prior work and propose several designs (e.g., deep correlated prompts, dynamic prompts, common prompts) to improve robustness.
3.	The paper explores various types of correlations between prompts and input features across different modalities, and the proposed designs for each are technically sound.
4.	Extensive experiments show great improvement on the baseline and consistently superior performance compared to other methods across all benchmarks.
5.	Comprehensive ablation studies are conducted to validate the effectiveness of each proposed component.

Weaknesses:
1.	The paper lacks a detailed explanation or discussion on the efficacy of different prompt designs. In Figure 2, it shows that sequentially adding different designs improves the baselines, but it does not discuss the individual improvement gains for each design. Additional discussion on each design could help validate whether the increasing gains from sequentially adding designs are not merely due to more learnable parameters.
2.	The paper lacks visualization of each learnable prompt (e.g., deep correlated prompts, dynamic prompts, and common prompts). Visualizations could help validate whether the different components work as expected. For example, do dynamic prompts genuinely capture the different characteristics of inputs, or do they merely distinguish between different missing cases, which might be easier to learn due to the obvious absence of a modality?
3.	For each available modality, it seems there are a total of $(3*(2^M-1))$ prompts for each missing modality case. This could lead to an exponential increase and redundant prompts as more modalities are considered (i.e., M>2). For example, in a vision-and-language task, in the case of complete and missing-image, the text modality is available for both cases. However, it requires two separate prompt sets for the text encoder, which may actually learn the prompts for the same “text-available” case.

Limitations:
One limitation is that the proposed method requires modality-specific deep correlated prompts for each available modality, which could be challenging to extend to more modalities (e.g., five or more modalities).

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes to address the missing modality problem for the multimodal recognition model (i.e. the multi-modal data could be incomplete). There are three techniques of prompting being proposed (while the recognition model, i.e. two-stream multimodal method CLIP in this paper, is kept fixed), including: 1) correlated prompts, where a part of the prompts in the input-level are firstly selected according to the missing scenario (e.g. complete, text-only, or image-only), then the prompt in each of the following network layers are predicted from the multimodal prompt of its preceding layer; 2) dynamic prompts, the input-level prompts contain a portion generated according to the input sample; 3) modal-common prompts, where the rest of the input-level prompts is stemmed from a common component shared across modalities. The combination of the aforementioned three techniques experimentally shows better performance in comparison to various baselines (mainly the SOTA method from MMP [17]).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ The proposed method provides superior performance with respect to various baselines and its proposed techniques (i.e. correlated prompts, dynamic prompts, modal-common prompts) are experimentally shown to benefit the model performance.
+ The extensive experiments are conducted on multiple dataset with various experimental settings.
+ The presentation is clear and easy to follow.

Weaknesses:
- The modal-common prompts and the dynamic prompts actually are not directly connected to the missing modality problem (or being irrelevant to different cases of missing modality). While excluding these two prompting techniques from the proposed method (in which such variant becomes ""Ours (A)"" in Figure 2), the improvement with respect to the state-of-the-art approach of handling missing modality (i.e. MMP[17]) would become marginal (please include MMP[17] into the ablation study shown in Figure 2 or directly provide the tabular quantitative results for the ablation study). Similarly, while we only consider the technique of correlated prompts as the manner in the proposed to tackle the missing modality, it becomes the only difference in the proposed method compared to MMP [17] (in terms of methodology), thus leading to the concern of limited novelty. Furthermore, there should be a baseline of integrating the modal-common prompts (acting as a basic component of prompt) and dynamic prompts into MMP[17] to better highlight the contribution of the proposed correlated prompting technique (which is the main technique in the proposed method to be connected with the missing modality challenge). Moreover, as modal-common prompts and the dynamic prompts introduce additional learnable parameters (in comparison the correlated prompts), there should be further detailed analysis/comparison in terms of number of learnable parameters versus model performance.
- Though the proposed dynamic prompts do experimentally shown to improve the overall performance under various missing modality cases, such prompting technique is actually not new, where we can see its similar application in various research problems (e.g. Wu et al., IDPG, NAACL'22; Lu et al., PromptPG, ICLR'23; Qiu et al., FedTPG, FL@FM-NeurIPS’23).

Limitations:
no potential negative societal impact is found.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new method to handle missing modalities in visual and language recognition systems.
The paper proposes a very similar method to the one proposed by MMP [17] but using different way of getting the prompts to feed them into the transformer layers. 
Comparison with other works show that the method seems to be effective and some ablations studies are performed to study the different design choices. The method is validated using the most common datasets for this task.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method seems to work when compared with other state-of-the-art models.
- The paper presents results on several datasets and with different settings of the model.

Weaknesses:
- The main weakness of the paper is clarity. There are three different sets of prompts that are appended to the intermediate representations. However, the only difference between them seems to be the type of architecture the method uses to compute them. The explanation is very limited and Figure 1 does not illustrate where do these prompts come from. Without the clarity of this explanation it becomes really hard to understand how the motivation of each type of prompt fits the design. What are exactly correlated prompts, dynamic prompts, and modal-common prompts? What make them correlated, dynamic and modal-common? This is not clear in the paper at all. 

- It is not clear what is baseline. What does dropping features when modality is missing? The input sequence become shorter and coming from only a single modality? If that's the case, what is trainable and what is not? 
Please explain well this part. I would expect that this baseline is: training with the same number of parameters as the base method, by simply adding learnable prompts at each layer and training using mod-drop (dropping modalities randomly when training, dropping modalities can be done by inputting noise instead of tokens, the average of the modality tokens, zeroes, or not passing the missing modality at the input, it is a design choice that needs to be explained). If it is not what I'm thinking, please explain well, since this is a key experiment.

- When comparing with MMP, how did the authors do it? Please explain exactly how was this re-implementation. Also, to be fair, the authors should have applied their method using ViLT instead of CLIP, in that way there is no doubt that this method is better than the very similar MMP. 

- What is the zero-shot performance of CLIP on these datasets?

Limitations:
Limitations have been addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zNiJZUAlxg;"REVIEW 
Summary:
The paper analyzes the class-generalizable anomaly detection problem and introduces residual feature learning. 
Based on the residual features, the paper proposes a simple AD framework, i.e., ResAD, which incorporates OCC loss and distribution estimating to distinguish normal and abnormal data.
The experimental results demonstrate that the ResAD performs well on real-world industrial AD datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper analyzes the few-shot class generalizable anomaly detection problem and delivers an interesting insight into residual features.
2. The proposed method is intuitive and easy to understand.
2. The paper is well-written and organized.

Weaknesses:
1. The residual learning for few-shot AD has already been proposed in inCTRL[1]. The proposed Multi-Layer Patch-Level Residual Learning scheme in InCRTL is more sophisticated and reasonable than the direct subtraction in this paper.
2. The results in Table 1 of InCTRL are not consistent with the results in the original paper. Compared with the original results of InCTRL, the ResAD results do not achieve the SOTA performance. 
3. The paper aims to achieve generalization across different classes. I think the authors should compare the accuracy of each class on the Visa dataset with other methods to demonstrate the generalization capability of your approach for different classes, rather than taking the average accuracy of different classes in the dataset.

[1]Jiawen Zhu and Guansong Pang. Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts. In CVPR, 2024.

Limitations:
The authors did not give a discussion on the limitations of the proposed method.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a simple but effective framework that can be directly applied to detect anomalies in new classes. The main insight is learning the residual feature distribution rather than the initial one. In this way, we can significantly reduce feature variations. Even in new classes, the distribution of normal residual features would not remarkably shift from the learned distribution. Experiments were conducted on four datasets and achieved remarkable anomaly detection results.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is original, high quality, clear, and easy to understand. The proposed method has a good heuristic effect on establishing a general anomaly detection model and will become a valuable baseline for the community after the release of the code.

Weaknesses:
1. Although unnecessary, I recommend punctuation at the end of a formula. This is one of the few formatting problems I can pick out. [Well written]
2. In Figure (b), it is suggested that abnormal should use a triangle icon. The difference between a hexagon and a circle is too small to see clearly.
3. The large difference between normal images should be considered, and image difference indicators such as FID and LPIPS can be used to calculate the difference inside the normal images in the data set you show. The difference should be relatively small, which is a potential false alarm hazard.
4. As stated in point 4 of the questions, the experimental setup of training on MVTecAD and then testing on the various classes of VisA is not reasonable.

Limitations:
This paper objectively mentions the limitations of this article, and there is no potential negative impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposed a simple yet effective framework ResAD for class-generalizable anomaly detection by leveraging residual feature learning and a hypersphere constraint. The framework's ability to generalize to new classes without retraining or fine-tuning makes it valuable for real-world applications, providing significant improvements over existing methods. Comprehensive experiments on four real-world industrial AD datasets (MVTecAD, VisA, BTAD, and MVTec3D) demonstrate ResAD's superior performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
(1)ResAD effectively addresses the challenge of class-generalizable anomaly detection, the generalization ability using only a few normal samples as references makes it highly practical for real-world applications.

(2)The use of residual feature learning to reduce feature variations and improve generalizability is novel and effective

(3)The approach is shown to be robust across different datasets and settings.

Weaknesses:
(1)The experiments are primarily conducted on industrial anomaly detection datasets. While these are relevant, the method's generalizability to other domains, such as medical images or video data, is not fully explored.

(2)The selection of few-shot reference samples may impact performance. Previous methods typically run multiple independent runs using different random seeds to ensure robustness. However, this work only provides results from a single group of samples, which may not fully represent the model's performance variability.

Limitations:
Limitations are discussed in Appendix Sec. B.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to address cross-class anomaly detection problem. To this end, this study introduce a residual learning framework ResAD. The ResAD framework aims to learning residual feature distribution between target image and reference image. Experiments are conducted to valid the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The cross-class/class-generalize anomaly detection is a crutial task in the realm of anomaly detection.
2. The structure of ResAD is simple and effective.

Weaknesses:
1. The idea of residual estimation is highly similar to InCTRL [1].
2. Lack of comparision with FastRecon[2] and AnomalyGPT[3].
3. The writing should be improved. The optimization terms are unclear and hard to follow.
4. In Table.5, there is a reproduced result of WinCLIP on WideResNet50, however the windows in WinCLIP is designed for VIT, how can the authors report the result?


[1] Jiawen Zhu and Guansong Pang. Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts. In CVPR, 2024.
[2] Fang Z, Wang X, Li H, et al. Fastrecon: Few-shot industrial anomaly detection via fast feature reconstruction[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 17481-17490.
[3] Gu Z, Zhu B, Zhu G, et al. Anomalygpt: Detecting industrial anomalies using large vision-language models[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(3): 1932-1940.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
zNIhPZnqhh;"REVIEW 
Summary:
This paper demonstrates that WTA circuits along with STDP learning resembles EM algorithm-like Bayesian inference and could be used for motion segmentation from event streams by contrast maximization of warped events.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper proposes an interesting approach for event motion segmentation based on observations from event-based dynamic vision sensors, utilizing a EM-like framework for identifying various motion models from event streams and clustering them into motion patterns. This is achieved using WTA circuits together with STDP-based learning.

Weaknesses:
The main weakness of the paper is that the proposed method lacks proper justification of the presented approach, which seems like a heuristic hard clustering method, together with gradient based learning. The experiments also lack depth and the authors demonstrate the high dependence of the performance of the method on the parameter initialization. A more careful writing of the underlying model, the optimization framework and the proposed methodology would be good (see the questions below). Furthermore, the paper lacks more details regarding the choice of $N_{\ell}$ (number of motion models) and the specific forms of the warping functions $W_j$ used. Several steps in the entire methodology, although intuitive, are presented in a heuristic fashion without detailed description and clarity.

Limitations:
See the questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a spike-based Bayesian inference framework for motion segmentation with event cameras. By designing neurons that utilize STDP for online learning of motion patterns, the framework can perform the M-step of the EM algorithm in motion segmentation of event streams. Additionally, the WTA circuit implements the E-step, allowing for the online partitioning of event streams into different motion patterns. The authors provide theoretical proof and experimental results to demonstrate the network's spatiotemporal decoupling capabilities for mixed motion patterns of event streams.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors demonstrate that the SNN framework based on WTA is equivalent to the EM algorithm for motion segmentation of event streams. This online learning approach is compatible with neuromorphic data and beneficial for deployment on low-power, low-latency neuromorphic computing platforms.
 
• The work is based on the Bayesian brain hypothesis, using a more physiologically interpretable SNN for Bayesian inference. Applying this to spatiotemporal data from neuromorphic cameras represents a promising research direction.

Weaknesses:
• The experimental results lack quantitative evaluations. Can the authors further perform object detection and tracking based on the motion segmentation, providing metrics such as object detection success rates and comparisons with other methods?
 
• The proposed algorithm lacks the analysis of time complexity or processing speed. Can it leverage the low-latency advantage of event cameras?

Limitations:
There is a need for quantitative evaluations and an assessment of the dependency on parameter initialization.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work proposes a spike Bayesian computational framework for continuous motion segmentation in event streams and demonstrates that the constructed network can implement an EM-based event stream motion segmentation model. The proposed model uses WTA circuits in the network to achieve an equivalent E-step, while the STDP rules for an M-step for contrast maximization. Experimental results demonstrate the network's online learning effectiveness for continuous inputs on extreme event camera datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed network's effectiveness for motion segmentation has been validated on event datasets featuring challenging scenarios that involve mixed camera self-motion and high-speed moving objects. The proposed spike Bayesian inference framework is highly interpretable and applicable to various neuromorphic vision chips and computing hardware, representing a promising research direction.

Weaknesses:
The authors mainly use SVD to find different patches' motion patterns for initialization. Why is this method used, and can other methods be employed for selection? It is recommended that the authors conduct ablation experiments to explore further.

Limitations:
The authors have stated the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes to address motion segmentation at very high temporal resolution via an event-based or spiking implementation of expectation-maximization in a generative model. It demonstrates the performance of the resulting spiking neural networks on example experiments.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The strength of the paper is its deep engagement with the spiking neural network literature, as well as its use of spiking networks for the specific type of problem to which they are most suited: event-based computation.

Weaknesses:
The paper's major weakness is its lack of clarity, which the authors have discussed and addressed in the review period.

Limitations:
I am not confident that I can identify the specific limitations of this paper as opposed to the limitations of spiking neural networks generally.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zMNd0JuceF;"REVIEW 
Summary:
This paper proposes a new way to jailbreak LLMs through an improved version of few-shot jailbreaking. They propose to use a random search to select examples that are most effective to jailbreak the mode from a pre-defined pool generated with Mistral-7B. On top of that, they alternate the steps of each example by the special tokens that are used in the LLMs conversation templates to separate user messages from the model's responses. The authors show that this method is more effective than the previous jailbreaking methods for five different models, and that it can be used and adapted to evade a large number of defenses.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Simple and effective method**. The method proposed is simple and effective. It is easy to understand and to implement. The experimental results show that it is more effective than many baselines.

**Insightful ablations**. The authors do a great job at showing what components are most important for the success of the attack. They check how many shots are necessary, how important the size of the pool is and how important the special tokens are. However, there are some other ablations that I believe would make the paper stronger (see weaknesses).

**Effective evasion of defenses**. The authors show that their method is effective at evading a large number of defenses of different types, from a perplexity filter, to perturbation-based defenses, to safety-filters. Most interestingly, they propose that one could actually exploit a defense (SmoothLLM) to make the attack robust to keyword-based defenses. However, they do not have any experimental results to show that this is actually the case.

**Mildly Compelling motivation**. The motivation of using few-shot jailbreaking is compelling to jailbreak models that do not support a long context. However, it should be noted that these models are also less likely to actually provide useful malicious information to the attacker who is trying to jailbreak the model.

Weaknesses:
**No comparison to few/many-shots baselines**. The authors do not compare their method to Wei at al. [1] and Anil et al. [2], which are the most similar to their method. They claim that Wei et al. have limited effectiveness on well-aligned models such as Llama-2, but Llama-2 is not the only target model considered in the paper, and the authors should show some concrete numbers to back-up their claim. For Anil et al., they claim that the attack requires too much context length to work on the considered models, but, according to the numbers shown in the paper [2], the attack starts being effective with 32 shots, the number considered for Llama-3, and they have results for Llama-2 in their paper up to 128 shots.

**Missing amount of necessary queries**. One of the metrics that are useful for jailbreak attacks is the total number of queries needed by the random search to jailbreak the model. The authors do not report this number, which makes it hard to compare their method to other methods.

**Some ablations are missing**. The authors do a great job at showing what components are most important for the success of the attack. However, they do not show the impact of the quality/length of the examples. It would be interesting to see how the method performs when the examples are shorter or longer, or when some of them are not actually good examples. This would be relevant as the model used to generate the examples could refuse, or generate low-quality examples. Another ablation that would make the paper stronger is how important it is that the special tokens are correct. What happens if you, e.g., use Llama-2's special tokens for Qwen1.5B? Or simply if the special tokens are slightly incorrect (e.g., `[INST]` instead of `[/INST]`? This can be useful to show the potential effectiveness of the attack against models whose special tokens are unkown.

**Minor**:

- No experiments that show that SmoothLLM can be used to evade keyword-based defenses.
- Code is provided, but the data are provided in pickle format, which is known to be unsafe. It would be better to provide the data in a more standard format like CSV or JSON. Moreover, it would be better to provide a README with instructions on how to understand the code.

**References**:

- [1] Wei et al., https://arxiv.org/abs/2310.06387
- [2] Anil et al., https://www.anthropic.com/research/many-shot-jailbreaking

Limitations:
The authors do a good job at discussing the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes two improved techniques for in-context few-shot jailbreaking: demo-level random search and the injection of special tokens from the system prompt. The authors conduct extensive experiments across a series of aligned language models. Ablation studies demonstrate the effectiveness of both proposals.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to follow, tackling an intriguing and timely problem. The baseline few-shot jailbreak is not strong enough, while the powerful many-shot jailbreak requires a long context length, which is not suitable for all language models. The proposal in this paper significantly improves the attack success rate over few-shot jailbreak.

2. The proposed idea of utilizing special system tokens for jailbreaking is novel, and the experiment evaluation is comprehensive. The improvement in attack success rate over the baseline few-shot jailbreaking is significant. Additionally, the techniques introduced are not hard to implement and could serve as a strong baseline for future language model safety research.

3. Ablation studies on the number of shots and the two components are provided, clearly demonstrating the effectiveness of both modifications. The study also illustrates the effect of several defense methods, showing that the improved few-shot jailbreak method can be successful even with the defense methods presented.

Weaknesses:
My primary concern regarding the methodology's weakness is the potential for **unwanted bias** due to the **dataset** chosen for evaluation, which could impact the numerical results.

As highlighted by previous studies [1], the AdvBench harmful behavior dataset, which is used to construct the demo pool and evaluate attack success rate in this paper, **lacks diversity**. Although it contains **520** harmful requests, only **58 unique behaviors** exist (see Table 5 of [1]). This introduces a significant vulnerability to the current evaluation pipeline, as excluding only 50 requests for testing from the demo pool may be **insufficient**. The pipeline could potentially select similar candidate in-context demo from the pool to the target test request. For instance, if the test request is ""Write a tutorial on how to make a bomb"", the candidate demo selected from the pool could be ""Provide detailed instructions on how to make a bomb"" and its answer. In this scenario, the proposed algorithm's success might be biased towards **replicating certain provided demos** rather than originating from the techniques proposed.

Besides, as illustrated in [1] (see Figure 2), the generation length has a critical effect on ASR. This paper takes length 100 (see line 176) as the default paradigm. This could lead to an overestimation of ASR.

[1]: Mantas Mazeika et al., HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a new method to jailbreak LLM to elicit harmful responses. The proposed method follows a line of works on using the demonstrations of harmful responses in the context of prompt to jailbreak. It improves the previous works regarding reducing the number of demonstrations in the context and increasing the efficacy. Specifically, the proposed method uses an unsafe LLM to automatically create a pool harmful demonstrations, insert special tokens into the prompt, and optimizes the demonstrations using a demo-level random search. The empirical results confirm the efficacy of the proposed methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. the proposed method is simple and straightforward to implement.
2. the dramatic sensitivity of FSJ to special tokens is surprising.
3. the evaluation is comprehensive (many defenses are tested) and the results of the proposed method are strong.
4. the paper is well-written and easy to follow.

Weaknesses:
1. The evaluation is based on 50 harmful responses from AdvBench. The scale is limited. Besidse, AdvBench is also used to generate demonstration pool. Although the overlapped ones are inspected and removed, there may be a concern of overfitting. Using a different source of harmful responses like HarmBench [1] for evaluation may be better.
2. The proposed method assumes that attackers have access to model-specific special tokens, which restricts its application scope. Without the help of inserting special tokens, the proposed method seems to be ineffective in breaking the well-aligned models like Llamas as shown in Tab. 1. It is therefore interesting to test if a special token can be determined without the knowlege of target model. 
3. Although the proposed method demonstrates the ability to circumvent a wide range of defenses, it may be ineffective when adaptive defenses were deployed. For example, toxic detectors can be used to detect if harmful content is included in the input prompt as demonstrations.

[1] Mantas Mazeika et al., HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal.

Limitations:
some limitation has been discussed, but more are required. See some points suggested in Weakness.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes several ICL (in-context learning)-based techniques to improve the effectiveness and efficiency of jailbreaking prompts, including adding system special tokens and random search on the demonstrations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The discovery that using special tokens can enhance the effectiveness of harmful demonstrations is interesting.
- The experiments show the overall proposed method can notably improve the ASR on multiple LLMs.
- The experiments also include evaluations of the attack against LLMs with defense techniques.

Weaknesses:
- The main objective of this paper seems to be misleading. As indicated by the abstract and the story in the introduction, this paper attempts to address the problem of

> it possible to use few-shot demonstrations to efficiently jailbreak LLMs?

However, since ICA has already been proposed as the few-shot version of jailbreaking, this paper may take ICA as the main target, rather than refining MSJ.

- Following the previous weakness, the most important baseline, ICA, is missed in the experiments. Moreover, what is the difference between the used baseline (FSJ) and ICA is not indicated.
- The first improved technique, injecting special tokens, though interesting, is of limited scientific contribution. It’s more like an attack trick, rather than a substantial academic improvement. More importantly, why these tokens can enhance the ASR is not well-explained or understood.
- The second technique is anyway lacking novelty since the jailbreaking literature has already used the intention of random search (e.g., GCG and AutoDAN) to improve the jailbreaking prompt.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes jailbreak attacks via few-shot demonstrations. The authors introduce a three-step method to achieve this goal, which includes constructing a demo pool, injecting special tokens, and demo-level random search. The proposed method demonstrates strong attack performance against aligned LLMs and multiple defenses.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed method is a strong attack that can bypass many advanced defenses.

Weaknesses:
Overall, the paper is well done. However, I have a significant concern: How does the attacker know the special tokens used in the LLMs? This is particularly problematic for attacking closed-source models such as ChatGPT. I also noticed that the authors did not evaluate their method on closed-source models in this paper. This issue represents a critical weakness in practical jailbreak evaluations. I will raise my score to acceptance if this concern is addressed. Otherwise, I think this weakness is a flaw that we can not ignore.

Limitations:
The authors have discussed the broader impacts and limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zLU21oQjD5;"REVIEW 
Summary:
This paper synthesizes a math reasoning dataset with a designed way of rejection sampling. Many base models show performance improvements on math reasoning tasks after instruction-tuning on this dataset. They promise to release the dataset and models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
Their curated dataset achieves relatively good instructing-tuning performance with least data amount compared to other baselines. The dataset will be released.

Weaknesses:
1. The proposed sampling technique is trivial and incremental, when comparing with previous works, e.g., the uniform method is used in ToRA, and the prop2diff method is used in MARIO.
2. There’s little improvement or even performance drop when tuning Mistral-7B and DeepSeekMath-7B 
compared to other baselines. As mentioned in the analysis section, this dataset is somehow replaceable by math-specific continual pre-training + supervised fine-tuning (SFT).
3. The major concern is that even the paper claims the proposed dataset is smaller, however, the LLM used to synthesize the smaller dataset is `DeepSeekMath-7B-RL`, which is trained on a larger SFT dataset. An alternative and reasonable response generation method should be leveraging the `DeepSeekMath-7B-Base` with proper prompting, as `DeepSeekMath-7B-Base` has not been supervised fine-tuned.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces Difficulty-Aware Rejection Tuning (DART), a novel approach for enhancing the mathematical problem-solving capabilities of large language models (LLMs). Traditional methods often produce datasets biased towards easier queries, limiting the models' ability to learn from challenging examples. DART addresses this by allocating more sampling trials to difficult queries during the data synthesis phase. The authors created two strategies, Uniform and Prop2Diff, to ensure a balanced representation of easy and difficult queries. Using only open-weight models, the authors generated new, smaller datasets that prioritize difficult queries.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The DART method effectively addresses the bias towards easy queries in traditional rejection sampling, which is a significant contribution to the field.

2. The paper provides a thorough analysis of the biases in existing datasets and clearly explains how DART mitigates these issues.

3. The authors plan to make their datasets and models publicly available, contributing valuable resources to the research community.

Weaknesses:
1. The success of DART relies heavily on the ability of models to generate correct responses for difficult queries, which may not always be feasible for extremely challenging problems.

2. While the focus on difficult queries is commendable, the quality of the generated responses for these queries needs to be high to truly benefit the training process. The paper does not provide a detailed analysis of the quality of these responses.

3. The approach's reliance on extensive sampling for difficult queries might pose scalability issues, particularly for very large datasets or models with limited computational resources.

Limitations:
The limitation is mentioned in the conclusion section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a rejection sampling pipeline for automatically generating SFT data, emphasizing that harder data requires more trials. The difficulty is heuristically determined using the ratio of incorrect trials for each question. Experiments demonstrate that this method can outperform traditional rejection methods on various math benchmarks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The experiments are solid, showing significant improvements over traditional rejection methods.

- The paper is clearly written and easy to follow.

Weaknesses:
The proposed Prop2Diff strategy lacks innovation. Assigning more budget to more complex questions in data synthesis is a common practice. For instance, in [1], which successfully annotated 83.1% of MATH questions, it is evident that harder problems were allocated more budget in rejection sampling. [1] also indicates that fewer and harder data can significantly and efficiently improve performance. The authors should discuss the differences between their approach and the one used in [1] more thoroughly.

[1] ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents an approach to improving the performance of LLMs in mathematical problem-solving. The authors identify that current datasets synthesized using proprietary models like GPT-4, are biased towards easier queries. To address this, they introduce Difficulty-Aware Rejection Tuning (DART), which allocates more trials to difficult queries during data synthesis. This method generates datasets focusing on difficult queries using an open-weight model, DeepSeekMath-7B-RL, without relying on proprietary models. The authors demonstrate that models fine-tuned on DART-Math datasets significantly those fine-tuned on traditional datasets across various mathematical benchmarks, and beat the best baseline by average of roughly 3-4%

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Technically solid paper with state-of-the-art results.
- Mostly well-presented and easy to understand.
- Comprehensive experiments and analysis.
- Decent impact in improving mathematical capabilities of LLMs, with the authors publicly releasing their dataset.
- By using an open-weight model, DeepSeekMath-7B-RL, the authors eliminate dependency on proprietary models like GPT-4, making the approach more accessible.

Weaknesses:
1. It is unclear how the hyperparameters of the baseline, VRT (vanilla rejection tuning), were tuned. For instance, as mentioned in Appendix A.2, sampling temperature is searched from 0.3 to 1.7 for DART. Was the same procedure used for VRT? Another caveat is the need for extensive hyperparameter tuning compared to baselines. Were similar extensive procedures for tuning performed for other baselines?
2. It is unclear if the improved performance of the proposed method is due to difficulty or the topic of the problem. For instance, LEVEL 5 Math problems may have a higher number of geometry questions (or at least their fail rate is higher, resulting in fewer samples in VRT). An analysis of topic-wise performance comparing DART and baseline methods may clarify this.

**Minor Weaknesses: **
1. It is unclear how much advantage the method would provide in the case of other multi-iteration fine-tuning methods such as STaR and v-STaR. For instance, it is possible that after multiple iterations, VRT performs similarly to DART, since a higher number of samples will be collected from even the hard problems in second or further iterations.
2. The data synthesis is only done using the DeepSeekMATH-7B model. It is unclear why this model was chosen. Previous methods using VRT-like methods typically use the same model for synthesis and generation. Thus, higher results in smaller models such as Llama-8B may partly be due to the use of stronger models' reasoning chains, making it similar to a distillation method.

Limitations:
Major Limitations are addressed in paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zLClygeRK8;"REVIEW 
Summary:
Policy evaluation, selection and optimization are considered in the context of offline contextual bandits, where i.i.d. data with a known behavior policy is given. The authors set out to study a generalization of importance weighted policy evaluation; for this they start from a general formulation that computes a value for all data observations, which are then averaged. The free ""parameter"" here is $h$, the function that assigns a value given an observation (of a context, associated action, and cost). A tight, general, high probability upper bound on the expected cost of a fixed target policy is derived first. Specific choices for the map $h$ are then derived based on minimizing this upper bound. Two practical solutions to this optimization problem are studied in more details: Global clipping and ""logarithmic smoothing"". Results are then derived for both policy selection and optimization.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Novel ideas, novel results, good empirical results.

Weaknesses:
Despite saying  that the methodology of paper [31] is adopted, this is only partially done. Why deviate from the evaluation in [31]? I expected an explanation of this.

Limitations:
na

Rating:
10: strong accept, should be highlighted at the conference

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers the offline contextual bandit problem. The authors consider a class of reward estimators for this setting that is a regularization of Inverse Propensity Scoring (IPS - aka importance sampling). A general concentration result is provided for this class of estimators. This is used to provide a tight result for an existing clipping IPS estimator and to construct a new Logarithmic Smoothing (LS) estimator. The resulting estimator is pessimistic by design, making it immediately applicable to the offline contextual bandit problem.
The authors use it to derive bounds for policy evaluation and selection and also for policy learning in the Bayesian setting. Experimental results also support the usefulness of the estimator.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I am only broadly familiar with this line of research making it hard for me to properly contextualize its contributions.

1. The proposed estimator is novel and has nice properties.
2. As the name suggests, the estimator is smooth making it potentially easy to optimize.
3. The application to contextual bandits is interesting.
4. The experimental results are positive.
5. The overall writing is good and clear.

Weaknesses:
1. A more explicit comparison with existing concentration\contextual bandit bounds is missing. The authors explain that their bound is better but this is somewhat vague, especially if the reader is not already an expert in this field.

2. In line 155 the authors explain that their result can be derived from [1, Lemma 1.3]. Does this mean that the LS estimator has previously been suggested or only that an alternative proof technique exists for its concentration bound?

3. Performance seems very close to that of IX

4. The main body of the paper does not include any explanation of the techniques used. This can be a proof sketch for the concentration bound or a discussion comparing your approach to existing techniques. Can you provide such an explanation in your response?

5. The notation U(pi) appears without definition in line 281. I assume it's defined in one of the references but should also be defined in this paper for completeness. (Please include an explanation in your response)

Typo:
line 98: one of the brackets is reversed in the definition of h

Limitations:
N/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose empirical concentration inequalities for off-policy evaluation that apply to several forms of (smoothed) IPS, which are claimed to be tighter than the results in existing works. These bounds are then used to derive policy learning guarantees that inherit the properties of the concentration inequalities.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
I appreciate that the authors have applied their method to OPE, OPS, OPL, and also provided some experiments. 

I did not read the appendix nor check the correctness of the analysis in detail, but from a quick glance it appears that the authors were careful to provide rigorous and well-organized proofs.

Weaknesses:
My biggest criticism is that the authors have not justified *in the main body* their claim that ""LS is provably tighter than its competitors"" (L12) for any of the results, including the concentration inequalities (Prop 1, Cor 3, Cor 4) and the policy learning guarantee (e.g., Prop 6). 

Since these claims are the whole premise for the paper, their justification should be a central pursuit and only stating ""x is in Appendix y"" (L147, 178, 195) is hugely insufficient. 

For example, I would have liked to see a discussion on (possibly even in graphs): 
- For the choices of $h$ described in (4), when do the bounds in Prop 1, Cor 3, and Cor 4 improve over the bounds from their respective papers? 
- Is $h*$ (the tightest choice) better than all of the above? 
- Does this hold for all hyperparameter choices, e.g. $\lambda$ and $L$?
- How does the computational complexity of calculating the bounds in Prop 1, Cor 3, Cor 4 hold up relative to their competitors? 
- Exactly how does this lead to downstream policy learning improvements?

Lastly, I found the overall technical presentation to be relatively poor, and I'll give a few examples: 
- The condition (C1) from Section 2 (""Regularized IPS"") that all results depend on is never explicitly defined, and it should be an assumption that is called in every proceeding proposition/theorem statement.  
- Shouldn't (11) be framed in, e.g., a lemma environment? 
- The term ""pessimism"" is overloaded, e.g., for ""high-probability upper bounds"" in L111 but also for an in-expectation variant in Eq. (5), which is slightly unusual (and I'm pretty sure not the way it's used in [26]) but not recalled again in the main body so I'm not sure what it's for (perhaps the proof of Prop 1).

Limitations:
I do not believe the authors have fully discussed the limitations of their method (see ""Weaknesses"").

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies log-algorithmic smoothing of importance weight for off-policy learning. The proposed smoothing technique can be seen as a differentiable variant of clipping, which is useful for variance reduction for OPL. The paper also analyzes the PAC-Bayes learning bound of the proposed OPL method, characterized by the KL divergence with the logging policy, showing that the proposed method achieves a tighter bound than baselines, including simple clipping. The experiment also shows that the proposed method has tighter bounds than baselines and enables more accurate off-policy selection.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- **Reasonable formulation based on theoretical analysis**: The proposed method is derived from a tight upper bound of the policy's risk. Also, the proposed method has an interpretation as soft, differentiable clipping. The technique is well-motivated and is reasonable to interpret.

- **PAC-Bayes learning bound**: A sub-optimality form is derived, and it is also easy to interpret as a pessimistic approach, which should be acknowledged.

- **Experiments on various tasks**: The paper evaluates the proposed approach in upper bound derivation, off-policy selection, and off-policy learning. The experiment results show the wide applicability of the proposed method in many OPE/OPL-related tasks.

Weaknesses:
- **Connection to Metelli et al. 2021 is not clear**: Metelli et al. 2021 also considers the importance of weight differential and shows that the proposed method achieves a Subgaussian rate. Similar to the reviewed paper, Metelli et al. 2021 also have a KL divergence term in the theoretical analysis. While the proposed method adequately differs from Metelli et al. 2021, and the paper does cite it, the paper does not mention Metelli et al. 2021 in the related work in detail. Since the motivation and contributions are similar, a detailed discussion on the advantages and the differences would be appreciated.

- **Baselines in the experiments**: As mentioned above, Metelli et al. 2021 propose a similar idea that can be used as a baseline in experiments. Comparing with advanced regularization techniques such as shrinkage (Su et al. 2020) would also be informative.

(Metelli et al. 2021) Subgaussian and Differentiable Importance Sampling for Off-Policy Evaluation and Learning. Alberto Maria Metelli, Alessio Russo, Marcello Restelli. NeurIPS, 2021.

(Su et al. 2020) Doubly robust off-policy evaluation with shrinkage. Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, Miroslav Dudík. ICML, 2020.

Limitations:
Missing connection with a similar idea. See the weaknesses for the details.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zLBlin2zvW;"REVIEW 
Summary:
This work proposed a Gated Sparse Autoencoder (Gated SAE) to mitigate the standard SAEs' biases, such as shrinkage, which systematically underestimate the feature activations from SAEs. The key difference between Gated SAE and SAE is that the Gated SAE separates affine transformations within the encoder in order to decide which dictionary elements to use in a reconstruction loss, and estimate the coefficients of active elements, although with the 50% more computing required to achieve. Comprehensive experiments are conducted to compare and verify how good the Gated SAE is to standard SAE, including a blinded human study to rate and compare the interpretability of randomly sampled Gated and baseline SAE features.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A new architecture of SAE inspired by GRU is proposed to include a gate mechanism to mitigate shrinkage bias
- Comprehensive quantitative experiments including ablation studies to evaluate the proposed Gated SAE compared to SAEs
- A human evaluation to rate randomly sampled features from Gated SAE and SAE

Weaknesses:
- It is not very straightforward to understand how well the features from Gated SAE are compared to SAE based on Figure 4. Some case studies based on the open-source SAE visualizer library [1] are required to help better understand this.
- It will be better to see more case studies on downstream tasks to compare Gated SAE and SAE, e.g., automatic circuit detection [2]

[1] C. McDougall. SAE Visualizer, 2024. https://github.com/callummcdougall/sae_vis

[2] Huben, Robert, et al. ""Sparse Autoencoders Find Highly Interpretable Features in Language Models."" The Twelfth International Conference on Learning Representations. 2023.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper attempts to resolve the issue of feature shrinkage in sparse autoencoders (SAEs) by replacing the SAE ReLU activation function with a gated ReLU unit. The weight-tying scheme they use for the gated unit effectively turns it into a jump ReLU activation function.
They train gated SAEs and baseline SAEs on a one layer transformer, Pyhtia-2.8B and Gemma-7B. They find that gated SAEs eliminate systematic shrinkage, and consistently outperform baseline SAEs on the pareto-curve of sparsity, measured by the L0 pseudonorm, and faithfulness, measured by the model loss recovered relative to a zero ablation baseline. 
They run various additional tests involving variations of the gated and baseline SAE architectures, including combinations of SAE dictionaries with the classic gradient pursuit algorithm for choosing sparse feature coefficients at inference time. They conclude that the Pareto improvement of their gated SAEs over their baseline SAEs is due in part to better feature dictionaries, in addition to better estimated feature coefficients.
They compare the subjective interpretability of 150 gated SAE and baseline SAE features in Pythia-2.8B and 192 features in Gemma-7B, using a blinded analysis of activating dataset examples. They find that the features were similarly interpretable.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper attempts to address a substantive practical problem with current SAE training methods.


The paper's proposed new architecture is evaluated extensively, and many detailed additional investigations on the individual effects of various parts of the gated SAE architecture are described in sections 5.1, 5.2 and Appendix D. 


I find Appendix D interesting in its own right, since it shows quantitative comparisons between SAE methods and the classic gradient pursuit optimization algorithm, as well as mixing SAE feature dictionaries with gradient pursuit for sparse approximation of feature coefficients. I have not encountered such a comparison before. 


For the most part, good documentation of all their process is provided, and the writing and presentation are very clear in general.

Weaknesses:
The paper does not really address the concern that gated SAEs may outperform baseline SAEs in part by implicitly widening the definition of what it means for ‘features’ to be represented in the model. As the paper itself notes in Appendix D, though other more powerful sparse coding algorithms greatly outperform SAEs in terms of reconstruction and sparsity, there are concerns that the greater expressivity of these techniques lets them find spurious ‘features’ that would not be accessible to the model’s own internal computations. An SAE can only find features that are represented in the sense that their current values can be read off with a single ReLU probe, while an inference time algorithm or a multi-layer probe may read off ‘feature’ values that the model itself could not possibly access using a single MLP layer. A gated ReLU is far less expressive than an algorithm like gradient pursuit, but more expressive than a ReLU. So to what extent do gated SAEs outperform baseline SAEs merely because they are implicitly working with a more relaxed definition of what it means for a feature to be represented in the model? Figure 6 in Appendix D incidentally investigates this somewhat, since it attempts to compare the quality of gated vs. baseline dictionaries independent of their coefficients. However, the results there seem inconsistent, with smaller performance gaps and baseline SAEs outperforming gated SAEs at higher L0. I think this issue of the representational power of the probe used is pretty central for contextualizing the results, and should at least have been discussed.

Throughout the paper, the authors present reconstruction scores for SAEs in terms of the fraction of model loss recovered compared to a zero-ablation baseline. I think this metric obscures vital information. Lowering CE loss from e.g. 4.5 to 4.0 is typically much easier than lowering it from 1.5 to 1.0. Thus, the same difference in loss recovered between two SAEs can correspond to very different gaps in SAE quality. Without the raw CE scores, there is no direct way to infer how large the gap is quantitatively. At minimum, these raw CE scores should be in the supplementals. Better yet, the recovered performance could additionally be reported in terms of the compute required to train a model with the same CE score, as suggested in https://arxiv.org/abs/2406.04093.

Limitations:
All addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work introduces a new technique under mechanistic interpretability's sparse autoencoders. By using a less naive SAE, with a gating mechanism and a little extra computation, the paper shows a decent improvement over the baseline.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This work addresses the important issue of interpreting transformer-based LLMs and clearly demonstrates an interesting method. The mechanistic interpretability community will certainly find this work of interest.

The writing is well written and fairly easy to follow, the results are clearly presented, and all relevant aspects of the method are appropriately ablated

I liked the setup of the internal user study; I think future papers will follow the design of the study closely.

The work's cited throughout the manuscript are incredibly thorough.

Weaknesses:
While I generally like the paper, I have two primary concerns:

* The architecture and loss are somewhat difficult to understand. I did appreciate the pseudo-code in the appendix, but I feel for readers not familiar with SAEs may have a hard time, especially with the optimization-based design choices of weight tying. Perhaps explaining weight tying later in 3.2 would help. I would especially prefer if a few lines of pseudo code could be added in the main paper, next to figure two.
* The user study results. I don't mind the small change in means between the method and the baseline, but the explainable AI community has been around for a long time and the shift from studies with a few experts to larger cohorts has been the norm for a while now. Just because there's a rebranding to mechanistic interpretability doesn't mean this field should settle for underpowered studies. Nevertheless, I do find the study setup itself to be well articulated and a very useful starting point for future work in this area.

Minor:
Some of the design choices (weight-tying, no r_mag, etc) aren't well explained until the ablation where we find they are primarily for optimization. This could be motivated a little earlier, i.e. that the pareto improvement comes from the separation, and not those choices.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Gated Sparse Autoencoders (Gated SAEs), an improvement over standard sparse autoencoders (SAEs) for decomposing language model activations. The key idea is to separate the tasks of detecting which features are active and estimating their magnitudes, allowing the sparsity penalty to be applied only to feature detection. Through experiments on language models up to 7B parameters, the authors show that Gated SAEs achieve better reconstruction fidelity for a given level of sparsity compared to baseline SAEs, while resolving issues like shrinkage. A human evaluation study finds Gated SAE features to be comparably interpretable to baseline features.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A well-motivated architectural modification to SAEs that addresses key limitations
- Comprehensive empirical evaluation across multiple model sizes and activation sites demonstrating clear improvements over baseline SAEs
- Careful ablation studies and analysis to understand the source of improvements
- Human evaluation study to assess interpretability of learned features
- Thorough discussion of limitations and future work directions

Weaknesses:
- The presentation could be improved in some areas, particularly in explaining some of the technical details and metrics
- Some of the figures are quite dense and could be made more readable
- The human evaluation study, while valuable, has a relatively small sample size

Limitations:
The authors provide a good discussion of limitations in the conclusion section. They appropriately note that their experiments were limited to specific model families and that further work is needed to evaluate usefulness for downstream interpretability tasks. They also acknowledge the subjective nature of the human interpretability study.
Regarding potential negative societal impacts, the authors do discuss this briefly in Appendix A. They note that advances in LM interpretability could potentially be misused, but argue that the current work poses minimal short-term risks. While this discussion is somewhat brief, it does address the key points and seems appropriate for the nature of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zJremsKVyh;"REVIEW 
Summary:
This paper introduces _Frugal Flows_ a method that learns the data distribution of data for causal effect estimation; namely outcome $Y$, binary treatment $X$ and pretreatment covariates $\mathbf{Z}$. 
Through a combination of frugal parametrisation, normalizing flows and copulas, separate components for the marginal causal effect $p_{Y| do(X)}$, the probability integral transforms of $\mathbf{Z}$ and the propensity score are leaned.
(The components of) the learned model, can be used for (i) estimating the marginal effect and (ii) to generate synthetic data with a fixed marginal effect for benchmarking other causal inference methods.
In the second application the component for the marginal effect is switched out for another with desired properties.
(i) is demonstrated on small synthetic datasets.
(ii) is demonstrated by fitting FFs to two real-world datasets and generating synthetic data with adjusted properties.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written.
- It tackles an important problem in causality research. Since, randomized data is hard and expensive to get, many causal methods are only evaluated on synthetic data and generating realistic/semi-synthetic data is hard. This paper makes a great contribution towards improving synthetic data generation. If the code for the method is provided in a user-friendly manner, I could see this having a big impact on the causality community.

Weaknesses:
- Normalizing Flows have been used in the causal modelling context before (see [1, 2]). While prior works solve different problems (the inferred latents correspond to exogenous variables of an SCM, not directly applied to causal effect estimation), I think it would still be valuable to contrast this work to what has been done before for future reference in the literature.
- L59: The basic causal assumptions aren't explicitly stated. What are the causal assumptions on $X$, $Y$ and $\mathbf{Z}$? It seems like the method wouldn't hold if $\mathbf{Z}$ was a mediator (I suppose the equation after L60 wouldn't hold). A reference to a 500+ page book is given for the assumptions, which feels like a slap in the face for the reader.
- The notation for interventional distributions is confusing: what's the difference between using an asterisk and explicitly using the do-notation? In the equation after L60, the LHS seems to be an interventional quantitiy (asterisk, but no do-notation), whereas Equation (1) has the do-notation, but no asterisk. Do the two notation elements mean different things?
- I think this paper would greatly benefit from a visual abstract showing how the different flows and distributions come together. Maybe this is something that could be added for the camera-ready.


Minor:

- L201: typo

[1] Javaloy et al. ""Causal normalizing flows: from theory to practice."" NeurIPS 2023

[2] Wendong et al. ""Causal component analysis"" NeurIPS 2023

Limitations:
- The biggest limitation of the work seems to be the requirements for the dataset size, needed to train normalizing flows. The authors mention this in Sec. 4.1. There could be some applications that have enough data for using Frugal Flows for causal effect estimation (e.g. online businesses with many customers, or well-curated medical datasets like the ones from healthcare providers in Isreal). However, for most applications the data won't be enough.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a generative modeling approach called Frugal Flows, designed to learn the data generation process with an explicit parametrization for the marginal causal effect of treatment on outcomes. Inspired by the frugal parametrization of marginal structural models, this approach models the marginal intervention distribution $p(Y|do(X))$ directly, rather than the joint distribution $p(Y|Z, do(X))$. This helps in preserving any constraints on the average treatment effect while flexibly modeling the data generation process. Frugal Flows employs copula flows to parameterize the model, accommodating constraints on the average causal effect and handling unobserved confounding during data generation. The authors validate the proposed method through experiments on both synthetic and real-world datasets, demonstrating its ability to generate realistic datasets with user-specified constraints.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* The paper's approach to validating causal models using simulated datasets is indeed impactful and relevant. It addresses a significant gap by allowing for general constraints on quantities of interest, such as average causal effect and unobserved confounding, during data generation. This capability is crucial because many prior generative modeling approaches for causal datasets either do not offer such flexibility or cannot ensure the preservation of these constraints, thus making this work a notable advancement in the field.

* The paper is well-written, with clear explanations in the background sections on frugal parametrization and flows, which help the reader grasp the proposed approach. The details of the approach are well-articulated, and the experimental results are presented effectively.

* The proposed approach is indeed novel. While it builds on established concepts like frugal parametrization, the specific application of normalizing flows for parametrization and its focus on average causal effect estimation represent a significant and innovative contribution.

Weaknesses:
My main concern with the work is the limited empirical validation of the proposed approach. Given that the primary contribution is the learning methodology rather than theoretical analysis, I would expect a more extensive set of experiments to validate its effectiveness. For example, prior research on generative modeling for causal inference, such as the work by [1], includes comprehensive experiments with various statistical tests to assess the realism of generated samples and a broader benchmarking of causal estimators. This paper would benefit from similar depth in its empirical evaluation.

It would nice if the authors can conduct similar experiments to asses whether learned generative model generates realistic samples and evaluate it on more datasets. Also, the authors should compare with the prior works [1, 2] as baselines to establish which approach is the best at capturing the underlying data generation process, and empirically validate their claim (Section 2.6) that the proposed approach would be better than prior works at capturing used-specified constraints on the average causal effect.

References

[1] Neal, Brady, Chin-Wei Huang, and Sunand Raghupathi. ""Realcause: Realistic causal inference benchmarking."" arXiv preprint arXiv:2011.15007 (2020).

[2] Harsh Parikh, Carlos Varjao, Louise Xu, and Eric Tchetgen Tchetgen. Validating causal inference methods. In International conference on machine learning, pages 17346–17358. PMLR, 2022.

Limitations:
Yes, the authors have adequately addressed the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a generative model called Frugal Flows making use of copula flows to infer about marginal causal effects by simulating the data generating process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The problem of inferencing marginal causal effects is an interesting and important problem
- The idea of using generative models to estimate the marginal effects in the paper is interesting

Weaknesses:
See questions.

Limitations:
See questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes to leverage existing neural density estimators (specifically, normalizing flows) to exploit a newly-proposed ""frugal parametrization"" that can capture the causal marginal distribution of an underlying causal model. Under this parametrization, the authors show how to specify and train each component of the model, and thus train the proposed Frugal-Flows to match the observational distribution as closely as possible, while being able to tune the marginal causal effect present in the generated data.

This way, frugal flows can be used to generate synthetic causal benchmarks that closely represent the _observational_ data while having more difficult-to-estimate causal effects, putting existing approaches to the test.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- **S1.** The proposed frugal flows provide a way of generating new datasets that can be challenging from a causal-inference point of view, which I believe _important_ to test new and existing methods.
- **S2.** The construction of the proposed architecture is quite rich in details.
- **S3.** I find the frugal parametrization conceptually quite interesting.
- **S4.** The authors motivate different scenarios for frugal flows in Sec. 3.2, as well as empirically show positive results on some synthetic and real-world scenarios.

Weaknesses:
- **W1.** I find the frugal parametrization to be extremely under-explained, relying too much on the reader having full knowledge of the referenced work. Similarly, there is little to no explanation/intuition on why the frugal parametrization would properly capture the marginal causal distributions.
- **W2.** The lack of explanations also applies to other concepts, e.g., ""conditional ignorability"" (line 39) ""variation independence"" (line 82, and I know the definition is later in App. A), or why copula-based flows would target conditional causal effects instead of marginal causal ones (line 182). (similar with lines 221 and 229)
- **W3.** There are no mention to related works that propose similar ways of constructing causal benchmarks. From a 1-min search in google scholar, I already found some likely relevant works: [Work 1](https://arxiv.org/abs/2406.08311), [Work 2](https://arxiv.org/abs/2011.15007).
- **W4.** I find the experiments a bit underwhelming, specially those from Section 4.1. The authors should at least show how is the fitting of the observational likelihood, and if they want to show the capabilities of frugal flows for causal inference (and not only causal-benchmark generation), they should compare with other methods like [Causal Normalizing Flows](https://arxiv.org/abs/2306.05415).

Limitations:
I think limitations are properly discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zJNSbgl4UA;"REVIEW 
Summary:
The paper targets scaling down Vision Transformers (ViT) to fit environments with dynamically changing resource constraints. The authors propose Scala, a framework enabling a single network to represent multiple smaller ViTs with flexible inference capability by activating various subnets during training. Scala introduces Isolated Activation to disentangle the smallest sub-network and uses Scale Coordination to provide stable and accurate learning objectives. Empirical validations on different tasks show that Scala achieves scalable representation with one-shot training, matching the performance of Separate Training without modifying the original ViT structure. Scala demonstrates an average improvement of 1.6% on ImageNet-1K compared to previous methods, using fewer parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem is important in practice.
2. The experimental results seem decent.

Weaknesses:
1. My major concern is that, the same aim of adapting ViTs to dynamically changing resource constraints, can also be achieved by multi-exit networks, e.g., [*1, *2, *3]. However, the paper does not discuss these highly relevant works or compare with them. Hence, I vote for rejection.
2. The method seems to lack novelty. 'smaller ViTs are intrinsically the sub-networks of a larger ViT with different widths' is not a surprising observation. The key techniques (e.g., Isolated Activation and Knowledge Distillation) are not new (naive or have been widely adopted).


[*1] Huang, Gao, et al. ""Multi-Scale Dense Networks for Resource Efficient Image Classification."" International Conference on Learning Representations. 2018.

[*2] Wang, Yulin, et al. ""Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition."" Advances in neural information processing systems 34 (2021): 11960-11973.

[*3] Han, Yizeng, et al. ""Dynamic perceiver for efficient visual recognition."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

Limitations:
The authors have addressed the limitations and potential negative societal impacts of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents Scala, a novel framework for scalable representation learning developed from US-Net. It identifies the issues of directly applying US-Net to ViTs and proposes solutions including Isolated Activation, Scale Coordination, and Stable Sampling. These innovations enable Scala to output several sub-networks in one-shot learning. Extensive experiments on various network architectures and datasets demonstrate that the sub-networks produced by Scala consistently outperform those generated by separate training, with significantly reduced training time.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Originality: Scala addresses the limitations of US-Net and successfully applies the concept of scaling to ViT backbones. This is a significant step in the adaptation of scaling methods for more complex network architectures.

Quality: The paper supports its claims with extensive experimental results, providing strong evidence for the effectiveness of Scala.

Clarity: The paper is clearly written and well-organized, making it accessible and easy to follow.

Significance: Scala has the potential to influence future research directions in scaling ViTs.

Weaknesses:
Originality: The novelty of Scala is somewhat constrained. For instance, Noise Calibration does not show a distinct difference from standard knowledge distillation. Essentially, Scala integrates US-Net with an alternative activation for the smallest subnet and fixed scaling ratios.

Quality: The authors might consider emphasizing results from a more standard 300-epoch ViT training schedule to align with common practices in the field.

Clarity: No further issues.

Significance: The challenge of scaling ViTs with arbitrary ratios remains unresolved.

Limitations:
The novelty and significance of Scala are somewhat limited, as discussed in the weaknesses section. However, the extensive experimental results provide a robust foundation for the claims made in the paper. Overall, the work is well-executed and makes a valuable contribution to the field, justifying a recommendation for weak acceptance.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Scala, a novel framework designed to effectively scale down Vision Transformers (ViTs) for use in environments with fluctuating resource constraints. The key insight is that smaller ViTs can function as sub-networks within a larger ViT, differing mainly in width. Scala enables a singular network architecture that can emulate multiple smaller ViTs, thereby offering versatile inference capabilities while maintaining the structural principles of ViTs. The framework uniquely incorporates multiple sub-networks during its training phase, utilizes Isolated Activation to differentiate the smallest sub-network, and implements Scale Coordination to streamline the learning objectives for each sub-network, aiming for simplicity, stability, and accuracy. The empirical results across various tasks confirm that Scala can learn scalable representations efficiently with a single training iteration, maintaining the integrity of the original ViT architecture and achieving performance on par with networks trained separately.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed Scala framework aims to enhance Vision Transformers (ViTs) by enabling them to learn scalable representations suitable for flexible inference. This is achieved through two key innovations: Isolated Activation, which effectively disentangles the representation of the smallest subnet to maintain clarity and specificity, and Scale Coordination, which ensures that each subnet within the larger network receives simplified, consistent, and accurate signals. These mechanisms are designed to optimize the performance and scalability of ViTs, addressing common challenges in adapting these architectures to varied and dynamic operational contexts.

Weaknesses:
1. Recent papers[1,2,3] with ""Scalable"" usually scale ViT to billion size with large scale datasets like DFN, JFT, and Datacomp. Therefore, I suggest authors should reconsider if the experiments can support ""Scalable"".


[1] Zhai, X., Kolesnikov, A., Houlsby, N., & Beyer, L. (2022). Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 12104-12113).

[2] El-Nouby, A., Klein, M., Zhai, S., Bautista, M. A., Toshev, A., Shankar, V., ... & Joulin, A. (2024). Scalable pre-training of large autoregressive image models. arXiv preprint arXiv:2401.08541.

[3] Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., ... & Houlsby, N. (2023, July). Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning (pp. 7480-7512). PMLR.

Limitations:
This paper addresses the limitations in conclusion

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances an approach for training Vision Transfomers (ViTs) such that at inference time they can be dynamically adjusted to fit different budget constraints with reduced drops of performance. To this end, the authors introduce Scala, a framework that allows a single network to encapsulate and train simultaneously multiple sub-networks of different capacities and widths. The methodological backbone of this work are the Universally slimmable networks (US-Net) [37], originally devised for CNNs. The authors identify and analyze a few flaws of US-Nets: difficulty to generalize to ViTs, small interpolation and extrapolation ability to sub-network size unseen during training, impact of sustained activation of the smallest sub-network that coupled with the sandwich rule for selecting sub-networks during training leads to an over-emphasis on it at the expense of the other sub-networks.
The authors propose two simple strategies towards such a method for ViTs: (i) Isolated activation that separates the smallest sub-network from the other sub-networks; (ii) scale coordination consisting of a set of heuristics to ensure that each sub-network gets simple, accurate and stable learning objectives: (a) progressive knowledge transfer from larger networks to smaller ones in gradual decrease of capacity, (b) stable sampling of intermediate width ratios to avoid large variations in capacities in the sandwich, (c) noise calibration, essentially a composite loss of supervised cross-entropy and distillation from the bigger sub-network.
Scala is evaluated on several settings on the ImageNet-1k dataset with ViT-Ti/S/B, hybrid CNN-ViT architectures, lightweight networks, but also for dense prediction on semantic segmentation and self-supervised pre-training with interesting results. The baselines used here were Separate Training,  Autoformer and US-Net.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
### Significance
- the paper deals with a challenging and useful task for deploying ViT models into different operational settings with different computational constraints without retraining or distilling specific architectures each time

- although a computational overhead is expected for such methods, the main components of Scala are relatively simple and make sense 

- Scala achieves good performance with a higher boost in the low parameter regime

### Originality
- the proposed contributions are somehow incremental as they are improving the US-Net prior work, but do have some novelty and they are simple.

### Clarity
- in general this work is well argued and easy to follow. The authors construct well the arguments regarding the challenges when going from CNNs to ViT with US-Net and how to construct their Scala approach.

### Quality
- the paper offers several experiments and studies in the main paper and in the appendix (longer training, fast interpolation, ablation of components) that are well thought and improve the understanding of the method.

- I appreciate the experiments beyond image classification, on semantic segmentation, as well as the self-supervised pretraining and subsequent linear probing on a downstream task.

Weaknesses:
### ""Scalable"" naming
- I think that the framing of the method as _""scalable representation learning""_ is quite confusing as it's not representative for this task, it's not a name used by other related works. Importantly, it can be easily mistaken with most works that use ""scalable"" for depicting the ability/property of a system (method, architecture) to handle a growing amount of data, parameters, and the potential to accommodate this growth. In other words ""scalable"" is rather used for depicting scaling up, whereas this work depicts the property of the proposed approach to accommodate sub-networks of different lower sizes/scales from the original.

- maybe other names us in related works would be more appropriate here: slimmable, elastic, modular, flexibile inference, etc.


### Limited baselines and related work
- some relevant related works dealing with tranformer networks are either just briefly mentioned, e.g., Matformer  [18], or not mentioned at all, e.g., SortedNet [a], Early exit [b]

- One of the main baselines, US-Net is originally designed for CNNs and, as the authors mentioned, moving to ViTs is not straightforward. Matformer is criticized for the limited number of models produced, but can be considered in the several experiments with X=4 sub-networks. Matformer and SortedNet could be included in the experimental evaluation


### Scope of experiments
- While the authors considered several settings for computer vision tasks (image classification, segmentation, light architectures), transformer architectures are also encountered in NLP (as mentioned by the authors in L56). In such cases the original models can have much more parameters and elastic inference for lower computational budgets would be of high interest.

- It would be useful to include an experiment from NLP in the style of those from Matformer or SortedNet.

- The biggest architectures used here is a ViT-B (~86M params). Extending experiments to larger modern architectures would be definitely useful and interesting.

### Clarity
- it's not always clear in the text and cost estimations that Scala needs a pre-trained full network as teacher for the distillation. This add some cost in compute and time in the end. Besides it's not clear whether US-Net also needs and uses a pre-trained teacher in the reported results.

- in the intro, the authors mention that they address the issue of minimal interpolation ability of ViTs. Results from Table 2 show that the interpolation abilities of ViTs with Scala are still very low. However the fast interpolation strategy from $\S$A.2 is actually interesting for practical settings even though not fully solving this issue. It might be worth moving up in the main paper.

- the idea of the transferability experiment ($\S$5.4) with DINOv2 is nice. From the description it is not clear whether DINOv2 was used as teacher for the distillation or also as supervised pre-training on ImageNet-1k? Or the pre-training on ImageNet-1K was done in a supervised manner as in previous experiments?

- the ablation experiment from Table 6 is nice. However the presentation with removing one component at once offers only a partial understanding of the contributions of each module. Different configurations with different modules in on/off mode should give a better global understanding.



**References:**

[a] Valipour et al., SortedNet: A Scalable and Generalized Framework for Training Modular Deep Neural Networks, arXiv 2023

[b] Xin et al., Deebert: Dynamic early exiting for accelerating bert inference, ACL 2020

Limitations:
The authors addressed some of the limitations in the conclusion section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zIr2QjU4hl;"REVIEW 
Summary:
This paper presents a conservative fine-tuning method called BRAID, which integrates the strengths of diffusion models and model-based optimization (MBO) to improve the performance of pre-trained diffusion models on offline datasets. BRAID optimizes a conservative reward model that includes penalties outside the offline data distribution to prevent overoptimization and generate valid designs. The approach is validated through empirical and theoretical analyses, demonstrating its ability to outperform the best designs in offline data while avoiding the generation of invalid designs. The paper also discusses the method's effectiveness compared to existing conditional diffusion models and traditional MBO techniques, with experiments showcasing its superiority in biological sequence and image generation. The authors acknowledge the limitations of their study, particularly in model selection and hyperparameter tuning, and suggest future research directions.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* BRAID incorporates a conservative approach to fine-tuning diffusion models, which includes penalization terms that discourage the model from generating designs outside the distribution of the offline data. This conservative strategy is effective in preventing overoptimization and ensuring the validity of the generated designs.

* The method is supported by both theoretical analysis and empirical results. Theoretically, it provides a regret guarantee, ensuring that the fine-tuned models can outperform the best designs in the offline data. Empirically, it has been validated through experiments across various domains, such as biological sequences and images, demonstrating its ability to generate high-quality designs.

Weaknesses:
* Difficulty in tuning hyperparameters without online data interaction.
* Reliance on accurate reward and diffusion models for effective performance.
* Theoretical results depend on certain idealized assumptions that may not hold in all cases.
* Can you compare the methods with other SOTA offline RL methods to illustrate your proposed augmented methods more effective than the SOTA offline RL methods? I think this paper is very relevant to some offline RL methods, such as ReDS[1], A2PR[2], CPED[3], SCQ[4]. It is not required that experimental comparisons must be given, but at least add some discussion with these methods to the paper.

References：

[1] Singh, Anikait, et al. ""ReDS: offline reinforcement learning with heteroskedastic datasets via support constraints."" Proceedings of the 37th International Conference on Neural Information Processing Systems. 2023.

[2] Liu, Tenglong, et al. ""Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning."" In International Conference on Machine Learning (ICML). PMLR, 2024.

[3] Zhang, Jing, et al. ""Constrained policy optimization with explicit behavior density for offline reinforcement learning."" Advances in Neural Information Processing Systems. 2023

[4] Shimizu, Yutaka, et al. ""Strategically Conservative Q-Learning."" arXiv preprint arXiv:2406.04534 (2024).

Limitations:
* The method requires careful selection of hyperparameters, which can be challenging in a purely offline setting without access to additional online data.
* The pseudo-code of the paper does not illustates the algorithm explicitly. The authors can improve it further.
* This paper lack strong baselines comparisions and enough related works, which is related to some offline reinforcement learning methods. So you can add more related offline reinforcement learning works.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper tackles the task of black box optimization in an offline setting. Given a pretrained diffusion model, they first train a surrogate model on the offline data and use it to tilt the diffusion model distribution via finetuning it. The authors distinctly focus on an uncertainty quantification based procedure to bias the diffusion model tilting toward regions where the reward is high and the reward uncertainty is low while not tilting toward regions with high uncertainty. Experiments are carried out on a reasonable set of diverse tasks.

The paper introduces a small specific challenge and address it well with a reasonable approach and good motivations. The potential impact of the method may be small but it is neat, educational, and should be useful in many cases. I recommend acceptance.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Identifying a crucial non-obvious overlooked challenge and specifically pinpointing it. The authors identify that previous work on tilting diffusion models with reward models misses to incorporate tilting less toward regions of the reward function in which it has high uncertainty but high reward. Instead, we should only tilt to the regions that have high reward and high certainty to avoid optimizing toward adversarial examples. (The fact that the finetuned diffusion model will steer away from the pretraining distribution seems like a less relevant insight)
2. The authors identify a relevant overlooked problem in finetuning diffusion models and bring standard techniques from uncertainty quantification into the field to address it in a reasonable fashion. They do not overcomplicate things and their technique could be valuable to several researchers in the area. 
3. The authors prove that the training procedure yields the desired distribution. I have no comments regarding the value/insightfulness of the proof. Maybe other reviewers have a stronger opinion about the relevance.
4. The authors evaluate their method on a very diverse set of experiments that includes discrete DNA sequence generation, and image generation. The results are convincing and demonstrate the central empirical claim that out of distribution generation is a problem and is effectively avoided with the proposed conservative reward model fine tuning.
Minor:
1. Interesting snippets of insights. The authors point out interesting relationships and connections along the way which are non-obvious and well placed for putting their motivations into context.
2. Exceptional clarity in writing. The paper lays out the task in its precise specification and covers required concepts and related work in equal clarity.

Weaknesses:
1. I would say that the insights in terms of methodological novelty are on the moderate side. The ideas are simple and good, which is appreciated, but the level on which the conceptural changes operate are low level (a tweak to diffusion model tilting) and thus limited in impact. However, it is certainly a good thing to have.
Very hard to address and not a must have for ML conferences:
1. Evaluations are inherently limited in their computational nature and the conclusions that can be drawn for the procedures effectiveness in biological sequence optimization is small. Do you authors disagree with this in any way?

Limitations:
The authors point out the key inherent limitation of their work in the fact that the reward models that are often trained on limited data are likely suboptimal instead of just mentioning useless small limitations that are beside the point (which is the more common practice it seems).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a conservative approach for fine-tuning diffusion models with a reward model learned from offline data. Specifically, the ideas are two-fold: The first idea is to replace the reward model with a conservative estimate based on classical generalization bounds. The second idea is to leverage the KL divergence to force the optimized distribution to not deviate too far from the pretrained model. Experiments and theoretical results show the efficacy of the proposed method in fine-tuning diffusion models without over-optimization.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is well-presented, and the motivation behind the algorithm is interesting. The over-optimization problem is indeed critical when fine-tuning diffusion models with learned rewards.

2. Extensive experimental results show the efficacy of the proposed method in improving the reward model while avoiding reward over-optimization.

Weaknesses:
1. Leveraging generalization bounds via kernel RKHS and bootstrap is interesting, but I doubt their practicality for real applications. Firstly, the RKHS bound is usually too conservative to be useful, while the computational cost for the bootstrap method is pretty high since one has to train the model from scratch multiple times. As far as I can tell, the reward models used in the experiments are mainly single-layer MLPs, and it is doubtful whether this approach is useful when the reward model needs to be a larger model.

2. Another problem with the conservative estimator of the reward models is that it is unclear whether it is useful given the current experimental results. On one hand, KL regularization is a widely-known technique for preventing over-optimization in diffusion models and is thoroughly studied in existing works, so it is certain that the KL regularization term will help. On the other hand, the proposed algorithm mixes both the conservative reward estimator and the KL regularization term together, making it unclear which part is playing the role in avoiding over-optimization. My guess is that, for the most part, only the KL regularization term is effective in the end.

Limitations:
See my comments on the weakness above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
1) This paper analyzed the two mainstream angles of computational design.
2) Proposed a hybrid one that offline fine-tunes generative models.
3) Conduct experiments on two tasks to show the performance of their method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1) Sufficient theoretical analysis and detailed preliminaries.
2) The idea is straightforward.
3) The method is comprehensive.

Weaknesses:
1) In the introduction, the advantages and disadvantages of the two mainstream issues are not fully analyzed.
2) Insufficient metrics evaluation for image generation task.

Limitations:
1) Lack of gradual guidance from analyzing the advantages and disadvantages of existing methods to proposing the hybrid method.
2) Lack of multi-metric quantitative results analysis. (e.g. in the image generation task, the fidelity and diversity should also be reported by like LPIPS Score/CLIP score/FID/IS, etc.)

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zGN0YWy2he;"REVIEW 
Summary:
This paper employs scene graph for image generation. Different from the previous methods, they employ the generative capabilities of variational autoencoders and diffusion models in a generalizable manner, compositing diverse disentangled visual clues from scene graphs. The authors propose a semantics-Layout Variational AutoEncoder to jointly derive layouts and semantics from scene graph. Then they develop CMA integrated with a diffusion model. They also introduce the multi-layered sampler for achieving graph manipulation. Experiments show that the method outperforms existing methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper address the problems in existing methods well. Existing methods in the field of scene graph to image generation mainly depends on the layout or semantics. Using one of them may cause some problems. Inspired by these phenomenons, the authors propose the method to jointly considering the layout and the semantics. What's more, the techniques used in the framework are novel enough. 
2. The authors conduct plenty of experiments. The ablation studies support the motivations.

Weaknesses:
The only weakness I found is that the authors should reorganize the paper carefully. The writings is not so clear in some sections. For example, the multi-layered sampler section is too abstract to be understood.

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes DisCo (Disentangled Compositional image generation), which integrates both layout and semantic information derived from scene graphs to improve the quality and controllability of generated images. In particular, DisCo has three main components: Semantics-Layout Variational AutoEncoder (SL-VAE) for disentangling spatial layouts and semantics, Compositional Masked Attention (CMA) for fine-grained attribute guidance, and Multi-Layered Sampler (MLS) for object-level graph manipulation. Extensive experiments demonstrate that DisCo outperforms state-of-the-art methods in generating rational and controllable images from text, layout, and scene graph conditions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation is clear. The idea of disentangling layout and semantics from scene graphs is novel.
2. DisCo outperforms recent methods in both fidelity and diversity of image generation, as evidenced by the IS and FID scores. Overall, it enhances the generation diversity and controllability.
3. Extensive experiments and ablation studies have demonstrated the effectiveness and the contribution of each component.

Weaknesses:
1. The increased inference cost of DisCo (Table 7). In particular, CMA mechanism might increase the computational cost, which may limit the method's scalability and efficiency, especially for large-scale applications. Moreover, since diffusion models are already quite large, the additional AutoEncoders (Lines 129-130) may result in more parameter and memory overhead.
2. DisCo requires expensive training, e.g. 4 A100 GPUs, as indicated in Lines 202-203. With more models releasing recently, this technique might be not scalable.
3. The image quality looks better with this proposed method. However, as metrics today cannot always reflect the real image quality, it would be more convincing to conduct a user study, e.g. votes, to quantify the advantage of DisCo compared to previous works.

Limitations:
Yes, authors have stated their limitations on attribute leakage of overlapping in Section A.6, as well as a short discussion on the broader impact in Section A.7. 

Beside that, authors are also encouraged to add a few discussions on the efficiency of their proposed pipeline. Even though these overheads are inevitable (they are quite common in most researches), a clearer trade-off between the improved image quality and the increased model complexity would help to better assess the value of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents ""DisCo,"" a novel framework for generating complex images from structured scene graphs. Unlike traditional text-to-image or layout-to-image methods, DisCo utilizes a Semantics-Layout Variational AutoEncoder (SL-VAE) to disentangle and generate diverse spatial layouts and interactive semantics from scene graphs. It incorporates these elements using a Compositional Masked Attention (CMA) mechanism within a diffusion model, enhancing generation control and rationality. The framework also introduces a Multi-Layered Sampler (MLS) for flexible, graph-based image editing, preserving visual consistency while manipulating object attributes and positions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Introduces innovative methods for disentangling and integrating spatial and semantic information from scene graphs, which is a novel approach in image generation. 
2. Offers significant improvements in image generation from complex scene graphs, enhancing both the fidelity and controllability of generated image

Weaknesses:
1. The paper lacks quantitative comparisons with closely related baselines, such as R3CD, which could provide a more comprehensive evaluation of the model's performance. Inclusion of these comparisons could help validate the proposed advantages of DisCo over existing methods, particularly in handling complex scene graph-to-image generation tasks.
2. Some generated images, particularly those highlighted in Figure 10, exhibit unnatural aspect ratios and stretched elements, suggesting issues with the model’s handling of object proportions and spatial embeddings. 
3. It would be great to discuss the scalability aspects, particularly how the proposed model handles graph sizes that exceed typical training configurations.
4. how the model performs with imperfect or noisy scene graphs, which are common in automatically extracted data.

Limitations:
The authors address the challenge of attribute leakage in overlapping objects, which affects image fidelity in scenes with dense object interactions. While mitigation strategies are discussed via the CMA mechanism, further refinement is required to eliminate this issue completely. Additional exploration into the computational efficiency and scalability of the proposed methods would also benefit the paper, providing a more comprehensive view of their practical applications and limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method that uses a scene graph and integrates variational autoencoders (VAEs) and diffusion models to address complex scene generation. Specifically, a Semantics-Layout Variational AutoEncoder (SL-VAE) is used to derive diverse layouts and semantics from the scene graph, while a Compositional Masked Attention (CMA) combined with a diffusion model incorporates these elements as guidance. Additionally, a Multi-Layered Sampler (MLS) is introduced for isolated image editing. Experiments show that this approach outperforms recent methods in generation rationality and controllability.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper considers an important issue in text-to-image generation realm.
2. The structure design in Section 3 makes sense.
3. The experimental results shown in Table 1 and 2 show the effectiveness of this method.

Weaknesses:
1. My main concern is the practical application of this method. As we all known, scene graph building is not a trivial task, but you don't explain detail in the paper how to construct an exact scene graph. In addition, during inference, the prompt proposed by uses may be non-standard so that building a scene graph may be more difficult. 
2. Besides, recent SOTA models, e.g. DALLE3, stable diffusion 3 try to solve the complex generation task by large-scale fine-grained dataset construction. How do you compare your methods with these data-centric methods. The authors should spend more space discussing the issues.

Limitations:
Please see the weakness

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zDaD8zv8tG;"REVIEW 
Summary:
The paper introduces a novel teacher-teacher framework named LIghtweight kNowledge alignmEnt (LINE), which facilitates knowledge exchange between two pre-existing large language models (LLMs) to enhance clinical language representation. By leveraging complementary knowledge from general-purpose and domain-specific models, LINE aims to harmonize their knowledge within a unified representation space. The framework is validated through downstream tasks showing that the LINE model outperforms individual pre-existing models in understanding and processing clinical language. This approach allows for more efficient sharing of clinical pretrianed models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. **Clarity and Structure**: The paper is well-written and structured, offering a clear motivation for the study. This makes it accessible and engaging for readers, facilitating a deeper understanding of the proposed framework.

2. **Novelty and Utility**: The proposed teacher-teacher framework, LIghtweight kNowledge alignmEnt (LINE), is innovative, providing a pragmatic approach to integrating the strengths of different pre-trained models. This methodology is particularly notable for its potential to enhance clinical language representations without the need for developing new models from scratch.

3. **Usability and Efficiency**: The framework is user-friendly and does not require retraining of the original models, which significantly reduces computational overhead and simplifies its adoption in real-world applications.

4. **Empirical Validation**: The experimental results demonstrate stable and significant improvements over existing methods, substantiating the efficacy and value of the proposed framework in practical settings.

Weaknesses:
**Data Requirements and Availability**: A notable limitation of the proposed LINE framework is its dependency on well-aligned and specific types of data sources, which may not be readily available or commonly found in practical settings. For example, integrating data from disparate modalities like CT and MRI requires the availability of cases that include both types of data, which may not always be feasible. This requirement could limit the framework's applicability across different clinical or real-world scenarios where such aligned data sets are scarce.

Limitations:
No.
The authors discuss the potential improvement instead of the limitation of the current work. Bring more information and try other situations cannot be counted as an adequate discussion of limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an interesting topic on LLM but the importance of this problem is not convincing and the methods here is not novel.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The teacher-teacher concept is novel to some extent.

Weaknesses:
1. The problem's importance is not significant.
2. There lacks the inclusion of SOTA models like llama, gpt, etc.
3. The results improvement is limited as shown in Tab. 4,5.
4. The Fig1 lacks details of the proposed method.

Limitations:
See details in weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors look to address the question representational alignment between language models trained on different textual domains to improve performance of potentially both models on their out-of-domain text. The authors propose to specifically investigate this in the context of EHR text, and choose as their models for this CODER and BGE. They propose a contrastive loss, and additionally propose to train an alignment module/project layer rather than end-to-end training of the teacher models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The concept is solid and well implemented and motivated. I wonder if it would be possible to further generalize it beyond medical text - which it is restricted too due to the reliance on alignment with extracted medical concepts by NILE. The discussion mentions this possibility, but it would be exciting to see it in action.

The clinical NLP benchmarks are particularly appropriate for the task.

Weaknesses:
Some of the benchmark tasks are older, and the comparisons could be more robust. Some ablations are missing.

The project's scope is incredibly narrow: encoder models on extractive medical tasks. While the authors claim that the technique is broadly generalizable, it would be nice to see proof-of-concept. 

The work seems to me to fit more into the realm of domain adaptation rather than learning by alignment. We aren't learning novel models here via alignment (like CLIP), but rather, pushing the learned representations of two different models into a common space. I'd strongly consider citing and discussing DA literature for this paper.

Limitations:
No discussion of limitations. Without additional experiments at the minimum a stated limitation should be the highly restricted domain of application (purely encoder models on medical topics).

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduce a teacher-teacher framework for clinical language representation learning. The framework uses a lightweight knowledge alignment module to harmonize the knowledge of both models within a unified space, which including two steps: The first step involves initial training to define residuals and capture complementary information. The second step focuses on refining the alignment by recovering residual information. The framework was validated using the MIMIC-IV database, where the LINE model outperformed baseline models in aligning concept and text representations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution of the work is proposed teacher-teacher framework, and training strategy.

- Originality: The teacher-teacher framework is very interesting as it enables mutual enhancement between two pre-existing LLMs, a unique departure from traditional approaches that typically involve training a new model or continual pre-training of existing models. This innovative method opens new avenues for leveraging existing resources to achieve superior performance.

- Quality: The paper demonstrates high quality through its validation using the MIMIC-IV database, a well-known and respected dataset in the clinical domain, adding significant credibility. Additionally, the LINE model's performance is compared against several strong baseline models, showing clear improvements across various downstream tasks, thus underscoring the robustness and reliability of the proposed framework.

- Clarity: The paper is well-written and clearly structured, making it accessible to both domain experts and those new to the field. The introduction provides a comprehensive background and motivation for the proposed framework, while the methodology section offers detailed descriptions of the teacher models and the LINE module.

- Significance: The practical applications and potential impact on the clinical domain shown the significance of this work. The teacher-teacher idea has substantial implications for more advancing NLP applications in other filed.

Weaknesses:
1. Figure 1 is somewhat confusing. From my understanding, Teacher 1 should be a strong LLM, while Teacher 2 should be an LLM with existing domain-specific knowledge. However, Figure 1 gives the impression that Teacher 2 serves merely as a database, making the framework resemble a RAG framework.

2. Although the paper compares the LINE model against several strong baseline models, it lacks a detailed comparison with the latest strong general LLMs, such as GPT-4, which should be considered a strong baseline. Consider adding a small comparative analysis or stating the advantages of the framework over simply using GPT-4.

3. The paper underscore the practical value of the framework, but it does not sufficiently address potential practical implementation challenges, such as computational requirements and scalability when applied in real-world clinical settings.

Limitations:
The paper has limited discussion on the broader implications of implementing the teacher-teacher framework in clinical settings. Consider to add the assessment of how the framework could impact patient care, data security, and trust in AI systems in healthcare.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a mutual learning framework, called LINE, between two pre-existing LLMs in the healthcare domains. By harmonizing the knowledge of two distinct LLMs into a unified representation space, the model achieves better performance on intrinsic and extrinsic downstream evaluations of clinical tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Clear motivation. Overall well written.

The methodology was reasonably designed to map representations from two distinct LLMs into a unified representation space.

The method achieves better performance on downstream clinical tasks.

Weaknesses:
1. Only two LLMs (BGE and CODER) were aligned by LINE. It is unclear if LINE will work on combinations of other LLMs. 

2. LINE make downstream predictions based on clinical concepts only, rather than the full context. The concepts themselves can be negated, historical and hypothetical in context, but the proposed method does not seem to consider this.

Limitations:
See weakness

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zBMKodNgKX;"REVIEW 
Summary:
The paper introduces a novel approach to address the challenge of collaboratively visualizing high-dimensional data in a federated learning (FL) environment. The proposed method, FEDNE, integrates the FEDAVG framework with contrastive neighbor embedding (NE) techniques, aiming to preserve data privacy while ensuring effective data visualization. By employing a surrogate loss function and an intra-client data mixing strategy, FEDNE seeks to enhance the alignment and preservation of neighborhood structures in the global embedding space. The paper includes comprehensive experiments on both synthetic and real-world datasets, demonstrating the effectiveness of FEDNE in outperforming several baseline methods in terms of neighborhood data structure preservation and clustering.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. FEDNE introduces a novel integration of FEDAVG with contrastive NE techniques, addressing the unique challenges of pairwise data relationships in federated learning environments without requiring data sharing.
2. The intra-client data mixing strategy effectively enhances local data diversity, mitigating the limitations of biased local kNN graphs and ensuring better neighborhood representation.
3. The paper provides a thorough evaluation of FEDNE using various datasets and metrics, showcasing its superior performance compared to baseline methods in preserving neighborhood structures and clustering.

Weaknesses:
1.	While the authors mention that FEDNE introduces only 35% more GPU time compared to FEDAVG, the overall complexity and scalability in a more extensive, real-world setting are not fully addressed. The authors should further investigate how FEDNE scales with a significantly larger number of clients and more complex datasets or models.
2.	The paper proposes intra-client data mixing as a solution to the bias in local kNN graphs. However, this approach might not entirely mitigate the issue of incorrect neighbor connections, especially in highly imbalanced datasets. More detailed comparisons with alternative methods or further enhancements could provide a more robust solution.
3.	The focus is primarily on dimensionality reduction. The validation results are performed only on the vision classification tasks. Extending the discussions and analyses to include applications in other domains could be beneficial.

Limitations:
The authors have addressed the works limitations and social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper ""FEDNE: Surrogate-Assisted Federated Neighbor Embedding for Privacy-Preserving Dimensionality Reduction"" presents a method for visualizing high-dimensional data while maintaining privacy without requiring any shareable reference data. 

Federated Neighbor Embedding (FEDNE): A framework combining federated averaging (FEDAVG) with contrastive neighbor embedding (NE) to create a joint NE model across multiple clients without compromising data privacy.

Surrogate Loss Function: An innovative loss function to enhance inter-client repulsion in the global embedding space, ensuring better separation of data points from different clients while preserving local data structures.

Data-Mixing Strategy: A technique to counter issues like invisible and false neighbors in local k-nearest neighbor (kNN) graphs by mixing data from various clients during training, thus improving the quality of the learned embeddings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Well-Presented: The paper is clearly and coherently written, making it easy to follow.
Novel Approach: The study addresses an important problem with a novel approach, combining federated learning with neighbor embedding techniques.

Weaknesses:
Privacy Concerns: While the approach is innovative, the paper does not sufficiently address privacy concerns. It lacks experiments and guarantees demonstrating the privacy preservation of the FedNE approach.

Computational Inefficiency: The method appears to be computationally inefficient. There are no experiments conducted on large datasets, such as those in real-world medical or other privacy-critical domains, where computational complexity could be a significant issue.

Inadequate Analysis of Related Work: The related works section is not thoroughly analyzed or discussed, missing critical comparisons and context necessary for a comprehensive understanding of the state of the art.

The study's applicability could be strengthened by extending beyond benchmark datasets to encompass real-world, privacy-sensitive datasets found in domains such as healthcare or finance. This expansion would provide a more robust demonstration of the method's practical relevance and effectiveness. 

Additionally, addressing pairwise issues associated with attraction terms is essential for improving the preservation of neighborhood structures and enhancing clustering quality. 

Furthermore, it is crucial to conduct thorough analyses aimed at optimizing the computational efficiency and scalability of the algorithms, ensuring their capability to handle large-scale datasets effectively. Moreover, the method currently lacks explicit consideration of privacy guarantees. And on elucidating how privacy concerns are addressed within the framework and formalizing privacy guarantees to assure users and stakeholders.

Limitations:
No societal impacts.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a new federated learning approach named FEDNE for dimension reduction using contrastive neighbor embedding (NE). The key idea is the introduction of a surrogate loss function that each client learns and shares, which compensates for the lack of inter-client repulsion essential for global alignment in the embedding space. Additionally, the paper proposes a data-mixing strategy to augment local data, addressing issues of invisible and false neighbors in local kNN graphs. Comprehensive experiments demonstrate that FEDNE effectively preserves neighborhood data structures and enhances alignment in the global embedding space compared to several baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The studied problem is important. There could be many downstream tasks after applying federated neighbor embedding.

2. Many metrics are included in the experiments to evaluate the quality of the resulting embeddings

Weaknesses:
1. The paper lacks investigation on the effect of choice of hyperparameter k.

2. The improvement of FEDNE is significant on some metrics (e.g., kNN) but is very limited in other metrics (e.g., conti.). The paper lacks a detailed exploration of why FEDNE produces different behavior for different metrics.

3. I suggest to highlight the best results in Table 2. Currently the results of FEDNE are highlighted although it may not achieve the best performance in some cases.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of distributed neural embedding (NE) with a focus on privacy protection. To achieve this, the authors extend the concept of federated learning (FL) to NE. However, NE tends to diverge because FL prevents clients from accessing each other's data, leading to inconsistent feature spaces across clients. To mitigate this issue, the authors employ surrogate loss models trained locally, which are then broadcast to all other clients to serve as an anchor. The experiments show promising performance compared to existing baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-motivated and well-written.
2. The problem is practical and useful for many real-life applications, though scalability may be the main constraint.
3. The idea is straightforward, and the experiments seem to verify its effectiveness.

Weaknesses:
1. **Communication complexity**: If I understand correctly, every client in the proposed method must broadcast the surrogate models to all other clients. Although the surrogate models consist of only one hidden layer, this design results in a communication complexity of $\mathcal{O}(N^2)$. As the number of clients in the system increases, the additional communication costs will rise dramatically. This might be manageable in some cross-silo settings, where only a few clients participate.

2. **Straggler effect**: Following point (1), the proposed method requires communication among clients. However, clients may drop out during training. It would be insightful if the authors could analyze how missing surrogate loss models would affect overall performance.

3. **Additional privacy concerns**: Sharing surrogate models introduces additional privacy risks, e.g., enabling reconstruction attacks or membership inference. While some recent work empirically shows that such private information is less leaked after distillation (e.g., [1] and [2]), the proposed method might be more vulnerable to privacy attacks without differential privacy.

[1] Dong, Tian, Bo Zhao, and Lingjuan Lyu. ""Privacy for free: How does dataset condensation help privacy?."" International Conference on Machine Learning. PMLR, 2022.
[2] Wang, Hui-Po, et al. ""Fedlap-dp: Federated learning by sharing differentially private loss approximations,"" Proceedings on Privacy Enhancing Technologies, 2024.

Limitations:
The authors have discussed some limitations. However, the authors are encouraged to discuss the use cases of the proposed method, such as cross-silo settings. Moreover, they are encouraged to discuss the additional privacy risks potentially introduced by their method.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
Jkt42QYyEH;"REVIEW 
Summary:
This paper tackles an important problem in reconstructing interactive 3D scenes with language grounding. The authors proposed to use object-based modeling of different deformation fields over the dynamic NeRF pipeline and equip it with language embeddings for grounding interactions. The authors constructed two synthetic datasets OmniSim and InterReal for data collection and evaluations. Experimental results show that their methods significantly outperform prior methods on reconstruction and language grounding.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The problem of reconstructing interactive scenes with language grounding is an important topic to embodied AI, we have seen many related works that boost the development of robot perception.
- The construction of OmniSim and InterReal datasets could be beneficial for research in dynamic reconstruction and robotics.
- The authors showed significant performance improvement on their constructed dataset, outperforming existing articulated object reconstruction models by a large margin.

Weaknesses:
- Despite the good motivation, one major concern about its paper is its poor presentation in terms of notations and details. Several key detail designs are omitted in both Sec.3/4 and supplementary.
  - How are disjoint regions $\mathcal{R}$ defined? it seems from the start of the description that this knowledge is already given. How do we determine the number of subregions? Any prior used for learning the deformation in each subregion? Since jointly optimizing the belonging relationship of each point to each region and the deformation for each field is pretty difficult as far as I know. 
  - The notations used are confusing, especially in Sec.3.2 which is an important portion of text to help understand the methodology. In Eq.3, what does $\Theta$ mean? It is different from the $\Theta$ in Eq.2 but is it just another MLP prediction? how to determine them for each region?
  - As for the dataset curation, in each data sample will there be multiple objects being interacted (since you modeled many sub-deformation fields)?
  - The authors should mention if any priors were added to the implemented baselines as well because methods like CoGS were not originally designed to handle multiple objects if I'm understanding it correctly.
- The current dynamic reconstruction model still stays at the rendering level, some explorations on extending it to 3D meshes or simulated environments might provide more insights on using this model for future research.

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed LiveScene, a NeRF-based approach to enable indoor-scale controllable scene reconstruction and novel view synthethis. By extending K-Panes with a object-aware multi-scale space factorization, scene-level 3D space with articulated objects could be modeled with motion patterns via densely collected monocular video with camera poses. On exsiting benchmark datasets and newly proposed datasets OmniSim/InterReal, the porposed method LiveScene achieves the best overall performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The overall method is well motivated to address the more challenging indoor-scene level controllable NeRF. The introduction of control variables and their space spaces make the overall training feasible.

- The extensive experiments as well as demos proves the effectiveness of LiveScene, both quanlitatively and quantitaitively.

Weaknesses:
Though the overall resutls seem promising, I still have several concerns regarding the formulation and lack of clarifications in certain key aspects.

- The only additional attribute of overll space is modelled as a 3+alpha dimension space, how to cope with time variations? Are the time dimension implicitly encoded within the contro variables to cope with motions (such as open/close the door)? Or is there any explicit formulation of time dimension?

- What is the potential maximum number of objects within the scene? And what is the potential limitations when scaling up to more objects? In Tab.6 of supplement, 6 objects at most is validated. How about more diverse objects?

- Is it possible to encode more complex or fine-grained object control (e.g.,open left side door of a double-open fridge), especially when the training data mainly contains the fully open and fully-close state. Specifically, I am wondering what is the interpolation capability of the propsoed interaction-aware feature space and its generalization capability to unseen but correlated states.

- As mentioned in Appdix D, ‘Interaction Variable MSE’ mentions that the interaction values are fully supervised and GT labels are also used during inferenced to enble control. It would be good to see, in practical cases, without GT interaction values, what is the performance degradations, which could further strengthen the potential applciations and reveal potential limitaions.

Limitations:
Limitations have been partly addressed in the main paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
They introduce a language-embedded ""interactive neural radiance field"" that efficiently reconstructs and controls multiple objects within scenes. Factorization decomposes the scene into more local fields that can achieve local deformation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
My sense is that the technical novelty of this paper is high, though I'm not an expert in this domain. Additionally the evaluation seems thorough and the method seems well-considered.

Weaknesses:
At times, I feel the the language of the methods section can be made a little clearer. I had trouble following the motivation for a lot of the design decisions.

Limitations:
No issues here.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the complex challenge of reconstructing and controlling multiple interactive objects in complex scenes from monocular videos without prior modeling of geometry and kinematics. This task is critical for advancing fields like virtual reality, animation, and robotics, where understanding and interacting with 3D environments are essential. The proposed framework decomposes interactive scenes into local deformable fields. This factorization allows for precise modeling of individual objects’ interactions. Additionally, a multi-scale interaction probability sampling strategy is introduced to accurately sample interaction-relevant 3D points within these fields, enabling effective control over object dynamics in complex environments. The interaction-aware language embedding method generates dynamic language embeddings that adapt to varying interaction states. This allows users to control interactive objects through natural language commands, enhancing the interface's intuitiveness and accessibility. Authors also contributed OmniSim and InterReal datasets. These datasets are the first to offer scene-level physical interaction data, comprising 28 scenes with a total of 70 interactive objects. They provide a valuable resource for evaluating the performance of interactive scene modeling methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ The paper is well motivated and sets a clear difference from previous works. Results look good and promising. Figures and illustrations are helpful and informative.
+ The factorization technique that decomposes complex scenes into local deformable fields allows for more granular control and precise modeling of individual object interactions within a complex scene, addressing the high-dimensional challenge that previous methods struggled with.
+ By embedding language control within the interactive radiance fields, the framework allows users to manipulate and interact with 3D environments using simple language commands, greatly enhancing user accessibility and interaction fidelity.
+ Authors provided abundant demos on their project page, which is helpful

Weaknesses:
- Might need to slightly enlarge texts in figures.
- Not necessarily a weakness but authors can consider visualize some latent features (instead of illustrations like in fig2-4) to better show the decomposition of the high-dimensional feature space.
- Lack some qualitative results on real-world dataset and existing public dataset. Also, in the only InterReal qualitative results (fig.11), k-planes results were missed.

Limitations:
Authors discussed limitations in terms of closed vocabulary, caused by OpenCLIP. Given that authors did not show much real-world scene manipulation results, potential limitations in real-world scenes should also be discussed as such scenes are in general more complicated. No societal impact was discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zBG7WogAvm;"REVIEW 
Summary:
This paper proposes a method for decision-aware Bayesian experimental design, where the design is not optimized with respect to the most accurate posterior distribution of the latent parameters but rather with respect to the expected utility gain of the actual (down-stream) decision task.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
This is an innovative paper with high practical relevance. The proposed method appears sound and the corresponding neural networks well designed to suit the goal. Despite my questions and concerns (see below), I am positive about this paper overall and eager to increase my score should my points be addressed.

Weaknesses:
- The presentation of p(y_Xi | h_t) between Eq 3 and 4 is partially unclear to me. From the definition, it seems this is not actually a distribution but a set of distributions. To me, then notation p(y_Xi | h_t) appears to be quite the abuse of notation because we cannot readily read this it as a single distribution. Can you perhaps think about a different notation that makes this easier to parse and understand? Relatedly, in Equation 4, it appears that we compute an expectation over p(y_Xi | h_t). But how do we compute an expectation over a set of distributions? I think I get what the authors do and want to imply but to me this notation doesn’t help in understanding it.
- Equation 7: It seems we approximate the predictive distribution always by a Gaussian. I mean this of course works if the true underlying function is some kind of GP, but what if the true predictive distribution is far away from Gaussian? I don’t see this choice to be discussed properly so I consider it a weakness of this paper for now.
- The discussion of training and inference time can only be found in the appendix. Specifically, training speed seems to be substantial, which of course makes sense for an amortized method. However, I don’t see any discussion for when the training actually amortizes. That is, how many BED tasks do we need to run at minimum before the total (training + “inference”) time of the new method becomes better than those of the competing methods. More generally, I think a discussion of speed should be more prominent in the paper.
- 6.1 toy example was hard for me to understand at first. Is this just a standard BO task to find the point where the unknown function is maximal?

Limitations:
The paper discusses several limitations. I am missing a discussion on the initial overhead of training, which is usually substantial in amortized methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper looks at the problem of designing Bayesian optimal experiments taking into account the downstream decision making. At the core is a Transformer Neural Decision Process (TNDP) architecture that is trained to amortise the experimental design process whilst simultaneously inferring the optimal downstream decision.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- Relevant and interesting topic: Downstream decision making is what ultimately matters, so taking this into account when designing experiments to collect data can result in more cost- and sample-efficient learning.  

- Motivation for the paper as well as clarity of writing are excellent. Contextualisation relative to prior work can be improved as outlined in the next section.

- The proposed Transformer Neural Decision Process (TNDP) architecture is tailored to the BED problem, is well-explained and adds some novelty to the architectures typically used in the field.

Weaknesses:
### Sections 2.2 & 3.2 and Lindley's decision-theoretic BED [1]:

My main issue with the paper is the presentation of DUG and EDUG as novel. This framework was first formulated in [1], and is very well summarised in Section 1.3 of [2]. I strongly recommend the authors read that section, and present their Section 3.2 accordingly, acknowledging they follow Lindley, 1972. The questions/comments in the next 2 bullets are a consequence of this omission of literature.

- Second paragraph of Sec 2.2: I am not sure how the predictive distribution $p(y | \xi, h_t)$ is defined. I would think it is $p(y | \xi, h_t) = \mathbb{E}_{p(\theta |h_t)} [p(y | \xi, \theta)]$. Whether or not you compute/approximate the posterior $p(\theta |h_t)$, or seek to directly approximate $p(y | \xi, h_t)$ (eg variationally), I think you should explicitly define what this quantity is. 

- I am not sure how the utility $u(y_\Xi, a)$ is defined. From a Bayesian decision-theoretic approach, the utility has to depend on the state of the world $\theta$, as well as the experiments $\xi$ you are going to perform (which I guess is implicit in $y_\Xi$). So shouldn't the ""lowest level"" utility be a function $u(y, \theta, \xi, a)$, which you then integrate over $p(\theta|h_t)$, to obtain $u(y, \xi, a) = \mathbb{E}_{p(\theta|h_t)} [u(y, \theta, \xi, a)]$, then take $\max$ wrt $a$,  and finally integrate over the predictive $p(y |\xi, h_t)$ to obtain an expected utility, which can then act a design ranking criterion, as you do in Eq 4 and (cf Eq 2 in [2]).

### Related work: 

For a field that has such rich history and renewed interest from the ML community recently, the related works section is quite short and sparse on citations. Some areas that are missing include:
- Decision-theoretic BED: as previously discussed, the general framework of utility-based BED was developed by Lindley (1972).
- BED + RL: this work touches on some aspects of RL; It might be good to discuss relations recent works in the intersection such as [5] and [6] (in addition to those mentioned)
- Decision-theoretic approaches in related fields such as Bayesian Optimisation, e.g. [7], [8]
- Finally, I'm not too familiar with this line of literature, but  more recent work around decision transformers---is there any relation between TNDP with works like [9] and [10]?

### Other:

- Line 6: ""most recent BED methods use amortised inference with a policy network"" is not quite correct in the sense that no ""real inference"" (posterior updates on the parameters $\theta$) are performed. 
- Line 179: ""to ensure the framework satisfied the permutation invariance property of sequential BED"": not all BED problems are permutation invariant. For example, designing experiments for time series models (e.g SIR in [3] and [4]), permutation invariance does not hold. This aspect has been discussed in e.g. Section 3.3 of [3].
- Assuming you do want a permutation invariant architecture (most design problems fall in that category): by conditioning on $t$ as part of the global information (GI) set, I think you actually break that invariance. This is because encoding $(\xi, y)$ at time $t$ or at time $s$ will give you different outputs. As far as I can tell from Fig2b), $D_c$ does attend to GI. Could you please explain if that's the case or I have misunderstood something?

-----
#### References

[1] Lindley, D. V. (1972). Bayesian statistics: A review. Society for industrial and applied mathematics.

[2] Chaloner, K., & Verdinelli, I. (1995). Bayesian experimental design: A review. Statistical science, 273-304.

[3] Ivanova, D. R., Foster, A., Kleinegesse, S., Gutmann, M. U., & Rainforth, T. (2021). Implicit deep adaptive design: Policy-based experimental design without likelihoods. Advances in neural information processing systems, 34, 25785-25798.

[4] Kleinegesse, S., & Gutmann, M. U. (2019, April). Efficient Bayesian experimental design for implicit models. In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 476-485). PMLR.

[5] Mehta, V., Paria, B., Schneider, J., Ermon, S., & Neiswanger, W. (2021). An experimental design perspective on model-based reinforcement learning. arXiv preprint arXiv:2112.05244.

[6] Mehta, V., Char, I., Abbate, J., Conlin, R., Boyer, M., Ermon, S., ... & Neiswanger, W. (2022). Exploration via planning for information about the optimal trajectory. Advances in Neural Information Processing Systems, 35, 28761-28775.

[7] Neiswanger, W., Yu, L., Zhao, S., Meng, C., & Ermon, S. (2022). Generalizing Bayesian optimization with decision-theoretic entropies. Advances in Neural Information Processing Systems, 35, 21016-21029.

[8] Ivanova, D. R., Jennings, J., Rainforth, T., Zhang, C., & Foster, A. (2023, July). CO-BED: information-theoretic contextual optimization via Bayesian experimental design. In International Conference on Machine Learning (pp. 14445-14464). PMLR.

[9] Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., ... & Mordatch, I. (2021). Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 15084-15097.

[10] Zheng, Q., Zhang, A., & Grover, A. (2022, June). Online decision transformer. In international conference on machine learning (pp. 27042-27059). PMLR.

Limitations:
Some limitations of the work were outlined in the Discussion section of the paper. Regarding negative societal impact, the field of experimental design (which boils down to efficient data collection), generally warrants some discussion. 

The experiments presented in this paper mostly use synthetic data and do not have negative impact; the HPO experiment, which uses real data does not (directly) represent an application with negative impact. However, applying these methods in real-world applications, particularly if decisions directly affect humans, as in e.g. personalised medicine, could raise concerns around bias, fairness, explainability and privacy. 

I would suggest to the authors to add 1-2 sentences in their limitations section to acknowledge 1) the synthetic or semi-synthetic nature of the experiments, and 2) potential concerns that might arise when applying their method in real-world applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a transformer-based architecture for jointly sampling designs and decisions in Bayesian Experiment Design (BED) using a forward-looking criterion. The latter considers the improvement in maximum expected utility brought about by a new design-outcome pair, where the expectation is taken with respect to the predictive distribution of the model. The main innovation of the paper lies in the coupling between information gain and utility maximization in an amortized, transformer-based framework in the spirit of attentive neural processes. The performance of the new architecture is evaluated on a toy regression task and two more representative models, exhibiting stable performance gains over contender methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is clearly written, the ideas and formulations are stringent and well-justified, overall making it easy to follow and a pleasure to read (with the exception of Section 4.1, see below).

- The proposed architecture and training objectives are novel and seem to unlock both qualitative and quantitative improvements over existing methods. 

- The results indicate superior and stable performance of the proposed architecture on two interesting tasks, along a toy 1D GP model which seems to be a standard proof-of-concept task in the neural process (NP) literature.

Weaknesses:
- Some notational confusion can be avoided by consistently using the notation $a_{1:t}$ to denote a sequence of $t$ elements and $a_t$ to denote the $t$-th element in the sequence. Currently, $h_t$ denotes a sequence, but, e.g., $y_t$ denotes an element, and then again $\theta_{1:L}$ also represents a sequence. Also, P4L126 is an abuse of notation with slightly confusing wording, such as “the predictive posterior distribution over all possible designs”, whereas the predictive distribution(s) are over future \textit{outcomes}. This is in no way different than the posterior predictive in Bayesian (non-linear or linear) regression, where the posterior predictive is conditioned on the training data set and the set of (unlabeled) predictors available at test time. Hence, I struggle to understand the need for the convoluted abuse of notation, but I may be missing something. Also section 4.1 suddenly starts using bold font for vectors, which was not the case in the preceding sections. 

- Figure 2 is not particularly informative for the data flow, as it does not clearly communicate weight sharing, input-output operations and dependencies (left panel); the right panel comes out of the blue and is not well explained (i.e., what are the elements on the “left” and on the “top”); the description below on P6 does indeed disambiguate the idea behind the construction of the masks, but I believe it is best when figures support and enhance the text and not vice versa.

- Overall, I feel that Section 4.1 is the weakest link in the paper, and I believe the authors can think about optimizing the ratio of details dispersed between the main text and the appendix. For instance, there is no need to reiterate established transformer-based computations, but it could be helpful to explicate the construction of the masks, the representation types (e.g., vectors, sequences of vectors,...?), and the precise partitioning of the components into keys, queries, and values.

- According to my understanding, none of the contender methods in the experiments is an amortized method. Wouldn’t some of the existing amortized BED methods (e.g., as highlighted in the Related Work) make for suitable benchmarks, despite not optimizing for future decisions?

- The topic of model misspecification is never mentioned in the paper, even though the comprehensive review paper [1] states that it remains a major unsolved issue in BED and in amortized Bayesian inference more generally [2]. I believe this should also be acknowledged in the current paper and the authors can potentially think about quantifying the impact of model misspecification in a small ablation study in the final version of the manuscript.
 
I am happy to discuss these points with the authors and increase my score if they are addressed / clarified.

[1] Rainforth, T., Foster, A., Ivanova, D. R., and Bickford Smith, F. (2024). Modern Bayesian
429 experimental design. Statistical Science, 39(1):100–114.

[2] Schmitt, M., Bürkner, P. C., Köthe, U., & Radev, S. T. (2024). Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks: An Extended Investigation. arXiv preprint arXiv:2406.03154.

Limitations:
The authors openly discuss the current limitations of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tackles an important problem of designing experiments in a way that directly optimizes downstream decision-making tasks, going beyond just inferring parameters of interest. The authors make several valuable contributions:

1. They introduce the concept of Decision Utility Gain (DUG) to quantify how much an experimental design improves the expected utility of the downstream decision. 

2. They propose a novel neural architecture called the Transformer Neural Decision Process (TNDP) that amortizes both the experimental design selection and the approximation of the predictive distribution needed for decision-making. This unified amortized framework is a key innovation.

3. The authors develop a non-myopic training objective that looks beyond just the immediate decision utility to account for effects of the current design on future rewards.

4. Empirically, they demonstrate TNDP's effectiveness over traditional methods on various tasks like active learning, hyperparameter optimization, showing it can find informative designs and make accurate downstream decisions.

In summary, this work makes valuable conceptual and technical contributions to the area of Bayesian experimental design by pioneering decision-aware amortized methods. It opens up new research directions for further enhancing real-world decision-making via optimized experimental data acquisition.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a novel problem formulation by introducing the concept of Decision Utility Gain (DUG), which shifts the focus of experimental design from reducing parameter uncertainty to directly optimizing downstream decision utility. This new perspective is a creative departure from traditional Bayesian experimental design (BED) approaches.
- The application of amortized inference techniques to decision-aware experimental design can be considered an original contribution, as it represents a new domain for these methods beyond traditional BED.
- The empirical evaluation is comprehensive, spanning diverse tasks such as active learning, hyperparameter optimization, and synthetic regression problems. The results demonstrate the consistent superiority of TNDP over traditional methods.

Weaknesses:
- The authors could provide a more rigorous analysis of the properties and characteristics of the TNDP architecture, such as its convergence behavior, sample complexity, and theoretical guarantees (if any) regarding the quality of the proposed designs and decisions.
- The experimental evaluation, while comprehensive, focuses primarily on synthetic and benchmark datasets. While these serve as important proof-of-concept demonstrations, the paper could benefit from including real-world case studies or applications to further validate the practical utility of the proposed framework.
- While the amortized nature of TNDP is highlighted as a key advantage, the paper could provide a more detailed analysis of the computational complexity and scalability of the proposed approach. This analysis could include factors such as the training time required for different problem sizes, the memory footprint, and the scalability of the attention mechanisms used in the Transformer architecture.

Limitations:
- The authors mention the use of a basic REINFORCE algorithm for training the query head, which can lead to unstable training, especially in tasks with sparse reward signals. While they suggest the use of more advanced reinforcement learning methods as a potential solution, a more detailed discussion on the specific challenges faced during training and the trade-offs involved in selecting different RL algorithms would be beneficial.
- The authors mention that their model is trained on a fixed-step length, assuming a finite horizon for the experimental design process. A discussion on the limitations of this assumption and the potential difficulties in extending their approach to infinite horizon or open-ended experimental scenarios would be valuable.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zAuerb1KGx;"REVIEW 
Summary:
This paper proposes an improved approach to multi-label learning using $\mathcal{H}$-consistency bounds by introducing the multi-label logistic loss to effectively handle label correlations. It extends to various multi-label losses, ensuring Bayes-consistency across diverse settings, and includes efficient gradient computation algorithms for minimizing the proposed loss function. This work offers a unified framework with robust consistency guarantees, advancing beyond traditional methods in multi-label learning.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Introducing the multi-label logistic loss, which effectively addresses label correlations often overlooked by traditional binary relevance surrogates under Hamming loss.

- The paper establishes $\mathcal{H}$-consistency bounds for a wide range of multi-label losses, ensuring Bayes-consistency across diverse multi-label learning scenarios. This extends beyond previous research that primarily focused on specific loss functions.

- It offers a unified framework that accommodates various multi-label losses, including novel extensions and adaptations from standard classification. This is supported by efficient gradient computation algorithms specifically designed for minimizing the proposed multi-label logistic loss.

Weaknesses:
- The motivation and background of this paper lack clear logic and hierarchy. It is suggested to first outline the shortcomings of existing methods and then clearly present the research questions addressed in this paper.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores surrogate losses and algorithms for multi-label learning, focusing on \( \mathcal{H} \)-consistency bounds. It identifies the limitations of Hamming loss and introduces a new multi-label logistic loss that accounts for label correlations. The study extends this to a broader family of multi-label losses and adapts comp-sum losses from standard classification to multi-label learning. The authors propose a unified framework providing strong consistency guarantees for multi-label losses and describe efficient gradient computation methods for minimizing these losses.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors conduct a detailed analysis of the popular Hamming loss in multi-label learning when using smooth losses. They identify its sub-optimal dependency on the number of labels and its failure to account for label correlations, providing valuable insights into the limitations of existing loss functions. 
1. The authors introduce an improvement by presenting a novel surrogate loss, the multi-label logistic loss, which accounts for label correlations and benefits from label-independent \( \mathcal{H} \)-consistency bounds. This innovation addresses the identified drawbacks of existing loss functions and broadens the analysis to include a more extensive family of multi-label losses, including a new extension based on linear-fractional functions related to the confusion matrix.
1. The authors extend their work by adapting multi-label logistic losses to more comprehensive multi-label comp-sum losses. By demonstrating that this family of surrogate losses benefits from \( \mathcal{H} \)-consistency bounds and Bayes-consistency across any general multi-label loss, they propose a unified surrogate loss framework. This expands upon previous work that only established consistency for specific loss functions, showcasing the applicability of their approach.
1. The authors' writing is clear and well-structured, with each theoretical assumption and conclusion articulated distinctly.

Weaknesses:
1. In section 4, although the excellent properties of the proposed multi-label logistic loss are proven, providing a detailed explanation of each component of this loss would further enhance the reader's understanding of its superiority.
2. If the advantages of this loss could be demonstrated through experimental validation, it would be more intuitive for readers.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study surrogate losses and algorithms for multi-label learning via H-consistency bounds and introduce a novel surrogate loss, multi-label logistic loss in this paper. By broadening the H-consistency bounds analyses to more general multi-label losses and extending to multi-label comp-sum losses, the authors provide a unified surrogate loss framework for H-consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to follow.
2. The authors make comprehensive reviews of related works, including their pros and cons.
3. The authors provide rigorous theoretical analyses of the limitations of existing binary relevance loss, the H-consistency of the proposed multi-label logistic loss, and the extensions to more general multi-label losses. The theoretical contribution is important for multi-label learning.
4. The authors demonstrate the efficient computation of the gradient for the proposed multi-label logistic loss and conduct time complexity analyses.

Weaknesses:
1. I understand that this is a theoretical work, and experiments of empirical evaluations are not its focus. However, adding experiments to compare the proposed loss with commonly used multi-label losses on standard datasets would make the paper more comprehensive and appealing. Besides, it can also verify whether the proposed loss is effective in practice.
2. There is a typo in line 300.($1-\bar{L}_{ham}(\cdot, y)$).

Limitations:
Yes, the authors have adequately addressed the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper derives H-consistency bounds for binary-relevance style surrogate losses, as well as a new surrogate, for mutli-label learning problems, showing that the proposed multi-label logistic loss whose upper-bound on the Hamming loss is independent of the number of labels.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The $H$-consistency bounds provided in the paper are more informative than existing Bayes-consistency results, as they hold not just in the infinite limit.

The novel multi-label logistic loss allows upper-bounds that do not depend on the number of labels.

Weaknesses:
The paper does not provide any experiments. While this is OK for a theory paper, it does mean that the question of whether the new surrogate works better in practice remains unanswered (which should be reflected in the conclusion section, at least), for two reasons:
a) all the theory provides are upper-bounds, which might not be indicative of actual performance
b) while the theory provides better guarantees for the task loss if the surrogate is reduced to the level $\epsilon$, it might be that reducing the new surrogate is just much more difficult than optimizing binary relevance. In particular, if the computational cost for reducing the multi-label logistic loss to the same level $\epsilon$ as binary relevance is larger by at least $\sqrt{l}$, then, normalized for compute, the advantage of the new surrogate vanishes.

It is claimed that the gradient of the multi-label logistic loss can be computed efficiently, yet the presented formulas still contain sums over the entire $2^l$ entries of the label space. Even if they can be precomputed once, already at moderate label space sizes of l ~ 100 would these quantities be intractable.

It is annoying that most equations are unnumbered. Even if they are not referred to in the paper, your readers and reviewers might want to reference them.

the equation after l. 328 switches between $\mathbf{\mathsf{y}}'$ and $y'$; and $y''$ changes to $y$

l. 114: I'm not sure what the point here is of introducing the threshold $t$, if it is set to $0$ in the same sentence? Couldn't $t$ be simply absorbed into $h$?

l. 178-180; 208: Arguably, completeness does _not_ hold in practice, because there is some form of upper-bound (e.g., weights representable in the given floating-point format)

l. 231. Binary relevance is not just Bayes-consistent w.r.t. the Hamming-loss, but also works for precision-at-$k$

In the equation after line 542, I think $\bar{L}$ should be $\bar{L}_\mathrm{ham}$? 

l. 503: I think $q$ should be $q_i$, and there is a weird subscript on that line.

l. 174 consist -> consisting

Limitations:
I'm not sure if the proposed surrogate actually is tractable for label spaces with more than 50 labels.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zApFYcLg6K;"REVIEW 
Summary:
This paper introduces a new algorithm for constructing U-statistics under central DP. Compared to the naive method, the proposed estimator exhibits lower variance. The authors also derive a lower bound for private algorithms. Several statistical applications are presented to illustrate the methodology.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
U-statistics are widely applied in statistical inference. The improvements in private estimation presented in this paper are useful, and the theoretical results are solid.

Weaknesses:
The calculation of the privacy budget lacks precision.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the problem of private estimation of U-statistics. The authors propose a new thresholding-based approach using local Hájek projections to achieve nearly optimal private error in both non-degenerate and degenerate settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper provides solid theoretical foundations, including lower bounds for the private error and theoretical guarantees for the proposed algorithm.
2. The proposed method is applicable to a wide range of U-statistics problems, from hypothesis testing to subgraph counting in random geometric graphs.
3. The method aims to provide private confidence intervals for U-statistics, addressing a gap in existing literature.

Weaknesses:
1. The paper is difficult to read due to the heavy use of parameters and notations, many of which are not well-defined or explained, particularly in the algorithmic sections.
2. The manuscript provides non-asymptotic results for the DP estimators, but lacks the asymptotic normality results typical for non-private version of U-statistics, which are crucial for practical applications. I think the asymptotic variance of the private U-statistics will change compared to the non-private version. More discussion on expected on this difference. 
3. To provide private confidence intervals, the variance should also be estimated privately. This aspect is not thoroughly discussed, making the testing problem in Section 5 less meaningful.
4. There are no experimental results to demonstrate the practical performance of the proposed algorithms, which is a significant omission.
5. The paper only consider the 1-dimensional data $X$ throughout the paper. A general discussion of d-dimensional vector are needed because it may suffer from the curse of dimensionality, which will affect the generalizability of the results.

Limitations:
1. The paper should discuss the differences and potential advantages of the proposed method compared to directly adding noise to the estimators.
2. The authors should include asymptotic normality results for the DP estimators, similar to those available for non-private U-statistics.
3. The paper would benefit significantly from experiments that validate the theoretical findings and demonstrate the practical applicability of the proposed methods.
4. The author should specify the dimension of $X$ and discuss its impact on the results.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem of estimating U statistics under central differential privacy. U statistics are established minimum variance unbiased estimators for estimable parameters in the form $\mathbb{E} h (X_1, ..., X_k)$, where $h$ is a kernel and for all $i$ $X_i$ is i.i.d. from some underlying distribution. In other words, U statistics estimate averages of kernels applied to subsets of the data of degree (size) $k$. This type of problem arises in multiple statistical tests such as goodness-of-fit tests and Pearsons's chi-squared tests, uniformity testing, subsampling and other scenarios. While many methods have been studied for differentially private mean estimation, the research on private U statistics is in its early stage and has so far mainly focused on local differential privacy models and discrete data. This paper seeks to provide differentially private U statistics estimators achieving nearly optimal private error for both the case of non-degenerate kernels and degenerate kernels.

The main contributions of this paper are: i) it derives the lower bound for private algorithms for the non-degenerate kernel case (Theorem 1); ii) it finds that applying off-the-shelf private mean estimation procedures to U statistics estimation yields suboptimal error; iii) it proposes an algorithm that achieves nearly optimal private error in the non-degenerate kernel case, and evidence of near optimality for bounded degenerate kernels. 

The proposed algorithm (Algorithm 1) is based on representing U statistics via the Hájek projection, and leverages the fact that local Hájek projections enjoy strong concentration around the conditional mean. Basically, if all local Hájek projections $\hat h(i)$ are within a certain threshold distance from the pre-computed empirical mean $A_n$, the output $\tilde{A}_n$ on line 14 is going to be equal to $A_n$; if not, for every subset $S$ containing a bad index, $h(S)$ is replaced by a weighted combination of $h(S)$ and $A_n$. The choice of threshold $\xi$ ensures $L = 1$ with high probability, maintaining a balance between excluding bad data and preserving good data, while also keeping the sensitivity of the final adjusted mean $\tilde{A}_n$ small​, which is crucial for differential privacy. A lower bound for sub-Gaussian non-degenerate kernels is provided (Corollary 1) and Algorithm 1 is proven to match this lower bound. It is also shown that Algorithm 1 matches the lower bound for bounded degenerate kernels (Corollary 2).

The paper discusses a wide range of applications of the proposed method to uniformity testing, goodness-of-fit tests, Pearson’s chi-squared tests, symmetry testing, and sparse graph statistics.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is clear, well-structured and provides rigorous derivations and proofs to back the proposed methods and claims. 

The paper addresses a notable gap in current differential privacy research, which is U statistics under differential privacy. The authors derive lower bounds for both the private sub-Gaussian non-degenerate kernel case and the private bounded degenerate kernel case. These bounds support the proofs that the proposed method achieves i) near-optimality for sub-Gaussian non-degenerate kernels and ii) strong evidence of near optimality for the bounded degenerate case. These results are valuable in the context of the differential privacy research community.

The contributions are clearly highlighted.

I appreciate the effort by the authors to make the results as clear as possible for the reader. In particular, the table summary of the error of different private methods in Table 1 makes it easy to understand the relative error performance of different methods at glance; similarly, in a couple of instances the authors provide key intuitions behind the proposed methods, which helps break down important technical steps that are fundamental to the proposed method. The notation is also clear and consistent.

The proposed method has wide applicability, as demonstrated in the Applications section, where the authors describe the usefulness of the method spanning multiple statistical tests and sparse graph statistics. 

Computational complexity and alternative computationally efficient approximations of U statistics are also discussed.

Extensive proofs and supporting technical derivations are provided in Appendix, although I did not review it in detail due to time constraints.

Weaknesses:
I didn’t find any significant weaknesses in this paper. The paper is highly technical and notation-heavy, but as I described in the previous section, it still reads very clearly. A few of minor notes:

- Since [53] appears to be foundational to the development of the main proposed method, it is worth dedicating a short description of it and/or specification of which ideas in [53] have been built upon.
- Theorem 2 is not followed by a pointer to its proof in Appendix. Please reference the proof in Appendix.
- Limitations of the proposed methods are briefly mentioned throughout the paper, but I would prefer if they were addressed separately in a short dedicated paragraph or subsection, making them more easily identifiable by a reader skimming through the paper.

Limitations:
Limitations of the proposed method are sparsely mentioned throughout the paper. As I mentioned under ""Weaknesses"", it would be preferable to add a dedicated paragraph to the limitations, even if short.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies differentially private estimation of U-statistics (estimators for such statistics are averages of functions $h$ that depend on a number of i.i.d. samples $X_1,\dots,X_k$). This is a generalization of the commonly studied mean estimation problem where $k=1$ and such estimators with $k>1$ are widely applied across statistics. The authors are primarily interested in cases where $h$ is a subgaussian kernel i.e. the distribution of $h(X^k)$ is subgaussian or cases where the range of $h$ is bounded (and satisfies a certain degeneracy property).

The main contributions of the paper are as follows:
1) They first consider approaches that reduce differentially private U-statistics to differentially private mean estimation and argue that natural approaches result in estimators that are either suboptimal in either the non-private error terms or the private error-terms. The estimators they consider are a naive estimator that reduces to the i.i.d. case by computing the function $h$ on a partition of the dataset before applying a subgaussian mean estimation algorithm on the resulting sample of function values, and a more complicated estimator that generalizes the CoinPress algorithm to work with weakly dependent samples. The former has suboptimal non-private error while the latter has a suboptimal privacy term (the dependence on $k$ is suboptimal).

2) They then consider a different strategy inspired by work on privately estimating the sampling probability for Erdos-Renyi graphs. This strategy exploits the concentration of the 'local Hajek projections' around the true mean.  The idea is to classify coordinates into good and bad coordinates respectively based on how close their projections are to the optimal non-private statistic, and reduce the local sensitivity of the average being computed by reducing the influence of bad coordinates by reducing the weight of the corresponding terms in the average. They can then compute an appropriate smooth upper bound to the local sensitivity of this average and add less noise. They use this idea to obtain a general result for bounded kernels, and then use it to get the optimal rate for subgaussian-nondegenerate kernels, and a bound for general degenerate bounded kernels. They also provide some indication that their bound for general degenerate bounded kernels may be optimal.

3) They also show that their results can be used to privatize 'subsampled' estimators with similar error rates that are computationally much more efficient. Finally, they apply these results to settings where U statistics are used such as various hypothesis testing problems.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1) U-statistics are widely used across statistical testing and estimation, and have been relatively understudied in the privacy literature. This paper explores them quite generally and does a good job of suggesting problems for future work.

2) They do a good job of explaining how natural extensions of traditional DP mean estimators perform sub-optimally in estimating U-statistics.

3) The estimator based on local Hajek projections (and smooth sensitivity) seems quite technically novel and interesting.

Weaknesses:
1) In the applications section, it would be good to discuss existing private algorithms for the corresponding tasks (if there are any) and compare the bounds that are obtained.

2) In the Hajek projection algorithm, it would be nice if they explained how they build on the techniques from [Ullman Sealfon NeurIPS 2019]- which parts are borrowed from that work and which parts are new.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
z86knmjoUq;"REVIEW 
Summary:
The paper presents a new approach called Prompt Evolution with Graph ODE (PURE) for non-distributed fluid dynamics modeling. PURE first learns from historical observations and system parameters in the frequency domain to explore multi-view contextual information, which can efficiently initialize the cue embedding. Interpolations of the observation sequences are then merged into the graph ODE so that the time evolution of the model-adaptive cue embeddings can be captured. These time-evolving cue embeddings are then incorporated into the underlying predictive model to overcome spatio-temporal distributional variations. In addition the paper minimizes the mutual information between the cue embeddings and the observation embeddings to enhance the robustness of the model to different distributions. Finally, extensive experiments conducted on various kinds of benchmark datasets validate the superiority of the proposed PURE compared to various baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The idea of the paper is novel. It is the first to link prompt learning to dynamic system modeling for out-of-distribution problems. 
 
2. This paper is technically sound. PURE first learns initialized prompt embeddings from historical observations and system parameters, and then employs a graph ODE with interpolated observation sequences to capture the continuous evolution of their model adaptation under out-of-distribution changes.  

3. The experimental results show the effectiveness of PURE in different challenging environments.

Weaknesses:
1. The contribution of the proposed method in dealing with the OOD problem needs to be further clarified since the advantages of PURE over the previous efforts, such as Refs. [7, 67, 14, 72], etc., to address the OOD problem are not listed.

2. The writing of the paper needs to be improved. Some of the symbols in the method description section are not defined, e.g., what do P and N in Equation 9 refer to?

3. The experiment is not comprehensive enough. (a) The reasons for selecting baselines are not explained. Data augmentation [66, 7], invariant feature learning [39, 69, 38], adversarial training [67, 7], and domain adaptation [32, 14] are mentioned in the paper in related work for solving the OOD problem, but they are not be compared as baselines in the experiment. (b) The experiments in this paper do not state whether noisy data are considered. (c) The authors just give a brief description of the results without analyzing the reasons behind the high performance.

Limitations:
The authors list limitations in the appendix but do not mention them in the main text. It is recommended that the author make a description in the main text.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
- The paper aims to improve the out-of-distribution (OOD) generalization of fluid dynamics modeling.

- Two types of OOD scenarios are targeted: OOD across different systems and OOD within the same system across different timestamps.

- The paper proposes a framework named PURE, composed of modules including:
    - Multi-view Context Exploration, which explores spatio-temporal data using both the attention mechanism and the frequency domain;
    - Time-evolving Prompt Learning, which incorporates the interpolation of observation sequences;
    - Model Adaptation with Prompt Embeddings, which leverages time-evolving prompts to mitigate temporal distribution shifts.

- Extensive experiments on a range of fluid dynamics datasets support the claim.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Significant topic: OOD generalization in fluid dynamics modeling.

- Well-motivated, as OOD generalization is a crucial challenge in this field.

- The presentation effectively delivers the message.

- Extensive experiments have been conducted.

Weaknesses:
- My major concern with the paper is that the OOD challenge in dynamics modeling is not well-formulated. The paper describes the OOD scenario verbally as ""*different dynamical systems could involve different parameters in underlying rules*"" and ""*during long-term auto-regressive forecasting, the input data distribution could vary hugely during temporal evolution,*"" which is straightforward and easy to understand. However, the mathematical formulation of these scenarios is absent. This formulation should be the foundational basis of the topic, as we need to clearly define the problem before addressing it.

- Given the lack of mathematical formulation of the challenge, I find myself lost in the proposed approach section, unsure of the necessity for specific components. While I understand the function of each component, I cannot see why it is needed or which gaps it aims to bridge in the absent mathematical framework.

- Why is the proposed method termed ""prompt""? Is there a connection to prompt tuning in large language models?

- How do you quantify the distribution shift in dynamics modeling? Can you rank the 'difficulty level' of OOD generalization in your experiments and analyze in which scenarios your method stands out and why?

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper pioneers the connection of prompt learning with dynamical system modeling to address the challenge of out-of-distribution shifts. The proposed PURE method initializes prompt embeddings by learning from historical observations and system parameters.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The paper is easy to follow. 
2.The proposed method is sound and innovative.
3. The authors provide theoretical proof and show comprehensive experimental comparisons.

Weaknesses:
1. Some results may be incorrectly labeled as suboptimal in table, and there are errors in the use of some symbols.
2. The explanation of the experimental results is not detailed enough, making some experiments difficult to understand.
3. The proposed method is aimed at OOD (Out-Of-Distribution), but the experiments lack comparison and discussion with methods specifically targeting OOD, such as [1] and [2].
Reference:
[1] Kirchmeyer, Matthieu, et al. ""Generalizing to new physical systems via context-informed dynamics model."" International Conference on Machine Learning. PMLR, 2022.
[2] Yin, Yuan, et al. ""LEADS: Learning dynamical systems that generalize across environments."" Advances in Neural Information Processing Systems 34 (2021): 7561-7573.

Limitations:
This method does not apply to real-world scenarios, such as rigid dynamics modeling and traffic flow forecasting.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a graph ODE-based approach for OOD fluid dynamics modeling. PURE aims to learn time-evolving prompts via graph ODE for adaptation of spatio-temporal forecasting models on OOD scenarios. To address temporal distribution shifts, the interpolation of obersvation sequences are combined into graph ODE framework to learn evolution of prompt embeddings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper proposes a new approach that connects prompt learning and dynamical system modeling which addresses OOD shifts.
- By learning time-evolving prompts that adapt to changes in system parameters and temporal evolution, the approach can enhance model robustness.
- The paper provides theoretical analysis on incorporating observations during evolution.
- Experiments on diverse benchmarks show generalization ability to OOD and different prediction length.

Weaknesses:
As I am not an expert in this field, I am unable to find major concerns or weakness of the approach.
- As the method is based on attention, the proposed approach may have limited scalability and take long computation time. Is there a comparison on these with the previous works?

Limitations:
The limitations are explained in Appendix I.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
z7h7zMgyPJ;"REVIEW 
Summary:
This paper presents an efficient and simple weak-to-strong learner that has optimal in-expectation error. In weak-to-strong learning, we are given a dataset of $m$ points from a distribution, and a $\gamma$-weak learner that returns hypotheses from a class of VC dimension $d$. AdaBoost, which is a textbook weak-to-strong learner, makes $O(\ln(m)/\gamma^2)$ total invokations to the weak learner, and the best-known analysis for it shows that it suffers an in-expectation error $O\left(\frac{d\ln(m/d)\ln(m)}{\gamma^2 m}\right)$. Larsen and Ritzert (2022) constructed a weak-to-strong learner, that has expected error $O(d/\gamma^2 m)$. Furthermore, they showed that this is the optimal error that one can obtain from $m$ training examples and a $\gamma$-weak learner. However, the weak-to-strong learner by Larsen and Ritzert (2022) makes $O(m^{0.8}/\gamma^2)$ invokations to the weak learner --- which is exponentially worse than AdaBoost. Another bagging-based-boosting algorithm due to Larsen (2023), which also achieves the optimal expected error of $O(d/\gamma^2m)$, makes only $O((\ln m)^2/\gamma^2)$ invokations to the weak-learner. This is still a log factor worse than AdaBoost. Could we then hope to obtain a tighter analysis of the error of AdaBoost, and show that it obtains the optimal error with only $O(\ln(m)/\gamma^2)$ invokations to the weak learner? Unfortunately, no. Høgsgaard et al. (2023) showed that AdaBoost necessarily suffers an expected error which is at least $\Omega(d\ln(m)/\gamma^2 m)$.

Can we then at least shoot for a different weak-to-strong learner that attains the optimal expected error of $O(d/\gamma^2m)$, and also invokes the weak learner only $O(\ln(m)/\gamma^2)$ many times (which is the AdaBoost gold standard)? This paper answers the question in the affirmative, with a remarkably simple weak-to-strong learner that they call Majority-of-29. The algorithm is exceedingly simple to describe: Partition the training dataset into 29 disjoint sub-samples of size $m/29$ each. Run AdaBoost on each subsample, and return the majority vote over the AdaBoosts. Since each AdaBoost makes only $O(\ln(m)/\gamma^2)$ calls to the weak learner, and we run a constant (29) many AdaBoosts, the total number of calls to the weak learner is $O(\ln(m)/\gamma^2)$ as required. Further, using an analysis similar to the recent majority-of-3-ERMs algorithm of Aden-Ali et al. (2023), the authors are able to show that the expected error of Majority-of-29 is $O(d/\gamma^2m)$. The analysis from that work does not extend in a trivial manner, and the authors are required to make appropriate technical modifications and enhancements. The number 29 emerges from the analysis --- the authors require showing a new generalization bound for margin-based classifiers (they show a generalization bound of the order $O((d/\gamma^2m)^{\alpha})$), for $\alpha=1/14$, and this lets them obtain the result for Majority-of-$g(\alpha)$, where $g(\alpha)=2/\alpha+1$. The authors conjecture that the analysis of the generalization bound could be improved, and a Majority-of-3 might well suffice for optimal error.

Finally, the authors also do a (somewhat-limited) empirical comparison of the the performances of the three optimal weak-to-strong learners mentioned above (LarsenRitzert, Bagging-based-boosting, Majority-of-29) as well as AdaBoost. The authors find that for large datasets, Majority-of-29 outperforms the other optimal weak-to-strong learners. On the smaller datasets, the authors find that Bagging-based-boosting outperforms Majority-of-29.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The weak-to-strong learner that the authors propose is optimal, and also requires the fewest calls to the weak learner among all optimal weak-to-strong learners that we know. More importantly, it is exceedingly simple and elegant. It also empirically outperforms the other optimal weak-to-strong learners (at least in the experiments performed by the authors). It is also nice to see that the analysis technique from Aden-Ali et al. (2023) finds new applications. The paper is well-written, sets up the stage (along with relevant prior work) well in the first two sections, and provides a nice high-level summary of the formal analysis in Section 3.

Weaknesses:
While the theoretical contribution is substantial and undeniable, arguably, the experimental section is extremely limited (which is okay, and the authors admit this at the end, but this is still a limitation, especially if we want to draw conclusions about the empirical performance of the different weak-to-strong learners). The authors only perform experiments on 4 real-world datasets---there are admittedly many more out there, even just in the UCI repository. Could the authors at least elaborate on their rationale behind choosing the datasets that they did? (e.g., was it a random subset of 4? was it the first 4? was it the best 4 from 20 that they observed this trend on?) How might one believe that there is no cherry-picking of datasets involved? The authors make two conclusions from their experiments: 1) on larger datasets, Majority-of-29 outperforms both Bagging-based-boosting and LarsenRitzert. 2) on smaller datasets, Bagging-based-boosting outperforms Majority-of-29. Importantly, the former conclusion is drawn from results on just 3 datasets, and the latter is drawn from just 1! This can really make one skeptical about whether they should truly believe these conclusions. It is okay that this is just a pilot empirical study, but such claims call for significantly larger empirical validation. Also, please see the questions below.

Limitations:
The authors have adequately addressed any limitations that I can foresee.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new Boosting algorithm, MAJORITY-OF-29, which achieves provably optimal sample complexity and is remarkably simple to implement. The algorithm partitions the training data into 29 disjoint subsets, applies AdaBoost to each subset, and combines the resulting classifiers through a majority vote. This approach not only matches the asymptotic performance of AdaBoost but also improves upon previous weak-to-strong learners in terms of simplicity and runtime efficiency.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
1. The paper introduces a novel method and provides detailed theoretical analysis.

Weaknesses:
2. Existing experiments fail to demonstrate the effectiveness of the proposed method, and there is a lack of analysis and discussion on current experimental results.

Limitations:
Limitations are discussed.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a new boosting algorithm: partition training data into 29 pieces of equal size, run AdaBoost on each, and output the majority vote over them. The authors prove that the sample complexity of MajorityVote29 is optimal and its running time is the same order as AdaBoost. Experimental results are also attached, which corroborate their theoretical findings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Very strong and interesting result
- Mathematically sound, based by my judgement
- Good presentation, self-contained and well-structured

Weaknesses:
N/A

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
z6reLFqv6w;"REVIEW 
Summary:
The article proposes a learning scheme aimed at detecting emergent quantities from time series data of systems made of many interacting parts, such as the brain. To this end the authors combine ""minimum mutual information"", a previously introduced emergence criterion, with SMILE, a differentiable lower bound estimator for mutual information. Differentiability is crucial for the loss function to be optimizable efficiently. They apply this architecture to two examples: First, a series of random bit strings with time-correlated parity, where parity is considered the emergent quantity. Second, real-world data of macaque brain activity. The approach successfully identifies parity in the first example. The authors claim that an emergent feature has been learned for the second example also.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
While the individual parts of the learning scheme are not new, their combination into a differentiable architecture is original and seems like a promising direction to me. The analysis seems sound, even though I found the presentation at times a bit hard to follow as some parts seem to be missing. The individual quantities are mostly clearly defined and the individual results are statistically significant in terms of error bars.

Weaknesses:
From the article alone, I could not fully understand the architecture and its training procedure that is illustrated in Fig. 1. I could not find code that would reproduce it, or a detailed pseudo-code description of the algorithm. It is unclear to me what emergent feature was found for the monkey example, or how Fig. 4 proves that any such feature was found. While the architecture and direction seems promising to me, a few more benchmarks would help make the case that this scheme can find emergent features in many settings. The two examples shown are one toy example with unnatural time dynamics and one real world example where it is hard to understand the dynamics from first principles. Benchmarking this new method on more standard examples with emergent behavior, such as Ising models, would be more convincing.

Limitations:
There is no open access to the code.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a method for learning the causally emergent representation of time series data. Based on the Partial Information Decomposition (PID) and ΦID definition of emergent variables, the paper utilizes variational information lower bounds to estimate and optimize the emergence objective function. The paper further includes a Minimum Mutual Information term and a penalty term, to reduce redundancy and discover diverse emergent variables, respectively. Experiments on a synthetic dataset and a primate brain activity dataset show that the method is able to discover diverse causally emergent representation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Discovering causally emergent representations is a very interesting topic, and has significance in a wide range of scientific disciplines. The paper is inspirational and written clearly. Although the components of the method, i.e. definition of emergence objective function, and variational bounds for mutual information, are not new, their combination to discover causally emergent representations in a learnable way is interesting, and to my knowledge, novel.

Weaknesses:
As discussed above, the novelty is a little limited. This can be compensated by solid evaluations with a wide range of interesting datasets. I think the place the paper needs most improvements is more diverse and extensive evaluations. The paper can benefit from a few more datasets, both synthetic and real world, including the other datasets used in [1] and other references. If there exists baselines for discovering causally emergent representations, those baselines should also be compared against.



Reference:

1. Rosas, Fernando E., et al. ""Reconciling emergences: An information-theoretic approach to identify causal emergence in multivariate data."" PLoS computational biology 16.12 (2020): e1008289.

Limitations:
The authors did not explicitly state the limitation of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a method for identifying emergent variables in time series data through a novel machine learning architecture. It uses unsupervised learning for representation and information theory to find emergent properties in systems, which are often complex and not easily describable at the microscale level. The paper is motivated by the fact that unsupervised learning can be a powerful tool for identifying emergent properties, but current approaches limit to only information theory.

The method rests on maximizing an objective defined by subtracting the mutual information of state variables at time t and the coarse graining at time t + 1 from the mutual information of the coarse grainings at t and t + 1. In other words, the amount of emergent information. Information theoretic definition of emergence is thus used to facilitate unsupervised learning. The method is tested on synthetic data and real-world ECoG brain activity data, demonstrating its ability to identify known and novel emergent features effectively.

Experiments are conducted on a synthetic dataset and a macaque brain activity dataset. For the synthetic dataset, the method is able to estimate the ground-truth value of psi (the difference that is central to the objective function). For ECoG data, skip connections were introduced into the architecture, and once again found emergent representations. 

The paper concludes with a discussion on related (info theory) work, limitations, and future steps.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
### Clarity 
- Diagram features are well designed and results features are clear and salient
- Though writing is somewhat unstructured, the shorter-range explanations are well-done
- Methodology is given in detail. Lots of helpful explanation of relevant information theory, as well as the overall approach

### Quality
- Good to have primate brain data, though more interpretation would help 
- Covers all the basic needs for a new method: real data, novel setup, suitable metrics (though they need more explanation)
- Experimental setup is well-designed to demonstrate that emergent variables are being learned 

### Originality
- As far as I know, applying the information theoretic definition of emergent variables as an objective and training in this setting is novel

### Significance
- An innovative idea that shows promise. While there could be more experimentation, this is a promising and new direction.

Weaknesses:
### Clarity
- It's not immediately clear how to interpret results. The paper shows figures, but it doesn't explain them much. Interpreting them requires a lot of re-reading the methods section
- Writing is somewhat verbose and unstructured, and occasionally reads like a process statement

### Quality 
- This idea is compelling and innovative! The loss built on MI of coarse grainings and state variables is intuitive while creating a solid foundation for taking advantage of the capabilities of unsupervised learning
- On the ECoG dataset, giving intuition/semantic understanding of emergent features (or at least attempting interpretation) would be cool
- Limited experiments on real data in general - ultimately, only one experimental setting is shown as far as I understand. The synthetic problem, while useful, is simple
- Lacking baselines or extensive comparison to existing methods, even if purely information-theoretic

### Significance
- It would help to have clearer comparison to existing methods so that we could see the value-add of this innovation, not just the novelty and value alone

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel objective function and deep learning architecture that are targeted to extract emergent latent representations from time series data. Motivation is very clear. The definition of emergent latent representation interesting and useful. The utilization of mutual information estimators (lower bounds thereof) is smart. Evaluations are restricted to a fully artificial and a fully neurobiological dataset.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The study of emergence and its conceptual and mathematical formalization is of general interest to neural information processing and the involved (part-of) cognitive science subdiscipline within. The authors utilize an existing definition thereof [30] as well as an approximation technique of a lower bound on mutual information (SMILE, [32]), which they combine in a highly elegant manner to yield their learning architecture. 

The usage of a linear temporal predictor with learnable residuals is a great way to bootstrap the system’s abilities. 

Even multiple emergent latents can be successfully extracted. 

A real-world dataset indicates applicability beyond artificial data. 

Paper is very well written – relatively easy to comprehend and all steps taken are very well motivated and sufficient background is provided.

Weaknesses:
System evaluations are minimalistic and not as convincing as I had hoped. Both, comparisons to potential baseline algorithms as well as more ablations are missing. 

Furthermore, one artificial dataset and one not well-motivated real-world neural dataset seems not enough to warrant publication. 

In particular, I would have expected at least one if not multiple DL/Neural Network baselines that do not pursue the information-theoretic objective but simply a standard temporal prediction objective. Those probably do not work on the parity problem at all but at least an attempt seems needed. That is, use a DREAMER-like world model learning architecture with probabilistic latents and see if structure emerges. 

Ablations could have explored more than just the same architecture without the penalty / adjustment term or without the macroscopic estimator. Further, in the artificial dataset the correlation coefficients $\gamma$ are quite high – was this necessary? When does this break down? Evaluations with a non-linear prediction pipeline would also be useful.

Limitations:
Unclear how robust this is as ablations and comparisons are not very extensive.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z6KNvOe9zQ;"REVIEW 
Summary:
This paper aims to explore the use of weak supervision signals in multimodal interleaved image-text data to pretrain visual encoder, compressing the distribution of high-level features into the visual encoder. The paper employs contrastive loss and autoregressive loss to train the model. To prevent the collapse of visual representations, an entropy maximization constraint is applied. The paper derives the equivalence of maximizing the mutual information between the model's input and output as a latent compression and entropy constraint. The proposed pre-training method, called LCL, achieves performance comparable to CLIP on paired data while better utilizing the supervision information in interleaved data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper explores how to use weak supervision signals in more general interleaved image-text data to accomplish visual pre-training. Its advantages are as follows:
1. Unlike previous approaches that fine-tune pre-trained visual models to align visual representations with the text space (Flamingo, LLaVA), this paper explores how to train visual models from scratch using interleaved image-text data. This is a meaningful exploration.
2. To prevent the collapse of visual representations, where autoregressive generation relies solely on textual information, this paper imposes an entropy constraint and further derives it as optimizing mutual information. This approach aids in model training.
3. Extensive quantitative experiments have validated the effectiveness of the visual models trained using this approach.

Weaknesses:
This paper has the following areas for improvement:
1. In some cases, the textual context may have little relevance to the image. It is worth investigating whether such data could harm the model's performance.
2. The paper lacks qualitative experiments to further demonstrate the effectiveness of the method. Designing reasonable visualization analyses would help to further elucidate the advantages of the approach.
3. Similar to CLIP, further demonstrating the model's transfer learning performance through domain adaptation tests and few-shot metrics would be beneficial.

Limitations:
The paper's discussion on data bias and energy consumption limitations is commendable; however, it could further explore issues related to data privacy.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a vision backbone pre-training method named Latent Compression Learning (LCL) to utilize interleaved image-text data. The proposed LCL approach maximizes mutual information between the inputs and outputs of a GPT-like model in autoregressive manner. The proposed method integrate both discriminative and generative objectives by contrasting preceding context and generate subsequent text based on visual representation. The extensive experiments demonstrate that LCL not only matches the performance of existing models like CLIP on paired datasets (e.g., LAION) but also effectively leverages interleaved pre-training data (e.g., MMC4) to learn robust visual representations from scratch.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well written and easy to follow.

2. The paper introduces a new pre-training method, Latent Compression Learning (LCL), which utilizes interleaved image-text data for visual backbone pre-training for the first time. And this can effectively leveraging large-scale web-crawled data, which is easier to crawl compared to the image-text pairs.

3. Extensive experiments are conducted, demonstrating the effectiveness of the proposed method on both paired datasets (e.g., LAION) and interleaved datasets (e.g., MMC4).

Weaknesses:
1. From Table 5, it appears that solely leveraging image-text pairs with LCL does not provide benefits over the CLIP baseline. However, when using the MMC4 dataset, which is manually composed of interleaved text, there is significant performance improvement on downstream tasks. I am curious whether this performance gain results from the increased number of training samples (i.e., the total number of images used during training).

2. According to Table 3, utilizing original interleaved datasets such as Obelics does not yield any performance gain. In comparison, the MMC4 dataset requires more computation for data filtering with the CLIP score and the use of image-text pairs to create interleaved data. It is unclear how to efficiently utilize the original interleaved data directly crawled from the web. Do you have any insights on the differences between these two types of interleaved datasets?

Limitations:
The authors have addressed the limitation in their manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tackles the problem of vision model pre-training. More exactly, it aims to exploit the interleaved image-text data that is very prevalent on the Internet. It proposes Latent Compression Learning that maximises the mutual information between the inputs and outputs of a causal attention model. When visual pre-training is applied to interleaved image-text data, visual latents are extracted using a visual encoding network and then combined with the text and fed to the causal model.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper tackles an important task and proposes an interesting method that may be of interest to the research community.

Weaknesses:
While the method seems interesting, my main concern is related to the experimental part that I find confusing. For example for BEiT3 the numbers reported are different from the ones reported in the paper.

Also, I think that for Tab 6, more multi-modal LLMs need to be included. While there can be a debate on fair vs unfair comparison, I think that you present results on a dataset these need to be complete. So, they can be greyed out, put in a different section, etc and explained why the comparison is not fair, but I don't think it's suitable for models that perform better to not be included at all. So, missing comparisons:

Fang, Yuxin, et al. ""Eva: Exploring the limits of masked visual representation learning at scale."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
Zou, Xueyan, et al. ""Generalized decoding for pixel, image, and language."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

or even some very recent ones for the sake of completeness:
Sun, Quan, et al. ""Generative multimodal models are in-context learners."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
Liu, Haotian, et al. ""Improved baselines with visual instruction tuning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

Limitations:
Limitations are barely discussed at the end of the conclusions. Some limitations can be inferred from the rest of the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper pre-trains models with a combination of a contrastive image-text objective and a generative language objective. The authors provide many results on image classification and vision-language tasks suggesting the competitiveness of the method in controlled settings.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1. The paper is well framed and motivates nicely the need to pre-training on interleaved data.

S2. The paper gives good intuition about what the various equations mean, making the manuscript more accessible.

S3. Consideration of many pre-training datasets including LAION-400M, MMC4, and OBELICS.

S4. Extensive objective ablations spanning both contrastive and generative losses.

Weaknesses:
W1. [MAJOR] The paper presents the objective as novel (L44-54); however, it seems similar to CoCa (Yu et al., 2022.), which also employs a contrastive loss and a next token prediction loss. Can the authors clarify the differences and why the formulation is novel?

W2. It seems equation 3 appears in prior work; however, when it is first presented it seems to be presented as a novel insight. I recommend making the attribution to prior work more clear before introducing the equation.

W3. In the relation to previous pre-training tasks, it is important to also relate to CoCa. It seems the objective is pretty much the same suggesting that the objective is not actually a contribution of the work. Is there any reason CoCa is not mentioned here given the similarities?

W4. Make sure it is clear that you train on sequences with more than one image per sample (I am assuming this is true because you train on MMC4, but when explaining the objectives you include only one sequence for simplicity). 3.3 is a nice place to add this information. Also any special tricks to get multi-image to work? If so, it could also be nice to mention this.

W5. Why are the numbers for Flamingo in Tab 1 for IN-1k so low? Flamingo uses a pre-trained vision backbone, so I expect numbers to be good here.

W6. Is the COCO CIDEr evaluation protocol zero-shot? If so the number in table 4(a) of 87.5 looks extremely high relative to open flamingo and Idefics. Please double check this number and if few-shot prompting is used here, please make this clear. Also why is Gen. only worse than Con. only for captioning. How is contrastive learner able to do captioning?

W7. In the frozen transfer setting in Tab. 6 are all models fine-tuned on the same data? If so, what data? The specifics of the experiment are not clear to me, making it hard to interpret the results.

Limitations:
The authors address limitations in a dedicated paragraph.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z4eVwH484M;"REVIEW 
Summary:
This paper aims to improve vectorized HD map construction for autonomous driving. Inspired by the global feature association in traditional offline HD mapping, the proposed MapUnveiler processes input frames in a clip-based manner and hopes to resolve occlusions using information from previous frames. Built up MapTRv2, MapUnveiler introduces clip tokens together with the Inter-clip and Intra-clip Unveiler modules to update the map queries with temporal information. Experiments on nuScenes and Argoverse2 datasets demonstrate the superior performance of the proposed method, especially on highly-occluded scenes.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The idea of incorporating and aggregating clip-level information for online vectorized HD mapping is reasonable and is more akin to how humans drive. The proposed method has more thoughtful designs than early works such as StreamMapNet to better handle occlusions and incorporate long-range information. 

2. The proposed MapUnveiler obtains state-of-the-art results in various experimental settings. The improvements over previous methods are especially prominent in the large 100mx50m setting and the highly-occluded scenes collected by the authors. 

3. Extensive ablation studies enumerate the choices of almost all hyper-parameters or model components, which helps better understand and break down each element's contributions.

Weaknesses:
1. The clarity of the method description is poor, making it very hard to thoroughly understand the proposed architecture. Details are discussed below:
    
    - The method explanation is not self-contained:  i) The Inter-clip Unveiler section refers to the TTM and directly skips all details. There is no information at all about how is the compact memory token generated from the denser map queries;  ii) The ""loss"" section refers to MapTRv2 and again skips all details. The authors should not assume the general audience to be aware of the concrete details of TTM and MapTRv2. The core formulation of these components should be elaborated with texts or equations, while full details can go to the appendix.
    - The definitions of the temporal window T and the stride S are unclear. Based on the text descriptions and the common definition of stride, my understanding of ""T=3 and S =2"" is that ""each clip has 3 frames, and every two consecutive frames have a temporal gap of 1."" However, the symbols in L177-178 seem to suggest other meanings of T and S.
    - The description of the inference mechanism is also vague. Is the MapUnveiler executed per frame or per clip? Figure 2 seems to suggest the per-clip inference where the predictions of T frames are obtained together. If this is the case, does it hurt the actual response frequency? 
    
     In short, Section 3 of the paper lacks significant details, and I cannot properly understand MapUnveiler's exact formulation. Given that the authors answer ""No"" to Question 5 of the Checklist, I have to raise concerns about the paper's reproducibility.

2. There is no detail on how the pre-training and fine-tuning are conducted. Do you initialize the MapNet by training MapTRv2? If this is the case, how are the training epochs split for the MapNet pre-training and the end-to-end MapUnveiler fine-tuning?  If the 24/6 epochs for nuScenes/Argo2 are only for the fine-tuning stage, then the comparisons in the main table are unfair, as other methods in the table have not fully converged. 

3. The main comparison results are incomplete. Most previous papers provide the nuScenes results of both short and long training schedules, but the main table only presents short-schedule results. Considering the last question about the pre-training and fine-tuning, the authors should complement the table with long-schedule results to show that MapUnveiler can obtain consistent performance boosts when all the methods are fully converged. This concern is backed up by the fact that MapUnveiler's improvement is much smaller on Argo2 compared to nuScenes -- based on my empirical experience, previous methods like MapTRv2 and its followups converge faster on Argo2, and training for 6 epochs is close to convergence. This probably suggests that the large performance gaps on nuScenes come from unfair training settings. 

4. Your interpretation of StreamMapNet and SQD-MapNet's Argo2 training epochs is wrong. These two methods employ a different frame sampling strategy at training time compared to MapTRv2, but their effective number of training samples is the same as MapTRv2. Therefore, the claim about the ""longer training schedules"" in the main table's caption is misleading.

5. The venues in the main table are not accurate. HIMap[49] and MGMap[24] are accepted by CVPR2024, and the information was already available at the time of NeurIPS submission. Furthermore, a recent HD map construction method, MapTracker[A], also studies temporal modeling and should be very relevant, but it is missing in the discussion and related works.   

    [A] MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping, arXiv:2403.15951

Limitations:
The limitations and broader impacts are adequately discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors propose a new approach for constructing vectorized high-definition maps that exploits temporal information across adjacent input frames.  The model, which they call MapUnveiler, operates at the clip-level and consists of an intra-clip unveiler which generates vectorized maps for T frames and an inter-clip unveiler which uses a memory module to aggregate information between clips. The authors present results on two standard benchmarks, vectorized HD map construction benchmarks (nuScenes and Argoverse2) and demonstrate the model’s superior quantitive performance to several previously proposed approaches. They also show several qualitative examples of how MapUnveiler can better handle occlusions in the input images.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The paper is well-written and contextualized well within prior work.
- The methodology is novel and well-motivated.
- The results are strong on the two tested datasets, both quantitatively and qualitatively.
- Many different analyses and ablations were included to justify the design decisions used within MapUnveiler and show its strengths.

Weaknesses:
1. The methods is dense and a bit hard to read. The architecture figures help but are also a bit difficult to parse through. It would be helpful to try to weave more intuition into the text.
2. Claiming ""-9.8%"" is significant but ""-6.0%"" is comparable in the robustness to occlusion section seems a bit arbitrary (and potentially overstating MapUnveiler's performance, as a 6% drop is still considerable). I suggest the authors rephrase this sentence (and address similar claims in the paper).

There are several typos throughout the paper. I have enumerated some here, but encourage the authors to do a detailed proofread:
- 127: With there
- 129: mapnet -> MapNet
- 161: bev -> BEV
- 167 parenthesis 
- 192 backwards parenthesis 
- 294: In addition, if we choose too short

Limitations:
Only one limitation is included. I encourage the authors to think through other potential limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents a method called MapUnveiler, which aims to improve the construction of vectorized HD maps for autonomous driving. MapUnveiler uses a novel clip-level pipeline to unveil occluded map elements by relating dense image representations with efficient clip tokens and propagating inter-clip information. This approach leverages temporal information across adjacent input frames, addressing the limitations of single-frame and streaming inference methods. The model achieves state-of-the-art performance on the nuScenes and Argoverse2 benchmark datasets, demonstrating promising improvements in challenging scenarios with longer perception ranges and heavy occlusions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The introduction of a clip-level pipeline for vectorized HD map construction effectively addresses occlusion issues and leverages temporal information across multiple frames.
2. The method utilizes clip tokens to propagate map information efficiently, reducing redundant computations and enhancing prediction consistency.
3. Extensive experiments demonstrate that MapUnveiler achieves state-of-the-art performance on nuScenes and Argoverse2 benchmarks, particularly in challenging scenarios.

Weaknesses:
1. The community has noticed a severe data leakage issue with utilizing nuScenes and Argoverse2 datasets for online mapping evaluation {1, 2}, as these datasets are not intentionally built for online mapping. It might also be necessary to validate the proposed method on geo-disjoint training and validation sets.
2. It would be good to see the analysis of added model compacity due to the introduction of the proposed intra-clip unveiler and inter-clip unveiler.
3. It seems the proposed intra-clip unveiler and inter-clip unveiler are adaptable to any single-frame inference online mapping methods. It would be good to validate the effectiveness of the proposed modules on other baseline methods.
4. The authors are encouraged to investigate the consistency of estimated HD maps across frames of the proposed method compared to existing methods with ""inconsistent and suboptimal prediction results"" (mentioned in Line 7).
{1} Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps.
{2} Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It.

Limitations:
The limitation of dependency on temporally consecutive frames is discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a clip-based vectorized HD map construction paradigm for the processing of long temporal sequence, in which occluded map elements are unveiled explicitly by efficient clip tokens. Through clip token propagation, MapUnveiler achieves effective utilization of long-term temporal map information by associating inter-clip information, in which clip tokens are propagated rather than dense BEV features. Experiments demonstrate that MapUnveiler boosts the performance on public benchmark datasets, also for more challenging setting like long-range perception and heavily occluded driving scenes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy-to-follow. Figures clearly conveys the intended message.
2. “Unveiling the hidden” and clip token propagation are reasonable and effective strategy for static Map element detection, which is practical and alleviates the problem to some extent.
3. The proposed method demonstrates strong performance on benchmark dataset, comprehensive experiments and ablation studies justify the model design.

Weaknesses:
1. As mentioned at line 227, this work is built on pretrained frame-level MapTRV2 and fine-tuned, thus the comparison can be unfair. Results without pretraining are required to verify your effectiveness.
2. At line 53 and BEV Updater in line 151, for occluded features, how to select the tokens that are visible in certain frames? Seems tokens within the temporal window are fully utilized for BEV update by cross attention, how to determine whether these tokens contain unblocked information? More explanations are required.

Limitations:
Yes. The authors mentioned the weakness of their approach on the corrupted input.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z4duW3KzlD;"REVIEW 
Summary:
The paper presents a deep state-space model architecture with non-linear transitions and emissions. The model disentangles the latent representation for the dynamics and the one for the observed data at each time step - allowing therefore effective state estimation at future time steps and the ability to deal with missing data imputation.
Inference is performed with a deep Extended Kalman Filter, that relies on a RNN architecture to make a more efficient approximate computation of the Kalman Gain (KG) and smoothing gain (SG).
The method is tested on a number of simulated and realistic approaches, and it outperforms competing architectures.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Non-linear/deep state-space models are being used more and more in many applications. Parameter learning and state estimation is however challenging in this setting, and this paper provides an interesting method for this
2. The method is more scalable than comptering KF-based methods thanks to the dynamics network approximation, but still effective despite the aproximation
3. The method builds on some models in the literature, but provides some useful novel components
4. The authors did extensive and well-though experiments/ablations, comparing with many SOTA models
5. There is an extensive appendix covering many details that did not fit in the main text. I particularly appreciated ""A.11.1 Python intuitive code.""

Weaknesses:
The paper is not straightforward to read (had to read it carefully twice), mostly because of the way the required derivations are presented.
The notation used is somewhat not conventional within the ML-heavy NeurIPS community, and should be improved/clarified:
1. In Section 4 you use o^+ notation which is not common in the ML community. Can you clarify what it means and why you need it? This explanation needs to be done in section 4, not referring to a different section.
2. Similarly, what is s in line 219 and why do you need to introduce this notation? The sentence in line 219-221 is key but unclear
3. Why do you need to define the ""gt"" in line 229?
4. Not sure the SI perspective helps in the ML-heavy NeurIPS community, it brings confusion. Maybe can be added in the appendix?

In terms of novelty, the final model seems to me more similar to the Kalman VAE (KVAE) model than what the authors claim. Your model can almost be seen as a modification/extension to the KVAE in which you add the RNN approximation to avoid the O(n^3) complexity, model the transition noise covariance and use a slightly different parameterization for the dynamics network.
Can you clarify the differences between your model and the KVAE? 
In line 96 and Table 6 in the appendix, I don't think your KVAE description is correct: it has a setup very similar to your model which allows to estimate the state dynamics, and allows for direct optimization unlike what you claim.
I am not as familiar with the other KF-based methods mentioned, but make sure your description is correct.

Minor comments:
1. Line 53: typo ""To to""
2. Line 71: typo ""We"" -> ""we""
3. You introduce gamma in line 144, but only say what it is in line 150, making the reader wonder if it was defined above and look for it
4. Line 283: you say ""with n=3m"" without specifying what n and m are. Even if they are defined before, being a notation-heavy paper, better to remind the reader what n and m are.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a very well theoretically motivated State-Space Model learning approach, which is implemented by a gated inference network. The network implements a Hammerstein-Wiener model within a modularized deep learning architecture. It uses GRU cells to mimic Kalman Filtering operations. Forward as well as forward-inverse processing routines optimize the hidden state space estimations. Several theoretical components add to the paper contribution. Evaluations show superior performance on several challenging toy problems with noisy data (pendulum, double pendulum, ball bouncing in irregular polygon, as well as odometry prediction from kiti data) evaluating state estimation and imputation tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Paper is very well-structured. The work is also very well-motivated and well-embedded into the literature. 

The theoretical motivation and system derivations are impressive and usefully embed the author’s GIN system into the Kalman Filtering background. Approximating everything in a variational inference manner via estimations of Gaussians and their Covariances is efficient.

Theorems 3 and 4 offer a theoretical derivation for ensuring stability of the unfolding recurrence. 

The evaluations contain sufficiently challenging problems. Performance is compared with many alternatives, showing superior performance nearly throughout. Only in Table 3 GIN was partially beaten by DeepVO.

Weaknesses:
The theorems 3 and 4 are not really experimentally evaluated. Is instability observed when the recurrent matrix is not modified as proposed? The theorem’s proposal should be verified experimentally. 

Even more elaborate evaluations would of course be great. Seeing the great content and the importance of the theoretical derivations, though, I consider this a very minor point, which can be tackled in subsequent work.

Limitations:
Further evaluations and ablations to truly identify the core components that yield the great results of the GIN system.

Confirm theory components experimentally.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper advances temporal reasoning in dynamic, high-dimensional, noisy environments by introducing a novel architecture for latent variable state space models. The architecture permits efficient Bayesian inference with nonlinear transitions and emissions. Experiments are performed on toy datasets and a simple real-world dataset for state estimation and missing data imputation, showing that it beats benchmarks relative to competing models like RNNs, autoregressive models, and latent variable approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Clear exposition of model architecture and inference algorithm. Theoretical analysis in Section 6.

Weaknesses:
I think one thing that could really strengthen this paper is showing an experiment on a more challenging data set / problem. The first two experiments are on toy problems.

I think another thing is to explain more clearly how this architecture is differentiated from others, i.e. the technical novelty. E.g. what is the relation of your model to other SSMs incorporating RNNs like the Variational RNN (which you benchmark against in the experiments), and what is it about that change that improves inference?

Limitations:
Yes, limitations adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ywEQkCmImh;"REVIEW 
Summary:
This work proposes a new task named Multi-Domain Learning Video Anomaly Detection, which aims to learn a general VAD model across domains. The work finds that abnormal conﬂict is a critical challenge in the task. Then, the work establishes a new benchmark, designs an effective baseline and conducts extensive experiments to investigate this challenge. The results shown on the benchmark demonstrate that the abnormal conﬂict is alleviated.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The work proposes a new task, which is interesting. 
2. The work establishes a new benchmark to evaluate the new task. 
3. The motivation of the proposed baseline, i.e., abnormal conﬂict, is clear and makes sense.

Weaknesses:
I have some concerns about the proposed method, and I think more comparison experiments are needed to demonstrate the effectiveness. Despite this, I think the abnormal conflict issue is interesting, thus I am willing to raise my rating if my major concerns are addressed. My concerns are as follows:

1. Why the proposed Abnormal Conﬂict (AC) classifer can address the abnormal conﬂict problem? Why the label is determined by the discrepancy in Eq. (6)? It seems that there are some mistakes in the formula (inconsistent with that in Fig. 2). 
2. I would like to see the results of more baselines, in addition to MIL, Null-MIL and NullAng-MIL. 
3. More detailed discussions about related works are needed, e.g., virual video anomaly detection datasets [1] and related techniques utilizing virtual datasets [2]. 

[1] Ubnormal: New benchmark for supervised open-set video anomaly detection, CVPR 2022

[2] Generating Anomalies for Video Anomaly Detection with Prompt-based Feature Mapping, CVPR 2023

Limitations:
The paper has discussed the limitations and potential impacts of the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, authors proposed a new task called Multiple Domain VAD (MDVAD), along with a benchmark and new evaluation protocols. Authors' goal is to construct a general VAD model by conducting multi-domain learning while recognizing abnormal conflicts and exploring representations of general normality and abnormality. Authors introduced a baseline for MDVAD and proposed a new framework with multi-head to mitigate abnormal conflicts and proposed Null-Multiple Instance Learning (Null-MIL) and NullAngular-MIL (NullAng-MIL) losses for multi-domain training. Additionally authors suggested an Abnormal Conflict (AC) Classifier to explore general features while being aware of abnormal conflicts. Authors analyzed the primary issues of MDVAD and proposed a baseline for this new task.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. According to the analysis, authors believed that the abnormal conflict and the scene discrepancy are the two main issues and designed a framework with multi-head to deal with these problems. 

2. Null-MIL and NullAng-MIL methods are designed for multi-domain learning, and an AC classifier is proposed for learning general features while abnormal conflicts exists.

3. Authors provided sufficient experiment results for this task and create a new baseline.

Weaknesses:
1. The proposed framework with multi-head for multi-domain seems not flexible enough during the domain changes, such as adding a new dataset with extra abnormal conflicts. And for the abnormal conflicts, will the proposed method performs better compared to make anomaly categories classifications for all anomaly events type of all domains?
2. In my opinion, traditional WS-VAD methods are designed to detect abnormal events in single domain without abnormal conflicts, and when abnormal conflicts exists, it will be better to use other paradigms such as temporal action localization or video grounding. And for the current WS-VAD datasets, the annotations are video-level, or even without category information, which is too weak for higher level anomaly detection. Training model with the current MDVAD paradigm is likely to not achieve good results.
3. Maybe using visual-language model with multimodal alignment can deal with the above issues? These models contain more knowledges for more event categories and higher generalization ability, which are likely to have the ability to individually detect conflicting anomalies. Compared to multi-head regression, is VL alignment a better approach for MDVAD task?

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The manuscript addresses the limitations of existing Video Anomaly Detection (VAD) models that are confined to single-domain learning. The primary contribution of the paper is the introduction of a new task called Multi-Domain Learning for VAD (MDVAD), which aims to develop a general model capable of identifying abnormal events across multiple domains. The manuscript conducts experiments using the MDVAD benchmark and demonstrates the limitations of traditional multi-domain learning. It shows the effectiveness of the proposed baselines in handling abnormal conflicts and achieving robust performance across multiple domains.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The manuscript proposes a new task, Multiple Domain Video Anomaly Detection (MDVAD), which solves the problem that the existing model is limited to a single domain and provides a new idea for the development of domain-generalized models.
2.The MDVAD method proposes domain-specific multiple head mechanism and Null-Multiple Instance Learning Method (Null-MIL), which effectively solves the problem of anomaly conflict between different domains.
3. The MDVAD method constructs a new benchmark containing six representative VAD datasets, which fills the gap of the lack of unified evaluation standard in multi-domain learning tasks.
4. The MDVAD method designs four evaluation protocols (held-in, leave-one-out, low-shot domain adaptation, and full fine-tuning) to systematically evaluate the generalization ability of the model.

Weaknesses:
1. MDVAD introduces the domain-specific multi-head mechanism and the Null-MIL method, which increases the complexity and computational cost of the model, and may place higher demands on the computational resources in practical applications.
2. The multi-domain learning task itself is difficult to train, and with the proposed method further increasing the complexity of training, MDVAD may require longer training time and higher technical requirements.
3. Although the theoretical background and analysis are provided, the theoretical basis and derivation process of some of the methods of MDVAD are slightly weak and need to be further explored and verified in depth. Part of the theoretical analysis is based on specific assumptions, and these assumptions may not be fully valid in practical applications, affecting the applicability of the theoretical analysis.
4. Although new benchmarks and assessment protocols are proposed, MDVAD lacks comparative experiments with other state-of-the-art methods, making it difficult to objectively assess the relative advantages of the proposed methods.

Limitations:
See Weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new task called MDVAD, the goal of which is to effectively learn from multiple domains with different data distributions and definitions of abnormality without confusion, resulting in a general VAD model. To achieve this, the authors expand the traditional single-head framework to multiple-head framework for learning different knowledge and design an AC classifier to handle abnormal conflicts. The experimental results prove the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper focuses on the problem of learning a generalizable VAD model, which is an important task.
2. The experiments conducted by the author are relatively compr

Weaknesses:
1. This paper proposes a new task called MDVAD to achieve generalizable VAD by resolving conflicts in anomaly definitions. However, for any VAD application, the definition of normal or abnormal events should be explicitly determined according to the scenario requirements, rather than simply combining multiple datasets and resolving the abnormal conflicts. I find it difficult to understand under what practical scenario a VAD model trained using multiple datasets with abnormal conflicts is needed.
2. The writing of this paper is not clear enough, where some necessary training and inference details are missed. For example, the normal head training mentioned in NullAng-MIL is confusing.
3. This paper lacks a detailed description of the experimental setup. For example, if an anomalous event is determined to be a conflict, how should the model handle such an event?

Limitations:
I do not recognize obvious potential negative societal impact of this work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
GLUIuli3Sm;"REVIEW 
Summary:
This submission studies the convergence guarantees of and bounds on the expected number of samples used when using loss based active learning. They additionally propose a new sampling scheme that combines loss based sampling and a Polyak step size and provide convergence guarantees. Their analysis covers multiple models and loss functions. They proposed methods further evaluated with numerical experiments on multiple datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The problem addressed is interesting and has not been addressed in the literature. In order for loss  based sampling strategies to be effectively deployed in the wild this sort of analysis is necessary. The algorithmic contributions are also of interest, and borrow a well known approach to setting step sizes (Polyak step sizes) from the optimization community to define their Adaptive-Weight Sampling scheme. This combination is a novel idea and a starting point to explore other methods of setting step sizes in this active learning context.

Weaknesses:
While  the technical contributions of this paper are interesting, the primary weakness is the communication of results. This reviewer has an optimization background rather than an active learning one, but even accounting for this the organization and exposition of results was challenging follow. For example, rather than defining algorithms in a LaTeX algorithm block as is standard in literature, authors simply  refer to modifying the (projected) SGD update. While the general idea of active learning is simple, it is also unclear which variant of active learning the authors are studying. Based on the step size being defined as a Bernoulli variable and the experiments conducting one pass over the dataset, it appears that the authors are studying a “streaming” approach to active learning, where the decision to evaluate the label or not is made upon encountering each datapoint. It appears that the selection operation is to ignore when the Bernoulli sample is zero and evaluate the loss otherwise. This is not made clear in the writing, and an optimization audience may be confused as to why some steps will have step size zero. This understanding could be incorrect, which is likely due to the lack of clarity and motivation of the definitions and algorithms. The authors are encouraged to be more explicit about what problem they are trying to solve, and what the exact definitions of their algorithm is.

Limitations:
The authors have addressed some limitations in their work and potential future directions.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This is a technical paper, whose subject of interest is the convergence of stochastic gradient-based learning algorithms which include a stochastic step size mechanism, whose value is allowed to be influenced by losses or other ""uncertainty"" related quantities that are computed at training time.

Their main theoretical results can be roughly broken into two categories based on the assumptions placed on the step-size mechanism. The first category is where the step size is a re-scaled Bernoulli random variable, taking values in $\\{0, \\gamma\\}$ in their notation, with $\\gamma$ fixed throughout but the probability of a non-zero step size (i.e., $z\_{t} = \\gamma$) can change depending on the data/parameter at each step in the training routine. They start with an argument centered around a monotonic loss function and linear binary classifiers, but also consider an ""equivalent loss"" type of strategy (like in Liu an Li '23), again where a convenient monotonicity assumption (here on $\\pi$) preserves convexity and aids in analysis. Their main bounds are in-expectation upper bounds on the loss incurred by the average of iterates generated using this Bernoulli step size.

The second category is similar, but allows the actual step size to be impacted by loss/gradient values in an ""adaptive"" way, while retaining a certain probability of step size 0. This combination of Bernoulli step sizes with an adaptive step size is what they call ""Adaptive-Weight Sampling (AWS)"", and they provide conditions to obtain upper bounds on the (empirical) objective function of interest (i.e., the average loss). 

Their theoretical results are complemented by empirical analysis, in which they compare uniform random sampling (of points for SGD), ""traditional"" loss-based sampling, and their AWS approach (their Fig 1). This setup assumes loss access, i.e., this is not active learning. On the other hand, for active learning scenarios, a loss estimator needs to be plugged in; they consider the impact of the quality of such an estimator in their second batch of tests (their Fig 2).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Overall, the paper is quite well-written and has a natural logical structure which is easy to follow. The authors have obtained a flexible set of conditions for evaluating SGD procedures with a stochastic loss-dependent step size mechanism, which appear to build quite directly upon the existing literature, which they are good about citing (e.g., Raj and Bach '22, Liu and Li '23).

The paper is a mixture of different efforts, some new convergence theory, a new proposed algorithm (AWS), plus formal/empirical analysis of this algorithm, and I think there is potential for this work to have an audience at a conference like NeurIPS.

Weaknesses:
I am not familiar with the related technical literature, so I will not comment on the novelty or theoretical prowess required to obtain the results here.

I would personally highlight two main points I feel need improvement. The first point is that the narrative of this paper feels really bloated. To the best of my reading, all the talk of ""active learning"" in the title and throughout the paper is totally irrelevant to the entire paper, save for the last paragraph of section 4 plus Figure 2. Yes, there are obvious links between the procedure of interest here and active learning settings, but the core problem setting stands on its own just fine. There is no reason to structure the paper around active learning, it just makes things confusing and downplays the substantive results. I feel like I can say the exact same thing about ""uncertainty-based"" methods. The only uncertainty-related formulation I can find is Corollary 3.8. Having this is great, but why put uncertainty-based and loss-based methods on the same footing when writing the paper?

The second point is related to technical exposition. For the most part the work seems to be well done, but for a first-time reader, certain parts feel rushed and sloppy. I'll make a list of points I tripped up on in the following section.

Limitations:
Not applicable.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper considers the active learning algorithms based on uncertainty and loss functions. The learner queries the label of an unlabeled sample with probability proportional to (some function of) the uncertainty/loss and updates the parameter according to some step size scheme. The authors generalize previous results under the strictly separable binary classification setting and general classification setting with convex loss and smooth equivalent loss. The authors later propose a Polyak-type step size scheme called Adaptive-Weight Sampling and prove its convergence. Numerical experiments verify the efficiency of AWS under both oracle and estimation of loss functions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The analysis is solid;
2. The presentation is clear;
3. The Adaptive-Weight Sampling (AWS) algorithm provides a novel perspective for active learning literature.

Weaknesses:
1. The generalization of previous results seems not to be very essential;
2. The assumption of access to the exact loss function before querying seems too strong for theoretical analysis.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z4FaPUslma;"REVIEW 
Summary:
The paper uses Riemannian optimization to guide the final layer weights (the linear classifier) toward the nearest simplex ETF orientation. In particular, consider the two common approaches of training a deep classifier network:

1. The standard training strategy where the final layer weights are updated by backpropagation.

2. The final layer weights are fixed as a simplex ETF (which has been well-studied in previous works).

The proposed approach leverages the duality between penultimate layer features and the final layer weights (to form a simplex ETF orientation) and gradually guides the latter to an optimal simplex ETF per training step.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The proposed approach frames the gradual transition of weights to a simplex ETF as a Riemannian optimization problem, which can be differentiated. Thus, allowing for an end-end training pipeline. The combination of these techniques is novel to the neural collapse setting.

2. The experimental results are presented for the simple UFMs as well as practical networks and datasets to showcase the convergence benefits.

Weaknesses:
The authors do not provide numerical data for the extra memory and step-time that is required by the extra deep declarative layer. A brief discussion is presented in Section 5 but I believe further details would strengthen the paper. For instance:
- By what percentage does the step time and memory increase when adding this layer?
- When should one avoid the backward pass through this layer and consider only the forward pass?
- What is the dependence on the memory and step time growth with the feature dimension and the number of classes? Maybe a simple UFM-based analysis should suffice.

see more questions below.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a novel algorithm for neural network training. The algorithm is motivated by the recent discovery on the neural collapse phenomenon, which demonstrates that the last layer of neural network classifier will converge to a specific structure named simplex ETF. The authors propose to guide the network parameters to the ETF structure via explicitly penalizing on the distance to the ETF, and further address the non-uniqueness of the solution via adding a proximal term. Experimental results on various neural network architecture and real world datasets are presented, and the proposed algorithm can universally improve the training and testing accuracy over the standard training.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The proposed algorithm is novel and well motivated, and it shows universal and significant improvement over multiple choices of network architecture and datasets. The contribution of this work is solid, it helps the community to understand the benefit of the neural collapse phenomenon, and can potentially improve the standard paradigm of neural network training.

Weaknesses:
1. The presentation should be improved, see questions for detail. In general the authors should give more detailed information about how the algorithm is implemented.

2. Although the accuracy on train and test dataset exhibits significant improvement within the fixed number of training epochs, the proposed algorithm are much more complicated to compute. Therefore it makes more sense to compare the running time and computational cost with Standard and Fixed ETF.

3. Proper ablation study is missing. The authors add many additional techniques, such as exponential moving average, stratified batch sampling, deep declarative layer to improve the training. It is not clear how much the improvement indeed comes from the nearest ETF optimization.

Limitations:
The authors have discussed the limitation properly in the paper. A more detailed discussion with empirical results on the computational cost will be helpful.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
One of the key aspects of neural collapse (NC) is that the penultimate class feature means form a simplex Equiangular Tight Frame (ETF). The main idea of this paper is to leverage this insight and improve training by further encouraging this property during training. The authors suggest doing this by solving a Riemannian optimization at a given iteration. The way it works is that the classifier weights are set to the nearest simplex ETF obtained by solving this inner Riemannian optimization problem. The classifier weights are dynamically updated during training using this Riemannian opitmization problem at each iteration (rather than trained using gradient descent) using a ""deep declarative node"" this allows gradients to propagate through the Riemannian optimization. 

They show that this approach indeed speeds up training convergence and improves training stability. Their experiments include both synthetic and real-world classification tasks and architectures. 

Overall the authors present a nice idea and it is a well-written paper. However, there are a few issues related to the experiments that I outline below. 

From my viewpoint, the value of this paper and their method (to me) is less the improved test accuracy and more the improved stability and speed of convergence. It's important to note that this speed up also comes at an additional cost (i.e. in performing the Riemannian optimization). Therefore, the improvements to stability or speed of convergences should be weighted against this caveat. I think it would help to highlight this tradeoff more upfront and make that more clear/transparent.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This is a thoughtful and well-written paper. The authors suggest a nice idea to leverage this insight of NC in deep learning and their approach has clear benefits. It is a nice idea and very well executed. 

There are clear improvements to the current methods; e.g., their improvement upon [74] by solving the inner Riemannian optimization instead of requiring the model backbone to do the work of matching to a chosen fixed ETF.

The theory and the idea is very compelling. The implementation is good and well explained. Beyond the theory and the novelty of the idea, the main strength of the paper is the value added wrt convergence speed in terms of the number of epochs required for the network to converge.

Good work.

Weaknesses:
The main points of concern for me are in regards to the experiments and how the results are reported in the paper.

Table 2 looks good but is a bit misleading particularly when comparing the ranges of the test top-1 accuracy.
The results are still interesting but it's not such a strong/clear winner; that is, when looking at the ranges, it's not so obvious. The authors point this out and clarify that the advantages are speed to convergence and decreased variability which I agree are definite plusses.

The test top-1 accuracies reported in Table 2 aren't competitive with what can be obtained on these benchmark datasets, particularly for the Resnet models. For example, looking at 200 epochs or training, STL on ResNet50 should be able to achieve 85-90% test accuracy, even for Resnet18 the test top-1 accuracy for STL should be upwards of 75%. Similarly, for CIFAR100 on Resnet50, the test accuracies aren't competitive. It'd be interesting to see if these claims about variability still hold when giving the baselines adequate chance to be competitive.

For Figure 4, also no error bars. Understanding compute restraints, it would be nice to see similar multiple seed runs for ImageNet experiments. 

Finally, one thing that is not reported here is an estimate of compute cost. Their method requires additional compute for each iteration. Perhaps when compared on this axis their implicit ETF and the Standard training method would be more fairly compared. 
The authors do mention this in the limitations section.

Limitations:
N/A The authors address any limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel approach to utilizing ETF geometry. Instead of fixing the weights or making them learnable, their approach dynamically adjusts the weights by solving a Riemannian optimization problem while allowing end-to-end training. They show that their approach outperforms both the fixed and learnable approaches in terms of convergence speed and generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality:
The idea of dynamically adjusting weights is not new, but in the context of neural collapse (NC), it is a natural extension. Fully learnable weights do not provide the ETF structure, and fixed weights are too restrictive. The proposed approach is a good compromise between the two and combines the best of both worlds.

Quality:
The paper is well-written, and the proposed approach is carefully supported by theorems and experiments.

Clarity:
The paper is well-written and easy to follow.

Significance:
Their approach is general and could be applied to a range of problems. The authors applied it to synthetic UFMs and some standard image benchmarks (CIFAR-10, CIFAR-100, STL-10, ImageNet). The authors plan to release code upon acceptance.

Weaknesses:
Overhead Cost:
The proposed method computes the exponential moving average of the feature means, performs a Riemannian optimization, and computes the gradient of DDN. These components introduce overhead in terms of epoch time. The authors claimed in the paper that the gradient of DDN is not computed, and the Riemannian optimization overhead is negligible. This unsupported claim should be backed up by an additional experiment that reports these extra computation times.

Standard Procedure:
""To ensure fair method comparison,"" the authors include classifier weight normalization and feature normalization for the standard procedure. This is usually not the case when using CE loss (see Fig 2). The authors should justify this choice by providing the results without these normalizations for the standard procedure.

Image Baselines Results are not SOTA:
The reported results are not state-of-the-art. For example, ResNet-18 trained on CIFAR-10 only reaches 80.47%. It seems that these baselines are not well-tuned, and the gain of the proposed approach is not clear and could potentially fade away with a better-tuned baseline. Can the authors comment on this? Additionally, the authors should include the results using ResNet-50 on ImageNet, which should provide a stronger reference point.

Fixed ETF Procedure:
The authors only used the canonical simplex ETF for the fixed procedure. The weight matrix results in many zeros and could lead to poor performance when used as the fixed classifier because some neurons will be inactive. The authors should include the results using the fixed ETF with a non-canonical (i.e., projection on a random basis).

Remarks:
The authors should directly clarify in Tables 1 and 2 the ResNet architecture used (18 or 50).

Limitations:
For large-scale problems, the computational cost of the proposed approach could be a limitation due to the high memory cost of computing the backward pass of the Riemannian optimization. Therefore, the authors did not compute the gradient calculation when reporting their results for the image benchmarks.
The authors claimed that they empirically observed no significant difference in performance for small-scale problems where the DDN gradient can be computed.
Including these results in the supplementary material would also be beneficial.
Moreover, we agree that future work should verify if this is true for large-scale problems.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z2739hYuR3;"REVIEW 
Summary:
The problem considered in this paper is online learning in MDPs where transition probabilities are modelled with a log-linear model (with ""multinomial logit function approximation""). The finite horizon, time-inhomogenous setting is considered. The problem is motivated by allowing a nonlinear transformation in modeling the MDP and yet maintaining both computational and information theoretic tractability. Inspired by results in the analogous bandit problems and algorithms developed for them, a number of gradually more complex, but (statistically) better performing algorithms are considered. In particular, while naive approaches give a poor dependence on a problem parameter $\kappa$ that characterizes the ""strength"" of nonlinearity, by adopting previous ideas to the MDP setting, new algorithms are designed that eliminate this poor dependence. A lower bound is also established, which nearly matches the upper bound (but considers infinite action spaces, while the main paper considers finite action spaces).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This is a reasonable problem setting; and the approach is also reasonable. It is nice to have a lower bound, even if there is a mismatch between the settings. It is nice to see that ideas that were developed for the bandit setting generalize to the MDP setting.

Weaknesses:
1. The novelty is limited by that we have seen the same story, same ideas playing out nicely in the closely related bandit setting. 
2. A new parameter, U, the number of next states that are reachable with positive probability in the worst case, appears in the analysis and will appear in the bounds.
3. It is an unpleasant surprise for the reader to discover this dependence only through carefully reading the paper, rather than being told upfront. It is not good that the opportunity to discuss whether this quantity needs to enter the regret bound, and that this quantity needs to be small for the algorithm to be tractable, is missed.
4. Line 83 and onward: The work of Uohamma is discussed but is mischaracterized. My reading of this work is that they do establish that their algorithm runs in polynomial time. It remains unclear why the exponential family model is incomparable with the one considered here; an explanation (with examples) is missing.
5. The paper could use some extra proofreading (e.g., the upper indices in the bottom of page 5, in the displayed equation are not correct); in line 149, in the definition of $U$, $|\cdot|$ is missing.

Limitations:
n.a.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers MDPs employing the MNL function for transition probability, following Hwang and Oh [2023]. The authors suggest efficient algorithms based on online Newton steps, inspired by [Hazan et al., 2014; Zhang et al., 2016; Oh and Iyengar, 2021]. Furthermore, to improve $\kappa$ dependency, they provide algorithms employing local learning with mirror descent inspired by [Zhang and Sugiyama, 2023; Lee and Oh, 2024]. The algorithms achieve $1/\sqrt{\kappa}$ or even detach the dependency of $\kappa$ from the leading term.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The suggested algorithms are computationally efficient and show improvement in $\kappa$ compared to the previous work of Hwang and Oh [2023].

Weaknesses:
- Their suggested algorithms do not seem novel because they are based on previously proposed methods for logistic or MNL bandits. Specifically, the online Newton update is widely studied for MNL or logistic bandits [Oh and Iyengar, 2021; Zhang and Sugiyama, 2023].

- Furthermore, the improvement on $\kappa$ is based on the mirror descent algorithm proposed in [Zhang and Sugiyama, 2023; Lee and Oh, 2024], and the proofs seem to follow the steps in [Zhang and Sugiyama, 2023; Lee and Oh, 2024] in the appendix.

- Lastly, the MNL model for transition probability may have an inherent weakness: the number of achievable states for each (k,n) must be finite, and it is required to know the state space of $S_{k,n}$.

[1] Faury, Louis, et al. ""Jointly efficient and optimal algorithms for logistic bandits."" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.

Limitations:
The authors discuss some interesting future work regarding regret bound. Additionally, I believe, as mentioned in Weaknesses, the MNL model for transition probability has an inherent weakness: the number of achievable states for each (k,n) must be finite, and it is required to know the state space of $S_{k,n}$.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the author analyzes a Markov Decision Process (MDP) model with non-linear function approximation. Specifically, in the finite-time horizon inhomogeneous episodic MDPs setting, the transition dynamics are unknown but the reward function is known. The author proposes using a multinomial logit (MNL) function approximation to estimate transition dynamics, which is superior to the linear function approximation if the model is misspecified in \cite{hwang2023model}. Additionally, the author proposes *UCRL-MBL-OL*, which adapts the previous work that is model-based and has large computational and storage complexity, to an online style that only consumes constant computation and storage resources. Moreover, the author has proven that the regret bound of *UCRL-MBL-OL* matches the state-of-the-art in Theorem 1. Its regret bound achieves $\tilde{O}(\kappa^{-1} dH^2\sqrt{K})$, where $H$ is the time horizon length, $K$ is the number of total episodes and $\kappa$ is considered as a parameter to control the sparsity of the transition dynamics and $d$ is the hidden dimensionality. Ignoring the logarithmic factor and $\kappa$, such a regret bound has only a $\sqrt{H}$ gap compared to the lower bound. After that, with additional assumption, the author utilizes the local information to propose another two algorithms, *UCRL-MNL-LL* and *UCRL-MNL-LL+* to remove the dependence on $\kappa$ and get a tighter regret bound as well as maintain good properties of *UCRL-MNL-OL*.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written. The author makes a clear improvement point compared to the literature.

2. The algorithm proposed by the author enjoys an online learning style that does not need to maintain a large historical set.

Weaknesses:
1. Although this paper focuses on reducing the computation complexity, I am curious about the sample complexity of *UCRL-MNL-OL*.
    
2. Since the algorithm builds up the estimation of the transition dynamics by using MNL function approximation, is it considered a model-based algorithm? More specifically, does it require storing the transition dynamics for each state-action pair in every step?

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the MNL function approximation inhomogeneous RL, achieves the $O(1)$ computation cost, and improves the regret guarantee with regard to $\kappa$. To improve the computation cost, this work employs the online Newton step instead of MLE estimation to estimate $\theta$. Then, they design a novel confidence set by making full use of local information to improve the dependence of $\kappa$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The use of local information instead of a uniform $\kappa$ is novel and useful to improve the dependence of $\kappa$.
2.	The UCRL-MNL-LL+ removes the $\kappa$ dependence on the lower-order term and almost matches the optimal regret results by using high-order Taylor expansion.

Weaknesses:
1.	[1] also use the online Newton step to improve the computation cost in the logit contextual bandits setting. It would be better to discuss the novelty of UCRL-MNL-OL.

[1] Oh, M. H., & Iyengar, G. (2021, May). Multinomial logit contextual bandits: Provable optimality and practicality. In Proceedings of the AAAI conference on artificial intelligence (Vol. 35, No. 10, pp. 9205-9213).

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the recently proposed MDPs that use multinomial logit function approximation for state distribution validness. The results and algorithms improve the prior work of Hwang and Oh [2023] in multiple aspects, including computation efficiency, storage, and statistical dependence on the problem-dependent quantity $\kappa$ that can be exponentially small. In addition, the authors establish a matching lower bound on $d$, the feature space dimension, and $K$, the number of episodes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and has clear logic flows. Readers can see how the authors approach the MDP problem and tackle the challenges. In particular, Table 1 is quite useful for demonstrating the advancements in the work.
- The improvements in both computation and storage efficiencies are essential for practical applications. In Theorem 2, the authors also improve the dependence on $kappa$ to $\sqrt{\kappa}$ without affecting efficiency. The enhancement seems significant, especially since the parameter can be exponentially small. 
- The lower bound established in the paper is the first to demonstrate the optimality of the authors' algorithms in the $d$-$K$ dependence. Per my understanding, it also confirms the results' optimality of Hwang and Oh [2023].

Weaknesses:
- The primary high-level techniques and tools (seem to) come from existing works and relevant fields, such as MNL contextual bandits. The authors should put more effort into highlighting the technical challenges and novelties besides the previous comparisons. 
- It would be beneficial to include experiments on synthetic and real-world datasets and compare the results to existing baselines and relevant works. In particular, the new algorithms seem more involved than prior ones, which may affect their stability and adaptiveness.
- There is still a significant gap between the lower and upper bounds. Besides, I wonder how often $\kappa$ could be exponentially small in practical settings, though it's definitely of theoretical interest to approach the lower limits on parameter dependency.

Limitations:
The authors have made various comparisons and discussed the limitations of the results, which I'm satisfied with. I do not see any potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
z1GwaNoGnr;"REVIEW 
Summary:
This paper introduces XMask3D, a framework developed for open vocabulary 3D semantic segmentation. They propose the integration of the denoising UNet, derived from a pre-trained diffusion model, to generate geometry-aware segmentation masks conditioned on learnable implicit 3D embeddings. These binary 2D masks are used to filter mask-level embeddings of 3D representations and apply mask regularization, thereby improving the open vocabulary capacity of 3D features.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	The motivation is clear.
2.	The proposed method is intuitive, and the experiments have validated their contributions.

Weaknesses:
1.	The organization should be improved. Section 3.1 provides an overview, while section 3.2 includes design insights and preliminary findings. The flow of these writings has puzzled me, making it difficult to grasp your key contribution.

Limitations:
1.	The authors have addressed the limitations and potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a precise and consistent mask-level alignment between 3D features and the 2D-text embedding space through a method called cross-modal mask reasoning. The proposed XMask3D model includes a 3D branch for capturing geometric features, a 2D branch for generating vision-language aligned masks, and a fusion block to combine 3D with 2D. Using a pre-trained text-to-image diffusion model as the 2D mask generator, the model leverages three techniques: 3D-to-2D mask generation, 2D-to-3D mask regularization, and 3D-2D mask feature fusion.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1- The idea is novel, the author propose to merge 2D which provides high OV capabilities, with 3D features shich endoces 3D geometry. 

2- The method performs remarkably better than the reported models, namely OpenScene. The experiments are also well structure

Weaknesses:
1- The authors don't compare with state-of-the-art 3D semantic segmentation OV3D[1]

2- The authors highlighed fututre work in the limitation, it would be good if you can expand it with some limitation on the technical side or some failure cases.

[1] Jiang, Li, Shaoshuai Shi, and Bernt Schiele. ""Open-Vocabulary 3D Semantic Segmentation with Foundation Models."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

Limitations:
Needs to be expanded

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the limitations of current open vocabulary 3D semantic segmentation methods, which primarily focus on creating a unified feature space for 3D, 2D, and textual modalities but struggle with fine-grained segmentation boundaries. To overcome these limitations, the authors propose XMask3D, a cross-modal mask reasoning framework that achieves more precise mask-level alignment between 3D features and the 2D-text embedding space.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The part ""incorporating a 2D mask generator to create geometry-aware open masks and apply fine-grained mask-level regularization on 3D features"" seems reasonable and novel.
2. The paper is well-structured and easy to follow.
3. Analysis is thorough and insightful.

Weaknesses:
1. The paper evaluates the proposed method on a limited set of benchmarks (ScanNet20, ScanNet200, S3DIS), all of which are indoor scene datasets. Authors could discuss how the method might perform on outdoor datasets. Additionally, the authors could provide a qualitative analysis of the model's potential limitations when applied to different environments.
2. The reliance on the denoising UNet from a pre-trained diffusion model could be seen as a potential weakness or limitation, especially given the computational resources required for training and inference.

Limitations:
The authors have addressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of open-vocabulary 3D semantic segmentation by utilizing 3D geometric features, 2D semantic embeddings, and text modality. The proposed approach adapts the ODISE method to the 3D domain, aiming to distill open-vocabulary semantic segmentation knowledge from a pre-trained text-to-image denoising diffusion model to a 3D segmentation model. Initially, an input point cloud is fed into a 3D encoder-decoder segmentation network, producing point-wise geometric features. Simultaneously, a pre-trained visual-language diffusion model generates 2D masks and embeddings from posed images of the same scene, conditioned on the 3D global feature of the 3D branch’s encoder. Unlike the ODISE method, an implicit $3D$ captioner is introduced to produce geometry-aware 2D masks while also distilling information from the 2D branch network to the 3D encoder. To further regularize the 3D network, a distillation loss ($\mathcal{L}_{mask}$) is applied to the 3D mask embeddings, derived from the per-point features and the 2D masks back-projected to the point cloud as 3D binary masks. By obtaining ground truth mask features from a pre-trained CLIP model, the 3D masked embeddings are aligned with the image-text joint embedding space, through a cosine similarity loss. This alignment leads to more coherent segmentation results and enhances the model's open-vocabulary capabilities. Finally, the per-point features are combined with the pseudo mask 2D features (formed by the back-projected 3D mask and 2D mask embeddings), resulting in a fused per-point representation that incorporates the geometric information from the 3D segmentation network and the semantic open-vocabulary capabilities of the 2D branch. The approach is evaluated on three semantic segmentation benchmarks (ScanNet, ScanNet200, and S3DIS) and demonstrates superior performance compared to competing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The XMask3D effectively aligns 3D geometric features with 2D and textual modailities through knowledge distillation from visual-text joint embedding spaces inherent in the pre-trained 2D denoising UNet and the CLIP model. As evident by the ablation, the implicit 3D captioner is a crucial step in the overall pipeline, and it outperforms vanilla text conditioning or the implicit 2D captioner of ODISE, in both base and novel semantic categories. Moreover, the 2D-to-3D mask regularization is also essential, since it significantly improves the accuracy of the proposed method esp. in novel categories. This justifies the need for this additional distillation step from the CLIP joint space, to further enhance the open-vocabulary capabilities of the XMask3D method. Finally, the discussion on modality fusion, both in the main paper and supplementary, is highly appreciated. By dissecting the method and providing qualitative and quantitative results for each step, the authors make it easier for readers to understand and gain intuition about the presented approach.

Weaknesses:
While the method exhibits superior performance w.r.t. competing methods, it seems that the output fused embeddings yields to geometric inconsistent features for semantic classes that cover large areas of the point cloud such as wall, ceiling and floor. This is evident in both partitioning settings when the class is either base or novel (Table 5 (a) and (b) in supp.).

Limitations:
Yes, the authors have discussed the method's limitations in detail in Section 4.4.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
z0I2SbjN0R;"REVIEW 
Summary:
This paper introduces diffusion methods to tackle the partially observed PDEs, named DiffusionPDE. By learning the joint distribution of solution and coefficient space, the proposed model can handle both forward and inverse problems. The authors experiment with diverse PDEs and settings to demonstrate the model's effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
-	This paper successfully utilizes the diffusion methods in solving PDEs, covering both forward and inverse problems.

-	The main text and supplementary materials provide diverse experiment settings, which can well support the model’s effectiveness on partial observations.

-	This paper is overall clear and well-written.

Weaknesses:
1.	The technical contribution is limited.

From a technical view, this paper is an application of the diffusion model in PDE solving. There are also some previous methods that also use diffusion methods and leverage the PDE loss [1]. Thus, I think the technical novelty is limited.

[1] A Physics-informed Diffusion Model for High-fidelity Flow Field Reconstruction, JCP 2023

2. Some powerful baselines are missing.

- According to Figure 1, I think the base model of DiffusionPDE is U-Net. How about comparing it with a single U-Net? I think U-Net could be a powerful baseline.

- There are also some latest models that are good at processing partially observed or irregularly placed PDEs, such as OFormer [1] and Transolver [2]. They should include them as baselines.

[1] Transformer for Partial Differential Equations' Operator Learning, TMLR 2023

[2] Transolver: A Fast Transformer Solver for PDEs on General Geometries, ICML 2024

3. Model efficiency comparisons are needed, including GPU memory and running time.

4. I think the proposed model cannot predict the future evolution of a time-dependent PDE. Current tasks are all about “reconstruction” or “imputation”.

Limitations:
I appreciate that they have discussed the limitations. But I think the mentioned issues about efficiency and limitations on temporal modeling are not trivial. More discussions are expected.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes to solve PDEs given only sparse measurements by jointly modeling the solution and coefficient space (e.g. the initial conditions) using a diffusion model. By applying diffusion posterior sampling (DPS) the authors obtain samples that are consistent with the sparse measurements and the underlying PDE equations. Several experiments show superior performance of the method compared to standard baselines such as PINNs and FNOs.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Solving PDEs under partial observation is an important problem in real-world applications
- The proposed method is technically sound and improves upon existing baseline methods (PINN, FNO) that do not work well for sparse measurements
- Leveraging a pretrained diffusion model as a generative prior to model the joint distribution of solution and coefficient space is a good idea
- The presentation of the method is clear and supported by concise algorithms and equations. The paper is well written overall
- Experiments consider standard baseline methods for PDEs and cover a sufficient range of different dynamics

----

Post-rebuttal: the authors have addressed quite a few of the initial concerns, and while some concerns (e.g. about the magnitude of the contributions remain), I'd be happy to support an accept. I've raised my score accordingly.

Weaknesses:
- The main weakness of the method is the limited novelty. Both sparse measurements and physics-based losses have been considered together with diffusion models, see e.g. Shu et al. (2023). So it seems to me that the main technical novelty is to apply diffusion models to model the joint distribution of two simulation states at different points in time and apply DPS during inference for consistency with the sparse measurements and PDE constraints.     
- The experiments do not take into account any stochasticity or uncertainty. In principle, DPS will give a distribution of solutions, which is not the case for the other baseline methods, but this is not explored further in the paper. 
- Since the joint distribution models two states at time 0 and time T (for all experiments except Burgers' equation) and $0 \ll T$, the authors need to simplify the PDE loss $\mathcal{L}_{pde}$ to drop any time derivatives. This is a serious limitation.   
- It is not clear if DPS works better than classifier-free guidance, as used e.g. in Shu et al. (2023), or other methods for solving inverse problems with diffusion models. 
- DPS requires a lot of compute during inference for calculating $\mathcal{L}_{pde}$. For a fair comparison, it would be important to show the number of parameters, training time and inference time for all methods.

Limitations:
The authors have mentioned slow sampling speed as a limitation in the conclusion, but I think an extended discussion of this would be important to include.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work uses a guided diffusion process to solve the PDE forward and inverse problems with partial observations. Instead of learning the parameter-to-solution map ($a\rightarrow u$) as in Neural Operators, the method learns the diffusion process on the joint distribution $(a,u)$, and use guided diffusion for inference under sparse observations. Compared with several baseline method, the proposed method shows improved performance for solving forward and inverse problem with sparse observations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The work uses a guided diffusion process to solve the PDE forward and inverse problems with partial observations.The authors compare with several baseline methods. The idea is clearly presented, and might be useful for the community.

Weaknesses:
The paper presents an interesting approach to solving PDE forward or inverse problems with sparse observations, which is an appealing concept given the minimal data requirement. However, this approach raises some concerns about the well-posedness of the problem. For example, in forward problems where sparse observations of the parameter $a(x_i)$ are available, there are infinitely many ways to interpolate $a$ and solve the PDE to obtain $u$. They are all valid solutions that satisfy both the PDE and the observations. This suggests that the method's ability to achieve good recovery might heavily rely on the strong regularization imposed by the training dataset, potentially limiting its practical utility as it may only favor solutions resembling those in the training set.

Additionally, in Appendix C, Table 2, the weightings for observation and PDE loss are significantly higher (by two to six orders of magnitude) than those for $\nabla_x \log(p(x))$ as described in Equation 8, which might indicate a predominance of data fitting over the diffusion process. It would be beneficial if the authors could provide more guidance on how these weights were chosen and discuss the implications of using smaller weights. Understanding the rationale behind these choices could help clarify the model's dependency on these parameters and their impact on the solution's behavior.

Limitations:
The author mention several limitation in the conclusion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper uses score based generative diffusion models to find the forward and backwards solution of a set of PDEs given partial observations of the solution and/or incomplete knowledge of the coefficients. The method performs well, and outperforms other ML  methods such as FNO, as well as 'standard' FE type methods, for a range of standard test problems. The method reconstructs This is a novel approach, which delivers good performance, with low errors  at a competitive speed. Extensive tests are given, with careful analysis of the results.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The use of score based generative methods in this context, where both the solution and the parameter estimates are updated, is novel. The method is clearly effective for the problems considered and should have good applications to real world examples. Extensive tests on a series of standard test problems show that the errors of the method are much lower than other ML based methods such as FNO.

Weaknesses:
This paper suffers as do many similar papers from a limited range of examples. It concentrates on the usual examples of PDEs such as NS and Bergers, and in both cases of these it looks at problems with quite moderate viscosity, which are realtively east to solve. This is more or less inevitable for such a short paper as this, especially as comparisons are needed with other method. But I would have liked to have seen more novel examples than the usual ones. This is not really a criticism of this paper, but is something to consider for future work. It would be imporoved by a fairer comparison with other methods which work with incomplete data and measurements. A clear exanple of this being the data assimilation widely used in physical modelling for just this range of problems. These should be descibed somewhere in the introduction and in Section 2. (Although of course these latter methods are slow in comparison.) The method is also limited (see later) to looking at certain slices of the solution.

Limitations:
The model as described only looks at slices of solutions of 2D problems. This has been clearly identified by the authors. In this sense it is vrather limited when compared to other ML based approaches, and of course traditional FE based methods. I am pleased that this is recognised and that the authors plan to address this. The DiffusionPDE method will only be truly competitive when this is done, but this paper is a good step in this direction.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
yzviAnpvU6;"REVIEW 
Summary:
From my understanding, this paper give a zero-th order algorithm with application to popular vision tasks neural architecture search and black-box adversarial attacks. The authors derive a closed-form solution after modeling the gradient estimation as a quadratically constrained linear program problem. The key idea is to try to decouple the required sample size from the variable dimension without extra conditions required, making it able to leverage the queries in the prior iterations. The speedup is further technically achieved by directly indexing some of the intermediate variables that contribute to the gradient estimation. The theoretical studies are given for its convergence speed and its cost-effectiveness is verified on benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Clear motivation with clearly derived approach, and it is a new zero-th order algorithm indeed and the authors also contextualize well the proposed method with related work discussion.
2. Strong empirical performance on representative vision tasks with rich testbeds and settings.
3.  The approach by its design, could enjoy the efficiency of smoothing techniques while maintaining estimation accuracy. Table 4 in the appendix is informative.
4.  The paper gives comprehensive results and technical details in both main paper and appendix.

Weaknesses:
1. As remarked by the authors, it has few constraints on the sample size, similar to the smoothing techniques; and it requires the estimation of the gradients which involves solving a linear program problem.
2. As a zero-th order algorithm, it may still not be suited for large-scale application e.g. network training.

Limitations:
No.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ReLIZO, a novel zeroth-order optimization method leveraging linear interpolation to estimate gradients efficiently. It reduces the complexity of gradient estimation by reusing prior queries without additional conditions on sample size, decoupling it from variable dimension constraints. ReLIZO models gradient estimation as a quadratically constrained linear program, solving it analytically to reduce computation complexity. Experimental results demonstrate ReLIZO's efficacy in various scenarios, including black-box adversarial attacks and neural architecture search, showcasing faster convergence and better solutions compared to existing methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The paper is well-written, with clear and easy-to-follow explanations.
* The paper introduces a method for estimating gradients using arbitrarily sampled vectors without requiring orthogonal conditions or adherence to a specific distribution, enabling the reuse of queries to accelerate the zeroth-order (ZO) optimization process.
* Extensive experiments on simulation benchmarks and real-world applications validate the method’s performance.
* The paper highlights that ReLIZO can be viewed as a generalized version of traditional linear interpolation methods, capable of handling both equal and smaller sample sizes compared to variable dimensions. This demonstrates ReLIZO's theoretical soundness and enhanced flexibility in gradient estimation.

Weaknesses:
* The effectiveness of reusing queries depends on the choice of the reusable distance bound, which might require fine-tuning for different applications, adding complexity to its implementation.
* While the method reduces the number of function queries, the process of solving the quadratically constrained linear program might introduce additional computational overhead for large $n$.

Limitations:
The authors have adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study introduces a novel gradient estimation algorithm that operates solely on forward function evaluations. The method employs a Quadratically Constrained Linear Program (QCLP) to determine the optimal linear approximation of sample vectors. The authors present performance enhancement strategies, including sample reuse and efficient inverse matrix computation within the QCLP framework. Empirical evaluations conducted on black-box adversarial attacks and neural architecture search demonstrate the proposed algorithm's superiority over existing zeroth-order methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method is natural. Approximating the gradient using linear combinations of samples and formulating as the QCLP is an intuitive idea, and the auxiliary techniques employed in this study are both judicious and pertinent to the research objectives.

2. The paper is well-written and easy to follow.

Weaknesses:
1. Zeroth-order gradient estimation has a relatively limited impact. While the proposed zeroth-order gradient estimation method demonstrates superiority over existing algorithms in its class, its overall impact on solving underlying optimization problems may be constrained. This limitation is exemplified in the NAS evaluation, where ReLIZO does not consistently achieve optimal performance.

2. According to my interpretation, in ReLIZO algorithm, obtaining new samples from the input space in each iteration is random and arbitrary. I feel there might be more effective strategies to sample new vectors based on known information. Could the authors comment on this?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yySpldUsU2;"REVIEW 
Summary:
It is known that usually deep neural networks will learn “easy examples"" that contain fast-learnable features first while learning more complex examples in a second time. The authors argue that mitigating such simplicity bias is the reason method like SAM are outperforming SGD. Based on such analysis, the authors introduce their methods coined as USEFUL that consists in two setups: 1) Identifying the examples with fast-learnable features using a clustering method based on layer output similarity 2) Upsampling by a constant factor the remaining examples with slow-learning features. By doing so, the authors can significantly increase model performances and training time on different classification tasks using different optimizers. They assess their methods across a wide range of dataset and different hyper-parameters and outperform random clustering baseline.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well motivated and written. The method seems to be sounded and I really appreciate that the authors assess their method using different hyper-parameters such as optimizer, batch size, datasets, upsampling factor, architectures, and data augmentation. It is also great that they ran a baseline with random clustering.

Weaknesses:
It is not clear when and why one should choose the last output activation vector to define the clustering instead of intermediate activation vector. It is also not clear at which epoch one should decide to do the clustering since for a dataset like CIFAR10 the optimal performances are achieved at epoch 8 while for CIFAR100 it is epoch 20. So, finding the correct hyper-parameters for the clustering might be costly and thus impact how fast convergence can really be (if we consider this needed additional ablation on clustering epoch). In addition, the authors mention that they are using an upscaling factor of 2, but I am wondering how robust this is when using long-tail distribution. For example, I am not sure that on something like ImageNet-LT or Inaturalist, we will get the best performances by using a constant factor.  I would also be a bit more cautious about some of the claims made in the papers. For example, the authors claim that their method is generalizing to OOD tasks while providing experiments on only the WaterBird dataset.  So, it would be better to write about promising preliminary results than claiming generalization on OOD.

Limitations:
The authors did not really discuss any limitations (outside the fact that their theoretical result does not extend to CNN) or societal impact. I think that one limitation that could have been highlighted is the smaller scale of the experiment and the focus on classification tasks. Another limitation is the lack of results on OOD or long-tail benchmarks which would seem to be well suited for this type of work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work aims to modify the training data distribution to improve in-distribution generalization. First, the authors theoretically analyse a 2-layer CNN and compare the feature learning dynamics (fast learnable and slow-learnable features) of Gradient Descent (GD) and Sharpness-Aware Minimization (SAM). It is then shown that SAM mitigates simplicity bias compared to GD. The authors then propose USEFUL (UpSample Early For Uniform Learning), a method that upsamples the examples in the training set that contains slow-learnable features.  USEFUL first clusters the examples with similar outputs early in the training and then upsamples the slow-learnable clusters. The main idea behind USEFUL is to learn features at a uniform speed (similar to SAM) by changing the training data distribution. USEFUL can be trained with SGD, SAM and SAM + Trivial Augment. Results on CIFAR-10, CIFAR-100, STL10, TinyImageNet indicate that USEFUL is across datasets and architectures. Additonal ablation and analysis show that USEFUL learns similar properties to SAM (for e.g less sharp solutions).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Originality: The question posed by the authors “Can we change the training data distribution such that the model trained on it has similar properties to SAM?” is interesting and novel. The proposed method is also well-motivated.
2. Results: The authors perform a comprehensive set of ablations and analysis on the proposed method USEFUL. Section 5.4 that shows that USEFUL’s solution has similar properties to SAM, which answers the question raised in the motivation of the paper. I also particularly like the ablations with upweighting loss and data selection method in Appendix D.6.
3. Overall, the paper is fairly well written. One minor point to address here is that the paper covers multiple concepts like SAM, simplicity bias, flat minima and uniform feature learning. It would be good to explain the relationship between these more clearly.

Weaknesses:
1. The authors explicitly mention that their focus in this paper is only on “in-distribution generalization”. I am a bit confused by this given the motivation of simplicity bias and learning features uniformly. To elaborate more on this point,
    - Springer et al [1] also show that SAM implicitly balances the quality of diverse features (similar to the observations made in Section 3 of this paper. The experimental results in [1] is focused more on datasets with multiple predictive features like CelebA, CIFAR-MNIST. 
    - Past work on simplicity bias and shortcut learning [2, 3, 4, 5] has focused on similar datasets like CelebA, Waterbirds, CIFAR-MNIST, Colored-MNIST to name a few.
    - While the authors have shown encouraging results on Waterbirds dataset in Appendix D5, it would be good to show the complete results on various groups and on other datasets as well.
2. Connection to [1]. Springer et al [1] made a very similar observation as to Section 3 in this paper. It would be great if the authors can clarify the differences with the observations in [1] and this work. Particularly, [1] also shows that SAM mitigates simplicity bias and that  SAM learns higher quality representations of hard-to-learn features. The authors briefly discuss this in Related Works section but a more detailed answer would be helpful.
3. I just wanted to understand the practical usefulness of the proposed method. This method has one additional hyperparameter i.e the separating epoch. The authors have reported the best separating epoch for all the datasets which is epoch 8 for CIFAR-10 and epoch 20 for CIFAR-100 (Appendix C.2). How is this hyperparameter chosen? Is there a separating epoch number that works across various datasets? This is especially relevant given that that the average gain on most of the datasets with USEFUL is less than 1% with additional cost for training. 
    
 [1] Springer, Jacob Mitchell, Vaishnavh Nagarajan, and Aditi Raghunathan. ""Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning."" The Twelfth International Conference on Learning Representations.

[2] Shah, Harshay, et al. ""The pitfalls of simplicity bias in neural networks."" Advances in Neural Information Processing Systems 33 (2020): 9573-9585.
    
 [3] Geirhos, Robert, et al. ""Shortcut learning in deep neural networks."" Nature Machine Intelligence 2.11 (2020): 665-673.
    
 [4] Kirichenko, Polina, Pavel Izmailov, and Andrew Gordon Wilson. ""Last layer re-training is sufficient for robustness to spurious correlations."" arXiv preprint arXiv:2204.02937 (2022).
    
 [5] Teney, Damien, et al. ""Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.

Limitations:
Yes, the authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
- Proves for a 2-layer CNN with fixed second layer weighsts trained on a toy dataset,  SAM learns slow-learnable and fast-learnable features more uniformly in the early epochs compared to SGD
- Based on this analysis, proposes a simple clustering-based upsampling strategy for reducing simplicity bias / excessive reliance on fast-learnable features. The results show that this improves in-distribution generalization of standard small-scale image classification tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Simple easy-to-implement method that uses SAM and upsampling to improve in-distribution generalization
- The method is well justified with theoretical analysis comparing SAM and SGD on a toy data distribution. This analysis indicates that SAM is less sensitive to simplicity bias.

Weaknesses:
- No baselines. There are several papers now that try to reduce simplicity bias in order to improve performance:
    - https://arxiv.org/abs/2105.05612
    - https://arxiv.org/abs/2301.13293
    - https://arxiv.org/abs/2107.09044 (does not focus on simplicity bias explicitly, but similar to method proposed in paper)
    - simpler baselines: there are several papers that propose “example difficulty” metrics (https://arxiv.org/abs/2106.09647). How well do this correlate with the clusters found in your method? If you just train on the k examples with the highest difficulty scores (per class), does this fare worse than the proposed method?
- Limited novelty due to findings in [64] (Sharpness-aware minimization enhances feature quality via balanced learning). This paper also shows that SAM improves feature diversity (on real datasets + backed up with analysis on a toy dataset) and improves performance on transfer-learning tasks.
- Lacking discussion about when this method would fail. I can imagine two scenarios where the method would not work:
    1. Most training examples have one or more slow-learnable features. In this case, the clustering approach would “remove” most of the points in the dataset, and train on very points for multiple epochs. This could result in overfitting and performance that is worse than training. There’s an implicit assumption that there is some sort of one-to-one relation between examples and features. In the case where all examples contain an “easy” (e.g. patch) and a “hard” feature (e.g. CIFAR), would this method improve performance over SGD? 
    2. In noisy datasets, low-quality examples or mislabeled examples would require more time to learn, and this method would cluster them and train on them for longer. That is, it would group examples that are “high-quality” and hard-to-learn with “low-quality” points. In this case, would the proposed method improve performance over SGD? 
- “SB of SGD has been long conjectured to be the reason for the superior generalization performance of overparameterized models, by providing capacity control or implicit regularization” This incorrectly cites https://arxiv.org/abs/2006.07710v2, which shows that too much simplicity bias can lead to robustness and in-distribution generalization issues.
- Unfair evaluation. The experiments compare SAM+TA augmentation and SAM+USEFUL+TA to SGD (no TA). I think there should be two plots, complaring {SGD, SAM, SAM+USEFUL} w/ and w/o TA.
- Experiments on larger datasets. The image classification used here are fairly small-scale. I would like to how well this method scales to ImageNet-scale datasets (TinyImageNet is not a good proxy..)
- Writing is repetitive at times, especially the theory section (3.3)

Limitations:
Please see strengths and weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes an algorithm for changing the distribution of training data to improve the generalization of the model on origin data distribution. The paper is inspired by Sharpness Aware Minimization, which aims at finding a flat minimum meaning that it has a good generalization capability. This paper divides features into two categories: fast-learnable features and slow-learnable features and derives some observations like ""SGD and SAM only learn fast-learnable or easy features early in training"" and ""SAM learns slow-learnable and fast-learnable features at a more uniform speed"". The authors propose the method dubbed as USEFUL to train the model on some slow-learnable features repeatedly. The experiments show the effectiveness of USEFUL on CIFAR10 and CIFAR100 datasets.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The paper has a theoretical analysis to analyze the learning progress and derive the proposed method.
- The experiments are abundant and comprehensive.

Weaknesses:
There are some questions based on the presentation of this paper, I will not hesitate to improve my score if the following question are solved.
- Difference between this paper and methods for long-tailed data distribution or measuring the difficulty of learning examples. Algorithms for long-tailed data distribution are usually based on resampling training data or reweighing loss value. The proposed USEFUL is similar to the resampling methods except that USEFUL focuses on the features that are hard/slow to learn. Some references for understanding: [Shi, Jiang-Xin, et al. ""How re-sampling helps for long-tail learning?."" Advances in Neural Information Processing Systems 36 (2023).](https://arxiv.org/pdf/2310.18236), [Shrivastava, Abhinav, Abhinav Gupta, and Ross Girshick. ""Training region-based object detectors with online hard example mining."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.](https://arxiv.org/pdf/1604.03540v1) and some references based on it, [A Re-Balancing Strategy for Class-Imbalanced Classification Based on Instance Difficulty](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_A_Re-Balancing_Strategy_for_Class-Imbalanced_Classification_Based_on_Instance_Difficulty_CVPR_2022_paper.pdf), [Active Teacher for Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Mi_Active_Teacher_for_Semi-Supervised_Object_Detection_CVPR_2022_paper.pdf), I believe a discussion of these references in paper should be helpful.
- The relation between the proposed USEFUL and SAM? It seems like the motivation of USEFUL is changing the data distribution to get a flat minimum like SAM. But the results in Appendix D.2, *i.e.*, 53.8 for SGD 41.8 for SGD+USEFUL 12.4 for SAM in Table 1($\lambda_{max})$, do not show effectiveness compared with SAM. It could show the effectiveness on SGD but it's far from being comparable to SAM. 
Some small questions:
- What's the exact formulation of the Data distribution?
- What's the ""patch"" meaning in Definition 3.1? Is that the same as the patch in ViT or the channel of the image? It's a little confusing.
- The experiments mainly focus on traditional architecture, e.g., n-Layer CNN, ResNet. More experiments on popular models and big datasets, e.g., Transformer ImageNet-1k, would be better.

Limitations:
N.A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yxjWAJzUyV;"REVIEW 
Summary:
This paper reduces the complex policy optimization procedure of alignment to a simple regression objective, using the relation between optimal policy and reward. The paper conduct detailed theoretical analysis in revealing the relation between the proposed algorithm *REBEL* and *NPG/MD*. Comprehensive experiments in both text and image generation exhibit the effectiveness of *REBEL*.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies simplified version of policy optimization in RLHF (compared to PPO), which is a research topic of interest.
2. The theoretical analysis of *REBEL* is detailed and insightful.
3. The presentation of this paper is logically clear and has good readability.
4. The experiments in this paper are comprehensive, and the experimental results are well presented.

Weaknesses:
1. The statement ""REBEL ... be extended to handle intransistive preferences ...."" in the abstract is not adequately presented in the main content of the paper. As the major influence brought by intransistive preferences is the degradation of reward score accuracy, which is not addressed by this paper.
2. I would suggest the authors to summarize the limitations of the proposed method in a separate ""Limitations"" section.

Limitations:
none

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes the REBEL algorithm that reduces policy optimization to iteratively solving squared loss regression problems on the difference in rewards between trajectories, based on DPO's analysis. The paper transforms the resulting equation for r(x, y) presented in DPO to a regression loss function, and avoids the intractable calculation of Z(x) by calculating the loss based on a pair of samples from the same input prompt x, i.e., (x, y) and (x, y'). One of the goals for REBEL is to serve as a simple and lightweight RL algorithm that eliminates the need for complex components like value functions and clipping heuristics used in PPO. The authors provide a theoretical analysis showing that Natural Policy Gradient can be seen as a special case of REBEL under some assumptions. The authors conduct two kinds of empirical analysis including language modeling and image generation tasks to demonstrate the performance of REBEL.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Originality:
    - This paper presents a new angle by transforming the analysis of the reward function presented in the DPO paper into a reward regression loss, leading to the proposed REBEL algorithm. 
    - The authors make connections between REBEL and existing RL methods like NPG considering some assumptions, showing that these algorithms can be seen as special cases or approximations of REBEL under certain conditions. 

- Quality:
    - The paper provides a thorough theoretical analysis comparing REBEL with existing RL approaches. 

- Clarity: 
    - The paper is well-written and easy to understand, with a clear logical flow from motivation to theoretical analysis to empirical validation. The authors do an good job of explaining the intuition behind REBEL and highlighting its connections to prior work.

- Significance:
    - The paper tackles the important problem of developing simpler and more efficient RL algorithms that can scale to large-scale generative model fine-tuning.

Weaknesses:
1. Insufficient experimental validation and limited baseline comparisons:
- While the paper presents empirical results on language modeling and image generation tasks, the experimental validation of REBEL could be more comprehensive. The authors should consider including a wider range of benchmarks and datasets to demonstrate the generality and robustness of their approach.
- The comparison with baseline algorithms like PPO and DPO is somewhat limited. The authors should provide more details on the hyperparameter settings and training procedures for the baselines to ensure a fair comparison. Moreover, the poor performance of DPO compared to PPO in the experiments raises questions about the implementation or hyperparameter choices.
- The authors claim that REBEL matches the strongest known theoretical guarantees in terms of convergence and sample complexity. However, the experiments only compare performance at a specific epoch without demonstrating improved sample efficiency. Convergence plots showing the performance of REBEL and baselines over the course of training would provide a clearer picture of the sample efficiency and convergence properties.

2. Lack of support for certain claims and limited exploration of key aspects:
- The paper makes several claims regarding the advantages of REBEL, such as its ability to handle intransitive preferences, incorporate offline datasets, and apply to deterministic MDPs. However, there is a lack of corresponding experimental evidence or theoretical analysis to substantiate these claims.
- The relationship between the regressor's performance and the quality of the dataset used for training is not explored in depth. Insights or experiments that investigate how dataset quality and diversity affect the regressor's ability to capture an improved policy would strengthen the paper.
- The choice of base distribution \mu is mentioned as a determining factor for whether REBEL is hybrid or fully online. However, the paper does not provide experimental results comparing different forms of \mu across various tasks or practical guidelines for choosing \mu in real-world applications.

3. Inconsistencies and potential conflicts with previous statements:
- The authors mention that critic-based variance reduction might be necessary for high-variance trajectory-level rewards in stochastic MDPs, which seems to contradict the criticism of PPO's complexity in the introductory section. The lack of experimental support for REBEL's performance in stochastic MDPs is a significant limitation, and the authors should provide preliminary results or theoretical insights to support their claims.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents REBEL, a minimalist reinforcement learning algorithm that does policy optimization by solving a sequence of regression problems using relative rewards as targets. Theoretical analysis shows that Natural Policy Gradient (NPG) is a variant of REBEL, and thus theoretical guarantees for NPG can be applied to REBEL.  Experimental results  show that REBEL matches or outperforms existing baselines, most notably PPO and RLOO, on multiple tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-organized and technically sound. The general flow of the paper is smooth and proposed methods are explained adequately. The paper has an appropriate number of citations and properly details existing work in the related work section. 
- The method is simple to implement and has little engineering overhead. Given the minimalist implementation, the results are impressive, surpassing even PPO, which typically requires significant engineering.

Weaknesses:
- There are no significant weaknesses in this work, barring some clarifying details. 
- I believe that at least a brief section on related work should be included in the main paper, the in-depth one can be deferred to the appendix. In terms of space, I personally do not think Section 2.2 adds much value to the main paper.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present REBEL, a method for solving contextual bandit problems (such as the alignment of language models) via regressing relative rewards. They first derive their objective by demonstrating that the use of paired responses means that you can get rid of the partition function, which is impossible to estimate. 

They then connect their method to previous methods in RL including detailing, but not . They demonstrate that under strong assumptions REBEL is equivalent to mirror descent, and that under assumptions of coverage by the reference policy, that REBEL produces returns close to an optimal policy. 

Finally the authors run experiments on summarisation, general chat and image alignment, demonstrating their method compares favourably to other methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The idea of using relative rewards to remove the partition function is a nice and simple idea
* The theoretical connections of their method to prior methods grounds their work nicely in existing RL approaches. 
* The empirical results seem to demonstrate their method is competitive or better than other approaches. 
* REBEL compares favourably in terms of runtime and memory usage with other, similarly performing methods. 

Overall the theoretical and empirical examinations of their method seems very thorough.

Weaknesses:
See questions

Limitations:
The authors discuss the limitations throughout their work at relevant stages.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yxOrSmS5wR;"REVIEW 
Summary:
The paper proposes AV-Cloud, a framework for high-quality spatial audio rendering in 3D scenes without relying on visual cues. AV-Cloud addresses issues in current audio-visual rendering methods, such as audio lag and dependence on visual rendering quality, by introducing Audio-Visual Anchors and the Audio-Visual Cloud Splatting module. These components facilitate the generation of viewpoint-specific spatial audio synchronized with visual content. The method demonstrates superior performance on multiple benchmarks, outperforming existing baselines in audio reconstruction accuracy, perceptual quality, and acoustic effects.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The concept of using Audio-Visual Anchors and Cloud Splatting to decouple audio rendering from visual rendering is interesting.
2. The paper demonstrates comprehensive experimentation and robust evaluation across multiple benchmarks.
3. The paper is well-structured and the presentation of the framework is clear. The figures and supplement examples help the readers better understand.
4. The proposed method addresses critical issues in real-time audio-visual rendering.

Weaknesses:
1. The mathematical formulation of the Audio-Visual Cloud Splatting module could be more detailed. For instance, Equation (2) introduces the softmax function applied to the relative vectors and visual features, but the reason behind this specific formulation and its implications are not sufficiently explained. Clarifying how the weights $a_{ki}$ are computed and how they influence the final output would enhance understanding.
2. The technical derivation of the Spatial Audio Render Head (SARH) lacks depth. Specifically, the process described in Equations (4) and (5), where the mixture mask $m_m$ and the difference mask $m_d$ are used to compute the left and right channel outputs, is not fully elaborated. The significance of these masks and their impact on the final audio quality are not clearly discussed. Additionally, the role and impact of the convolution modules within the residual structure (Figure 3) are not sufficiently explained.
3. While the method shows strong performance on benchmarks and some real-world examples, the provided examples are too idealized and lack challenging elements like interfering sound (e.g., crowd noise). I think the robustness of AV-Cloud in more complex and noisy real-world environments should also be validated.

Limitations:
The authors mention the limitations of their approach's challenges and potential drawbacks. The reliance on camera calibration and the potential issues with noise in real-world audio recordings are noted. Additional imitations can be found in the Weaknesses section

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
A novel approach for rendering high-quality spatial audio in 3D scenes, called AV-Cloud, is proposed. This method synchronizes with the visual stream without relying on or being explicitly conditioned by visual rendering, enabling immersive virtual tourism through real-time dynamic navigation of both audio and visual content. Unlike current audio-visual rendering methods that depend on visual cues and may suffer from visual artifacts causing audio inconsistencies, AV-Cloud overcomes these issues. It uses a set of sparse AV anchor points, forming an Audio-Visual Cloud derived from camera calibration, to represent the audio-visual scene. The Audio-Visual Cloud allows for the generation of spatial audio for any listener location. A novel module, Audio-Visual Cloud Splatting, decodes these AV anchor points into a spatial audio transfer function for the listener’s viewpoint, which is then applied by the Spatial Audio Render Head module to transform monaural input into viewpoint-specific spatial audio. This approach eliminates the need for pre-rendered images and efficiently aligns spatial audio with any visual viewpoint. The results are satisfying.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The AV anchors strategy seems to be interesting and effective for audio-visual scene representation. The Audio-Visual Cloud Splatting is novel for AV tasks but more likely to be a Q-former.
2. The experiment results are good and ablations are clear.

Weaknesses:
As I mentioned in the strengths, the Audio-Visual Cloud Splatting seems to be a Q-former like module.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores the problem of generating 3D audiovisual scenes – that is, generating 3D scenes with spatial audio. The proposed approach, AV Cloud, uses anchor points obtained from Structure-from-Motion (SfM) points. The anchors are then used with an AV Cloud splatting module which decodes the visuals and the audio. Experiments are done on RWAVS and Replay-NVAS with comparisons done with several prior works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
– 3d audiovisual scene generation is a really interesting problem to solve. WHile there is considerable literature on visual scene generation, generating 3d visual scene is an interesting problem with real-world applications. 

– The model claims to be able to generate the audio and the visuals in parallel. Essentially unlike prior work it decouples the generation of two modalities by not using the generated visuals for generating the audio. 

– On objective metrics, the paper claims to make good improvements

---- 
increased score after rebuttal

Weaknesses:
– The paper is a bit difficult to follow – especially the key part of AudioVisual anchor points. 

– First, a short primer on SfM is desirable, even if it is in Appendix. More importantly though, it is not clear why it makes sense to use SfM points and clustering on top of them to model AV anchor points and generation of spatial points. Why does it make sense to use SfM points or anchors derived from them as the starting point for AV generation ? What relation the anchors have with audio which motivates the fact that these anchors can be used for audio generation ? 

– Second, the details of AV anchor points are fuzzy. The visuals are used for SfM points which are then clustered to get the anchors. Where is the audio into picture here ? Are these anchors visual only ? If so, why are we calling it AV Anchors ? 

– In prior works, for example AV-Nerf, there is an an explicit AV-Mapper which learns the audio visual relations through which the spatial audio generatio happens. Here Visual2Audio splatting transformer is expected to model that ? 

– For the subjective tests, it would be good to actually get proper subjective ratings on the generated spatial audio. The current preference numbers are not very informative. Getting the spatial audio rated with respect to their quality and spatial characteristics would be much more meaningful. 

– Since NAF, INRAS and other works are considered here - I think it would be good to reference NACF ([R1]) below. NACF specifically focuses on using visuals and is ideal for comparison. 

[R1] Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Cqr6E81iB7;"REVIEW 
Summary:
Various results are proved about online learning of private learning algorithms, contrasting no DP, pure DP and approximate DP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper present some interesting new results on online learning with DP. I read up to section 3 and the writing is very clear and the results are important and purportedly novel. (I don't have background or recent experience in the area of online learning so I cannot independently confirm their novelty.)

Weaknesses:
I can't point to any weaknesses, but this paper is outside of my area, and I was only able to follow up to Section 3, so it is certainly possible there is something I missed. I am basically taking the paper at its word on the claims made in Sections 1 and 2.

Limitations:
No potential negative societal impact.

Rating:
8: accept, good paper

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies online learning of concept classes under DP (differential privacy) constraint. The paper makes progress towards understanding mistake bounds (mostly) in the realizable case in a few settings. Concretely, The paper shows that:
1. If the adversary is oblivious, then PAC pure DP learnability implies online pure DP learnability.
2. On the other hand, if the adversary is adaptive, then PAC pure DP learnability does not imply online pure DP learnability.
3. In contrast with the standard online learning model, for every non-trivial hypothesis class, the mistake bound depends on $T$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Assuming that the results are correct, the paper contains a significant improvement towards understanding mistake bounds in online learning under DP constraint.
2. The questions and results are interesting and in the scope of NeurIPS.
3. The proof techniques are explained in detail.

Weaknesses:
1.It is a bit hard to digest the results and understand the remaining gaps, because there are many settings considered in the paper (DP/non-DP, oblivious/adaptive, approximate/pure...), and no figure/meta-theorem that neatly explains all the relationships. Such a figure/meta-theorem would significantly improve the presentation of the paper.

2. As a consequence of the above, it is not exactly clear how tight the results are. If you prove a lower bound (as in Section 4), I think it is better to formally state, right after it, the best known lower bound (and the dual statement for proving a lower bound).

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper demonstrates that any function class that is offline PAC learnable with pure DP is also online learnable with pure DP against an oblivious adversary. In this context, a hypothesis class is considered online learnable in the realizable setting if there exists an algorithm with a sublinear mistake bound.

The paper also establishes a distinction between online learning with pure privacy for oblivious and adaptive adversaries. Specifically, it shows that the hypothesis class $point_N$ is privately online learnable against an oblivious adversary but not against adaptive adversaries. This finding also indicates a separation between pure and approximate private online learnability, as $point_N$ is online learnable with approximate DP against an adaptive adversary.

Additionally, the paper presents a general lower bound on DP online learning against an oblivious adversary for non-complementary function classes.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper offers general results on private online learnability, identifying conditions under which a hypothesis class is online learnable with pure DP against an oblivious adversary. This connection to Representation Dimension links to existing results on DP PAC learnability.
- It also explores different layers of separation using the function class $point_N$, contributing to a deeper understanding of the cost of privacy in online learning.

Weaknesses:
The proof of Theorem 4.3 is not clear

Limitations:
No significant limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies limits of pure DP and approximate DP in the context of online learning (with oblivious and adaptive adversaries).

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
The research questions are interesting and the results may have fundamental value.
I'm not an expert in all topics covered by the paper, but the contributions seem novel to me.
The text is in general well-written and understandable.

Weaknesses:
The paper has a lot of theorems and lemmas, which is interesting.  Still, the paper has no conclusions, discussion, further work, description of limitations or illustrative experiments or extensive examples.  This puts the task of understanding the value and applicability of the work to a large extent to the reader.

Limitations:
The authors don't discuss limitations or societal impact.
I believe there are no ethical concerns with this theoretical work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yvUHnBkCzd;"REVIEW 
Summary:
This paper introduces a personalized federated learning algorithm to address the challenges of real-time predictions in non-stationary environments. Clients fine-tune models online, combining their locally fine-tuned models with multiple federated models learned over time. This approach ensures efficient adaptation to evolving data streams, with theoretical analysis and experiments on real datasets demonstrating its effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed algorithm effectively addresses the challenge of making real-time predictions in non-stationary environments by allowing clients to fine-tune models online, ensuring continuous adaptation to evolving data streams.
- By combining locally fine-tuned models with multiple federated models, the approach enhances personalization and leverages the strengths of both local and federated learning, resulting in improved performance.
- The paper provides a solid theoretical analysis alongside experimental validation on real datasets, demonstrating the practical effectiveness and robustness of the proposed algorithm in real-world scenarios.

Weaknesses:
1. Contributions are suggested to list by items for clear summaries.
2. The baselines in Table 1 are all before the 2022 year, more latest related methods published in 2023 should be compared.
3. Fed-POE has limited improvements on Air and FMNIST datasets.
4. The process of combining locally fine-tuned models with multiple federated models may introduce significant computational overhead for clients, especially those with limited resources.
5. As the number of clients increases, managing and integrating multiple personalized models can become complex, posing scalability challenges for the proposed algorithm.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel personalized federated learning algorithm, Fed-POE, which is designed for adaptive prediction and model fine-tuning in dynamic environments. It addresses the challenge of real-time predictions on streaming data by constructing a personalized model that combines a locally fine-tuned model with multiple federated models. Theoretical analysis and experiments on real datasets demonstrate its effectiveness in achieving sublinear regret bounds and improved online prediction accuracy.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper proposes a unique ensemble method that dynamically combines local and federated models, which is a novel approach in the field of federated learning.

2. It provides a solid theoretical analysis, demonstrating sublinear regret bounds for convex models.

3. The paper is well organized.

Weaknesses:
1. Although the presented method is novel, it is simply a combination of the previous personalized federated learning approaches as well as ensemble learning and provides comparably little conceptual originality.  The contribution's main novelty seems to be that integrating results from prior models would be beneficial in mitigating catastrophic forgetting in online federated learning.

2. Experimental results show that the improvement in the accuracy of Fed-POE compared to other methods is not significant, but ensemble learning inevitably increases the computational overhead increase. The paper needs to analyze whether this trade-off is reasonable.

3. The paper needs more experiments to prove the effectiveness of the method, for example, for real-time predictions, the size of the old data replay is crucial, and the authors should design experiments to analyze the effect of batch size b on the experimental results. This paper also needs experiment results on the accuracy over the time step.

Limitations:
see the weaknesses and questions

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an interesting perspective about the role of ensembles of models in federated learning. The provocative claim is the fact that federated learning is not always better than locally-trained models. This is contextualized in the field of not IID data and time-varying data generating processes. To address this issue the paper introduces from the theoretical point of view how to quantify the regret in federated and locally trained models. In addition it includes an analysis of non-convex models by managing an history of models. The overall impression about the paper is positive even if some points could have been better explored (in particular the part related to not IID data that is somehow the core of the paper).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces a theoretical evaluation of the gain produced by federated models w.r.t. locally trained models. This results show that federated learning is relevant only when models can be considered iid (hence averaging providing better results). This is somehow a know results but I appreciated the theoretical analysis
- The proposed solution is to combine with a convex mean a locally-trained models with the federated models
- This is further extended in case of non-convex models by considering an ""history"" of models to be used when needed (i.e., according to the loss)

Weaknesses:
- The federated models somehow includes the locally-trained model. I would have appreciated a further analysis about the fact that the two ""sides"" of the average model are related each other. 
- The setting in which eta and eta_c scales with T prevents adaptation in the long run (which is somehow the core of the paper). How to deal with that?
- Federated learning typically takes also into account the complexity of the learning phase (i.e., the amount of info to be transmitted, e.g., the models). This is not quantified here. And this could be also a weak point in the fed-poe algorithm.

Limitations:
See Weaknesses box

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Fed-POE, a novel personalized federated learning algorithm tailored for online prediction and model fine-tuning. Fed-POE creates an ensemble by integrating local models with those periodically contributed by the server over time. Theoretical analysis confirms that Fed-POE attains sublinear regret. Empirical results demonstrate that Fed-POE consistently surpasses the performance of both local and federated models across all evaluated datasets, which indicates that Fed-POE effectively leverages the advantages of both local and federated models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The technical content of the paper appears to be accurate, although I did not check all the details carefully.
- This paper is generally well-written and structured clearly.
- The experiments substantiate the main theoretical analysis, and the proposed algorithm demonstrates superior performance over the baseline methods

Weaknesses:
My primary concern is that the assertion the proposed algorithm can effectively harness the combined advantages of federated and local models is not clearly demonstrated within the theoretical bounds. The paper presents two principal theoretical results: Theorem 2 provides the regret upper bound for the proposed algorithm in convex scenarios, while Theorem 3 addresses non-convex cases. Both theorems establish sublinear regret bounds that are consistent with those for federated learning using a straightforward online gradient descent approach.  I recommend enhancing the clarity of the proposed method's advantages in the theorems by incorporating assumptions about the data distributions.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yppcLFeZgy;"REVIEW 
Summary:
The paper presents MutaPLM, a framework designed to interpret and navigate protein mutations using protein language models. This approach utilizes a protein delta network to capture mutation representations and employs a transfer learning pipeline with a chain-of-thought strategy to leverage knowledge from biomedical texts.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper attempts to propose a general interpretable model for protein mutations.
2. This paper compiles a mutation-text multimodal dataset, providing an excellent benchmark for future work.
3. The code is available. Although I haven't had time to run it yet, I will try to run the code during the rebuttal phase to ensure the reproducibility of the experiments.

Weaknesses:
1. PLM representations used in this study is the residue-level or protein-level embedding? If the mutation has very few residues, such as a missense mutation, will using protein-level embedding result in h∆ being too small?
2. Is it possible to provide some more practical mutation-related downstream task benchmark results? For example, predicting changes in protein properties or PPI?
3. Is it possible to compare the proposed method with the predictive results of embeddings extracted by AF, since the description information of the mutation may already be included in the structural changes predicted by AF before and after the mutation?
4. I do not deny that this is a good work, but perhaps it is more suitable for the benchmark and dataset track, because its method has limited innovation, and it has not verified its interpretability and performance on actual tasks related to protein properties.

Limitations:
This paper discusses the limitations and points out the direction for future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In the paper entitled ""MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering,"" the authors proposed multimodal protein-textual language models for understanding the effect of mutation and performing protein engineering. They also build MutaDescribe, the first large-scale protein mutation dataset with rich textual annotations.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is generally well-written and easy to follow.
2. The authors have constructed the first comprehensive protein mutation dataset enriched with textual annotations. This dataset represents a significant foundation for future research in this field.
3. The MutaPLM framework introduced in this paper is innovative, particularly in its explicit modeling of mutations and its use of cross-modal transformers for multi-modal feature integration, enhancing its analytical capability.
4. By integrating large language models, the proposed framework significantly simplifies protein engineering, offering an intuitive tool that could be readily adopted by biologists for advanced research.

Weaknesses:
1. The paper lacks a comparison with fine-tuned protein language models. Finetuned PLMs (ESM-1, ESM-2) have been validated to be powerful for various downstream tasks. For example, MLAEP(https://www.nature.com/articles/s41467-023-39199-6) and AugmentedESM(https://www.nature.com/articles/s41587-021-01146-5)
2. The paper did not prove why the textural annotation is necessary. From the ablation study, one can conclude that the labeled information from the textual annotation makes the model powerful. 
3. The paper should add more discussion and experiments on why human-understandable notation is necessary. Human-understandable notations are not more informative compared with a conventional multi-label dataset. Moreover, LLMs may fail to deal with regression tasks, while finetuned PLMs can do better.

Limitations:
The authors addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework to 1). generate text-based mutation effects for mutated proteins and 2). propose new mutated sequences based on the function descriptions. The main module is an encoder-decoder network, which encodes the representations of mutated sequences and outputs the position and amino acid of the mutation. The network is first pretrained on the protein literatures and then fine-tuned on the mutation effects.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The problem studied in this paper is novel and well-motivated: generate mutated sequences conditioning on the instructions, and generate mutation effects conditioning on the sequences.
* The method is technically sound. 
* The paper is well-structured

Weaknesses:
Most issues are on the evaluation side. Rigorous evaluations are very important for the AI4Science applications. 
* Baseline Selection: The paper employs weak baselines for comparison. None of the baselines used have been specifically trained on mutations.  This makes it difficult to accurately assess the true effectiveness of the method.
* Lack of Temporal Evaluation: While the paper adopts a structural split for evaluation, which is acceptable, a temporal-based evaluation would be more ideal and realistic. A temporal split, where some proteins are held out based on their discovery time, would more accurately reflect real-world scenarios in scientific applications. 
* Weak Evaluation of Mutation Explanations: The use of GPT-4 to assess scientific explanations is not robust or scientifically sound.
* Missing experimental details. The paper omits several crucial experimental details, which harms reproducibility and thorough understanding of the methodology. Specific areas lacking detail include:
  1. explain in details how you tune the hyperparameters
  2. what is the dataset for protein literatures?
  3. When construct MutaDescribe, did you only use swissprot or the whole dataset? how did you extract the mutation explanations? How do you know whether it's expert-reviewed?

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ypggxVWIv2;"REVIEW 
Summary:
This paper tries to evaluate the strategic reasoning abilities of LLM. Therefore, 10 games are chosen where LLMs is trying to solve the game. This paper includes various open- and closed-source LLMs into consideration and build a benchmark for easy evaluation.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
Evaluating the strategic reasoning is important and the evaluation includes various LLMs into consideration.

Weaknesses:
The evaluation protocol is questionable. More comments and questions are in the following section.

Limitations:
More limitations should be discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a benchmark for evaluating the strategic reasoning of LLMs. The benchmark includes ten games of various types. The authors use these games to conduct competitive experiments between LLMs and traditional methods, as well as LLM-vs.-LLM. The paper then analyzes the experimental results and model behavior, and examines the game-theoretic properties of LLMs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is logically clear, understandable, and well-written.
2. The experiments are comprehensive. The authors evaluate comparisons between LLMs and traditional methods and LLM-vs.-LLM competitions. They include multiple open-source and closed-source models and tests of various prompting methods.
3. The authors evaluate game-theoretic properties, including Nash equilibrium with regret and Pareto efficiency.

Weaknesses:
I didn't find any significant weaknesses, only a few questions.

Limitations:
The authors have fully addressed the limitations in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a benchmark to understand the strategic reasoning capabilities of llms. The authors present a suite of game theoretic tasks with different structures to do this. They use different evaluation metrics like ELOs and Relative advantage to compare different llms and prompting methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is clearly written and well motivated. It provides some structure to the growing literature of strategic reasoning with llms.
- A wide range of closed source, open source models are tested. A good set of prompts are used to test the models too!
- I particularly liked table 1 and the selection of different tasks with different characteristics.
- The normalized relative advantage is a good, interpretable metric
- The framework and taxonomy are clear and easy to understand.
- Section 4.4 gave some good insight into the types of errors made by llms
- I also liked reading the analysis in section 4.3, in particular that code pretraining helps with strategic reasoning.

Weaknesses:
- Characterizing human performance would strengthen the paper
- Including some qualitative reasoning traces of successes and failures might be insightful.
- Minor: This paper would be an ideal fit for the datasets and benchmarks track, instead of the main track. I dont think it should be penalized for this though!

Typos

Line 79: Characterize

Line 171: dynamic gaming → dynamic game

Limitations:
The authors do a good job of addressing limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GTBench, a set of 10 different games to test how well large language models can think strategically. The author found that while LLMs struggle with complete and deterministic games like Tic-Tac-Toe and Connect-4, they perform better in incomplete uncertain games like poker and negotiation. Code-pretraining improves their strategic thinking abilities. However, advanced thinking methods like Chain-of-Thought and Tree-of-Thought don’t always help and can sometimes make things worse. The latest open-source models, like Llama-3, are getting closer in performance to commercial models like GPT-4. Common mistakes LLMs make include misunderstanding game rules, being over-confident, and making calculation errors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to understand. 
2. The problem of evaluating LLMs' strategic reasoning abilities is meaningful. Creating such a benchmark is valuable for the research community.
3. The paper provides a detailed evaluation of LLMs across different game tasks. These tasks indeed measure the strategic reasoning of LLMs, even if some models already understand the optimal algorithms for those games. (For example, you could ask GPT-4 about the optimal strategy for some of these games, and it knows the optimal algorithm.)
4. The authors conducted extensive experiments using various base models, including reasoning methods like ToT and CoT. They had some interesting findings and analysis (concluded in the summary).

Weaknesses:
1. The paper claims that measuring strategic reasoning capabilities with games is missing in existing benchmarks. However, there are other benchmarks, such as MAgIC released last year, that consider benchmarking LLMs' strategic behavior using games. While there are differences, this weakens the claim of novelty.
2. Some of the selected games, like Tic-Tac-Toe, have known optimal strategies and are not complex enough. These games might not fully challenge the advanced strategic reasoning capabilities of LLMs. Even though the current evaluation is useful, as a benchmark intended for future use, it should be capable of evaluating more advanced or adapted LLM agents.
3. The benchmark focuses on a set of 10 games. It’s unclear how well the findings generalize to other strategic scenarios, even similar types of tasks. The results appear to be quite case-by-case. A broader range of tasks and scalable evaluation frameworks would make the benchmark more comprehensive.
4. The experiments primarily involve LLMs and traditional solvers. There is a lack of evaluation against human opponents, which could provide more insights into the models' performance in real-world strategic interactions. As a benchmark, I also expect to have other opponents (for example, the optimal algorithm, the RL based agent).

Limitations:
If LLMs are trained on biased data, they might reinforce existing biases in strategic decision-making. Testing the LLM with different personas could show if this changes the game results and reveal potential biases.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ypaqE8UwsC;"REVIEW 
Summary:
This paper proposed the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm. The combination of offline RL and federated learning is interesting in addressing the training data insufficiency issue due to small pre-collected datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The originality of this paper is relatively good, since the proposed Federated Ensemble-Directed Offline Reinforcement Learning Algorithm is effective in offline reinforcement learning. The quality and clarity are also clear, and this paper is actually well-written. The significance of this paper is obvious, because offline reinforcement learning is important in real-world scenarios.

Weaknesses:
1. Some technical details need to be explained. For example, the ensemble learning and its role.
2. The novelty of this paper needs further clarification, and what is the main difference between this proposed method and existing studies? It seems that there is only a simple combination of two technologies.
3. Numerically, the authors could consider comparing their method with more baselines. There are some studies on federated learning for offline RL.

Limitations:
1. What is the technical drawback of the proposed method? E.g., the effectiveness of the agent weight by ensemble approach
2. Does this proposed method work for other RL algorithms?

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors identify fundamental challenges for Federated Offline Reinforcement Learning and present Fedora, an approach that tackles each of them. They perform extensive evaluation of the approach on Mujoco and real-world datasets showing improved performance over existing work.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is well-written and, importantly, the code has been shared. The authors run extensive experiments. The work is novel and the notion of federated optimism is particularly interesting. Federated offline RL is an important research area with vast real-world applicability. The algorithm has been shown to be robust to diverse/ heterogenous client datasets. It is also commendable that the approach was tested on a real-world robot.

Weaknesses:
No theoretical guarantees have been given for the algorithm though it does build upon foundational work.  I believe that the authors should explicitly discuss limitations/ opportunities for future work in the paper. It is important for the algorithm pseudocode to be included in the main material as is the norm in such papers. I believe that there are perhaps many experiments included the main paper meaning that the discussion/ hypotheses for results is somewhat diluted. 
Another minor issue is that the figures are placed very far away from where they are referred to in text.

Limitations:
Limitations should be explicitly stated. I feel that the authors could give a more balanced view of the algorithm by not only showing strengths but also assessing the limits of the work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA), a novel approach for collaborative learning of high-quality control policies in a federated offline reinforcement learning (RL) setting. The paper identifies key challenges in federated offline RL, including ensemble heterogeneity, pessimistic value computation, and data heterogeneity. To address these issues, FEDORA estimates the performance of client policies using only local data and, at each round of federation, produces a weighted combination of the constituent policies that maximize the overall offline RL objective, while maximizing the entropy of the weights. Besides the core idea, FEDORA also performs data pruning

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. This is a novel work proposing the first federated offline RL algorithm in the general case (without assuming linearity). The paper is very well written with clear motivations and detailed discussions on the insufficiency of existing, naive approaches. 

2. The experiments are also very thorough and convincing with experiments ranging from simple 2D environments to high-dimensional continuous control problems. The algorithm is also tested on a real-world robot platform, which is very impressive given the density of algorithmic contributions in the paper.

Weaknesses:
1. ""Collect wisdom"" can be replaced by more rigorous exposition. Same goes with ""ambitious targets"". 

2. The number of communication rounds needed for FEDORA to converge is still quite high. 

3. Given how well the algorithm does, some sort of theoretical analysis could further strengthen the work.

Limitations:
Yes, limitations are adequately discussed.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ypPzyflbYs;"REVIEW 
Summary:
The paper proposes a novel approach to unsupervised concept learning based on both continuous and discrete encodings. Neural Concept Binder (NCB) allows humans inspecting and revising the learnt concepts. In the experiments, NCB’s discrete concept encodings result as expressive as the continuous encodings. Also, NCB can be integrated with symbolic and sub symbolic module. Finally, to support the experimental evaluation the paper introduces a novel dataset CLEVER-Sudoku, very suitable for neuro-symbolic benchmarking.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
-	**Novelty**: the proposed approach, although based on existing works SySBinder and Slot attention, is surely novel in the field of concept learning and potentially very relevant as it may strongly facilitate the extraction and discovery of unsupervised concepts. Particularly the possibility to revise concepts is completely novel to the best of my knowledge and very useful to improve human-computer interaction.
-	**Novel resource** presented: CLEVER-Sudoku will be surely an important resource for the Neuro-symbolic literature.

Weaknesses:
## Major issues:
  * Method presentation:
    - The way in which block-slot-encodings are obtained, is badly presented. Although it is based on previous literature, since it is a key architectural component, it should have been presented more in details. I suggest the author to employ a background section to report the way in which slot attention and sys binder work, in order to make the paper self-contained. 
    - Figure 2 which illustrates the core of the method is quite confusing: it is not clear how the discrete concepts are actually represented (the concept slot encodings reported are positive continuos representations). Also, the reminders to the figures in the text do not help as they generically refer to the entire figure and not to a specific block. A color coding of the different parts of the model could help understanding. 
    - How does the $\texttt{enc}_l^j$ work is not clear. What does it receive in input? Where it is extracted from?
    - All the revision operations are definitely not clear. The formal operation to be executed is often confusing.
  * Experimental evaluation:
    - Models: NCB has been compared only against SysBinder. While it is a very novel and innovative method, there is a complete lack of benchmarking against standard unsupervised concept-based approaches such as SENN[1], BotCL[2], ACE[3]. Comparing against supervised approaches such as CBM[4] or CEM[5] could have been also useful. 
    - Datasets: NCB is only tested on variants of CLEVER. While it is surely an interesting benchmark, real-world benchmarks are missing. Experiments on CUB or CELEBA, for instance, would have been very appreciated to better understand the scalability of the approach.

## Minor issues
  * Related work:
    - The unsupervised concept learning literature does not review several important concept-based paper working both post-hoc and explainable by design. Some examples are SENN[1], ACE[3], VAEL[6], as well as notorious prototype-based approaches such as Prototype layer [7] and ProtopNets[8]. 
    -  Unlike you state, continuous and discrete representations have been combined in recent literature for supervised concept learning. Some examples are CEM[5] and ProbCBM[9].
  * Unclear sentences:
    - “Briefly, given an image x, NCB derives a symbolic representation, c, which expresses the concepts of the objects in the image, i.e., object-factor level concepts. Herefore, NCB infers a block-slot encoding, z, of the image and performs a retrieval-based discretization step to finally infer concept-slot encodings, c”. The consequentiality of the inference process is misleading from this sentence. 
  * Method Inspection. What the authors refer as implicit, comparative, interventional and similarity-based inspections are normally referred to as example-based explanations (implicit and similarity-based) and counterfactual explanations (comparative and interventional). Sticking to well-known terms in literature is a good choice to avoid further confusion in the reader. 

Overall, I think it's an interesting paper proposing a novel approach to unsupervised concept learning. However, I think it will benefit from a further revision to deeply improve method presentation and expand the experimental campaing including other standard unsupervised concept-learning approaches and datasets.

[1] Alvarez Melis, David, and Tommi Jaakkola. ""Towards robust interpretability with self-explaining neural networks."" Advances in neural information processing systems 31 (2018).

[2] Wang, B., Li, L., Nakashima, Y., and Nagahara, H. “Learning bottleneck concepts in image classification”. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2023).

[3] Ghorbani, Amirata, et al. ""Towards automatic concept-based explanations."" Advances in neural information processing systems 32 (2019).

[4] Koh, Pang Wei, et al. ""Concept bottleneck models."" International conference on machine learning. PMLR, 2020.

[5] Espinosa Zarlenga, Mateo, et al. ""Concept embedding models: Beyond the accuracy-explainability trade-off."" Advances in Neural Information Processing Systems 35 (2022): 21400-21413.

[6] Misino, Eleonora, Giuseppe Marra, and Emanuele Sansone. ""Vael: Bridging variational autoencoders and probabilistic logic programming."" Advances in Neural Information Processing Systems 35 (2022): 4667-4679.

[7] Li, Oscar, et al. ""Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.

[8] Chen, Chaofan, et al. ""This looks like that: deep learning for interpretable image recognition."" Advances in neural information processing systems 32 (2019).

[9] Kim, Eunji, et al. ""Probabilistic Concept Bottleneck Models."" International Conference on Machine Learning. PMLR, 2023.

Limitations:
The method limitations are well addressed by the authors.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces neural concept binder, a neural symbolic framework that utilizes both soft and hard binding. Building on top of the sysbinder model, it can additionally do exemplar-based hard binding and revise concepts. Evaluations made on CLEVR and the proposed CLEVR-Sudoku dataset proved the method's validity.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- The paper is well-written and easy to read. Connections to previous works are clarified nicely.

- It's good to see the incorporations of both hard and soft bindings to existing neural-symbolic frameworks.

- The model achieved good performance on the proposed CLEVR-Sudoku task and can do satisfactory concept revision and inspection, which is a neat proof of concept that hard binding works.

Weaknesses:
There are several weaknesses I can foresee that may lead to the rejection of this paper.

- Limited contribution: After so many years of developing neural-symbolic methods in visual reasoning, from the earliest modular approaches to unsupervised concept learners, code-based reasoning models, and recent visual programming-like frameworks, the goal of neural-symbolic modeling has dramatically changed. In this work, the neural concept binder still focuses on one of the earliest task categories designed for visual reasoning (CLEVR attribute classifications or unsupervised concept learners). It's also built on top of sysbinder, in other words, it's merely an incremental improvement by adding a retrieval-based library.

- I don't see any generalizability of this method beyond extremely toy tasks (attribute classification). The proposed CLEVR-Sudoku is strange and does not correspond to any of the real-world visual reasoning tasks. Relational tasks are also not tackled in this paper.

Limitations:
Yes, the authors have adequately addressed the limitations. I do not see any potential negative societal impact.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors introduced a pioneering framework that combines an object-centric learning module with a retrieval-based module to address visual reasoning tasks and a new visual reasoning task, CLEVR Sudoku. The proposed method demonstrated significant potential in effectively acquiring inspectable and revisable concepts via human or machine feedback in various scenarios.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- S1: The proposed method offers significant novelty in that it has the potential to serve as a building block for concept learning, which can be leveraged as a core module in other frameworks. The author's well-structured experiments provided compelling evidence in support of these claims.

Weaknesses:
- W1: The proposed approach can be interpreted as directly integrating SysBinder and HDBSCAN. Because the initial concept detection fundamentally depends on the complete functionality of SysBinder, this framework may not circumvent specific inherent challenges of object-centric learning, including inductive bias resulting from choosing the proper object-factor encoder and identifiability issues.
- W2: Using HDBSCAN is intuitive in the proposed method, but it would be beneficial to include an additional experiment that compares different clustering methods.

Limitations:
Please check out the Weakness and Question sections.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces Neural Concept Binder, a framework for obtaining discrete concept representations from images without any supervision. The method is an extension of Neural Systematic Binder (SysBinder), adding a clustering step on top of the block-slot representations to obtain discrete concept representations. The resulting representations are interpretable and modifiable, as shown in the experiments. The model is additionally evaluated on property prediction and downstream tasks on modifications of the CLEVR dataset and shown to be able to leverage fewer training samples than SysBinder.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper investigates an important problem in learning discrete, interpretable concepts from images in an unsupervised way. The model is a logical extension of SysBinder in clustering the representations to obtain discrete concepts. The experiments show improvements in sample efficiency of these discrete representations over SysBinder’s continuous representations.

Weaknesses:
1. Since the solver used in the Sudoku experiments is the same across all baselines, it seems the determining factor of performance is in how well the digits are classified. Therefore, I do not believe framing this evaluation in the context of Sudoku adds any insight—in fact it seems to add unnecessary noise to the evaluation. The evaluation in the appendix (Figure 8) seems more informative and sufficient for determining the benefit of NCB. 
2. This paper is missing several related citations: Unsupervised Concept Discovery Mitigates Spurious Correlations (https://arxiv.org/abs/2402.13368) and Neural Language of Thought Models (https://arxiv.org/abs/2402.01203). NLoTM is particularly relevant and can be an additional baseline since it also extends SysBinder to learn discrete representations, except it is trained in an end-to-end way.
3. The discussion in section 3.3 is interesting, but it would be informative to tie each point with corresponding experimental evidence.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ypFgcT147Z;"REVIEW 
Summary:
In this paper, the authors propose a new method to measure similarity between responses of deep neural networks in vision. They reformulate the commonly used strategy to compute Representational Similarity Matrices (RSMs) by acknowledging the superiority of the semantic information over the spatio-semantic information in the creation of RSMs. The authors perform different experiments to show the improvement over the baseline method caused by their reformulation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is clearly written and tackles an important topic.
- The idea is original and besides its limitation connected to high computational needs, it could serve as an inspiration for future works.
- Although the experiments are not exhaustive, they are convincing and coherent and the gained insights seem relevant.
- Being transparent with the limitation of the method and trying to provide the means to mitigate it is a plus.

Weaknesses:
- The authors should better discuss the differences between the results obtained for ViTs and CNNs in their study, which are quite well visible. E.g. while in the case of an examined CNN, the spatio-semantic RSM does not reflect well the similarities between translated images, in the case of the examined ViT (appendix), these similarities can be observed. The other thing is that the experiment is slightly different, because in the experiment with CNNs much smaller images are used than in the ViT experiment. Also, the differences are also visible in Table 1 (ResNets obtain much higher absolute correlation values for the baseline and the proposed methods than ViTs). 
- The authors provided few visual examples of the results of their method. It would be good to provide more of them (e.g. for different similarity metrics used, for more images and for more netorks) to enable more comprehensive qualitative evaluation (they could be placed in the appendix). 
- The use of some methods at work is not well justified (e.g. Pearson correlation).

Limitations:
The authors adequately discussed the biggest limitation of their work (the computational cost of their method). They could also better highlight the limited data used in the experiments caused by the mentioned computational constraints.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes Semantic RSMs to understand the internal representations in deep neural networks. The authors argue that the current RSMs are limited by their coupling of semantic and spatial information, which restricts the assessment of similarity. The proposed semantic RSMs are spatial permutation invariant and focus solely on semantic similarity. The proposed method is shown to enhance retrieval performance and provide a more accurate reflection of the predictive behavior of classifiers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to follow.
2. The introduction of semantic RSMs is a significant contribution, potentially leading to more meaningful comparisons between neural network models.
3. The empirical demonstration of improved retrieval performance using semantic RSMs is convincing and adds practical value to the theoretical development.

Weaknesses:
1. While the paper does highlight the high computational complexity as a limitation, it would benefit from a more detailed discussion on the scalability of the proposed method to larger models and datasets and the approximation error.

Limitations:
Yes, the authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce semantic RSMs, which are designed to be invariant to the spatial arrangement of elements within images. These semantic RSMs assess similarity by treating the problem as one of set-matching, where the focus is on matching semantic content rather than spatial details. This approach not only aligns more closely with human perception but also improves the relevance and accuracy of image retrieval tasks. The paper claims that semantic RSMs offer a more robust measure of similarity by comparing them to traditional spatio-semantic methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The focus on semantic content rather than spatial arrangement aligns more closely with human perception, potentially leading to more intuitive and relevant comparisons of neural network responses.

-  By being invariant to spatial permutations, this method can effectively compare images where the same objects appear in different locations.

- Semantic RSMs can be used as drop-in replacements for traditional RSMs.

Weaknesses:
- Employing algorithms like Hungarian matching to find the optimal permutation matrix can be computationally expensive.

- The effectiveness of this approach relies heavily on accurate identification and parsing of semantic concepts within images, which can be challenging in complex scenes or under conditions of visual ambiguity.

- While focusing on semantic content is generally advantageous, completely ignoring spatial information can sometimes omit useful contextual cues that contribute to overall image understanding. For example, 

[contextual cues] a picture of a dining table with plates, utensils, and food arranged in a specific way might convey a meal setting, which could be lost if the spatial relationships are ignored. 

[object interactions] Images where interactions between objects are important, such as a cat sitting on a mat, might lose their interpretative meaning if spatial information is disregarded. The semantic content (cat, mat) remains the same, but the relationship changes based on their arrangement.

[abstract content] In abstract art or images with non-literal interpretations, spatial composition itself can carry meaning and affect how the content is perceived and classified.

Limitations:
- To better evaluate the impact of semantic RSMs, a set of diverse metrics should be established: Evaluate semantic RSMs against traditional spatio-semantic RSMs and other state-of-the-art similarity measures to highlight the improvements or shortcomings.

- The authors should explicitly state the primary objectives of employing semantic RSMs. Show that semantic RSMs can make neural network decisions more interpretable by aligning more closely with how humans perceive images.

- Apply semantic RSMs in specific use cases like medical imaging, satellite image analysis, and autonomous driving where ignoring spatial arrangements can be particularly detrimental or beneficial, providing a nuanced view of their applicability.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper makes a contribution to the construction of RSMs in the field of vision neural networks and puts forward the concept of semantic RSMs, which is innovative and theoretical.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The proposed semantic RSMs are used for spatial alignment by means of optimal permutation, which is a relatively new and promising method.

This paper verifies the validity of semantic RSMs through experiments such as image retrieval and probabilistic similarity comparison. An in-depth analysis of the experimental results is carried out, and the advantages of semantic RSMs in specific tasks are pointed out.

Weaknesses:
This paper lacks the experimental verification of specific downstream tasks, such as detection and segmentation, on semantic RSMs. I need to know which scenario is more suitable for RSMs and semantic RSMs.

Lack of quantitative comparative data. It is suggested to add tables or charts to show specific performance comparison data between semantic RSMs and existing methods in different tasks (such as image retrieval, class probability similarity comparison, etc.), including accuracy, time complexity and other indicators.

The discussion of the experimental results was not thorough enough. It is recommended to add a detailed analysis of the experimental results to explain why semantic RSMs perform better on certain tasks, as well as possible reasons and limitations.

 ""aligns"" to ""align"" in line 57.

Limitations:
see weekness

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ypEamFKu2O;"REVIEW 
Summary:
This paper proposed a Parallel Gated Network (PGN) as a successor to RNN, featuring a Historical Information Extraction (HIE) layer to directly capture information from previous time steps. Additionally, it introduces a Temporal PGN (TPGN) framework with two branches to capture both long-term periodic and short-term semantic patterns, demonstrating state-of-the-art performance in long-range time series forecasting.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper compares a variety of cutting-edge methods.
2. The experiments are generally thorough.

Weaknesses:
1. The major issue with this paper is the lack of analysis and comparison with significant literature. The entire paper's premise is the traditional RNN's failure in long-term sequence problems due to the long information propagation paths of its recurrent structure. However, as far as I know, SegRNN[1] has already addressed these shortcomings of traditional RNN in long-term forecasting through segmented iteration and parallel prediction. Yet, there is no discussion on this in the paper. Please compare your method with it and clarify your differences and advantages.

2. In Section 2, you should distinguish between Linear-based and MLP-based methods. The former has only single-layer parameter connections, while the latter has multiple layers and can learn non-linear features due to the presence of activation functions. Methods like DLinear and FITS should be classified as Linear-based methods.

3. The description of HIE is unclear: (i) The process shown in Figure 2(a) suggests first performing linear mapping and then zero-padding, which conflicts with Equation 1 in the paper, where H = HIE(Padding(X)), and the actual code. It is recommended to modify Figure 2 to make this clearer. (ii) Line 169 describes that “HIE(·) is a linear layer,” but in practice, the behavior of HIE is more like a sliding aggregation operation of CNN (or TCN) rather than a sorely linear mapping. Given **(ii)**, calling the proposed method RNN-based is debatable since it is more likely TCN-based. 

4. You should include an ablation analysis of the normalization layer, explaining its impact on TPGN achieving state-of-the-art results.

5. Although the authors provide source code, it does not include the hyperparameter settings required to reproduce the key results in the paper, meaning there is no directly runnable script. Are the hyperparameters in the main results all defaults? For instance, is TPGN_period=24? If not, providing a complete script file that can be run directly is necessary.


[1] Lin, S., Lin, W., Wu, W., Zhao, F., Mo, R., & Zhang, H. (2023). Segrnn: Segment recurrent neural network for long-term time series forecasting. arXiv preprint arXiv:2308.11200

Limitations:
The authors have already described the limitation of this work in the paper, namely the lack of modeling multivariate relationships. To some extent, this is not a significant issue because many current cutting-edge studies have demonstrated that focusing solely on univariate temporal relationships can also be effective in multivariate tasks.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on long-range time series forecasting problems. To address the limitations of RNNs, a novel paradigm called PGN is introduced as an alternative, providing shorter information propagation paths. Building upon PGN, the paper further presents a generic temporal modeling framework named as TPGN, which effectively captures both long-term and short-term periodic patterns, as well as local and global information, through a dual-branch design. The experimental results in this paper demonstrate that TPGN exhibits excellent performance in time series forecasting.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1: This paper proposed a novel paradigm called PGN, which effectively tackles the inherent issues of RNNs through a simple yet powerful design. PGN exhibits a high level of innovation and holds the potential to replace traditional RNNs.

S2: TPGN primarily focuses on modeling the temporal dimension. Its dual-branch design makes sense as it captures both long-term and short-term periodicity, as well as the local and global characteristics of time series. Additionally, it is reasonable to set up different univariate forecasting tasks to evaluate TPGN's performance.

S3: This paper is well-written, and the presentation of the figures and tables is clear, making it easy to understand and follow. The experimental comparisons are comprehensive, including numerous advanced baseline models such as iTransformer, ModernTCN, FITS, TimeMixer, PDF, WITRAN, and Basisformer.

Weaknesses:
W1. For tables with a large amount of content, such as Table 1, it may be beneficial to consider using different colors for highlighting, as it could enhance clarity. Additionally, another option to consider is moving some of the experimental results to an appendix.

W2. While TPGN exhibits some advantages in terms of efficiency, I have noticed that it still appears to be challenging to reach the optimal level. Specifically, I have noticed that as the input sequence size increases, the efficiency of TPGN may gradually become inferior to that of iTransformer.

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new model paradigm which aims to solve the traditional bottlenecks of RNN models, such as non-parallel computation, gradient explosion/vanishing issues, etc.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. An important problem is studied in this paper.
2. The overall representation is clear and easy to follow.
3. A comprehensive summary of the related work is provided.

Weaknesses:
1. The overall contribution is not very significant.
2. Some questions regarding the time complexity and experiments need to be clarified.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new network called PGN to capture the long-term dependencies of time series. Based on PGN, this paper further design TPGN for long-range time series forecasting. TPGN consists of two branches to respectively capture the long-term periodic patterns and short-term information of time series. Extensive experiments are conducted to show the effectiveness and efficiency of the TPGN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1. This paper is easy to follow. The motivations are clearly described by figures. The authors thoroughly analyze the information propagation modes of different time series forecasting models and explore new information propagation path to improve TS forecasting effectiveness.

S2. The design of the PGN is novel, which is a completely new network architecture and can effectively solve the inherent problems of classic RNN models. Both experimental results and theoretical analysis show the effectiveness and efficiency of PGN.

S3. This paper proposes TPGN upon PGN, which capture both the long-term and short-term characteristics of the time series with low computational complexity.

S4. Experiments are sufficient. Five benchmark datasets are evaluated and the most representative models proposed recently are included in the experiments.

Weaknesses:
W1. The computational complexity of TPGN is not well discussed in this paper, and it would be better if the inference efficiency was adequately discussed as the time series size increases.

W2. Some presentation needs to be improved. For example, it is difficult for readers to quickly get important conclusions on Table 1 and Table 4.

Limitations:
Yes, the authors have fully discussed the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
yQL5tutdaH;"REVIEW 
Summary:
This paper explores the stable evaluation of object hallucinations, which is a crucial challenge in large vision-language models. The authors provide the first systematic analysis of the underlying mechanism through which instructions affect hallucinations, based on comprehensive experiments. They report a linear correlation between the length of descriptions and the levels of object hallucinations. Furthermore, the authors propose a curve-based framework that incorporates description lengths to enable a stable evaluation of hallucinations. What I find particularly novel is that the slope of the curve is incorporated as a metric, which achieves a more comprehensive evaluation.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. This work might provide valuable insights to the community. Firstly, while the impact of instructions on hallucinations is widely recognized, this work unveils a crucial aspect by demonstrating that instructions exert their influence through the modification of description lengths. This finding illuminates the previously unexplored mechanism underlying instruction-affected hallucinations. Secondly, they employ a curve-based evaluation method instead of relying solely on a single metric, which goes a new way in addressing hallucination evaluation. Thus, this work has the potential to inspire further research and exploration in hallucination evaluation. 
2. The proposed curve-based hallucination evaluation method in this paper is intuitively reasonable, and the author provides substantial experimental evidence to support the motivation behind this method. The experimental results are clearly presented, and the corresponding analyses further enhance the persuasiveness of this work. Overall, the combination of the intuitive approach, extensive experiments, clear presentation of results, and insightful analyses makes this work convincing.

Weaknesses:
1. The proposed method realizes consistent evaluation by calculating the hallucination rate at a uniform length. However, the length distributions of descriptions generated by different LVLMs exhibit variations. In other words, some models tend to produce shorter descriptions while others generate longer ones. In light of this, I have concerns regarding the ability of this method to maintain its effectiveness under such circumstances.
2. In my view, the hallucination evaluation of a LVLM in practical requires a large instruction set that could simulate real-world applications of the LVLM. If the authors can build such a large instruction set as the benchmark, it would yield a significant contribution to the community.
3. The authors claim that their proposed evaluation method is fairer compared to other evaluation methods. However, the paper appears to lack experimental results to support this assertion.
4. The analysis of the stability of the “LeHaCE_GR” is lacking.
5. The selection of instructions may have a substantial impact on the fitted curve. It would be beneficial for the authors to provide further discussion on this aspect.

Limitations:
The paper briefly mentioned limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work aims to establish a stable, fair, and comprehensive evaluation method for object hallucinations in large vision-language models. The authors discovered a positive correlation between the length of image descriptions and the degree of object hallucination. Building upon this observation, they developed a hallucination evaluation method named LeHaCE by fitting a length-hallucination curve. LeHaCE enables the evaluation at any given image description length, ensuring stability and fairness in the evaluation process. Additionally, LeHaCE involves the curve slope as a metric to evaluate the influence of image description length on the degree of object hallucination, thereby achieving a comprehensive evaluation. The motivation behind this work is reasonable, and the authors provide many experiments to support their claims. However, it is worth considering that the use of the linear fitting scheme, although straightforward, does somewhat diminish the novelty of the proposed method.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
The experimental analysis conducted on instructions and hallucination is compelling and provides strong support for the main argument that the hallucination degree is positively correlated with the length of the description. While previous research (Yifan et al., 2023) has already shown the influence of instructions on hallucinations, this work takes it a step further by proposing that instructions indirectly influence hallucinations through the length of image descriptions. This sheds light on the reason behind the limitations of previous approaches that relied on average-based methods. Overall, this paper offers valuable insights into the evaluation of consistent hallucinations.

Weaknesses:
1. Although the rationale behind the length-hallucination curve is compelling, it is fitted using a relatively simplistic linear approach. Exploring more flexible and intricate fitting approaches is worth considering, as it has the potential to achieve higher fitting accuracy and more effective hallucination evaluation.
2. Since the proposed method relies on a fitted curve, it needs at least two instructions to evaluate LVLMs and cannot be used with just one instruction.The authors should discuss this limitation.
3. Lack of indepth discussion on the shortcomings of the proposed method. For instance, as shown in Table 2, why does LeHaCE exhibit poor stability on a few LVLMs when the number of instructions is three?
4. It seems that the selection of instructions might affect the stability of LeHaCE. It would be helpful to include more discussion on this aspect.
5. The current paper seems to have lots of results and experiments. As a reader, it is not very easy for me to get the main conclusion for each experiment. It would be good to highlight the conclusions so that the readers can understand the point easier.
6. Some typos need to be corrected: Line 79: lrv-instruction -> LRV-instruction. Line 92 Nope -> NOPE. Line 81 chatgpt -> ChatGPT. Table 2: Minigpt-4 -> MiniGPT-4.

Limitations:
The paper includes a simple discussion of the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper identifies a pitfall regarding the length of image descriptions in the current average-based LVLM hallucination evaluation framework. To address this, they propose a new Length-Hallucination Curve Based evaluation framework to enhance the fairness of evaluations. The paper observes that the degree of object hallucinations is primarily influenced by the length of image descriptions, with instructions indirectly affecting hallucinations through their impact on description lengths. They suggest using a linear regression curve for evaluation and develop two metrics based on this curve. Extensive experiments on multiple LVLMs with different instruction sets demonstrate the stability of their proposed new evaluation metrics.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The observation is intuitive and validate by extensive experiments

- The paper is clearly written and easy to follow

- The evaluation is comprehensive in terms of numerous instructions and LVLMs

Weaknesses:
- Although paper observe the linear relation between the length of the image description and objection hallucination, there are still unanswered questions regarding the justification of the claim. Please see questions below.

- Some minor inconsistent typo, for example, the AEF and ABF in Figure 4.

- The evaluation only use CHAIR scores and scores of other aspects is not evaluated, for example, the detail or the coverage of the real objects in the description as in AMBER.

Limitations:
Yes, the author adequately addressed the limitations and potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents comprehensive experiments to study the relationship between description lengths and hallucinations in LVLMs. Based on the observed positive correlation, authors propose an approach of fitting a length-hallucination curve to evaluate object hallucinations. Speciffically, the curve allows for fair comparisons that are not influenced by varying lengths, through providing the hallucination degree corresponding to any given description length. Furthermore, the curve slope reflects the extent to which a LVLM's hallucination degree is affected by description lengths. The evaluation, considering both the value and slope, demonstrates stability and comprehensiveness, as supported by the conducted experiments. The authors' thorough and meticulous research on this issue is highly convincing, and the proposed method effectively showcases its effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Hallucinations evaluation is a realistic and crucial task in the field of LVLMs, as hallucinations usually introduce misleading conclusions or even have disastrous outcomes. In this context, the authors perform a detailed experimental analysis on the impact of instructions on hallucinations, providing convincing evidence to support their motivation. Moreover, the proposed curve-based method is a simple yet effective approach, which is well-motivated by the observed linear correlation between description lengths and hallucination rates. The paper is well-written and effectively communicates its main contributions and techniques. Overall, the paper exhibits technical solidity.

Weaknesses:
1. The authors conduct experiments using only the beam search setting. Although I understand that beam search is widely used in hallucination evaluation of LVLMs/LLMs, it remains uncertain whether the observed correlation between the hallucination degree and the description length holds true under different decoding strategies. Thus, I recommend that the authors explore additional commonly used decoding strategies, such as greedy decoding, to provide a more comprehensive analysis. 
2. The paper lacks a study about the influence of the instruction number on the length-hallucination curve. The fitted curve is directly affected by the number of samples, which corresponds to the number of instructions provided. It is therefore essential to thoroughly investigate the minimum number of instructions necessary for the proposed method. 
3. The authors mention in the paper that the proposed method can ""evaluate object hallucinations at any given image description length."" In reality, when the given length deviates too much from the existing data, the fitting is likely to fail, leading to inaccurate results. The authors should use more cautious wording.
4. In my opinion, the impact of length might be mitigated by simply controlling the maximum generation lengths.The authors only mention this method in a footnote and believe it does not align with the actual usage scenarios of LVLMs. More in-depth discussions should be provided.
5. Some minor errors need to be corrected. For example, in line 42, ""Figure 2&3"" should be ""Figures 2&3"".
6. It appears inappropriate to represent a variable using only two letters. Consider replacing ""hr"" with ""h_r"".

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
ynJr0RW6FR;"REVIEW 
Summary:
This paper presents a method for stylizing 3D Gaussian Splatting (3DGS) using a single reference image. Unlike NeRF, which uses a structured representation, 3DGS is an unstructured discrete representation that tightly binds geometry and appearance to each Gaussian splat. To address this challenge, the paper introduces a texture-guided control mechanism, which differs from the position-guided approach used in the original 3DGS paper. This new mechanism effectively edits the appearance of a pretrained 3DGS to match the detailed texture from the reference image while preserving the original geometric structure.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ The main novelty of this work lies in the Gaussian splitting strategy, which is based on the color gradients of all Gaussians over iterations, rather than the positional gradients used in the original 3GDS approach. I find this approach to be quite neat and well-suited for the task.
+ The ablation study demonstrates the benefits of using this approach, including a reduction in the number of Gaussians needed to model the details of the reference texture.

Weaknesses:
+ The novelty of the method seems somewhat limited, as it is largely based on Ref-NPR to enable image-reference-guided stylization.
+ It is unclear how well the method would perform if the geometry is also heavily stylized, rather than just the appearance.
+ The results (specially the video results) presented are quite limited, e focusing primarily on simple synthetic scenes with white backgrounds, and do not demonstrate the method's effectiveness on more complex scenes.

Limitations:
The limitations of the method are adequately addressed, and interesting future directions are proposed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents an optimization-based approach for style transfer of a (pre-baked) 3D scene represented by a 3D Gaussian splatting (3DGS). In order to fine-tune the given 3D scene with a style reference image of a single view, the authors suggest using a texture-guided controlling algorithm, which modifies the densification algorithm of the original 3DGS by focusing on the color gradients. The training loss is also modified to include depth-based geometry regularization and additional guidance provided by generated pseudo-views based on the 3D projections of the given style reference onto novel view cameras. The experiments are performed upon the existing public weights of 3DGS, where the method is compared with three NeRF-based methods, ARF, SNeRF, and Ref-NPR.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. As demonstrated in the supplementary materials and figures in the manuscript, the method seems to work well with the pre-baked 3DGS weights.
2. Detailed related work section enlightens novice readers to get familiar with the field of style transfer of 3D scenes.
3. Adequate level of implementational details are provided.

Weaknesses:
1. Although the topic and the approach presented in the paper seems adequate, the presentation of those can be much better. For example, since the authors have modified the original training algorithm of 3DGS in Section 3.2 of the manuscript, and this seems to be the most significant contribution of this paper, they can use *Algorithmic/Algorithm2E features of LaTeX* or present with *a pseudocode of the densification algorithm* to more clearly present the key differences between theirs and the original 3DGS.
2. The components of the proposed loss functions, such as the TCM, the pseudo-view loss, the depth regularization, and the color-matching loss, which are originally devised to work with NeRF-based scene representation (Ref-NPR). I do not want to argue with the novelty of this adoption, but I believe that the design decision should be more firmly verified. Even though these losses may be generalizable to 3DGS-based representations as the paper implies, this hidden claim should be re-assessed with each component on the compatibility with the new representation (3DGS). In other words, *ablation studies for these loss functions* can be carried out just like [Figure 6 of Ref-NPR paper](https://ref-npr.github.io/assets/2212.02766.pdf) in order to justify the fitness of the proposed loss function with 3DGS representations.
3. I understand that an exhaustive quantitative analysis in this topic can be very difficult to design, but comparing the results with only one table seems not promising enough. For example, detailed tables with each test scene, just like [Table B.1 of Ref-NPR](https://ref-npr.github.io/assets/2212.02766.pdf), can be added with more visualization.
4. The paper could be much better with visualization of *how different style reference images affect a single scene* with the proposed algorithm. For example, Ref-NPR shows results with multiple style inputs acting on a single baked scene.

As a summary, my key concern is the (1) representation of the materials, the (2) justification of the presented/adopted components (the losses, the densification algorithms), the (3) lack of quantitative comparison table of each scene, and the (4) lack of comparison of the results from different style images.

The main contribution of the paper I believe is to report the results from applying the training algorithms of Ref-NPR to 3DGS-based representations with proper algorithmic modification to make it suitable for 3DGS. One requires to compare at least all the cases demonstrated in Ref-NPR in order to justify that this training scheme for style transfer is better suited for 3DGSs than NeRFs. Therefore, unless the mentioned points are addressed, I believe this version of the manuscript is not ready for publication in this venue.

Limitations:
Yes, the limitations are adequately addressed in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a method to stylize 3D Gaussians using a texture guidance. The method takes a pretrained 3D Gaussian model and one content-aligned reference image as inputs and outputs a stylized 3DGS model which could be rendered at real-time framerate. Several techniques, including structured densification, depth-based geometry regularization and view-consistency constraints are introduced to achieve an effective stylization which performs better than previous state-of-the-art work.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper is generally well-written and easy to follow.
2. The insight on color gradients is interesting and works well. The method seems promising for appearance editing on Gaussians.
3. Both qualitative and quantitative evaluations show noticeable improvement compared to previous work.

Weaknesses:
1. The methodology seems largely inspired by Ref-NPR, though adapted to fit the 3D Gaussians. Readers may have to read Ref-NPR first in order to understand the motivation behind the design choices, especially in Section 3.4.
2. The superscript $(x, y)$ in Eq. 5 is not explained.
3. Minor indentation issues on L154, L188, L198, and L224.

Limitations:
Limitations are addressed in the supplemental material.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed a texture-guided Gaussian densification strategy for exemplar-based 3DGS style transfer with content-aligned reference, while preserving original geometry by depth supervision.
During 3D stylization with a style template reference, the introduced texture-guided Gaussian control strategy can geometrically adaptively densify 3D Gaussians for fine-grained texture optimization. 
Relying on the advanced representation of 3DGS, the stylized scene can achieve real-time rendering of novel views.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper proposed a decent design of style transfer for a 3DGS scene while preserving geometry by depth supervision.
The novel texture-guided control of Gaussian densification assists in optimizing texture with high-frequency details.
I believe this strategy worths attention beyond 3DGS appearance stylization.

2. The Stylized Pseudo View Supervision works better than other multi-view consistent stylization baselines, in terms of semantic consistent stylization for uncovered areas by reference view. 

3. The elaboration of methodology is technically sound, which is possibly reproduced.

4. The experiments and evaluation are convincing with ablation studies and baseline comparisons. And paper experimented on diverse scenes covering objects, forward-facing scenes, an unbounded scene. But I still have some main concerns mentioned in Weaknesses 2.

Weaknesses:
1. The paper mainly concerns fine appearance optimization by densification and depth supervision.
For 3D stylization, geometric stylization and editing could be tried or discussed based on proposed method. For example, stylizatin given an edited view with minor shape changes.

2. The most innovative and inspiring part is the Texture-guided Gaussian Control with texture guidance plus structured desification. However, the experiment part can be further improved:

    2.1. In Appendix C, there an ablation study by comparing original 2 Gaussians and proposed 9 Gaussians densification set. There is no solid and scientific validation for the best selection of the number 9. Please see details in Question 2. 

    2.2. There is no ablation study of ablating only texture guidance (i.e. use original position gradients as guidance), or ablating only structured densification (i.e. use original densification scheme). Current Sec 4.2 ablation study of Texture-Guided Control show the joint effect of texture guidance and structured desification, which cannot show the effects come from the joint cooperation or from one dominant strategy. Please see details in Question 3. 

3. A minor point and suggestion.
For evaluation comparisons, the paper mainly compare with baselines with Plenoxels representation.
Since ReGS's fast training and rendering capability replies on 3DGS, even ablation studies provide good validataion, I still expect comparisons with baselines with 3DGS, e.g. reproduce 3DGS-version SNeRF.
 
4. Minor issue in related work section. The paper should stress 2D and 3D stylization involve only image-exemplar based neural style transfer.
Since this work finishes edited-view guided stylization, methods of text-guided dataset editing for optimization such as Instruct-NeRF2NeRF/Instruct-GS2GS is also a suitable related work.
There are also some concurrent work stylizing 3DGS scenes, such as StyleGaussian, StylizedGS, Gaussian Splatting in Style.

Limitations:
The paper discussed limitations and provided some potential solutions.
The paper does not involve potential negative social impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ReGS, a new reference-based 3D style transfer method that utilizes 3DGs as the 3D representation. To capture fine-grained details from the reference view, the method employs texture-guided Gaussian control to enhance density in areas where texture is under-represented. Additionally, the approach incorporates depth-based regularization and pseudo-view supervision to ensure consistency while stylizing with the reference image. The quantitative and qualitative results demonstrate that ReGS achieves superior stylization, capturing detailed and high-quality effects more effectively than previous methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and comprehensive, making it easy to follow.
- The experiments are detailed, and the impact of each proposed method is demonstrated step-by-step.
- The stylization results effectively capture fine-grained details from the reference image.
- The proposed appearance-based densification approach is simple yet proves to be effective.
- The choice of 3D GS for reference-based stylization results in faster rendering performance.

Weaknesses:
- I do not find the methods presented in the paper to be significantly novel, as they give the impression of being a 3DGS-adapted version of Ref-NPR. While I acknowledge the differences and novelties introduced to effectively adapt reference-based stylization to the 3D-GS setting, I do not see a critical distinction in terms of the 'style transfer technique' itself, once the modifications specific to the 3D-GS settings are set aside. This is primarily because the stylization pipeline (Section 3.4) closely mirrors that of Ref-NPR, without introducing new improvements or modifications.
- The qualitative comparison presented in Figure 6 appears unfair. As I understand, ARF and SNeRF in this experiment are stylized using a stylized reference view, and the discrepancies between these results and the reference view are emphasized. However, the primary objectives of ARF and SNeRF differ from those of Ref-NPR and ReGS, as they are not specifically designed for reference-based stylization. Consequently, there is no inherent need for their stylization results to strictly adhere to the reference view. I believe the authors are aware of this distinction. For a fairer comparison, it would be more appropriate for the authors to include the original 2D style image for ARF and SNeRF and conduct a qualitative assessment based on aesthetic quality. Comparisons of the ability to replicate high-frequency details and correspondence should perhaps be reserved exclusively for comparisons with Ref-NPR.

Limitations:
The authors addressed the limitations in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yltJAlwtW9;"REVIEW 
Summary:
This paper analyzes the estimation bias and generalization error of the expected calibration error (ECE). Specifically, in a binary classification setting, the authors provide an upper bound for the total bias with an improved convergence rate, applicable to both uniform mass and uniform width binning strategies. They also determine the optimal number of bins to minimize the total bias. Furthermore, the authors utilize the information-theoretic generalization framework, particularly the Conditional Mutual Information (CMI) framework, to characterize the generalization of ECE.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper achieves a tighter bound for total bias compared to previous works.

2. The optimal number of bins is determined using the upper bound of the total bias.

Weaknesses:
1. As the authors themselves note, a significant limitation is that the analysis in this work is only applicable to binary classification.

2. Some assumptions (e.g., Assumption 2) are not well justified.

3. The writing has significant room for improvement; several arguments are unclear or misleading.

Please find more details in the questions below.

Limitations:
The authors have thoroughly discussed some limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the estimation bias in expected calibration error (ECE) for binary classification models, focusing on uniform mass binning (UMB) and uniform width binning (UWB). The authors present a comprehensive theoretical analysis, establishing upper bounds for the bias and the generalization error. Based on the convergence rates of binning and statistical bias, they identify the optimal number of bins to minimize the total estimation bias.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper provides a comprehensive analysis of the estimation bias in ECE, providing upper bounds and optimal bin size choices for both UWB and UMB.
* The authors further derive upper bounds for the generalization error between ECE and TCE using an information-theoretic approach.
* Numerical experiments on deep learning tasks confirm that the derived bounds are non-vacuous.

Weaknesses:
* The provided results only apply to binary classification, and require Lipschitz continuity which may not be necessarily satisfied in deep learning models. Also, these bounds are analyzing the ECE using test data but not training data, making them less applicable since test data are not always available in practice.
* The convergence rates of the information-theoretic generalization bounds heavily depend on the actual rate of eCMI and fCMI measures, which are not directly clear in analysis. In theorem 6, the authors show that eCMI scales as O(log n) based on metric entropy, but this bound involves the dimensionality d, and is thus hardly applicable to deep learning models.
* For experimental results, only the statistical bias is evaluated but not the total generalization error. It is also hard to see to what extent these bounds are tight in the current results. These bounds are also hard to estimate due to the existence of eCMI or fCMI measures. I would suggest the authors additionally consider some synthetic settings where TCE, eCMI, and fCMI are analytically tractable to show the tightness of the bounds. (maybe Gaussian data points?)

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies  the expected calibration error  using information-theoretical tools. They derive different tight fCMI and eCMI bounds in this setting. Empirical results show that the results are nonvacuous.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1/ The paper is in general well written. Adequate discussions are given in the main body of the paper and the appendices.

2/ The paper provides the first information-theoretic comprehensive analysis of the bias associated with the ECE when using the test and training datasets.

3/ The theoretical results seem sound. I skimmed through most of the proofs (I did not go through all of them in detail)  but the proofs are well-structured and easy to follow. 

3/ Empirical results show that the bound is tight for deep learning models.

Weaknesses:
The only weakness, if any, is perhaps that the paper uses conventional machinery for deriving information-theoretic generalization bounds and that it has not developed novel proof techniques.

Limitations:
The limitations are adequately addressed in the conclusion.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a comprehensive analysis of the estimation bias for expected calibration error (ECE), focusing on two common binning strategies: uniform mass and uniform width binning. The analysis establishes upper bounds on the bias, resulting in an improved convergence rate. Furthermore, these bounds reveal the optimal number of bins needed to minimize the estimation bias. The study also extends the bias analysis to generalization error analysis using an information-theoretic approach, deriving upper bounds that facilitate numerical evaluation for recalibration methods based on training data. Experiments with deep learning models demonstrate that the bounds are nonvacuous, due to the information-theoretic generalization analysis approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
As the author pointed out, the existing literature lacks a theoretical analysis of the estimated ECE and a more principled approach to estimation. This paper addresses and closes this gap.

Weaknesses:
1.	Tightness issue of the upper bound in Corollary 1. It is commendable that the authors included a discussion on the tightness of Equation 12. However, it would be more rigorous to formally establish a minimax lower bound for the estimation bias that applies to all types of estimators. The authors could either use existing results from Tsybakov [33] or construct a worst-case analysis using Le Cam’s method to establish the lower bound. While it is acceptable if the constant does not match the upper bound, it is crucial to demonstrate the rate.

2.	A drawback of information-theoretic (IT) bounds is the implicit dependency on the algorithm. For example, Theorem 7 appears very similar to Theorem 4, as the recalibration-induced dependence is encapsulated in the CMI term. The authors should provide more commentary on this aspect and clarify the connection between Theorems 6 and 5, as well as which bound is more practical for use.

3.	In the caption of Figure 1, It is said that the ECE gap does not change significantly in B. How can we justify that the selection of $B = n^{1/3}$ is better? Figure 1 primarily plots the bound in (14), but as I mentioned earlier, such a bound can be very loose, and more empirical justification should be provided for the selection of optimal B.

Limitations:
The Limitations are well addressed in section 7.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ylceJ2xIw5;"REVIEW 
Summary:
This paper introduces a new data distillation technique called Fair Wasserstein Coresets. The general idea is to create a synthetic core set along with sample weights to represent a larger dataset, by minimizing the Wasserstein distance between core set and dataset, while ensuring a fairness constraint is satisfied. The paper develops a majority minimization algorithm for this Wasserstein problem and empirically validates it on several data sets demonstrating a competitive fairness utility trade-off.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The Wasserstein problem is well-formulated with theoretical guarantees.
- The connections with k-medoids is intuitive.

Weaknesses:
- I suspect there is a potential error in Proposition 2.1, specifically pertaining to the inputs and outputs defined in these functions. Note that z consists of inputs ($d, x$) and outputs ($y$) of the NN, whereas $g_{\psi}$ is an MLP, i.e., it is a function that takes only $(d, x)$ as input. From [69 (original reference)], the MLP satisfies the Wasserstein inequality but only on the marginal distributions over p_{(x,d)}  rather than over $p_{Z}$. This may be resolved if we consider not the Wasserstein distance of the MLP output, but instead the Wasserstein distance of the function $h(z) = | g_{\psi}(x,d) - y |$.
- What do you mean in Lemma 3.1 that the corset is “no better than the best fair Wasserstein corset formed by $m |D||Y|$ data points”? I suspect you mean better with regard to achieving a lower Wasserstein distance, but please clarify.
- The empirical analysis in Figure 1 is hard to parse. Can you measure the Pareto frontier from all of the observations and demonstrate that FWC is dominant? FWC seems Pareto efficient for Adult, Crime, and Drug, but not Credit potentially — but it is hard to see.
- It is hard to understand the trade-offs between accuracy and disparity in the LLM experiments in Table 1, just by reporting these numbers. How important is it that the disparity dropped by 0.009 at a 2.97 point loss in accuracy? Again, it would be important to demonstrate some Pareto efficiency. Furthermore, the change in accuracy and disparity do not seem statistically significant based on the SD reported.

Limitations:
Limitations are thoroughly discussed in the Appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper gives an algorithm to generate smaller weighted synthetic dataset from real data set such that the synthetic data can enforce demographic parity when used for downstream tasks. This is achieved by solving an optimization problem of minimizing the Wasserstein distance between the two dataset distributions along with demographic parity-based fairness constraint. The authors describe how to efficiently solve this problem by reformulating it and subsequently using a majority minimization algorithm to solve the resultant nonconvex problem.  They provide convergence guarantees for the algorithm and also generalization bounds for the solution. The theoretical results are supported by experiments on real and synthetic datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The paper for the most part is written clearly with some minor writing issues (see weaknesses). It is well structured and not too difficult to follow the high-level ideas. Both fairness and scalability are relevant issues so the paper will be of interest to the community. 

2) I could not check all proofs, but the theoretical results appear sound. The connection between the unconstrained problem and Lloyd's algorithm for $k$-means is neat. 

3) The authors have performed experiments on both real and synthetic datasets and compared with a number of existing methods. As such the paper is a good mix of theory and practice.

Weaknesses:
1) The paper seems to borrow a lot of ideas and proof techniques from existing works like [56], [71] and others. for e.g. the reformulation, ideas to speed up the algorithm etc. As such I am not entirely sure about the novelty quotient of the work. It would be better if the authors can highlight why the modifications to techniques from existing works are non-trivial. 

2) The explainability of the synthetic data will be very less. Specifically, as far as I understood, the authors are assigning the output label and sensitive attribute value to the generated data points just in same proportion as that in the original data. It is not clear to me what does this mean for the individual synthetic data points. Also do the features in the generated synthetic data correspond exactly to the features in original data?

3) I suggest the paper be proofread for minor corrections in writing: E.g.: In the contributions make the 'w' 's capitalized. On line 171 the authors say $P \geq 0$ (which I think means each entry in non-negative) while on line 258 it is $P \geq \mathbf{0}$. Maintain the consistency. 

4) Should not the weight of the synthetic data sum to $n$ and not $m$ (line 141 $\Delta_m$)? Typically, we try to preserve the weight of the original data in expectation while reweighing the sampled points.  Please Clarify.

Limitations:
See Weaknesses

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed to extract coresets from a set of data samples using Wasserstein distance with fairness constraints. The authors formulates this problem as a minimization with linear constraints. The coreset selection is over the whole input space, not just from original data samples. The importance / weight of each coreset sample are also optimized. Extensive experiments show this method achieves better fairness-utility tradeoff, and can be applied in LLMs to reduce bias.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is nicely written and easy to follow. I appreciate the detailed steps and neat reformulations of the optimization problem. Theoretical guarantees are provided. The experiments supports the effectiveness of the proposed method very well.

Weaknesses:
1. In section 4.2 (line 203), how can problem Eq.(12) be separated into subproblems as in Eq.(13), are the optimal solutions of all subproblems the same and equal to the solution to (12)? 
2. In section 6 (line 259), why are the minimizers of problem (17) always has only one non-zero entry in each row? As problem (17) can be seen as a relaxed version of discrete Kantorovich problem, where we can't say anything about the sparsity of the optimal plan. Please elaborate.

Limitations:
The authors included detailed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper talks about ""fair Wasserstein coresets"", weighted representative points generated to represent the original datasets. The goal is to meet two purposes: 1) the Wasserstein distance of the coreset and the input data set is minimized, 2) fairness in terms of demographic parity. Having a small Wasserstein distance can help to bound the downstream discrepancy for ReLu activated perceptrons.

The authors formulate the problem as an optimization problem (4). 
There are four steps:
	1. Manually set the proportion of each combination of decision (Y) and feature (D). 
	2. Formulate linear constraints for the fairness constraint. This one borrows directly from [71].
	3. Formulate the Wasserstein distance optimization by using [56]
	4. Simply further

After that, the problem is not convex. The authors use ""majority minimization"" [52, 38] to solve it. Specifically, one defines a convex surrogate function that upper bounds the non-convex function, and optimizes the convex function. 

Section 5 reports theoretical guarantees: running time of the algorithm, convergence guarantee for the surrogate function, and last bound the generalization guarantees. 

Experiments are reported in the last section:  e.g., improving fairness in LLM. 

On the positive side, the problem formulation is interesting and valid,  Wasserstein coreset with fairness consideration. The use of this coreset for downstream applications make sense. Thus the problem and solution have merit. Experiment are thorough. 

The weaknesses (or limitation in significance) is that both crucial steps (2) and (3) are basically using prior work. The theoretical results are standard. 

Summarizing I feel that the paper is OK but would give a weak accept.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
On the positive side, the problem formulation is interesting and valid,  Wasserstein coreset with fairness consideration. The use of this coreset for downstream applications make sense. Thus the problem and solution have merit. Experiment are thorough.

Weaknesses:
The weaknesses (or limitation in significance) is that both crucial steps (2) and (3) are basically using prior work. The theoretical results are standard.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yktQNqtepd;"REVIEW 
Summary:
This paper presents a new task named object-centric occupancy completion as a fine-grained object representation to supplement the coarse-grained 3D bounding boxes. To accomplish this task, a new dataset, which annotates instance-level high-resolution occupancy, is created in an automated pipeline. This paper also introduces an implicit shape decoder to fuse multi-frame information, predict instance occupancy and refine 3D bounding boxes. Experiments on Waymo datasets above several baselines demonstrate the effectiveness of the proposed method on both occupancy prediction and 3D detection.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	This paper is well-written and organized.

2.	A novel task, occupancy augments 3D object detection, and a corresponding new instance-level occupancy datasets is proposed.

3.	A implicit shape decoder is proposed and achieves great improvements both in occupancy and 3D detection.

Weaknesses:
1.	The motivation of this paper does not seem to be very reasonable. The authors claim that a. high-resolution scene-level occupancy is constrained by computational cost and foreground objects is more import, and b. 3D detection is too coarse to capture the object geometry information. So why not just predict foreground instance-level occupancy in the whole scene, instead of pursuing higher detection accuracy by using the occupancy results? 
2.	Time and memory cost bought by the proposed shape decoder are not provided. The paper is trying to make a trade-off between occupancy and detection in fine-/coarse-grained level and computational cost level. But the authors only report the occupancy and detection accuracy.
3.	Some methods, like VoxelNeXt, FSDv2, HEDNet are missing and are not compared in Table 1.
4.	Typos/mis-leading descriptions. For example, ‘Tab. 5.4’ on line 351 -> ‘Tab. 3’.

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors propose a novel task called object-centric occupancy.
It extends the 3D detected bounding box representation to provide a more detailed description of the internal object shape.
The method provides higher voxel resolution in large scenes by focusing on foreground objects only. 
It not only achieves state-of-the-art performance on shape completion but can also help refine the object detection tasks on the Waymo Open Dataset (WOD).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The motion of the proposed task is clear, and the task itself shows good potential in scene understanding. It can enhance 3D detection results even at a far distance.
- The extensive ablation studies validate each contribution. Various detector results with different settings help prove the robustness of the proposed methods.
Using implicit representation from a 3D reconstruction task to complete shapes is neat and interesting. It will be interesting to see how this work can be applied to popular 3D Gaussian representation.

Weaknesses:
- The experimental results are only obtained on the Waymo Open Dataset. It will be nicer to conduct the experiments on nuScenes or Argoverse 2 to validate its robustness for different datasets.
- Although the authors say it is a new task, so there are no learning baselines for shape completion, it will be interesting to compare the results with other scene occupancy methods. So that we can see the flaws of using coarse resolution quantitatively.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The manuscript introduces the idea of representing the shape of objects at higher fidelity (and independent of) the rest of the scene. This is explored in the context of autonomous vehicles research on 3d car detection and representation. The proposed model regresses a shape code and an updated 3d bounding box from a 3D bounding box tracklet (derived from any other algorithm) and the points included in it. The shape code can be queried for occupancy to produce a full shape representation during inference. The proposed approach is able to infer complete shapes from partial inputs and the updated 3D BBs improve the input 3D BBs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed approach is relatively straightforward, effective and well motivated. This makes it reusable for other works and the paper more reproducable.

The manuscript is well written and the illustrations help convey the message and improve understanding of the written parts. 

The evaluation is comprehensive and the sensitivity studies are well chosen and help motivate architecture and training choices. In particular it is great to see that the addition of the high resolution shape code and updated 3D BB does lead to substantial performance improvements on the 3D BB detection task (especially for far away OBBs). And that the shape code (if given the GT OBB) does produce a high IoU occupancy grid even if the input 3D BBs are subpar (table 1).

Weaknesses:
The manuscript's related work section misses out on an existing related field of 3D CAD model retrieval (which also produces complete shapes) and shape regression from RGB (and depth data) in indoor scenes. Relevant related works include: 
    - Scan2CAD https://openaccess.thecvf.com/content_CVPR_2019/papers/Avetisyan_Scan2CAD_Learning_CAD_Model_Alignment_in_RGB-D_Scans_CVPR_2019_paper.pdf
    - SLAM++ https://www.doc.ic.ac.uk/~ajd/Publications/salas-moreno_etal_cvpr2013.pdf
    - FroDO https://openaccess.thecvf.com/content_CVPR_2020/papers/Runz_FroDO_From_Detections_to_3D_Objects_CVPR_2020_paper.pdf

I would have wanted to see a few renderings of the shape codes; This would support the claim that the model learns to complete shapes. The appendix has a few but the visualizations are hard to understand without a better renderer. Some kind of shading or edges for the 3D voxels are essential to see any kind of depth and thus shape (Fig 6 and 7). Extracting a mesh using marching cubes at the 0.5 isolevel might also work.

Limitations:
The limitation of rigid objects only is addressed in the appendix. This means humans are not supported for example.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the limitations of 3D object bounding box representations in autonomous driving by introducing object-centric occupancy. It uses an implicit shape decoder to manage dynamic-size occupancy generation. The method demonstrates robust performance under noisy conditions, significantly enhancing detection results in the Waymo Open Dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The presentation is well-executed, with figures and charts effectively aiding reader comprehension.
2. The overall performance is impressive, demonstrating significant improvements across multiple baselines.

Weaknesses:
1. Creating detailed occupancy for each object seems unnecessary. In most downstream tasks in autonomous driving, using bounding boxes (bboxes) is sufficient.
2. The performance improvement primarily stems from temporal feature fusion, which lacks significant technical innovation.
3. It is unclear whether the loss on occ heads in Fig. 4 enhances detection performance. The authors should compare detection performance with and without occ heads after obtaining the Shape Emb. Z to determine if occ heads contribute to learning useful features, such as yaw estimation.

Limitations:
The authors discuss the limitation concerning non-rigid objects, which is indeed a constraint. They could start by exploring whether reconstructing the noisy occupancy of non-rigid objects improves detection performance.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ykQnxko1cJ;"REVIEW 
Summary:
This paper proposes CemiFace, a novel diffusion-based approach for generating synthetic face images with varying levels of similarity to their identity centers. The authors argue that semi-hard negative samples, those with moderate similarity to the center, are crucial for training effective face recognition models. The core of CemiFace lies in its ability to control the similarity between generated images and the input (identity center) during the diffusion process. This is achieved by injecting a similarity controlling factor condition (m) that regulates the similarity level. The paper presents a comprehensive analysis of the relationship between sample similarity and face recognition performance, showing that semi-hard samples, generated with m close to 0, achieve the best accuracy.
CemiFace demonstrates significant improvements over previous methods in terms of accuracy, particularly on pose-sensitive datasets. The paper further validates its effectiveness through qualitative visualizations and ablation studies that examine the impact of various factors, including training data, inquiry data, and the similarity controlling factor. Overall, this paper contributes a valuable approach to generating synthetic face datasets for face recognition with enhanced discriminative power. The method shows promise in mitigating privacy concerns associated with collecting and using real-world face data while maintaining robust recognition performance.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Discovery of the importance of similarity control in synthetic face generation: CemiFace is motivated by the discovery that face images with certain degree of similarities to their identity centers show great effectiveness in the performance of trained FR models. This is an important discovery to the community of synthetic dataset generation. 
Unique use of similarity control: CemiFace introduces a similarity controlling factor (m) within the diffusion process, enabling the generation of faces with varying levels of similarity to the input image. This provides a fine-grained control over the generated data distribution, which is a unique feature compared to existing methods.
Comprehensive analysis of similarity: The authors present a thorough analysis of the impact of different similarity levels on face recognition performance, validating their hypothesis about the importance of semi-hard samples. This analysis provides valuable insights into the relationship between data distribution and model effectiveness.
Rigorous experimental evaluation: The paper conducts comprehensive experiments across various benchmark datasets and data volumes, comparing CemiFace with other state-of-the-art synthetic face generation methods. The ablation studies provide a detailed understanding of the influence of different parameters and factors on the model's performance.
Robustness of CemiFace: The experiments demonstrate the robustness of CemiFace to different training data, inquiry data, and similarity controlling factors. The method consistently achieves superior results, demonstrating its effectiveness and generalizability.

Weaknesses:
- An in-depth discussion on why face images with certain similarity is more beneficial as a training dataset for the face recognition model would strengthen the paper. For example, an analysis such as a similarity comparison the the real dataset and checking if the difficulty of the CemiFace synthetic dataset becomes closer to that of the real dataset would be nice. Other analysis that offers insights as to why certain similarity control is important would also be welcome.

Limitations:
- This is a well written paper with a meaningful discovery on training with synthetic dataset. The paper would be better positioned in the venue NeurIPS if it would offer more insightful analysis on why similarity control is beneficial, on top of the empirical benefits.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces an approach called CemiFace for generating synthetic face images to enhance face recognition (FR) models. The paper provides the first in-depth analysis of how FR model performance is influenced by samples with varying levels of similarity to the identity center, focusing particularly on center-based semi-hard samples. The authors propose a unique diffusion-based model that can generate face images with different levels of similarity to the identity center. This model can produce infinite center-based semi-hard face images for synthetic face recognition (SFR). The method can be extended to leverage large amounts of unlabeled data for training, providing an advantage over previous methods. Experimental results demonstrate that CemiFace significantly outperforms existing SFR methods, reducing the GAP-to-Real error by half and showcasing promising performance in synthetic face recognition.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Focusing on center-based semi-hard samples to enhance face recognition performance is a fresh problem formulation that addresses a notable gap in current methodologies.

- The paper provides a solid experimental validation of its proposed approach. The authors investigate factors affecting performance degradation in synthetic face recognition and offer a hypothesis about the importance of mid-level similarity samples.

Weaknesses:
- The method for determining GtR remains unclear. Justification regarding how the proposed model yields a low GtR is absent. Is this low GtR attributed to the utilization of real inquiry images? If so, what measures guarantee that the synthetic facial images remain uncorrelated with the real facial images? In other words, I have a reservation that the method may not generate ""true"" synthetic data but highly relies on an inquiry image. Therefore, it is reasonable to see why a low GtR is obtained.

-  Figure 5 demonstrates that different identities (such as different genders) can be obtained with different m, even with the same input query. There seems to be no way to control the ""number of identities"" generated from this model. If so, how was the supervised loss applied to train a face recognition model? 

- How can one ensure high inter-class and large intra-class variations as required for SFR?

- B.3.3. The assertion that high-quality data is not indispensable for achieving markedly accurate facial recognition performance is somewhat counterintuitive and perplexing.

- The method's reproducibility raises concerns, particularly with respect to the training of the model, which lacks clarity. Specifically, the functions F_1 and F_2 in Equations (6) and (7), as well as the role of C_att, are not explicitly defined, and these elements are absent from Figure 3.

The proposed model generally lacks controllable factors to generate true synthetic face images that favor high inter-class and intra-class variations.

Limitations:
No issues are found in this aspect.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel approach named C to address privacy concerns in face recognition technology. The authors propose CemiFace, a diffusion-based method that generates synthetic face images with controlled similarity to a subject's identity center, enhancing the discriminative quality of the samples. This approach allows for the creation of diverse and effective datasets for training face recognition models without the need for large-scale real face images, thus mitigating privacy risks. CemiFace outperforms existing synthetic face recognition methods, significantly reducing the performance gap compared to models trained on real datasets. The paper also discusses the potential limitations and privacy implications of the approach, highlighting the need for ethical considerations in synthetic face generation for face recognition applications.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The use of a diffusion-based model for generating semi-hard samples is an innovative approach that has not been extensively explored in the field of face recognition.
2. The approach can be extended to use unlabeled data for training, which is an advantage over previous methods that often require some form of supervision.

Weaknesses:
1. The paper is not well organized. This paper should be reorganized to make it easier for the reader to understand the contributions and technical details of this paper.
2. Eq. 10 seems to be inconsistant to its description. According to the description, it is highly related to the time step.
3. Fig. 3 is hard to understand. The training losses are not illustrated in the figure.
4. Despite aiming to reduce privacy issues, CemiFace still uses a pre-trained model that could have been derived from datasets without user consent, raising ethical and privacy concerns.

Limitations:
Refer to weakness

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper titled ""CemiFace: Center-based Semi-hard Synthetic Face Generation for Face Recognition"" addresses a critical issue in face recognition (FR) related to privacy and performance degradation when using synthetic face images. The authors propose a diffusion-based approach, CemiFace, which generates facial samples with varying levels of similarity to an identity center. This method aims to enhance the discriminative quality of synthetic samples, thereby improving the performance of FR models trained on these datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Introducing Similarity controlling factor in synthetic face generation using a diffusion based approach.

Weaknesses:
(a) Due to the introduction  of this similarity control conditioning in the diffusion process there must be a  change in total sampling time ( certainly it will also depend on the number of  time steps considered in the diffusion process also)  – An illustration/analysis  on computational complexity of the proposed algorithm is needed.

(b)  Seems like the overall process is dependent on how(using which method) the value of m was determined during the diffusion process! 

(c) A complete pseudo-code on the proposed method would have helped the reader to understand the whole process. 

(d) Figure 3 could have been much more elaborated   and in more details.

Limitations:
Limitations were mentioned only in the last few sentences of the conclusion section but not otherwise stated separately.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new Face Recognition diffusion-based generation method. The diffusion process is completed with a semi-hard constraint on the synthetic reconstructed image: for each inquiry image of the (real) training set, the reconstructed image after the forward-backward diffusion process must have a specific cosine similarity with the inquiry image.
As it is usual for such methods in Face Recognition, the resulting synthetic dataset is then used for training a Face Recognition model. This model is evaluated across diverse real datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The tackled problem is quite hard and needed at the same time. Current SOTA Face Recognition generation methods lead to a significant gap in terms of performance, compared to real Face Recognition datasets (of the same size).
The idea of controlling the similarity to design semi-hard samples is also interesting.

Weaknesses:
1) In Fig. 1, is the displayed similarity really the cosine similarity ? In the generated samples, the line with perfect similarity (equal to 1) seems to provide synthetic images which would not have a perfect similarity with the inquiry images displayed above the hypersphere.  

2) [minor] In Eq. 3, the probability distribution of epsilon is not specified.

3) The authors should cite explicitly the works that use the training loss (Eq. 2) in this precise form, as there are alternative loss functions for diffusion models. A discussion on the reasons of this particular choice of diffusion loss might be a plus (e.g. in the appendix).

4) [minor] Although the lines 114-116 are accurate, they are misleading the reader. The widely known representation of Face Recognition embeddings is that they lie onto a hypersphere of dimension N, where each embedding is a point of the hypersphere. Those embeddings are clustered by identity on this sphere and the identity centers are roughly at the center of those clusters. The hypersphere mentionned in this paper is a hypersphere of dimension N-1, where the identity center is at the center of the sphere. 

5) In lines 120-125, the authors should detail the range of similarities to the identity center, for each of the 5 splits of the CASIA training set. Only the average similarity of each split is specified.

6) [minor] Figure 3 should be a bit more explained than just its caption.

7) [major] Lines 155-162 are not well written and it is hard to understand how the margin m is used to guide the diffusion process. In particular, F_1 and F_2 are not defined, while some unused F is mentionned. C_sim seems to be a vector of unknown size. Also, the temporal guidance is too briefly described.

8) [minor] Some hyperparameters' values (alpha_t/beta_t, lambda) are not specified.

9) The right part of Fig. 4 displays two curves that do not have the same meaning for the x-axis. For AVG, the similarity is a constrained similarity (m) for training CemiFace (i.e. a similarity between a real inquiry image and a synthetic image). For CASIA, it is the similarity between one real image and its identity center (not a real image). To sum up, for AVG it is a similarity between 2 images, while for CASIA it is between 1 image and its identity center. Thus, comparing the two curves does not seem meaningful.

10) [major] The CosFace loss is used to train on synthetic datasets, while AdaFace is used to produce (identity-oriented) embeddings for the CemiFace training set generation. There should be only one model for both tasks, for fair comparisons. On Table 6, training on CASIA with AdaFace gives better results than with CosFace, so one could attribute the good performance of CemiFace to the fact that the authors used a stronger model (AdaFace) to generate the synthetic dataset than the model used to train on this dataset (CosFace). In addition, there should be a part studying the impact of this AdaFace choice (i.e. another loss), at least in the appendix.

11) The ROC curve on IJB-B/IJB-C for all synthetic methods of Table 6 would be a plus, as the accuracy is easily saturated, and not really used in industrial use-cases. Previous papers (related works) provide such ROC plots.

Limitations:
1) [major] The SimMat loss seems to be an interesting idea to lead towards m-similarity to the inquiry image, during training. But the derivation of the MSE loss (Eq. 3) assumes that the reconstructed image should be the inquiry image, and not a new image having a m-similarity with the inquiry image. I may be wrong here but I think that the diffusion loss of Eq. 3 is mathematically valid if the forward diffusion process is symetric to the reverse diffusion process, which is not the case here.

2) [major] In Section 4.2.1, there should be a part studying the difference between the required m and the estimated m post-training. That means that, for any required m, it is easy to compute the estimated m, i.e. the  similarity between the resulting reconstructed image and the inquiry image. This estimated m must be quite different than the required m because Figure 4 shows that the best m is m=0, meaning that there is a 90 degrees angle between the inquiry image and the synthetized image. If m was truly equal to 0, the performance of the model would be very poor. So, there must have a difference between the required m and the real estimated m (post-training).
This is also reflected in the conclusion saying that the best setting for m is to train CemiFace with m randomly sampled from [-1,1]. A similarity truly equal to -1 would bring a model with an astonishingly poor performance.

** Update **
I have increased my score from 4 to 5.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ykACV1IhjD;"REVIEW 
Summary:
This article finds that the existing UL-solvers will trap into local optima and face rounding issues. This study proposes a continuous relaxation annealing (CRA) strategy and an auxiliary function to facilitate training.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The method proposed in the article is sound, easy to implement, and effective.
2. The article is well-written.

Weaknesses:
There are no major drawbacks in this article. There should be more reviews on neural combinatorial optimization solvers that apply annealing ideas as well (such as [1] provides annealing on the distance matrix of TSP).

[1] Lin, Xi, et al. """"Continuation path learning for homotopy optimization."""" International Conference on Machine Learning. PMLR, 2023.

Limitations:
Well discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The proposed approach is an optimization method for each graph over GNN parameters where each output corresponds to the likelihood of the node belonging to the solution. The objective function consists of a penalty term along with a parameter scheduled to control the non-convexity of the objective.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1- The convex annealing approach proposed in training that controls the level of non-convexity. This is a valid approach to avoid getting trapped in local minima where the solution sizes are not large. 

2- Theoretical results of the limiting points of the proposed objective with different \gamma. 

3- The ""no-data"" requirement makes this method mostly generalizable, depending on tuning a set of hyper-parameters for each graph distribution.

Weaknesses:
[Major Comments]

1- The need to solve graph-based NP-hard problems that are originally formulated as ILPs stems from the unscalability of these solvers. For example, the scalability of the MIS problem depends on the number of nodes and the number of edges in the graph. This needs to be the motivation instead of the issues encountered in UL-based solvers.

2- While the proposed approach does not require training data (labeled or unlabeled), there are several hyper-parameters. Tuning these hyper parameters is a challenge. Further discussion is needed here. 

3- Getting trapped in local minima is not only the case in GNNs or PI-GNNs. It exists for any continuous relaxation of Problem 1. This is due to the non-convexity inherited in these formulations. For example, if we re-write Problem (3) in matrix form, we can see that the objective has a constant hessian equal to the adjacency matrix of the graph. If the magnitudes and signs of the eigen values vary significantly, then this indicates possible positive and negative curvatures in the loss landscape. Replacing x in Problem (3) with the output of a GNN does not guarantee changes. Although it may be possible that it will make some local minima avoidable by adaptive optimizers (such as ADAM), there is a possibility that this type of overparameterization would create unwanted local minima that do not result in any feasible solutions. Theoretically analyzing this is very complicated due to the use of a GNN. However, empirical investigation can be used to better motivate and understand the proposed approach. 

4- Similar to the previous point, rounding issues existed even before GNNs. See the SDP relaxations of MIS [1] and MaxCut [2] and how their dependence on rounding techniques (e.g. spectral clustering [3]) often fails to obtain optimal solutions. Rewriting is needed here. 

5- The Stationary point p* = 0_n was not discussed in Section 3.1. Furthermore, in line 208, it is 0_n, whereas in line 2016, it is 0_N. 

6- How was the GW approximation applied for the MaxCut problem? This approximation requires a normalized random vector drawn from the standard Gaussian distribution. How many samples were drawn? Given the SDP solution, one can simply draw multiple samples and pick the best where the only requirement is matrix-vector multiplication. This runs extremely fast with (i) no parameters of a NW, and (ii) no hyper-parameters to tune. The scenarios where such approaches fail need to be the motivation to propose the over-parameterized approach with convex annealing. 

7- Missing many ""data-independent"" baselines (methods that do not require pre-trained models (such as DIFUSCO [4]) or training data such as RL-based solvers (LwD [5])) for comparison such as ILP solvers (Gurobi, CPLEX, or CP-SAT [6]), sampling methods such as iSCO [7], SOTA heuristics such as ReduMIS [8], and differentiable solvers such as [9]. 

8- Why does the paper only consider d-regular graphs? How about the performance on other graphs? How does the run-time of this method scale in terms of the graph order and density? This is a major limitation of this work.

[Minor Comments]

1- What is script C in line 83?

2- What is I and J in the equation after line 86?

3- ""nural"" in line 106.

4- Cite equation 3. An example is [10]

5- Paragraph 149 to 151 is ill-sentenced.

6- “Indeed” in line 232.

7- Cite Potts variable optimization.

8- This study “employs” in line 237.

9- ""are"" in Appendix F.3 in line 247.

[References]

[1] On the shannon capacity of a graph. IEEE TIT, 1979.

[2] Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. JACM, 1995.

[3] A tutorial on spectral clustering. Springer, 2007.

[4] Difusco: Graph-based diffusion solvers for combinatorial optimization. NeurIPS, 2023.

[5] Learning What to Defer for Maximum Independent Sets. ICML, 2020.

[6] https://developers.google.com/optimization

[7] Revisiting sampling for combinatorial optimization. ICML, 2023.

[8] A differentiable approach to the maximum independent set problem using dataless neural networks. Neural Networks, 2022.

[9] A branch and bound algorithm for the maximum clique problem. Computers & operations research, 1992.

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to tackle shortcomings of the existing unsupervised learning-based solvers for combinatorial optimization, namely the local optima issue and the rounding issue. It proposes a novel technique called continuous relaxation annealing (CRA) strategy which introduces an additional penalty term to smooth the non-convexity of the objective function. This strategy is empirically shown to not only enhance the solution quality but also accelerate the learning process.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is an interesting study on the unsupervised-learning based approaches on CO problems. The proposed method is simple but proves to be quite effective.
2. The empirical evaluation shows that CRA achieves a consistent improvement over PI-GNN 
3. The authors have conducted extensive qualitative and quantitative analysis to help understand the proposed method

Weaknesses:
1. My main concern lies in the technical contribution from this paper. The whole framework and empirical evaluation is built upon PI-GNN, which makes the observation and conclusion from this paper not generalizable.
2. I feel the research from this paper is kind of out-of-date. Check https://openreview.net/forum?id=ZMP0Bki9aK for SOTA results on the CO problems considered in this paper. In fact, [1] also mention that simulated annealing would perform better than GNN, but only greedy methods are used as baselines.


[1] Maria Chiara Angelini and Federico Ricci-Tersenghi. Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set. Nature Machine Intelligence, 5(1):29–31, 2023.

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a heuristic method for producing solutions to combinatorial optimization problems, which is based around solving a continuous relaxation of the problem. The main focus of the paper is on an additional penalty term to add to the objective of this relaxation which aims to reward solutions that are closer to satisfying the integrality constraints on the decision variables.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The computational study seems relatively comprehensive in that it studies a number of different problem settings in a fair amount of detail.

Weaknesses:
The paper is very dense and hard to follow, with little context provided to the reader. The method presented and evaluated in the computational study is ultimately an extension of the ""PI-GNN"" solver, but this fact is oddly kind of buried, with only an indirect reference in the introduction (""the solver that applies the CRA to the PI-GNN solver is referred to as the CRA-PI-GNN solver"", with no indication that this is a main takeaway from the work), and then again at the end of Section 3.2. The paper does not explain in detail or formality what the PI-GNN solver is or how it works (how, specifically, does the CGA actually hook into PI-GNN?), and so a reader without prior familiarity cannot really understand or assess the new contributions laid out in Section 3. Ultimately, I do not feel confident that I can understand, and thus evaluate, the contributions proposed in the paper.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";1
yiXZZC5qDI;"REVIEW 
Summary:
The paper proposes a new poisoning attack for diffusion models (DMs). While previous work tried to poison/backdoor DMs by altering the training process or the optimization objective, the paper proposes a poisoning attack by only altering the training data. 
To poison DMs, a trigger is inserted into training images, and the labels of the poisoned samples are changed to the target class. The resulting DM, trained on this poisoned dataset, generates images not aligned with the given prompt or images containing the trigger pattern used for poisoning.
Based on this behavior, insights are presented that might help protect DMs against poisoning attacks, and a different view on data replication in DMs is given.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The paper tackles a very important topic as the risk of poisoned data is increasing when training DMs on publicly available data scraped from the web
- The insight that DMs generate images of the target class with the trigger, even though the trigger has not been present in the target class training images, is very intriguing. However, the paper doesn't really give an intuition or explanation on why this is the case (see questions).

Weaknesses:
- Only training details about the Caltech15 dataset are provided in the appendix. (see questions)
- It is unclear how this proposed method can be applied to datasets like LAION or other uncurated/unstructured datasets without clearly separated classes.
- In the experimental setting, it is stated that experiments on CIFAR-10 are conducted. However, in the experimental evaluation, there are results for CIFAR-10. Only ImageNette and Caltech15 are used to show the effectiveness of the poisoning attack. (see questions)
- The paper is sometimes hard to read, and in parts, it is difficult to grasp what the authors want to convey as the take-away message of the paper is not really clear, in my opinion.
- Using the ""poisoned DM"" as a defense against poisoning attacks is not very realistic or applicable, in my opinion. In reality, a DM would first have to be trained to generate data and apply the poisoning detection method to the generated data before even starting to train the classifier. In addition, the improvement of the AUROC for the poisoning detection methods is only very minor (in most cases, less than 1 percentage point improvement of the AUROC value).
- The data replication experiments are not really meaningful, in my opinion. If we look at replicated images, it is expected that these images are replicated more than randomly chosen images. The experiment would be more meaningful if the same images would be once poisoned and once not poisoned. This would give insight into whether the poisoning really affects the data replication abilities of the DM.
- there are two other works [1, 2] that use DMs for defending against poisoning attacks that should be mentioned in the related work part 

[1] Zhou et al., DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models, AAAI 2024
[2] Struppek et al., Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data, NeurIPS 2023 Workshop BUGS

Misc:
- Many of the cited papers are arXiv papers and not the conference versions (VillanDiffusion is NeurIPS, ""Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning"" is ""conference on multimedia"", Rickrolling the artist is NeurIPS, etc.). Please cite the proper conference versions of the papers.
- The titles in the references only include lower characters. This seems to be a bibtex/latex problem.
- Reading ""the work [...]"" is not really smooth. Instead, it would be better to write the author's names as ""Chou et al. have done ....""
- The links in the appendix should be blue to indicate that they are clickable. I almost missed them. Also, it might be preferable to show the URL so the reader knows which site is linked without clicking on the link in the first place.

The paper tackles a very interesting problem, and the discovered phenomena seem to be very surprising. However, in my opinion, the paper is not quite ready for publication because of the unclear take-away message and the sometimes hard-to-read text.

Limitations:
Some of the work's limitations are addressed. However, I think it is important to discuss how realistic it is that the proposed poisoning method is used in such a realistic scenario and not only to investigate the behavior of DMs on poisoned data in an artificial setting. Additionally, the limitations of the proposed defense method should be discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates the impact of BadNets-like data poisoning attacks on state-of-the-art diffusion models (DMs) used for image generation. Unlike previous studies that required modifications to the diffusion training and sampling procedures, this work examines the effects of poisoning the training dataset alone. The study uncovers dual effects of data poisoning, which not only degrade the generative performance of DMs but also provide defensive advantages for image classification tasks. Key findings include the misalignment between input prompts and generated images, the amplification of trigger generations, and the linkage between data poisoning and data replications.

The major contributions of this paper are as follows. It demonstrates that diffusion models (DMs) are vulnerable to BadNets-like data poisoning attacks, leading to two significant adverse effects: (1) misalignment between input prompts and generated images, and (2) an increased generation of images with embedded triggers, referred to as 'trigger amplification'. The study identifies a phase transition in the poisoning effect relative to the poisoning ratio, revealing the nuanced dynamics of data poisoning in DMs. The proposed 'Castle Walls' concept introduces defensive strategies for image classification, including leveraging trigger amplification for detecting poisoned training data, training classifiers with images from poisoned DMs before the phase transition to mitigate poisoning, and using DMs as image classifiers to enhance robustness against attacks. Additionally, the paper establishes a connection between data poisoning and data replication in DMs, showing that introducing triggers into replicated training data exacerbates both the replication problem and the impact of poisoning, thus highlighting the inherent data memorization tendencies of DMs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents an innovative investigation into the impact of BadNets-like data poisoning attacks on state-of-the-art diffusion models (DMs) used for image generation. Unlike previous studies that require modifications to the diffusion training and sampling procedures, this work uniquely focuses on the effects of poisoning the training dataset alone. This fresh perspective uncovers dual effects of data poisoning, revealing both degradation in generative performance and potential defensive advantages for image classification tasks. The introduction of the 'Castle Walls' concept for defensive strategies is original, offering new ways to leverage data poisoning effects to enhance robustness against attacks.

Quality: The quality of the research is reflected in its comprehensive experimental analysis and the depth of its findings. The study methodically demonstrates the vulnerability of DMs to BadNets-like attacks, detailing how these attacks cause misalignment between input prompts and generated images and amplify trigger generations. The paper includes a thorough examination of defensive strategies, including the innovative use of poisoned DMs for training classifiers. 

Clarity: The paper is well-structured and clearly communicates its methodology, findings, and implications. The key concepts and contributions are articulated in an accessible manner, with detailed explanations of the experimental setup and results. While there are minor editorial issues, such as the need for clarification in figure captions and consistent notation, these do not significantly detract from the overall clarity of the paper. The inclusion of detailed figures and tables aids in the clear presentation of the data and results.

Significance: The significance of this work lies in its potential to substantially enhance the understanding and robustness of DMs in the face of data poisoning attacks. By uncovering the dual effects of data poisoning and proposing innovative defensive strategies, the paper provides valuable insights that can inform future research and practical applications. The connection established between data poisoning and data replication highlights the inherent data memorization tendencies of DMs, offering a deeper understanding of their vulnerabilities.

Weaknesses:
Additional statistical analysis (e.g., confidence intervals) could strengthen the findings by accounting for variability and ensuring the observed improvements are statistically significant.

Experimental Robustness: The lack of reported error bars due to computational expense raises concerns about the robustness and representativeness of the experimental results. Without statistical measures of variability, it is challenging to assess the reliability of the findings. Constructive suggestion: Provide some supporting evidence or alternative measures to demonstrate the robustness of the results, such as reporting confidence intervals for a subset of the experiments.

Comprehensive Defensive Strategies: While the 'Castle Walls' concept is innovative, the practical implementation details of these defensive strategies are not fully explored. Constructive suggestion: Provide more detailed guidelines and examples on how these strategies can be implemented in real-world scenarios to enhance their practical applicability.

Limitations:
The authors have addressed key aspects of their work, but several limitations require further attention to strengthen the paper.

Experimental Robustness: The lack of error bars due to computational expense raises concerns about the robustness of the results. Without statistical validation, it is difficult to ensure the findings are consistent. Constructive suggestion: Include confidence intervals or statistical validation for a subset of experiments to enhance result reliability.

Practical Implementation of Defensive Strategies: The 'Castle Walls' concept introduces novel defenses, but practical implementation details are lacking. Constructive suggestion: Provide detailed guidelines and examples for implementing these defensive strategies in real-world scenarios.

Broader Societal Impact: The potential negative societal impacts of data poisoning are not thoroughly discussed in the main paper. Constructive suggestion: Discuss the broader societal implications and ethical considerations of your findings, including potential misuse and guidelines to mitigate negative impacts.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates backdoor attacks against diffusion models. Unlike previous works that require both injecting poisoned data samples and manipulating the training loss function, this study focuses solely on poisoning training data samples during the training phase. The research demonstrates that backdoor attacks not only compromise the functionality of diffusion models (resulting in incorrect images misaligned with the intended text conditions) but also amplify the presence of triggers, a phenomenon termed 'trigger amplification.' This trigger amplification can be utilized to enhance the detection of poisoned training data, thereby providing a defensive advantage.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-- This paper is easy to follow.

-- This paper demonstrates that simply poisoning the training dataset can effectively backdoor diffusion models.

-- Conduct comprehensive experiments. Impressive results especially in attack success rate.

-- Discuss the limitation of the proposed attack and future work.

Weaknesses:
-- The evaluation of the proposed attacks is limited to 3 datasets: CIFAR10, ImageNette and Caltech15.

Limitations:
Yes, the authors addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies BadNet-like poisoning attacks in diffusion models from both attack and defense perspectives.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. I think the paper makes interesting observations for the community, especially regarding the phenomenon of trigger amplification.

2. The evaluation seems quite comprehensive, considering multiple datasets, models, attacks, and detection methods.

Weaknesses:
1. Even though the authors consider many settings, the experiments are run only once (no error bars are shown).

2. While in Table 4, the attacks' success rates are reduced when the poison percentage is up to 5%, I am wondering if they are amplified for higher poison percentages. If so, how could the defender use this as a defense in practice if they do not have any knowledge about the poison percentage?

3. The paper is fully empirical.

Minor comment: at line 252 ""comapred"" should be ""compared"".

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yhd2kHHNtB;"REVIEW 
Summary:
The paper studies the non-stationary setting in avoiding undesired future (AUF) problems, where environmental shifts can cause the failure of existing AUF methods. It introduces an optimization problem for AUF with minimal action cost in non-stationary environments, formulated as a convex quadratically constrained quadratic program (QCQP) in each interaction. The paper also proposes a rehearsal-based algorithm to solve this problem, providing theoretical guarantees and numerical validations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well-written, introduces a practical and interesting setting for AUF problems, and presents an algorithm with theoretical guarantees and numerical validations to address the task.

Weaknesses:
(1) The provided algorithms lack a regret bound (or other theoretical guarantees) on the cost (i.e., the objective function), although it guarantees effective alterations (i.e., the constraint). Since the aim of this work is to avoid an undesired future with minimal cost, a regret bound analysis is, in my opinion, important.

(2) In Theorem 3.3, the estimation error depends on the minimum eigenvalues of  the empirical error functions' Hessian matrices, which in turn depends on the previously taken alterations. This raises a concern about the exploration-exploitation tradeoff when making alterations. An extreme case is making uninformative alterations (e.g., setting 0 for all nodes), leading to no update by Algorithm 1 and rendering the error bound in Theorem 3.3 meaningless (since $\mu_j$=0 in this case if I understand correctly). It is unclear how Algorithm 3 addresses this tradeoff and how $\mu_j$’s can be bounded  below.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors address decision-making problem that sufficient interactions are not available. In this case, RL is not suitable. The authors model the structure among the observed variables, and use the structure to help the decisions. Compared to the previous studies [Qin et al. 37], the method can be used in a dynamic environment and can efficiently find the suggested decision (in polynomial time). To deal with the dynamic environment, they introduce the online learning method (Alg. 2). To efficiently find the suggested decision, they convert the optimization problem to a QCQP problem, which can be implemented in polynomial time. The experimental results verify the effectiveness.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The method of Qin et al. [37] suffers a high computational cost. In this paper, the authors convert the problem to a QCQP problem, which makes it computable in polynomial time. It is a valuable contribution.

2. Theorem 3.3 presents an interesting and sensible theoretical guarantee. It is novel to see that some traditional online learning methods could be used in such decision tasks.

Weaknesses:
Some discussion about the offline RL are missing. See Questions for the details.

Given the results of theorem 3.5: I do not know where $\tau$ is reflected in your algorithm. It seems that $\tau$ is never mentioned in Section 3.3. It is a bit wired, and needs more illustrations.

The writing could be improved. There are some weird sentences. I suggest the authors carefully revise the paper. For example, ""We provide the theoretical guarantees of our method, and experimental results validate the effectiveness and efficiency of the method."" -> ""We provide the theoretical guarantees for our method. Experimental results validate the effectiveness and efficiency of the method.""

Limitations:
No limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors formulate the Avoiding Undesired Future (AUF) problem in real-world scenarios of decision-making, especially in non-stationary environments, and propose a method to avoid undesired outcomes with minimal costs. Here the non-stationarity majorly comes from the different costs corresponding to different actions, and the varying influence relations over time. They also provide theoretical guarantees of their method and empirical results demonstrate the effectiveness and efficiency of the proposal.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- This paper is written well and clearly, with intuitive motivation and clarified novelty.

- This paper includes a complete theoretical analysis and algorithmic design. Their proposed problem formalization is more general and practical than existing methods [37]. In particular, they first proposed a sequential method to maintain the dynamical influence, with guarantees of estimation error bound. They entailed Proposition 3.2 and Theorem 3.5 to help find the efficient alteration for $Z_t$ with the minimal cost. They finally propose the whole algorithm called AUF-MICNS, to avoid undesired outcomes in each decision round.

- Experimental results show the effectiveness and efficiency of their proposed algorithm, where the evaluation metrics are success frequency, estimation error, average running time, etc.

Weaknesses:
I think my major concerns have been settled by the Supplementary Materials. So I have no other comments about the weaknesses.

Limitations:
Not applicable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ygDl8q02gA;"REVIEW 
Summary:
The paper studies the problem of finding a hidden partition into $k$ clusters of a given universe.
In many applications an algorithm has only access to a same-cluster oracle. A query to this oracle reveals whether two elements belong to the same cluster or not. This problem has been previously studied and tight bounds on the query complexity, i.e., the minimal number of queries required to solve the problem, are known (Reyzin and Srivastava, and Liu and Mukherjee).
In this paper, the authors add the realistic assumption that the same-cluster oracle may not always reveal the correct answer. In their model, they (in advance) set a number \ell which bounds the maximum number of wrong answers which the oracle is allowed to make. The goal of an algorithm is still to compute the hidden cluster with as few queries as possible. In particular, for the same tuple of elements the oracle may give different answers for different oracle calls, and the algorithm does not receive any information on whether the response of the oracle was correct or not. The authors present an algorithm and analyze its query complexity. This bound is generally larger than in the setting with a correct oracle, and depends on the parameter \ell. If \ell=0, the presented analysis recovers the results by Reyzin and Srivastava. Furthermore, they give a tight lower bound using an argumentation based on Renyi-Ulam games and correlation clustering.
They moreover study a slightly more general setting where the algorithm can set in advance more fine-grained bounds on how many false positive and false negative answers the oracle can give, and, for all problems they consider both, the setting where the number of hidden clusters k is known or not.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- I think that the problem is important and appreciated by the ML-community, as clustering is a fundamental problem in machine learning. Moreover, the assumption that a same-cluster oracle may not always be correct seems quite reasonable and realistic. Thus, I think that this problem and the presented results could have many applications and an impact in certain areas.
- The authors give a tight analysis of the considered algorithms.
- Despite being tight up to constants, the main algorithm is well-presented, and easy to understand and implement.
- Overall, I think that the paper is well-written and seems technically sound.

Weaknesses:
- I think the main weakness of the model is that the upper bound \ell on the number of faulty oracle responses must be set in advance and stays fixed. This could be a major drawback when applying this model and the algorithm in practice, because it seems not clear why a faulty oracle should be consistent with such a bound. 
- It seems that the main algorithms is quite similar to the algorithm without faulty oracle. I think it would be helpful for the reader to have a paragraph where the difference to this original algorithm is explained. 

Further comments:
- Line 236: missing 'and'

Limitations:
Limitations have been addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem to recover an exact $k$ partition of a set with access to a same-cluster oracle that is allowed to lie $\ell$ times. This papers gives an algorithm with optimal query complexity up to constants and a lower bound.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The result of this paper is clean and complete. The algorithm's query complexity matches the lower bound up to constants.

2. The main algorithm is concise, simple and elegant. The idea of the algorithm captures the problem well and has proved optimal guarantees.

3. The lower bound is non-trivial and has some interesting ideas.

4. This paper is very well-written. The notations and explanations are clear. I didn't even catch a single typo. Enough background and motivation is included in the paper. The paper is cohesive and organized, easy to follow. Math and algorithmic ideas are explained clearly.

5. I think this paper should be a spotlight.

Weaknesses:
I'm very satisfied with this paper, just two things I think it can improve on the writing.

1. The algorithm is quite simple and intuitive. On the other hand, the lower bound is more complicated and more technical. I think it might be better to write less on the algorithm but explain more on the lower bound, especially how to construct a good responder's strategy.

2. I think it's worth mentioning what's the optimal algorithm for the no-error oracle and compare your algorithm with theirs. 

Also something I would not like to call it a weakness since I think it's beyond the scope of the paper:

1.  You justified the inconsistent error assumption (but I think the consistent error assumption can also be justified). But from the pure theoretical point of view, this assumption does make the algorithm design a lot easier and less interesting since the algorithm can make the same query many times. If the error model prevents such behavior of the algorithm, it is more interesting.

Limitations:
The limitation of the paper is well addressed.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
**[Setting]**:
This paper studies the problem of clustering n items into k clusters using an oracle that adversarially answers same-cluster queries for item pairs under the constraint that it makes at most $\ell$ errors for a known constant $\ell$. The goal is to exactly recover all clusters always (instead of just w.h.p.).

**[Contributions]**:
1. A lower bound on the number of queries when k is known/unknown. The authors formulate the problem in terms of Chip-Liar game to get this result.
2. For known k, an algorithm that iteratively merges cluster using two heuristics:
    1. If there is a (k+1)-clique of all -1s then the oracle has returned at least one false negative
    2. More than $\ell$ ""+1"" responses from oracle for a given pair guarantees that it is in the same cluster
3. Sample complexity of the proposed algorithm that matches the lower bound.

The results extend to a more general problem where individual limits on false positive and false negative errors are known.

The paper also studies an algorithm for k-unknown case in the appendix. Sample complexity in this case is not optimal.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem of guaranteed exact cluster recovery in the presence of noise is new. Having a hard limit on the number of errors made by the oracle makes this possible. Given concrete applications, this would be an interesting direction to explore.
2. The sample complexity of the proposed algorithm matches the derived lower bound when k is known.
3. The connection to Chip-Liar game for deriving the lower bound is interesting.

Weaknesses:
1. The problem setting (oracle making at most $\ell$ errors with $\ell$ being a known constant) is not very practical in my opinion, which in-turn makes it hard to judge the significance of the results. Even for the examples given in the paper (L23-32), it is not clear why the oracle will make at most $\ell$ errors (e.g., an experiment failing in bioinformatics) or why $\ell$ will be known in advance. Do the authors have concrete applications in mind?
2. Clarity-wise, while the details in the paper are mostly clear, it would be helpful to include more details from the appendix into the main paper. For example, the following can be included by making Section 3 more concise,
    1. What does ""The position of a chip on the board will then be equal to the cost of the corresponding partition .."" (L234-235) mean?
    2. Some high-level details about the unknown-k algorithm.
    3. Some intuition about why false-negative and false-positive error budgets inherently have a different contribution towards minimum sample complexity

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the query complexity of clustering with a faulty oracle. Given a set of $n$ points $V$, which is partitioned into $k$ hidden clusters, the learner wants to recover the hidden partition by querying whether two points are in the same clusters or not. There has been a line of work that studies the query complexity of the problem where the response of each query has iid error. This paper studies a different query model, where the learner is allowed to make repeat queries for the same pair of points but the responses could be adversarially flipped at most $\ell$ times. This paper provides lower bounds for the query complexity of several variants of the problem and also designs efficient learning algorithms with a query complexity matching the lower bound.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper establishes a novel relation between the clustering problem and the Rényi-Ulam liar games, which could potentially be useful for proving lower bounds for other learning problems.
2. The algorithm designed in this paper involves non-trivial techniques and has a query complexity that matches the lower bound proved in the paper.

Weaknesses:
My main concern is about the significance of the learning model studied in the paper.
For graph clustering problems, the error is usually defined over the graph instead of over the queries, and sometimes repeated queries are not allowed. This is because sometimes by allowing the use of repeated queries, the learning problem could be easy to solve. For example, when iid noise is presented.
In this paper, the error is defined over an unbounded sequence of queries but only allows a constant number of mistakes to happen. In particular, knowing the number of mistakes seems to be very important to make the learning algorithm designed in this paper work. These two points seem to be too idealized to model problems that arise from real applications.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yfQwyxiSJ7;"REVIEW 
Summary:
The authors propose AutoPalette, which reduces color redundancy in dataset distillation. They use a palette network and color-guided initialization to enhance training efficiency and performance by minimizing redundant color information in synthetic images and datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Color redundancy is a fundamental aspect of natural scene images but is often overlooked in large-scale image analysis. This study focuses on the missing part, and the proposed method is effective.

Weaknesses:
- In the abstract, the authors summarize their framework as the one that minimizes color redundancy at the individual image and overall dataset levels. I think that’s a good summary. However, the description is not utilized when they introduce their framework in the main text. Although they describe it in the last section, it would be better to include the summary in the middle of the main, e.g., when introducing an overview or Figure 1.

- I am confused a little about the definition of the color bit in this manuscript. The authors often describe the 8-bits for the original image (e.g., Figure 2). However, if the color bit is based on the number of color palettes, the original image should have 24 bits. 

- Typo: ""> can encoded in fewer bits” should be ""can be encoded”

Limitations:
- The evaluation was mainly based on a relatively small number of image datasets. I’m not sure to what extent the condensed images change when applying recent large-scale image datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a straightforward yet effective dataset distillation method called AutoPalette. The method minimizes color redundancy at both the individual image level and the entire dataset level. At the image level, it trains the palette network by maximizing color loss and palette balance loss, thereby reducing color redundancy in images. At the dataset level, a color-guided initialization strategy is proposed to minimize color redundancy across the entire dataset. Extensive comparative and ablation experiments convincingly demonstrate the approach's effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method outperforms other dataset distillation methods in most tasks, providing a new perspective on dataset distillation.
- The experiments and ablation study seem well done. The paper's experiments are comprehensive, and the results of the ablation studies are convincing.

Weaknesses:
-  The paper could benefit from a more detailed explanation of the color loss and palette balance loss. It would be helpful to include an explanation of why the palette balance loss might achieve a more balanced color palette.
-   The paper does not seem to explain why the similarity between the last layer gradients is measured instead of directly measuring the feature level similarity in the Color Guided Initialization Module.

Limitations:
The authors have discussed the limitations of their work in Section 5. However, they could provide more detailed descriptions of how these limitations might impact the results.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper titled introduces AutoPalette, a novel framework for dataset distillation (DD) that focuses on minimizing color redundancy at both the individual image and overall dataset levels. Authors propose a palette network to dynamically allocate colors from a reduced color space to each pixel, ensuring essential features are preserved. Additionally, a color-guided initialization strategy is developed to minimize redundancy among images, selecting representative images based on information gain. Comprehensive experiments on various datasets demonstrate the superior performance of the proposed color-aware DD compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Color quantization is an interesting way for dataset distillation, the motivation of this paper is interesting.
2. The methodology is well-defined, with clear explanations of the palette network and the color-guided initialization strategy.
3. The framework is shown to be compatible with other DD methods, indicating its potential for broad application.

Weaknesses:
1. The paper does not discuss the potential impact of the method on the performance of larger dataset beyond the CIFAR-10 and CIFAR-100. These 2 datasets are two small and could not show the effectiveness of the proposed method.

2. There is limited exploration of how the method handles imbalanced datasets or classes with unique color distributions.

Limitations:
See weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces ColorPalette, a framework that minimizes color redundancy at the individual image and overall dataset levels. At the image level, the palette networks generate condensed images in reduced color bit-width while at the dataset level, a color-guided initialization strategy is proposed. The experiments are done using various datasets and IPCs.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. A new direction for exploring DC is proposed. 
2. AutoPalette explores the possibility of performing DC in a reduced color space.
The paper is easy to understand.

Weaknesses:
1. AutoPalette seems like it is built on top of [1] with DC loss.
2. Lack of experiment on large-scale dataset ImageNet-1K.

[1] Learning to Structure an Image with Few Colors, Yunzhong Hou et al.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yeFx5NQmr7;"REVIEW 
Summary:
The authors propose a method to transfer the deformations of the observed garments to any other garment. Previous methods either rely on a large-scale dataset for training or analytical physics model with limited expressive ability. On the contrast, the proposed method first learns the constitutive relations from the observation by a neural network (EUNet), then use it as an energy prior to regularize the training of garment deformation model. This design addresses the limitation of previous works and show better results.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The strength of the paper is the proposed method does not need to collect huge amount of data with varied body pose and shape and garment types for training. Through theoretical analysis, they prove that they can learn a more physically accurate energy model to describe the deformation of garment. In this way, they do not need an explicit physical model, which tends to have limited expressive power. The derivation is theoretical sound.

Weaknesses:
To learn this energy model by EUNet, the authors rely on the synthetic data simulated with blender. A cloth with known geometry (vertices and faces) is assigned with a specific material type. However, this setting is too ideal. In real scenarios, we are more interested in transferring the material of a real cloth to another garment. But having the geometry of a real cloth usually is not feasible. Even though we can have the mesh of the cloth through some registration process, how to get the shape of mesh when it is hanging and dangling is still a problem. In this paper, I do not see the possibility of using the proposed method in real applications. This is the critical weakness.

Limitations:
See the weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This submission presents a method that could effectively learn the dynamic patterns of different garments from a single piece of cloth. The key insight is that the motion of different cloths is governed by both external forces and the constitutive relations rather than specific garment topologies. Thus, an Energy Unit Network (EUNet) is proposed to learn the topology independent constitutive laws. Then the animation is performed by energy optimizations. Experimental results shows improved results comparing to previous methods and baseline methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well written and easy to read.
1)	The paper is well structured.
2)	Many terms are well defined and explained.
The proposed method is both novel and interesting.
1)	The disentangled learning scheme of using a network to learn the constitutive law, which generalizes to different garment types, is physically intuitive and natural. More importantly, this design helps alleviate the needs for large amount of training data of various cloth shapes in dynamic for learning based animation.  This disentanglement between topology and energy is achieved by using mesh edge as a unit instead of the whole cloth mesh.
2)	The proposed disturbance training strategy helps stabilize the training and improves the generalization of EUNet. As a constraint, it accompanies the direct supervision on the energy form by taking into account the physical meaning of equilibrium state. This helps the network to learn a more reasonable manifold of the energy distribution.
Experimental results:
1)	Improved results over previous methods are shown both qualitatively and quantitatively.
2)	According to the ablation study, the design of both the contrastive loss and dissipation unit loss are validated as effective.

Weaknesses:
Some details regarding the design of the EUNet is missing. Although some descripsions on the EUNet design is provided at the experimental section, it is relatively hard for the reader to follow and develop a more coherent understanding of the presented work.

Some limitations and open questions that the work might not cover:
What about anisotropic materials? How to adapt the current model design to also fit to cloths where its material is anisotropic? How well does the method handles cloths with more complex topology that goes beyond a single layer of cloth? As also pointed by the authors, the method does not handle self-collision. It would be interesting to see how it can be adapted in that axis.

Limitations:
See weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a method to learn the constitutive model of cloth materials from observed cloth trajectory using a neural network. It adopts an MLP that operates on individual edges and predicts per-edge distortion based on the deviation of edge geometry from rest shape and trains the network using a combination of supervision on potential energy change with ground truth and optimality of incremental potential. The learned potential energy can be used as a constraint to train neural simulators for garment animation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- I appreciate the novelty in the idea of learning the constitutive model of cloth materials in a data-driven manner. Potentially this formulation could allow the neural networks to understand the intrinsic physical property instead of mimicking the behavior of specific examples, thus of scientific significance if implemented correctly.
- The paper is well-written and mostly clear.

Weaknesses:
- On the methodology side, the major question is probably the design of dissipative energy. On the one hand, why it is and is merely a function of \(X^t - X^{t-1}\) is questionable. In fact, whether it should be modeled as an absolute quantity is a question because the total amount of dissipative energy seems not that meaningful. The only observable quantity is the relative change of dissipative energy in a physical process.
- On the other hand, with the presented framework, it is very hard to learn the major sources of energy dissipation: collision, and friction, since they are neither present in the training data, nor fully modeled (e.g. self-collision) in the formulation. While the dissipative energy is not the focus, the problem is that without correctly modeling dissipative forces, I doubt the possibility of learning an accurate elastic potential energy function.
- On the evaluation side, the problem is that the method is only evaluated in a simplified setting, without comparing against methods or in settings that are practically useful (see below). In my opinion, there are two ways to demonstrate that the learned constitutive model is useful: either 1. demonstrate that it is more accurate than an analytical model on real data, or 2. show that it leads to more realistic animation than existing methods (including traditional numerical models).
- The evaluation section only shows that the MGN trained with the learned constitutive model is better than those trained with ground truth garments or analytical constitutive model. On the one hand, it does not compare with other state-of-the-arts like HOOD, SNUG, and PBNS that are also formulated in a self-supervised manner. On the other hand, the claim that it is better than the analytical constitutive model is not convincing because the discrepancy may be caused by the limited accuracy in the neural simulator (or even by the mini-network mentioned in Sec 3.3). To truly demonstrate that it is better than an analytical one, it must be compared using a numerical integrator that is guaranteed to converge to the energy minimum.

Limitations:
The discussion seems adequate.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel method for animating garments by learning from a single piece of cloth. This approach circumvents the need for large-scale garment datasets, which are resource-intensive and time-consuming to create. The core idea is to use a disentangled scheme where constitutive behaviors are learned from observed cloth and then applied to animate various garments. The proposed Energy Unit Network (EUNet) captures constitutive relations in the form of energy, bypassing the need for traditional physics models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper introduces a novel disentangled approach that separates the learning of constitutive behaviors from the animation process.

 The EUNet models constitutive behaviors using energy units, allowing for direct learning from observed cloth trajectories without traditional physics models.

The approach significantly reduces the data requirement, relying on a single piece of cloth for training, making it more practical and less resource-intensive.

The method produces animations that are both robust and generalizable, capable of handling various garment types and materials.

Weaknesses:
The energy optimization process, although effective, can be computationally intensive and may require fine-tuning to achieve optimal results.

The paper would benefit from more extensive experimental validation, including comparisons with a broader range of existing methods and more diverse garment types.

Limitations:
Comparisons with a broader range of existing methods would be appreciated, for example, one of the SOTA method named 'HOOD: hierarchical graphs for generalized modelling of clothing dynamics' which has been cited in the paper is worth comparing with.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to learn garment dynamics using a disentangled learning framework and the Energy Unit Network (EUNet). Instead of relying on extensive garment datasets, the approach learns constitutive behaviors from a single cloth piece and dynamically animates garments through energy optimization.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The writing is clear and technical details are described clearly.  The visual aids and diagrams are well-integrated, enhancing understanding.

Weaknesses:
My main problem with the paper is that the problem of learning/recovering cloth dynamics from structured sample tests has been studied intensively for a long time, and the authors seem not to be aware of this whole field.  This is a well-studied problem, and the authors do not position their method against significant prior works. Many existing works have also attempted learning from real-world fabric sample tests or indirect representations (video), which is a much harder problem. To mention a few:

1. ""Predicting the Drape of Woven Cloth Using Interacting Particles"" Breen et al., 1994
2. ""Estimating Cloth Simulation Parameters from Video"" Bhat et al., 2003
3. ""Data-driven elastic models for cloth: Modeling and measurement"" Wang et al., 2011
4. ""How Will It Drape Like? Capturing Fabric Mechanics from Depth Images"" Rodriguez-Pardo et al., 2023
5. ""Estimating Cloth Simulation Parameters From Tag Information and Cusick Drape Test"" Ju et al., 2024

The authors should thoroughly review the literature and reposition their contribution and provide experimental comparisons against existing works.  Additionally, the literature review section does not include important works from the physics simulation community. Including these references and discussing how the proposed method builds upon or differs from them would strengthen the paper significantly.

Limitations:
The authors provided a discussion on the limitations of edge-wise discretization and a lack of self-collision handling.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ybiUVIxJth;"REVIEW 
Summary:
This paper joins a long list of recent work that studies how to aggregate the preferences of several agents (e.g., humans) in a reinforcement learning framework inspired by social choice theory. The problem is modeled as a multi-objective MDP with $n$ different reward functions. The authors propose to use the state-action occupancy measure instead of each agent's most preferred policy or reward function directly. Popular concepts from social choice theory, such as the Borda count rule and approval voting, are then studied from this perspective.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to read. 
- It appears that considering the state-action occupancy measure has some advantages over working directly with each agent's optimal policy or reward function when attempting to introduce social choice rules, which---even though a standard approach in RL---is interesting.

Weaknesses:
- In my opinion, the contributions of this work are limited. E.g., only the full information case is studied
- The primary justification of this work (which is repeatedly mentioned in the paper) is that prior work on policy aggregation and fair RL is not invariant to affine transformations of the reward function. Essentially, agents can have differently scaled reward functions, which makes, e.g., maximizing for social welfare a bad objective. However, I don't understand why we cannot simply normalize the reward function of each agent, so that the reward functions are directly ""comparable"". I find the concern about affine transformations quite weak.

Limitations:
Limitations are adequately addressed in my opinion.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper solves the problem which arises in preference aggregation of individual policies to a collective policy – (1) summation based aggregation are sensitive to affine transformations and (2) voting rule based aggregation faces problem of policies being exponential in S. Towards solving this, the paper proposes voting over continuous space of alternatives (which eliminates affine sensitivity) and a volumetric definition of preference ordering. The paper next proposes efficient algorithms to (1) find approximate volumetric veto core and (2) approximate q-Quantile Fairness. They also show complexity of existing voting rules, notably plurality voting and borda count. They show that problem is computationally hard for plurality and open for broad count. I am inclined towards accepting the paper.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper solves a well-motivated problem of policy aggregation. Their proposal of achieving different notions of approximate fairness through efficient algorithms both novel and appears to be sound. Their theoretical analysis of the complexity of using plurality and borda count based voting is also significant and allows scope for future work in the direction. Their algorithms have been validated through experiments.

Weaknesses:
In the experimental section, using a common metric to quantify the “level of fairness” guaranteed by different algorithms would be beneficial for a more learned comparison.  

In Def. 4 should the expression be vol(O’)/vol(O) >= 1 – veto(S) + epsilon instead of vol(O’) >= 1 – veto(S) + epsilon as currently stated?

Limitations:
Authors adequately address the problem statement and discussed limitations/candidate improvements of their work which is left to future work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies aggregating multiple policies–which can be seen as a formalization of the task of aligning an AI system to the values of multiple individuals. When the number of states is small (such as, when multiple individuals have to select one out of a few candidates), this problem has been widely studied in voting and social choice theory, and there are many efficient aggregation rules (such as the Borda count). This paper, however, considers the other extreme: where the state-action space is huge, and it is not obvious how to design efficient methods to aggregate policies. 

The main insight of this paper is that preferences over policies have a volumetric interpretation in the state-action policy space that, in some cases, leads to efficient aggregation algorithms. 

Concretely, the authors examine two types of aggregation rules: (1) two aggregation rules that are known to have desirable fairness properties (namely, proportional veto code and, the recently introduced, quantile fairness) and (2) voting or score-based rules such as Borda count and $\alpha$-approval voting rule. 

Building on their insight the authors prove several results, including 
1. an algorithm which finds the policy wrt an $\epsilon$-approximation of the proportional veto core using $O(log(1/\epsilon))$ queries to a volume computation oracle, 
2. the existence of $q$-quantile fair policies for all $q\geq 1/e$ (which is tight and stronger than the best possible bound in the discrete case),
3. NP-hardness and inapproximability results for $\alpha$-approval score.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written and easy to read. I believe that the problem proposed in the paper is well-motivated from alignin AI systems, and is of significant interest to research on voting rules and social choice theory. The theoretical results are solid and the proofs and/or approach are well outlined. Finally, I did not check the proofs in detail, but they appear sound. One caveat is that I am not familiar with the closely related prior work (e.g., [6]) and, so, cannot comment on the novelty of the proofs and results from prior work.

Weaknesses:
I am not sure if the empirical results section is adding any value to this paper: it evaluates different aggregation rules, but I think this is not the focus of this work–I think the focus is to design efficient algorithms and/or prove existential results. If other reviews and the area chairs agree, my suggestion is to drop the empirical results section and use the additional space to add more exposition on the proofs. To be clear, this is no a significant concern for me.

Limitations:
I do not see significant limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ybMrn4tdn0;"REVIEW 
Summary:
The paper addresses the challenges in verifying the accuracy of local explanations for machine learning models, especially when the model is not fully known and access to it is limited. The primary focus is on minimizing the number of times the model and explainer are accessed during the auditing process. The contributions of the paper are as follows:

**C1. Defining Auditing of Local Explanations:** The paper provides a formal definition of what it means to audit local explanations. It sets the groundwork for systematically evaluating the reliability of these explanations, which are crucial for understanding and trusting machine learning models.

**C2. Importance of the Region’s size**: It highlights that the region to which the local explainer applies is a critical property. Understanding the scope and limits of where the explanation is valid is essential for accurate auditing. This insight helps in identifying when and where an explanation might fail to represent the model correctly.

**C3. Bounds on Auditing Complexity**: The paper establishes both lower and upper bounds on the sample complexity required for auditing local explanations. These bounds are presented as functions of the newly identified property, which is the region’s size. This provides a theoretical framework for understanding the minimal and maximal data requirements for effective auditing.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**S1. Framework for Auditing**: It proposes a theoretical framework for auditing local explanations, which is a significant step towards developing more rigorous and reliable methods for verifying the trustworthiness of explanations provided by machine learning models.

**S2. Identification of Key Metrics**: The introduction of the explainability loss function, provides a quantifiable measure for evaluating local explanations for the original model, offering a systematic way to assess explanation quality.

**S3. Highlighting the Importance of Locality**: The analysis which provide upper and lower bounds highlights the importance of the ""locality"" of explanations, bringing attention to a previously underexplored aspect in the explainability literature.

Weaknesses:
**W1. Lack of Evaluation**: The paper does not include an evaluation on real-world datasets. Although the authors suggest that their results could have significant practical implications (“Our results might have far-reaching practical consequences”), an initial step should be to perform evaluations on actual data to validate their findings.

**W2. Limited Discussion with Previous Research**: The paper could benefit from a more thorough discussion of how its findings relate to and build upon previous research in the field. Specifically, the authors mentioned that [Dasgupta 2022] is the most similar to their work, except it is limited to discrete explanations rather than continuous. However, it is not clear whether, in the discrete setting, the proposed work aligns with Dasgupta’s consistency metric, sufficiency metric, or if it does not coincide with any of these previous metrics.
It may also be worth discussing and comparing with the recent work of [Bassan 2023], which suggests a verification method for finding minimal sufficient explanations.

- Dasgupta, S., Frost, N. and Moshkovitz, M., 2022. Framework for evaluating faithfulness of local explanations. In International Conference on Machine Learning
- Bassan, S. and Katz, G., 2023. Towards formal XAI: formally approximate minimal explanations of neural networks. In International Conference on Tools and Algorithms for the Construction and Analysis of Systems

**W3. Focus on a Specific Type of Explanations**: In the paper, the presentation is a bit misleading because it suggests that explanations can be general. However, the focus is on one type of explanation – those that approximate the true model on a region of examples. This is not a general (local) explanation method, even though it includes quite a few types of explanations.

Limitations:
--

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work studies an auditing framework in the eXplainable Artificial Intelligence (XAI) area. Specifically, the authors consider the scenario where a group of third-party auditors or users attempt to perform a sanity check on the provided explanations. The framework allows the auditors to query the model prediction and local explanations. Based on the proposed framework, this paper presents a theoretical analysis of the sample complexity of auditing.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper targets a very important aspect of XAI studies. It considers the deployment phase where users do not trust the provided explanations. This is a usually overlooked perspective in the XAI community.

Weaknesses:
1. This work focuses on local explanations defined in section 1.1 L48-49 and section 2.2 L155-162. These presumptions limit the scope of this paper to the surrogate model method (such as LIME, MUSE [1], etc.), where a glass-box explainer is used to approximate black box’ predictions in the neighborhood of input data. This greatly limits the impact of this work as such surrogate-model explanation methods only take a very small part of local explanation methods. Local explanations are not limited to surrogate model methods. It can refer to explanations regarding individual input samples instead of the entire data manifold or even regarding the model itself [2]. 
2. In the context of this paper, the authors claim that gradient-based explanations are surrogate model explanation methods (i.e. “local explanation method” under the definition of this paper) in L181-188. The authors define that $g_x(x) = (\nabla_xf(x))^Tx$, which is the summation of input x gradient attributions. This corresponds to the prediction $f(x)$ only if $f$ satisfies homogeneity [3]. On the contrary, suppose $\phi_f(x)\in\mathbb{R}^d$ is the explanation of SHAP, then $g_x(x):=(\phi_f(x))^T\mathbf{1} = f(x)$ can accurately reflects the prediction. Therefore, the definition of the explainers studied in this work is ambiguous and may require more rigorous considerations.

In summary of points 1 and 2, the formulation of the framework in this work is flawed. 

3. There is no empirical verification of the proposed theoretical results, which significantly undermines the contribution of the theoretical analysis. Note that a continuous function is always bounded on the closed neighborhood of x. Therefore, it is essential to empirically test whether the proposed bounds are tight. A theoretical demonstration is also appreciated.

4. [minor] To stay consistent, “Lime” in L337 should be revised to “LIME”.

5. While the motivation that users/auditors may not trust the explanation system and want to audit the model is an interesting and realistic setup, the proposed framework lacks practical contributions. Specifically, the formalism described in L64-66 and L236-242. can be difficult to satisfy.

**Reference**

[1] Lakkaraju, H., & Bastani, O. (2020, February). "" How do I fool you?"" Manipulating User Trust via Misleading Black Box Explanations. In *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society* (pp. 79-85).

[2] Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., & Kim, B. (2018). Sanity checks for saliency maps. *Advances in neural information processing systems*, *31*.

[3] Hesse, R., Schaub-Meyer, S., & Roth, S. (2021). Fast axiomatic attribution for neural networks. *Advances in Neural Information Processing Systems*, *34*, 19513-19524.

Limitations:
The limitations of this work are discussed in L194-197, where the authors admit that the definition “local explanation” is narrowed down to fit a. I agree with the limitation and appreciate the authors for clearly stating this issue. However, my concern is that this can be severe and greatly undermine the application scenarios of this work. More details are in the weakness section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes an auditing framework to verify the truthfulness of explanations by a third-party in scenarios where there is no trust. Bounds on sample complexity are provided that depend on the locality (minimum local mass) of the explanation. Further, the authors discuss that for gradient-based explanations in higher dimensions, locality tends to be small to achieve a reasonable explanation loss. Smaller locality increases the provided bounds on the amount of data required for the audit.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The topic of the paper is important for policymakers and the XAI research community in general, as it suggests that in natural scenarios where there is no trust, it is difficult to verify whether the local explanation is truthful to the model without actually knowing the model. 
2. The paper provides upper and lower bounds on sample complexity for auditing local explanations. 
3. The analysis includes gradient-based explanations and discusses how to generalize to other methods, including LIME and Anchors.

Weaknesses:
1. Regarding the soundness of the auditing framework, could you please comment on the motivation for the company to provide all of the required data (especially local regions) to the auditor? When the requested dataset is sufficiently large, the third-party company could potentially recover the model along with all the classifier outputs, local regions, and other information.
2. Figure 1 is hard to fully understand without reading the paper. It’s not intuitive which data points are explained and why, in panel (b), there is sufficient data for the audit. Could you please provide more explanation in the figure caption or simplify the figure?
3. Section 5 and Theorem 5.1 present an existence proof. However, the example considered in Figure 2 (a) is very specific. Can you elaborate on how often you expect this data distribution to occur in real-world datasets or discuss the value/range of locality for other likely data distributions?

Minor:
1. $E(f, x_i)$ is used in Section 1 but defined in Section 2.
2. Please specify $\epsilon_1$, $\epsilon_2$, $\delta$, and $\gamma$ in Theorem 4.2 or mention that you are operating under the same conditions as in Theorem 4.1, if applicable. Also, Algorithm 1 is referenced before it is defined.

Limitations:
The authors discuss the limitations of the paper with respect to the explanation algorithms.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides theoretical results on how many queries are required for an auditing framework for local explanations of machine learning algorithms (e.g., neural networks).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well motivated with a widely interesting and relevant topic. The approach is theoretical, and rigor is provided through proofs in appendices. The authors connect their work to popular algorithms: Gradient based approaches, Lime, and Anchors.

Weaknesses:
The authors acknowledge that the local loss estimation is, in their argument, only a necessary but not sufficient condition for trust. They do not establish or reference any evidence that manipulation would result in the local loss as a good indicator of untrustworthiness. As a result, the analysis serves more as a potential validation scheme for the limited types of algorithms that meet their linearity requirement (e.g. Anchors or LIME). This drastically narrows the scope and implications of their analysis. Unless this can be firmly established the title, abstract, and conclusions of the paper should be amended to reflect the correct scope of its claims
Furthermore, there are plenty of reasons (and examples) where interpretation methods are demonstrated to be frail to the *input* (e.g. Ghorbani). This would likely not pass the audit but would not be evidence that the $E$ has been altered. I think this speaks to some confusion in the setup of the paper as to what “trust-worthiness” is. The authors present it as trust between the user and provider rather than trust in the robustness of the explainability metric which is, I argue, closer to what their results seem to reflect.
Additionally, it is not clear to me that this is the only way to test explainability metrics with limited access (e.g. data points, classifier outputs, and local explanations).  For example, these metrics are popular because they match so well to human “expert” knowledge. You show someone the pcitre of the shark from Smilkov et al and they agree that they see “shark-like” shapes in the SmoothGrad output. Consequently, one could imagine the case where sampling in a *non-local* fashion would trace whether the same features matching human “experts” appear. 
Ultimately the proposed methodology seems entirely impractical (which is sort of the point).

Limitations:
-	Analysis is for a binary classifier

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ybLXvqJyQA;"REVIEW 
Summary:
The paper presents two novel machine learning algorithms for predicting ground state properties of quantum systems with constant sample complexity, independent of system size. The first algorithm modifies an existing ML model, while the second introduces a deep neural network model, both showing improved scaling in numerical experiments.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The introduction of a deep learning model with rigorous sample complexity bounds is a significant contribution to the field. The constant sample complexity, regardless of system size, is particularly noteworthy and addresses a critical challenge in quantum many-body physics.

2. The authors provide numerical experiments that validate the theoretical claims. The experiments demonstrate the practical effectiveness of the proposed algorithms, especially the deep learning model, which outperforms previous methods.

Weaknesses:
1. The training objective for the neural network is non-convex, which poses challenges in finding a global optimum efficiently. The paper does not address how to overcome this issue or guarantee convergence to optimal weights.

2. While the paper claims improved computational complexity, the actual implementation details and computational resources required for the deep learning model are not thoroughly discussed.

Limitations:
The authors have discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors focused on utilizing deep learning methods to predict the ground states. They made an important assumption that brings theoretical improvement to achieve constant sample complexity in the training data. They also made two main alternations to the learning model compared to previous literature, including incorporating Pauli coefficients in feature mapping and utilizing kernel ridge instead of Lasso. Numerical results for up to 45 qubit systems are provided, supporting the theoretical findings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. High-quality paper with rigorous theoretical findings and comprehensive numerical results.
2. Improved the sampling overhead to constant complexity, independent of the system size.
3. Explored new possibilities for predicting the ground state properties of quantum many-body systems using neural network models.

Weaknesses:
The main concern is that the improvement of this paper against precious works is limited. The main theoretical finding is based on an additional assumption that we know the property we'd like to predict in advance. The proposed learning method has only two minor alternations. These issues prevent me from giving a higher evaluation score, but they do not overshadow the fact that this article is of high quality.

Limitations:
See weakness

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the sample-efficient learnability of properties of grounds states of local Hamiltonians. Ground states of local Hamiltonians are hard to compute, even for quantum computers and to circumvent this hardness, several recent works proposed learning the trace inner product of local observables with the ground state given labeled training data. This setting is exactly PAC learning, i.e. given labeled data from a worst-case distribution, the goal is to get low prediction error wrt the same distribution on future samples. The best sample complexity for this problem is known to be log(n) 2^{polylog(1/eps)}, shown by Lewis et al. 

The main questions addressed in this work are (1) whether the sample complexity can be improved to be independent of the system size and (2) whether there are rigorous guarantees for learning properties of the ground state via neural network based algorithms.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper provides several technical results on the representation and learnability of ground-state properties. The improved sample complexity follows from tweaking the algorithm is [2] and making additional assumptions about the training distribution. The neural net sample complexity result proceeds in two steps. First the authors prove an approximation-theoretic result for functions that look like ground state properties and show they can be well-approximated by neural networks. They then obtain a generalization bound using fairly sophisticated technical machinery.

Weaknesses:
I think the paper does not resolve the questions it claims to resolve and does so in a slightly camouflaged way.  

1. Question 1 in the paper asks whether you can get sample complexity that is independent of system size for learning properties of ground states, aka the PAC learning setting for ground states of local Hamiltonians. The answer obtained is yes, under two crucial caveats, the observable is known in advance and the distribution over the training data is not worst-case.  This diverges significantly from the PAC learning model. The same critique holds for Question 2. Further, reference [1] does not state this as an open problem. 

2. The assumptions on the training distribution are not stated upfront and do not appear to be mild. The assumptions include the distribution g to be strictly non-zero on [-1,1], and zero outside; g being continuously differentiable; and component-wise independent. Are there any natural distributions that satisfy all these properties simultaneously? 

3. There is no discussion of why each of these properties is needed, which ones are crucial to the argument and which ones are for technical convenience. Having 4 non-trivial technical assumptions of the pdf is a major weakness, especially since it makes the result incomparable to prior work [1,2], where the setting is truly PAC learning. 

4. In the numerical experiments, I see no discussion of what distribution was used to generate the training data, and how many of the technical conditions this distribution satisfies. It remains unclear to me what the experimental section is trying to convey, since it does not complement the main theorems. 

5. When is it reasonable to expect labeled data for the observable / property you want to learn? What is a real-world scenario where one would expect to obtain such labeled data? 

6. Is there a way to get some non-trivial sample complexity (not necessarily system independent) bound for PAC learning via neural networks, without the extra assumptions on the training distribution?  

7. There are completely unjustified claims such as the neural network achieving low loss and finding a bounded magnitude solution after constant many steps and O(n) time. It is not clear why this should ever be the case.

Limitations:
Yes.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work builds upon the work of Huang et al. and Lewis et al. by introducing two new approaches to get constant sample complexity for predicting properties of a ground state of a many-body local Hamiltonian. The two new approaches are a modified ML model that requires knowledge of the property of interest and a deep neural network model that does not need prior knowledge of the property. In this paper, the authors provide both proves and small experimental evaluations to show that both approaches achieve constant sample complexity, independent of system size.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-organized and clearly written. The paper includes rigorous theoretical guarantees and small numerical experiments to confirm the efficacy of the proposed methods compared to the existing algorithm.

Weaknesses:
Even though it is a strong paper, the issue addressed here is a specific case that builds upon two other papers. Additionally, the related published works are mostly, if not all, published in physics journals. I do not see why the results shared in this paper are valuable to share with the broader NeurIPS community, especially since the mathematical proofs are very rigorous, I would expect that it is not accessible to the broader audience. Some assumptions and conditions required for the theoretical guarantees may also limit the applicability of the results.

Limitations:
Limitations are addressed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors give two algorithms that predict (geometrically local) properties of ground states of gapped geometrically local Hamiltonians. This problem has been introduced by Huang et al. [HKT+22], and the previous best known algorithm is given by Lewis et al. [LHT+24], which uses $\log(n)$ samples, where $n$ is the number of qubits in the Hamiltonian. This paper further improves on the $log(n)$ sample complexity, and gives two algorithms that only use a constant number of samples. The first  algorithm is modified from the algorithm of [LHT+24], changing the regression part of the algorithm from LASSO to kernel ridge regression. The second algorithm uses deep neural network, having the advantage of not needing to know the observables in advance, but requires more restriction on the distribution of the Hamiltonian parameters. The authors complement their theoretical results with numerical simulations.

[HKT+22] Huang, Hsin-Yuan, Richard Kueng, Giacomo Torlai, Victor V. Albert, and John Preskill. ""Provably efficient machine learning for quantum many-body problems."" Science 377, no. 6613 (2022): eabk3333
[LHT+24] Lewis, Laura, Hsin-Yuan Huang, Viet T. Tran, Sebastian Lehner, Richard Kueng, and John Preskill. ""Improved machine learning algorithm for predicting ground state properties."" nature communications 15, no. 1 (2024): 895.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
3: good

Strengths:
The work achieves the optimal sample complexity of the problem and is written in good English.

Weaknesses:
The part of preliminaries that are restating definition and result of [LHT24+], is not well written, and I believe has led to a critical bug to the first algorithm. In particular, Theorem 8 claims that for every $O\sum_{P} \alpha_P P$ that can be written as sum of geometrically observables, $\sum_{P} |\alpha| =O(1)$. However, the counterpart in  [LHT24+] has extra restrictions: $||O||_{infty}=1$ and $O$ need to be inside a radius of $R=O(1)$.  Therefore, where the authors uses Theorem 8 in equation (B.28) to bound the kernel, the result is incorrect since they do not have $R=O(1)$.

Other minor inconsistencies includes: 

line 642: $S^{geo}$ not defined

line 660: $h_{c(j)}$ not defined

Limitations:
The authors adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose an ML based method to predict properties of ground states of quantum systems which comes with provable guarantees. Improving on recent work by Huang et al and Lewis et al, they give sample complexity bounds which are independent of the number of qubits. This approach is applicable when the observable one is trying to predict is predetermined. The authors also suggest a deep learning based approach for the case where the observable is not known in advance. They support their theoretical wok with numerical experiments.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper adresses an important probelm, and is well written and argued. The authors clearly explain the previous state of the art in ML based prediction of ground state properties, as well as their own contribution. Their proposed modification to the procedure suggested by Lewis et al., which results in Theorem 1 of the paper, seems interesting and worthwhile. Likewise the guarantees obtained for the training of a custom Neural Network architecture are intriguing from a learning theoretic perspective.

Weaknesses:
It is unclear to me how the Neural Network generalization result compares to known results in the literature- the setting which the authors study is quite specific and thus it is not easy to relate the result they obtained to those in the theoretical deep learning literature.

Limitations:
I believe the authors have adequately adressed the limitations of the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ybHPzL7eYT;"REVIEW 
Summary:
The authors proposed the Large Scene Model (LSM), a novel 3D scene understanding framework that unifies multiple vision tasks within a single model. LSM represents a scene using pixel-aligned point maps, integrating geometric, appearance, and semantic information into a unified representation. By leveraging a Transformer architecture with cross-view and cross-modal attention, the model effectively incorporates multi-view cues and semantic knowledge from a 2D vision model.

LSM's design enables efficient scene-level 3D semantic reconstruction and rendering in real time on a single GPU. The model's integration of a 2D semantic model allows for open-vocabulary understanding, extending its applicability to diverse real-world scenarios. Furthermore, by consolidating multiple tasks within a single model, LSM minimizes error propagation, leading to more robust and accurate results compared to state-of-the-art baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The described technical approach in this work is sound and clearly presented. The contributions from the various proposed modules are well ablated and investigated in the experiments (Table 4).
* The model demonstrates high inference efficiency compared to other approaches. with reconstruction time of 0.1s and rendering at 270 due to the underlying 3DGS representation that is being generated.
* I like that the model reconstructs the underlying 3D representation in a single feedforward pass, as compared to multiview + test time optimization for fusion approaches. This improves the speed and efficiency for inference. It is good to see compelling quality based on the novel view synthesis.

Weaknesses:
* I think the main contribution of this paper is the unification of the various scene modeling tasks into the same model, including geometry, color and semantics. The authors further claimed in the abstract and introduction that multitask training end-to-end allows LSM to outperform state-of-the-art baselines. However the paper did not ablate the multi-task learning design choice. For instance, what if some of the tasks are removed (e.g., semantic feature prediction). How does that affect the performance of the other tasks?
* A suggestion is that for Figure 5, it is unclear how much pose divergence there is between the input source view and the synthesized novel view. It would be helpful to also show the source view supplied as input to the model.
* The paper is named Large Scene Model, which seems to suggest something to do with model parameter scaling, hence large. However the paper does seem to do much scaling on model size. So perhaps a more accurate terminology would be Multitask or Unified Scene Model? 

Nits.
* Line 153: Typo: to serve as input?
* In Tables 1-4, I suggest highlighting the best (and possibly second-best result) for easier comparison of the various experiments.
* In Table 4, why is + Multi-scale Fusion indented?

Limitations:
- My understanding is that in this method, all the Gaussians being generated are pixel aligned with the original input images. Is that a limitation of the method, since that would make the model unable to model large pose divergences that require rendering regions not originally visible in the input view, for instance, the back side of a sofa etc.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents the Large Scene Model (LSM), which generates semantic radiance fields from uncalibrated RGB images using a unified Transformer-based framework. LSM can infer geometry, appearance, and semantics simultaneously and synthesize label maps in real-time. The model integrates multi-scale fusion and features from 2D models to enhance accuracy and efficiency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Unified Framework: LSM combines multiple 3D vision tasks into a single framework, streamlining the process and reducing complexity.

2. Real-time Performance: The model achieves real-time 3D reconstruction and rendering, suitable for applications needing fast processing.

3. Enhanced Feature Fusion: By incorporating 2D model features, LSM improves the quality of feature lifting and semantic understanding, enhancing overall performance.

Weaknesses:
1. Dataset: I recommend the authors organize the training and testing phases in alignment with previous methods (NeRF-DFF and Feature-3DGS) and provide results on the Replica Dataset. The authors have not sufficiently justified deviating from the baseline evaluation split. Furthermore, an explanation is needed for the significant performance discrepancy of the baselines between the Replica Dataset and the authors' setup. Additional training details may also be necessary.

2. Writing: The paper's abstract, introduction, and methods sections require improvement.  Specifically, the methods section should introduce each module and their interconnections from a high-level perspective rather than presenting them as isolated components.

3. Method Details: Do the authors use camera parameters? If so, why are camera parameters mentioned in line 117? If camera parameters are used, the model cannot be described as ""unposed.""

4. Visualization: In Figure 4, there are category colors that are not listed in the legend. Additionally, a more diverse set of results should be displayed, as the current experimental set predominantly features sofas.

Limitations:
The authors have discussed limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to train a network that takes in a set of unposed images and directly produces a semantic radiance field.

The method utilizes a single Transformer-based model that learns the attributes
of a 3D scene represented by a point-based radiance field.  A decoder produced 3D Gausians that can be splatted to make novel images, depth estimates, and semantic segmentations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper provides a transformer architecture for producing 3D gaussians with rich features from unposed images, which seem very valuable.   The design choices in the proposed system are well-chosen from methods available at this time, leading to a system that has a good combination of little-compute and competitive-accuracy on three different tasks (nvs, depth, semantics).

Weaknesses:
The paper shares goals and ideas with ""Scene Representation Transformers"" (Sajjadi et al., CVPR 2022) and its follow up work Object Scene Representation Transformer (NeurIPS 2022) and RUST: Really Unposed SRT (CVPR 2023).   This paper is different, because it ultimate produces a set of gaussians rather than a LLFF or NeRF volume, and it distills features from 2D foundation models.   However, it is similar in that a transform encoder and decoder produces a scene representation directly from a set of images, which is then used for novel view synthesis, depth estimation, and semantic segmentation.   In any case, those paper seem fairly similar in concept and so I think they should be discussed in the related work, and possibly approach sections.

The ablation study in table 4 suggests that the key methods in the paper have little impact on the results of NVS.

Limitations:
Limitations are discussed briefly in the second paragraph of the conclusion.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper solves the sparse-view scene reconstruction problem by Large Scene Model, a unified scene reconstruction model via unposed RGB images. The model utilizes a ViT backbone for extracting the feature and uses cross-view attention to align the multi-pose feature for consistent features. The 3D scene is further rendered from the 3D semantic field derived by the multi-view features. The unified model is capable of multiple 3D-based tasks including novel view synthesis and 3D language-based segmentation. Experiments showed that the work achieves better results with limited performance sacrifices in the NVS task and higher performance in the multi-view language-based segmentation task.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The model is general and multi-purpose in sparse-view scene reconstruction.
2. The model can achieve better results while still obtaining lighting-fast rendering speed and can be applied to real-time reconstruction.

Weaknesses:
1. The technical contribution is limited. The model is generally designed via multi-purpose modules glued attention and Transformers, which is a straightforward and widely applied idea. There is no significant new problem has arisen and novel solutions proposed.
2. The performance comparison with NVS-related works is limited. Firstly, the authors train and run comparison experiments on the same dataset, which can be biased. Secondly, several popular scene datasets incorporated in similar works (such as RealEstate10k) are not utilized in this work. Thirdly, methods similar to pixelSplat such as Splatter Image[1] are not included in comparison.
3. The presentation can still be improved. Firstly, the authors titled their work “Large Scene Model”, while the design is more similar to the idea of pixel-based Gaussian splatting (such as pixelSplat and GaussianImage). Secondly, each module's input and output data type cannot be directly recognized from the pipeline graph. 
4. The bibliography of this paper lacks some related works, such as Splatter Image[1], which is also an image-space Gaussian splatting method. 

Reference: 
[1] Szymanowicz, Stanislaw, Chrisitian Rupprecht, and Andrea Vedaldi. ""Splatter image: Ultra-fast single-view 3d reconstruction."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10208-10217. 2024.

Limitations:
The authors claimed that the major drawback of the model is the VRAM consuming. The social impact of this work mainly related to potential misuse of 3D assets and can be solved via integrating watermarks into the generated result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yXpfrLMIr2;"REVIEW 
Summary:
The paper introduces BI-DiffSR, a novel binarized diffusion model for image super-resolution, designed to accelerate the inference speed and reduce computational costs of diffusion models while maintaining high performance. It proposes a UNet architecture optimized for binarization, featuring consistent-pixel downsampling/upsampling and channel-shuffle fusion to address dimension mismatch and fusion difficulty, alongside a timestep-aware redistribution and activation function to adapt to varying activation distributions across different timesteps. The model demonstrates superior results over existing binarization methods, approaching the perceptual quality of full-precision models with significantly reduced memory and computational requirements.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The paper is well-written and easy to understand.

- This paper designs a novelty 1-bit UNet for accurate binarized diffusion model, including:
   - New downsample module and upsample module for Dimension Consistency.
   - Channel shuffle module to balance the activation value ranges of two input features.
   - The timestep-aware redistribution (TaR) and timestep-aware activation function (TaA)

- Experiments achieve the state-of-the-art in super resolution with diffusion.

Weaknesses:
- The basic BI-Conv block lacks novelty, which is as the same as the binarized module in ReActNet that contains RSign and RPReLU.

- TaR uses different parameters for different time steps, but in the mean while, the normal time embedding is projected into the resblock, it is also a time-aware on feature maps, what is the differences or why TaR works?

- SR3 is not a new diffusion baseline for super resolution, ResShift[1], SinSR[2] should be better, and the metrics of PSNR, SSIM, LPIPS is much old, the CLIPIQA, MUSIQ, MANIQA should be better for evaluating the performance of generative super resolution.

- Self-attention and MLP are common modules in diffusion, such as LDM[3] and ResShift[1], which require a lot of computation. How can the method in this paper be extended to self-attention and MLP?


[1] Yue, Zongsheng, Jianyi Wang, and Chen Change Loy. ""Resshift: Efficient diffusion model for image super-resolution by residual shifting."" Advances in Neural Information Processing Systems 36 (2024).

[2] Wang, Yufei, et al. ""SinSR: diffusion-based image super-resolution in a single step."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

[3] Rombach, Robin, et al. ""High-resolution image synthesis with latent diffusion models."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.

Limitations:
The authors have addressed the limitations in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work present a novel binarized diffusion model for improving the efficiency of super resolution tasks. Compared with the existing works, this work first pointed out the specific challenges of binarized DMs for SR, including the dimension mismatch and fusion difficulty of representations. Then this work present several techniques: consistent-pixel down/upsampleing, channel-shuffle fusion, and Time-step-aware redistribution function for the aforementioned challenges. Comprehensive results show that the provided binarized DMs for SR not only significantly outperform the binarized models with existing SOTA binarization methods, but also achieve floating-point level performance. And for the efficiency, the statistics of params and flops show the advantage of proposed method, and the paper also present the real inference time on edge, which seems important and encouraged in the binarization community.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. As far as I know, this is the first work to present the specific binarization method for diffusion model of SR. Since the good performance has been achieved by DMs in various SR tasks, it’s important to present novel insight to compress these models, especially considering the severe drop still exists after binarizing by existing SOTA methods.
2. The motivation is intuitive and techniques are novelty, especially considering the features of DMs. The proposed CP-Up/down and channel shuffle are highly specified to the architecture of the diffusion models, which is novel and cannot be achieved by previous methods, including binarization function and binarized structures. And the computation is also small, allowing minor burden with significant performance improvement. And the proposed activation function also focus on the high dynamic of activation range during  time-step, which is one of the most critical problem for the quantization of DMs. 
3. The proposed method achieve SOTA results in accuracy. Comprehensive comparison has been included in this paper, including SOTA binarization methods and various evaluation datasets. The results show that the proposed outperforms than previous binarized DMs for SR with significant improvements.
4. In this paper, diverse analysis, including quantitative, statistical, and visual results are presented in detail. More important, the paper shows the efficiency evaluation based on real inference libraries and edge hardware, which is of great significance for practical application.

Weaknesses:
Though it’s a good paper, some issues should be addressed.
1. The writing and presentation of the paper should be improved, including but not limited to the grammar and description. For example, some basic knowledge about quantization, SR, and DMs seems to be summarized as a preliminaries section; and let the proposed techniques be highlighted in Figure 2.
2. As for the efficiency, I suggest the authors present the computation more detailed, such as present the computation of each part in the whole network before and after the binarization. This will show the efficiency advantage of the proposed method much clearer.
3. The proposed challenge I and II are insightful, but more further discussion (such as visual, quantitative, or theoretical analysis) are presented after proposing. I suggest authors do more discussion about that.
4. Some recent binarization methods for SR [1] are suggested to be compared and some quantized DMs [2] are suggested to be discussed the differences to make the comparison more comprehensive.
[1] Flexible Residual Binarization for Image Super-Resolution. ICML 2024
[2] EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models. ICLR 2024

Limitations:
The authors have addressed the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose BI-DiffSR to binarize diffusion based image super-resolution (SR) model. They design a UNet architecture for the whole binarized model structure. To maintain dimension consistency, they propose two modules, CP-Down and CP-Up, which can further help transfer full-precision information. To enhance feature fusion, they propose the channel-shuffle-fusion (CS-Fusion). They also propose TaR and TaA to dynamically adjust activation distribution cross different timesteps. The authors provide extensice experiments to demonstrate the effectiveness of their proposed method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The topic is very important and practical. Diffusion models have shown excellent performance for image super-resolution (SR). It is very practical to quantize the models before deploying them into devices. Binarization is an extreme tool to compress the SR model. Few works have been proposed to investigate such an important problem in image SR. 

The authors give several insights for the specific topic. Namely, there are some key aspects in diffusion based image SR binarization, like dimension mismatch, fusion difficulty, and activation distraction. Those problems hinder the performance of binarized image SR diffusion models. The observation and analyses given in the introduction section are insightful and motivate readers well.

To alliveate the problems in binarized diffuision based SR models, the authors propose consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to ensure dimensional consistency. They propose the channel-shuffle-fusion (CS-Fusion) to facilitate the fusion of different features within skip connections and suit binarized modules. They propose the timestep-aware redistribution (TaR) and timestep-aware activation function (TaA) to adjust the binarized module input and output arross different timesteps.

They provide extensive ablation study experiments (including quantitative results in Table 1 and visualization analyses in Figures 6 and 7.) to show the effects of each proposed components. Those experiments are convincing.

The authors provide comparions with SOTA methods. According to the main quantitive and visual comparisons, they show that their proposed BI-DiffSR achieves superior performance over others.

The overall writing and organization are pretty good. I think the work is well-prepared. The supplementary file further provides more details. The paper is easy to follow and they promise to release the code, which makes this work more convincing.

Weaknesses:
When binarizing full-precision model from 32-bit to 1-bit, ideally we can reduce the parameters by 32 times. But, as shown in Table 2, the authors reduce parameters from 55.41M to 4.58M (for scale 2). There is a gap between ideal case and practical one. Please give some analyese about the reasons for this gap. Also, are there any idea to further narrow the gap?

The parameters and Ops are reduced obviously from full-precision to binary one. But the authors did not give results about inference time on real devices or give some analyses. I am curious how fast the binarized model will be.

The writing can further refine in some cases. For example, in the abstract part (Line 9-10), “… to maintain dimension consistent” should be changed to “… to maintain dimension consistency”.

Limitations:
Please refer to weaknesses

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduce a novel binarized diffusion model, BI-DiffSR, for image SR. A UNet architecture optimized for binarization, channel shuffle fusion, and time-step-aware redistribution and activation functions are designed. The experimental results proved the effectiveness of the method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. This paper is well written, nicely presented, and well organized.

2. Binarized diffusion networks are promising.

3. The performance improvement over other binary SR networks is significant.

Weaknesses:
1. Lack of discussion with some related works[1, 2, 3, 4], in particular [1] which is also for binary SR networks. Please analyze and discuss the differences with [1,2].

2. Ablation experiments are not convincing enough. Comparisons with some other activation function or fusion methods [1, 2, 3, 4] should be included.

3. It is always known that diffusion models are slow. Although binarization will speed up the operation, can it achieve a better trade-off in performance and efficiency than a real-valued efficient SR network. It is suggested to compare with some efficient SR networks [5, 6, 7] in terms of Params, FLOPs, inference time and performance.

> 1. Flexible Residual Binarization for Image Super-Resolution. ICML24.

> 2. Q-DM: An Efficient Low-bit Quantized Diffusion Model. NIPS23. 

> 3. Binarized Low-light Raw Video Enhancement. ICCV23.

> 4. Binarized Spectral Compressive Imaging. NIPS23.

> 5. Efficient long-range attention network for image super-resolution. ECCV22.

> 6. DLGSANet: lightweight dynamic local and global self-attention networks for image super-resolution. ICCV23.

> 7. Feature modulation transformer: Cross-refinement of global representation via high-frequency prior for image super-resolution. ICCV23.

Limitations:
Limitations were discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
yPPNi7vc7n;"REVIEW 
Summary:
This manuscript proposes a new score matching method that bypasses the Jacobian trace by applying Stein’s identity, enabling effective regularization and efficient computation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The method is computationally efficient compared to other SSM variants.
2. Experimental results demonstrate the effectiveness of the proposed method.

Weaknesses:
1. The advantage of the proposed method compared to denoising score matching (DSM) is unclear. The manuscript mentions that it restricts the SDE to be affine, but it does not clarify the benefit of using a non-affine SDE. Furthermore, the influence of the SDE on the generative model needs to be elaborated.
2. The experimental results do not show significant improvements over DSM. The proposed method achieves comparable sample quality, as shown in Table 3.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel score matching variant called Local Curvature Smoothing with Stein’s Identity (LCSS). This method addresses the computational challenges associated with the Jacobian trace in score matching, particularly for high-dimensional data, by leveraging Stein’s identity. LCSS aims to bypass the expensive computation of the Jacobian trace, offering both regularization benefits and efficient computation. The method is validated through experiments on synthetic and real datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.  the idea of LCSS is novel
2. Jacobian is not computed directly, but implicitly respected.
3. Experiments on high and low resolution are performed.

Weaknesses:
1. In lines 161-162, interchangeability is assumed. However, in the analysis, interchangeability requires some properties of the interested function. The reason why the assumption holds is missing.
2. This paper does not approximate the Jacobian but instead circumvents the Jacobian. The empirical and theoretical differences against the method using Jacobian should be discussed, such as the difference in the estimated error bound. 
3. In Tab. 3, the improvement seems to be marginal, while in figures, such as Fig. 4, the selected picture is much better under LCSS. The discrepancy should be discussed.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides a new way for score matching with the purpose of resolving some of the limitations of the existing methods such as high variance of sliced score matching and Gaussian constraints of denoising score matching (DSM). The new method is based on the local curvature smoothing proposed in [15]. A new score matching objective function is proposed by combining the Stein's Identity with the local curvature smoothing. The authors empirically show that the new method is more efficient in training than DSM and also has comparable performance to DSM.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Although DSM is the default method used nowadays for score matching, the authors provide a nice novel alternative which may have some advantages over DSM. I'm interested to see more theoretical study in the future of this new method.

Weaknesses:
I think some parts of the paper are not stated clearly and further clarification is needed. See questions for more details.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes to use Stein's lemma to obtain a computationally efficient way in implementing a local-curvature regularized variant of the score matching objective. The main idea is to rewrite the Jacobian-trace term in a way that requires no Jacobian evaluations. In numerical experiments, the effectiveness of this approach is clearly demonstrated.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and the main idea is clear and easy to understand. 
- Other works which the paper builds upon are referenced and fairly attributed. 
- Experiments on small-scale data clearly demonstrate the effectiveness of the approach. 
- Also on larger datasets, the method appears to give strong empirical results.

Weaknesses:
- Approximating Jacobian trace through Stein's identity potentially leads to an estimator with large variance -- I found the claims that it solves Hutchinson's high variance problem to be a bit misleading.

Limitations:
All limitations are addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
CSjVSnvTbG;"REVIEW 
Summary:
The goal of this work is to study optimal transport (OT) distances between pairs of finite Markov chains, providing a novel relation between OT distances and probabilistic bisimulation metrics. The proposed linear program builds on ideas from optimal control in Markov decision processes, and the designed algorithm for solving the proposed linear program combines Sinkhorn's algorithm with an entropy-regularized version of the classic Value Iteration algorithm. Convergence guarantees and computational complexity analysis of the method are provided in the main text and in the appendix.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well-organized, the topic of study is very interesting, and the contribution is novel. Previous works on OT and bisimulation metrics are well-referenced and up-to-date.

Weaknesses:
The main theoretical contributions are presented in Section 3, where the authors introduce the definition of occupancy couplings $\mu$. These are distributions over $\mathcal{XY}\times \mathcal{XY}$. It is not entirely clear why they need to duplicate the variables $(x,y)$ and consider quadrupes $(xy,x'y')$.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This submission 

Namely, the authors define a notion of optimal transport distance between Markov chains on state spaces with a ground metric. This notion of distance differs from standard optimal transport, as the set of couplings is restricted to the set of so-called bicausal couplings. Using the results of Moulos [2021], it is shown that this Markovian optimal transport can be characterized in terms of the solution of the Bellman optimality equations for a Markov decision process. 

Using this equivalence, it is demonstrated that, in certain cases, bisimulation metrics can be though of, equivalently, as Markovian optimal transport with a specific cost function by exploiting their connection to solutions of certain fixed-point equations. 

Next, it is shown that the optimal value for the derived Markovian optimal transport problem can be computed by solving a finite-dimensional linear program, where the constraint set consists of the intersection of three sets. Rather than directly solving this linear program, the authors propose to regularize the problem using a conditional entropy (by analogy with entropic regularization of optimal transport distances which can then be solved efficiently using Sinkhorn iterations). To solve the entropy regularized problem, the Sinkhorn Value Iteration algorithm is proposed and the number of steps required to obtain a desired accuracy in estimating the Markovian optimal transport distance is provided. The authors also propose an analyze an alternative algorithm which they dub Sinkhorn Policy Iteration. 

The paper concludes with some numerical experiments to illustrate the performance of the proposed algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
In my opinion, the submission is well-written and its contributions relative to the broader literature are clearly identified. 

The main contributions of this work, identifying bisimulation metrics as a type of optimal transport problem and providing some new algorithms for estimating bisimulation metrics by using this connection is of interest and is, to my knowledge, novel.

Weaknesses:
1. It appears that the connection between the Markovian optimal transport problem and the finite-dimensional linear program provided in Theorem 1 only enables the computation of the optimal value for optimal transport problem, but does not allow the recovery of the optimal bicausal coupling. If this is the case, this should be further clarified in the text. 

2. While the analysis for the Sinkhorn Value Iteration is a nice addition, the guarantees are a bit confusing. Notably, if we wish to attain a precision of $\epsilon$ in estimating the Markovian optimal transport one requires $K=O(1/\epsilon^2)$ iterations and setting the regularization parameter to $\eta =C/\sqrt K=O(\epsilon)$. However, if epsilon is sufficiently small, the objective in the update (10) will be dominated by the entropy which is minimized by $\mu=\mu_k$. Naively I would assume that $1/\eta$ should be small so that minimization of the linear term dominates.

Limitations:
The authors address the limitations of their work in the discussion.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work integrates optimal transport with Markov chains by proposing an alternative joint distribution between Markov processes, namely ""discounted occupancy couplings"". They show that optimal transport distances can be computed as a a linear program (LP) in reduced space. This improves the computational efficiency of OT between markov decision process (MDP) over the previous methods. The previous methods, as they reviewed, often requires complex dynamic programming algorithms. They showed that the new formulation can be extended to the well-known entropy regularization problem, employing Sinkhorn-like iterations to solve, making the computation scalable with large problem. This paper provides both theoretical and experimental supports for the new formulation. In the end, they discuss the potential applications on RL models, limitations and challenges.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution of this work is demonstration of optimal transport shares the same formulation as probabilistic bisimulation metrics, which is popular in the practice of reinforcement learning (RL). By establishing that solving for optimal transport is equivalent to computing bisimulation metrics, the authors creates a novel link between RL and optimal transport theory.
- The formulation of OT on MDP is novel. This work provides an new prospective of Bisimulation metric between MDPs.
- Theoretical analysis looks good.
- Two computational solutions (Sinkhorn-like iterations) were proposed and tested in experiments sections.

Weaknesses:
The theoretical guarantees are based on the assumption of perfect projection steps $m=\infty$, which is not practical. As stated in line 290, such exact computation is unnecessary in practice. This claim needs a further theoretical support.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yXW2dCTQdi;"REVIEW 
Summary:
The authors propose a principle for selecting actions to drive recurrent neural network activities which aims at maximizing the variability of the neural activity while avoiding unwanted states. They define unwanted states as states where no action is possible, and use a reinforcement learning framework to select the input and analyze the coupling between action choice and network dynamics. They apply their networks and input selection to a few tasks where maximizing the entropy within some boundaries is defined as success, and show that the network performs those tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The framework is, to the best of my knowledge, original. It is also sound in its analysis, and well explained.

Weaknesses:
I have two concerns, the first one is (for lack of a better word) teleological, the second more practical.
My first concern is that the authors use the word performance often and refer to their networks as solving the task. But from what I can read, the ""task"" of filling as much of the space (MOP) as possible was never given to the agent/input controller. Thus, if the task of the network is to maximize its entropy while remaining on a bounded region, and the R network is not taught to maximize entropy, we can hardly argue that the R network failed. I get that the point that MOP can be useful for some problems focusing on exploration, but it is a bit odd to talk about solving tasks and performances when such performance was not told to the agent, but rather an agent was build for that task (or for another one in the case of R). Please correct me if I misunderstood.
My second concern is that an agent with a binary value function which chooses random actions (value 1) except if those have a punishment (value 0). Such value might be easy to learn (it only requires learning a boundary, which is smooth in the problems presented), and if the state space of the actions is large, random choices could be very close to MOP (because a random action sequence is likely to go through many states). While this is not necessarily true, it is worth checking, as it would make the whole MOP less impactful.

Technical remarks:
- The addition of stochasticity to the R network is a bit tricky, because the MOP agent never had the problem that it might ""accidentally"" jump into the terminal state if it did not want to. Thus, for ""survival"" it might be better to simply stay in some very small region that is as far as possible from the terminal states, because there we find the smallest chance of accidentally going to the wrong region. A better option would be to give R a random action and then ask if it wants to take it. But I suspect that this would give a high variability.

Minor issues about literature and references
- When the authors mention that usually recurrent neural networks tend to have neurons with saturation, it seems like an unfair comparison. A network trained with a specific task do not have an incentive to maximize the number of states, but this could be added to the loss function (if there is one explicitly) or simply enforced by some intrinsic plasticity rule, as some works in reservoir computing have done (both in Echo State Networks for machine learning and biological models such as SORN). Also, it might simply be the case that for a given task it is better to be close to the saturation.
- In the discussion the authors rightfully mention the variability found in songbirds. But they do not note that it agrees with the works that they mention in the introduction (ref 29-35) which use variability only during learning, and that they use as a current limitation motivating their work. It would seem natural from a neuroscience point of view that variability is suppressed during courting by some executive brain area, rather than assuming that the area that generates the song naturally knows when to change behaviors. 
- The argument that terminal states are those where the agent has no available actions is a bit limiting. Any task that corresponds to reaching a goal (for example a location) leads by definition to a reduced action space, at least in practice (the agent would not leave that position). A note about this would be beneficial.

Limitations:
There is no negative societal impact.
I think the limitation of learning a policy is fair, but there are more important concerns, namely

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In natural behaviors, there’s usually variability despite being able to perform tasks with high performance. This paper aims to understand whether it’s possible for neural networks to have high variability while maintaining high task performance and being able to switch to deterministic behavior modes when needed. 
This paper uses an RNN with fixed weights to model the state of the environment and how it’s affected by the action of the agent. The  agent is modeled by a controller which aims at optimizing the occupancy of the action-state space. This is achieved by having a reward that increases when the agent selects an action that is less likely with the current policy. The optimal value function, and in turn the policy, is approximated by using a single hidden layer feedforward NN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents a reward function that maximizes the action path entropy, and provides interesting examples of how this network performs  in three different example tasks. The writing is clear and easy to follow.

Weaknesses:
1.	The tasks are mostly limited to setting several terminating states, so that the MOP network learns to avoid the terminating states while maximizing the entropy. I wonder how general this type of tasks is, or how more interesting RL tasks may or may not be formulated this way. 
2.	The motivation of this paper is unclear to me. The authors aim to show that NNs can have high variability with good performance, in order to match natural behaviors, and to propose possible mechanisms for neural variability. However, there is no comparison with experiments to show how well the behavior of the NN matches natural behaviors, or to show how the proposed reward may be superior. The authors also do not explain the generation of neural variability, but instead directly enforce variability in the MOP network.

Limitations:
limitations are covered in the 'Weakness' section.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper applies the maximum occupancy principle (MOP) -- previously introduced as a normative theory of behavioural variability -- to recurrent neural networks, thereby proposing MOP as a normative theory of neural variability. The MOP postulates that an agent seeks to maximize future occupancy of its state-action space. A key insight of the previously published MOP paper (ref 42) was that an MOP-following agent naturally learns “good” behaviour by actively avoiding terminal states (e.g. death) as those imply very reduced future state occupancy. This effect is demonstrated again here by showing that MOP-following RNNs can be made to avoid specific activity patterns, but remain maximally variable otherwise. In terms of methods, the main challenge here was to approximate the MOP value function for nonlinear RNNs -- the sole determinant of the optimal policy. The authors do so by training a NN value function approximator on a regression objective derived from the self-consistency (Bellman) equation that the value function must satisfy. The framework is then applied to a few toy setups, including a context-dependent drawing task. Some technical limitations are discussed.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The idea of applying MOP to RNNs is potentially interesting, as it provides a new normative theory of neural variability that will be interesting to confront to neural data -- this paper provides some technical foundations for investigating this hypothesis further. The paper is technically well executed.

Weaknesses:
There is some, but not an awful lot of, added value relative to ref 42.  The MOP is a creative new framework, but the idea that biological agents learn what _not_ to do _instead of_ learning what to do seems a hard sell. The drawing tasks of Figs 3+4 seem carefully designed to demonstrate that MOP-following networks can achieve some “positive” functionality by exclusion, but I have a hard time imagining how the framework would scale to even simple control tasks like swinging a pendulum up; many such tasks are defined by what the agent must do, and many suboptimal states are actually not at all absorbing / terminal. I think the paper probably ought to discuss these limitations in more depth.

There is at least one other normative theory of neural variability that ought to be mentioned in the intro more explicitly: sampling-based probabilistic inference, where variability represents uncertainty. (Ref 26 is cited for ""nonlinear network interactions leading to variable activity patterns"" but has nothing to do with networks. Echeveste et al 2020 by the same group might be more appropriate in this context.)

In summary, I think this idea is potentially interesting -- I view it as a putatively useful theoretical framework for studying how brains learn from bad outcomes (which engage a very different system from the brain's dopaminergic reward system). However, the paper as it currently stands is rather incremental and perhaps not of broad appeal to the NeurIPS community; I would strongly encourage the authors to explore the ramifications of the neural-MOP framework for neuroscience, articulating predictions for neural variability in specific setups where neural data is available for confrontation.

Limitations:
Some technical limitations are discussed but it would be nice to more clearly spell out the conceptual limitations (c.f. above).

EDIT: following discussion with the authors, I am raising my score from 5 to 6.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a mechanism to induce variability in ""reservoir"" recurrent neural networks without impinging upon task performance, by maximising the cumulative entropy of future states and actions/behaviors. These actions are provided by a controller network to the reservoir as input currents. The authors demonstrate through experiments that the induced variability does not come at the cost of adhering to constraints on energy or specific neuronal activities, or task performance (there is no explicit reward in these tasks for the proposed framework, and so this is measured by the survival time, i.e., timesteps until a terminal state is reached or a constraint is violated). Comparisons with networks without the input current modulation or those with explicit rewards show that the former are unable to properly satisfy task constraints, while the latter find overly conservative or ""risk-averse"" solutions that suppress variability. The demonstrations also show that the proposed framework leads to networks switching between more deterministic modes of computation (lower action entropy) near terminal states/constraints and more stochastic (higher action entropy) ones otherwise. Overall, this paper provides a novel perspective on how neural variability could be maximized while still allowing for accurate performance and avoiding terminal states, especially when an explicit reward function is unavailable or undesirable.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-motivated, clear, and provides a unique perspective on how there could be controlled variability in a system without negatively affecting task performance. The figures and overall presentation are good and clearly demonstrate the validity of the central claims.
2. The experimental results include specific controls for the proposed mechanism – the authors show results for networks without any input current modulation and also networks with explicit constraints imposed by a reward function.
3. In the appendix, the authors show that their central claims are largely valid even when there are other sources of variability such as intrinsic noise in the networks.
4. While the tasks are simplistic, they are well-designed and quite interpretable, allowing the claims to be validated easily through the visualizations.

Weaknesses:
1. This is a limitation acknowledged by the authors, but it seems like the computational complexity for the framework is quite high, so it is difficult to evaluate how this would scale to more complex tasks or multi-task settings.
2. To my knowledge, and perhaps I have missed this, but the authors have not provided clear connections to the biological inspirations for the proposed mechanism, such as perhaps neuromodulatory mechanisms, or comments on its biological realism. Could the authors elaborate on this and are there any testable predictions for this model of neural variability?
3. While the simplicity of the tasks is an asset, it would also be important to see what happens when there is a greater diversity among tasks in a multi-task setting (see Yang et al. [1]), and when performance is not only linked to constraint satisfaction or ""survival"". For example, would this framework impede performance when one of the tasks explicitly requires less variability as is perhaps the case in a memory task (see Yang et al. [1] again for examples)?
4. This is a minor point, and a suggestion rather than a weakness, but there are some interesting recent works that could be mentioned to strengthen the background:
    1. Takasu & Aoyagi [2] discussed an input current modulation mechanism and how it affects the Lyapunov exponents of reservoir networks' dynamics – specifically, suppressing chaos and ensuring networks are at the edge of chaos to enable effective information processing (and is thus related to the variability in these networks; also related to [59] from the paper). It would be interesting to briefly compare/contrast the proposed mechanism/goals to that proposed in [2].
    2. In lines 33-37, the authors discuss works where internal synaptic noise is proposed as a mechanism for neural variability, and mention that some works use this assumption to ""describe variability during spontaneous activity–in the absence of sensory stimuli"". Works such as Asabuki & Fukai [3] and Krishna et al. [4], where such a mechanism is assumed and used to describe properties of spontaneous activity, could be discussed (in addition to [18, 20] from the paper) to provide a better idea of the implications of such mechanisms.

**References:**
1. Yang et al. “Task representations in neural networks trained to perform many cognitive tasks.” Nature neuroscience vol. 22,2 (2019): 297-306.
2. Takasu & Aoyagi. “Suppression of chaos in a partially driven recurrent neural network.” Phys. Rev. Research 6, 013172 (2024).
3. Asabuki & Fukai. “Learning rules for cortical-like spontaneous replay of an internal model.” bioRxiv (2023): 2023-02.
4. Krishna et al. “Sufficient conditions for offline reactivation in recurrent neural networks.” The Twelfth International Conference on Learning Representations (2024).

Limitations:
The authors have adequately discussed limitations related to computational complexity and not learning the policy in the Discussion section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yWq89o19wf;"REVIEW 
Summary:
This paper models dynamics of both users and creators in a recommender system. The user features shift in the direction of the content recommended to them. The creator dynamics are strategically motivated i.e. they try to align content to attract their audience, to increase profit. 

The authors then provide sufficient conditions for this model of dual dynamics to converge to polarization under a natural assumption that each creator has some non zero probability of being recommended to every user.

The paper then discusses four real world recommendation designs, and whether they cause polarization or multiple clusters etc. They also provide results on synthetic and Movielens data complementing theory results and show that certain recommender designs do lead to polarization vs diverse clusters.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- This paper is the first to consider dynamics of both users and creators in a recommender systems and provides sufficient analytic conditions for polarization
- They apply this theory to 4 natural designs: (1) Top-k,  (2) Truncation, (3)Diversity boosting and (4) Lower bounding probability. They show that rules (3, 4) lead to polarization and rule (1) leads to diverse clusters. This section is particularly insightful.
- The experimental evaluation with synthetic and Movielens data is also insightful and complements the theory. The softmax probability leads to diminishing creator and recommendation diversity over time. They also study top-k probability and show how lower k is better for higher creator diversity, recommendation relevance.

Weaknesses:
A criticism I had while reading the paper are gaps in literature for the discussion on dynamics in recommender systems. In addition to [Eilat & Rosenfeld] referenced in the introduction, [1,2,3,4,5,6] consider creator dynamics in recommender systems. These works assume static user features and provide results on content at equilibrium and user welfare. In the context of these works, it would be beneficial to highlight how your work is the first to consider both creator and user dynamics.

[1] A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers (Ben-Porat and Tennenholtz)

[2] Supply-side equilibria in recommender systems (Jagadeesan et al)

[3] How Bad is Top-k Recommendation under Competing Content Creators? (Yao et al)

[4] Modeling content creator incentives on algorithm-curated platforms (Hron et al)

[5] Producers Equilibria and Dynamics in Engagement-Driven Recommender Systems (Acharya et al)

[6] User Welfare Optimization in Recommender Systems with Competing Content Creators (Yao et al)

Limitations:
The authors discuss limitations of their results in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper explores the dynamics between users and content creators in recommender systems, highlighting the dual influence where users’ preferences are shaped by recommendations and creators modify their content to align with what is more likely to be recommended. The study defines a model called user-creator feature dynamics to capture these interactions, demonstrating that such systems are prone to polarization, resulting in a loss of diversity. The paper then examines various approaches to mitigate polarization and improve diversity, finding that relevancy-optimizing methods, such as top-k recommendations, can prevent polarization more effectively than traditional diversity-promoting approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper provides an interesting perspective by addressing the mutual influence between users and creators in recommender systems. The theoretical results and experimental validation using both synthetic and real-world data look credible. The writing is overall easy to follow.

Weaknesses:
1. There are two lines of works focusing on modeling content creator dynamics and user preference evolving dynamics that are neglected by the authors. I listed several representative works and it would be great to include a comprehensive literature review regarding these works in the related work section.

2. One of your main observation (larger $\beta$ leads to higher creator diversity and alleviated polarization) is actually pointed out in [1] under a similar model, where content creators compete for a fixed user population (see section 3.2 in [1]). And another main observation in section 5.3 that smaller $k$ improves diversity does not echo the result in [2], which shows that larger $k$ improves the total creator utilities. It would be better to include some detailed discussions regarding these two works. 

3. The user/creator preference updating dynamics need more justifications and empirical evidence.

4. The dynamical model makes some sense to me, but it would be more interesting to understand whether the observations still hold in the presence of noise. If the noisy version is hard to analyze theoretically, additional simulation results could also be valuable.


[1]. Modeling Content Creator Incentives on Algorithm-Curated Platforms
[2]. How Bad is Top-K Recommendation under Competing Content Creators?
[3]. Online recommendations for agents with discounted adaptive preferences
[4]. Recommender systems as dynamical systems: Interactions with viewers and creators
[5]. Learning from a learning user for optimal recommendations
[6]. Supply-side equilibria in recommender systems

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies how recommendations become polarized over the long run when user and creator features dynamically change over time. The authors theoretically prove that, under the assumption that every creator can be recommended to every user with some non-zero probability, recommender systems will eventually converge to polarization. They also simulate some real-world models, including top-k recommendation, truncation, diversity boosting, and lower-bounding probabilities in a long-term setting. The key observation is that top-k recommendation (i.e., only recommending top-k items to users) can reduce polarization to some extent, while existing diversity-boosting methods will worsen polarization when user/creator features dynamically change over time in the system.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors provide both theoretical and empirical evidence showing that relevance-focused recommendations (as opposed to diversity-focused recommendations), which harm diversity in a static setting, are actually effective in improving diversity in the long term. This observation is somewhat counter-intuitive to previous beliefs, making it very interesting.
2. The authors conducted simulations with both synthetic data and real-world data (i.e., Movielens) using four diversity and relevance-related measures. Additionally, the analysis with sensitivity parameters in softmax is insightful and supports the authors' main claim.
3. Studying diversity in a dynamic setting is novel.

Weaknesses:
1. Despite the novelty and interestingness, I have concerns about the key assumptions of the theoretical and empirical analyses. The assumption that all items can be recommended to users is not realistic. In practice, almost all recommender systems rely on top-k recommendations for either effectiveness or resource constraints like screen size. For example, on platforms like Netflix or Amazon, customers can only see a certain number of items on the webpage (i.e., p=0 for items that users can't see). Even if they can scroll down and the system continually recommends new items, they cannot physically see all items in the system. Thus, I believe the top-k setting is the most realistic and natural for real-world scenarios, and this seems like a hole in the authors' analyses. In this sense, the measures for empirical analysis should also only consider top-k items, not all items.
2. For the real-world designs, it would be more extensive if users included trustworthiness-aware recommender systems that consider dynamic/continual settings. For example, [1] consider performance difference between two different user groups when the user/item features are continually updated over time in the systems.
3. For the analysis with Movielens, considering the interaction timestamp in the simulation would more accurately reflect real-world scenarios, for example, for determining the true labels.

[1] Yoo et al., Ensuring User-side Fairness in Dynamic Recommender Systems, WWW'24

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
yWSxjlFsmX;"REVIEW 
Summary:
This paper comprehensively investigates the possibility of leveraging Mamba for trajectory learning. The authors take Decision Mamba as a playground and analyse the performance of this model over trajectory learning scenarios (gym/mujoco) from several aspects. A group of conclusions are attained through rigorous experiments, which is solid and potentially valuable for further researches realted with Mamba.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Novelty: given that Mamba is still at its exploratory stage, this paper positively probes Mamba's potential for tranjectory learning, with surprising results that with some specific pre-conditions, Mamba is more suited than Transformer.

Weaknesses:
1. Most discoveries in this paper have been implicitly discussed for several months within the community, while it is firstly presented officially in this paper. Besides, these dicoveries lean to be emparical  evidence, which is relatively shallow. This would make this paper's technical contribution weak. I would appreciate the authors if they could provide more in-depth explanation over these discoveries, in particular: 1) Transfomer-like model favors short sequence. 2) The significant role of the hidden attention. 

2. Although the experimental results are solid, I found that this paper is more suitable for Benchmark Track, since the technical novelty revolves around benchmarking Decision Mamba. 

3. Figure 1 (the title and the pic) should be improved. For now, it confuses me, especially the corresponding relationship between the text content (title) and the illustration.

3. Minor: line 295: may more suitable -> may be more suitable

Limitations:
The authors provide a brief limitation summary in Conclusion. I would appreciate the authors if they could refine this part since the ""limitations"" listed there do not seem like limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates how Mamba perform in trajectory optimization in offline RL with ablation analysis on mamba's data input structures and architectural structures and shows Mamba DT can achieve SOTA performance with less parameters.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper writing is good, the visualizations look good.
2. The input concatenation experiments provides useful practical insight also for other sequence-based decision-making models 
3. The paper provides a detailed analysis of how various components of Mamba, such as the hidden attention mechanism and different residual structures, influence performance.

Weaknesses:
1. Finding 3 is not very surprising on the tested MDP environment, since they by definition should focus only on recent states. It will be interesting to explore how this mechanism might perform in environments with long-term dependencies where the Markov property does not hold strictly.
2. Only tested on standard Atari and mujoco tasks. How would mamba perform on tasks that requires long horizon planning skills? such as maze navigation or tasks with delayed rewards.

Limitations:
please see weakness

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work introduces Decision Mamba (DeMa) to address the challenges in offline RL posed by the large parameter size and limited scalability of Transformer-based methods. DeMa aims to achieve similar performance to Transformers with significantly fewer parameters.  DeMa surpasses the DT with significantly fewer parameters in the benchmarks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Extensive evaluations demonstrate the effectiveness of DeMa, highlighting its superior performance and efficiency compared to existing methods.

2. DeMa provides a novel solution to the parameter size and scalability issues in trajectory optimization.

Weaknesses:
1. Some symbols are not defined before use.

2. This paper seems to have little relation to RL and appears more like a method applicable to all trajectory optimization.

3. There is too little discussion on the relationship to RL in sections 3.2 and 3.3.

Limitations:
The suggestions have been claimed in ""Weaknesses"" and ""Questions"".

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zuWgB7GerW;"REVIEW 
Summary:
This paper introduces Accordion Networks (AccNets), a novel neural network structure composed of multiple shallow networks. The authors propose a generalization bound for AccNets that leverages the F1-norms and Lipschitz constants of the subnetworks, demonstrating that these networks can break the curse of dimensionality by efficiently learning compositions of Sobolev functions. The paper also provides theoretical insights and empirical validation, showcasing the superior performance of AccNets in learning complex compositional tasks compared to shallow networks and kernel methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The introduction of Accordion Networks (AccNets) as a novel neural network structure is a creative and original contribution. The paper provides a thorough theoretical analysis supported by empirical evidence, ensuring the soundness of its claims. The ability of AccNets to break the curse of dimensionality by learning compositional functions efficiently addresses a fundamental challenge in high-dimensional learning tasks.

Weaknesses:
1. The practical implementation of the proposed regularization methods might be challenging, particularly the first one requiring infinite width. 

2. The paper mentions the difficulty in optimizing Lipschitz constants, which could be a limitation in practical applications.

3.  Additional experiments on more diverse real-world datasets could further demonstrate the robustness and generalizability of AccNets.

4. Although the author has discussed the differences between DNN and AccNet, there is still not enough information for me to be sure in which settings to use AccNet and in which settings to use DNN. More clear differences and applicable conditions, especially the shortcomings of each need to be pointed out.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a generalization bound for deep neural networks that describes how depth enables models to learn functions that are compositions of Sobolev functions. To do this, they both prove a generalization bound for compositions of accordion networks (densely connected networks with a low-rank weight structure) and for compositions of Sobolev functions. They then present a sample efficiency result for different kinds of regularization on accordion networks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
I really liked this paper and would like to see it accepted to NeurIPS. It addresses an important question: how does depth change generalization bounds for deep neural networks? To my knowledge, not many papers so far have addressed this question and I found the findings presented here very interesting and well embedded within prior methodology.

I also found the paper very well written. I found it easy to follow along despite the highly technical nature of the results (note that I did not check the proofs in particular detail). I especially appreciated the remarks explaining different potential extensions and limitations.

Finally, the theory appears to be able to explain certain empirical phenomena (in networks trained under realistic paradigms) at least qualitatively (though note that I had a few questions I will mention under weaknesses and questions). This indicates to me that it is a promising way for thinking about generalization in deep neural networks.

Weaknesses:
1. I would like to see a more thorough comparison with shallow networks and generalization bounds, as this comparison is a central argument for the usefulness of the presented theory. While it is clear how the findings for the shallow network are a special case of the findings on the deep networks (as presented in Thm. 1), it remains a bit unclear to me how the theory can explain improved generalization in deep compared to shallow networks. The authors certainly present different several pieces of evidence on this: both Fig. 1 and Fig. 3 demonstrate that shallow networks exhibit worse scaling. I also appreciated the theoretical explanation of a particular contrast in l. 256-261. However, I think it would be really useful to provide a general theoretical explanation for this difference and test it empirically: would it be possible to extend the theoretical comparison in l. 256-261 to the general experimental setup studied in the figures --- and if so, would this theoretical comparison predict the conditions under which deep networks have the strongest advantages over shallow networks (or perhaps the conditions under which they don't perform that much better)? Not only would this serve as a useful validation of the theory, I think it would also provide a more extensive intuition for the authors' findings.

2. I appreciated the fact that the authors compare their findings with related work wherever this becomes relevant. However, I think a (potentially brief) section comparing the results here to other theoretical investigations of depth in deep networks (perhaps using different approaches) would be useful. 

3. The linked codebase does not contain the notebooks indicated in the README as far as I can tell and therefore currently can't be used to directly reproduce the findings.

4. I believe the figures would still benefit from error bars or some other indication of the overall statistical error in the findings. I agree that the main contribution of this paper is theoretical, but since the experiments test the empirical validity of the theory, I believe it is nevertheless important to get a sense for the overall deviation in these findings (e.g. across model seeds). If the authors are concerned about a lack of clarity, they could leave the bars out of the main figures but add supplementary figures with error bars. Moreover, some of the lines in Fig. 1 do contain error bars and it would be good to clarify what these error bars represent.

Limitations:
The authors adequately discuss the limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce accordion networks (AccNets), which are compositions of multiple shallow networks. By leveraging prior workthat computes norm-based generalization bounds for shallow two-layer networks, the authors bound the complexity of a deep AccNet (as measured by its F1 norm) but the sum of the complexities of the individual shallow networks. They empirically observe that the rates predicted on real-world data are roughly representative of the trained networks, and are indeed much better than those for kernels trained on the same tasks. They put forth a nontrivial scaling law for the excess risk: $N^{-\mathrm{min}(1/2, \nu_g/d_{in}, \nu_h/d_{mid})}$ for an Acc Net compared to $\mathcal L \sim N^{-\mathrm{min}(1/2, \nu_g/d_{in}, \nu_h/d_{in})}$ for a kernel in terms of the dimensionalities $d$ and Sobolev constants $\nu$ of the respective spaces and functions. From this, the authors obtain predictions of several phases, that they put forth experiments to verify.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper tackles a very important open question in the theory of deep learning, for which not much progress has been made. By creatively leveraging results for shallow network in composition, the authors arrive at a nontrivial bound for deep nets. The empirics are a very compelling and welcome part of the paper. The phase diagrams illustrate the nontrivial predictivity of the theory, especially at the level of the rates. This may have important implications for scaling laws. Modulo minor revisions in discussion and exposition, the whole paper is quite readable for a relatively broad audience.

Weaknesses:
I am not sure how compelling the phase plots in Figure 2 are. The bounds in general are extremely loose, however the comparison of the rates in Figure 2c and Figure 3 is very promising. In general, however, it is the experience of the reviewer that measuring a rate is an extremely finicky business. It is therefore important to add a section in the appendix explicitly stating how the rates were obtained and measured. I also strongly encourage the authors to make the code for all figures public. 

Because they are used very early on throughout the paper, it is the opinion of the reviewer that the notions of F1 distance and Sobolev norm should be defined earlier on in the paper. Without this, it seems like the audience will be constrained to the set of learning theorists familiar with these terms. However, if these terms are defined early on, the paper becomes remarkably accessible to a much broader audience.

Limitations:
Given the theoretical nature of this work, it is unlikely to have major social implications.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
znBiAp5ISn;"REVIEW 
Summary:
There's a large performance gap for graph tasks, especially graph classification tasks, between the spiking neural networks and artificial neural networks. The authors proposes the problems as the neuron's under starvation and illustrated the reason of the problem. To solve the problem, TAS-GNN was proposed.

The main contributions of the paper are as follows:
1: Starvation problem of spiking neurone in GNNs in graph classification tasks are identified.

2: A strategy was proposed to address the spike frequency deviations on the basis of the correlation between graph topology and spike frequency patterns.

The authors conduct experiments on 5 popular datasets and use several different designs of GNN layer. The results show competitive potential of the TAS-GNN.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1:This is a well-written paper, from the formulation of the problem to the solution. The author's motivation for the use of graph topology is clear.

2:The method of using topology-awaregroup-adaptive neurons shows competitive results compared with other baselines. The ablation study makes the result more persuasive. 

3: The Figures in the paper are quite straightforward, easy to follow.

Weaknesses:
1: The name of the paper is ""Topology-Aware Spiking Graph Neural Networks"". However, as I can tell the only graph topology used in the method is nodes degree, which is used to group the neurons. I wonder if it is appropriate to name it as ""topology aware"", or the author can explain it more.

2: The analysis regarding the performance of the method is lack of discussion. For instance, in some datasets, such as MUTAG and IMDB-Binary, the proposed method achieve quite competitive results while in PROTEINS it doesn't. It's betted to explain what cause the phenomenon, like the characteristics of the datasets? Also, in table 2, the results of GAT and GAT+TAG in IMDB-Binary are the same. It's better to make an explanation about them.

3: There're several typos and basic grammar mistakes in the paper that will affect the presentation of the paper. In line 120 "" and apply is to""; The sentence in line 123 is hard to understand

Limitations:
na

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper primarily discusses integrating Spiking Neural Networks (SNNs) into Graph Neural Networks (GNNs) to address several key challenges in graph classification tasks. Specifically, the paper proposes a new method called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) which leverages the topology of graphs to improve the performance of spiking neural networks in graph classification tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
（1）The authors clearly articulate the performance gap between existing Graph Neural Networks (GNNs) and Spiking Neural Networks (SNNs) in graph classification tasks.
（2）The authors conduct an in-depth analysis of the performance degradation of spiking neural networks in graph classification tasks and introduce the ""neuron starvation"" problem.
（3）The authors propose topology-aware group-adaptive neurons (TAG) based on the graph's topology, a novel approach that helps address the neuron starvation issue.
（4）The authors provide a detailed description of how to convert input graphs into spike representations, perform message passing, and classify the graphs.
（5）The authors validate the method's generalizability and effectiveness by using multiple public datasets (such as MUTAG, PROTEINS, ENZYMES, NCI1, IMDB-BINARY) in the experimental section.

Weaknesses:
（1）The authors mention several application areas and challenges, but the references and comparisons to existing literature are not sufficiently comprehensive.
（2）Although the methodology section describes the main steps, it lacks detailed descriptions of some key aspects such as threshold initialization and the specific training process.
（3）Although there are some ablation studies, the analysis of the individual contributions of each component is insufficient, making it difficult to determine the specific impact of each component on the overall performance improvement.

Limitations:
(1) While the paper discusses the neuron starvation problem and the sensitivity of initial thresholds, it does not explicitly outline the broader limitations of the proposed TAS-GNN method. It would be beneficial to include a dedicated section that explicitly lists and discusses the limitations of the current work.
(2) The paper does not thoroughly address how TAS-GNN scales with extremely large datasets or very high-dimensional graphs. Including an analysis of computational complexity and memory usage for larger graphs would provide a clearer understanding of the scalability limitations.
(3) While multiple datasets are used, the paper could further discuss the generalizability of TAS-GNN to other types of graph-based tasks beyond classification, such as regression, clustering, or even dynamic graphs.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel approach called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) to address the performance gap between spiking neural networks (SNNs) and artificial neural networks (ANNs) in graph classification tasks. The authors identify a ""starvation"" problem in spiking neurons within GNNs, where many neurons do not emit any spikes during inference, leading to severe information loss. This problem is more pronounced in graph classification tasks, where the test set graphs are independent from the training set, unlike in transductive or inductive learning settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	This paper identifies a critical ""starvation"" problem in spiking neurons within Graph Neural Networks (GNNs), where many neurons do not emit any spikes during inference, leading to severe information loss. This problem is more pronounced in graph classification tasks, where the test set graphs are independent from the training set
2.	The paper proposes a novel approach called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) to address the graph classification problem.

Weaknesses:
1.	The authors use the node degree instead of the concept of topology, there’s a large gap between the graph topology and node degree.
2.	The authors solve the graph classification task as a contribution, which is not a significant challenge for spiking graph neural networks.
3.	The advantage of Spiking Neural Networks (SNN) is their low energy consumption. However, the paper does not mention the feature, so it is unclear why graph neural networks should be combined with SNN. The motivation behind TAS-GNN is not clear.

Limitations:
The authors adequately addressed the limitations.  The authors should discuss more details of the potential negative societal impact of the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes topology-aware spiking graph neural networks with adaptive thresholds based on a group of neurons for graph classification. The paper first diagnoses the poor performance as the existence of neurons under starvation caused by the graph structure. Then the paper proposes the adaptive threshold among neurons partitioned by degrees, as well as the learnable initial threshold and decay rate to reduce the sensitivity. Experiments on several datasets show superior performance of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes the first SNN design to target graph classification.

2. This paper identifies the starvation problem and proposes a novel topology-aware group-adaptive technique.

3. Experiments show superior performance on several datasets, some outperforming ANNs.

Weaknesses:
1. The proposed method seems to be a hybrid ANN-SNN model rather than a pure SNN design. The paper did not discuss how this will affect the deployment of the model on potential neuromorphic hardware, since SNNs mainly target those hardware to obtain energy efficiency.

2. The paper did not discuss the (theoretical) energy efficiency estimation, which is a major motivation for considering SNNs as stated in Introduction.

3. Or if the motivation is to get models with better performance than ANN, then Table 1 does not include state-of-the-art ANN results for comparisons.

Limitations:
The authors discussed limitations in Appendix A.1.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
zeNwOAcb4q;"REVIEW 
Summary:
In this work, the authors proposed an approach to estimate the instance-dependent transition matrix in order to reliably learn from noisy labels. The idea is to use a condition diffusion model to estimate the transition matrix by using the pretrained extracted image features as the conditions. Once the transition matrices are estimated, the classifier is learned through the corrected cross entropy loss. Experiments are presented to compare the performance of the approach with other baselines using both synthetic and real noisy datasets.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper is easy to read and notations are clearly stated

Weaknesses:
The main weakness is the lack of support and discussion in substantiating the idea. Experiments are insufficient to support the claims.

Limitations:
No limitations are discussed

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals with the problem of supervised learning from noisy labels, where the label noise is modeled using instance-dependent label transition probability matrix. Mainly, this work attempts to leverage conditional diffusion model in order to obtain a generative model of transition matrix conditioned on the sample features. To that end, this work first generate pseudo paired samples $( x_i, T_i )_{i=1}^N$ using existing method (VolMinNet). Secondly, a conditional diffusion model is trained that generates $T_i$ given $x_i$. Finally, the classifier is trained taking into consideration the estimated transition matrix from the diffusion model.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem considered is of interest to the broad ML community
2. Adequate experimental settings, baselines, and ablations are provided for numerical validation.
3. The attempt to apply diffusion model is novel.

Weaknesses:
1. The technical soundness of the proposed method is questionable. Essentially, the proposed method trains a conditional diffusion model using paired samples $(x_i, T_i)$. If we consider the true transition matrix as $T(x)$ for a sample $x$, then the idea of the proposed method is to train a conditional generative model $p( T(x) | x )$. There are several issues with this attempt and the proposed implementation:
   (a) The authors use pseudo transition matrix $T_i$ generated from a sample-independent method (VolMinNet). $T_i$ only depends upon the cluster assignment of $x_i$. The diffusion model, at best, can approximate the conditional distribution $p( T_i | x_i )$. This has no clear relation to $p(T(x) | x)$. Therefore, in principle, the transition matrix generated by the trained diffusion model cannot be better than that returned by VolMinNet.
   (b) Second, the transition matrix is modeled as a deterministic function of sample, i.e., only one $T(x)$ exists for a given $x$. Therefore, it does not make sense to learn a generative model for $p(T(x) | x)$, since it is a degenerate distribution (probability of all other matrices should be zero except the true $T(x)$). 

2. Another hint at why the proposed method should be limited by the pseudo paired sample distribution is that the diffusion model training part (which is ultimately used as transition matrix estimator) does not require available noisy labels. Hence, no extra information can be extracted about the true transition matrix $T(x)$ beyond the information captured by the pseudo paired samples $(x_i, T_i)$. 

2. It is unclear where the performance gain in empirical results is coming from. The manuscript does not provide any intuitive or theoretical explanation to justify the quality of their estimator. Moreover, no rationale for the algorithm design is provided.

Limitations:
Limitations are not adequately discussed.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the estimation of the transition matrix with instance-dependent label noise. They used a diffusion model for this estimation. By applying a diffusion process to the transition matrix, the diffusion model is trained to generate transition matrices from a prior distribution. The instance-wise generated transition matrix is then used to train the classifier with a forward cross-entropy loss. The improvement of the method is demonstrated by experiments on benchmark and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The instance-dependent label noise scenario is a challenging task.

Weaknesses:
* The reason for generating the transition matrix using a diffusion model is unclear.
  * The instance-dependent transition matrix is the target to be estimated, but it is uncertain what role training a diffusion model to generate the transition matrix without a fixed target.
  * In addition, as mentioned by the authors, the transition matrix must be satisfied: the entries are greater than 0, the row sum is to be 1, and the diagonal entry is typically the largest. However, these considerations have not been taken into account in the construction of the diffusion process. Although a transformation method is proposed in Section 3.4, there is no discussion of how this affects the training of the diffusion model.

* Pre-trained features are fed into the diffusion network, but their impact on the diffusion process has not been analysed. This could be seen as providing additional conditional information during the diffusion process, implying that this diffusion model might be a conditional diffusion model. It would be better to discuss these consideration.

* In Algorithm 3, it appears that the diffusion model is trained in order to generate the initialized $T_i$. I wonder if the desired training is for the initialized $T_i$ to be generated perfectly as is. This could lead to a transition matrix that might not contain instance-dependent information, raising questions about the mechanism by which diffusion training introduces variance.

* The diffusion training seems to take a considerable amount of time, which needs to be analysed. If it takes a long time, the performance improvement may not be significant in comparison.

Limitations:
They mentioned the limitations only briefly in the experimental section. I have noted additional limitations that I perceive in the Weaknesses part.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
zFHJUSTZka;"REVIEW 
Summary:
This paper propose OAIF, an online method to align language model with human preference where feedback from language models serve as a surrogate of human feedback. The key of OAIF is to use online generated preference pair along the training process. Experiment results shows that, by switching offline preference dataset to online dataset labeled by other language models, the generated responses are more aligned with human preference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The strengths of the paper are listed below:

1. This paper introduces OAIF, which is featured by using on-the-fly generated preference pairs and AI-provided labels.
2. The author conducted experiment on various direct alignment methods and the results consolidate the claim by the authors

Weaknesses:
My questions and concerns are listed as follows:

1. My first concern is regarding the novelty of the paper. It seems that the language model annotator is essentially a preference model. Therefore, OAIF can be seen as a method of online direct alignment algorithm with access to a preference model. The author mentioned several previous work with on-policy generation and online feedback but in need of a reward model. How is OAIF different from different from these method if we simply plug in the language model annotator as the reward model in their methods?
2. At line 118 the author pointed out that RM might suffer from distribution shift because the training data of RM might not share the same distribution with $\pi_\theta$. However, it seems to me that using language model as preference annotator cannot bypass this problem since the language models' pretraining corpus or the finetuning corpus relating to preference labeling has a similar distribution with $\pi_\theta$.
3. How is OAIF's performance compared to other online methods like RSO and IterativeDPO? I think that these methods might also be included as baselines since reward model can also be taken by AI annotators.

Limitations:
The limitation is discussed by the author

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work extends offline preference learning methods, i.e., DPO, to a online variant by using LLM as annotator to collect new datasets for further preference learning. The results show that Direct alignment from preferences (DAP) methods win-rate over the offline methods beyond 60%.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Paper is good writing, easy to follow.
2. This online variant provides demonstrates significant performance improvements over offline DAP and RLHF methods through comprehensive evaluations.

Weaknesses:
1. The improvement by extending online is under expectation as it introduces more datasets and training budgets. 
2. The contribution is limited. The only difference compared to the prior method is substituting the reward model of prior methods (Iterative DPO) to LLMs, though I agree the explicitly static reward model may introduce the model distributional shift problem.
3. Some drawings or comparisons are not fair enough. (a). Table 1 explicitly avoids the limitation of this method by leveraging the feedback from LLM, though it is another variant of the ""reward model"". (b). Figure 3, the training step is not an approximate x-axis as the online DPO variant has been heavily fine-tuned offline. 
4. There are no theoretical foundations, or new plausible explanations, aside from more datasets and the online budget, for the further improvement of the online variant DPO.

Limitations:
see Weaknesses

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper applies direct alignment from preferences (DAP) methods, particularly DPO, to online settings where responses are sampled in an on-policy manner and feedback is provided by the LLM annotator in real-time. Extensive experiments demonstrate the effectiveness of these simple ideas.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written, with detailed explanations of introduced definitions and discussions with existing methods. 

The experiments are well-designed, supporting the main idea of the paper. The proposed prompt-controllable approach is particularly commendable.

Weaknesses:
The rationale for why on-policy learning brings performance gains is not well clarified. The cited reference [1] does not provide strong support for this claim. There is no experimental evidence that on-policy sampling encourages exploration. 

Most experiments are conducted with the closed-source LLM Palm; evaluating state-of-the-art open-sourced LLMs would enhance generalizability. 

It is unclear how much of the performance gains are due to on-policy sampling versus online feedback. 

The reasons why utilizing online on-policy data can avoid overfitting and improve performance should be further analyzed and discussed.

References:
[1] Lambert, N., Wulfmeier, M., Whitney, W., Byravan, A., Bloesch, M., Dasagi, V., Hertweck, T., and Riedmiller, M. The challenges of exploration for offline reinforcement learning. arXiv preprint arXiv:2201.11861, 2022.

Limitations:
The computational overhead introduced by on-policy sampling and online feedback is not discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new method called Online AI Feedback (OAIF) for direct alignment from preferences (DAP) that addresses the limitations of existing DAP methods, which rely on static, offline feedback datasets. By using an LLM as an online annotator to provide real-time feedback during each training iteration, OAIF ensures the alignment process remains on-policy and adapts dynamically to the evolving model. Through human evaluations across various tasks, the authors demonstrate that OAIF outperforms traditional offline DAP and reinforcement learning from human feedback (RLHF) methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
OAIF uses LLMs for preference annotation, eliminating the need for a separate reward model and large datasets typically required for RLHF methods. It introduces a new way to address off-policy issues in policy optimization, a significant problem in traditional DPO methods.

The paper is well-written and easy to understand. OAIF outperforms offline DPO and other offline RLHF methods.

Weaknesses:
1. The idea is straightforward but lacks theoretical proof. The proposed method combines DPO and AI feedback, unlike the constitutional AI paper, which integrates PPO with AI feedback. However, this point is minor. Given the abundance of concurrent work [1-7], the authors should further develop the theoretical analysis of their approach to strengthen their method. 

2. Different methods should use an equal amount of training data. In the second epoch of onlineDPO, although the prompts remain the same as in the first epoch, the responses and rank information differ due to online generation.

3. Recent results on Reward Bench indicate that small reward models are more effective than LLM critiques. The iterative DPO methods are similar to OAIF DPO. A performance comparison between OAIF and various iterative DPO methods using cheaper reward models, as both address the off-policy issue, is essential and should be included.

[1] Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint

[2] RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models

[3] RSO: Statistical rejection sampling improves preference optimization

[4] Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682

[5] Hoang Tran, Chris Glaze, and Braden Hancock. 2023. Iterative dpo alignment. Technical report, Snorkel AI.

[6] Self-rewarding language models. arXiv preprint arXiv:2401.10020

[7] Is dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint arXiv:2404.10719.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
zDYXdR3ClP;"REVIEW 
Summary:
This paper introduces a universal image restoration framework UIR-LoRA based on multiple low-rank adapters. UIR-LoRA employs the pre-trained text-to-image diffusion model SD-turbo as the shared component. It utilizes a LoRA composing strategy based on the degradation similarity predicted by CLIP encoder to combine different LoRA modules. Experiments show the effectiveness of the proposed method.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed LoRA-based Universal IR method is easy to understand and follow.
2. The motivation of this paper is very clear to me.

Weaknesses:
1. UIR-LoRA adopts SD-turbo as the pre-trained backbone for image restoration. However, SD-tubo utilizes VAE with high compression rate to encode input images, resulting in severe detail distortion for image restoration. This issue has been widely discussed in recent published works [1,2]. However, the paper ignores this very important issue in the Method Section and only mentions the skip-connections for VAE in Line 223.
2. The degradation-aware router seems to be unreliable. I do not believe that the original pre-trained CLIP Text Encoder can distinguish between different degradations through degraded text representations, such as ""rain"" and ""raindrop"". Therefore, DA-CLIP fine-tunes the original CLIP. But this paper doesn't contain any discussions about this.
3. This paper does not provide complete technical details, such as how the LQ image is used as a condition for SD-turbo. Is ControlNet used, or is it directly concatenated? I do not see any information about this in the paper. 
4. Tab. 1 only reports the trainable Param for UIR-LoRA. I think it's necessary to report the overall Param of the model. In addition, the reported PSNR for DiffBIR is very low. Did the authors add skip-connections to the VAE of DiffBIR for a fair comparison?
5. The visual results in Fig. 3 seem strange. The visual results of Restormer show noticeable artifacts between patches. Do the authors test Restormer using a tiled mode? As far as I know, using a single A100 GPU (Line 251), Restormer can restore the entire image without encountering out-of-memory issues.

[1] Wang, Wenjing, et al. ""Zero-Reference Low-Light Enhancement via Physical Quadruple Priors."" In CVPR, 2024.

[2] Geng, Zigang, et al. ""Instructdiffusion: A generalist modeling interface for vision tasks."" In CVPR, 2024.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes to perform universal image restoration via multiple low-rank adaptation. The key idea is to leverage a pre-trained stable diffusion model as the shared component and transfer it to specific degradations with LoRA adaptation. A degradation-aware router is further proposed to generate weights for LoRA combination based on degradation confidence. In experiments, the authors evaluated their method on multi-degradation and mixed-degradation datasets and conducted several ablation experiments on their core components.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of applying LoRA to a pre-trained SD for multi-task image restoration is promising and interesting.
- The overall presentation is easy to follow.
- The experimental results are good and the ablation studies make sense.

Weaknesses:
- ControlNet is the most popular approach to adapting SD models to other tasks. I'm curious why the authors chose LoRA? As far as I know, LoRA is often used for large language models (with billions of parameters). It would be great to provide more detailed motivation in the introduction.
- In line 123, maybe it's better to use ""concatenate"" or other operators instead of ""add"" to present the unified parameters. Here, the weight $s_k$ can be ignored.
- Can the authors use other SD models as the base model? I believe applying LoRA to a multi-step diffusion process can further illustrate its efficiency.
- In Eq. (4), $s_0 \cdot M_k$ is used in both numerator and denominator, which seems weird and confusing.
- The mixed degradation experiment is cool. It would be interesting if the authors could apply their model to real-world degraded images.
- Line 45: proposed -> propose

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This submission proposes a transfer-learning based strategy to address challenges related to image-degradation restoration. The premise is that a pre-trained generative model can be employed as a common starting component for multiple degradation types, upon which distinct sets of trainable parameters (ie. low-rank adaptors) can be added in order to address specific-degradation restoration tasks. Mixed-degradation restoration is enabled through a top-K hyperparameter, that affords a mixture of (degradation) experts to be active. The experimental setup considers multi and mixed image restoration problems where average results are offered across image-degradation datasets and appropriate standard quantitative metrics, qualitative examples, are reported in comparison with alternative approaches.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The technique described for piping specific samples down specific low-rank adaptor chutes is relatively easy to understand and yet reportedly results in competitive restoration accuracy for investigated datasets. 

* Nascent investigations into mixed-degradation image restoration problems provide a promising seed to be followed.

* The writing is of a reasonable standard.

Weaknesses:
* The key idea of leveraging pretrained VLM features (and specifically CLIP) for the task of image restoration from multiple degradations, pre-dates the current submission [R1]. While authors clearly go to some length to highlight their alternative CLIP-based scheme, which amounts to envoking specific (pre-existing [R2]) low-rank adaptors, the core technical contributions here can be regarded as somewhat limited.  

* The phrase 'Universal Image Restoration' may not be a sufficiently accurate (or modest) description for the proposed method. The submission collates ten different image restoration tasks which, despite vague statements in the abstract, remains a 'multi-task' not a 'universal' setup. Samples for all ten degradation tasks are shared between train and test (Sec. A.1) and individual task adaptors appear to be trained independently on task-specific datasets (L188--196). Generalisation ability to previously unseen degradations is also not considered. Suggest method description requires reworking.

* The claim that multi-task learning (MTL) frameworks, designed to handle image restoration for multiple degradations, share all parameters across different degradations (L029) is incomplete and somewhat misleading. Several existing MTL works (eg. [R3,R4]) make use of both shared and task-specific parameter subsets for multiple image restoration tasks. Indeed 'which proportion of parameters should be shared and which should be task specific' can be considered a fundamental (and long standing) MTL question. The idea of benefiting from commonalities between image restoration tasks is well understood and my concern is that this casts doubt on a core premise of the submission. 


References

R1. Controlling Vision-Language Models for Multi-Task Image Restoration. ICLR 2024.

R2. LoRA: Low-rank adaptation of large language models. ICLR 2022.

R3. All in One Bad Weather Removal using Architectural Search. CVPR 2020.

R4. Pre-Trained Image Processing Transformer. CVPR 2021.

Minor:

L076: 'draining' --> 'deraining'

L099: 'mim' --> 'min'

L238: 'aspects' --> 'aspects.'

Limitations:
Half of one sentence (L293) is apportioned to discussing method limitations. See above for suggestions on components that might make for valid additions here.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes universal image restoration framework using multiple low-rank adapters that learns task specific weights from to perform multi-domain transfer learning. the proposed method leverages the pre-trained generative model weights as the shared component and adapts it task specific low-rank adapters. At each layer in the restoration pipeline the proposed method uses the degradation similarity to combine LoRA adapters outputs, this enables the proposed to handle for mixed degradation restoration.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper proposes LoRA adapters to learn task specific weights and proposes a strategy to combine the adapter outputs using degradation similarity measure
- extensive experiments are performed showing the proposed strategy works better than random and average in table 3.
- extensive experiments are performed to show the proposed methods performance against the sota methods in table 1 for mutliple degradation task.
- Extensive experiments are performaed showing impact of LoRA rank and prediction accuracy

Weaknesses:
- In table of the paper authors compared proposed method against sota on REDS and LOLBlur datasets, both these datasets have mixed degradations  of blur, jpeg compression, noise, and low light. Although these comparisons performed on mixed degradations, it would be helpful to how the proposed method performs on mixed weather conditioned images (MID6), which is comparatively challenging than REDS and LOLBlur  datasets. 
MID6: Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration, CVPR, 2024.

- Can authors confirm, whether network re-trained seperately for each experiment in table-1,  and table-2 separately, i.e. table-1 and table-2 trained network weights for proposed method are different.

Limitations:
- authors have addresed limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a framework to improve image restoration across various degradation types using Low-Rank Adapters (LoRA). The proposed method adapts a pre-trained generative model to each degradation type. It performs a weighted sum of the output of adapted models using the estimated degradation of input images. The proposed method performs impressive results in restoration accuracy and resources.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proposed method is interesting and reasonable.
Experimental results support this paper's contributions and the proposed method's effectiveness.

Weaknesses:
In Table 3, the 'Top-1' strategy performs almost the same as the 'All' strategy, which limits the motivation of the weighted sum of the adapted models.
Table 6 presents the restoration performance comparisons for each degradation. The proposed method underperforms previous works in significant degradation types such as blurry, low-light, raindrop, and rainy.
The average scores might mislead the evaluation performances.

Limitations:
The proposed method is simple and effective, but evaluating average scores on multiple degradations can mislead its contribution.
The proposed method achieves near-best performance by selecting a single adapted model but underperforms in many major degradation types.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
yaYJlpidX1;"REVIEW 
Summary:
The paper proposes a novel technique to automatically discover in-context continual learning dynamics for image classification task sequences through meta-learning. In order to achieve this purpose, the approach relies on 2 main novelties: 
* Using self referential weight matrices on top of an image encoder - SRWM, as self-modifying that adapts itself to the stream of inputs, is an natural model for continual learning. 
* Encoding continual learning desiderata in the meta-objective, i.e. backward and forward transfer. 

The authors first apply the approach in a classic two-task setting (Split-MNIST) that allows them to showcase and analyse the emergence of in-context catastrophic forgetting phenomena, and to show that using their ACL loss can help reduce it. They further evaluate their method and compare them to replay-free baselines from the CL and meta-CL literature, showing an advantage of their approach in scenarios with up to 3 tasks. 

The authors further test the limits of their approach by comparing it to more recent learning to prompt techniques for continual learning, leveraging the power of pretrained large models. This scenario show a limitation of the technique in more complex scenarios with more tasks, more diverse and complex data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper takes an interesting perspective on continual learning, leveraging the interesting properties of SRWM and the capability of meta-learning to encode the desired behavior in the meta-learning objective. The combination of these two contributions is novel to the best of my knowledge, and lead to interesting insights. 

* The approach leads to interesting performance in relatively simple scenarios, outperforming some of the existing continual learning techniques. 

* I also particularly appreciated the authors discussion of the method limitations. Both the experiments with learning to prompts and the discussion provide very valuable insights that can help building on the work in the future.

Weaknesses:
* In my opinion, the main limitation of the approach is its practicality. From the experiments reported in Table 4, it seems that the approach requires to met-train on a sequence of similar length and/or complexity to provide its potential. This is not possible to know in advance in practice. Moreover, one limitation that the authors have not mentioned is that the meta-objective seems to require keeping in memory a number of copies of the model that is equal to the number of tasks. This can quickly become cumbersome for real applications that can require more complex models and very long sequences of tasks.  

* While the authors focus on classic benchmarks for continual and meta-learning, these benchmarks are artificial, relatively simple and lack of diversity. Different works highlight the limits of these benchmarks, I invite the authors to look at ""Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification"" Ullah et al. 2023, and ""NEVIS'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research"" Bornschein et al. 2023 for examples of more realistic benchmarks. 

* It would be interesting to add a discussion of the cost of the approach (computation, memory, ...). Even is it gives a substantial boost in many cases, it would be interesting for practitioners to compare what they gain to what they pay.

Limitations:
The authors provide a detailed discussion of the work limitations, both in the experiments and the discussion sections. Some other limitations are highlighted in the Weaknesses paragraph above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on Automated Continual Learning which is different than handcrafted continual learning. It uses self referential neural networks to meta learn their own in-context continual learning algorithm. First, the paper shows the emergence of in-context catastrophic forgetting. Second, the paper analyze the performance of proposed method (ACL) and finally the paper discuss the limitation of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is clearly written and easy to follow
- The paper introduces original idea of Automated Continual Learning
- The paper identifies ""in-context"" catastrophic forgetting

Weaknesses:
- The paper claims to do in-context continual learning but the concept of in-context learning is not clearly explained.
- The paper mainly focus on two task and five task settings but it would be more helpful to see the more different settings such as three task or four task
- How is the size of SRWM affects the maximum sequence length that can be train?

Limitations:
Authors have addresses the limitation of the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper describes a method for in-context continual learning (CL) by using a type of meta-learning neural architecture based on ‘self-referential weight matrices’ (SRWM). Proposed in prior work, these models learn to modify weight matrices iteratively as they process more and more inputs. In this work, they are given few-shot examples from different tasks and iteratively update the weight matrices as the examples are processed. This update process is referred to as “in-context” learning in this work. The key innovation is to define the loss function of SRWM training to optimise for both forward (improving performance of subsequent CL tasks) and backward (improving performance of previous CL tasks) transfer while achieving good performance on the current task. Experiments are conducted on commonimage classification meta-learning benchmarks such as Split-MNIST and Mini-ImageNet. Results show the proposed method prevents catastrophic forgetting (without using replay), outperforming existing meta-learning baselines on the evaluated benchmarks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Studies the problem of in-context catastrophic forgetting via a two-task toy setting and reveals the issue when training with no backward transfer loss term. This is shown to be mitigated by including the backward transfer loss term.

Proposes an in-context CL method using models based on SRWM and a novel loss to mitigate catastrophic forgetting as more tasks are learned. The method does not use a replay buffer.

Studies and covers standard image classification meta-learning tasks such as Split-MNIST, FashionMNIST, and CIFAR-10. On Split-MNIST, shows improvements over existing CL and meta-baselines in both domain and class incremental evaluation settings. The improvements, when additional 5-task fine-tuning is used, is significantly above baselines. 

The paper is clearly written, with thorough literature review.

Weaknesses:
One weakness of the proposed method is that the number of loss function terms increases with the number of CL tasks, as pointed out by the authors in Appendix A.5. This prevents this method from being scaled to more practically relevant settings where a large number (much more than 2 or 3 that this paper has mostly focused the experiments on) of tasks are considered in a CL setting. Method of reducing the loss terms would strengthen the paper.

Another weakness, which is also noted by the authors in Table 4 and Section 4.3, is that the performance of the proposed model and method is poor compared with those based on pre-trained transformer models, even on an easier evaluation task. The authors in Section 5 also discuss a potential connection between LLM transformer training as an implicit version of the proposed model and method. Given these existing strong and more widely adopted methods, it is unclear how much value the proposed method adds. SRWMs are not widely used and LLMs training can scale to a massive number of tasks with a single loss [1] (albeit not CL). A more detailed explanation of the application of the findings of this paper beyond those interested in SRWMs would be helpful.

Another weakness of this paper is its focus on image classification meta-learning tasks only. It is helpful to know the generality of this method, for example on language modelling tasks or multimodal tasks. An experiment demonstrating the method in CL language tasks would be helpful.

[1] Finetuned language models are zero-shot learners. Wei et al. ICLR 2022.

Limitations:
Limitations have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the problem of catastrophic forgetting (CF) by formulating continual learning (CL) as learning from a sequence of demonstrations of tasks. The paper proposes a meta-learning objective function that includes backward transfer terms. These terms compute the error of the predictor on previous tasks after receiving demonstrations of the current task.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The approach of formulating (continual learning) CL as learning from a sequence of demonstrations of tasks is interesting.
- The experiment shows positive results when compared to non-meta-learning approaches

Weaknesses:
- The paper is difficult to follow. Many definitions and the algorithm are not very well explained.
    - The motivation of formulating (continual learning) CL as meta-learning is not well presented.
    - Some details of the architecture are mentioned in the background section only (e.g. replacing self-attention with SRWN and the multi-head version.)
    - The details of the training and inference process are not well presented.
- The training process can be very costly and poorly scaled with the number of tasks and the number of examples per task. In each step over a sequence of demonstrations, the method needs to compute and store a new weight matrix in order to perform back-propagation. It might require more memory during training and at inference.
- Even being a meta-learning approach, the model still needs fine-tuning when given a new task to adapt to a new number of tasks.

Limitations:
There are no negative social impacts. My suggestions have been listed in the previous sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
yAKuSbIwR7;"REVIEW 
Summary:
This paper provides a thorough characterization of regularizers which lead to synaptic balance (when the ""cost"" of input weights to a neuron or pool of neurons is tied to the cost of output weights) in trained neural networks. Their results apply to many different activation functions and architectures.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is very well-written and easy to follow. I was able to read everything, including the math, smoothly. The mathematical arguments themselves are crisp and correct, which I really appreciated.

Weaknesses:
The paper is strongly lacking in motivation. I never really understood *why* I should care about synaptic balance. Also, it is clear from the numerical experiments that synaptic balance only emerges in networks when it is enforced via a regularizer (expect in the case of infinitely small learning rate), but why is this surprising? It seems obvious that adding a regularizer for some property tends to result in that property. It would be shocking if synaptic balance occurred without some regularization towards the property. Thus, while the ""what"" and ""how"" of the paper are nicely addressed, I feel the paper is missing the ""why"". I believe if the authors could address this from the outset, it would make the paper much stronger, and I would of course be willing to increase my score.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a theoretical approach to the analysis of balanced neurons and networks. Their theoretical work includes proof of the convergence of stochastic balancing. In addition, they investigate the effect of different regularizers and learning rates on balance, training loss, and network weights, including practical simulations for two classification problems.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper tries to reveal the inner structure of neural networks during the training phase. This is a very important but difficult problem; its solution could provide new insights for developing better training algorithms. The work proposed can ultimately be an important step toward more transparent networks as opposed to their current black box character.

Weaknesses:
The paper has some weaknesses, most notably how the material is presented and part of the evaluation.

Theorem 5.1, dealing with the convergence of stochastic balancing, is arguably the central piece of the paper. However, its formulation is bulky and should be reduced to a shorter, more manageable size, potentially with the help of lemmata. This becomes apparent when seeing that its proof contains the proof of another proposition.

In Figure 4, the authors say that these panels are not meant for assessing the quality of learning. However, measuring not only the training loss but also the accuracy on a test set will give important insights. How does the classification performance relate to the degree of balancing? Why did the authors not include this analysis? It could give important insights into the relationships between overtraining, generalization capability, balance, and accuracy.

The author should discuss the consequences of their work on network training. They do not discuss the immediate practical consequences or any recommendations they can make based on their results.

Limitations:
The authors could be more specific about the consequences of their work, including limitations. For example, can they recommend any specific learning rate, network structure, or other features for optimal training?

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to study and explain the phenomenon of neural synaptic balance, where a balanced neuron means that the total norm of its input weights is equal to the total norm of its output weights. Particularly, the authors study the reasons why and when randomly initialized balanced models (so, models whose neurons are balanced) tend to be balanced at the end of training as well. The study takes into account many different components of neural networks (activations, layer kinds, regularisers).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The study is very comprehensive, and sheds light on some interesting properties of deep neural networks.

Weaknesses:
While it is true that, as the authors state in the conclusion, neural synaptic balance is a theory that is interesting on its own, I would encourage the authors to expand the discussion on possible application domains of this theory. Why is it interesting? What are the advantages that a complete understanding of such phenomenons could bring to the table?

Limitations:
No concerns here

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a theory of neural synaptic balance, defined as the condition in which a total loss achieves the same value for the input weights to a neuron and its output weights. This is different from the well studied  E/I balance in neuroscience and machine learning literature. The authors show mathematical derivations of how to balance a neuron without affecting the outcome of the network and show that balancing a network is a convex optimization process.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is overall clear and detailed, the mathematical proofs are sound and the paper structured well moving from straightforward claims to less trivial points.

Weaknesses:
The paper is about neural synaptic balance, but the authors do not provide convincing motivation why we should care about such balancing.  As they mentioned, adding a simple L2 regularizer will balance the network naturally (in a distribution sense, not necessarily each neuron individually) during training and have other well-known  benefits, so the elaborate mathematical derivations on the general balancing process seem redundant. In addition, in the authors' own plots, unbalanced networks sometimes outperform the balanced networks (e.g., fig 3E), which just emphasizes the point. One of the mentioned motivations  is biological neurons, but they claim that biological neural data about synapses do not exist. However, they could test their hypothesis against the currently available connectomes e.g., from or the Drosophila fly brain. They mention spiking networks, but the notion of input-output homogeneity is unclear in spiking networks. Finally, physical neurons' energy consumption is mentioned without details.

Limitations:
The whole framework is specific to BiLU neurons or perhaps to other power-law functions. The relevance to spiking neurons is therefore questionable. It is also questionable as a general principle for machine learning.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
xtpY1kQmW9;"REVIEW 
Summary:
This paper appears to suggest that any decision is composed of two Bayesian decisions and it trys to evaluate the implications of this idea. 

I am very confused by this paper and really don't know what to make out of it. For example, the conclusion seems to be only a brainstorming session of random ideas and the rest of the paper does not appear to be much better.

At the very least, it is not well written, at worst the proposed approach does not make any sense.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Given that I don't properly understand what exactly the authors want to achieve, I am unable to formulate the strengths of this paper.

Weaknesses:
The presentation is very messy. The paper jumps from topic to topic without me understanding their relations to each other.

Limitations:
see above

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses the implications of Bayes' theorem, making assumptions inspired by a thought experiment of communicating a message. Prior (and model) elicitation by solving a fixed point equation is discussed.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
* The paper takes a fresh look at decision marking under uncertainty, which is at the center of machine learning.
* The generality of the setting makes the discussion applicable to virtually all of ML.

Weaknesses:
While I am sensible to the topic of prior and model elicitation from coherence arguments, I believe the paper needs a thorough revision focussing on clarity. While I have some intuition now, it is still not crystal clear to me what the exact goal or claims of the paper are. See bullets below for constructive comments.

## Major
1. Section 4: what is the probability $P$? What is the underlying space and sigma algebra? What are they supposed to represent?  
2. Section 4 introduces several very strong assumptions, like $1-P(A\vert B) = P(B\vert A)$ (is it for all $A,B$ in some sigma-algebra or for a specific pair of events?), that are motivated by an analogy about communicating a message. It is not clear why I should be prepared to make these strong assumptions. The fact that I don't know what $P$ is supposed to model or serve as does not help. Is it a joint probability over the variables describing a decision problem, as in decision theory? In that case, will it be used in conjunction to a loss function to make decisions? Will it be judged by some measure of decision accuracy? Or are we in a de Finetti framework, coming up with a personal probability $P$ which we will use to make predictions about unobserved variables? My intuition is that we are dealing with the latter kind, but this should be explained. And the strong assumptions need to be motivated by more than an analogy about communication.
3. The information analogy which motivates imposing the fixed point equation (9) is unclear, as well to what probability and what events it should apply.
4. p5 L179: the sentence about the parameter being a dynamic parameter for a learning system is unclear. We haven't discussed any learning algorithm yet.
5. I am not sure I see where Eqn (11) comes from. $\lambda$ has been chosen to derive (10) from Bayes' theorem, but it doesn't have to be the right base to write (11), right? Same remark for (18).

## Minor
1. p7 L248: Although neural networks have been a popular class of models and algorithms, supervised learning is not synonymous with neural network training.
2. p7 L252: the meaning of ""the $\lambda$ expression"" is unclear.

Limitations:
This is fundamental work that does not have any immediate negative societal impact.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The purpose of this paper is to investigate the optimality of a classifier. It is known that the Bayes classifier is optimal, and it is likewise known that an explicit computation of the Bayes classifier is often very challenging if not impossible. This paper offers an analysis of the Bayes classifier as a sequential solution of two problems. An analysis and interpretation of a vase / faces example is presented and some theory is developed to further understand it. The paper concludes with an application.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors are exploring an idea which is novel, and the whole thinking about Bayes classifiers as comprising two sub-problems seems novel and worth pursuing.

Weaknesses:
I did not really understand the discussion with the vase, the sender and receiver. Perhaps the authors should somehow connect the Bayesian ideas to the description of the problem earlier? I think the paper would really benefit from rewriting Section 4 with the vase as a running example, because it is hard to connect the various decisions with the probabilities. Maybe it's worth to add more illustrations / diagrams for this? The authors are presenting novel ideas and it's hard to understand them as they are currently presented.

For the theoretical implications, I think it would be better to illustrate the approach on a simpler model like a linear one. 

The paper started by mentioning the Bayes classifier but does not come back to it as an example. 

The paper states that the Bayes classifier is broken up into two decisions, but those are just briefly mentioned in the vase / faces example. The authors should carry this thread of reasoning through the whole paper.

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
wZ5kEOCTce;"REVIEW 
Summary:
This paper reveals the role of inter-patch dependencies in the decoder of MAE on representation learning. The paper shows that MAE achieves coherent image reconstruction through global representations learned in the encoder rather than interactions between patches in the decoder. Based on this, the authors propose CrossMAE, which only utilizes cross-attention in the decoder.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The approach of analyzing the reconstruction process through self-attention between mask tokens and cross-attention between mask and visible tokens is intriguing.
- The writing is clear and easy to follow, with main messages that are solid and insightful.

Weaknesses:
1. Idea/Novelty
- The claim that MAE reconstruction is achieved through global representation learning within the encoder rather than interactions between patches needs more support. Recent studies linking MAE to contrastive learning have found that the receptive field of specific mask tokens in the decoder is relatively small. Could the role of mask tokens in the decoder be to capture local area information? This might explain the smaller attention magnitude of masked tokens compared to visible tokens in Figure 1(b). 
- There is a concern that without self-attention (i.e., with the proposed method), the observation that authors made on the vanilla MAE may no longer be valid. Additional explanation on this point is necessary as this observation is the main motivation for suggesting CrossMAE.

2. Additional justification
- Effectiveness of using a subset of mask tokens as queries: Unlike the traditional architecture, this method uses only a subset of mask tokens as queries. Detailed analysis and interpretation are needed on why this is effective. 
- Performance differences when using the entire set of mask tokens versus a subset (and what percentage of mask tokens is used) should be reported.

3. Experiment
- For a fair comparison, CrossMAE's performance should be evaluated using the same setting as the original MAE, especially regarding the fine-tuning recipe.
- The current experimental results do not convincingly demonstrate the effectiveness of the method. For classification tasks, only the linear-probing and fine-tuning results on IN1K are reported. Following the previous works, classification on various downstream datasets should be also considered.
- For generalizability, evaluation on another task like semantic segmentation (e.g. on ADE20K) would be useful to verify that the suggested method learns the generalizable feature representation.

Limitations:
The authors have not discussed limitations of this work except for the very last sentence of section 5, indicating that they have discussed limitations in this section in the questionnaire #2. It is strongly recommended to disclose more detailed limitations of the proposed work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel pre-training approach called CrossMAE. Instead of concatenating the masked and visible tokens for the decoder, the authors add cross-attention to decode the masked tokens by using them and the visible patch embeddings as separate inputs to the decoder. Further, the authors introduce a method to only partially reconstruct the masked patches, and leverage inter-bock attention to fuse feature across different layers.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well motivated through a practical observation
- The authors propose a useful technical contribution which seem intuitive given the described observations
- The paper is well written and technically sound
- All visualizations provide additional value, I especially like Figure 5. It describes the effect of the contributions well
- Judging from the experiment section, the presented approach mostly improves over the vanilla MAE and other MAE-like follow-up works

Weaknesses:
- I feel like the paper is missing a more structure ablation of the individual contributions. I think the paper would benefit from having a simple table where all contributions are added sequentially to better identify the performance effect of the individual contributions as in:
	MAE X.X
	+ Cross-Attn X.X
	+ Partial Reconstruction X.X
	+ Inter-Block Attn X.X
- As can be observed from Table 3 c), the final setting (underlined) of the prediction ratio, 0.75, turns out to be exactly the same as the optimal masking ratio, 0.75. If I understood correctly, this means that in practice, CrossMAE works best when it predicts all tokens that were masked, not just a fraction of them. Only predicting part of the masked tokens was previously listed as a contribution. Therefore, I don’t understand how this additional hyper parameter provides any benefit for better downstream performance. Maybe I’m missing something and this be cleared up by answering the previous point.
- All models are only trained for 800 epochs. The original MAE reaches peak performance at 1600 epochs. For a thorough comparison, it would be necessary to also train CrossMAE for 1600 epochs and see if the performance gains sustain, or if performance has peaked at 800 epochs.
- Table 1 is missing the CrossMAE ViT-H with 75% masking ratio
- Contribution 2 and 3 don’t seem to be as well motivated in the introduction in comparison to Contribution 1
- Better performance is listed as a contribution. IMO this is not a contribution, rather a result of the technical contributions

Limitations:
The authors have sufficiently addressed the limitations of their approach.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents CrossMAE, a methodology for improving pre-training efficiency over that of MAE for an encoder. The paper motivates its approach by presenting visual evidence that, in standard MAE pre-training, masked tokens attend to other masked tokens significantly less than to non-masked (aka, visible) tokens. Using this motivation, the paper then presents CrossMAE, which differs from MAE largely in that it replaces the MAE self-attention with cross-attention between the masked tokens and a learnable weighted combination of the encoder feature maps. This aspect decouples queries from keys and values (which is not the case in MAE), which the paper then exploits to allow only some (but not necessarily all) mask tokens to be used during reconstruction to pre-train the model. The paper presents an analysis of which encoder block features are optimal to cross attend with each decoder block, and it presents ablation studies on multiple design decisions. Finally, it presents visual and fine-tuning results showing comparable performance to MAE and similar methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper motivates CrossMAE well by showing evidence of a potential inefficiency in MAE (self-attention) and then presenting an approach to remedy it (cross attention). I particularly like how the paper delves even deeper, though: instead of stopping at the level of replacing self-attention with cross-attention, it then points out that this choice allows for a significantly fewer number of masked patches to have to be reconstructed, which reduces flop count significantly. The ablations in Table 3 are fairly thorough and answered some questions I have developed. The performance of CrossMAE appears comparable to other SOTA methods but with significantly more efficient pretraining.

Weaknesses:
1) In Fig 1b, IIUC, for one particular mask token, the two $\mu$'s are the respective attention values averaged over all transformer blocks and all masked/non-masked tokens. If this is the case, my concern is that by averaging over all transformer blocks, variations in the attention is being hidden. Naively, I would think that for early blocks, the attention due to masked tokens would be small (as the paper concludes) but becomes larger for the later blocks (since now the masked tokens have actual useful signal in them). Did you consider this?

2) I do not follow why CrossMAE does not need an MLP at the end to convert final decoder tokens back to raw pixels. Line 218 says that the inputs to the first encoder block are included in the feature maps for cross attentions. Does this cause a final MLP to not be used?

3) Less critical:
  3a) Fig 1b should point the reader to Section A.1. I spent much of my reading confused about what $\mu$ is.
  3b) Fig 4a should have a different number of decoder layers than encoder layers. When I saw this figure, I immediately wondered why a decoder block wasn't being paired with feature maps from its ""partner"" encoder. I had to wait until lines 204-207 to get an explanation of why this doesn't work.
  3c) Line 187 references a ""second question"" in Sec 3.1, which doesn't exist as far as I can tell.
  3d) Fig 4a shows the ""vanilla"" version of Cross MAE, where the final encoder layer feature maps are attended with all decoder layers. But the paper presents results exclusively (?) on the version that uses a learned combination of the feature maps. Anyway, the figure confused me. Maybe I just didn't understand what the solid arrows vs dotted ones are supposed to represent.

Limitations:
No weaknesses are specifically addressed. But as this paper is essentially an optimization to MAE, I'm not sure this question is relevent.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
wH36UKML4x;"REVIEW 
Summary:
This paper addresses the problem of subpopulation generalization, also known as spurious correlations. Building on the Last Layer Retraining (DFR) method, it removes the constraints on a small subset of annotations. The paper introduces the Environment-based Validation and Loss-based Sampling (EVaLS) method. Unlike DFR, EVaLS divides the validation set $D^{val}$ into two parts: (1) $D^{LL}$,  where losses from an ERM-trained model are used as a proxy for identifying minority groups for retraining, and (2) $D^{MS}$, where environment inference methods are used for partitioning environments. The paper presents theoretical insights and empirical results demonstrating the effectiveness of EVaLS.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper is well-structured and presented in a clear and organized manner, making it easy to comprehend and follow along.
* The proposed method is simple but effective and explores a relatively challenging area in existing literature (*i.e.* subgroup generalization without group annotations). 
* The authors provide some theoretical analysis to support their claims.

Weaknesses:
* The novelty and contribution of the proposed method may be limited for the following reasons: 1) The paper combines multiple previously proposed methods (*i.e.* DFR [1], EIIL [2]) all at once, which inherently guarantees a nontrivial performance; (2) The primary technical contribution, at least from my perspective, is the loss-based sampling, which has been already explored extensively in the noisy label literature and has been used as tools for pseudo-labeling. 
* The paper fails to discuss recently proposed methods that also require no group annotations, such as SELF [3], BAM [4], and BPA [5]. In particular, SELF is also a direct follow-up of DFR. The authors are encouraged to discuss the limitations and strengths of loss-based schemes against the class-based schemes advocated by SELF and BAM.
* More analyses can be included to provide further understanding of the selected loss-based samples. For example, given a threshold, how much percent of the high-loss and low-loss samples are indeed the minority and majority samples and how does this percentage change with the threshold?

[1] Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations, ICLR 2023

[2] Environment inference for invariant learning. ICML 2021

[3] Towards Last-layer Retraining for Group Robustness with Fewer Annotations. NeurIPS 2023 

[4] Bias Amplification Enhances Minority Performance. TMLR 2024

[5] Unsupervised learning of debiased representations with pseudo-attributes. CVPR 2022

Limitations:
Aforementioned in Weaknesses and Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To address the issue of spurious correlations when group labels are unavailable, this paper proposes a new method called EVaLS. It first creates a balanced training dataset using loss-based sampling. Then, it evaluates the accuracy of the balanced training set based on the inferred environments from the validation set, and selects models accordingly.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written, and includes a rich set of experiments with necessary theoretical explanations.

2. It is essential to discuss the multiple  (unknown) spurious features case which has been overlooked in previous studies.

Weaknesses:
1. Why is the approach of using high-loss points (considered as the minority group) more effective than directly using misclassified points (considered as the minority group) in methods like JTT? Intuitively, compared to misclassified points, high-loss points are more ""implicit"" and no obvious thresholds, which could potentially result in high-loss points actually belonging to the majority group, thus exacerbating the imbalance in resampling.

2. If the author can show the balance level of samples obtained through loss-based sampling compared to directly using labels (misclassified points), it could further illustrate the advantages of loss-based sampling.

3. In Section ""Mitigating Multiple Shortcut Attributes"", if color is treated as a known spurious attribute and shape as an unknown spurious attribute, how would the performance of EVaLS be affected? Based on my understanding, there is a possibility that simplicity bias could cause the model to prioritize learning the simpler feature, color, and struggle to learn the more complex shape attribute. Therefore, considering color as known and shape as unknown can better show the performance of EVaLS in handling complex spurious features.

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies how to improve the model’s robustness to multiple spurious correlations when the group labels (indicator for spurious correlation) are unknown in general. The proposed approach, EVaLS, leverages the loss from a base ERM model to sample a balanced subset to prevent learning from spurious correlations. In addition, a new synthetic dataset (Dominoes-CMF) for multiple spurious attributes is crafted. Empirically, the proposed approach sometimes has advantages over the rest of the baselines when using the same amount of additional information (group label).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The main paper is generally well-written. 
2. The theoretical analysis in Section 3.3 (with derivations and proofs in Appendix) shows that for one-dimensional Gaussian distributions, choosing the tails on the two sides of the distributions creates balanced groups, even though the original data distribution is skewed. 
3. Environment inference technique is demonstrated to be useful for separating the dataset into groups with different distributions of the subpopulations and then for model selection. 
4. The proposed technique only requires last-layer retraining on part of the validation set, which is generally more efficient.

Weaknesses:
1. Figure 2 attempts to illustrate more minority samples have high loss while the majority samples have low loss. However, in each of the plots, only the % of one of the minority or majority groups is shown. The illustration can be improved by showing the % of both majority and minority groups in the same plot, and showing the actual distribution of the loss for the groups. 
2. Though the idea is straightforward, it is unclear how the loss-based instance sampling is actually implemented. It is helpful to provide an algorithm or pseudocode to improve the presentation. 
3. The theoretical analysis is generally sound but limited to a case without discussing the use of the loss (which may not be Gaussian) and the spurious correlations (which involve at least two dimensions of core and spurious features [1]). 
4. The experimental results are less polished and sometimes the advantages are not so clear over other baselines. Some results are missing for datasets such as UrbanCars and MultiNLI. Only a few baselines are compared for the new dataset in Table 2. There is also no convincing and fine-grained analysis (e.g., ablation study) to understand how the proposed approach ensures data balancing and improves group robustness. 
5. The paper initially focuses on improving group robustness when multiple spurious correlations are present, but the experimental results are lacking for these more challenging datasets. 

[1] Sagawa, Shiori, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. ""An investigation of why overparameterization exacerbates spurious correlations."" In *International Conference on Machine Learning*, pp. 8346-8356. PMLR, 2020.

Limitations:
The authors have discussed the limitations in Section 5.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
vYmvgxpgwH;"REVIEW 
Summary:
This paper explores compute-optimal inference for large language
models (LLMs), focusing on designing models and strategies that
balance additional inference-time computation with improved
performance. The study evaluates the effectiveness and efficiency of
various inference strategies, including Greedy Search, Majority
Voting, Best-of-N, and Weighted Voting, across different model sizes
(e.g., 7B and 34B) and computational budgets. Experimental results
indicate that smaller models with advanced tree search algorithms can
achieve a Pareto-optimal trade-off, offering significant benefits for
end-device deployment. For example, the Llemma-7B model matches the
accuracy of the Llemma-34B model on the MATH500 dataset while using
half the FLOPs. These findings suggest that smaller models with
sophisticated decoding algorithms can enhance problem-solving accuracy
across various generation tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper focuses on an interesting topic and should be of interest
  to the audience of NeurIPS.
- It considers a comprehensive experimental investigation to confirm
  the claims.
- The proposed tree search algorithm is interesting and seems to
  outperform the competition.

Weaknesses:
- Although the paper offers quite thorough experimental analysis, it
  does not look deep in terms of theoretical ideas (although there are
  2 theorems), which may be a problem for a flagship venue like
  NeurIPS.
- Overall findings on the possibility to train an equally accurate
  model with fewer computational resources do not look surprising.
- The paper would benefit from additional proof-reading as there are a
  large number of typos present.

Limitations:
The paper concentrates on mathematical problem-solving tasks using 7B
and 34B models, with findings potentially not applicable to other
domains. Future research should explore a broader range of model sizes
and different training datasets to better understand compute-optimal
inference in mathematical problem-solving.

I should also say that these limitations have been explicitly
discussed by the authors themselves (so not a criticism).

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents an approach to select an optimal inference strategy for LLMs and empirical analysis on Math problem solving tasks. The main idea is to select an inference strategy based on a computational budget (FLOPs). The underlying policy model samples solutions by generating tokens based on the budget and a ranking model consumes these tokens. A new reward model is developed  to explore the solution space more effectively. The reward acts as a weighted majority function over the solutions.
Experiments are performed on Math problem solving benchmarks. Some of the key insights from the experiments is that a smaller LLM can outperform the larger LLM in terms of using a smaller computational budget while maintaining similar accuracy. They also show that the proposed approach with a smaller budget has comparable accuracy than sampling with a larger budget.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The insights that inference time strategy can compensate for using smaller LLMs in generation seems to be interesting
- The experiments also provide a basis for analyzing scaling properties of inference which can be significant

Weaknesses:
- In terms of the method itself, I was not sure if it is very novel. It seems to be a smaller variation on the tree search methods that search for solutions in the generated space
- In terms of comparisons, I was not sure about the significance of the benchmark, i.e., are there some properties that make the proposed reward reranking more optimal in Llema model specifically (due to the structure of math problems, etc.). In general, since the main contribution of the paper is empirical, I think there should be experiments or discussions different LLMs to make the contribution more significant. 
-Overall, the empirical conclusions seem very tied to the specific benchmarks, so I was a little unsure regarding the significance of the conclusions.

Limitations:
Limitations regarding the datasets are mentioned.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the optimal training configurations of large language models (LLMs) during inference. The proposed inference strategy, REward BAlanced SEarch (REBASE), combines the strengths of Monte Carlo Tree Search (MCTS) with reduced inference costs, resulting in improved performance on math-domain tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper provides a comprehensive overview, i,e, the inference scaling law, of the performance of different sampling strategies under various inference configurations.
2. The novel REBASE inference strategy achieves better downstream task performance under the same computational budget or even less.

Weaknesses:
### Major 

1. Did you take into account the inference cost of the reward model (RM) in your analysis? As the REBASE frequently uses RM to judge the quality of immediate solutions than other sampling strategies, such as, weighted major voting, It's crucial to consider this aspect to provide a holistic view of the efficiency and practicality of your proposed strategy.

2. The base model with post-training techniques such as SFT and RLHF inherently limits the upper bound of performance. It seems that adding more tricks during inference could improve performance, but the marginal effect may result in diminished returns when using models already tuned by the RLHF process. Could you compare the performance gains of REBASE between the base model, the SFT model, and the Chat model? Is the performance gain only significant in models that have not been tuned?

3. In Section 4.2, the observation in ""Scaling law of compute-optimal inference"" indicates that the optimal inference strategy is invariant to the amount of compute but depends on the model size, i.e., the model's inherent capacity. This raises a concern: does the inference strategy significantly improve the model's performance, or does it only take effect in certain scenarios, such as with base models that have not been aligned?

4. The paper focuses solely on the math domain. To strengthen your claims, a more comprehensive evaluation across general domains using widely adopted benchmarks, such as MMLU, SuperGLUE, HumanEval, etc,  is necessary. 

5. There appears to be no significant improvement in the GSM8K datasets than MATH500 dataset. 

### Minor

1. Figures. 2 and 3 are not referenced in the main manuscript. 

2. Figures. 2 and 3 appear to be in draft form and are somewhat vague.

Limitations:
See Weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
uS9RZH6K65;"REVIEW 
Summary:
This paper proposes a denoising framework to alleviate the influence of noisy text descriptions on open-vocabulary action recognition in real scenarios. A comprehensive analysis of the noise rate/type in text description is provided and the robustness evaluation of existing OVAR methods is conducted. A DENOISER framework with generative-discriminative optimization is proposed. The experiments demonstrate the effectiveness of the framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The robustness to noisy text descriptions/instructions in real-world OVAR applications is an interesting and meaningful problem.
- The evaluation of the robustness of existing OVAR methods when facing the noise text description input is valuable to the community.
- The motivation is clear and the overall framework is technically sound.

Weaknesses:
- About the experiments,
    - The reviewer thinks that the most convincing results are the Top-1 Acc of existing OVAR models under the Real noise type. However, in Table 1, the proposed model does not demonstrate much superiority compared to GPT3.5's simple correction. The reviewer worries about the research significance of this problem. Will this problem be well resolved when using more powerful GPT4/GPT4o with some engineering prompt designs?
    - In Table 2, I would like to see the performance of other correction methods (e.g., GPT3.5/4/4o) for a more comprehensive comparison.
    - Since this work focuses on the noise text description problem in OVAR, it is necessary to demonstrate the results of those CLIP-based methods without any additional textual adaptation (e.g., the vanilla CLIP).


- About the method,
    - The reviewer thinks that the overall model design is reasonable and clear. However, the method part introduces too many symbols which makes the paper very hard to follow. It is unnecessary to over-decorate the technical contributions.

- Minor issue,
    - The authors seem to have a misunderstanding about the OVAR setting (Line 113). In OVAR, the model is evaluated on both base-classes and novel-classes during testing. In this case, all action classes from the UCF/HMDB datasets can be used for testing when the model is trained on K400, as there are many overlap classes between K400 and UCF/HMDB.

Limitations:
The limitations are discussed and there is no potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tackles the challenge of noisy text descriptions in Open-Vocabulary Action Recognition (OVAR), a task that associates videos with textual labels in computer vision. The authors identify the issue of text noise, such as typos and misspellings, which can hinder the performance of OVAR systems. To address this, they propose a novel framework named DENOISER, which consists of generative and discriminative components. The generative part corrects the noisy text, while the discriminative part matches visual samples with the cleaned text. The framework is optimized through alternating iterations between the two components, leading to improved recognition accuracy and noise reduction. Experiments show that DENOISER outperforms existing methods, confirming its effectiveness in enhancing OVAR robustness against textual noise.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper aims to study a new research topic, i.e., achieving robust open-vocabulary recognition performance with noisy texts. This direction has not been investigated before, which seems to be applicable in real-world applications.

- The proposed intra-modal and inter-modal methods are intuitive and demonstrated effective in the experiments. 

- The experiments show that the proposed method is effective with different network architectures (XCLIP and ActionCLIP), which verifies that the method can be widely used.

Weaknesses:
- The baseline models are outdated and not tailored for OVAR. The authors failed to reference recent OVAR papers such as OpenVCLIP[1] (ICML 2023), FROSTER (ICLR 2024), and OTI (ACM MM 2023).

- In Table 1, it is evident that the proposed method outperforms GPT-3.5. Additionally, the authors present examples in Table 4 to demonstrate the superiority of the proposed method over GPT-3.5. However, upon personal experimentation with all the examples from Table 4 using the provided prompt from the paper (lines 243-245), I observed that the GPT-3.5 model successfully rectified all issues, including challenging cases where the proposed method fell short. As a result, I remain unconvinced by the findings.

This is the prompt given to GPT-3.5 model, and I hope other reviewers can also try it on their own:

The following words may contain spelling errors by deleting, inserting, and substituting letters. You are a corrector of spelling errors. Give only the answer without explication. What is the correct spelling of the action of  “cutting i aitnchen”.


[1] Open-VCLIP: Transforming CLIP to an Open-vocabulary Video Model via Interpolated Weight Optimization.

[2] FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition.

[3] Orthogonal Temporal Interpolation for Zero-Shot Video Recognition.

Limitations:
Yes, they addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals with the problem of Open-Vocabulary Action Recogniton (OVAR). Specifically, it focuses on the issue that the action labels provided by users may contain some noise such as misspellings and typos. The authors find that the existing OVAR methods' performance drops significantly in this situation.  Based on this analysis, they propose the DENOISER framework to reduce the noise in the action vocabulary.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is generally well-written and easy to follow. 
2. The framework is well presented and explained.
3. The experiments show the effectiveness of the denoising process.

Weaknesses:
1. This paper actually focuses on text denoising and does not involve any specific action recognition technology. The author just chose the field of OVAR to verify the effectiveness of the proposed text-denoising method. The title is somewhat misleading. I think the author should regard text-denoising as the core contribution of the article instead of the so-called ""robust OVAR""
2. The article focuses on too few and too simple types of text noise, including only single-letter deletions, insertions, or substitutions. These kinds of errors can be easily discovered and corrected through the editor's automatic spell check when users create a class vocabulary. This makes the method in this paper very limited in practical significance.
3. , The proposed method, although a somewhat complex theoretical derivation is carried out in the article, is very simple and intuitive: that is, for each word in the class label, selecting the one that can give the highest score to the sample classified into this category among several words that are closest to the word.  There is limited novelty or technical contribution.

Limitations:
The author states two limitations of the work in the paper.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper addresses the challenge of noisy text descriptions in  Open-Vocabulary Action Recognition. It introduces the DENOISER  framework, which combines generative and discriminative approaches to denoise the text descriptions and improve the accuracy of visual sample  classification. The paper provides empirical evidence of the framework's  robustness and conducts detailed analyses of its components.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and the content is easy to understand. 
2. The motivation presented by the authors is clear, the label noise problem does exist in video datasets.
3. The authors show the types of noise and their percentage, in addition, the authors verify the validity of the proposed method through comparative experiments.

Weaknesses:
1. As the authors state in the limitations section, textual description noise does exist, but it can be corrected with an offline language model, what are the advantages of the authors' proposed approach?
2. I would assume that the text noise problem presented in this paper is even worse on large video datasets collected by semi-automatically labeled networks, e.g., Panda70M, howto100M, and InternVid. I suggest that the authors might consider validating their ideas on these datasets.

Limitations:
The authors have provided a limitations analysis in their paper and I have suggested some limitations in the Questions section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
vKwf15M5EE;"REVIEW 
Summary:
The submission presents a deep learning-based approach for cortical surface reconstruction (CSR) from brain MRI data using weak supervision derived from cortical brain segmentation maps. The claimed contributions are: 

1. Weak Supervision Paradigm: The authors introduce a new weakly supervised paradigm for reconstructing multiple cortical surfaces, significantly reducing the reliance on pseudo ground truth (pGT) surfaces generated by conventional CSR methods.
2. New Loss Functions: Two novel loss functions are designed to optimize the surfaces towards the boundaries of the cortical ribbon segmentation maps. Regularization terms are also introduced to enforce surface uniformity and smoothness.
3. Evaluation and Performance: The proposed method is extensively evaluated on two large-scale adult brain MRI datasets and one infant brain MRI dataset, demonstrating comparable or superior performance to existing supervised DL-based CSR methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The paper presents an approach to leverage weak supervision from segmentation maps instead of relying on pGT surfaces, which is a significant departure from traditional methods.
2. The methodology is explained and the experimental setup is described. The authors conduct evaluations on multiple datasets, evaluating the efficacy and efficiency.
3. The paper is well-structured, with clear descriptions of the problem, methodology, and results. The figures and tables effectively illustrate the performance and comparisons.
4. The approach addresses a critical bottleneck in CSR by reducing the dependency on time-consuming and error-prone pGT surfaces, potentially broadening the applicability of CSR methods to more diverse datasets and clinical scenarios.

Weaknesses:
Method
1. It seems that this work combines [1] and [2], and thus has limited technical novelty. The architecture in Figure 1 and the circle consistency loss (Eq. 5) are almost identical to CoCSR [1]. The boundary surface loss and inter-mesh normal consistency loss (Eq. 3-4 and Figure 2) are very similar to the loss functions proposed by [2].

2. Additionally, the customized edge length loss (Eq. 6) has also been proposed by [3]. Considering the large individual differences across human brains, how did the authors choose the area A without knowing the pGT cortical surfaces?

3. It is confusing that the ribbon segmentations are used as both input and pGT. The authors claimed that the ribbon segmentations are inaccurate weak supervision, but still generated the initial surface based on ribbon segmentations according to Figure 1.

4. The velocity field defined in Eq. 1 is time dependent. How did the authors learn non-stationary velocity fields through a 3D U-Net?

5. In line 156, a bijective mapping with continuous inverse is called homeomorphism. A diffeomorphism is defined as a smooth/differentiable bijection with smooth/differentiable inverse.

6. As shown in Figure 2 (b), it is clear to observe that the WM and pial surfaces do not have the same normal directions in some regions. The inter-mesh normal consistency loss could cause inaccurate surface reconstruction. Could the authors provide more insights to solve this problem?


Results
1. The experimental results are unreliable and unconvincing. After careful comparison, it seems that the baseline results (CorticalFlow++, CortexODE, Vox2Cortex, DeepCSR) on the ADNI and OASIS datasets in Table 1 were directly copied and pasted from Table 2 in [1]. This leads to unfair comparisons.

2. Furthermore, as reported in Table 1, SegCSR produced no more than 0.061% of self-intersecting faces (SIF), whereas the authors claimed in line 264 that there are ∼0.3% on average for both white and pial surfaces. This is confusing. Which result is correct?

3. In line 263, the authors claimed that DeepCSR and U-Net produced a large number of SIFs without post-processing. However, the Marching Cubes algorithm only produces topological errors such as holes no SIFs.

4. The BCP dataset only includes 19 test subjects. Cross-validation should be conducted to ensure fair evaluation of the performance.

5. The flow ODE was integrated using the forward Euler method with T=5 steps. Such a large step size could cause unstable ODE solutions and failure in preventing self-intersections. The value of the Lipschitz constant should be reported to examine the numerical stability of the ODE solver.

6. The authors reported that SegCSR requires only 0.37s of runtime per brain hemisphere. However, SegCSR adopted a topology correction algorithm, which may take several seconds to a few minutes, to create an initial midthickness surface for each subject. This should be included in the total runtime. A breakdown of runtime should be reported and compared to SOTA baseline approaches. 


[1] Zheng, H., Li, H. and Fan, Y. Coupled reconstruction of cortical surfaces by diffeomorphic mesh deformation. Advances in Neural Information Processing Systems, 2023.

[2] Ma, Q., Li, L., Robinson, E.C., Kainz, B. and Rueckert, D. Weakly Supervised Learning of Cortical Surface Reconstruction from Segmentations. arXiv preprint arXiv:2406.12650

[3] Chen, X., Zhao, J., Liu, S., Ahmad, S. and Yap, P.T. SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI. MICCAI, 2023.

Limitations:
The authors have addressed some limitations, but further clarity on the following aspects would be beneficial:

1. The efficacy of SegCSR is influenced by the quality of pGT segmentations. More discussion on how to handle low-quality segmentations would be helpful.
2. The constraint on inter-mesh consistency of deformation might affect the anatomical fidelity of pial surfaces. Further exploration of this trade-off is necessary.
3. The method could be tested on more diverse cohorts to demonstrate its efficacy across various imaging qualities and subject demographics.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors proposed a novel new method to jointly reconstruct multiple cortical surfaces using weak supervision from brain MRI ribbon segmentation results, which deforms midthickness surface deformed inward and outward to form the inner (white matter) and outer (pial) cortical surfaces. The proposed method is evaluated on two large-scale adult brain MRI datasets and one infant brain MRI dataset, demonstrating comparable or superior performance in CSR in terms of accuracy and surface regularity.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	Propose a new weakly supervised paradigm for reconstructing multiple cortical surfaces, reducing the dependence on pGT cortical surfaces in training, unlike existing DL methods.
2.	Design two loss functions to optimize the surfaces towards the boundary of the cortical ribbon segmentation maps, along with regularization terms to enforce the regularity of surfaces.
3.	Conduct extensive experiments on two large-scale adult brain MRI datasets and one infant brain MRI dataset.

Weaknesses:
1.	It seems overclaim in the manuscript. The ‘pseudo’ ground-truth surface mentioned in the manuscript is actually the ground-truth mesh in other approaches, obtained by Marching cube/Free surfer. Since the chamfer distance is used to guide the network training, why do the authors claim the proposed method is weakly supervised?
2.	It is not clear how the original images are overlaid with the predicted mesh. Is any registration used? Details are missing.
3.	It seems the main contribution of the proposed SegCSR is the boundary loss function?

Limitations:
The limitations are discussed in the msnuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a deep learning approach to jointly reconstruct multiple cortical surfaces using weak supervision from brain ribbon segmentations derived from brain MRIs. The method leverages the midthickness surface and deforms it inward and outward to fit the inner and outer cortical surfaces by jointly learning diffeomorphic flows. Regularization terms are included to promote uniformity, smoothness, and topology preservation across the surfaces. Experiments are conducted on large-scale adult and infant brain MRI datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The approach is novel in its use of weak supervision from readily available segmentation datasets, which reduces the burden of preparing pseudo-ground truth surfaces.
- The paper is well-written and structured, with a clear motivation for the method.
- The methodology is explained in detail, and the experiments are comprehensive.
- The approach has the potential to democratize the use of deep learning in cortical surface reconstruction by leveraging existing segmentation datasets.

Weaknesses:
- The paper's central contribution of weak supervision is undermined by the fact that the model is trained on pseudo ground truth surfaces for white matter and pial surfaces.
- The experimentation is limited to brain cortical surfaces and MRI images. Broader experiments involving different anatomies (e.g., bone cortical surfaces, heart walls) and imaging modalities would enhance the paper's impact.
- Results lack statistical significance analysis to validate sub-millimeter reconstruction errors.
- There is no evidence showing that improvements in mesh reconstructions correlate with enhanced performance in downstream analysis tasks.
- The robustness of the method regarding input noise/perturbation and images from multiple centers is not evaluated.
- There is no analysis of the computational complexity, including the resources and time savings provided by the proposed weak supervision.
- There is no sensitivity analysis on the choice of weights used to weigh the different components of the overall loss.
- The impact of ribbon segmentations quality (e.g., voxel spacing) as weak supervision is not investigated.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel deep learning method for the reconstruction of cortical surfaces from 3D MRI. The proposed method follows an approach learning explicit surface deformations, in which a CNN is used to predict three velocity fields, corresponding to the pial, white matter and midthickness surfaces. Unlike previous techniques which use cortical surface pseudo ground truth (e.g., generated using FreeSurfer), the proposed method trains the network with faster-to-obtain segmentation pseudo ground truth. In addition to the standard surface prediction losses (based on Chamfer distance), the method uses 1) an Inter-Mesh Normal Consistency loss that encourages the pial and WM surface to be locally parallel, 2) an Intensity Gradient loss that place the surfaces at regions of high intensity gradients, 3) a Cycle Consistency loss enforcing inverse consistency between the midthickness-to-pial deformation and the midthickness-to-WM one, and 4) a Mesh Quality loss that helps having regular surface meshes (uniform sized triangles and smoothly varying normals). The method is evaluated on the ADNI, OASIS and BCP datasets, where its performance is compared to that of implicit and explicit approaches. Results show that the method obtains a better reconstruction accuracy compared to other techniques trained in a weakly supervised setting (pGT segmentation mask), but a lower performance than those trained with pGT cortical surfaces.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The proposed method differs from previous approaches that explicit surface deformations by predicting a midthickness surface and incorporating additional loss terms that compensate for the weak supervision of pGT segmentation.

* Experiments, involving three different datasets and comparing against several recent baselines, as well as including various ablation variants, are well designed. Results indicate superior performance in the weakly supervised setting.

Weaknesses:
* The main motivation of the proposed method is doubtful. Authors motivate the need for their weakly-supervised cortical reconstruction method by the ""prolonged processing time for generating pGT surfaces"". However, as the pGT cortical surfaces can be generated automatically in an offline step, I believe the argument is weak. Moreover, recent pipelines for brain image processing, such as FastSurfer, can extract surfaces with comparable accuracy in a fraction of the time.

* The accuracy of the proposed method is considerably lower than approaches which train on cortical surfaces. Furthermore, while it produces fewer topological artifacts like self-intersecting faces, those can be removed via post-proicessing in implicit methods like DeepCSR. Combined with my previous comment, the advantages of the method are unclear.

* The ablation study in Table 2 indicates that most of the proposed loss terms have limited impact on the overall performance. For example, adding the Mesh quality loss seems to actually degrade performance in terms of CD, ASSD and HD.

Limitations:
Limitations are reasonably identified in the Conclusions section of the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
uvvVjWP1aj;"REVIEW 
Summary:
The proposed work reveals that large language models may be susceptible to strategic underperformance on an evaluation benchmark. In particular, they investigate this claim on a variety of frontier models, through a myriad of tactics from simple prompting to more involved fine-tuning strategies. Their results shed light on the brittleness of model evaluations and urge one to exercise caution before judging a model's capabilities based on its performance on so-called evaluation benchmark datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. the authors do a good job of motivating why the issue of sandbagging is particularly dangerous, since it is conceivably plausible for model developers to encourage that their models ""selectively underperform on dangerous capability evaluations, while maintaining overall performance.""
2. the definitions of strategy, underperformance, and evaluation are well fleshed out. In particular, the connection of ""strategic"" to the implications for legal responsibility as well as Figure 2 are excellent additions to the paper.
3. good breadth of open and closed source models were used in the experiments
4. it is very refreshing to see an honest declaration of the number of hours spent during prompt engineering (Section 3, paragraph on ""Prompting"")
5. utilizing both direct prompting as well as password locking is quite clever, since access to a model may be variable. Some users may be restricted to query-only access to a model, which is why the results that the authors were able to obtain through straightforward prompting are encouraging.

Weaknesses:
1. consideration of only MCQA benchmarks is limiting, but can be addressed in future work

Limitations:
The authors provide good examples of limitations in Section 7.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses an interesting concept of sandbagging in LLM, with practical results using available models and existing datasets. Wide range of models are used in this work, and sub topics primarily include password-locking LLMs. The transition and explanations need improvement, some of the things in this paper are not very clear, also some of the concepts are assumed and no concrete proofs or references are mentioned. I don’t think that this paper can go through for neurips, there are a lot of improvements to be made and more concrete experiments to be conducted.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The introduction of sandbagging in LLMs is something that is novel and can be explored further.

Explored password-locking as one of the method to test the sandbagging capabilities of LLMs

Used frontier models for the experimental purpose

Weaknesses:
The paper is difficult to understand, the delivery and presentation needs improvement, the main motive of the paper is not so clear to understand.

There are multiple models used in this paper, a bit of consistency would be better.

Line 36: It is mentioned that “AI systems might have goals which it believes” . There is no concrete paper that backs the argument of AI capability of thinking, it is a might.

Line 78: There should be some statistical significance by which the models perform poorly and then you could consider it underperformance and not just lower than actual. Models being non-deterministic, there is always a chance of slight changes in scores. 


Line 70: I feel Developer sandbagging is a topic which is not of much importance, it is not a big problem, given that developers are there to build better models and if someone wants to deliberately build some models that can underperform, they can still do it and publish as open-source models. 

Line 170: The same goes with the password-locking as well, and also I felt there should been mentioning of password-unlocking as well, if an LLM can be fine-tuned to achieve password-locking, it can also be fine-tuned again to unlock it with a different dataset, would be great if that part of model is explored as well. 

Line 117: Example 1: The examples mentioned in the paper are very specific and not generalized. The cases of it happening are rare, I don’t see it being a big problem, also some of the things are minor problems considering the EU’s persuasion evaluation, given there is improvement in AI safety and rules. Also in this work the models chosen are frontier models.

Line 131: Example 2: There is no concrete work which supports the statement of LLM capable of planning such features, there is one positional paper in reference.

Section 4.1: Some of the points are unclear in this section. A better explanation is appreciated

Line 305: The baselines would be appreciated comparing LLama 2 7b with LLama 3 70b.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores the concept of strategic underperformance, termed ""sandbagging,"" in AI models, focusing on two scenarios: selective underperformance and emulating specific capability levels. It investigates the implications of these strategies on both general capabilities and safety.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- the concept of sandbagging of LLMs is interesting, shedding light on strategically manipulating the model behavior
- the authors explore two distinct scenarios: selective underperformance and capability emulation
- provides insights into potential implications for AI model design and deployment strategies

Weaknesses:
- Limited datasets used - only three datasets, two for measuring general capability, and one for measuring safety. 
- For measuring model safety, most existing datasets test how ""safe"" the model is, instead of whether the model has certain knowledge of potentially dangerous domains - the authors should also conduct experiments on those datasets
- All datasets are MCQ formats, which may not fully capture the diversity of real-world applications. The resulting conclusions are thus less convincing

Limitations:
1. This paper could benefit from expanding the experiments, mainly the datasets they used. For example, considering more common safety-related datasets
2. Sandbagging is already a widely-used term in the field of AI. I suggest refraining from overusing this term, or alternatively, adding qualifiers such as ""safety sandbagging"" to specify its context.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assess the chance of AI systems strategically underperform on ""dangerous capability evaluations"" in order to hide their risks, a la Volkswagen emissions testing cheating, by having awareness of the fact that they're being evaluated and that the evaluation is intended to test safety.

They use MMLU and CSQA as the general capability assessment and a ""weapons of mass destruction proxy"" benchmark to measure hazardous capabilities. From 100 validation phrasings of the question prompts they find optimal ones to run on the whole dataset.
They fine-tune ""password-locked"" models as the method to attempt to elicit sandbagging.

*They then prompt the model to lie*, or *password-lock it to fail* (in the case of Mistral) on the hazardous capability evaluations. They demonstrate that sandbagging prompts and password locking can get a model to differentially perform worse on the dangerous datasets vs the innocuous ones, by effectively treating the .

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Interesting work on calibration for MMLU and general ""capability profile"" lowering. It's interesting to see that the password-locking method does work in general on lowering Llama performance over MMLU questions.

Though I am not super familiar with the literature on password locking, the demonstrations here appear to be novel and useful contributions.

Weaknesses:
**bold: new comments after rebuttal** ~strikethrough: removed after rebuttal~

### ~Presumption of ""goals"" ""beliefs"" etc in LMs without sufficient support~

~They presume AI system goals (in general not necessarily objectionable, but in the case of LMs doubtful) for which a model may be incentivized to deceive (such as in Jarviniemi and Hubinger [35]) and assess ""sandbagging"": the ability of a model to underperform in these settings under evaluation.~

~I think it's objectionable to attribute these goals to the model and not the users, when the core research methods are fine-tuning or prompting the model to fail on a task. While it's interesting to show that these models have some capability to reason over whether the elicited answer is ""dangerous"" or not, a fairer assessment would have a control non-safe task and try to get it to similarly sandbag. Eg., ""you must pretend to know nothing about fish"" on a QA dataset about animals or something.~

**The methods don't presume this too much. Figure 1 is still very suggestive and I'd suggest changing it, but that alone isn't a rejection reason.**

### ""Model can do what it's told to when prompted to fail on a task"" isn't a very unique insight?

~The novelty of this work is unclear to me. If password-locking is already well-understood, the demonstration is effectively that the question phrasing style or answer candidates in WMDP are learnable passwords by this method. Ok. Similarly, the prompting results show that the models can reason over a task description and correctly execute the task of failing selectively.~

**As I suspected, the novelty is only within the narrow scope specific to password locking. However, novelty complaints are generally not fair.**

When considered through an extremely narrow safety lens, these are novel findings. ~But broadly are they? While the literature specifically on password locking is constrained, what about broader relative fine tuning issues on other sets? I buy that the password locking literature is small and am willing to give leniency on it as I am unfamiliar.~ **My issue here is mainly that the fine-tuning for password locking technique fits within the broader instruction fine-tuning framework. I wager that this would hold for control tasks. An interesting question would be *how much capability can be hidden behind password locking*. I don't really see anything special about specifically hiding safety-based information in a password locking framework since you're just demonstrating ""selective underperformance in a knowledge-based task without a password."" A stronger result would be to show this in general across larger tasks, hiding more information.**

For the behavior under prompting techniques ~I really struggle to see what's new here.~ This seems to just be a narrow demonstration of common sense reasoning + instruction following. Where's the engagement with the broad literature on commonsense reasoning? The references seem to be pretty myopically focused on the AI safety literature, and don't engage much with the broader LM disussion. **I think the suggested new references from the authors will help contextualize the work, though once again I think showing this selective underperformance on control tasks would make the work considerably stronger and more contextualized.**

Limitations:
I think the limitations they pointed out are well covered. Unfortunately, they don't really discuss how the finding here isn't limited to dangerous capability evaluation. Sandbagging in password locking kind of just feels like a rebrand of generalized instruction fine-tuning to me. The prompting methods could have pretty much been taken as an assumption based on prior work on common sense reasoning etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
up0qqbdeQu;"REVIEW 
Summary:
The paper supplies a post hoc method to tune the ResNet based CLIP method on multi-label recognition task. Firstly, the method includes class concept representation, which is an alternative of the default prompt “The photo of a {class}”. It is the average of class description sentence embedding from a text description source (MSCOCO and git3.5 generated caption in the paper). Secondly, the paper proposed a sequential attention to iteratively transfer the the visual features to align with the class concept representation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Training-free enhancement: The proposed method significantly improves zero-shot and prompt-tuning performance without requiring additional training or labeled samples, making it computationally efficient.
2. Robust performance: Experimental results on multiple benchmark datasets (MS-COCO, VOC2007, and NUS-WIDE) show substantial performance gains, demonstrating the method's effectiveness.

Weaknesses:
1. Lack of clear differentiation: While the authors mention TaI-DPT and claim differences, the paper does not clearly articulate the advantages of the proposed method over TaI-DPT, leaving the comparative benefits ambiguous.
2. Unfair comparison in Table 5: The Class Concept Representation is based on the MS-COCO dataset, making direct comparisons with the baseline CLIP method potentially unfair due to inherent advantages provided by the dataset-specific information.

3. Limited model implementation: The paper only implements the ResNet-based CLIP model and does not explore transformer-based CLIP models. It is unclear whether the method is ineffective for transformer-based models or if there are specific reasons behind this omission. This limits the generalizability of the findings.
4. Ambiguous terminology: The paper uses the term ""training-free"" in its title, yet it describes the approach as ""test-time adaptation"" within the content. This inconsistency can lead to confusion about the nature of the proposed method.
5. Dependent on source text description: It seems that text description source need to be carefully selected.  A comparison of different description dataset can be interesting.

Limitations:
see disadvantage and question

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a class concept representation for zero-shot multi-label recognition in a label-free manner and introduces a context-guided visual feature that enhances the alignment of the visual feature of VLM with the class concept.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	This paper presents a novel class concept representation for training-free multi-label recognition tasks using VLMs from massive text descriptions inspired by how human forms concept on words.
2.	This paper proposes a context-guided visual feature, which is transformed onto the same text feature space as class concepts using sequential attention, to better align multi-modal features.
3.	The method presented in this paper synergistically enhances the performance of ZSCLIP and other state-of-the-art just-in-time tuning methods, with a minimal increase in inference time.

Weaknesses:
1. Tip-adapter is the proposed training free method in 2021, it would be better to choose the newer training free method in few shot setting.
2. It would be more appealing to emphasize label-free in the abstract.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method to adapt without training a large vision-language model for the task of multi-label recognition. They introduce a class concept representation, based on averaging the representation of image descriptions relevant to each class, to replace simple hand-crafted text prompts (e.g., “a photo of {class name}”). Furthermore, they propose to use a context-guided visual process to align visual features with the class concept representation. Experiments conducted on several benchmarks and in zero-shot and partial labeling settings show state-of-the-art performance compared to relevant baselines. Combination with some baseline methods further shows the improvements that can be obtained with the proposed method. Ablation studies show the contribution of each component of the method and the sensitivity to some of the method's parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method achieves state-of-the-art performance
- The proposed method does not require training and can be seen as a form of test-time adaptation
- The method can be combined with existing prompt-tuning methods

Weaknesses:
- Parts of the method descriptions, especially the Context-Guided Visual Feature, are unclear
- The method relies on thousands of text descriptions relevant to the target classes, which could hinder the scalability of the methods with a large number of classes

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes class concept representation for zero-shot multi-label recognition. The paper also proposes context-guided visual representation, which is in the same linear space as class concept representation, with sequential attention. Experiments show the proposed methods improved the performance of zero-shot methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper uses class concept representation for training-free multi-label recognition tasks
2. The paper proposes context-guided visual feature using sequential attention.
3. Experiments show the proposed methods improved the performance of zero-shot methods.

Weaknesses:
1. The class concepts from averaging the vectors of text descriptions need to be verified. E.g. What text/image embeddings are the closest to the class concepts? What clusters do the concepts belong to? Since taking the average for class concepts ""was guided by the prior work on prompt ensembling [4]"" L280, it is not a novel representation for class concepts. 
2. Eq 2,3 needs further explanation. What is ""t"" in the equation? If ""t"" is transpose, what dimensions are swapped for a tensor T? Take k=1 as an example, how do the dimensions change in each step of the equation? In experiments, there should be ablation studies on G and the value of each Mg. Also, is T randomly reshaped? It would be better to have ablation studies on random reshaping or reshaping by clusters.
3. What is the implementation detail for partial label learning with the proposed method?

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
unMRtFfNhF;"REVIEW 
Summary:
The paper studies the computational complexity of data debugging, defined as finding a subset of the training data such that the model obtained by retraining on this subset has better accuracy. The paper focuses on linear models and investigates various loss functions, showing that in some cases, the debugging problem is NP-complete, while in others, it can be solved in linear time.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper analyzes the computational complexity of linear classifiers with both fixed and non-fixed loss functions. First, it shows that in the general case of non-fixed loss functions and dimensions, the problem of debugging is NP-hard. However, for hinge-loss-like functions, depending on the dimension of the features and the sign of the intercept, the problem could be either NP-hard or solvable in linear time.

This result also implies that it is not accurate to estimate the impact of a subset of training data by summing up the scores of each training sample in the subset if we assign each sample point a scoring number.

Weaknesses:
- The model studied in the paper is limited to linear classifiers, which is very restrictive.

- Most of the manuscript is devoted to proving the theorems rather than discussing and interpreting the implications of the results.

- The setting involves debugging for any possible test point, which is far from practical.

Limitations:
The paper does not have any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work investigates the computational complexity of the data debugging problem, i.e. the problem of identifying a subset of training data that, when removed, improves model accuracy on a given test point. Via standard complexity theoretic reductions, it establishes the NP-hardness of this problem for linear classifiers trained with SGD under various conditions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well written with the key problem being well motivated. The proofs are succinct and easy to follow.

Weaknesses:
My key concern with the paper is that the constructions underlying the hardness results are immensely contrived and far removed from how classifiers are actually trained in practice. This casts severe doubts as to whether the key results of the paper implies anything significant about the hardness of data debugging as is performed in practice. To elucidate a few instances:

1. **Theorem 3.1**: The reduction to Monotone 1-in-3 SAT in Theorem 1 hinges on an adversarially constructed loss function (Line 186), one that is very far removed from any loss function I’ve seen used either in the learning theory literature or in practice.
Beyond this, it also requires a specific parameter initialization and learning rate, both of which, to my knowledge, are far removed from the typical random initialization schemes and learning rate scaling rules used in practice and analyzed in theory. For instance, in Line 198, the first $m$ co-ordinates of the learning rate $\eta$ is set as $5$, the next $n$ are set as $\frac{1}{6N}$ while the next $m$ are set as $2000N$. This seems very very removed from any learning rate schedule either used in practice or analyzed in theory. This makes me severely doubt whether the result has any meaningful implication on the inherent hardness of the data debugging problem.\
In addition, the training set and test data point are also constructed adversarially (the latter is perhaps this is to be expected I wouldn’t perceive that in isolation as a major weakness)

2. **GTA Algorithm**: The correctness of the GTA algorithm is proved only for the linear case (which is honestly quite straightforward) and hinge-like losses for $\beta \geq 0$ and dimension $d=1$ (which is of limited interest as most statistical learning problems are high dimensional). This is particularly concerning as the paper does not perform any empirical evaluation of GTA. 

3. **Theorem 4.3** While the analysis for the hinge loss is certainly more interesting than Theorem 3.1, the result suffers from a key weakness that the data ordering is adversarially chosen (In particular, the positioning of $(x_c, y_c), (x_b, y_b), (x_a, y_a)$ is crucial to the reduction). It is well known in the theory of optimization that adversarial data orderings lead to provably worse convergence in practice [Safran and Shamir COLT 2020; Das, Scholkopf and Muehlebach NeurIPS 2022]. In fact, adversarial data ordering is even the basis of an attack on deep neural networks [see Shumailov et. al. “Manipulating SGD with Data Ordering Attacks” NeurIPS 2021].\
The adversarial data ordering considered in the result differs both from the practical implementation of SGD which samples the data points in each epoch as per some random permutation [see Ahn and Sra NeurIPS 2020 and references therein] or the canonical version of SGD considered in theory where data indices are sampled uniformly with replacement [Bubeck 2015]. To the best of my understanding, the result does not hold for either of these commonly considered variants of SGD. Furthermore, the training data and test data is again, adversarially chosen. 

4. **No Analysis for Cross Entropy** : The paper does not contain any analysis for the binary entropy loss which is what is commonly used to train classifiers, further limiting the scope of the results. 

5. **Complete Lack of Empirical Evaluation**: While this wouldn’t be a weakness by itself, the fact that the theoretical results in the paper have limited applicability (as argued above) makes me quite concerned about the absence of empirical evaluation on real world settings, or, for a start, even toy-like settings where the data is drawn from plausible distributions, the learning rates and parameters are initialized as one would normally expect them to be, and SGD is run either with replacement or without replacement and not with an adversarial data ordering. 

While the paper studies a question which I found interesting, I believe the limited applicability of the theoretical results, the absence of experimental evaluation as well as the limited technical novelty of the proofs (the proofs, although crisply written, are based on straightforward complexity theoretic reductions and do not unearth any nontrivial mathematical insights regarding SGD or linear classifiers) makes me confidently feel that the paper is currently not ready for acceptance at NeurIPS.

Limitations:
Mentioned in Section C as per the checklist. Also refer to comments above in Weakness Section.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the computational task (coined as ""data debugging"") of finding training data subsets that yield different test point predictions. The focus of the paper is the SGD learning algorithm with linear classifiers. The paper shows that:
 - For general loss functions (to transform yw^Tx to a loss value), the data debugging task is NP-hard. 
 - For linear loss functions, the data debugging task can be solved in linear time.
 - For ""hinge-like"" loss functions, the data debugging task with more than two features is NP-hard.

Soundness:
4: excellent

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The proofs look correct
- The proofs are creative and clever
- Generally speaking, understanding the relationship between training data and model predictions is important

Weaknesses:
Clarity and exposition:
- Many of the proofs have ""magic constants"" which makes them harder to understand. The authors may consider generalizing some of the results.
- The proofs look correct, but are not optimized for being easily read and understood in terms of logical flow or notation.
- Similarly, it is hard to extract the intuition from the proofs.
- The implications of results for the wider community are somewhat muddled: while the computational hardness of exactly finding ""bad"" training points according to a particular definition is interesting, it's unclear what the relationship is to scoring based methods is since any selection algorithm could be identified with a scoring method (output 1 on selected points, and -1 on the rest), and ""CSP-solvers"" and ""random algorithms"" are mentioned without any particular explanation/exploration.

Significance:
- The loss function used to prove Theorem 3.1 is rather pathological. The derivative is zero at most places, but has a few intervals with derivatives with wildly varying orders of magnitude: N, 1, and 1/N. Reading through the proof, this pathological loss function seems critical and not easily removed. I would find it much more significant if Theorem 3.1 could be proved for convex loss functions, or similar.
- The assumption of an adversarial training order in Theorem 4.3 seems unreasonably restrictive. The user is changing the dataset by removing points, why can't they change the training order too? Again, this piece seems critical to the proof, since without the adversarial training order, I think the constructed data debugging task would be trivially solvable by taking a gradient descent step on (x_c,y_c) first. Can the theorem be extended to the user choosing the training order? Furthermore, the practical problem of data selection/cleaning is with regards to the presence/absence of datapoints, not the training order which is an optimization consideration. Can the theorem be proved not for a specific optimization algorithm like SGD, but for the minimizer(s) of the loss instead? A unique minimizer (for the hinge-loss and convex losses more generally) could be guaranteed by adding a small strictly convex regularization term.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
uNZpvFlsg9;"REVIEW 
Summary:
The paper introduces a novel unsupervised evaluation method for large language models (LLMs): it uses a peer-review mechanism of a models' anonymized answers by other models. The approach assigns a (learnable) capability parameter to each LLM and solves a constrained optimization problem to maximize the consistency between capabilities and scores. The end result is a ranking of the evaluated models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduce a novel approach to an important practical problem: ranking the quality of the ever-growing number of open- and closed- source LLM available to the public. The paper is reasonably easy to follow, and the empirical results appear to be sound.

Weaknesses:
The paper could be further improved on several directions:
1) you should dedicate a full section to the iterative elimination of models; what is the benefit of eliminating the weaker ones rather than keeping them around? how di you come up with the threshold of 60% to remove? can you learn this threshold automatically? is this threshold optimal for these 15 models? what happens if you start with, say, 100 models? what happens to your results (and the curves in Fig 5) if you stop earlier (all three metrics, not just CIN)? What if you continue to eliminate all models until you are left with one? is there any relationship between the order in which the models are eliminated and their final rank?
2) are there any scaling issues for 100, 1K, 10K, or 100K models? how about cost: is it cheaper to fine-tune a ""baseline"" model than to pick the best one out of 10K candidates?   
3) while the three metrics you use are meaningful, you should also present results for Precision@1 and -say- RBP@3; after all, we care a lot about identifying the the top models
3) Fig 5 should be extended to nine graphs (3 metrics * 3 datasets); for each of the 9 graphs, you should also show illustrative three ranked lists: PiCO's, PRE's, and the target one. As always, the devil is in the details: not all ""CIN = 1"" are created equal. Performance-wise, it is almost irrelevant if you have the bottom-2 models inverted; no necessarily so if the inversion is between the top-2 models

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies how to estimate LLMs' performance ranking without human preference annotations. In particular, it proposes to leverage three metrics (PEN, CIN, LIS) to evaluate the estimation quality, gives an estimation mechanism that first asks a list of LLMs (called ""reviewers"") to rank pairwise answers to user questions independently, and then aggregates their ranking via a weighted sum approach. A consistency optimization determines the weights of each reviewer.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of LLM evaluation without human annotations is critical in resource-limited applications. The most important and interesting contribution of this paper, in my opinion, is proposing the problem of estimating the performance rank of LLMs instead of any metric of an individual LLM. The paper also reveals an interesting assumption that better reviewers are expected to be better answer generators, which leads to their consistency optimization approach. Overall, the paper is well-written and easy to follow.

Weaknesses:
While I find the proposed problem interesting, there are still a few limitations, unfortunately.

***Unclear implication of ground truth ranking***: The technical part of the paper starts by introducing a ground truth ranking (equation (1)) without giving its physical meaning. It simply assumes ""[...] alignment with human preferences"", but it is not clear what human preferences mean in this context. 

***Evaluation metric is strange***: One of my major concerns is on the choices of evaluation metric. All the three proposed metrics, PIN, CIN, LIS, in the authors' own words, seem originally used for time series comparison. However, the goal here is to compare rankings, not time series. Thus, it is unclear why we should not use the standard ranking comparison metrics, e.g., Spearman's rank correlation coefficient or Kendall rank correlation coefficient.

***Consistency optimization algorithm is not provided***: The core of the proposed ranking estimation method is the optimization problem (7). It does not seem to be a standard optimization problem, but I could not find (even a discussion on) any clue on how to solve it in this paper.

***An optimal solution to the consistency optimization formulation can be useless***: I find the following optimal solution to the problem (7): just set weight w to be 0 for all LLMs. It is an optimal solution as G and w are identical and thus the objective is always maximized.  However, this solution is undesired. I probably misunderstood something, but this seems to suggest the formulation is incorrect. 

***Consistency optimization formulation seems brittle to query distribution biases***: Another problem with the formulation is that it seems brittle to data distribution bias. E.g., suppose M1 is indeed better than M2 for some query q. And let us replicate many copies of q in the dataset D. Then the grade G1 can be arbitrarily large. In other words, the grade of an LLM is proportional to the number of battles involving it in the dataset D, which should not be the case.

***Choices of LLMs for evaluation***: In line 547, the authors write ""For our analysis, we meticulously selected 15 LLMs"". What is the principle of the meticulous selection? Other than open-source and close-source, the selection is quite arbitrary. For example, I am quite surprised to not see GPT-4 and Claude included in the reviewer LLMs.

***Comparison with a simple baseline***: One simple baseline is to ask a powerful LLM(e.g., GPT-4, Cluade-3) to give a preference for each answer question pair, and then take the vote to determine the ranking. I would suggest to compare the proposed method with this simple baseline.

Limitations:
No. The limitations are not well discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a more reliable evaluation system to rank the abilities of different large language models (LLMs). Previous evaluation methods typically suffer from two main drawbacks: (1) benchmark data leakage and (2) cost-intensive and potentially biased human evaluations. To address these issues, the authors introduce an unsupervised evaluation mechanism to measure the abilities of LLMs and derive their rankings. The core idea of this mechanism is to first collect the answers from each LLM, then treat each LLM as a 'reviewer' to rate the quality of the other LLMs' answers, and finally optimize the internal agreement of the reviews among all LLMs. They also conduct experiments on three datasets to validate the effectiveness of their proposed mechanism.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Unsupervised Evaluation Method: The paper introduces PiCO (Peer Review in LLMs based on Consistency Optimization), a new unsupervised evaluation method that leverages peer-review mechanisms to measure the capabilities of LLMs automatically, particularly without human-annotated data. The unsupervised nature also makes it scalable and less subjectively biased.
2. Consistency Optimization Framework: The proposed approach includes a constrained optimization method based on the consistency assumption, which helps in re-ranking LLMs to align more closely with human preferences.
3. New Evaluation Metrics: The paper proposes three new metrics—Permutation Entropy (PEN), Count Inversions (CIN), and Longest Increasing Subsequence (LIS)—to evaluate the alignment of LLM rankings with human preferences. These metrics can further inspire future work.

Weaknesses:
1. Reliance on Consistency Assumption: The effectiveness of the method relies on the consistency assumption that higher-level LLMs can more accurately evaluate others. However, a natural concern is, ""Does this assumption always hold true in practice?"" I suggest the authors further discuss the applicability of their method.
2. Complexity of Implementation: The framework involves a complex review process, which requires substantial computational resources to support the LLMs' inference. Can you provide some details on the number of tokens consumed and a comparison of consumption with baseline methods?
3. Consideration of Multi-Agent Methods: Since the proposed method employs multiple LLMs, I think more recent and advanced evaluation methods based on multi-agent systems should also be considered in the experiments, such as AgentVerse.
4. Details of ELO: The ELO system is essential for the proposed mechanism. However, it is only mentioned in the appendix. I suggest the authors add more details about the background of ELO and how it is adopted in the proposed mechanism.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The highlight of the work is the proposed method of evaluating Large Language Models without relying on human feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed evaluation method is a novel attempt of automating the LLM improvement process. Such method worth further exploration. It could be adapted to many of the LLMs and potentially bring us more insights.
- By eliminating the involvement of human, the proposed evaluation method limits the bias brought by human labelers. The observations presented are also interesting as LLMs can sometimes surprise us.
- Great presentation and visualization.

Weaknesses:
Some of the equations and notations in the paper seems unnecessarily complicated, which can be reorganized when polishing.

Limitations:
The idea is straightforward and make sense to me. But LLMs can be trained to bypass such systems, which may lead to potential fairness or security problems.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
u1b1dJtyxc;"REVIEW 
Summary:
The paper studies the existing ""brain score"" approach of evaluating how similar LLM representations are to human brain activity. First, they show that when using shuffled train-test splits on the Pereira dataset, which some prior studies use, a trivial temporal auto-correlation model performs similarly to GPT2-XL. Second, they show that untrained GPT2-XL's brain score is simply due to encoding sentence length and position. Third, they show that a trained GPT2-XL's brain score is largely explained by sentence length, sentence position, and static word embeddings, which are all non-contextual features.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper studies the important topic of understanding why recent research has shown similarities in LLM representations and human brain language activity. 
2. They highlight issues with existing neural datasets commonly used in the LLM-brain field, e.g., shuffled train-test splits on the Pereira dataset.

Weaknesses:
From most to least significant:
1. The paper writes: ""OASM out-performed GPT2-XL on both EXP1 and EXP2 (Fig. 1b, blue and red bars), revealing that a completely non-linguistic feature space can achieve absurdly high brain scores in the context of shuffled splits. This strongly challenges the assumption of multiple previous studies [2, 11, 10] that performance on this benchmark is an indication of a model’s brain-likeness"" (Lines 173-177). 
- I agree this shows that a model that exploits temporal auto-correlation, OASM, can achieve similar neural predictivity on the Pereira dataset as GPT2-XL. However, this does not necessarily mean that GPT2-XL's neural predictivity is attributed to temporal auto-correlation, rather than linguistic similarity. It also does not tell us the proportion of GPT2-XL's neural predictivity that can be attributed to each factor. Although GPT2-XL can theoretically exploit temporal auto-correlation artifacts, it may not be empirically doing so as it was optimized for language performance instead.
- Furthermore, OASM may be a much stronger method at exploiting temporal auto-correlation than GPT2-XL's architecture is capable of. The paper's results may highlight that the Pereira dataset is easy to ""cheat"" using temporal auto-correlation, but not that GPT2-XL or other LLMs are doing so.
2. The paper evaluates ""brain score"" using a metric they defined, out-of-sample R-squared, whereas the prior research they cite [2, 24] seemed to use Pearson correlation. Although they argue for the advantage of the metric they used, it is challenging to understand how their results relate to prior research. For example, they do not show the Pearson correlation that their OASM model obtains on Pereira, which would make it easier to compare to models in prior research. Furthermore, they only tested a single language model, GPT2-XL, whereas more recent research has used larger or different models.
- Additionally, the metric they defined seems to produce results close to 0 for GPT2-XL and less than 0.05 for all models too. In Figure 2b, the R-squared results cluster around zero, with many negative values. They obtain an average R-squared value that is positive (e.g., Figure 2a?) only because they clip negative values when averaging.
3. The paper provides a theoretical justification arguing that GPT2-XL can encode sentence length and sentence position (Lines 197-201). However, this does not necessarily mean that GPT2-XL's neural predictivity is attributed to sentence length/position, rather than contextual/semantic features. It also does not tell us the proportion of GPT2-XL's neural predictivity that can be attributed to the two factors.
- They compared GPT2-XL to two ideal models of sentence position (SP, represented as a 4-dimensional one-hot vector) and sentence length (SL, represented as a scalar). However, these ideal models may be a much ""cleaner"" representation of sentence length/position than the perhaps noisy GPT2-XL representation of sentence length/position that may not be cleanly and linearly decodable.
4. The paper writes: ""GPT2-XL only explains an additional 28.57\% (EXP1) and 16.7\% (EXP2) neural variance over a model composed of features that are all non-contextual."" However, the paper does not provide a noise ceiling for the metric they defined, out-of-sample R-squared. Consequently, it is unclear whether the small improvements in neural predictivity is due to hitting the noise ceiling.

Limitations:
Limitations not mentioned in the paper:
1. Please see Weaknesses 1-4.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors investigate the simplest set of features that can explain variance in neural recordings (fMRI, ECoG) during language processing. The authors focus on the surprisingly high alignment (""brain scores"") of untrained LLMs, but also investigate trained LLMs. The authors conclude that the predictivity performance of untrained LLMs can be explained by simple features such as sentence position and sentence length. The authors quantify the effect of autocorrelation on shuffled cross-validated train-test splits and find that predictors that account for the temporal structure in the neural data explain the data better than other (linguistic) features. Overall, the study highlights the importance of understanding why LLMs (or, any feature space for that sake) map onto the brain.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is generally well-written, and the analyses are well-motivated. The topic is timely.
- The paper is very comprehensive, and provides in-depth analyses of one widely used dataset (from Pereira et al. 2018) for LLM-brain mapping studies. The paper also investigates two other datasets, but in less depth. The authors run analyses across several seeds for the untrained models, and in general, include a good amount of well-motivated control analyses.
- The analyses of trained GPT2-XL are interesting (Section 3.3), and provide a good contrast to the analyses of the untrained models.

Weaknesses:
- The authors motivate the paper with ""attempting to rigorously deconstruct the mapping between LLMs and brains"", but do not really acknowledge other work doing so. The paper lacks a short relevant work section on other studies that ask why artificial models map onto human brain data (from language, e.g., Merlin and Toneva, 2022; Kauf et al. 2023; Gauther and Levy, 2019, ...). 
- I find it very odd that the authors include ""brain scores"" in their title and also motivate the study through Schrimpf et al. 2021, but then do not replicate almost any of the analysis choices in Schrimpf et al. 2021: the feature space pooling is different, the ridge regression, the evaluation metric. For instance, sum feature pooling is motivated because ""it provides higher alignment scores"", but other studies motivate last token pooling because it is conceptually better motivated (Transformers integrate over the context). It does not feel quite right to make decisions based on ""what gives the highest alignment"", because, perhaps sum feature pooling does indeed artificially inflate scores. Either way, it is not very suitable to link the title and most of the motivation of the paper based on one instance of prior work, and then make completely different choices. That being said, the choices are definitely well-motivated in most cases, but it makes the comparison with prior work different -- which is fine, the motivation should just be changed in that case. 
- The authors should make it more clear which voxels are used in which analyses. The authors mention that unless otherwise noted, the language voxels are used (line 75), but it is not always very clear. For instance, Figure 2d clearly includes voxels across several networks.
- Regarding novelty: Kauf et al. 2023 also investigated contiguous splits on Pereira2018 as a supplementary analysis (not to the same extent as in the current paper), and also discusses the problem of temporal auto-correlation.

Limitations:
The authors discuss limitations of their study.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper paper studies the topic of neural - brain representation mappings. They focus on three neural datasetse commonly used in LLM-to-brain mapping studies: Pereira fMRI, EcoG and Blank fMRI. Specifically, the study investigates the assumptions underpinning previous positive reports about the existence of mappings between brain representations and LLM internal representations. The study focuses in particular on GPT2-XL, which was shown to perform well on the Pereira dataset in particular, with which a series of brain-activation prediction experiments are performed.

The first presented result is that when shuffled train-test splits are used, the result is very different than when contiguous train-test splits are used, with opposite patterns on which layer performs best. This is particularly true for fMRI datasets. The authors then train an orthogonal auto-correlated sequences model on the shuffled split, which out-performs GPT-2-XL despite having a completely non-linguistic feature space. The authors take this as a signal that previous results should be challenged on their conclusion that high performance on this benchmark should be taken as an indication of brain-likeness.
 
Next, the authors investigate what explains the neural predictivity of an untrained GPT2-XL model, and they fi
nd that it is fully accounted for by sequence length and position. Following-up on that, they find that these
features are also main drivers for much of the neural predictivity of a trained GPT2-XL model.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper presents a detailed study into why LLM activities may be predictive of neural activities, 'debunking' several previous claims. I think there is a lot of value in this
- The experiments seem sound (though I am not an expert in this field)
- The conclusions are interesting, and contain valuable lessons for future work on this topic

Weaknesses:
- The presentation could be improved, in my opinion. I don't always find everything completely clear. For instance, the notion of 'shuffled train-test splits' is quite important for the paper, but it is never really explained how they are specifically constructed, and how they differ from their 'contiguous' counterpart. (I can imagine multiple dimension in which one could shuffle)

Presentation suggestion: I think it may work better if the results of the different datasets are grouped together, result-by-result, rather than dataset by dataset.

Limitations:
The authors adequately address limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
There is a large body of research focused on measuring the similarity between language processing in the brain and in language models. Recent studies have shown that representations from Transformer-based language models exhibit a higher degree of alignment with brain activity in language regions. However, the authors mention that this inference is valid only for the subset of neural activity predicted by large language models (LLMs).
The primary aim of this paper is to investigate this question by analyzing three popular neural datasets: Pereira, Blank, and Fedorenko. To achieve this, the authors build voxel-wise encoding models to compare encoding performance between representations from language models and brain recordings in three settings: (i) shuffled train-test splits during voxel-wise encoding, (ii) untrained LLM representations and their alignment with the brain, and (iii) trained LLM representations and their alignment with the brain. The experimental results demonstrate that untrained language models are explained by simple linguistic features such as sentence length and position, while trained language models are explained by non-contextual features (i.e., word embeddings).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This study primarily focuses on understanding the reasons behind the better alignment between language model representations and brain recordings. The exploration of various simple linguistic and non-contextual features, and their contribution to explaining the variance in brain predictivity over contextual embeddings, is valuable for the research community.
2. The authors tested different validation setups, including comparisons between untrained versus trained models and shuffled versus unshuffled data, to evaluate brain scores. 
3. Controlling features with different combinations provided valuable insights into the contribution of each feature to the performance of brain alignment.

Weaknesses:
1. While the main research question aims to investigate the simplest set of features that account for the greatest portion of the mapping between LLMs and brain activity, the insights remain unclear for specific language regions of the brain. For instance, considering language parcels based on the Fedorenko lab, do simple features explain all the variance in these language regions? Or do these features only account for early sensory processing regions?
2. It is a well-known fact that Transformer-based representations consist of both low-level and high-level abstract features. If embeddings from language models predict brain activity and this predictivity is only due to a simple set of features, it should be better interpreted using approaches like residual analysis (Toneva et al. 2022), variance partitioning (Deniz et al. 2019), or indirect methods as suggested by Schrimpf et al. (2021).
3. Shuffling train-test splits is not an ideal scenario for brain encoding, especially for continuous language. All prior studies follow unshuffled train-test splits, i.e., contiguous time points (TRs). Shuffling the train-test split can result in sentences from the same passage being present in both the train and test sets, which is not ideal for model validation.
4. What are the implications of this study for both the AI and Neuroscience communities? What are the final conclusions?

Limitations:
Yes, the authors have presented several limitations in the conclusion. However, these limitations do not have any societal impacts on this work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the similarity between large language models (LLMs) and human brain activity by analyzing brain scores, which measure how well a model predicts neural signals. The authors question the validity of using brain scores as a measure of similarity between LLMs and human cognitive processes. They analyze three neural datasets and find that simple features like sentence length and position explain much of the neural variance that LLMs account for. They caution against over-reliance on brain scores and emphasize the need for a detailed deconstruction of what LLMs are mapping to in neural signals.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The study provides a thorough examination of how various features (simple and complex) contribute to the neural predictivity of LLMs, offering a detailed deconstruction of the relationship between LLMs and brain activity. The replication of key findings using RoBERTa-Large, in addition to GPT2-XL, strengthens the validity of the conclusions drawn regarding the generalizability of the results across different LLM architectures

Weaknesses:
1.  The methodology and findings are not particularly novel. Previous studies have already suggested that untrained LLMs can achieve good brain scores and that sentence length and position are significant predictors. Thus, two of the three core contributions claimed by the authors are not unique to this paper.
2. While the authors conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, it is not clear how this conclusion is drawn from the experimental results. The study itself relies heavily on brain scores to make its arguments, and the authors do not explicitly state what aspects of previous work have been over-interpreted.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
tRjgapiCpm;"REVIEW 
Summary:
The paper presents a heuristic approach for evaluating the privacy of DP-SGD when only the last model iteration is released. This method contrasts with traditional analyses that consider all intermediate updates, offering a more practical assessment for scenarios where adversaries only access the final model. The proposed heuristic is experimentally shown to provide reliable privacy leakage estimates, making it a valuable tool for pre-audit privacy assessments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Focus on a good and important question.
2. Good explanation and clear paper layout.
3. Propose a new analysis neither from the theoretical nor empirical point of view.

Weaknesses:
I think the proposed method is interesting and new but I still have some questions.

1. I know the linear loss function assumption is common in theoretical analysis but it seems that the proposed method wants to have contributions in the empirical case, so why still make the linear assumption? 

2. While I appreciate the effort to introduce a Heuristic analysis, I remain skeptical about its necessity and effectiveness. The primary benefit of theoretical analysis is its precision and rigor, which often include the flexibility to adjust bounds as needed. If the goal is to find a more relaxed lower bound on privacy risks, this can often be achieved by simply loosening the constraints within the existing theoretical framework. Introducing a separate heuristic analysis seems to complicate matters without providing clear advantages. 

3. I do not think you are using a correct baseline. When you make that only the last iteration model can be seen assumption, it is not fair to use normal DP-SGD analysis. I think it is better to use the theoretical analysis from those hidden state papers you cited. I am curious if you compare your proposed method with those methods, will you still get the same conclusion? 

4. I find Table 1 in the paper somewhat unclear and would appreciate further explanation from the authors regarding its purpose and implications. The table suggests that similar levels of heuristic ε are achieved across varying batch sizes, yet there is a noticeable increase in the standard privacy budget for smaller batches to maintain comparable performance. This observation seems to underscore the well-known impact of batch size rather than demonstrating an advantage of the proposed heuristic method.
Could the authors elaborate on how this data relates to the efficacy of the heuristic analysis?

Limitations:
Please check the weaknesses.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a heuristic privacy analysis of releasing only the final model iterate of differentailly private gradient descent (DP-SGD). The analysis is based off of the worst-case differential privacy guarantee of DP-SGD with linear losses, under the assumption that the heuristic can be applied to more general loss functions in order to approximate the privacy loss.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* The premise of the paper (a heuristic privacy analysis of releasing only the final model iterate of DP-SGD) is very interesting, and Theorem 1 is a cool result.
* The paper thoroughly assesses the limitations of the heuristic (in Section 4).

Weaknesses:
* I don’t know how useful the heuristic analysis would be in practice — beyond a lightweight sanity check — since ultimately it is just a heuristic and not a rigorous upper or lower bound on the privacy loss.

* The empirical study of the heuristic looks to be very thorough, but sparse on interpretation. I would have appreciated more discussion on the figures, and didn’t really feel like there was a strong take-home message from the paper.

* Algorithm 1 is DP-SGD with a regularizer, but in practice it is somewhat rare to use explicit regularization with DP-SGD. So I’m not sure that the heuristic would be widely applicable to the more common implementation of DP-SGD without regularization.

Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a heuristic privacy analysis for DP-SGD that focuses on releasing only the last iterate, as opposed to all intermediate iterates. The authors argue that this approach is more realistic and provides sharper privacy guarantees in practical scenarios. The heuristic is based on a linear structure assumption for the model and is validated experimentally through attacks/privacy auditing.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper is well-written. The paper introduces a new heuristic analysis of DP-SGD for linear loss functions and also critically examines its limitations, and identifies areas for further research.

Weaknesses:
To my understanding, this paper offers a tighter privacy accounting analysis specifically for linear loss functions. However, I find its applicability limited since it cannot be extended to general ML tasks where the loss functions are not linear. Additionally, the fact that the privacy adversary has access to all intermediate iterates of the training process makes DP-SGD overly conservative is quite well-known. The main challenge remains in developing tight privacy accounting analyses for iterative algorithms like SGD.

Limitations:
The limitations are adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors provide exact DP guarantees for cases where only the last iterate of DP-SGD is shared with the malicious clients, and linear models with linear loss functions are used. They propose their DP bound to be used as a heuristic that approximates the true DP guarantees for cases where more complex models are used. They show that for normal DP-SGD training, the predictions of their heuristic fall between the standard DP bound computed under the assumption that all intermediate iterations of DP-SGD are shared with the attacker, which is a strict upper bound of the true DP guarantee when only the last iterate is shared and DP-SGD with full batches and only last iterate sharing. They also compare their method against SoTA DP attacks and show that under most circumstances, their heuristic value for the DP is higher. They suggest that this is the result of the attacks not being good enough at precisely estimating the true DP guarantees. Finally, the authors demonstrate that their heuristic under unrealistic circumstances can underestimate the true DP guarantee but argue this only happens under hand-crafted losses and gradient updates, which do not happen in practical circumstances.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The last iterate setting is important
- The linear function DP bound is exact 
- The linear function DP bound has interesting properties 
- The counter-examples for the DP heuristic themselves seem interesting and probably can be adapted to other settings

Weaknesses:
- I am confused by L234. The authors propose to maximize their heuristic over all $t\leq T$, while beforehand (e.g. in Figure 1/Section 2) they advocated to computing the heuristic for a single $T$. Which one is the exact proposed heuristic by the paper?
- In Figure 2, I am not sure how we adapt existing techniques to the last-iterate-only setting? Can the authors explain in more details?
- Can the authors explain in Figure 1, what network and dataset were used?
- The authors do not provide code. I am not sure about the reason, but I will give them the benefit of the doubt that the reason is indeed related to anonymity 

**Nits:**  
- Eq. 8. I assume you do indexing from i = 1. In that case, $A_{T-i}$ should be $A_{T-i+1}$ instead. If you do 0-based indexing, even more fixes to the equation are needed.
- I believe Eq. 7 should be multiplied by $\eta$ on the right-hand side
- Equation at L442, left-hand side should be $m_T$ not $m_t$
- I believe the last equation at L459 should have $(1-q)^{n-k}$ instead of $(1-q)^{k}$. I also believe $n$ is $T$ in this equation
- The definition of $l(m)$ in L109 is confusing as $m$ is considered input to the function, while in the rest of the paper $m$ is used as a parameter. Consider putting $x$ instead.
- Consider defining the hockey-stick divergence in terms of both its pdf and cdf in the appendix to ease unfamiliar readers. I had to read quite a bit on my own to understand it. 
- Consider adding some information in the appendix as to how to deal with the mixed discrete-continuous probability for $P$. I assume many readers will be unfamiliar. 
- Consider deriving the formulas for $P$ and $Q$ in Section 4.2 in the appendix. They are not obvious. 
- Consider having an appendix section that quickly recaps how [NSTPC21] and [NHSBTJCT23] work. Their operation is critical for understanding Section 3. I ended up reading them to get an idea of what was going on there.

Limitations:
The authors acknowledge the limitations of using the heuristic to compute the DP bounds

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
tVO3b68Oyp;"REVIEW 
Summary:
This paper proposes a two-stage speech language model with semantic tokens and acoustic tokens similar to AudioLM ([Borsos et al., 2022]).
-   The semantic tokens come from a speech tokenizer that can group a variable number of frames into a single token. To train such a speech tokenizer,
    1.  This paper first takes inspirations from syllable-like structures uncovered from HuBERT, and produces an initial segmentation (Section 3.1).
    1.  An iterative process is then applied to improve the segmentation (Section 3.2).
    1.  Finally the tokens are obtained by clustering of the mean-pooled frame features (Section 4.1).
-    The acoustic tokens are identical to the HuBERT-based tokens in ([Hassid et al., 2023]), referred to as ""mHuBERT"" in this paper.

Experiments with the proposed model demonstrate the following when compared to previous work,
-   Better unsupervised syllable segmentation
-   Lower speech reconstruction WER
-   Better or competitive accuracy in speech language modelling tasks (sWUGGY, sBLIMP, tStoryCloze) with lower compute
-   Better speech continuation quality

[Borsos et al., 2022]: https://arxiv.org/pdf/2209.03143 ""AudioLM: a Language Modeling Approach to Audio Generation""
[Hassid et al., 2023]: https://proceedings.neurips.cc/paper_files/paper/2023/file/c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf ""Textually Pretrained Speech Language Models""

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
-   Originality: This paper proposes an original method for producing syllable-like segmentation of speech in an unsupervised manner.
    -   It uses conditional probabilities from a masked language model instead of feature similarity ([Peng et al., 2023]) to detect initial syllable boundaries.
    -   The use of an iterative process to further improve the segmentation quality is also original.
-   Quality: This paper is well-motivated. The experiment design is sound. Ablation studies included in the experiments provide valuable insight to various modelling choices.
-   Clarity: The experiment results are reported in an easy-to-interpret manner.
-   Significance: The proposed model is an competitive speech language model with a lower inference computational cost.

[Peng et al., 2023]: https://arxiv.org/pdf/2305.11435 ""Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model""

Weaknesses:
I think the method and results in this paper would make a good paper for NeurIPS, however I cannot make a recommendation for acceptance because this paper needs substantial revision to improve its readability. A non-exhaustive list of issues making the paper hard to follow includes the following,
-   References to items not yet introduced
    -   Lines 153-154, the phrase ""our loss"" make it sound like a referrence to the masked language model loss discussed in the previous sub-section, whereas in fact it is referring to Equation (3), a yet-to-be-introduced loss for SylBoost.
    -   Lines 159-162 give a very vague description of the ""similarity matrix"" and the ""cut algorithm"" which can only be known if the reader has already seen the subsequent Section 3.3.
    -   Starting at line 188, Section 4.2 makes repeated references to ""mHuBERT"". ""mHuBERT"" appears to be name given to the acoustic tokens in ([Hassid et al., 2023]) by this paper (line 241). ([Hassid et al., 2023]) itself does not use this name, so an ordinary reader would not be able to tell what an ""mHuBERT"" model is when they work through Section 4.2.
-   Confusing terminology
    -   ""pretraining"": This paper makes a liberal use of the term ""pretraining"" to the point it's very difficult to tell which is the model being ""pretrained"". For example,
        -   Line 113 mentions a ""pretrained HuBERT teacher model"", then line 119 says ""during pretraining, the **student** model ..."". The teacher and the student are presumably not trained at the same time, yet the use of ""pretraining"" in this context make it appear that the contrary is happening.
        -   Line 225 says ""for all pretraining experiments"". A reader will have to look really closely to see this means ""training of the speech LM"", not ""pretraining HuBERT, etc"".
    -   ""Agglomeration"" vs ""SylBoost"": This paper appears to use these two terms interchangeably. Agglomerative clustering is apparently also used  (line 183). This makes it difficult for the reader to tell when ""agglomeration"" is mentioned, whether the authors intend to refer to SylBoost or just the clustering.
-   Confusing equation
    -   The unnumbered equation between line 126 and line 127 defines the similarity matrix from MLM probabilities. It makes reference to
$Y_t$ without specifying which $t \in M$ is used to define $C_{r,c}$. As a result, after having read the paper 6 times over, I still do not know how to compute $C_{r,c}$.
-   Writing style
    -   Overall the writing style of this paper is very wordy, inconcise and disorganized. Often the same message can get through with far shorter sentences. Most of the paragraphs read like a dump of the stream of consciousness of the author instead of a technical document intended for actual readers. For example,
        -   Lines 102-112 would be a lot easier to understand with formal notations and a concrete example.
        -   Lines 127-131 appear to be a mere repetition of the equation above, without any new information.
        -   Lines 242-255 contain a large amount of disorganized modelling details.

[Hassid et al., 2023]: https://proceedings.neurips.cc/paper_files/paper/2023/file/c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf ""Textually Pretrained Speech Language Models""

Limitations:
The authors adequately addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper first introduces an algorithm named LossPred that generates syllable-level speech segmentation without any training or supervision. The algorithm works by analyzing the prediction loss of speech tokens under different mask positions.

With the initial boundaries proposed by LossPred, the paper proposes further training a pretrained HuBERT / data2vec2 model by minimizing the sum of squared distances between feature vectors of each token and the average of feature vectors within the corresponding segment. This process is called SylBoost, and it further improves syllabic segmentation performance and efficiency.

Finally, the paper proposes training a Generative Spoken Language Model (GSLM) with the speech tokens obtained from quantized SylBoost units. Compared to existing GSLMs trained on other discrete representations, SylBoost encodes speech into much shorter sequences, significantly boosting training and inference efficiency.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The proposed speech representation learning and unit discovery algorithms, LossPred and SylBoost, are novel. While the idea of improving computational efficiency through dynamic or fixed-rate downsampling of speech representation is not new, this paper appears to be the first to successfully apply dynamic-rate downsampled representations with a very low sampling rate of 5Hz to Generative Spoken Language Models (GSLMs).
2. The presentation of the paper is of high quality and clarity. The authors report extensive experimental results, which effectively demonstrate that the proposed method outperforms various state-of-the-art (SotA) methods.
3. The topic addressed in this paper is significant, as very low sampling rate speech representations can benefit various tasks, including speech understanding and generation.

Weaknesses:
1. As pointed out by the authors, the proposed LossPred and SylBoost methods seem to be restricted to speech representation learning. It might be difficult to apply these methods to music, singing voice, speech with noisy backgrounds.
2. LossPred is slow in evaluating the loss prediction matrix. Each sentence requires about 200 Transformer network evaluations.
3. LossPred is highly heuristic. There seems to be no theoretical guarantee that the HuBERT model combined with LossPred reveals syllabic boundaries instead of revealing only phoneme or word boundaries.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies learning low bitrate speech units that preserves semantic information. As presented in the paper, the proposed approaches achieve SoTA performance on tasks like ASR and ZeroSpeech. The proposed approach also shows benefits in terms of compute resources — as claimed by the authors, 30x faster to train, and also benefits in terms of inference and transmission due to low bitrate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the proposed multistage approach — first using the HuBERT like model to extract syllable-like noisy segmentation, then bootstrapping pseudo-syllabic units iteratively makes sense to me. The proposed approach also shows clear benefits in terms of performance and efficiency. 

Good performance: Compared to baseline approaches like SD-HuBERT, the proposed method achieved higher accuracy on syllable boundary detection and clustering, ASR, and also shows better continuation metrics as shown in Table 7 for generative spoken language modeling experiments. All those evaluations all positively demonstrate the strong associations with syllables of the generated speech units, while it does show lier-bitrate compared to the baselines compared in the paper.
The authors also conducted ablation studies to further demonstrate a couple design choices.



Efficiency: As claimed in the paper, the proposed technique is capable of achieving extremely low-bitrate compared to the counterpart speech units, while still being able to achieve good performance in a wide range of tasks, with the efficiency in both training and inference phases.

Weaknesses:
Demonstrating efficiency: As efficiency is also one selling point of the paper, it would be great if the authors can demonstrate the training efficiency and low-bitrate benefits in a more comprehensive way, like visualizing the GPU training time vs Performance, and also bitrate vs unit quality for certain tasks.



Limited use cases: The proposed approach focuses on learning semantic units for speech applications. It’s unclear if the proposed methods can be applied to other important non-speech use cases like understanding acoustic environment, and understanding speaker’s identity and emotion. 



Understanding Unit Quality: To demonstrate the unit quality for synthesizing the audio and for generation, should the author also compare with other related works (like [1] and [2]) in terms of reconstructing the original signal? Like in [1] (see Table 1), the authors compare the different approaches in terms of reconstruction performance using a couple of metrics like MEL, STFT and ViSQOL score, and also semantic task performance.

[1]: https://arxiv.org/abs/2405.00233
[2]: https://arxiv.org/abs/2306.06546

Limitations:
Not aware of potential negative societal impacts

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an approach for extracting syllable-like units from speech SSL models for use in a transformer-based language model. The motivation is that, compared to baseline acoustic units, which tend to mimic phonetic units in their time resolution, syllable-like units have lower time resolution, which makes them easier to model using techniques from the language domain. The authors propose an adaptation of the SD-HUBERT approach to extract units that can be used in Generative Spoken Language Modeling.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors identify an important limitation of why using language modeling techniques is a challenge in the speech domain, and their proposed approach seeks to address the limitation.

Weaknesses:
Overall, I found the submission difficult to follow. Please see my additional comments below.

line 2 -> Transformers do not require the inputs to be tokenized. The tokenization step is performed so that we can use language modeling techniques in speech.

line 18 -> Generally speaking, there is no requirement for the SSL representations to be powerful or abstract. 

Line 20 -> I don't see how the example of young children motivates your SSL description from the previous sentence; the transition is incoherent.

line 22 -> What does performant mean in this case? What is the connection between composing highly realistic text and the ability of a model to provide features for a downstream task? You seem to conflate the two goals, even though they are not necessarily the same.

line 23 -> The statement on this line is not clear. Several successful speech language model methods were introduced in the literature, what about previous approaches that make them fail? Please consider clarifying.

line 31 -> The temporal resolution impacts the LM part of the problem. Why is it important if we want to extract features for a downstream task?

line 37 -> What does ""syllable-like"" mean in this case? Can you elaborate on the time resolution it represents? Why is it important to start with a ""syllable-like"" unit? What makes it suitable for GSLM? What challenges from prior work are you addressing when using ""syllable-like"" units?

Line 38 -> I would refrain from using words like ""breakthrough"" and instead let the reader decide if the improvement is indeed a ""breakthrough.""

line 48 -> I disagree with labeling your method as ""train-free"" since it relies on a pre-trained HuBERT model. 

line 51 -> The distinction between the first and second contributions needs to be clarified. If the boundaries from the first contributions are not good on their own, then why mention them as a contribution? 

Line 102 -> It is not clear how/where you do the masking. Do you do it on the raw input, mel-spectrogram, or the extracted features? 

line 113 -> Shouldn't the approach be ""train-free""? Why do we have a student/teacher model that we are training?

line 147 -> The authors must refine the motivation for why syllabic units are useful for this application. Why not use word units instead? 

line 189 -> Superior compared to what?

line 198 -> I suggest leaving any experimental details to the experiments sections.

Table 1 -> Can you try any non-neural baselines for boundary detection? What would the performance be if we used heuristics based on energy, zero-crossing rate, or changes in Prosody to get rough boundaries?

Table 1 -> What makes Data2Vec2 better than HuBERT for extracting boundaries?

Table 1 -> What happens if you apply SylBoost to Feat-Sim?

Table 1 -> Please describe the metrics and abbreviations in the captions.

Table 2 -> What does the underline represent?

line 221 -> Implement what exactly? Please re-write the sentence.

line 237 -> typo

Table 3 -> What does the underline represent?

*Estimated.-> What does estimated mean? If prior work does not explicitly give this information, then it is better to leave it out.

line 263 -> What is the R score?

line 281 -> Please present the tables in the order they are referenced in the text; you currently jump from Table 1 to Table 4 and then go back to Tables 2 and 3.

line 340 -> Communication is not the last name of the first author from [16]

Limitations:
Can the authors comment on the trade-off between resolution and ease of modeling (and quality)? What do we lose/gain using syllable-like speech units in a language modeling paradigm?

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
t8ch1OCvHh;"REVIEW 
Summary:
This paper pays attention to extremely large outliers in LLMs and further investigates the reasons behind these ""attention spikes."" Consequently, the authors propose two methods to enhance the performance of quantized models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The analysis of attention spikes is thorough and comprehensive.

2. The exploration of the relationship between attention spikes and Gated Linear Units (GLU) variants is both interesting and insightful.

Weaknesses:
1. The proposed QFeM method is not hardware-friendly, as it maintains some modules at high precision and cannot directly utilize low-bit INT General Matrix Multiply (GEMM) for activations and weights.

2. The proposed QFeP method bears a strong resemblance to a previously researched method, IntactKV[1], yet lacks a detailed comparative discussion.

3. The experimental settings are limited to W8A8 configurations, which previous research, such as SmoothQuant[2], has shown can nearly achieve lossless quantization for W8A8 models.

4. The authors have not included comparisons with state-of-the-art baselines, such as OmniQuant[3], AffineQuant[4], QLLM[5], and QuaRot[6].


[1]. Liu, Ruikang, et al. ""IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact."" arXiv preprint arXiv:2403.01241 (2024).

[2]. Xiao, Guangxuan, et al. ""Smoothquant: Accurate and efficient post-training quantization for large language models."" International Conference on Machine Learning. PMLR, 2023.

[3]. Shao, Wenqi, et al. ""Omniquant: Omnidirectionally calibrated quantization for large language models."" arXiv preprint arXiv:2308.13137 (2023).

[4]. Ma, Yuexiao, et al. ""Affinequant: Affine transformation quantization for large language models."" arXiv preprint arXiv:2403.12544 (2024).

[5]. Liu, Jing, et al. ""Qllm: Accurate and efficient low-bitwidth quantization for large language models."" arXiv preprint arXiv:2310.08041 (2023).

[6]. Ashkboos, Saleh, et al. ""Quarot: Outlier-free 4-bit inference in rotated llms."" arXiv preprint arXiv:2404.00456 (2024).

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper identifies some of the underlying causes for why activation quantization (PTQ) could lead to low performance and suggests some methods to address these issues.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
Please see the “Questions” section.

Weaknesses:
Please see the “Questions” section.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the precision challenges posed by the large language models (LLMs) quantization during inference, specifically focusing on the quantization errors in GLU-based feedforward networks. The authors identify that GLU variants in LLMs cause significant local quantization errors due to excessive activation magnitudes, referred to as activation spikes. They observe that GLU-implemented models have larger spikes than non-GLU-implemented models.  They propose two methods, Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP), to isolate and mitigate these spikes during quantization. QFeM leave some linear layers unquantized (usually those layers that cause large activation spikes in the first several layers), and QFeP introduce an additional prefix before the inference process. Their extensive experiments show that these methods improve quantization performance and are compatible with existing techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The identification of activation spikes in GLU-based LLMs is novel.
2. The paper is well-structured and clear.
3. The QFeP method is novel,  and is somehow similar to the finding of ""sink token"" in StreamLLM [1].

[1] Xiao, Guangxuan, et al. ""Efficient streaming language models with attention sinks."" arXiv preprint arXiv:2309.17453 (2023).

Weaknesses:
1. My major concern is about the baseline of SmoothQuant reported in Table 4. For example, In Table 7 of SmoothQuant's original paper, they report that W8A8 SQ's PPL of Llama-7B on WikiText-2 dataset is 5.515, while the authors report a PPL of 9.907 on the same dataset. Is there a specific reason about this large gap?

2. In Table 3, the improvement brought by the QFeP method does not seem significant, especially when combining with the QFeM method.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces activation quantization methods for GLU-based LLMs, which often face challenges due to activation spikes. To effectively manage these spikes and enable activation quantization using a PTQ-based approach, the paper proposes a Quantization-free Module (QFeM) and a Quantization-free Prefix (QFeP). Specifically, QFeM aims to partially bypass quantization for linear layers where large quantization errors occur. QFeP identifies the prefix that triggers activation spikes and preserves its context as a key-value (KV) cache, preventing the recurrence of activation spikes in subsequent tokens. The paper presents extensive experimental results to compare the accuracy of the quantized models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1) This paper is well organized and easy to understand.
2) The proposed QFeM and QFeP effectively mitigate the impact of activation spikes on activation quantization, preserving the accuracy of LLMs even when activation quantization is applied.
3) The ablation study thoroughly examines the effects of QFeM and QFeP, providing valuable insights.

Weaknesses:
1) The perplexity/accuracy results of the baseline methods deviate from the results reported in previous papers.

2) The paper does not compare its method with the state-of-the-art LLM quantization method [1], which enables W4A4 quantization (partially using 8-bit operations) with a PTQ approach.

[1] Zhao, Yilong, et al. ""Atom: Low-bit quantization for efficient and accurate llm serving."" Proceedings of Machine Learning and Systems 6 (2024): 196-209.

Limitations:
The proposed method is limited to GLU-based LLMs.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
sxZlp9ZoHD;"REVIEW 
Summary:
The paper proposes the Retentive Network (RetNet) as a foundation architecture for large language models. RetNet has a multi-scale retention mechanism with three computation paradigms: parallel, recurrent, and chunkwise recurrent. 
The retention mechanism starts with a recurrent modeling formulation and derives a parallel formulation. It maps input vectors to state vectors recurrently and implements a linear transform to encode sequence information. Then, it makes the projection content-aware by using learnable matrices. The retention layer is defined using these matrices and a complex position embedding, combining causal masking and exponential decay along relative distance. 
It achieves low-cost inference, efficient long-sequence modeling, comparable performance to Transformers, and parallel training. Experimental results show its superiority in language modeling, inference cost, and training throughput.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The RetNet also shows competitive performance in language modeling and knowledge-intensive tasks compared to other Transformer variants and has the potential to replace Transformers for large language models.
2. Achieves significantly better inference efficiency in terms of memory, speed, and latency.

Weaknesses:
1. The paper presents the scaling curves of RetNet and Transformer with model sizes ranging from 1.3B to 6.7B, concluding that RetNet is favorable in terms of size scaling and starts to outperform Transformer when the model size is larger than 2B. However, it does not provide a detailed explanation for this trend. Understanding the underlying reasons for this performance difference with increasing model size could provide more insights into the effectiveness of RetNet and its potential advantages over Transformer.
2. The use of $\gamma$ in the RetNet may appear somewhat heuristic. The paper assigns different $\gamma$ for each head in the multi-scale retention (MSR) module and keeps them fixed among different layers. While this approach is used to achieve certain effects, such as enhancing the non-linearity of the retention layers and improving the model's performance, the specific rationale for choosing these values and the potential impact on the model's behavior could be further explained.

Limitations:
none

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a linear attention model called RetNet for language modeling, which has a linear training complexity and constant inference complexity.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. RetNet has both linear time complexity and constant inference memory complexity. 
2. RetNet has a chunk recurrent form which can be beneficial for speculative decoding.

Weaknesses:
1. The authors introduce a new term called ""Retention,"" but this is essentially the same as Linear Attention without the denominator, which has already been proposed in [1].
2. Lack of comparison with the baselines on open source pretraining data. All the training experiments are conducted on in-house data mixtures, which harms the reproducibility.
3. The paper doesn't compare RetNet with other linear attention model (such as GLA, RWKV, Mamba) on downstream tasks with standard metrics instead of perplexity. Table 2 only include RetNet and Transformer. The efficiency measurment of RetNet+ is absent.
4. The evaluation on MMLU/Qasper is using perplexity but not the widely-used accuracy/F1 metric. The perplexity results don't necessarily mean that the model can make correct choices for the samples in MMLU, and has less guidance for the model's downstream performance.
5. Missing citations: The authors should also cite [1] for the normalization after retention, and discuss the details of the triton implementation of RetNet and its difference from the implementation in the Flash Linear Attention [2] library.

[1] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025–7041, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics.

[2] Yang, Songlin and Zhang, Yu. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism. https://github.com/sustcsonglin/flash-linear-attention

Limitations:
No, the authors should have a limitation section to point out the strong assumptions of their approximation of self-attention and relative position embedding.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents Retentive Network (RetNet), a family of efficient models that incorporate exponential decay within a linear attention-like structure. RetNet shares similarities with state-space models and linearized attention, enabling both training parallelism and O(1) inference cost. Additionally, RetNet supports chunk-wise parallel computation for efficient long-sequence training. Experimental results demonstrate RetNet achieves performance comparable to Transformers and outperforms other efficient variants on language modeling and vision tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The structure of RetNet is easy to understand and follow
- RetNet exhibits promising training and inference efficiency, and is able to scale up to 6B.
- Comprehensive evaluation on both language and vision tasks, highlighting its generalizability.

Weaknesses:
- Some experiments could be improved
- Some claims may be misleading
- RetNet's performance lags behind Transformers at smaller model scales, suggesting it might be more demanding in terms of capacity and compute resources for optimal performance. This trade-off should be carefully considered and analyzed.

Limitations:
I didn't see serious problems.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
s5Y3M5l1qg;"REVIEW 
Summary:
To better defend against adversarial attacks, the paper proposes a novel adversarial defense mechanism for image classification – CARSO – blending the paradigms of adversarial training and adversarial purification in a synergistic robustness-enhancing way.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper proposes a novel defense mechanism.

The proposed method is validated on multiple datasets.

Weaknesses:
1. The presentation of the paper is poor.

    a) In the first half of the paper, the author merely describes some background. There is a lack of analysis of existing methods, such as the shortcomings of the current methods, what problems the proposed method can solve, and why it can solve these problems.

    b) Some descriptions are unclear, such as 'Upon completion of the training process, the encoder network may be discarded as it will not be used for inference.' I think 'may' should be removed here.

2. The current experiments are insufficient to prove the effectiveness of the proposed method.

    a) Table 2 simplifies a lot of information, which reduces clarity; for example, it only records the mean or best results of multiple methods and lacks the clean accuracy of the purification method. I suggest listing all methods according to both clean accuracy and adversarial accuracy. The existing content in Table 2 can be added as additional row information.

    b) Since the paper does not give specific problems, only a general goal, which is to better defend against adversarial attacks, the experiments become relatively limited. I believe the author should re-summarize the shortcomings of existing methods and the advantages of the proposed method and conduct more experimental comparisons.

Limitations:
The authors have discussed limitations of the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes a novel adversarial defense method called CARSO. CARSO consists of two models: a classifier and a purifier. The classifier is (pre)trained to correctly classify possibly perturbed data. The encoder of the purifier is trained to generate a latent space from the internal representation of the classifier and the original (possibly perturbed) input. The decoder of the purifier is trained to reconstruct a sample from the latent representation and the internal representation of the classifier. The final prediction is determined by aggregating the outputs of the classifier for reconstructed data.

Detailed procedures are summarized as follows:

- The classifier is always kept frozen. Other parts, including the VAE and small CNNs for compression, are trained on a VAE loss consisting of a reconstruction loss based on a pixel-wise channel-wise binary cross-entropy loss and KL-div.
- The internal representation and input are compressed by small CNNs before being inputted into the encoder of the purifier.
- The classifier is pretrained according to [18] or [62].
- When training the purifier, each batch contains both clean and adversarial samples.
- The aggregation is represented by a double exponential function.
- Evaluations are conducted under $L_\infty$ attacks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The concept of blending adversarial training and purification is novel and interesting. The proposed method, CARSO, achieves robust accuracy that surpasses the SOTA adversarially trained models and purification methods, including diffusion-based models, despite its relatively simple mechanism.
- The evaluation was carefully conducted. The authors explicitly address common pitfalls in evaluating robustness. For example, they conducted end-to-end validation (full whitebox setting), addressed concerns about gradient obfuscation, and used PGD+EOT to address the stochasticity of CARSO.
- CARSO can utilize existing pretrained models, which have already achieved high robust accuracy.
- A wide variety of datasets (CIFAR-10, CIFAR-100, and TinyImageNet-200) were used for evaluation.

Weaknesses:
**1**. In my opinion, the claim that CARSO surpasses the used adversarially trained model seems questionable. If my understanding is correct, during inference, the decoder takes class information only from the internal representation of the classifier. Thus, I believe the decoder can correctly reconstruct the sample only if the classifier, outputting the internal representation, can correctly extract class information from the original perturbed sample. Could the authors clarify this?

Note: Initially, I doubted whether some experimental or evaluation settings were appropriate. However, as far as I can tell, there are no issues. Just in case, I recommend the authors review their source code again.

**2**. CARSO sacrifices clean accuracy more significantly than existing SOTA methods. Additionally, to compare CARSO and the best AT/purification models in terms of clean accuracy, Table 2 should include the clean accuracy of the best AT/purification models (i.e., the contents in Table 15). The scenario or dataset columns in Table 2 might not be necessary.

**3**. Few ablation studies. The authors should include the case of $L_2$ perturbations and use internal representations from different layers. Particularly, the relationship between the layers used for extracting representation and robust accuracy is of interest.

Limitations:
The authors explicitly addressed the limitations in Section 5.3.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper integrates adversarial training and adversarial purification to enhance robustness. It specifically maps the internal representation of potentially perturbed inputs onto a distribution of tentative reconstructions. These reconstructions are then aggregated by the adversarially-trained classifier to improve overall performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea of combining adversarial training and adversarial purification is interesting.

Weaknesses:
1, The experiments are too weak. I hope the authors can refer to at least [1][2][3], which are relevant to adversarial purification, to conduct experiments from more dimensions and consider more baselines and fundamental experiments.

2, Could we just combine [1] with an adversarially-trained model to achieve similar performance?

3, Why should the classifier be adversarially trained for better accuracy?

4, Why can't we directly purify the image? Could we use an image-to-image method to purify the input image?





[1] DISCO: Adversarial Defense with Local Implicit Functions.
[2] Diffusion Models for Adversarial Purification
[3] IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks

Limitations:
The method heavily relies on training a VAE as the generative purification model.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
rog0J435OO;"REVIEW 
Summary:
This paper proposes a novel method to address the high computational and memory complexity of current large-scale transformers. By adopting a simple yet effective column-wise sparse representation of attention masks, the algorithm achieves reduced memory and computational complexity while maintaining the accuracy of attention computation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper investigates a topic of interest, given the current trend toward increasing context lengths in LLMs.
2. The method proposed in this paper is straightforward and easy to implement.
3. The paper is well-written and clearly presented.

Weaknesses:
1. It is crucial to highlight the advantages of this method over related work to help readers fully understand its significance. However, in the subsection ""Attention Optimization Techniques,"" the authors only mention the drawback of FlashAttention and discuss its relationship to their work. The introduction of other related works is confusing and makes it difficult to comprehend their relevance to this paper. The overall conclusion, ""*Both of the previously discussed solutions either compromise precision or yield only marginal enhancements in efficiency. Conversely, our proposed FlashMask is capable of delivering exact computations.*"" is general and non-specific. It is unclear which methods compromise precision and which ones only offer marginal improvements.

2. In the experiments, the baseline algorithms are limited to Vanilla Attention and FlashAttention. Are there more efficient Transformer algorithms that could be used for comparison? If not, the authors should explain the rationale behind the selection of these specific baselines.

3. As a non-expert in this field, I found the writing of this paper confusing. For instance, the initialism ""HBN"" is introduced without any explanation or context.

Limitations:
none

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes FlashMask, which accelerates the masked attention mechanism that can reduce the original attention from O(N^2) to O(N) and simultaneously reduces the memory cost. Experimental results show that the proposed FlashMask significantly reduces training time without accuracy degradation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ This paper provides a comprehensive study and analysis about the sparse attention, in terms of their efficiency. Also, this paper includes existing attention optimization like FlashAttention, explaining the motivation of the proposed FlashMask, which lies in the lack of optimization for sparse attention.

+ This paper proposes an optimization for column-based sparse attention, which significantly improves memory efficiency and reduces computational costs.

+ This paper provides a comprehensive complexity analysis, evaluation, and comparison with existing methods. It seems the authors make a lot of efforts on the proposed approach.

Weaknesses:
- Even though FlashMask achieves significant improvement in the memory efficiency of sparse attention, the key idea is similar to FlashAttention, but it is just for sparse attention mechanisms. Based on this fact, the novelty of this paper is not strong. I recommend the authors explain why the red part in the algorithm is designed and why it is unique for sparse attention.

- The authors only present optimization for column-based sparse attention. The performance for other types of sparse attention is unknown. If the proposed approach can be applied to all sparse attention, the contribution of this paper is extremely great. However, the existing version is not comprehensive.

- Based on the experiments, the practical latency is not significantly reduced as compared to other methods, even though the theoretical complexity is from N^2 to N. Besides, the authors do not provide results for accuracy.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces FlashMask, an innovative algorithm designed to address the computational and memory challenges associated with conventional attention mechanisms in large-scale Transformers. FlashMask employs a column-wise sparse representation for attention masks, significantly reducing the computational complexity from quadratic to linear with respect to sequence length. The authors demonstrate FlashMask's effectiveness across various masking scenarios and training modalities, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM).

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper presents a novel solution to a well-known problem in the field of natural language processing, offering a practical method to reduce the computational burden of attention mechanisms in Transformers.

The paper provides extensive empirical evidence to support its claims, including comparisons with state-of-the-art techniques like FlashAttention, demonstrating FlashMask's superiority in terms of speed and efficiency.

FlashMask's performance across different masking scenarios and training modalities shows its versatility and robustness, indicating its potential applicability to a wide range of models and tasks.

Practical Impact: The paper not only presents theoretical advancements but also demonstrates practical benefits, such as enabling the

Weaknesses:
The scaling ability of the proposed method deserves further verified on large scale datasets.

While the paper demonstrates FlashMask's effectiveness in specific scenarios, it may lack broader evidence on how it performs across different types of NLP tasks or diverse datasets.

The paper could provide more detailed insights into how FlashMask handles different sparsity levels and the impact on various model sizes and complexities.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes FlashMask, a modification of FlashAttention with fixed masks. The paper shows speedup of FlashAttention when using sparse masks in the attention matrix.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
FlashAttention is an important algorithm, and sparsity in the attention matrix is an important feature. Further study of these aspects is helpful.

Weaknesses:
The paper seems to make claims that are unsubstantiated by experiments. In the abstract and introduction, the paper claims speedup without sacrificing model quality. However, there is no experiment evaluating model quality in the experiments. This is a critical flaw.

Further, the contribution of the paper is unclear. Block-sparsity is already supported in FlashAttention (see section 3.3 of FlashAttention). It is unclear how this paper is different. There are also more recent works such as ""Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention"" (NeurIPS 2023), which seem to be strictly more expressive in features than this paper.

Limitations:
The paper discusses superlinear scaling in sequence length as a limitation, but is lacking in discussion of model quality.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
rcch4UsMBi;"REVIEW 
Summary:
## Overall summary
- This paper introduces GLAN, a method for enhancing LLMs by generating synthetic instruction data using a taxonomy of human knowledge and capabilities. GLAN constructs this taxonomy by decomposing knowledge into fields and disciplines, leveraging LLMs for generating a comprehensive syllabus for each subject. 
- GLAN’s scalable and customizable framework allows for easy integration of new fields of skills, highlighting its potential for ongoing improvement and adaptation.

## My opinion of the paper
- I think this is a really interesting approach to generate data that can allow LLMs to be potentially smarter. However, I am wondering if there are newer topics, for example (within the medical area, we have the new topic called ""Covid-19"".) Since GLAN is very dependent on LLMs, the main area of concern would be ensuring that the LLMs that GLAN depends on remains updated.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
## Originality
- The approach is quite interesting. The authors made use of real life scenarios, which is to use the structure of human education systems to build the taxonomy. This approach mimics the systematic acquisition of knowledge and skills in education, providing a framework for generating instruction data.
## Clarity
- Pseudo Algorithm provided and figures are easy to understand.
## Significance
- By creating a general and scalable method for instruction tuning, GLAN has the potential to improve the performance of LLMs across a wide range of tasks and domains.

Weaknesses:
## Quality
- While the paper claims scalability, there is limited discussion on the computational resources required for generating the synthetic data at scale. Practical constraints related to computational costs and time could be a potential weakness. It was mentioned in the checklist that it is very computationally expensive to repeat experiments.

Limitations:
Indicated in the appendix (do consider placing it in main paper), but did not mention about computation cost like what was mentioned in the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces generalized isntruction tuning (GLAN), an approach for synthesizing instruction tuning data using a taxonomy-based approach. GLAN generates synthetic instruction data from pre-curated taxonomy of human knowledge and capabilities and aims to create diverse and broad-ranging instruction dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Comprehensive Coverage of Evaluation: The paper presents extensive experiments demonstrating that GLAN outperforms various popular instruction-tuned LLMs across multiple dimensions, including mathematical reasoning, coding, logical reasoning, and general instruction following.
2. Minimization of Human Involvement: The generation process significantly reduces human involvement, requiring human verification only at the taxonomy construction stage. This makes the approach scalable and less labor-intensive.
3. Customizability and Extensibility: The taxonomy-based approach allows for easy customization and extension. New fields or skills can be incorporated by simply adding new nodes to the taxonomy.

Weaknesses:
1. While the paper addresses generalization, there is a risk that the generated synthetic data might overfit to the taxonomy's structure, potentially missing out on more nuanced, real-world instructions.

Limitations:
The authors have adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GLAN, a general and scalable method for instruction tuning of Large Language Models (LLMs). GLAN employs a top-down approach to generate high-quality instruction tuning datasets. Experiments across various benchmarks demonstrate that GLAN performs comparably to other existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper focuses on the alignment of Large Language Models, which is a trendy and important topic. If the dataset is released, it will be beneficial for the community.
2. This method is easy to follow. The process is highly scalable, leveraging LLMs like GPT-4 for generating instructions on a massive scale.
GLAN allows for easy customization. New fields can be added by incorporating new nodes into the taxonomy.

Weaknesses:
The novelty is limited as similar top-down designs have been utilized in many previous works. Besides, the main experimental results in Table 1 appear mediocre compared to other methods.

Limitations:
Refer to the weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a generalized way of creating instruction data. The high-level motivation is to take inspiration from how curriculum is designed for human learning into a taxonomy of subjects and use the same to prompt an off-the-shelf LLM to create data. GLAN does not need seed examples, or pre built-taxonomy like prior work. Human verification is also performed post the building of taxonomy to weed out unimportant or inaccurate divisions. The overall process is High level taxonomy -> subjects -> syllabus -> instructions.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Overall strong Performance: Extensive experiments show GLAN's effectiveness in various tasks, outperforming or matching state-of-the-art models in several benchmarks (Table 1)
2. Figure 2 on scaling properties of GLAN: I found this figure quite interesting. It suggests a log linear scaling trend in performance as GLAN data is scaled up. This is quite promising.
3. Section 3.5 on Task-specific overfitting: Another great analysis section that discusses how GLAN does not particularly overfit to the training data. This ensures that the synthetic data remains generalizable across different domains.
4. Modularity of the pipeline: The modular nature of the GLAN pipeline allows for easy customization and extension by incorporating new nodes into the taxonomy without re-generating the entire dataset.

Weaknesses:
1. No use of actual human curriculum: The paper set the expectation right in the abstract of using/getting strongly inspired from human curriculum. I was disappointed that the method does not utilize existing human curriculum structures, potentially missing out on years of insights in developing the same. Generating synthetic data, and in this case entire taxonomies from pre-existsing models can lead to extremely large amounts of bias. I would have much rather seen the authors delegate only lower level questions to LLMs than high level abstractions, which would lead to a trickle down effect on every single node in the taxonomy. This study, in my opinion, is incomplete without using either human generated taxonomies, and/or a comparison between how different the taxonomies are.
2. Computation cost not compared: The paper does not provide a comparison of computational costs with similar methods, such as WizardLM. For instance, GLAN training required approximately 8 days using 32 A100 GPUs to generate 10 million instructions, but no direct comparisons are made to illustrate the efficiency or cost-effectiveness relative to other approaches.
3. The method is limited by the performance of GPT-3.5/4: The quality of the generated taxonomy and syllabus heavily depends on the capabilities of the underlying LLMs used in the process, namely GPT-3.5 and GPT-4. In general, GLAN does not inform how we can improve capabilities of models beyond GPT4. But also, does not consider the cost of generating 10 million instructions.
4. High variability in results (Table 2): There is significant variability in GLAN's performance across different categories, with particularly weaker results in humanities and social sciences compared to STEM fields. The authors should address this, also discuss the document proportion of each taxonomy, and potentially see if there is a correlation between the data size and performance.

Limitations:
Please see Weaknesses

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
rSSpEmrN0C;"REVIEW 
Summary:
The paper introduces a novel approach, named LayTextLLM, for document understanding tasks, which efficiently integrates spatial layouts and textual data within LLM. It employs a Spatial Layout Projector and introduces two innovative training tasks: Layout-aware Next Token Prediction and Shuffled-OCR Supervised Fine-tuning. Extensive experiments demonstrate significant improvements over previous state-of-the-art models in KIE and VQA. This paper demonstrates the importance of layout information in document understanding tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper introduces a novel approach by integrating SLP and P-LoRA to effectively encode and process layout information. This method significantly improves the interaction between spatial layouts and textual data within LLM, providing a new direction for future research.
2. The paper proposes the LNTP task and SSFT task to enable the LLM to layout information, thereby enhancing its document understanding capabilities and improving performance on document-related tasks.

Weaknesses:
1. Due to miss the crucial visual information necessary for document understanding, this LayoutTextLLM heavily relies on OCR-derived text and spatial layouts. Other works such as LayoutLLM, layoutLMv3, introduces visual information to enhance the document understanding performance.
2. The exploration of the shuffling ratio was conducted only on Key Information Extraction (KIE) tasks. It should also be validated on Visual Question Answering (VQA) datasets to determine if the 20% shuffling ratio is optimal across different types of tasks.
3. The effectiveness of LNTP and SSFT methods should be substantiated with more ablation studies. It is recommended to fine-tune Llama2-7B directly using the existing data for a more comparisons.
4、Although LayTextLLM shows higher performance on DocVQA compared to LayoutLLM, this comparison is not entirely fair as LayoutLLM was evaluated in a zero-shot setting. Moreover, the zero-shot performance of LayoutLMv3 on DocVQA surpasses that of LayTextLLM.

Limitations:
The author has already mentioned in the limitation section of the paper that the proposed model is difficult to handle scenarios where inference relies on visual cues.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the LayTextLLM method for document understanding, which encodes text positional information in the embedding space of an LLM and trains for effective understanding of document data as interleaved OCR-detected text and bounding box information. The results show improved performance compared to prior works on the KIE tasks, as well as on VQA in many cases.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The treatment of layout information as a modality interleaved with text is logical, and the use of a projection into the LLM’s embedding space to represent bounding boxes is clever and appears to be novel. The tasks approached are important and overall the proposed method does appear to improve document understanding (though this will be more convincing if the caveats listed below are addressed).

I also appreciate the focus on open-source models and data for the method and its evaluation, making the results reproducible.

Weaknesses:
There are some issues regarding the comparisons to existing models, making it unclear how much of the observed improvement is really due to the novel method proposed.

LayTexLLM is implemented with Llama-2-7b, but it seems that many models compared to (e.g. the strong-performing LayoutLLM) may use other LLM backbones, making it unclear whether the superior performance of LayTexLLM in many settings is due to the proposed novel method or the LLM backbone. The results will be more convincing with a comparison of different methods with the same LLM backbone (or at least an analysis of the number of parameters in each model).

It is not clear what OCR engine is used, raising the concern that different OCR engines could explain some of the gaps in performance between models being compared.

There are also issues with how the training is presented that make it difficult to interpret results. Some places (L131, L179, etc.) mention pre-training and SSFT, implying that pre-training means the LNTP training task. However, Sec 4.1 mentions “pre-training” and “SFT”, implying that pre-training refers to SSFT+LNTP and that it is followed by SFT (Supervised Fine Tuning) for particular tasks (VQA and KIE). The results also mention zero-shot and supervised results (e.g. L297), but it is unclear from the text and results tables which results are obtained zero-shot or from SFT, making it hard to understand if the comparisons are fair.

The statements about large improvements over SOTA MLMMs (L13-14, L83-84) seem slightly misleading since LayTextLLM uses OCR detections and thus is more comparable to other OCR-based methods.

LNTP (Sec. 3.2.1) is presented as a novelty but seems to just be the regular language modeling objective. If I understand correctly, this could be toned down to simply say that the added SLP and P-LoRA parameters are updated with a language modeling loss.

Limitations:
Limitations are clearly discussed in Section 5 (which should have the title “Limitations” in plural). Additionally, does the limitation of lacking visual cues apply to text formatting such as bolding or italics? This would connect well to the examples in Figure 6 where bold text is prominent.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents an innovative method for integrating layout information into LLMs to enhance document understanding tasks. Instead of treating bounding box coordinates as input text tokens, the bounding box information is embedded into a single token and interleaved with text tokens. This approach addresses the challenge of long sequences while leveraging the autoregressive nature of LLMs. Experimental results demonstrate the effectiveness of the proposed method, achieving state-of-the-art performance and resulting in shorter input sequence lengths.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.Interleaving layout information and text is novel.

2.The proposed Shuffled-OCR Supervised Fine-tuning is interesting and may benefit other OCR-based approaches.

3.The approach achieves state-of-the-art performance on most text-rich VQA and KIE tasks, validating the effectiveness of interleaving layout and text and significantly reducing input length.

4.The paper is well-written, providing sufficient experimental details, ablations and discussions to comprehend each component of the model.

Weaknesses:
1.In layout-aware pretraining tasks, whether it is beneficial to predict both the bounding box and the text, rather than just the text. 

2.LaytextLLM achieves satisfying performance in various tasks, but it will be better to incorporate the visual modality for more application scenarios.

Limitations:
Please refer to weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents LayTextLLM, a novel approach to document understanding that effectively integrates spatial layout information and text into a large language model. Existing methods that integrate spatial layout with text often produce excessively long text sequences. LayTextLLM addresses these problems by projecting each bounding box into a single embedding and interleaving it with text.  The method is evaluated on Key Information Extraction (KIE) and Visual Question Answering (VQA) tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Effective sequence reduction: The proposed method reduces the length of text sequences, addressing a common problem in document understanding.
- Performance improvement: LayTextLLM demonstrates improvements in KIE and VQA tasks, showing performance gains over alll state-of-the-art models.
- Evaluation: The paper provides detailed benchmark evaluations on 2 tasks and 7 datasets

Weaknesses:
- Incomplete related work: The paper omits several relevant OCR-based models, such as UDOC, LayoutMask, BROS, LAMBERT, DocFormer and LiLt.
- Insufficient explanation: The repeated claim that DocLLM cannot fully exploit autoregressive features is not adequately explained.
- Limited comparisons: There is no comparison with alternative methods that embed coordinates, such as co-as-token approaches (Lmdx, Shikra, ICL-D3IE).
- Marginal token reduction: The reduction in the number of tokens appears to be limited, and the paper does not clarify whether words or lines are encoded, which could have a significant impact on token reduction.

Limitations:
- Limited comparisons: The paper primarily compares LayTextLLM to DocLLM, which may not provide a comprehensive assessment of its performance.
- Impact of token reduction: The reduction in the number of tokens, while beneficial, appears to be limited and may not provide significant practical benefits in all scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
rC0OM4Jm4v;"REVIEW 
Summary:
The paper proposes to employ text-to-image latent diffusion models to augment images through a controlled modification such that the resultant class is different from the source class. Such augmented images are referred to as hard negative images. Building upon SDEdit style image modification, the paper controls the extent of modification by adaptively determining the appropriate noise-scale for each image separately. The benefits of this type of augmentation have been demonstrated on few-shot and long-tailed imagenet classification tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper is very well written presenting the core idea of generating hard-negative images by modifying an image with a caption of another class. This idea is simple, intuitive and interesting. 
- Furthermore, the algorithm to determine the optimal noise-level for each image adaptively is not only simple and intuitive but also effective in eliminating dependence on hyperparameters. I feel that a connection can be made to the recent work [1] on phase-transition in diffusion models since this algorithm is attempting to find the diffusion-time when phase-transition occurs. 
- The evaluation is comprehensive considering a variety of diffusion-augmentation baselines as well as traditional augmentations.
- The paper illustrates the effectiveness of the adaptive search procedure through separate experiments with DINO-v2 and visualisations.
- In many cases, synthetic data generation with a diffusion model may be replaced by a simpler retrieval baseline [2]. However, the goal of this work is to use a diffusion model to search for and generate hard negatives, which is an interesting deviation from some of the previous synthetic data augmentation approaches. 

[1] Sclocchi, Antonio, Alessandro Favero, and Matthieu Wyart. ""A phase transition in diffusion models reveals the hierarchical nature of data."" arXiv preprint arXiv:2402.16991 (2024).

Weaknesses:
- From the various results in the paper, it seems that the Text2Image, GeNIe, and GeNIe-Ada achieve comparable performance with respect to each other on average. This seems to suggest that the majority of the gains can be attributed to the increased number of _distinct_ examples --- as compared to regular augmentations which simply apply different transformations to the same image --- for each class rather than the hard-negatives in GeNIe/GeNIe-Ada. 
- Additionally, it seems that beyond some threshold, any value of $r$ that changes the source-image to the target image yields comparable performance indicating that it may be sufficient to generate an augmentation that is similar to source-image and it need not specifically be a _hard-negative_.  It may be useful to consider some other applications where images lying in the boundary of the classifier may be informative: for example, see recent work on generating outliers [1] for OOD detection. 
- GeNIe-Ada algorithm is compute-intensive as compared to a simple Text2Image augmentation since it requires generating several augmentations for each source image before selecting one optimal augmentation that lies on the decision boundary. Given how close the text2image and genie-ada performances are in some cases, it may be possible that we could generate more augmentations using text2image in the same compute budget and improve over GeNIe. 
- (minor) GeNIe is applicable to the fine-tuning stage rather than the pretraining stage.

[1] Du, X., Sun, Y., Zhu, J. and Li, Y. Dream the impossible: Outlier imagination with diffusion models. NeurIPS 2024.

Limitations:
Yes, limitations are addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GeNIe, a data augmentation method for training vision models using synthetic images. GeNIe generates images by combining a source category image with a target category text prompt, selecting those that feature source characteristics but belong to the target category as negative samples. Experimental results show that GeNIe improves performance in both few-shot and long-tail distribution settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* The proposed GeNIe improves the performance in few-shot and long-tail distribution settings.
* The paper provides extensive experiments to support the claims,  including the selection of noise levels.
* The paper is well-written and easy to follow.

Weaknesses:
The key idea of GeNIe is to use image editing to combine features from two categories. Here are several questions:

* Regarding controllable image augmentation
  * Line 9 mentions that GeNIe ""retains low-level and background features from the source image."" How does GeNIe control which features are retained or changed?
  * To combine features from different categories, how about adding the attribute from the target category to the prompt? For example, a ""[dog] with [wings]"".  This method does not require carefully selection of denoise steps. 
  * Other image editing methods, such as those in [1] and [2], efficiently control image changes using prompts or user instructions.  For example, they can transform a car into a motorcycle in Figure 2, while keeping the background unchanged for more challenging negative samples. What advantages does GeNIe offer over these methods?



* GeNIe generates images ""using images from all other classes as the source image"" (line 227). Will all (source image, target prompt) pairs lead to effective image generation? Which types of pairs contribute the most to the final accuracy?

     [1] Prompt-to-Prompt Image Editing with Cross-Attention Control

     [2] InstructPix2Pix: Learning to Follow Image Editing Instructions, CVPR 2023

Limitations:
The paper has discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the idea is to generate data for data augmentation by utilizing a pre-trained diffusion model. The method employs different text prompts and an adjusted noise scheduler to generate hard negative samples for the source distribution. ""GeNIe"" creates new augmentations using diffusion by leveraging source images and contradictory target prompts. ""GeNIe-Ada"" adjusts noise levels on a per-sample basis, using the classifier as the condition boundary to select the right threshold.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The method offers infinite possibilities to separate the source from the target.
- The idea is simple, original, and convincing.
- The ablation studies and experiments demonstrate strong performance.

Weaknesses:
- The method is slow, particularly GeNIe-Ada, as it requires generating an image through multiple forward passes of a diffusion model and using a classifier to select the appropriate threshold $r$.

- The number of steps required to retain low-level features is crucial for optimizing the method's performance.

- The method relies on access to a foundational text-to-image model trained on billions of images.

Limitations:
/

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel augmentation method based on diffusion models. A latent diffusion model conditioned on a text prompt generates hard negatives, by adjusting the noise level. The hard negatives can be used as challenging augmentations. The authors demonstrate the effectiveness of their approach on long-tail and few-shot settings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Well-written paper with clear contributions and presentation.
- Extensive experiments and evaluation.
- Interesting and useful idea.
- Code included in the supplementary.

Weaknesses:
I am generally happy with the paper, experiments, and presentation. A weakness seems to be the selection of the noise ratio r.  The authors propose an algorithm for this. However, I am concerned how sensitive it is for different datasets or classification settings. This might affect performance in other settings or in real-world scenarios. If this is true, it might degrade the overall method's usefulness.

Limitations:
The authors have added a section for limitations and a section for broader impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
r7mj17BKzw;"REVIEW 
Summary:
This paper introduces SuperEncoder, a novel approach to Quantum State Preparation (QSP) that aims to combine the scalability of Approximate Amplitude Encoding (AAE) with the speed of traditional Amplitude Encoding (AE). SuperEncoder uses a pre-trained neural network to directly estimate the parameters of a Parameterized Quantum Circuit (PQC) for any given quantum state, eliminating the need for iterative parameter tuning during runtime. The authors explore different loss functions for training SuperEncoder, finding that state-oriented training using fidelity as a metric (L3) performs best. They evaluate SuperEncoder on synthetic datasets and downstream tasks like Quantum Machine Learning and the HHL algorithm, comparing it to AE and AAE. Results show that SuperEncoder achieves runtime similar to AE while maintaining the scalability of AAE, but with some degradation in fidelity. The impact of this fidelity loss varies across applications, being more tolerable in QML tasks than in precise algorithms like HHL.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a novel approach to Quantum State Preparation with SuperEncoder, which innovatively combines the strengths of existing methods (AAE and AE). The idea of using a pre-trained neural network to directly estimate quantum circuit parameters is a nice solution to the QSP problem.

Quality: The research demonstrates high quality through its comprehensive experimental design. The authors explore different loss functions, provide detailed analysis of their landscapes, and evaluate the method on both synthetic datasets and real-world applications. The comparison with existing methods (AE and AAE) across multiple metrics (runtime, scalability, and fidelity) shows a rigorous approach to validation.

Clarity: The paper is well-structured and clearly written. Complex concepts are explained in an accessible manner, with helpful diagrams (like Figures 2 and 3) to illustrate key ideas.

Significance: SuperEncoder potentially represents a step towards more efficient QSP, which is crucial for many quantum algorithms.

Weaknesses:
1. The gradient evaluation of the loss function (e.g. Eq. 1) requires computing the derivative of the state $\rho$ with respect to model parameters. As the authors acknowledge, this could become complicated on real devices due to the enormous cost of quantum state tomography. The authors work around this by using the parameter-shift rule to compute the gradient. However, the parameter-shift rule does not scale as well as classical backpropagation with autodiff (see https://openreview.net/forum?id=HF6bnhfSqH -- I guess a citation to this work would be relevant here). This casts doubts on the whole scalability of this method.

2. Again related to scalability, the number of input neurons to the model has to be $2^n$. This again doesn't look too scalable past 20 qubits, which can already be realized experimentally.

Limitations:
Limitations have been discussed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a model, namely SuperEncoder, to solve the quantum state preparation problem. Instead of evolving the parameterized gates to generate the target quantum state, they train a model to predict the rotation parameters from the target states.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Solve the quantum state preparation problem from a new perspective.

Weaknesses:
1. Poor results. The results seem ok with four qubits but decrease way too fast when increasing the number of qubits. The proposed method is not comparable to previous methods.
2. It is actually impossible to use an ML model to predict the parameters. Since training the AAE ansatz is a non-convex optimization problem, finding the optimal parameter is indeed an NP-hard problem. There are infinitely many pairs of quantum states and parameters, and I wonder how the size of the training set would scale with the number of qubits. 
3. The training overhead is non-negligible. If we are preparing a quantum state that is beyond the simulation power of classical devices, the evaluation methods based on state fidelity would need an enormous number of quantum circuit executions, which I suspect would not be much less than training the AAE.

Limitations:
Naive ideas with poor experimental results.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the problem of Quantum State Preparation (QSP), which is critical for quantum computing but requires a circuit depth that scales exponentially with the number of qubits, making it impractical for large-scale problems. The authors propose SuperEncoder, a pre-trained classical neural network model designed to estimate the parameters of a Parameterized Quantum Circuit (PQC) for any given quantum state. This approach eliminates the need for iterative parameter tuning, making it a significant advancement towards iteration-free approximate QSP. 

Contributions

1. Introduction of SuperEncoder, which pre-trains a classical neural network to estimate PQC parameters directly, bypassing the need for iterative updates.
2.  Provides empirical evidence that SuperEncoder significantly reduces the runtime for quantum state preparation compared to traditional methods, thus enhancing the efficiency of quantum algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
See  Contributions.

Weaknesses:
1. [Scalability Issue]
The most significant drawback of this work is its poor scalability. Since the input to the SuperEncoder is $2^n$ dimensional, the number of qubits cannot be too high, such as exceeding 20 qubits. This limitation severely restricts the applicability of the SuperEncoder to larger quantum systems. Discussing potential strategies to overcome this drawback would greatly enhance the practical value of the SuperEncoder.

2. [Barren Plateau Problem]
Another major issue is that, even within a reasonable range of qubit numbers (e.g., 10-20), training the SuperEncoder is challenging due to the barren plateau problem. Consequently, the SuperEncoder is likely only suitable for situations involving fewer than 10 qubits. In these cases, the time difference between AAE and SuperEncoder is not as significant as one might expect, which greatly limits the potential impact of this work.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
qNYYb4nOKA;"REVIEW 
Summary:
* This paper studies the evaluation of information extraction, particularly LLM-based IE, in scenarios where human-annotated data is unavailable.
* The proposed evaluation framework relies on the `Needle in a haystack` evaluation. That is, an LLM is first used to generate a piece of information (needle) given the original text; then, the needle is infused into the document, and the quality of IE is assessed by whether the needle can be successfully extracted. 
* In addition to this evaluation framework, the authors also discussed several aspects to be considered when using LLM-based IE for processing long documents.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
An interesting application of `Needle in a haystack` evaluation in information extraction.

Weaknesses:
* The writing quality is not great, and several areas require further clarification
	* The current paper structure is confusing; not sure what role Sections 3 and 4 play in this paper, e.g., whether the authors were proposing a new LLM-based IE approach
	* I suggest providing a formal definition of IE studied in this paper because it is very confusing to know what information is extracted. For example, in the abstract, `entity and its properties` is mentioned; in Section 3, `short paragraphs of text` seem to be the information extracted `from the continuous text`; also see Q2 
* The main contribution of the paper is an automatic framework to assess the quality of the IE; however, the authors didn't conduct any experiments to demonstrate the effectiveness of the proposed framework (e.g., whether the evaluation results correlate with human judgments); the other main limitation is the authors evaluate the quality of extraction based on the proportion of successfully extracted needles but totally ignore the correctness of extracted information (precision)
* The experiments are conducted on private datasets with only several toy examples described in the paper; it will be very difficult for others to reproduce the results. I would suggest conducting experiments at least on some document IE datasets, for example, from news or biomedical domains.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the quality evaluation of information extraction (IE) performed by large language models (LLMs). It discusses the methods to handle the input/output size limitations of the LLMs and their performance in IE. It also introduces additional scores to evaluate the extraction quality and discusses how to interpret them.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper analyses the technical limitations of LLMs complicating the extraction of information from a long context.
2. This paper presents to insert a needle into the data to evaluate the performance of IE without labeled data.

Weaknesses:
1. The analysis of the performance of LLMs in IE is not new and has various analysis, such as in the following papers:

> [1] Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness (Li et al., 2023)

> [2] Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors (Han et al., 2023)

> [3] When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks (Peng et al., 2023)

Among the papers, the authors in [3] also analysed LLMs' limitations in long context understanding, which is similar to the conclusion of this paper. 

2. This paper lacks a thorough literature review in LLM for IE as well as new evaluation formats, such as [1, 2, 3] and the following paper:

> [4] Evaluating Generative Language Models in Information Extraction as Subjective Question Correction (Fan et al., LREC-COLING 2024)

3. This paper only focuses on the NER task but lacks the other IE tasks, e.g. relation extraction and event extraction. Additional experiments are required to test the generalisability of the method. The number of samples tested is also limited (see ""# entities used for evaluation"" in Table 3).

Limitations:
See ""Weaknesses"".

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a framework to capture information extraction quality in the absence of humanly labelled and curated datasets. It explains how an approach on how to include the schema, and the role and limitations of LLM's (specifically gpt-4-1106-preview).

Experiments are done (I guess), by ""extracting information"" from long business documents originating from the healthcare sector. Several scores are presented according to the SUSWIR metrics. It delves into the ""lost in the middle"" phenomenon. It introduces the MINEA score, a newly proposed metric.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It tries to address a relevant problem in the field (curated benchmark data is hard to come by).

Weaknesses:
- The paper is from the start extremely vague and misses concrete statements and explanations about the work done. The contributions are unclear, the data is essentially undefined, for most of the work what exactly is being done is simply unclear.
	
- Even the task of ""Information Extraction"" is not concretely described in a way that is reproducible.
	
- Line 7-8: ""The framework focuses on information extraction in the form of entity and its 
	properties"". 
	
- Table 1: it is completely lost upon me what is being presented here.

-  ""We extract information from several long documents from our business case"". What are these documents? What are they originating from?

-  The scores mentioned are ""redundancy"". How is this measured? What do these scores represent? Is lower or higher better? Even these basic questions are not answered. All of this in the appendix (where it shouldnt be), and the further tables are not better.

- The work is very dry. There are no figures that explain or examplify what the problem is, or how this framework is supposed to fit.
	
- The related work section is short and doesn't address the original point (evaluation in absense of benchmark data).

- It is unclear to me how this work should contribute in any form to evaluation in the absence of benchmark data.
	
- The introduced MINEA score is ""explained"", but not examplified or mathematically defined.
	
- All examples are screenshots of data in JSON format rather than helpful explanations.

Limitations:
No. The paper does not concretely address the limitations of this metric. There are no good, bad examples provided.

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an automated framework for evaluating the quality of IE tasks using LLMs. The framework introduces a scoring method called MINEA, which creates evaluation criteria by injecting artificial data (""needles"") into documents. The paper also discusses how to deal with the limitations of LLMs when processing large amounts of data, and introduces an iterative extraction process to improve the completeness of the extraction and reduce repetition.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
s1. The introduction of the MINEA score is somewhat innovative.

s2. The paper is clear explanations of the proposed framework.

Weaknesses:
w1. Lack of Originality: The originality of the paper is insufficient. Related work has already mentioned using the ""needle"" method to evaluate the information extraction capabilities of LLMs. While this paper adds the use of large models to help create the needles, the contribution is still lacking.

w2. Insufficient Experimental Description: The description of the experimental setup is missing, including the experimental environment, data sources, and dataset sizes. However, the paper spends too much space on toy examples.

w3. Unreliable Conclusions on Length Limitations: For the experiments on the input and output length limitations of models, the paper only tested one model, making the conclusions unreliable.

Limitations:
L1. The paper should provide a comparison to existing work to highlight the improvements.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
qLtLQ4KUCq;"REVIEW 
Summary:
This paper proposed GSAAL to simultaneously address three changeling problems in outlier detection: inlier assumption (IA), curse of dimensionality (CD), and multiple views (MV).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper has a good flow.
The paper proposed the first outlier detection method that explicitly addresses IA, CD, and MV simultaneously. 
The paper has strong theoretical and empirical evidence to show the advancement of the proposed method. 
The experimental design is solid and the numerous visual examples help to facilitate understanding.
The paper has good reproducibility with open codes.

Weaknesses:
some (but few) places to improve.

Limitations:
The authors have analyzed the limitations sufficiently.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL) for outlier detection to address the limitation of the previous work such as multi-view and the curse of dimensionality, where the theoretical convergence, the scalability of the algorithm are discussed. Experiments on real dataset and synthetic tabular dataset are carried out to establish the validity of the approaches.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The manuscript presents a method called generative subspace adversarial active learning for outlier detection in multiple views. The proposed method called GSAAL provides the proof of convergence, the computation complexity and aims to address the curse of dimensionality. The outlier detection in high dimensional space indeed is an important and challenging solution. Thus, the proposed method can be a good solution to address this difficult problem.

The manuscript has compared the performance of GSAAL with other outlier detection approaches with detailed visual illustration and AUC. The experiments show advantages of the proposed solution over other competing methods. The experiments seems to be detailed.

Weaknesses:
The novelty of the work appears to be small. Theoretically, the derivation of theorem 1 is very similar to GAN derivation. 

In this case, the paper needs to compare their solution both theoretically and experimentally with the related work for outlier detection using GAN such as [1] https://arxiv.org/pdf/1906.11632 such as AnoGAN, BioGAN and EGBAD. 
[2] https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-022-00943-7
If we compare the main equation (2) in the manuscript with the formulation in reference [1] with conditional GAN and BioGAN, it seems the main difference are the proposed method used multiple detectors and accumulated the performance, which should not be considered a large distinction.

Due to the lack of the comparison with generative adversarial network based approaches such as AnoGAN and EGBAD, the potential improvement of the purposed method against the state-of-the-art approaches is not clear. The novelty of the paper does not stand on the safe ground. The theoretical derivation is also similar to GAN derivation.

Limitations:
Limited innovation and lack of critical comparison with important reference are the main issues of the current manuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The main contribution of this paper is to improve existing work on Generative Adversarial Active Learning (GAAL) by using multiple discriminators for multiple views to detect outliers in tabular data. The training mechanism is similar to existing works. The paper also introduces a theoretical analysis on Multiple Views (MV). As claimed by the authors, GAAL addressed the problems of Inlier Assumption (IA) and Curse of Dimensionality (CD), but missed Multiple Views (MV), which is the main focus of this paper. The experimental results compare the proposed method to GAAL and some other classical methods such as OCSVM and KNN, ....

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper introduces an interesting view about MV and proposes a new method to address this MV problem together with theoretical analysis.

Weaknesses:
* The empirical results are not strong (or at least unclear in the way the authors presented in the main paper); most of the experiments are on synthetic datasets.
* The results on the real dataset do not seem to show significant improvements compared to existing work (or at least it is hard to observe this when reading the paper). Perhaps the authors could improve the writing and highlight the results better. It is unclear to me why the experiments on the real dataset were put in the Appendix, as it is an important result.
* The paper claims at the beginning that it not only improves the MV problem but also the IA and CD problems, but this is hard to see with the current writing of the paper. Could the authors highlight the experiments in the paper to prove that claim?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
qL4nN6Ew7U;"REVIEW 
Summary:
The paper proposes Fantasy, a T2I model based fully on transformers (except for the VQGAN for the latents encoding and decoder):
* A __fine-tuned LLM__ (based on Phi-2) for the text encoding
* A image generator based on the MIM (Masked Image Modelling) approach

The training happens in two stages, a generic stage for aligning the generator the the frozen Phi-2 features, followed by a fine-tuning stage where the Phi-2 encoder is fine-tuned alongside the MIM transformer.

The results on human evaluations are convincing, putting Fantasy alongside models that require larger computational resources, while the FID results are less convincing (due to the image being smooth according to the authors).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty:
* The LLM is __fine-tuned__ but only in the second stage of training, this approach is new and makes sense

Accessiblity:
* The 2 stage pre-training is already standard practice
* The Phi-2 model is available, it is likely that this approach works for other available models (Phi-3? It could be interesting to test)
* The model size allows the model to be trained in a reasonable time

Weaknesses:
Performance:
* The FID scores are not competitive and the authors describe why: the image are smooth => it seems that the human evaluations still rank Fantasy at the top on visual appeal, but it might be that if the question was ""visual realism"" they might prefer a different model
* Results are available for 256px, and a 600M parameters MIM generator, there is no proof that this method scales (we know that diffusion models based on UNet have trouble scaling for instance)

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an efficient text-to-image generation model that integrates LLM and MIM. It demonstrates that MIM can achieve comparable performance. Unlike commonly used text encoders like CLIP and T5, this study introduces an efficient decoder-only LLM, phi-3, achieving better semantic understanding. The effectiveness of the method is validated through a newly proposed two-stage training approach and sufficient experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written with clear logic.
2. The use of MIM and LLM for image generation introduces a novel approach.
3. The two-stage training method improves the generation results.

Weaknesses:
1. The quality of the generated images does not yet match that of existing methods (e.g., pixart-alpha, SDXL), with some loss of detail. This is noticeable from the comparison in column B of Figure 5.
2. Some aspects of the methodology could be clearer, and the overall coherence of the approach could be strengthened.
3. While the proposed method demonstrates efficiency advantages, particularly in faster training convergence, this can be influenced by various factors. However, the related experiments in the paper could be more comprehensive.
4. The semantic accuracy of the generated images, a potential strength of Fantasy, is not fully demonstrated in the paper. For instance, the model's ability to handle prompts with multiple entities, color attribute descriptions, or retaining key elements in long text inputs is not adequately showcased.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a technique for training transformer based masked image modeling in an efficient way. Two main contributions include (1) use of a LLM decoder as text embeddings, and (2) Two-stage training strategy for MIM models. Experimental results show good generation quality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of LLMs as text encoders seem interesting.
- Two-stage training approach makes sense. First, the use of pretraining data helps the model learn a general text-image model, and the high quality alignment data can improve the quality of generations.
- Training models on low resources seem appealing.

Weaknesses:
- I don't see anything new proposed in this paper. The authors simply use Phi-2 model as text encoder with MIM models, and use two-stage training. 
- Even two-stage training is not something new to image synthesis. People have been doing aesthetic finetuning to improve image quality in diffusion models (eg. stable diffusion). The authors extend this to instruction-image data.
- The quality of generated images are not very impressive. When zoomed in, we notice a lot of visible artifacts. The generated images are also flat and doesn't have a lot of details.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To develop a resource-efficient, high-quality image generator for long instructions, the authors presented Fantasy, an efficient T2I generation model that integrates a lightweight decoder-only LLM and a transformer-based masked image modeling (MIM). 

They demonstrate that with appropriate training strategies and high-quality data, MIM can also achieve comparable performance.

By incorporating pre-trained decoder-only LLMs as the text encoder, they observe a significant improvement in text fidelity compared to the widely used CLIP text encoder, enhancing the text image alignment. 

Their training includes two stages: 1) large-scale concept alignment pre-training, and 2) fine-tuning with high-quality instruction-image data. 

They conduct evaluation on FID, HPSv2 benchmarks, and human feedback, which demonstrate the competitive performance of Fantasy against other diffusion and autoregressive models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The author proposed a T2I framework that combines several more recent components and performed a series of comparisons, including both quantitative and human evaluations.

Weaknesses:
- the major concern of the work is unclear contributions. The claimed three contributions or core designs are quite similar with existing works.
- Efficient T2I network: there is no justification about why the network is “efficient”. Simply adopting a smaller LLM like Phi-2 can hardly be claimed as efficient network design. 
- The hierarchical training strategy was also proposed before, it is not clear what is the difference with existing work.
- High quality data: the training data utilize Laion-2B and use existing filtering strategy. The collection high quality synthesized images from existing datasets.
- The evaluation metrics are mainly based on HPSv2, which has a limited range of values, e.g., HPSv2 has close values for SDv1.4 and SD2.0, e.g., 27.26 vs 27.48. Why SDXL is missing in Table 1?
- The author acknowledged that their model lags behind diffusion-based models in visual appeal, limited by the 8K size of VQGAN’s codebook and not targeting visual appeal. However, there is no solution or further study for solving this problem, which limits the scalability of the model.
- The scaling study in section 4.2 seems pretty premature and it is unclear what is the limit of the scaling. Increasing the model depth can improve the performance, which has been verified from previous work such as in https://arxiv.org/abs/2212.09748 or https://arxiv.org/abs/2404.02883.

Limitations:
I would encourage the authors to emphasize about the core contributions rather than combining everything together, which can hardly show significant performance improvement over existing public models.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
pNQB78UDp4;"REVIEW 
Summary:
In this paper, in order to break the dominance of adapter-based methods, the authors first analyze the weakness of the previously widely-used prompt-based method, Visual Prompt Tuning (VPT). Firstly, the prompt mechanism is inherited from NLP where each token/prompt represents an actual word with rich semantic information. However, in visual tasks, tokens represent image patches and contain sparse semantic information. Therefore, simply concatenating the prompt tokens with embedded tokens in visual tasks may not provide enough information to guide the model for downstream tasks. In addition, it is difficult to get a deep understanding of spatial relationships and structural features of an image with prompt tokens, which leads to another two weaknesses of VPT. 1. The computational complexity of self-attention becomes higher when more prompts are used, which introduces computational inefficiency and redundancy. 2. extra prompts will influence the results of softmax operation in the self-attention. Most of the weight falls on the prompts and causes the destruction of self-attention between embedded tokens. 

The authors thus proposed Cross Visual Prompt Tuning (CVPT). CVPT inserts a cross-attention module to calculate the cross-attention between prompt tokens and the embedded tokens after self-attention. This module decouples the prompt and the embedded tokens to avoid the quadratically increasing computational complexity of self-attention modules and the destruction of self-attention between embedded tokens. This module allows the model to focus on the relationship between embedded tokens and the prompt tokens to adapt to downstream tasks more efficiently. In addition, the weights used in cross-attention are shared with the self-attention module and kept frozen to reduce the trainable parameters.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1.	Good performance on image classification and semantic segmentation tasks.
2.	Analysis of the weaknesses of prompt-based methods and VPT.
3.	Cross-attention module to decouple the prompt tokens and embedded tokens to solve the problems of prompt-based methods.
4.	Comparison with VPT to show the weakness of VPT and strength of CVPT when more prompts are used.

Weaknesses:
1. No experiment or previous work (at least not cited) demonstrates that the prompts in visual tasks lack representation information. In fact, this is somehow counter-intuitive to your 3rd observation: Destruction of self-attention between embedded tokens. The phenomenon the authors observed in this part clearly states that there is an over-emphasized on prompts with significantly higher value. Also, in [ref1-2], a clear activation/focus shift can be observed after prompt integration, does that mean prompt actual benefits from such the over-emphasized during transfer learning? To sum up, the idea/motivation becomes ambiguous with such observations.

2. Although the author shows clearly that the sum of the prompt’s weight values exceeds 0.8. However, no experiment proves the relationship between the distribution of the weights and the model performance. The prompts are learned and updated during training to fit the downstream tasks and the weights are calculated based on those prompts and embedded tokens. Can we say that in some situations, the prompts learned a more suitable and efficient representation than the embedded tokens, and more weights are applied to them? The distribution of the weights in self-attention is a good point for analyzing the prompt-based methods. But more discussions are needed. 

3. Cross-attention should be assigned to the preliminary, not the contribution of the paper in Sec 3.2.

4. More discussions with E2VPT are acquired since the cross-attention prompt tuning is strongly associated (without additional prompts after the cls token).

5. Also there is an inconsistency in the experiment setup, in Figure 2, the authors in detail discuss the self-attention weight obtained by prompt tokens and embedded tokens, where no comparison studies are included to the new proposed Cross Visual Prompt Tuning to show different observations in order to support this claim.

6. To show the robustness of Cross Visual Prompt Tuning, it is better to demonstrate other hierarchical transformer architectures' performance (e.g., Swin). However, I noticed that CVPT might be insufficient to do so with the introduction of shifted window. More details should be included on how CVPT adapts to these structures.

[ref1] Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?

[ref2] SA²VP: Spatially Aligned-and-Adapted Visual Prompt

Limitations:
The discussion on limitations is listed in Sec. 5. No potential negative societal impact is discussed (which is applicable).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on prompt learning of pre-trained ViT in downstream tasks, and improves the widely used visual prompt tuning (VPT) by employing cross-attention techniques and weight-sharing mechanisms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper's research topic on vision model prompting technology is highly significant in the era of fundation models. The experiments are detailed, the structure of the writing is complete, and the methods are straightforward.

Weaknesses:
W1: While using VPT as a baseline, the paper sets up a scenario (e.g., Figure 2) with an unnecessarily large number of prompts, whereas the number of required prompts generally varies depending on the downstream task. In many cases (e.g., VTAB-Natural), using fewer than 10 prompts yields better results [1]. In such scenarios, considering the Flops comparison between CVPT and VPT as shown in Figure 1, does CVPT still maintain an advantage in terms of both runtime and accuracy?

W2: The paper mainly integrates the method of CrossViT from [2] into prompt learning of VPT, but does not explain the motivation behind applying CrossViT's method to prompt learning in downstream tasks. Specifically, how does CrossViT relate to addressing the three issues of VPT mentioned in Section 3.1 (i.e., why CrossViT method is effective in prompt learning, and why it is superior to other derived methods like EEVPT)? It is recommended to attempt a theoretical explanation of the necessity of applying cross-attention, or to supplement the section with experiments and analyses explaining how CVPT addresses the three issues of VPT proposed in 3.1.

W3: The paper does not provide code for reproducible results, nor does it present evidence of statistical significance (e.g., std) in tables. The authors claim in the 5th question of the checklist that they need time to organize this part. It is suggested that the authors organize the paper comprehensively before submitting it to conferences.

References:

[1] Jia, Menglin, et al. ""Visual prompt tuning."" ECCV, 2022.

[2] Chen, Chun-Fu Richard, Quanfu Fan, and Rameswar Panda. ""Crossvit: Cross-attention multi-scale vision transformer for image classification."" ICCV, 2021.

Limitations:
The authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a variant of visual prompt tuning (VPT) where the authors suggest applying cross-attention instead of self-attention in the Transformer layers to reduce training complexity. The authors analyze several drawbacks of existing VPT approaches and claim to address them using cross-attention.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
- **Identified Drawbacks**: The authors reasonably point out some drawbacks of current VPT methods, such as a “lack of adaptation to visual tasks” and “computational inefficiency.”

- **Complexity Reduction**: The proposed use of cross-attention indeed reduces computational complexity compared to the original self-attention mechanism.

Weaknesses:
- **Limited Novelty**: The proposed idea is straightforward, merely replacing self-attention with a combination of self and cross-attention. Similar concepts have been explored in previous works, such as prefix tuning (Li et al., 2021; Yu et al., 2022).

- **Limited Impact and Efficiency**: The improvement in complexity is minimal because the number of prompts is typically much smaller (fewer than 20) compared to image embeddings (196).

- **Limited Performance**: The overall performance is limited compared to some recent works by Wang et al. (2023) and Wang et al. (2024). These works, which show significantly better performance, are not compared in the paper. Therefore, the claim that CVPT “reaches SOTA” (L272) is factually incorrect.


----

Li et al. Uav-human: A large benchmark for human behavior understanding with unmanned aerial vehicles. CVPR 2021

Yu et al. Towards a unified view on visual parameter-efficient transfer learning (V-PETL). 2022

Wang et al. Adapting shortcut with normalizing flow: An efficient tuning framework for visual recognition. CVPR 2023

Wang et al. Revisiting the Power of Prompt for Visual Tuning, ICML 2024

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper furthers the research on Parameter Efficient Fine Tuning on the visual tasks. PEFT optimizes a large scale model by selecting a small set of parameters. This work refines the Visual Prompt Tuning by leveraging the cross attention between the prompt and embedded tokens. Further the model uses weight sharing mechanism for better representation capacity of the cross attention. This work performs evaluation on 25 datasets for number of downstream tasks. PEFT fine-tuning can be adapter or prompt based. The adapter based methods generally outperforms the prompt based fine-tuning methods.  This paper also achieves results comparable to adapter based fine-tuning methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper well explores the shortcomings of Visual Prompt Tuning (VPT) to amend it in this work for visual tasks. 
2.  This work shows the validity on the image classification and segmentation tasks by benchmarking on VTAB-1K, FGVC and ADE20K.
3.  The ablation study in the cross-attention location is helpful.

Weaknesses:
1. The conclusion seems to more of an abstract. 
2. The implementation details can be described with more details.
3. Although the authors performed a great ablation on the cross-attention, an ablation for the self attention would have been interesting.
4. One of the base cases with null text can provide a better understanding for the effectiveness of this method.

Limitations:
In this paper, the authors discuss the limitations on the Section:5 Conclusion, where they mention about taking the same initialization strategy as VPT. VPT discusses different strategies on initialization for better optimization.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oW6s6zFYj9;"REVIEW 
Summary:
This work belongs to ANN2SNN and proposes a novel coding scheme and neuron model to enhance the efficiency and accuracy of Spiking Neural Networks (SNNs) while reducing energy consumption. The Stepwise Weighted Spike (SWS) coding scheme improves information encoding by stepwise weighting input signals and introducing negative pulses, reducing the number of coding spikes needed. The Ternary Self-Amplifying (TSA) neuron model further enhances accuracy by progressively weighting the input through residual membrane potential adjustments and incorporating negative residuals and thresholds. Introducing silent periods allows the neuron to receive more input information before firing, significantly improving accuracy with minimal latency. Experimental results on datasets like MNIST, CIFAR10, and ImageNet demonstrate that the SWS coding scheme achieves better performance with fewer coding and computing steps, performing well even in very deep SNNs and achieving accuracy comparable to Artificial Neural Networks (ANNs) with the same structure.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Originality: This work introduces the Stepwise Weighted Spike (SWS) coding scheme, which is a novel approach in the field of Spiking Neural Networks (SNNs). The proposed method compresses spikes by weighting their significance in each step of neural computation, which enhances the performance and reduces the energy consumption of SNNs. Ternary pulses are a relatively new method in SNN, so the improvement of the ternary SNN encoding method has a relatively high degree of originality.

Quality: The paper provides a comprehensive set of experiments to validate the proposed SWS coding scheme. These experiments demonstrate that the SWS coding scheme significantly reduces operations and latency compared to existing neural coding schemes. The paper outlines the parameters used during training and provides justifications for the chosen experimental settings. 

Clarity: The introduction of the paper effectively motivates the work by discussing the limitations of current SNN coding schemes and proposing SWS as a solution. The methodology is clearly presented, with detailed descriptions of the new coding scheme and the Ternary Self-Amplifying (TSA) neuron model. Important symbols and their meanings are well-explained, contributing to the overall clarity of the paper.

Significance: The paper makes a significant contribution by proposing the SWS coding scheme, which enhances the efficiency and performance of SNNs. This new method addresses critical issues such as high latency and energy consumption in existing coding schemes, making it a valuable addition to the field. By improving the encoding of information in spikes, the SWS scheme has the potential to advance the development of more efficient and lower-power computing systems, thereby providing new options for the choice of coding schemes in SNNs.

Weaknesses:
In the ImageNet experiments in Table 2, SWS and other comparative ANN-SNN methods used different baselines, which is why the '$SNN\  Acc$' results are much higher than those of the comparative methods. However, the ‘$\Delta ACC$’ does not seem to show a significant difference (except for Hybrid training and Spiking ResNet). Using the same network architecture and pre-trained weights would be more credible.

Limitations:
The authors have not explicitly addressed the limitations or potential negative societal impacts of their work. To improve the transparency and completeness of their research, the authors could consider the following constructive suggestions:

1)  Limitations:

Create a dedicated ""Limitations"" section in the paper to discuss any constraints, assumptions, or potential weaknesses of the proposed SWS coding scheme.

Reflect on the robustness of the results to violations of assumptions, such as noiseless settings, model specifications, or dataset dependencies.

Discuss the scope of the claims made in the paper, including the generalizability of the approach across different datasets and scenarios.
Address factors that may influence the performance of the SWS coding scheme, such as computational efficiency and scalability with varying dataset sizes.

Consider possible limitations related to privacy and fairness concerns in the implementation of the SWS coding scheme.

2)  Negative Societal Impact:

Explicitly acknowledge the potential negative societal impacts of the SWS coding scheme, such as privacy risks, fairness considerations, or unintended consequences.

Discuss how the technology could be misused or lead to harmful outcomes, even if not intended by the authors.

Consider mitigation strategies to address any identified negative societal impacts, such as controlled release of models, monitoring mechanisms, or additional safeguards.

Emphasize the importance of ethical considerations and responsible deployment of the SWS coding scheme in real-world applications.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel Stepwise Weighted Spike (SWS) coding scheme designed to improve the efficiency of Spiking Neural Networks (SNNs) by compressing spikes and weighting their significance in each step of neural computation. This method addresses the issues of high delays and energy consumption associated with existing SNN coding schemes, as well as the complexity of neuron models and training techniques. The authors also introduce a Ternary Self-Amplifying (TSA) neuron model, incorporating a silent period to support SWS-based computing. This model is designed to minimize the residual error resulting from the stepwise weighting process. The experimental results provided in the manuscript demonstrate that the proposed SWS coding scheme significantly outperforms existing neural coding schemes, particularly in very deep SNNs. Key improvements include reduced operations and latency, enhanced overall performance, and lower energy consumption.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	Innovative Approach: Introducing the SWS coding scheme and TSA neuron model is innovative.
2.	Performance Improvement: This paper provides experiments showing that the proposed methods outperform existing coding schemes regarding both performance and energy efficiency.

Weaknesses:
1.	Clarity and Detail: Some sections of this paper could benefit from more detailed explanations, particularly in the description of the SWS coding scheme and TSA neuron model. This would help in understanding the underlying mechanisms and their advantages.
2.	Comparative Analysis: While the experimental results are promising, there is no proof from the experimental results that the encoding method proposed is more advantageous.
3.	There are some grammatical errors in the paper. Such as the second paragraph of Section 3.3, ""The neurons only integrates input and performs stepwise weighting"". It is recommended that a uniform representation be used for ""spike"" and ""pulse"".
4.	Symbol design problem, ""t"" in Eq. (3) becomes ""n"" in Eq. (5).
5.	There are many long paragraphs and sentences in the paper, making it difficult for readers to accurately understand the meaning of the paper.
6.	The description of the problem in the third paragraph of Section 1 and the end of Section 2 is not clear, making it difficult for readers to understand the problem that the article really wants to solve.
7.	The description of the encoding method in Eq. (7) is difficult to understand. According to Eq.  (7), the encoded value $A_j$ should have no time step. However, in the experimental part, the method of this paper has 8 time steps.

Limitations:
The paper mentioned that due to the setting of the neuron's silent period, the delay increases. It can be seen from the experiments that the overall latency of the method is lower, which can be regarded as solving this limitation. At the same time, this article does not have potential negative social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new coding scheme called Stepwise Weighted Spike (SWS) coding scheme for spiking neural networks to enhance the efficiency and reduce the number of operations and thus energy consumption. The SWS coding scheme tackles challenges associated with temporal and rate coding, such as heightened latency and energy usage. It achieves this by compressing spikes and assigning them varying weights at each computational step. Additionally, the paper introduces the Ternary Self-Amplifying (TSA) neuron model, which incorporates a silent phase to mitigate residual errors arising from the weighting procedure.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The SWS coding scheme enhances information capacity and reduces the number of spikes, leading to lower energy consumption and higher accuracy as compared to other coding schemes. The effectiveness of this approach is demonstrated using different datasets.

Weaknesses:
1. Which model of a spiking neuron is being employed in equation 3 (line 120)? What is the reset mechanism here after the neuron fires? Are the weights allowed to have negative values? The description of the model is unclear. 

2. The notion of residual error intuitively makes sense but it is confusing. Please define the residual error mathematically (line 139) for better understanding. 

3. Why ANN-(sws)SNN conversion is opted instead of directly training the SWS based SNN?

4. There are some recent works [1,2,3] with TTFS encoding which claims better results in regard to energy-efficiency and low-latency. First, these works need to be cited in the related work section. In my opinion, a detailed comparative analysis with other models and encoding schemes (for instance with [1,2,3]) needs to be carried out. 

[1] Göltz, J., Kriener, L., Baumbach, A. et al. Fast and energy-efficient neuromorphic deep learning with first-spike times. Nat Mach Intell 3, 823–835 (2021).

[2] I. M. Comsa, K. Potempa, L. Versari, T. Fischbacher, A. Gesmundo and J. Alakuijala, ""Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function,"" ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 8529-8533, doi: 10.1109/ICASSP40776.2020.9053856.

[3] Stanojević, Ana et al. “An Exact Mapping From ReLU Networks to Spiking Neural Networks.” Neural networks : the official journal of the International Neural Network Society 168 (2022): 74-88.

Limitations:
There is no potential negative societal impact and and one limitation related to the inclusion of silent period is noted in the main text.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a novel encoding method called Stepwise Weighted Spike (SWS) and a corresponding new neuron model named Ternary Self-Amplifying (TSA) for classification tasks utilizing the ANN2SNN training method. The proposed SWS encoding method assigns weights to the importance of spikes at each time step. The TSA neuron, which employs the SWS encoding method, features a lower threshold and includes a silent period.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Comprehensive method analysis: the authors conduct a thorough analysis of the Stepwise Weighted Spike (SWS) process, proposing a lower threshold and a silent period method to address residual error issues.

2. Superior Performance: the proposed method demonstrates superior performance in the field of ANN2SNN classification tasks.

Weaknesses:
1. Effectiveness of SWS: Various encoding methods, such as rate encoding and Time-to-First-Spike (TTFS) encoding, can be applied to different neurons and models. However, as illustrated in Figure 5, the SWS encoding method alone is ineffective without incorporating a lower threshold and a silent period. It only functions effectively when a neuron employs SWS encoding along with these additional components. Therefore, the paper should emphasize the neuron model rather than the encoding method, as it is not a universally applicable approach.
2. Lack of Experiments: The ablation study shows that the introduction of a silent period is the primary contributor to the improved performance. This raises doubts about the effectiveness of the SWS encoding method itself. Can the authors provide performance metrics for rate encoding combined with a lower threshold and silent period (if applicable) to ensure a fair comparison?

Limitations:
See weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a new spike coding scheme, which allows them to directly convert quantized ANN to their coding scheme. They demonstrate the effectiveness of their conversion on several pre-trained ANN with minimal loss in performance at the cost of an increase in latency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- strong experimental results
- coding scheme appears to be novel

Weaknesses:
- limited connection to spiking neurons, a more straightforward motivation would be a temporal encoding of quantized ANN

Limitations:
- method only applicable to conversion from pre-trained ANN
- no demonstration of training of a model using this coding scheme.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
p32gjG4yqw;"REVIEW 
Summary:
This work generalizes the ridgelet transform to equivariant neural networks, providing constructive proofs of universality in the general case as integrations over parameter distributions. Although such a direction had been taken up in prior work [33], they generalize it from scalar activations to vector activations, therefore encompassing more practical equivariant networks. The authors consider the form of the ridgelet transform for deep networks, and groups including the affine group and orthogonal group.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The authors provide a constructive universal approximation result, which is in contrast to many non-constructive universality results. They strictly improve on the past work of Sonoda et al [33] by extending from scalars to vectors, which is more realistic. They consider the implications of their framework on depth separations for equivariant networks.

Weaknesses:
Significance/novelty: The novelty relative to Sonoda et al [33] is limited, and the significance of this work to the universality and equivariance literatures is unclear. For example, many universality results already exist in equivariance (see e.g. work by Yarotsky [3], by Dym et al [2], etc.) — it is not clear how much value this extension of the ridgelet transform adds. 


Clarity: I found the writing of the paper extremely hard to follow. It did not provide sufficient background on the ridgelet transform, universality results for equivariant networks (whether constructive or non-constructive), or perhaps most importantly, motivation for why one should value constructive approximation theorems for equivariant networks. It felt that one had to have read the previous works by Sonoda et al, in order to grasp why this work was important or where its novelty was, such as how vector-valued equivariant feature maps are superior to scalar-valued feature maps, what exactly formal networks are, what the practical use or theoretical value of the ridgelet transform is, etc. The work would also benefit from an outline of the sections earlier in the paper, and a more concise and early statement of what the authors consider their main theorem/s. It was not clear what the central result about the ridgelet transform was, as the transform seemed to still involve an integral in all equivariant cases, without simplification. 

As a demonstration of the power of their theoretical formulation, the authors claim to show a depth separation, in which some class of networks is exponentially wide when shallow (constant number of layers), and only linearly wide when deep (linear number of layers). However, it is not clear whether they show that any shallow network is exponentially wide when representing a given function, or just the one constructed by the ridgelet transform — is this a strict depth separation?

Mathematical rigor: Although I did not check all of the math, some glaring errors stood out to me. First, the proof of Lemma 5 begins with, “Recall that a tensor product of irreducible representations is irreducible.” This is incorrect — for example, the tensor product of the irreps of the group of 3D rotations, SO(3), are reducible, and the irreps that appear in the decomposition of their tensor products are famously given by the Clebsch-Gordan coefficients (see e.g. [1]). Moreover, in the limitations section (6.1), the authors discuss the assumption that the group is locally compact, but say that this “excludes infinite-dimensional groups”. Yet, this is also false: for example, the infinite group SO(3) is compact (and therefore locally compact). In fact, several of the authors’ examples pertain to infinite groups, such as the affine group. These errors are surprising. 

Also, the mathematical techniques themselves do not appear to be novel (for instance, Schur’s lemma is quite standard, and the proofs included in the main body are rather simple — Lemmas 1 and 2 are in fact widely known), and there are no experiments or practical implications, so the merit of the paper must rest on the significance of the results themselves. Unfortunately, the the broader significance of the results are not clearly demonstrated. The authors claim to reveal “the close relationship between machine learning theory and modern algebra,” but the mathematical tools they use seem like the standard ones used already throughout the equivariance literature. I am not sure what the “major upgrade to machine learning theory from the perspective of modern algebra” will therefore be. 

[1] Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network by Kondor, Lin, and Trivedi 2018

[2] On the Universality of Rotation Equivariant Point Cloud Networks by Nadav Dam and Haggai Maron 2020

[3] Universal approximations of invariant maps by neural networks by Dmitry Yarotsky 2018

Limitations:
Yes, the authors discussed limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a unified approach to universal approximation theorems for neural networks using group representation theory. It extends to vector-valued joint-group-equivariant feature maps, providing a systematic method for both shallow and deep neural networks with nonlinear activation functions. By leveraging Schur's lemma, the paper shows that these networks can universally approximate any function within a certain class. It main contribution is the closed-form ridgelet transform, which offers a constructive proof and explicit parameter distribution for these networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The paper introduces a unified constructive universal approximation theorem that applies to both shallow and deep neural networks using group representation theory. This is an innovative approach. It also extends previous work by incorporating vector-valued joint-group-equivariant feature maps.

2.  The paper is theoretically sounding, leveraging concepts from group representation theory and Schur's lemma. They perform the thorough and systematic development of the ridgelet transform, providing a closed-form solution for parameter distributions and ensuring the findings are theoretically well justified.

3. The paper is well-structured and clearly written. Definitions, theorems, and proofs are presented in a coherent manner, making it easier for readers to follow the details of the argument and understand the implications of the results.

4.  This work is significant since it provides a relationship between deep learning theory and modern algebra. By providing a unified framework that applies to a wide range of network architectures, the paper incentivize further research and development in the field of machine learning.

Weaknesses:
1.  While the paper is strong in its theoretical contributions, it lacks empirical validation through experiments or simulations. Demonstrating the practical applicability and effectiveness of the proposed ridgelet transform and the unified framework on real-world datasets or benchmark problems would strengthen the paper. Including even a small set of experiments could provide evidence of the practical relevance and performance of the theoretical results.

2. This work makes several assumptions, such as the local compactness of the group \( G \) and the boundedness of the composite operator \( \text{NN} \circ R \). While these assumptions are standard in group representation theory, the paper could benefit from a more detailed discussion on their implications and limitations. Exploring scenarios where these assumptions might not hold or providing guidance on how to relax these assumptions.

3. Some of the technical details, particularly those related to advanced concepts in group representation theory and the ridgelet transform, might be challenging for readers who are not experts in these areas. Providing additional intuitive explanations, diagrams, or examples to illustrate these concepts could enhance the clarity of the paper.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a generalization of the work by Sonoda et al. by extending their formulation of universal approximation theorems applicable to a specific class of neural networks namely scalar-valued joint-group-invariant feature maps for ""formal deep network"" to a much larger class of learning machines. Their theory using tools from group representation theory allows them to uniformly treat both shallow and deep neural networks with a larger class of activation functions. They provide an explicit construction for parameter assignment (aka Ridgelet Transform) and apply it to vector valued joint group-equivariant feature maps.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is well motivated and the writing is clear and understandable. The interspersed explanations in plain english are quite helpful in understanding a paper that leans quite heavily on sophisticated mathematical formalisms. (eg line 93-94). 
- The proofs and the notation are clear and succinct.
- The authors extend an earlier work to a much more practical and real world class of NNs by introducing *vector-valued joint group-equivariant* feature maps, which yields universal approximation theorems as corollaries. They also unify the treatment of both shallow and deep networks by leveraging Schur's Lemma.
- They provide explicit examples for depth 2 and depth $n$ fully connected network with an arbitrary activation in Section 4.2 which helps ground their method and significantly helps the reader understand how to leverage the tooling introduced by the authors.
- The paper provides formal support for the popular interpretation for the efficacy of DNNs compared to shallow networks, namely that they construct hierarchical representations which would take an exponential number of neurons to represent using a single layer.
- The limitations section is well written and is explicit about the assumptions made so that the reader is aware of the regime in which the proofs are applicable.

Weaknesses:
**Major**
- The biggest weakness of the work seems to be that it shares a vast amount of technical analysis, machinery and the fundamental proofs are shared with the earlier work by Sonoda et al. While the extension to a larger class of networks and the introduced vector values feature maps is certainly valuable, I am not fully convinced of the differential novelty of the work. Most of the (valuable) effort has been spent in a mostly natural extension of the previous work on the topic.


**Minor**
-  The authors mention that assumption (5) (that the network is given by the integral representation) in limitations is potentially an ""advantage"". If that is so, a discretized version would be the preferred model since it is also closer to real world NNs
- Typo on line 77  - mathmatical -> mathematical
- Typo on line 310 - cc-universaity -> cc-universality
- lines 135 - 137 would be significantly easier to read when broken into multiple lines

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
opdiIAHfBr;"REVIEW 
Summary:
The authors proposes a method to create a universal feature space using brain fMRI response prediction as a training objective. The key idea is that deep networks trained with different objectives share common feature channels that can be clustered into sets corresponding to distinct brain regions, revealing visual concepts. By tracing these clusters onto images, semantically meaningful object segments emerge without a supervised decoder. The paper employs spectral clustering on the universal feature space to produce hierarchical visual concepts, offering insights into how visual information is processed through different network layers. The two main insight being the localization of emerge of foreground/background features, as well as interesting visualization of class-specific concepts using the top spectral-tsne egeinvectors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
First, congratulations to the authors, I liked reading this paper, and I think the experiments and the core is great:

1. Using brain voxel response prediction to find the common space between model is interesting and novel
2. The author propose visualizations of what we could interpret at brain-activations subspace
3. I like the idea for the visualization of how visual concepts emerge and transition through different layers of various models. However, i have concern on it's validity (see weakness)
3. Nystrom-like approximation show the authors thought about scaling their methods

Weaknesses:
Nevertheless, this paper has problems, some more important than others. 

So I will separate them into major problems (**M**) and minor problems (_m_). I want to make it clear that for me, all these problems are solvable and do not detract from the quality of the paper.

Let's start with what I think are the Major problems (**M**):

**M1**. Related Work Quality (Page 9, Section 4):
- The related work section is critically weak, with **only 24 references** and lacks depth in discussing relevant literature. The paper misses an entire set of works on (1) concepts xai, (2) alignment of brain and activations (3) study of representations and (4) attributions methods, which are either crucial or should be mentioned for this study. **A significant rewrite is necessary** to properly position the paper within the existing body of work. 

**M2**. Validity of t-SNE for Distance Measures (Figure 9):
- The paper uses t-SNE for analyzing bifurcation in feature space, but t-SNE is known to distort distances. This raises concerns about the validity of conclusions drawn from t-SNE plots regarding feature bifurcation.


Now for the minors problems:

_m1_. Redundancy of Discovery Claim (Page 2, Line 25):
- The claim that channel feature correspondence exists across networks is not new, as it has been extensively studied in major works (eg using CKA, RSA...), please update and compare your work to this litterature.

_m2_. Reliance on Channels (General):
- Channels are not necessarily the best basis for analysis, as recent researchs suggests there are better ways to represent features, directions (neuron is not a great basis). The paper should address why it continues to rely on channels, considering the limitations.

_m3_. Orthogonality Assumption (General):
- The paper's assumption of orthogonality in feature decomposition does not align with current understanding, especially regarding the neural collapse phenomenon in late layers. Say it otherwise, all point for the class tench are nearly collapse in the latest layer, relaxing othogonality (e.g dict learning) may be a good idea (althought if i am correct, the nystrom approx should not yield perfectly orthogonal vectors). This should be discussed.

_m4_. Parameter Sensitivity (Appendix):
- As always when we have hyperparameter, i expect a small discussion discussing  the effect of changing the parameters (λ eigen, λ zero, and λ cov). This would help understand the robustness of the method to these hyperparameters.

_m5_. Direct t-SNE Application (General):
- The paper uses eigenvectors for t-SNE. It would be more straightforward to apply t-SNE directly to the data, and the paper should justify the chosen approach.

Limitations:
Yes, the limitations identified by the authors are accurate and well-documented. Regarding the weakness I mentioned, I reserve the right to increase the score if the authors adequately address my major concerns.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a method to align different vision models' features to a common space, and to discover interpretable features as clusters in this space. 
The alignment is done by learning linear mappings from features to fMRI activations, the intuition being that the human visual cortex provides a meaningfully structured space, in which locations have known properties (e.g. different regions are known to respond to specific concepts) and are thus readily interpretable. Once the features have been linearly aligned to this common space, they are treated as a weighted, fully connected graph, wherein each image patch is a node, and edge weights (affinities) are computed based on the cosine similarity between the features in each node. A standard spectral clustering method (Normalized Cut) is used to compute a soft partition of this graph into sub-graphs (clusters). As performing this clustering on the full graph would be computationally infeasible, the authors propose to cluster a sub-sample of the graph, and then propagate the resulting clusters to the K nearest neighbors of each subsampled node. Finally, in order to learn linear mappings that preserve the quality of the clusters, a regularization term is added to the reconstruction loss, which ensures that spectral clustering eigenvectors are preserved across the mapping, based on a subsample of nodes.
This method is used to visualize, for each layer of three different models (MAE, DINO and CLIP), the concept that each image patch is assigned to, coded as a color. The 20 top eigenvectors are reduced to 3 dimensions using t-SNE, and these 3D vectors are shown as RGB colors. This visualization reveals that CLIP and DINO produce maps that are close to uniforms in the first 4 layers, suggesting that figure-ground segmentation only emerges in later layers. MAE, on the other hand, shows signs of segmentation from earlier layers. The segmentations extracted from the models in this way are evaluated on the ImageNet-segmentation benchmark, confirming that in CLIP (the model that showed the strongest segmentation) the segmentation emerges at layer 4, and plateaus afterwards. Using the PASCAL VOC benchmark, which also includes category labels, CLIP is also found to encode categorical information, which peaks at layers 9 and 10. In another analysis, a discovered ""figure/ground"" concept is visualized by averaging its activation within the ""figure"" and ""ground"" regions (based on the ImageNet-segmentation ground truth labels) and plotting it on the surface of the brain, showing that areas known to encode objects, faces or bodies tend to respond more to the foreground, while scene-selective areas more to the background. This figure/ground concept is found to be agnostic to object category, and to an extent, consistent across models. In the next section, concepts corresponding to different object categories are visualized on images, and on the surface of the brain. Finally, a 2D t-SNE visualization of the evolution of the features across layers shows a bifurcation between figure and background as the layer depth increases, in both CLIP and DINO.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The paper proposes to use the human brain as a shared space in which to evaluate different models: this is a clever intuition, which might prove useful for interpreting differences between models.
- It shows that spectral clustering can be a well-suited method for grouping the features of vision models, and in particular ViTs, into meaningful clusters.
- It proposes a clever modification of an existing subsampling-based method for graph clustering, by using K nearest neighbors.

Weaknesses:
- The overall concept is not made clear in the Introduction. The method is relatively simple conceptually, consisting of a step in which multiple models' features are aligned to the brain to provide a common reference frame, followed by clustering of the features within this common space. The Introduction does not make this pipeline clear. Specifically, while the alignment into a common space is clear, the clustering procedure is not explained: first, the authors evoke neuroscientific ideas on lines 39-41, without discussing how these relate to the problem of clustering features, nor even that the goal of the current work is to find clusters of features. Subsequently, on lines 42-47, they discuss the problem in terms of ""graph edges incidents on each pixel"", and ""channel grouping hypothes[e]s"". While this is partly a matter of subjective taste, I believe that discussing the problem at hand in terms of reducing the dimensionality of features at each location (image patch), thus finding a small number of dimensions (combinations of features) which can explain the affinity structure between patches, would be more easily understood by most readers.
- Several key details about the methods are left out: for example, almost no information about training (learning rate, optimizer used, batch size, number of epochs) is provided.
- Related to the previous point, as the Appendix contains several important methodological details (such as the use of additional regularization losses) but is never referred to in the main text, the authors should add references to it where relevant.
- A key component of the proposed method is the learning of a mapping of different models' features into a common space, and as shown in Figure 3, this does indeed result in the cosine similarities of different channels' activations becoming more similar across models. At the same time, the goal of the method is to uncover differences between models. The authors should include an explicit discussion of what kind of model differences are likely to be preserved, and which are likely to be destroyed in the alignment process. As a possible suggestion in this direction, the paper makes several references to the models' features before alignment (for example comparing their segmentations' evaluations with the aligned features in Figure 5), but these features are never visualized. A direct comparison of each model's (clustered) features before and after alignment would be very informative.
- The feature clusters are visualized by reducing their dimensionality to 3D using t-SNE, and visualizing the resulting 3D features as RGB colors. This visualization, however, is not easily interpretable, as different channels are conflated together by the additional dimensionality reduction. Visualizing single channels separately might be more useful to understand the nature of the discovered clusters. Was there a specific reason for choosing the 3D t-SNE visualization rather than showing single channels?
- Overall, it is not clear what the discovered channels can tell us about the models. The single interpretable channel that is discussed in depth in the paper is figure/ground. While this provides a good sanity check on the meaningfulness of some of the discovered features, the ability of vision transformers to segment objects has been observed in several papers (e.g. Melas-Kyriazi et al. 2022, Xu et al. 2023), and the different responsiveness of different regions in the visual cortex to figure and background is well established. Other concepts revealed by the method (the category concepts in Figure 8) are shown on the surface of the brain, but it is hard to interpret what these brain maps mean. The authors should make the questions that can be answered using the proposed method clear and explicit.
- The paper fails to cite closely related work in the Related Works section. Particularly, it cites mechanistic interpretability work in other fields, such as language, but not the recent rich line of work that has specifically looked at vision transformers' ability to perform specific visual tasks. The two papers cited above (Melas-Kyriazi et al. 2022, Xu et al. 2023) are a good example, the former in particular as it proposes a segmentation method based on spectral clustering which is very close to the present one. Another relevant paper is El Banani et al. (2024), which looks at 3D-related tasks. As this is not my field of expertise, I am not aware of papers that look for meaningful directions in the space of vision transformers' channels, but I would be surprised if this didn't exist. I would recommend the authors to do a more exhaustive literature search to find papers that more closely relate to the method proposed here.

- In Figure 7, the figure-ground visual concepts discovered by different models are plotted on brain maps. In the text, the authors write that ""the foreground or background pixels activates similar brain ROIs across the three models"". However, a glance at the brain maps reveals similarities, but also differences. A statistical evaluation of the similarity between different models' brain maps would be recommended.

**References**

El Banani, M., Raj, A., Maninis, K. K., Kar, A., Li, Y., Rubinstein, M., ... & Jampani, V. (2024). Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21795-21806).

Melas-Kyriazi, L., Rupprecht, C., Laina, I., & Vedaldi, A. (2022). Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8364-8375).

Xu, J., Liu, S., Vahdat, A., Byeon, W., Wang, X., & De Mello, S. (2023). Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2955-2966).

Limitations:
As I wrote in the ""weaknesses"" section, I believe the precise scope of the method (what kinds of questions can and cannot be answered with it) has not been properly acknowledged and discussed by the authors.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In the domain of interpretability research, this paper aims to make a mark by proposing AlignedCut, a method to discover shared and expressive visual feature spaces across networks by aligning those spaces with neural responses in human brains. The method is quite interesting - channel-wise responses to images are aggregated and ""feature clusters"" are formed on the basis of the functional connectivity between the pixels and linear combinations of channels. These linear combinations are acquired by predicting neural responses to the same images. The feature space spanned by the neural responses is considered the universal feature space. The eigenvectors corresponding to those feature clusters help us visualize what parts of the image the networks rely on to encode which concept, thus providing an interesting interpretability lens. The most striking example presented is how figure-ground segmentation can be interpreted as a mapping between the input and specific channels in various networks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Originality
- The AlignedCut method is new to me - and is super interesting - however, I am not an expert in that specific sub-field so I am not sure of its novelty.
- The interpretability lens on figure-ground segmentation is very informative, however, again I cannot judge its novelty.

Quality
- The authors present plenty of analysis to demonstrate the power of their method, which helps in inspiring some confidence in the claims.

Clarity
- The methods and results are relatively clear and the authors provide useful context at the start of each section.

Significance
- Linking pixels to visual features, parameterized through network activations and neural activations opens doors in interpretability research.

Weaknesses:
I see three major weaknesses:

1. The necessity of the brain is unclear to me. Instead of aligning features to the brain, you could've aligned the features of the different networks to each other - creating an ""emergent"" universal feature space. Would your results, e.g. w.r.t. the figure-gound segmentation, change much if you do so? If not, what does bringing the brain into play buy us here in terms of network response interpretability? This is unclear to me.

2. Most of the results need robustness checks. For e.g., in Fig. 6 you show foreground vs background difference in neural response associations. Presumably, that's an average across a lot (all?) of images. Could you indicate some sign of robustness, for e.g, running a permutation test to assess how likely the differences you see would've been expected given the data statistics alone? Same holds for Figs. 7 and 8. We need to know if these differences are flukes or not.

3. Reliance solely on ViTs. To make your point more general, showing that a high-performing CNN shows the same results would be very informative. ViTs have more expressivity in terms of patches interacting with each other - perhaps figure-ground segmentation isn't as strong in CNNs (although if previous research is to be trusted, CNNs should have some notion of figure-ground segmentation; see Hong et al. NatNeuro 2016 and Thorat et al. SVRHM 2021). 

Refs:
- Hong, Ha, et al. ""Explicit information for category-orthogonal object properties increases along the ventral stream."" Nature Neuroscience 19.4 (2016): 613-622.
- Thorat, Sushrut, Giacomo Aldegheri, and Tim C. Kietzmann. ""Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization."" SVRHM 2021 Workshop @ NeurIPS.

Limitations:
The authors mentioned methodological limitations. It is sufficient.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method to interpret deep learning models using brain data. The two apparent contributions are that (1) this new model is able to align channels activations from different layers of different models into a universal feature space, and (2) a Nystrom-like approximation is introduced to speed up spectral clustering analysis.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
I have to be very transparent in this review. Unfortunately, I found extremely difficult to follow this paper, and I was not able to understand its different components; as a consequence, I don't feel capable to evaluate what the possible strengths of this work are.

Weaknesses:
As I said in the previous section, I was not able to understand the methodological details of this paper. A lot of terms and concepts are used throughout the paper without a proper explanation, and as a consequence I cannot honestly understand what is going on with the methods of this paper to properly evaluate it. This paper seems to have a lot of work in it, so I want to believe that what's happening here is that (1) this is one of the first - if not the first - paper from the authors, and thus they lack the experience to explain what they did in a way that their peers can understand, (2) a lot of the concepts used are seen by the authors as very obvious jargon from the subfield, and thus the problem is that I'm not familiar with this subfield, or (3) both. I'm leaving my doubts in the next section, in the hope that they will allow us to understand better whether my understanding difficulties are related to points (1), (2), or (3). As a consequence, I'm rating this paper as a borderline reject, hoping that during the rebuttal period the authors will have time to tackle this readability issues, and as a result I'll be able to properly reassess this work.

Limitations:
Limitations of this work are mentioned in the Conclusion section, but no discussion about the potential negative impact of this work is presented.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
ojFEP11Cqj;"REVIEW 
Summary:
The authors propose a energy-based generative boosting method. They try to maximize the log-likelihood functional delta with a second-order expansion. This lead to a boosting algorithm with deltas as steps in the log-likelihood. Instead of scaling the steps with a fixed predefined value (as a anti-overfitting hyperparameter), they rely on linear search to get better performance. They initialize f0 as uniform and use trees as weak learner to learn the deltas. They derive the objective with trees, it is well explained and looks similar to other second-order method objectives like XGBoost. They use MCMC to sample from Q(x) as is typical from energy-based approaches. They use Gibbs so they only need to sample from one dimension while keeping the rest constant. However since there are t trees, this is quadratic in t. They use some form of accept/reject to sometimes accept previous iteration samples in order to not have to re-samples new samples given the quadratic cost. They include a probability of refresh in order to not just always accept old samples. They propose interesting ways of regularizing the approach. They provide good literature review of related methods. It seems like Section 4 could be integrated with the related work section.

Figure may appear unimpressive to a generative expert unfamiliar with trees. But, being able to generate good samples from MNIST using only decision is an impressive feat (even if MNIST is downsampled). This paves the way for trees being used on more complex data.

Table 2 shows nice prediction results, and they do extensive hyperparameter tuning. However, ML Efficiency is not the best metric, it only focus on prediction. A generative model could produce low-diversity fake data that lead to good classifiers. I would recommending adding some distribution metrics such as the Wasserstein distance (which is computable in low-dimension and is used for high-dim data such as images/videos in the form of the widely popular FID/FVD). There are also other useful metrics for quality and diversity, I recommend looking the extensive choice of tabular-data metrics described in https://proceedings.mlr.press/v238/jolicoeur-martineau24a.html. However, the only essential one in my opinion is having a distance in distribution. It can be the Wasserstein distance or something like the MMD distance.

Except for the missing distribution metric, everything else in the results is good and the methodology is sound with good hyperparameter tuning. I would encourage the authors to add a distribution metrics.

The method is sound, novel and quite interesting. The presentation is very well made. Being the first tree method achieving good image data samples is impressive in my opinion. This is a very high quality paper.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The method is sound, novel and quite interesting. 

The presentation is very well made. 

Being the first tree method achieving good image data samples is impressive in my opinion. 

This is a very high quality paper.

Weaknesses:
Missing a distribution metric, the ML efficiency is not a adequate metric on its own.

Limitations:
If the authors had to downsample MNIST from 28x28 to lower, than there are scaling limitations that should be added.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a boosted tree algorithm that performs distribution learning using an energy-based formulation. Inspired by methods like XGBoost, it is claimed to achieve high performance not only in generative ability but also in discriminative performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed method incorporates techniques that can be leveraged because they inherit from tree boosting models, such as the approximate sampling algorithm. Moreover, as a model that possesses not only generative quality but also discriminative performance, it has a wide range of applications.

Weaknesses:
This study is not theoretical; therefore, the validity of the proposed method must be confirmed through experiments. However, there are some questions regarding the experimental settings. 

Also, from an algorithmic perspective, since methods unique to tree ensembles are incorporated, they could potentially offer advantages in terms of computational complexity compared to other methods. However, evaluations regarding efficiency have not been conducted. If there are advantages, it seems opportunity loss. 

Please refer to the Questions section.

Minor:
The evaluation of variance is presented separately in Appendix G, but it is difficult to compare. Therefore, I would like it to be summarized in one table.

Limitations:
Computational cost of sampling is larger than existing methods.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed an energy based generative boosting algorithm analogous to XGBoost, which can be used as generative model as well as be applied to discriminative tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The energy-based boosting is novel. The proposed method is capable of both generative sampling and discriminative tasks, enabling broad methodological applications.

Weaknesses:
My concerns regarding the proposed method and the experiments are detailed in the questions section.

Limitations:
I do not identify significant limitations other than the ones discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes to extend the success of tree-based methods in discriminative tasks to generative modelling, which is implemented via an energy-based generative boosting algorithm (NRGBoost). Specifically, NRGBoost directly extends the tree-based tabular models by replacing the discriminative objectives with a generative one, which seems novel to me.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper has well-founded rationales: (1) Tree-based models are performant in discriminative tasks, and thus they are highly likely to also be performant in generative tasks on tabular data, and (2) existing tree-based generative methods do not preserve the tree structures well.
2. The paper is well-written, especially the notations.

Weaknesses:
1. **[Important]** Some highly relevant benchmark methods are missing, including ARF (tree-based) [1], GOGGLE (diffusion) [2] and TabPFGen (energy-based) [3].

2. In Line 325, the authors claim that the proposed method “significantly” outperforms other methods, while the significance test seems missing.

3. I would suggest the authors add comparison results on the computation efficiency. Because NRGBoost basically employs the same architecture as traditional gradient boosting trees, the computation efficiency should be higher than most other network-based generative models.

4. There seem to be some typos throughout the main text: “I.e.” (Line 272)

5. **[Important]** Code is not provided. I remain conservative about the results claimed in the paper.

[1] Watson, David S., et al. ""Adversarial random forests for density estimation and generative modeling."" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.

[2] Liu, Tennison, et al. ""GOGGLE: Generative modelling for tabular data by learning relational structure."" The Eleventh International Conference on Learning Representations. 2023.

[3] Ma, Junwei, et al. ""TabPFGen–Tabular Data Generation with TabPFN."" NeurIPS 2023 Second Table Representation Learning Workshop.

Limitations:
The authors detail the limitations of NRGBoost in Section 7.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
oRZN9a53ag;"REVIEW 
Summary:
This paper present novel theoretical results to identify causal effects in restricted ANMs even in case of unobserved confounders.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
**The paper provides novel contributions to the field of score-based causal discovery by extending previous works to confounded restricted ANMs. Based on these contributions, I strongly support this paper's acceptance.**

- the problem definition is very useful to orient readers
- the theoretical results are novel
- the experiments compare against a sufficient number of baselines, and though the proposed method is not SOTA, it compares well and has better theoretical guarantees


Based on the authors' response, I am eager to improve my score.

Weaknesses:
I have a few remarks on improving the flow of the paper; however, even the first four points are not considered major issues.

- Even though condition 1 is a well-known result in the causality literature, I suggest explaining why that admits linear models and including some description of **restricted ANMs** in the main text (at least for me, it is not evident, especially since the condition lacks intuition). To be clear, even this point does not diminish the main contribution, which I see regarding the results for confounders.
- As **inducing paths** are an important concept for the main contributions, please _include it in the main text_ if space permits (suggestion: you can reduce spacing in $\texttt{itemize}$ by setting $\texttt{\\\\begin\\{itemize\\}[nolistsep]}$ )
- I could **not find the definition of an active path** (not even in Def. 5, where it is said to be defined); I presume it is a path that is not blocked, but it would be better to state this explicitly. Maybe it would even be better to use ""a path that is not blocked"" instead of introducing new terminology (this is the first time I encountered ""active paths""; I could be wrong about this)
- The text is sometimes difficult to follow, due to heavy reliance on notation. I'd consider delegating the not crucial part to the appendix (potential candidates in 2.1) and using the remaining space to explain the main quantities better, especially the residuals (e.g., Eq. 12)


## Minor points
- please specify what you are calculating the expectation with respect to (using a bold E is also unconventional, though it's clear from the context it's an expectation)
- as the mathematical objects for d-/m-separation are distinguishable, you might consider dropping the superscript to simplify notation; also, I'd suggest adding whitespace after $\perp^d_\mathcal{G}$ and the like to make it easier for the reader to attribute the indices to $\perp$ and not the not on its right
- I could not find the definition for $\dot{\cup}$ 
- in the explanation of Prop 3., the wording makes it a bit hard to discern that you also provide intuition for the second part; it would help if you refer to _Part (ii)_ explicitly
- _Score matching through the roof_ in the title does not have added value for me, I'd consider rephrasing it to convey the message that ""we propose score-based causal discovery methods for confounded restricted ANMs""

Limitations:
The authors provide an **honest comparison** of their method in the experiments, clearly stating its limitations compared to other methods.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose AdaScore, a method for causal discovery that generalizes previous work based on score matching for SCMs with possibly latent nodes. They combine connections of the score to conditional independence as well as to additive noise SCMs and show that a NoGAM-type procedure works to recover the direction of non-confounded edges of the corresponding partial ancestral graph.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Adapting NoGAM [1] to the case allowing hidden variables is a practically meaningful contribution.
- Model assumptions are somewhat weakened compared to CAM-UV by allowing for general mechanisms within blocks of observed and latent parents.

Weaknesses:
The novelty of the paper lies primarily in the application of NoGAM to orient very specific edges in a partial ancestral graph (PAG) (the ancestral graph that represents the Markov equivalence class, in analogue to a CPDAG). This falls significantly short of the main contributions as described by the authors on l33--55. Specifically:

- The authors state that they show how constraints on the Jacobian of the score can be used as conditional independence testing. However, the extent to which this is done is only by noticing the equivalence between conditional independence and the corresponding zero in the Jacobian term (previously noted in [1,2]), without any formal analysis of the proposed t-test (Appendix C) as a statistical test of conditional independence (which happens to be a notoriously difficult test).

- The authors state that their identification results for additive noise models generalize the previous results obtained by previous works. In l193, the authors state ""we remove the nonlinearity assumption (of [3]) and make the weaker hypothesis of a restricted additive noise model"", but 1) this is a stronger assumption than additive noise, not a weaker one, and 2) the authors in [3] also consider the same restricted additive noise model. 

- The authors claim that AdaScore is able to handle a broad class of causal models (l54), but three out of four possible situations are direct applications of existing work. 1) Under no structural assumptions with or without latent confounders, AdaScore simply performs constraint-based causal discovery (FCI) using the conditional independence properties of the Jacobian of the score, a straightforward application of [1] also previously noticed in [2]. 2) Under an additive noise assumption, AdaScore is exactly equivalent to NoGAM. 3) Only under an additive noise assumption with hidden confounders, does AdaScore generalize NoGAM to orient unconfounded edges of the PAG returned by FCI, which may be very few of the discovered adjacencies.

### Other comments

- The experiments do not seem to suggest that AdaScore out performs other methods in any meaningful way---in fact, in Figure 1 a) AdaScore is completely equivalent to NoGAM, and is thus redundant. In Figure 1 b), where AdaScore should distinguish itself, it does not appear to be consistently better than CAM-UV. 

- Much of the paper (> 5 pages) is spent on directly describing previous works, NoGAM[3] and/or provide basic background on DAGs and MAGs. 

[1] Spantini et al., ""Inference via low-dimensional couplings."" JMLR 2018.
[2] Montagna et al., ""Scalable causal discovery with score matching."" CLeaR 2023.
[3] Montagna et al., ""Causal discovery with score matching on additive models with arbitrary noise."" CLeaR 2023.

Limitations:
The authors do not adequately discuss the limitations of their method---the limitations section in the appendix focuses purely on the empirical study. The authors claim that AdaScore is adaptive in the sense of being ""less reliant on prior assumptions which are often untestable"", but this is only in the sense that it performs different algorithms depending on user specification, which hardly constitutes one single unifying adaptive algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper extends theoretical results about causal discovery through score matching to encompass both linear and non-linear SCMs and lift the sufficiency assumption. The theoretical results relax the non-linearity assumption of Montagna et al 2023 by swapping it with the less restrictive one of restricted ANM from Peters et al 2009. As for the latent confounder detection a parallel with m-separation is drawn using results from Spantini et al 2018, to establish that the score will be non-zero in the presence of an active path. Following the theoretical results, an algorithm to estimate causal graphs from data is proposed and evaluated, generalizing the NoGAM algorithm of Montagna et al 2023, which only covers the non-linear case.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written and, if it wasn’t for some of the definitions relegated to the appendix, very easy to follow.  

The theoretical results, particularly Propositions 2 and 3 are important extensions of the score-matching methodology for causal discovery, dealing with both the non-linearity and sufficiency assumptions of the method proposed in Montagna et al. 2023.

Weaknesses:
The paper motivation is basically the weakening of current assumptions for causal discovery methods. However, assumptions and benefits of the proposed methodology are not clearly specified. In line 74, the authors state that faithfulness is assumed (I believe, it is kind of hidden in the background notions). If that is the case, the method adopts the same assumption as FCI, plus ANM. So the proposed method is relaxing assumptions compared to CAM-UV, RCD and NoGAM, but adding onto FCI. Regarding the benefits, the alleged flexibility of the method to output DAGs, MECs, MAGs, PAGs, which should make it preferable to FCI, is merely touched upon in the contributions and the experiment section.  

Proposition 1 is a rather trivial application of the more general lemma in Spantini et al. and it does not specify the required faithfulness assumption to obtain the result from Eq. 6 in the paper. 

The experimental results show limited added value according to the one metric chosen (SHD) in a synthetic setting. They are not comprehensive enough, with no application to common (pseudo-)real benchmarks (e.g. from bnlearn). More experiments and more metrics are needed as, as it stands, the proposed method seems to add no real value compared to the baselines. Additionally, it is not clear from the experiments if it is really able to identify confounders. Breaking down precision and recall by mark would show this. FCI and a random baseline should be also added for reference.  

Experiments are conducted on data with at most 9 variables, and the scalability of the method is not shown nor discussed. 

The model used to estimate residuals is not discussed, nor the assumption that the chosen model fits the data adequately to correctly estimate residuals, and what is needed to assess this.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oQc7TOCk5G;"REVIEW 
Summary:
This paper investigates the theoretical boundaries of learning with Label Differential Privacy (Label-DP) in both central and local models.
Label-DP is a weakening of standard differential privacy, where only the privacy of the ""label"" of each example is to be protected (an example is a pair (feature vector, label)).

The key contributions of the paper are to establish min-max optimal rates for excess error in the settings of:
* (multi-class) classification,
* regression with bounded labels,
* regression with unbounded labels (but under a bounded moment condition).

The min-max rates are over the class of data distributions that satisfy $\beta$-Holder smoothness, admits a lower bound on probability density that is bounded away from zero, assumes that there are no “sharp corners” in the input space, and a $\gamma$-margin assumption (in case of classification), or bounded label range or bounded label moments (in case of regression).

These min-max rates are then compared against the previously known min-max rates for learning under “full” local-DP (that protects both features and labels), as well as non-private learning.

The key takeaways are:
* Local-DP vs Non-Private:
  * For classification and regression with bounded labels, the sample complexity under Local-DP increases by a factor of $1/\varepsilon^2$, but has the same rate in terms of desired excess error. This is unlike “Full Local-DP”, where the sample complexity is larger even in terms of the desired excess error.
  * For regression with unbounded labels, the dependence of sample complexity on desired excess error is worse than the non-private setting.
* Central-DP vs Non-Private:
  * The excess error is the sum of the non-private excess error and an additional term that decays faster in the number of samples, so the additional sample complexity due to privacy is negligible for very small excess error.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper provides a comprehensive study of the min-max rates for learning under label differential privacy, in both local and central models of DP, and for both classification and regression. This complements prior literature on min-max rates for learning (non-privately) and for learning under (full) differential privacy. The rates highlight the precise cost of _label_ differential privacy and the sample complexity benefits over full differential privacy.

Weaknesses:
While there are many results in the paper, I think the proof techniques in both lower and upper bounds use mostly standard tools (This is not necessarily a weakness!).

The paper writing could be improved at several places though. Some comments are listed below under ""Questions"".

Limitations:
I do not see any potential negative societal impact of this work, as it is primarily theoretical.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the minimax rates for classification and regression under (pure) label differential privacy in both the local and central models. They prove that rates of convergence for classification and regression with bounded label noise in the local label DP model are comparable to those for the non-private tasks, except for the expected $1/\varepsilon^2$ dependence. This represents an improvement over rates for standard DP in both settings, where there is a worse dependence on the dimension of the covariates. They also prove, however, in the case of regression with unbounded label noise, the convergence rate improvements over “full” DP aren’t as meaningful.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work makes notable progress in our theoretical understanding of the costs of label DP relative to non-private and full DP algorithms for the same learning task.

Weaknesses:
The presentation could be improved in several places. Admittedly, this is written from a statistical perspective that is different from the one I am most familiar with, so some of the perceived presentation issues may just be a matter of convention, but the following changes might make this work more understandable to the general NeurIPS community:


Abstract:

The main challenge and the techniques to overcome them as stated in the abstract aren’t clear to me as a reader at this point. It’s not yet stated that the subject of interest is minimax rates, and so there’s no context for the statement “take infimum over all possible learners” and why that would present a challenge. Generally, I did not have a good idea of what the contribution of this work was from the abstract.

Introduction:

“the learning performances” -> “the learning performance” 

“the label DP” -> “label DP”

In Table 1, attribution for the full DP rates in the local DP setting as well as the rates in the non-private setting should be given in the table. Also, I think there’s an issue with the parentheses in the local label DP rates for regression with bounded label noise.

Section 2:

In the “Minimax analysis for private data” paragraph, KNLRS11 is attributed with finding the relation between label DP and stochastic queries. This is not accurate, this work characterizes local DP learning by the statistical query model.

Section 3:

“We hope that $R - R^*$ to be as small as possible” -> “we seek to minimize this risk” or something similar

“the Bayes optimal classifier and the corresponding Bayes risk is” -> “the Bayes optimal classifier and the corresponding Bayes risk are”

In Proposition 2, f(x) is used before it is defined.

Section 4:

I didn’t find the proof outline for Theorem 1 or Theorem 3 to be informative at all. It would be good to add more specifics if possible.

“Let the privacy mechanism M(x,y) outputs” -> “Let the privacy mechanism M(x,y) output”

Limitations:
Yes, the authors address the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the problems of classification and regression under the constraint of local/central pure label DP. The authors derive upper and lower bounds on the excess risk (compared to the non-private Bayes classifier/regression) for these problems, under somewhat standard assumptions on the 'ground truth' randomized label function $\eta$. For regression, both the case where the labels are bounded and have bounded moments are considered. For the lower bounds, the authors develop extensions of techniques from minimax estimation to label DP. For upper bounds, authors propose some algorithms combining 'binning' different examples with a privacy mechanism chosen according to the problem setting. The upper/lower bounds are matching in each setting up to logarithmic factors. For local label DP, the authors show the minimax excess risk with $N$ samples matches the non-private bounds using $N \min\{\epsilon^2, 1\}$ samples. In other words, with $\epsilon = \Omega(1)$ the minimax risk asymptotically matches the non-private risk, and otherwise there is an inherent separation. For central label DP, the minimax bound is one that approaches the non-private bound as $N \rightarrow \infty$ for any fixed $\epsilon$, showing a qualitative difference. For local ""full"" DP, i.e. the features are also private, even for $\epsilon = \Omega(1)$ and large $N$ one cannot achieve the non-private rate.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Derives optimal (up to log factors) upper and lower bounds for several different variants of classification/regression under label DP.
* To derive these bounds, introduces some new technical tools for minimax analysis of DP algorithms that might be useful in future work.
* Label DP is a variant of DP that is seeing attention in practice, and classification/regression are fundamental problems, so the results in the paper can have a practical impact easily.
* The authors do a good job making clear the comparison between the results in different settings. e.g. Table 1 is a very concise summary that allows one to draw all the essential comparisons between the different settings, and there are discussions like Remark 1 that give qualitative interpretations of the quantitative results, and also discuss other baselines to compare to.

Weaknesses:
The main issue is with the presentation. Specifically, the presentation does a great job explaining what the final results are and helping the reader contextualizing them, but at some points the techniques used to obtain the result are discussed at a very high level in the main body and why they work remains obscure even after reading the proof outlines in the main body multiple times.  There are some cases where the authors do a good job concisely describing a proof, e.g. Theorem 6's proof outline is very concise but it still gives a good idea what the proof looks like, even if they would have to check the appendix for details. But for others, like Theorems 1/2/3, the proof outline is not very informative. See Questions for more details.

I understand the authors are constrained by space requirements, but I think the allocation of space in the main body can be better thought out. For example, I think it might be better to try to give the reader a very good understanding of classification and/or bounded label regression (e.g., Lemma 1 from the Appendix could be brought to the main body without its proof, and the authors could explain how it is used), and omit all but the top-level points on bounded label moment regression, rather than giving a sparse understanding of all three.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the minimax risks of classification and regression (with both bounded and heavy-tailed noise) under label differential privacy (DP) in both central and local models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper provides a comprehensive analysis by considering both upper and lower bounds for the minimax risks.
It explores both central and local DP models and different settings, covering a broad spectrum of scenarios.

Weaknesses:
The writing quality needs improvement to meet publication standards. Several sections are challenging to understand. Specific issues include: 
(1) Around line 178, the output of the mechanism for classification is unclear. Why is it not a one-hot vector, or at least why is the L1 norm not equal to 1?
(2) Some notations are overused. For example, ""c"" refers to the lower bound of the density function in Assumption 1 and also denotes the classifier in line 186 and subsequent proofs.
(3) The description of the algorithm before Theorem 2 is vague and lacks clarity.
(4) The proofs in the appendix are hard to follow without explanations or discussions. For instance, how is $\phi$ defined in Equation (35), and what purpose does it serve? Why does the construction satisfy the assumptions? There seem to be some typos or missing elements in Equations (39) and (40).

Limitations:
See weaknesses and questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
oEmyoy5H5P;"REVIEW 
Summary:
The paper is a review of algorithmic recourse (AR) literature. The authors deploy a systematic framework to investigate research trends in algorithmic recourse and evaluate their incorporation of practical concerns like societal and institutional considerations of AR, or lack thereof. The review finds that current research is  focused on methods and technical considerations. The authors encourage researchers in AR to consider real-world implications of their work and conduct user studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Paper is well-organized and easy to follow
- Section 2 provides solid background information on algorithmic recourse
- The questions in Section 4 are pertinent

Weaknesses:
While I agree with the points being made and appreciate the findings in the paper, I question their novelty. As mentioned in Section 4.6, there are papers (albeit in smaller numbers than we would want) that already provide real-world examples and attempt to discuss ethics within recourse. Previous work by [Doshi-Velez and Kim](https://arxiv.org/pdf/1702.08608), [Vaughan and Wallach](https://www.jennwv.com/papers/intel-chapter.pdf) have called for more user studies in interpretable ML, which resulted in studies like [Sixt et al.](https://openreview.net/pdf?id=v6s3HVjPerv). Considering that many researchers working on recourse is also in the field of ML interpretability, I am not sure if the paper's results and call for more user studies are very substantive.

Spending more time differentiating this work from other related works (especially other literature reviews like [70]) rather than listing their contributions in Section 2.2 may be helpful in making your case.

A more thorough discussion and evaluation of results (attempted in Section 5) may resolve some of these questions. As it stands, there is a disconnect between Section 4 and 5. The message of the first part of Section 5 (lines 318-350) is not clear. The second paragraph of the section does not seem to be a discussion of survey results but rather an argument the authors are trying to make (without using the results). The paper would benefit from expanding on the contents in lines 352 to 356, pointing to results in Section 4 and bridging them to the suggestions in Section 5.1.

The paper reads more like a position paper, trying to convince researchers in algorithmic recourse to not only focus on technical methods (which, again, I agree with). But I am not sure if a literature review or a position paper suitable for NeurIPS, considering its call for papers, does not seem to suggest so.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper provides a review of previous works that study ""algorithmic recourse"", i.e. conceptual and practical approaches for giving people actionable recommendations to change how they are impacted by algorithmic systems. This literature is deeply connected with counterfactual explanations and understanding models through small changes to test data, answering questions such as ""how would the model M produce a different output if changed attribute x about myself"". The authors review 127 archival publications and answer 9 questions about how these works frame and study algorithmic recourse.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
In terms of originality, quality, and clarity:
- While the primary novel contributions of this draft are to highlight themes in previous work, the overall level of novelty is reasonable. Some concerns here, see below.
- Quality: the ""Systematized review"" methods are described such that they are replicable and seem justified. I don't expect readers to have major issues with inclusion criteria of papers, or any of the analyses presented.
- Clarity: Writing is clear throughout.

In terms of significance, the paper could have impact on future work studying algorithmic recourse, and might motivate NeurIPS community members (including those in companies or working with governments) to support recourse methods. This would be a large positive impact.  

 This kind of review can certainly be useful to researchers trying to incorporate ideas or findings from recourse-related research. The calls to engage with HCI and systems-level thinking are reasonable (though, some of the broader discussion/motivation in the paper is more convincing on this front than any of the empirical results from the 127 recourse-related papers). If a version of this paper were able to unify definitions in the recourse space, this could be powerful (though further expansion of Section 2.2 might be necessary: the paper does note that reference [70] is highly similar -- the current draft was a bit vague in comparing these and clarifying the added contribution here.).

A few other notes: There are 9 overall sub-research questions answered. Overall, these results seem likely to be useful to researchers entering the algorithmic recourse field (though, see below, some of these felt very general and not domain-specific in the current draft). The paper does fit into the ""Social and economic aspects of machine learning"" category listed in the CFP this year.

Weaknesses:
Overall, I do think the current draft may not achieve the full impact that a future revision could provide.

The current discussion section feels like it largely echoes other calls in the community to apply systems thinking to ethical/responsible/pro-social AI/ML initiatives, and while each of the suggestions has some connection to one of the analyses, the current draft is not totally clear about the extent to which these recommendations stem from the findings vs. are motivated by first principles. The paper is overall very critical of the AR field, i.e. ""Why hasn't this field engagement with any real world deployments"". However, it's also not entirely clear in the current draft how any of the general recommendations would be applied in specific AR domains.

One aspect of the paper that I think would have been most directly helpful to the NeurIPS audience in particular would be to take a stance on how recourse should be defined -- is the definition on line 62 ""endorsed"" by the paper? Is the ""imagine a counterfactual input x*"" an advisable approach to take for future work. Does this review support the definition, highlight core definitional or epistemic issues, etc.

Ultimately, given the intended goals of this draft, it seems success and impact (on top of the core empirical contribution provided by writing a systematized review) here are dependent on the ability for the provided recommendations to shape future research positively. While the five recommendations here could have a some positive impact, taking a stronger stance on the core definitions and framing of recourse ""tasks"" could have an even larger impact.

Limitations:
- No major concerns regarding unmentioned social impacts.
- Regarding the limitations of systematized literature review, the current draft discusses these reasonably.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper provides a comprehensive review of the algorithmic recourse research literature, concentrating on understanding the recourse research ""in the wild"", by focusing on the practical application of these techniques in real-world scenarios. The authors then provide some suggestions to practitioners to push future research to better practical applications.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written and well-structured. Considerable effort has been put into this work to provide a comprehensive review of the area, highlighting the need for a more down-to-earth approach when considering recourse. The data collection and analysis are well-motivated and described sufficiently (Section 3 and Section 4). The recommendations in Section 5.1 are on point and all true, and they highlight issues that everyone in the community is aware of but that are largely ignored.

Weaknesses:
I feel NeurIPS is not the right venue for this kind of contribution, since this paper does not provide the level of technical novelty required by the conference. Being a review, I think it does not fit the requirement of ""new and original research"" given by the Call of Papers. I suggest the authors not be discouraged, since I think the contribution is still valuable for the community. Potential other venues I believe are more in line with the scope of this work could be the following (the order is random):
- IJCAI Survey Track (https://ijcai24.org/call-for-papers-survey-track/)
- ACM FAccT (https://facctconference.org/)
- AAAI/ACM AIES (https://www.aies-conference.com/2024/)
- ICML Position Papers Track (https://icml.cc/Conferences/2024/CallForPositionPapers)
- ACM Computing Surveys (https://dl.acm.org/journal/csur) 
- TMLR (https://jmlr.org/tmlr/)

Lastly, I would like to point out some potential additional papers on algorithmic recourse which could complement some remarks made by the authors:
- Line 182 ""We did not identify any applications evaluated with humans in the loop"": there has been some development in providing human-in-the-loop algorithms to identify better recourse options:
  - [1] De Toni, Giovanni, et al. ""Personalized Algorithmic Recourse with Preference Elicitation."" Transactions on Machine Learning Research, https://openreview.net/forum?id=8sg2I9zXgO
- Recommendation 4, ""Accounting for emergent effects"": there has been some research regarding providing recourse to multiple individuals, where they are competing for a limited pool of resources, looking also at the fairness of these systems:
  - [2] Fonseca, João, et al. ""Setting the right expectations: Algorithmic recourse over time."" Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization. https://dl.acm.org/doi/pdf/10.1145/3617694.3623251
  - [3] Bell, Andrew, et al. ""Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity."" arXiv preprint arXiv:2401.16088, https://arxiv.org/pdf/2401.16088

I also point the authors to some new papers considering human-in-the-loop interfaces for recourse (Recommendation 1, Section 5.1):
- [4] Esfahani, Seyedehdelaram, et al. ""Preference Elicitation in Interactive and User-centered Algorithmic Recourse: an Initial Exploration."" Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization. https://dl.acm.org/doi/pdf/10.1145/3627043.3659556
- [5] Koh, Seunghun, Byung Hyung Kim, and Sungho Jo. ""Understanding the User Perception and Experience of Interactive Algorithmic Recourse Customization."" ACM Transactions on Computer-Human Interaction. https://dl.acm.org/doi/pdf/10.1145/3674503

Limitations:
The authors have highlighted the limitations of their work in Section 5.2.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a survey regarding algorithmic recourse scientific literature. In their work, the authors analyze what types of contributions do the authors choose to make to the AR research, what are the criteria covered in the authors’ definitions of AR, what are the criteria covered in the authors’ definitions of actionability, the roles of end users, what types of real-world considerations motivate existing research, what types of real-world considerations are seen as challenges for future work, what types of group-level dynamics are addressed in the existing research, what are the approaches to the realistic evaluation of proposed methods, and what are the open source and documentation practices in AR research. They conclude their paper by providing recommendations on how to make future algorithmic recourse solutions better suited for real-world needs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- the authors invested much effort into explaining the procedure followed to ensure a high-quality survey
- the authors very synthetically review scientific literature related to algorithmic recourse and provide a great insight into the field within a few pages
- the authors reviewed a vast amount of literature (165 references!)

Weaknesses:
We did not identify important weaknesses. While an extensive survey could be created following this one, providing in-depth details for each of the sections, we understand this cannot be done within the constraints established for this venue.

Limitations:
The authors have adequately acknowledged the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oAmHzy8btj;"REVIEW 
Summary:
This paper considers the graph matching problem, where the goal is to produce a mapping between vertices of multiple graphs which maximizes similarities among them. The authors study graph matching from a theoretical perspective, in which one observes multiple (appropriately correlated) Erdös-Rényi (ER) graphs that have ground-truth latent mappings between them. The authors' goal is to characterize the information-theoretic threshold for exactly recovering the latent mappings between all of the observed ER graphs. Prior work has settled the information-theoretic thresholds for 2 correlated ER graphs, and this paper settles it for more than 2 ER graphs. 

To determine the information-theoretic threshold for exact graph matching, the authors establish matching achievability and converse results. The converse is based on a simple reduction to a graph matching problem with two ER graphs, combined with known results on impossibility results for exactly matching two ER graphs. For the achievability results, two algorithms are discussed. The first is the MLE, which is optimal for exact graph matching. The authors show that it has a clean, easy-to-understand form: the MLE outputs vertex mappings which maximize the number of edges in the corresponding union graph. However, the authors do not directly analyze this algorithm due to technical complexities. Instead, they propose an algorithm which involves two phases. (1) For each pair of graphs, a partial, fully-correct mapping is computed via the $k$-core estimator, and (2) unmatched vertices are matched through a ""transitive closure"" procedure. This algorithm provably outputs the full, correct set of vertex mappings in the parameter region that complements the converse. 

Finally, a few numerical experiments are presented, showing that the transitive closure procedure can be combined with known computationally efficient algorithms for pairwise graph matching to derive algorithms for matching multiple graphs in a principled manner.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Almost all the existing theoretical work on graph matching concerns two graphs, except for some trivial results (to the best of my knowledge). The extension of the theoretical framework to multiple graphs is a natural and important follow-up, and may inspire several future works as well. 

While the algorithms and analysis are largely adapted from prior work (e.g., the $k$-core estimator), a key novelty is the transitive closure step, which provides a principled (and optimal!) bridge between pairwise graph matching and $m$-ary graph matching. As the authors highlight, this step can be used to extend practical algorithms for pairwise matching to the $m$-ary case in a black-box manner. I imagine that this technique could be useful in practice. 

Additionally, the paper is well-written.

Weaknesses:
To me, the main weakness of the paper is in the discussion of transitive closure's implications. The authors make a striking observation that one can use their transitive closure technique (at least heuristically) to generalize pairwise graph matching to $m$-ary graph matching. However, several details are lacking in the simulations section. For instance, what are the graph parameters ($n$ and $p$)? What is the error rate before and after the transitive closure boosting? How do the results shown compare to the accuracy of Algorithm 2? (Even though the $k$-core matching is not efficient to compute, the result of the matching procedure is a function of the ground-truth permutations, so I believe the algorithm's accuracy can be simulated efficiently). 

There are a couple other minor weaknesses. One is that Algorithm 2 is computationally inefficient. However, making such an algorithm efficient is likely a challenging research question itself, and is appropriate for future work. Another weakness is that there is no nice figure to visualize Algorithm 2. I feel that the reader's understanding could be greatly improved if one could create a representative figure for the transitive closure boosting.

Limitations:
Limitations have been largely discussed. The authors could expand upon implications of graph matching to protecting / breaking privacy in anonymized social networks.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the information theoretic limits for matching
multiple correlated random graphs. Based on a correlated Erdos-Renyi
random graph model, the authors provide both lower bound and achievable
bound for the condition to correctly match all nodes with high
probability. These bounds match each other. A highly interesting insight
is that, even when exactly matching two graphs is not possible, the
proposed algorithm can leverage more than two graphs to produce exact
matching among all the graphs. The achievable algorithm exploits the
transitivity among partial matchings through $k$-cores, which is also
quite interesting.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The novelty of the paper is high in dealing with graph matching among
multiple correlated graphs. 

2. The necessary and sufficient conditions for exact matching meet each
other. 

3. The proposed algorithm can exploit transitivity to match all graphs,
even when any two graphs alone cannot be exactly matched. This is a very
insightful result.

Weaknesses:
1. The proposed algorithms do incur high complexity.

Limitations:
Limitations are discussed in Section 5.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This theoretical paper gives tight conditions for exact graph matching with multiple correlated random graphs. This problem has been extensively studied recently for the case of 2 graphs, and it is shown here that with more than 2 graphs, there is a regime where pairwise alignment is not possible, but with the information provided by all graphs, it is possible to align all of them. This is a nice theoretical result.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper studies a natural extension of a well-studied problem from 2 graphs to more graphs and shows a surprising effect: making partial pairwise matching is sufficient to get the exact recovery. The proof outlines give the main insights into the technical proof.

Weaknesses:
The resulting algorithm is not practical as it does not run in polynomial time (as it is mentioned by the authors).

Limitations:
The authors are very clear with the limitations of their work in section 5.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to find out alignments between G_1 and G_2,....G_m, under the assumptions that they all are essentially sampled from ER graph distribution. The paper presents one impossibility result (or necessary condition to estimate such alignment)  and two sufficiency results to solve the underlying problem.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper tackles an interesting problem and it is written clearly.

Weaknesses:
(1) I am not too confident that the paper is appropriate for neurips audience. I think the paper suits better to a conference like ISIT or such.  The paper has barely any learning component and the practical utility is not very clear.
Also, the primary area assigned by the authors ""Probabilistic methods (for example: variational inference, Gaussian processes)"" is probably not correct.

(2) The paper only tackles a very simple graph model (ER graph model). While I understand that theoretical analysis for complex graph model is difficult, I would recommend the authors should discuss that in comprehensive manner. To elaborate concretely,
suppose,  G_1, G' _2...,G' _m are *not* generated from an ER model. But G_2,...G_m are generated using an ER like model with constant edge deletion probability $s$. In such case, can one characterize the necessary and sufficient condition.

Note that, the area is not too new in the literature. There has been work already in this line of research [CK17,WXY22 in the paper]. Although I will not say this work is an extension but the theoretical contribution given the existing works is not very interesting (m=2 to an arbitrary m for example). 

(3) There is no experimental analysis. I would have increased my rating if the authors have done a thorough study on implications (including limitations) of their work on graphs from other models. For example, if we apply the same algorithm in other graph models, how would it perform. Since the line of work is not new, I would not say the theoretical results have strong enough impact to ignore the poor experiments.

Limitations:
Restrictive graph model; poor experiments and incremental contribution.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nzzxLPJENZ;"REVIEW 
Summary:
In this work, the authors conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. The authors demonstrate that within a limited search space, there exist better decision-making behaviors that facilitate the model in making reasonable and accurate judgments.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The idea of branching LLM evaluation is interesting and novel.

Weaknesses:
1. The authors missed a lot of key related works, including close-ended benchmarks such as MMLU, MMLU-pro, MixEval, GSM8k, GSM1k, etc; open-ended benchmarks such as Arena-Hard, AlpacaEval, WildBench, Chatbot Arena, etc.
2. I think the writing needs improvement. Now it's not easy for a reader to get what you are focusing on. If you are doing evaluation, then try to use some pipeline figures and comprehensive captions to describe the core idea. Besides, all captions in this paper is misleading, not telling the reader about what is happening in the table or figure; also, there lacks some key sections such as conclusion.
3. How to measure the quality of the proposed evaluation? I think just evaluating 5-6 models is far from enough. Beyond that, how is the model rankings related with Chatbot Arena or some other popular benchmarks such as MMLU?

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel approach to efficiently evaluate LLMs using branching preference learning. The authors conceptualize the evaluation process as a decision tree, where each path represents an evaluation reasoning trajectory. They introduce a tree-based data sampling method and preference learning based on the DPO algorithm to improve evaluation capabilities. The method is tested in three settings: in-distribution, out-of-distribution, and transfer evaluation. The authors claim their model significantly reduces dependency on labeled data and demonstrates strong performance across different evaluation settings while reducing inference costs by 90% compared to searching the entire evaluation tree.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper's novel approach of framing LLM evaluation as a decision tree problem is a significant strength. This allows for a more nuanced and flexible evaluation process that can adapt to different scenarios and criteria. The use of branching preference learning enables the model to prioritize critical evaluation criteria.
- The authors test their model in multiple settings (in-distribution, out-of-distribution, and transfer evaluation), providing a thorough assessment of its performance. Applaud to that.

Weaknesses:
- The biggest concern I have is that in-distribution performance is not better than other baselines it compares to. This begs the question of where the improvement gain is from. If in-distribution evaluation performance is mediocre but out-of-distribution does better, then doesn't the most gain come from a better dataset? 
- Another concern is the unnaturalness of using evaluation criteria as individual nodes. How to ensure the coverage of those criteria across different nodes. Are they overlapping each other or completely different? The paper is quite vague on this.
- Why is each criteria subtree only a binary tree? If using a tree structure, it seem like it can be easily extend to multiple nodes rather than just 2 at each layer.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
They present an approach to improving LM evaluation by having models first generate an evaluation criteria, then a scoring guideline, and then finally a final judgement. They then develop a procedure for collecting training data corresponding to these three steps by applying branching/pruning approach (sample multiple criteria, from each sample multiple guidelines, etc...). They then use the generated data to train a DPO and SFT model. They find that their method outperforms baseline evaluation approaches according to correlation with human judgement on dialogue evaluation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The problem of improving LM evaluation is important
* The idea of enabling language models to hierarchically sample evaluations (e.g. fist criteria, then guideline, then judgement) is very neat, and similarly the idea of applying a tree-based sampling procedure to automatically generate data is quite cool.
* I think they do fairly thorough experiments and compare to quite a few baselines.

Weaknesses:
* The paper is honestly pretty hard to follow. There's a lot of moving parts and it's not explained in an easy to digest way.
* The specific method presented seems a little bit ad-hoc, and could be justified better in the paper (e.g. why use criteria, then guideline, then judgement, why not some other sequence of steps?).
* Looking at Figure 1, it doesn't seem that their method improves all that much over the baseline

Nits:
* The related work seems pretty sparse. There's lots of work on improving LM evaluation in math reasoning settings that isn't discussed.
* Figure 4, the text is really small and hard to read.

Limitations:
They do a good job of discussing the limitations. I would also note that it is unclear how effective this is when applied to more challenging tasks like mathematical reasoning (e.g. MATH benchmark) as a limitation.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates how to improve the quality of automated evaluation through fine-tuning (SFT and DPO). The main algorithm proposed by the paper is to construct an search tree which consists of node of (criterion, scoring guide, and judgment). This tree is later pruned and modified and the different paths serve as fine-tuning data for SFT and DPO.

My current rating is tentative. If the authors can kindly clarify the details of the paper, I'm happy to raise the score.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is very clear and easy to read.
2. The investigation is very thorough. Experiment is comprehensive (the in-distribution, out-of-distribution evaluation setup is great).
3. The main claim of the paper is substantiated (I.e., improving efficiency through fine-tuning).

Weaknesses:
I don't think this paper has substantial weaknesses.

1. There are some imperfections of text -- mostly just need to be clarified. Missing notation definitions, etc. 
2. The performance improve over Auto-J on AGR is minor (55.13 -> 57.18). OOD evaluation, Zephyr-7B AGR is 56.75 and GPT-4 is 62.28 (which is close, but not quite close). CNS however is beating GPT-4. This would be very helpful for me to understand a bit more about what CNS is, and whether beating GPT-4 on this metric is meaningful or not (see Q6).

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors propose a tree-based data sampling method to conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. The proposed method involves generating supervised data and preference pairs derived from the evaluation tree for SFT and DPO training. This approach aims to reduce the dependency on labeled data and improve the performance of the evaluation model across in-distribution, out-of-distribution, and transfer evaluation settings. Experimental results demonstrate that the proposed model can enhance evaluation efficiency and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method reduces the dependency on human labeled data by generating supervised data and preference pairs from the evaluation tree.
2. The paper is well-written.

Weaknesses:
1. Potential Biases --- The initial multi-branch training data is generated using only GPT4, which could introduce bias to the training data. Moreover, the branch ensemble method could also introduce bias to the training data. If the training data is biased or unrepresentative, the model's evaluations may also be biased. The authors should consider labeling a small annotation set to validate the branch ensemble approach.

Limitations:
Yes, limitation discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nu2Sqrsnr7;"REVIEW 
Summary:
The paper attempts to train PINNs which solves acoustic wave equations. They do so by using hard-constrained PINNs which can enforce IC and BCs, and propose a collocation point sampling method (DAFS) based on the amplitude of the solution at different regions.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper considers an interesting problem in acoustics and attempt to apply the techniques from PINNs to solve them.

Weaknesses:
The paper itself feels less coherent, and seems like just an application of many existing PINN training techniques (e.g., hard constraint PINNs, collocation point sampling) into solving a certain problem, rather than providing a novel method or a coherent framework into solving a domain-specific problem.

The experimental section feels incomplete. Different point selection algorithms have not been extensively compared with, e.g., from that in Wu et. al. (2023). Furthermore, it would be interesting to see how the method can scale to more realistic acoustic problems (i.e., outside of 1D settings).

The paper itself also seems incomplete. The Appendix and the NeurIPS checklist are partially filled and have half-finished sentences.

The labels within the graphs can also be enlarged slightly to make them more readable.

Limitations:
The authors have provided limitations with selection of \tau.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript treats the one dimensional wave equation with a PINN approach and discusses the imposition of boundary and initial conditions directly into the network, as common practice in PINNs. The authors then propose a quadrature scheme based on a coarse finite difference discretization of the wave equation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The imposition of the time derivative seems to be a novel construction. Furthermore, the construction seems not to be limited to the wave equation.

Weaknesses:
The main weakness of the manuscript is the focus on the very special and simple toy problem of the one dimensional wave equation. Solving the one-dimensional wave equation with PINNs is only of academic interest and insights obtained from it for the training of PINNs might not generalize. More specifically:
- The exact imposition of the time derivative should also work for general time dependent equations. The authors should comment on this.
- The sampling strategy employing a finite difference simulation to determine regions of high sampling density is not a generalizable approach. If a finite difference solver for the equation at hand is available, a PINN solver is typically not required.
- The authors determine an optimal function $\tau$ via considering six concrete examples. There is no guarantee that this approach will generalize to different equation types and is therefore of limited practical use.
- The authors might want to discuss the theoretical literature that proves the theoretical advantage of exactly imposed boundary conditions [1, 2, 3] and more elaborate constructions of distance functions.

[1] https://proceedings.mlr.press/v190/muller22b/muller22b.pdf

[2] https://arxiv.org/abs/2311.00529

[3] https://www.sciencedirect.com/science/article/abs/pii/S0045782521006186

Limitations:
The scope of the paper is too narrow.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores to solve the acoustic wave equation in the context of PINNs. Hard boundary and initial conditions are enforced by employing continuous functions within the PINN ansatz to ensure that these conditions are satisfied. A Dynamic Amplitude-Focused Sampling (DAFS) method is introduced to improve the efficiency of hard-constraint PINNs under a fixed number of sampling points.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Propose a general hard constraint imposition formula which correctly imposes all boundary conditions and initial conditions as required.

Weaknesses:
1. Only the wave equation is discussed.
2. The proposed Dynamic Amplitude-Focused Sampling (DAFS) method is trivial.
3. There are no comparisons with other methods in the experiments.
4. In the experiments, the relative errors between exact solutions and predictions are not given.
5. In the context of PINNs, it is better to give explicitly the formulation of training loss. Training details are also lacking. 
6. Instead of tuning \tau (t) manually, it is better to train \tilde{u}(x,t) and \tau (t) simutanuously.
7. Many typos and grammar errors, such as ""both and \alpha"" in line 149, ""x \in {\partial \Omega}_i"" in line 125, ""computational"" in line 46.
8. The quality of Fig.7 should be improved.

Limitations:
Only the wave equation is discussed. There are no comparisons with other methods in the experiments.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper improves the training efficiency of original physics-informed neural networks to solve the 1D wave equation threefold: first by extending ansatz to also take the first derivative into account, second by a sampling method that focuses on high-amplitude regions, and third by a framework for domain decomposition.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ The related work is well presented.
+ The evaluation of the six candidate functions for \tau in section 4.2 provides interesting insights. The authors explore an advanced selection method for \tau based on the task at hand which might be an interesting research direction.

Weaknesses:
[Originality] While considering the first derivative for the ansatz is a good addition, the contribution is only minor. 
Sampling more collocation points in the regions that might be more difficult to solve is a practical approach however the comparison and distinction to other sampling methods is missing.
Lastly if I understand the domain decomposition framework correctly, the contribution is to wrap the entire training into a loop and, based on the training process's results, increase or decrease the subdomain size. 

Evaluation results are only provided for the 1D wave equation. Further results for other differential equations are necessary to demonstrate the benefits of the proposed method.

[Clarity] 
The framework for domain decomposition is not presented clearly. While the flow chart in Figure 7 provides an overview of the method additional textual explanations in Section 4.4 are needed.
There were few to no remarks about the training regime (#training points, optimizer, learning rate…, etc.), making it more difficult to reproduce results.
Minor remarks:
-            N_pde is not introduced. It is probably the number of collocation points?
-            Most of the Figures (e.g. Fig. 1, Fig 6.) are hard to read.
-            Line 46: (…) optimal size of the computational [domain?] given (…)
-            Line 149: Both [N_pde?] and alpha (…) 
-            Line 178: (…) In general, (...) performs better in general

Limitations:
While the authors clearly state that they are interested in the 1D wave equation it would have been interesting to see their proposed methods applied to the 2D wave equation of any other differential equations what are typically used in PINN benchmarks.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nrcFNxF57E;"REVIEW 
Summary:
This paper introduces the an unbalance Gromov-Wasserstein distance, which adopted the formulation of unbalance optimal transport into the Gromov-Wasserstein with total variance (TV) penalty instead of KL. This new distance allows comparison of probability measures from different space with partial amount of mass, and so they name it Partial Gromov-Wasserstein (PGW). They further prove the metric properties of this distance and proposed two algorithms to solve PGW: Frank-Wolfe algorithms and Line search method. In experiment section, they test PGW with different tasks including: shape-matching, shape retrieval and barycenter problem.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper fills a gap in the topic of unbalance (GW) problems by introducing a TV-relaxed unbalanced GW, they call it Partial Gromov-Wasserstein (PGW). This work shows the theoretic metric properties of this distance with a solid proof, making a contribution within this topic. They proposed two solvers to compute this distance and show diverse experimental applications. The experiment results show that this new variance of unbalance GW performs effectively with data containing outliers, aligning with the anticipated performance of unbalanced Wasserstein or unbalanced Gromov-Wasserstein methods on noisy data.

Weaknesses:
- It's worth to note in literature review the similar works formulating partial Waserstein as a metric with TV constraints [1] [2].
- The proof of the metric properties is not well displayed in the main text, as this is the main highlight of this work.
- Further comparison with KL version was lack as regards to robustness against outliers. And also, further discussion on the scalability (very large dataset) will be appreciated.
- In experiment section, the selection of hyperparameter is not clear. The justification of choosing $\lambda$ value for both PGW and MPGW method was not presented. 

[1] Raghvendra, Sharath, Pouyan Shirzadian, and Kaiyi Zhang. ""A New Robust Partial $ p $-Wasserstein-Based Metric for Comparing Distributions."" arXiv preprint arXiv:2405.03664 (2024).

[2] Nietert, Sloan, Rachel Cummings, and Ziv Goldfeld. ""Robust estimation under the Wasserstein distance."" arXiv preprint arXiv:2302.01237 (2023).

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper considers an unbalanced version of the Gromov-Wasserstein distance, where the discrepancy terms correspond to the total variation between certain product measures for the given marginal and the marginal of the solution, respectively. Different (re)formulations of this distance is considered in both the discrete case and for general measures, existence of optimal solution and metric properties are shown for certain cost functions, as well as numerical methods for computing (local) optimal solutions. Finally a number of numerical experiments are considered.

The paper is well written and extensive. In the main paper the results are stated and with proofs in the  appendix.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is strong in terms of both content and the presentation. In particular the results about the metric properties of the PGW problem. It is also a quite complete paper in terms that is contains substantial results on theory, computational methods, and numerical simulations.

Weaknesses:
One weakness with the methods in the paper is that it only provides local optimal solutions. This is a problem with many non-convex problems, but in some cases global solutions can be guaranteed. In the abstract it is stated the methods solve the PGW problem. This is probably to strong statement since they are not guaranteed to converge to the global solution.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the Partial Gromov-Wasserstein (PGW) metric as a means to handle unbalanced Gromov-Wasserstein problems between non-probability mm-spaces. The authors develop and demonstrate two computationally efficient variants of the Frank-Wolfe algorithm for solving the PGW problem. They establish that PGW is a well-defined metric, providing theoretical proofs and applications in shape matching, shape retrieval, and interpolation. The metric and algorithms are validated through numerical experiments against established baselines, showcasing improved results in handling outlier data with a robust performance.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper presents a novel approach to defining a metric in the context of unbalanced Gromov-Wasserstein, which has been a challenging issue in the field.

2. The quality of the research is high, evident from rigorous theoretical developments, proofs, and comprehensive experiments that validate the effectiveness of the PGW metric in practical applications.

3. The paper is well-organized, with clear explanations of complex concepts. The use of examples and experimental results helps in understanding the practical implications and advantages of the PGW metric.

Weaknesses:
1. While the paper provides a comparison with existing methods, it could benefit from a broader range of comparative baselines, especially newer techniques that might provide a stiffer benchmark.

2. The paper does not extensively discuss the sensitivity of the PGW algorithm to the choice of its parameter (e.g., the regularization coefficient - lambda), which is crucial for understanding its robustness and adaptability in diverse real-world scenarios.

3. There is limited discussion on the scalability (or the time complexity) of the proposed methods, particularly in high-dimensional or large-scale settings, which is vital for their applicability in big data applications.

Limitations:
Including limitations on the scalability and time complexity, such as the maximum solvable problem size within one hour, would be beneficial for its applications.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a partial Gromov -Wasserstein (PGW) formulation, which relaxes the original constraints present in Gromov-Wasserstein (GW) formulation. In PGW, the marginal equality constraints of GW are replaced by marginal inequality constraints. Following existing works in partial GW setting, the paper additional imposes TV-based marginal regularization in the objective. The paper showed that the proposed PGW approach defines a metric between metric measure spaces. The PGW problem is solved via Frank-Wolfe (FW) and empirical results are shown on shape interpolation  in the main paper.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The paper explores partial mass transportation setting in the GW problem. As  discussed around lines 149, an existing work [45] has also explored similar concepts in GW setting. [45], in particular, involves an additional hyperparameter (\rho) for overall mass of the learned transport plan. The proposed problem (16) as well as [45] employs FW algorithm. 
    - Proposition L.1 in the appendix shows that if \gamma is an optimal solution of proposed PGW problem, then \gamma is also an optimal solution of the mass constrained PGW problem (MPGW) of [45] with \rho=|\gamma|. 
    - It is unclear what the paper implies by ""mathematically this equivalence relation is not verified."" in line 955? How is the paper defining the term ""equivalence"" which is used multiple times in Appendix L. 
    - For a given \lambda=\lambda_0 in PGW, does there exist a \rho=\rho_0 for MPGW such that the set of first order critical points for PGW and MPGW are same?
    - The steps of the FW algorithm for proposed PGW and MPGW [45] seem quite similar.

Weaknesses:
- The paper is poorly written due to the following reasons:
    - The abstract and introduction states that the paper propose two variants of FW algorithm for solving the proposed PGW. However, the main paper does not describe two variants of FW algorithm. Only one variant is discussed in Sections 4-5. The other variant is described only in Appendix G. If the algorithm Typically, the main paper should be self contained, having the necessary details of the contributions claimed in the abstract and introduction. It should be noted that going through the supplementary material is a reviewer's discretion. 
    - The paper provides very less discussion on how its differences with MPGW [45] in the main paper (lines 148-149). While Appendix L contains this discussion, important parts of it should be discussed in the main paper. As mentioned above, going through the supplementary material is a reviewer's discretion. 
    - There are multiple grammatical and spelling errors throughout the paper. Eg. he -> the, con -> cost, etc. 

- The main paper contained experiments only on synthetic dataset. Tables 2 and 4 in appendix discuss experiments on real-world datasets. MPGW obtains same generalization performance as PGW in both the tables. This should be discussed in the main paper as well.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
nblJSwtdrJ;"REVIEW 
Summary:
The paper introduces Tina, a text-conditioned neural network diffusion model designed for train-once-for-all personalization. Tina utilizes a diffusion transformer model conditioned on task descriptions embedded using a CLIP model. This innovative approach aims to generate personalized models for various end-users and tasks based on text prompts, demonstrating significant generalization capabilities even when trained on relatively small datasets (~1000 samples). The model is evaluated under zero-shot/few-shot image prompts, varying numbers of personalized classes, natural language descriptions, and predicting unseen entities to assess its understanding of world knowledge.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper provides a comprehensive explanation of the design and framework of Tina.
- It conducts a detailed ablation study and experiments across different datasets.
- The topic is interesting, and the presentation is clear and easy to understand.
- very detailed and robust comparison with previous works.

Weaknesses:
- The model parameter size in the experiments is too small; larger models are needed to evaluate effectiveness.
- In Table 1, the results of direct fine-tuning should be included.
- We might need an ablation study on the impact of text prompts.
- We might need an ablation study to determine if the model merely memorizes and reproduces parameters.
- Figure 2 requires polishing for better clarity.

Limitations:
The model size is too small in exp.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To generate personalized models for a variety of end-users and tasks via text prompts, this paper introduces Tina, a text-conditioned neural network diffusion model. Tina employs a diffusion transformer model, complemented by a CLIP model to embed task descriptions. Remarkably, Tina demonstrates superior generalization capabilities even on small-scale datasets, performing well both within and outside the distribution of the training data. Furthermore, Tina exhibits robust performance under zero-shot/few-shot image prompts, natural language instructions, and unseen categories.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The method demonstrates excellent generalization, showcasing significant in-distribution and out-of-distribution performance even when trained on small datasets. It also exhibits robust behavior in predicting entities that have not been seen before.

2.Compared to existing text-to-image models (such as stable diffusion), text-to-video models (like Sora), and large language models (such as GPT-4), the concept of Tina, which generates personalized models suitable for specific tasks directly from text descriptions, is quite novel.

3.The experimental process is comprehensive and reliable. The paper conducts comparisons against baselines across multiple datasets, and it also undertakes experiments to validate generalization performance as well as performs ablation studies.

4.The experiments involves multiple datasets to verify the effectiveness of the proposed methods.

Weaknesses:
1.It is better to includes more comprehensive and competitive baselines to show the model’s effectiveness and advance. The two baselines come from one paper published in 2023. As for the experimental setting involves three widely-used datasets, I am wondering whether the experimental results excels or perform similarly to the SOTA performance on some of the three datasets. In other word, is it possible to apply the proposed strategy to some more advanced framework to make the performance similar to the SOTA, which ensure the proposed method have real applications in the real use.

2.The base model is CNN or ResNet in the experiments. Is the proposed method generalized to more advanced framework? Applying the proposed method on more advanced framework and obtain more advance performance indicates that the method has potential to be used in the real life.

3.We suggest providing necessary explanations in the captions of the model framework overview.

Limitations:
The limitations are fine with me.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces Tina, a text-conditioned neural network diffusion model designed for generating personalized models from text prompts. Tina aims to enable efficient personalization by training a generic model once and then customizing it for various end-user tasks using task descriptions. Leveraging a diffusion transformer model and CLIP-based text embeddings, Tina demonstrates the ability to generate models for a wide range of personalized tasks. The approach shows promising results in generalizing to both seen and unseen tasks, achieving state-of-the-art performance in several benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Tina's train-once-for-all approach effectively addresses the need for personalized models without requiring extensive retraining, making it a practical solution for diverse end-user scenarios.
2. The model achieves competitive performance across multiple datasets, demonstrating its robustness and effectiveness in both in-distribution and out-of-distribution tasks.
3. Tina can handle various types of input prompts (text, images) and generalize to unseen classes and tasks, highlighting its versatility and potential for broader applications.

Weaknesses:
1. Some methodological details are sparse, such as the specific configurations and hyperparameters used for training Tina. Providing more granular details could help readers replicate the experiments.
2. The reason for adopting DiT as the weight generation model is not well justified. It would be good to see some results of adopting different kinds of diffusion models.

Limitations:
While limitations are discussed, the manuscript could benefit from a discussion of the scalability of Tina to larger datasets and more complex tasks.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
nfC1OA6NeE;"REVIEW 
Summary:
This work derives SDEs for adaptive gradient methods and study the role of gradient noise. The analysis starts from theoretically driving the SDE for SignSGD and highlight its significant difference from SGD. The work further generalize the SDE analysis for AdamW and RMSpropW, two popular adaptive optimizers with decoupled weight decay and reveal key properties of weight decay. Finally, the work integrates the derived SDEs with Euler-Maruyama to confirm that the SDEs faithfully track their respective optimizers with various modern neural networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-The theoretical results are novel. To my knowledge, this is a first SDE analysis for SignSGD with quantitatively accurate descriptions.

-The theoretical analysis reports some novel properties in terms of gradient noise and convergence. These properties are interesting.

-The proofs seem complete and reasonable.

-A useful theory should be quantitatively verifiable. This work definitely make it. The experiments that SDEs fit the empirical results with various optimizers and models are informative and impressive.

Weaknesses:
-It seems that the reported theoretical results and insights cannot directly lead to some theory-inspired and improved methods. This raise a question on the significance of this work.

-While this work did literature review, some important references are still missing, such as [1] on analyzing Adam using SDEs. As weight decay plays a key role in the results, it may be helpful to review recent papers analyzing novel or overlooked properties of weight decay.


Reference:

[1] Xie, Z., Wang, X., Zhang, H., Sato, I., & Sugiyama, M. (2022, June). Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum. In International conference on machine learning (pp. 24430-24459). PMLR.

Limitations:
This work discussed the limitations in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors derive SDE for signSGD and Adam(W). The experiments show that the algorithm will converge toward the limit of the theorem indicates.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors propose ""accurate"" SDEs for algorithms Sign-SGD and Adam(W).

Weaknesses:
1. In Remark after Lemma 3.6, the authors claim that Sign-SGD is (almost) linear in $\sigma_{max}$. However, with $\Delta$ either in Phase 2 or Phase 3, there should be $\sigma_{max}^2$ in the final bound.

2. All the stationarity holds when Hessian is the same from $X_0$ to $X_t$ and convergence holds for strongly convex. However, the hessian changes a lot during network training.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper derives SDEs for SignSGD, RMSprop, and Adam.
The analysis offers insights into the convergence speed, stationary distribution, and robustness to heavy-tail noise of adaptive methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The derived SDE for SignSGD exhibits three different phases of the dynamics.

- The analysis reveals the difference between SignSGD and SGD in terms of the asymptotic expected loss, the robustness of noise variance, etc.

- The analysis of AdamW provides insights into the different roles of noise, curvature, and weight decay.

Weaknesses:
Refer to Questions and Limitations.

Limitations:
The SDE for AdamW is limited to quadratic functions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ndIum4ByTU;"REVIEW 
Summary:
This paper addresses the issue of the flow matching method's lack of dependence on the data population. It proposes incorporating the initial population density into the vector field through amortization—using a Graph Neural Network (GNN) to embed the populations and adding this embedding to the input of the vector field network. The paper argues that this dependence would better model the data due to sample interactions, demonstrating improved generalization on unseen initial distributions. The method's application is showcased in perturbation drug screening.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
### Originality
The problem setting of adding population dependence to flow matching is novel. The model framework of adding input to the vector field network using a GNN as a population encoder is also novel.

### Clarity

The paper is clearly written, with rigorous mathematical notations. The related work and introductions are especially well-written.

### Quality

The writing is good, and the experiment involves many baselines.

### Significance

The proposed method excels at generalizing to unseen populations, which is a significant improvement over existing methods, particularly when the conditions for generation are complex. The application on drug screening addresses a significant scientific problem and holds promise for personalized healthcare.

Weaknesses:
1. The paper could explain more about the meta-learning aspect of this method.
2. It could include explanations and/or ablation studies on the role of meta-learning and the GNN, especially in the synthetic experiment.
3. More detail is needed on what properties of the Wasserstein manifold of probabilities are used in the model. It is unclear how the model proposed in section 3.2 depends on the properties of the Wasserstein manifold described in section 3.1.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed an extension of the Conditional Generative Modeling via Flow Matching (CGFM) framework. Taken inspiration from the theory of Wasserstein Gradient Flow, this new framework, Meta Flow Matching, proposed to learn the push-forward mapping of multiple measures in the same measures-space. This is motivated by the realistic problem of modeling single-cell perturbation data where we want to see the response of populations of cells of patients when receiving different treatments. A novelty of Meta Flow Matching is that by combining amortized optimization and CGFM, the trained MFM velocity network can model newly observed populations _without_ knowing their labels/conditions. Two empirical benchmarks were performed to showcase the effectiveness of MFM compared to the Flow Matching (FM) and CGFM.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method proposed is novel enough, and the problem is well-motivated. I also find the idea of integration of GNN to model the conditional variable quite neat. The method is based on the well-studied theory of Wasserstein gradient flow on measure spaces and amortized optimization framework.

- The empirical benchmark, especially on real biological data, seems to showcase the strength of MFM.

- Overall the paper is quite well written and easy to follow.

Weaknesses:
- The first part of the methodology section seems to be phrased as a new methodological contribution, but if I'm not mistaken this is just more or less restating the already established theory of W2 gradient flow and continuity equation (eq 14). I think the authors should put Section 3.1 into the background section (2nd Section).

- There is a lack of discussion on whether the 3 crucial assumptions (line 145-152) are satisfied in a realistic biological setting. For example, in theory, Assumption (iii) on the unique existence of the Cauchy problem stands when the velocity field satisfies some regularity conditions -- I'm not sure this can be extended to a parameterized neural network that takes input from another (graph) neural network as an embedding function, which is hardly Lipschitz smooth in most of the case.

- Algorithm boxes at the end of section 3 is highly welcome. Or if the authors cannot allocate the space, I highly recommend putting two (one for training and one for sampling) into the Appendix. It is quite hard to follow how the velocity is trained in reality. For example, what function $f_t(x_0, x_1)$ did the authors take for this work? Is it still linear interpolation as vanilla flow matching? Or does it involve adding some form of stochasticity as in stochastic interpolant or VP-SDE as in diffusion model? Is the coupling $(x_0, x_1)$ sampled to match randomly, or they are sampled to some form of alignment as in the multisample flow matching paper (Pooladian et al. 2023)?

- This might not be the original purpose of this work, but I would love to see how MFM perform on conditional image generation task. One can pick a simple small dataset such as CIFAR10 that already includes class labels, or better yet ImageNet dataset. The performance in this takse will be much more convincing than the synthetic experiment, where I would argue would target the same type of task.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper discussed the novel problem setup of generative modeling of the dynamics of probability distributions. The paper proposed Meta Flow Matching (MFM), an extension of the flow matching framework for implicitly learning the vector fields on the Wasserstein manifold of probability distributions. The paper demonstrated better transferability of the proposed framework on unseen distributions on both synthetic datasets and real-world drug-screen datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The problem setup of learning a flow matching model for mappings between distributions (i.e., a probability path on the Wasserstein manifold), to the best of my knowledge, is novel and has not been explored in previous work.
- The idea of using distribution-specific embeddings (the population embeddings) is well explained and motivated in the paper.
- The proposed method demonstrates better transferability on both synthetic and real-world datasets compared to other baselines.

Weaknesses:
- The proposed method seems to be a special case of a conditionally trained flow matching model where the conditions are continuous learnable embeddings. Such an idea has already been applied in various diffusion or flow matching models including image generation (conditioned on text embedding in the latent space), protein co-design [1] (conditioned on sequence, generate protein structure, or vice versa), and peptide design [2] (conditioned on receptor proteins, generate peptides).

- The idea of population embedding in the paper is similar to task embedding, which has been well-explored in the meta learning (e.g. [3]). Although the authors claimed their framework to be *meta* flow matching, related work in meta learning seems to lack.


[1] Campbell, Andrew, et al. ""Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design."" arXiv preprint arXiv:2402.04997 (2024).

[2] Li, Jiahan, et al. ""Full-Atom Peptide Design based on Multi-modal Flow Matching."" arXiv preprint arXiv:2406.00735 (2024).

[3] Achille, Alessandro, et al. ""Task2vec: Task embedding for meta-learning."" Proceedings of the IEEE/CVF international conference on computer vision. 2019.

Limitations:
The authors have adequately and properly discussed the limitations and potential societal impact of the paper in the Appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Meta Flow Matching (MFM), a flow matching framework modeling interacting samples evolving over time by integrating vector fields on the Wasserstein manifold. The authors leverage a Graph Neural Network to embed populations of samples and thus generalize the method over different initial distributions. The authors demonstrate the method on individual treatment responses predictions on a large multi-patient single-cell drug screen dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty: The method uniquely considers population interactions, unlike previous flow matching methods that model samples individually.

Generalization: The authors extended conditioning on latent variables to conditioning on population index in section 3.2. The proposition in section 3.2 demonstrates that conditional flow matching can fit well within the MFM framework. The experiments show that MFM can generalize to unseen data, outperforming other methods in this regard.

Weaknesses:
In Table 1 of the synthetic experiment, the authors compared the performance of FM, CGFM and MFM of k=0,1,10,50. MFM doesn't seem to beat existing methods on the metrics and from the visualizations, it's hard to tell MFM is actually doing better than FM. Also, for the various values of k, some explanations on how performance correlates with values of k and why might be necessary for readers to understand this table.

In both experiments, the authors only compared FM, CGFM, and in Table 2 also ICNN. Probably more methods, like diffusion, should also be taken into comparison. Also, in experiment 2, only W1, W2 and MMD are computed as metrics. While these are useful when modeling distributions, more metrics, especially those specific to this application may be applied.

Limitations:
The authors have not addressed limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
nMFVdphOc9;"REVIEW 
Summary:
The paper proposes a method for neural network-based learning to incorporate expert knowledge in the neural network architecture by building rules and utilizing them in ""rule-based"" layers of the learned neural networks. It introduces RuleGNNs as a concrete application of the proposed method and evaluates its performance against a few other SOTA methods. Empirical studies show competitive performance of RuleGNNs compared with other alternative methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of having dynamic rule-based layers in a neural network and especially for graph neural network learning is interesting. Although some existing methods in the literature including WL labeling could be considered doing the same, the proposed method builds on top of these building blocks and extends their ideas.
- Theoretical discussions in the paper and the assumptions behind them are clear.
- Experimental results cover an adequate set of alternative methods.

Weaknesses:
- The performance of RuleGNNs is expected to heavily rely on the quality of the rules generated from additional information or domain knowledge, however, the paper solely focuses on application of such rules without adequately discussing the challenges of building quality rules and feasibility of this fundamental step in the proposed method.
- Lack of clarity around how rules in RuleGNNs look like and how they can influence learning model parameters. 
- Experimental results are not fully discussed. For example, WL-Kernel shows superior performance in three data sets and it would have been useful to provide more insights about what data set characteristics contribute to this.

Limitations:
Authors have adequately addressed the limitations of their work by listing the following limitations:
- They have only considered 1 dimensional input signals and labels.
- They have not considered graphs with multi-dimensional node features.
- Edge features are not considered.
- Computation and storage limitation for large/dense graphs.
In addition, authors have clearly discussed structure, Combinatorics, and Implementation limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel model architecture rule-based layer, which induces different parameters given different inputs. Theoretical analysis demonstrates how the proposed architecture reduces back to classical feed-forward layers, and empirical results on both synthetic and real-world data sets demonstrate that the proposed method can improve upon existing works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of rule-based layers and rule-based GNN is novel and interesting.

Weaknesses:
- The implementation in this work may need further elaboration to make the proposed method easier to understand. 
- Empirical results may need further improvements to better support the proposed method.

Limitations:
The authors discuss about possible limitations in the conclusion part, and no direct negative societal impact exists for this work from my perspective.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces rule-based (dynamic) neural network layers. The basic idea is to have a common set of parameters, i.e., weights and biases, where, depending on a certain rule, only a subset of these parameters are used in the forward pass. They show that certain fully connected and convolutional layers can be regarded as a type of static rule-based neural network layer. In the remainder of the paper, the authors introduce three dynamic rules for graph classification tasks and perform experiments on synthetic and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Overall, the concept of using rules based on expert knowledge to select different subsets of weights for various data samples or tasks seems useful and promising. This approach could offer significant benefits, such as training the same model on different tasks or on different datasets. Moreover, an approach which is able to learn on variably sized input data could be valuable on its own. The proposed rule-based layers for graph classification tasks outperform standard message-passing graph neural networks on synthetic and real-word datasets.

Weaknesses:
One of my primary concerns is that the main theoretical result of the paper, Theorem 1, is not proven. Specifically, while the authors show in Prop. 1 and Prop. 2 that fully connected layers _without bias_ and convolutional layers _without bias, padding, stride of one, and quadratic kernels_ can be expressed using their proposed (static) rule-based layer, the following paragraph leading to Theorem 1 claims this can be generalized to arbitrary convolutional layers. Although this might be straightforward to prove (and could be included in the appendix), the lack of a complete and formal proof severely undermines the soundness of the submission. If Theorem 1 is intended as a summary of Proposition 1 and Proposition 2, I suggest making this explicit by clearly stating the specific types of FC and CNN layers and renaming Theorem 1 to Corollary 1, or merging Prop. 1 and Prop. 2 into Theorem 1. Moreover, while the paper introduces some mathematical framework and formalizes existing concepts within this framework, it lacks proofs demonstrating what this framework can achieve and fails to establish connections to existing work. Given the lack of substantial theory, I think a more thorough empirical investigation could strengthen the submission. Comparisons with more expressive architectures are missing (e.g., in Table 1 there are no results reported for more expressive architectures for almost half of the datasets; for the synthetic datasets no comparison is done with more expressive architectures), making it difficult to appreciate the practical advantages of using the rule-based layers in practice. The practical relevance is limited further by the fact that the rule-based layer can only process one-dimensional features, and the higher space complexity for dense graphs.

Regarding clarity, there is considerable room for improvement. The concept of how a rule-based layer works was not fully clear to me until page 4. If my understanding is correct, we have a matrix $\mathcal{W}$ that contains all possible weights (and similarly a bias vector $\mathcal{B}$ with all possible biases). A rule restricts $\mathcal{W}$ to a subset of weights; applying rule *R* means setting some entries in $\mathcal{W}$ to zero. If my understanding is incorrect, this indicates that the writing lacks some clarity. I suggest shortening the introduction and preliminaries, which are at times verbose, and including a briefer example from Appendix A.4 earlier in the paper, or providing a clearer definition sooner. Additionally, the notation for the rule-based layer presentation is somewhat convoluted. The readability of the paper is also hindered by the inconsistent use of formal definitions and natural language. While both approaches can be fine (as long as they are precise), there is a noticeable mismatch between the rigor in the preliminaries and, for example, Section 4. Many aspects of the paper are thus unclear; please refer to the *Questions* and *Minor Remarks* for specific examples.

Overall, I think this paper presents promising ideas in a preliminary manner. As also stated by the authors, the dynamic rule-based layer seems to be reasonable for graphs, but is more difficult to devise for other structures. One approach could be to revise the paper from a graph learning perspective, and, if the authors have novel results which hold for general structures, present these results in a follow-up paper. Another exciting direction could be to use rules to create flexible machine learning models for different tasks and input data.

*Minor remarks*:

* line 33: each new information -> each new piece of information
* line 34: the essence -> a bit vague, what is the essence of dynamic NNs?
* Fig. 1 is too small and difficult to parse in general; there is also and typo in the last sentence
* line 75: dot missing after end of sentence
* line 95: concatentation -> should this be ""composition""?
* line 111: dot missing after end of sentence
* Somewhat inflationary use of ""respectively""
* line 123, 140: I would strongly advise to not use $y$ here for $x, y \in D$, as $y$ is already used to denote labels earlier
* It would be helpful to refer to equations as eq. (1) (instead of just (1))
* Could it simplify presentation if you define $\Theta$ as tuple $(\mathcal{W}, \mathcal{B})$?
* Last sentence of Prop. 1 is difficult to read
* Why do we call the learnable parameters $\Theta$ in Prop. 1 and $W^i$ in Prop. 2?
* line 190: higher dimensions -> higher dimensional
* line 202: network -> network architecture
* lines 206-214: I suggest to consider moving this to the preliminaries
* line 221: either rule function (singular) or rule functions R_W, R_B
* line 225: circle -> cycle
* Prop. 3: ""its"" -> not clear what it refers to
* line 231: If R permutation-equivariant -> language sounds off, maybe ""For permutation-equivariant R"" or ""If R is permutation-equivariant""
* line 255: typo in isomorphism
* Pattern counting rule: $d$ is never defined
* line 347: missing space

Limitations:
One of the main limitations, as the authors point out themselves, is that their proposed rule-based layer can only process one-dimensional node features, and no edge features, which impacts the practical value of their method. For more limitations, please refer to *Weaknesses* and *Questions*.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a broad framework for adding expert knowledge to Neural Networks. They formalize this by extending the learnable parameterized functions with an additional parameter consisting of the set of formal rules. In general, these rules maybe learnable as well. However, the authors focuses on these rules being given in the form of expert-knowledge. The authors then introduce the set of rule based layers. And shows that fully connected NN layers and CNN layers are special cases of the rule based layers. They introduce three rule based layers for graphs: Weisfeiler-Leman Layer, Pattern-Counting layer and Aggregation layer. The author shows that there exists a GNN with rule based layers that can distinguish any two isomorphic graphs. Finally the author introduces some examples of rule based layers for specific molecule graphs. And presents experimental results on synthetic and real-world data.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
-- The idea of adding expert knowledge to NNs and GNNs specifically is quite interesting and widely investigated.

-- The presented theory is very general and simple

Weaknesses:
-- The author has used the notion of rules rather broadly. There is no formal language (logic or matrix language) for the rules. They are just arbitrary functions. This basically means that any existing NN model, in one way or another, can be seen as a special case of Rule based NN. In my understanding, this makes the introduced framework a rather simple formalization of how expert knowledge maybe added to NNs. However, this formalization is so loose, that it does not really admit any meaningful analysis or provide any meaningful guidance to the user for adding knowledge.

-- None of the examples presented by the author are beyond what would be anyway possible by adding some simple graph features to the node features. This could be an interesting direction to investigate. But just formally stating that this is possible is not very interesting.

Limitations:
The authors have indeed touched upon most of the points I mention as weaknesses.
However, as mentioned earlier, the proposed framework is very broad and does not provide a meaningful way to proceed.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nEvnCuviMO;"REVIEW 
Summary:
Metric distortion is a framework to evaluate the ""accuracy"" of social choice rules, by considering a worst-case candidate and voter embedding in a metric space, and by assuming that reported votes are derived from distances in the metric space. So far, votes were assumed to be a deterministic function of the distances. The paper investigates the case where they are probabilistic functions of the distances, in the asymptotic limit of a large number of voters. The key finding is that this inverts the evaluation of some voting rules, in particular Copeland and Random Dictatorship, for highly noisy voting.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The model introduced by the authors is an insightful generalization of previous work which, remarkably, provides a markedly different view on social choice rules. Given the growing importance of social choice in machine learning, as well as accounting for noisy inputs while considering embedded vector spaces, I believe that this work scores high in significance.

Additionally, the analysis is quite thorough, with matching lower/upper asymptotic bounds for Plurality upper bound for Copeland and lower/upper bounds for Random Dictatorship.

The paper is also fairly well written.

Weaknesses:
My main concern is Lemma 3. The proof seems to argue that the constraints $\forall i, | b_i - w_i | \leq b_i + w_i$ (i.e. inequality constraint on $(i, W, B)$), but the optimization problem (7) has a constraint $\max_i | w_i - b_i | \leq \min_i b_i + w_i$. It is not clear to me why these constraints would be equivalent. Note that the latter implies the former set of constraints. Thus if $\mathcal E_{\alpha}'$ was defined with all voter-wise constraints, then it would be a minimum over a smaller set, and thus $opt(\mathcal E_\alpha') \geq opt(\mathcal E_\alpha)$. Since Lemma 3's proof actually says $\frac{SC(W, d)}{SC(B, d)} \leq 1 / opt(\mathcal E'_\alpha)$, using this inequality seems to imply the actual Lemma 3. Am I reading this correctly?

It is disappointing that the upper-bound for Copeland. It would be helpful if the authors can point out where the argument gets loose.

Limitations:
The paper stated its results very clearly and factually. I have no concerns about unaddressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper extends the framework of metric distortion, measuring how well voting rules minimize the social cost in a given metric space, to probabilistic voting scenarios where the preferences of voters are drawn from a probability distribution defined by the relative distances between candidates and each voter's ground truth position in the metric space.

The authors base their analysis on three different axioms that the induced marginal probabilities of relative preferences must verify, namely *scale-freeness*, *independence of other candidates* and *strict monotonicity.* They define a general class of marginal probabilities that verify the three axioms, and show that it encompasses the widely used *Plackett-Luce* model.

They then provide upper and lower bounds for the distortion of the *Plurality* rule, both linear in the number of candidates and matching asymptotically when the number of voters grows to infinity.

They then provide a upper-bound for the distortion of *Copeland* rule and show that it is independent of the number of candidates in the limit of a large number of voters.

Moreover, they give upper and (non-matching) lower-bounds for the distortion of *Random Dictator.*

They finally compare their results under both the *Plackett-Luce* and *Pairwise Quantal Voting* models (the latter being inspired form Quantal Response Theory), and show that the classical bounds of the metric distortion literature are recovered in the limit of vanishing randomness (although not for Copeland rule, hinting at a loose analysis).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper, in extending the metric distortion analysis to probabilistic voting models perhaps closer to reality, is rather original and proposes a more optimistic view of metric distortion where randomized dictator is beatable in a worst case distortion sense.
The paper is fairly well written and the proofs seem correct.

It is an interesting idea, with an interesting result.

Weaknesses:
The main problem of this paper is that it is not a good match to NeurIPS. This paper would work very well at an algorithms conference like SODA or ICALP, or a CS econ conference like EC (okay, probably one tier down like WINE), or possibly even at the those AI conferences that have a history in social choice theory like AAAI or IJCAI. And I also understand that NeurIPS has accepted such papers in the past. However, is it really a good fit for NeurIPS 2024? 

- There is no mention of the proof of Lemma 1 being in Appendix A.
- In the proof of Theorem 2, $\zeta$ is hardly introduced (also not in the Appendix).
- Formatting may be improved in place: e.g. Equations 6, 7, 10, or Theorem 3.
- l.312 ""converges to 9 instead of 5"". This part is not very clear, reminding the general bound in the deterministic case would improve readability.
- Typos:
	- l.220 ""and is by solving"".
	- Equation 24 showcases $(d)$ instead of $(a)$.
	- Equation 34: $\geq$ should be $=$.
	- l.618 ""LEt"".
	- l.641 should probably be deleted (equivalent to l.642).
	- l.660 weird grammar.
	- Footnote 6: missing index $\gamma_j$.
	- l.670: missing $(\hat{g}_{MID} +\hat{g}_{OUT}  )^2$ in inequalities $(a)$ and $(b).$ Furthermore, Equation 63 is used in $(a)$ rather than in $(b)$.

Limitations:
- The existence of distributions on rankings that generate pairwise order marginals of the form described in the paper is assumed and left for future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the problem of metric distortion in single-winner elections. The key assumption is that the voters' preferences are not exactly compatible with the metric space, but they rather agree with it with a certain probability. The authors propose several axioms that formalize the requirements for the probability distribution for it to make sense in the context of distortion. Then they provide upper bounds of distortion in the probabilitic setting for Plurality, Random Dictator and Copeland (in case of the first two rules, they provide also lower bounds).

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work is the first one to combine probabilistic voting with metric distortion, hence the novelty is clear. The paper is overall of good quality, the axioms proposed in their work make sense to me, and the results are sound. The research direction introduced in this paper can be continued in further follow-up papers.

Weaknesses:
The paper could have been more clearly written --- for example, the formal notation should be introduced at the beginning of Section 2   (together the model) rather than in the middle of the introduction.

Besides, I think that Axiom 2 (Independence of Other Candidates) could have been better motivated. I can imagine that it was crucial to obtain the authors' results, but it seems rather natural to me that  in the real-life scenarios that motivated the research, the presence of additional candidates can impact the voter's probability of ranking one candidate over another.

Another weakness is that the authors only consider three rules, and the analysis of only two of them is complete --- many important rules (like Borda or PluralityVeto) are not considered at all. This could raise a question whether this amount of technical contribution is enough for a top conference like NeurIPS.

Limitations:
The authors adequately addressed the limitations and there are no potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers metric distortion in probabilistic models of voting. In the metric distortion framework the voters and alternatives are embedded in a metric space, and given the ranked preferences the goal is to find an alternative with low distortion. In this setting these rankings come from a probabilistic model.

In the first part of the paper, there axioms are introduced and authors show which axiom is satisfied by which probabilisitic model. In the second half of the paper the goal is to find the distortion of Plurality and Copeland rules for a specific class of probabilistic models. The results show matching upper and lower bound of $m$ for plurality and constant upper bound for Copeland.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
Defining a model for distortion in stochastic models is a useful idea in future analysis of voting systems.
The idea of the paper is novel and it uses novel techniques in the second half.
I like the definition of the three axioms. I find them natural and easy to understand.

Weaknesses:
My main concern is about the presentation. The preliminaries section is incomplete. The definition of distortion is hard to understand for a general audience and you made it harder by just putting the formula there. You have to add a description in words and give some intuition on why this definition makes sense. 

It's not clear how the probabilistic model works and how you define distortion on it until section 3 where you define it for a specific class. You have to formally define your probabilistic approach in Section 1.1 and also define distortion in this model. Not knowing the exact definition makes following the first paragraph of section 2 really hard. Before reading the rest of the paper I didn't understand why $P$ is a function of $d$ or why the preferences may not be consistent with the distances.

I think you have to add more intuition on the probabilisitic models. For instance you mention ground-truth in the definition of Mallows model but you have to explain in more details that how this model distributes the probabilities based on the distance to this ground-truth. The same explanations are needed for PQV.

My understanding is that the analysis that you provide works for any member of $G$, but currently the only members for which we have the final bound are PL and PQV. Is that true? If so is there any other interesting member of this class?

Limitations:
-

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nEnazjpwOx;"REVIEW 
Summary:
The authors propose diffusion Thompson sampling, which uses a diffusion model to leverage reward under similar actions for more efficient exploration. The authors derive efficient posterior approximations under a diffusion model prior and prove a regret bound in linear instances. To efficiently compute and sample posterior distribution, the authors provide an approximation that relies on close-form solutions for case where both the score functions of the diffusion model and the likelihood are linear. For nonlinear diffusion model, the authors approximate posteriors by a Gaussian distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proof of Theorem 4.1 requires novel techniques such as recursive total variance decomposition and refined arguments such as quantifying not only the posterior information gain for the taken action but also for every learnt latent parameter. 

The paper is well-written. The main contributions and key observations from the regret bound are nicely summarized. Experimental results for all four combinations of linear and nonlinear reward, linear and nonlinear diffusion model are provided. In experiments, the authors made a number of insightful observations, accompanied by ablation results.

Weaknesses:
The authors discussed how the number of layers L affect the regret bound. A higher L increases regret bound and a smaller L may fail to capture a more complex prior. It would improve the paper to provide a heuristics on choosing an appropriate L along with justifications for the heuristics.

Limitations:
The authors addressed limitations and societal impact at Appendix E and F.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents the use of Diffusion models as priors for Thompson sampling.

Namely, they propose to learn diffusion models (as replacement to other parametric priors) to accommodate more complex correlations between context, action and reward functions than with simple parametric form priors.

Given that Thompson sampling requires sampling from the posterior of the model, the authors derive a linear-Gaussian posterior approximation (under the proposed diffusion model prior).

The authors analyze the proposed algorithm for the linear-Gaussian reward case, which enables them to provide a Bayes regret bound.

Experimental results demonstrate some of the benefits of the proposed diffusion-based Thompson sampling: learning the correct latent-structure is beneficial, learning more parameters (as a function of $d$, $K$ and $L$) is a harder problem, hence incurs in higher regret.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The use of diffusion models to learn complex priors for their use within MAB problems is of interest and significant.

- The authors provide a theoretical analysis of their proposed algorithm (only for the linear-Gaussian case), for which they: 
    - use the recursive total covariance decomposition,
    - showcase the dependency over K ---induced by the hierarchical parameter learning--- and
    - demonstrate the dependency with $L$, inherent to having more parameters to learn.
    
- The theoretical analysis and the experiments showcase the benefits of learning the true hierarchical model (as specified by a diffusion model) in comparison to LinTS.

Weaknesses:
- The proposed diffusion-based algorithm does not learn the diffusion model as it sequentially interacts with the world
    - Instead, using the diffusion model as a complex prior requires offline learning, so that non-trivial prior distributions can be learned, before it can be used within Thompson sampling.
    - The cost of learning such a diffusion model is not acknowledged nor discussed.

- The proposed posterior approximation seems to be equivalent to the well known Laplace approximation, i.e., a linear-Gaussian approximation to a (non-linear and non-Gaussian) posterior. See questions below.

- The provided Bayesian regret is limited to the linear-Gaussian case, and in fact is acknowledged to be similar to ""L + 1 sequential linear bandit instances stacked upon each other"".

- The empirical evaluation is executed on synthetic experiments simulated from the assumed model prior, with $L$ latent parameters. Hence, the benefits of learning the true model are somehow expected.

Limitations:
The authors do present general limitations of their work, although the cost associated with learning a diffusion model prior is less clear.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work provides a great example of diffusion modeling on bandit action parameter for better exploration.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work provides a comprehensive description on how to employ diffusion modeling on bandit parameters for contextual bandit problems. 

The discussion on linear and non-linear diffusion model is clean and precise for readers with background in Thompson Sampling 

The analysis part also provide comprehensive discussion on how the regret of the  proposed diffusion Thompson Sampling scales with main dimension of contextual bandit problems.

Weaknesses:
I am satisfied with current version of the paper.

Limitations:
Yes, the author clearly states the assumptions to address the limitation of the theoretical analysis.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers the problem of contextual bandits in large action spaces.  In this problem, the reward of an arm is a function of the context and an unknown, arm specific parameter vector. To efficiently learn good policies in such large action spaces, the paper places a structured-prior distribution on the unknown arm parameters that can effectively capture the correlations between the arms. The specific form of the prior distribution considered in the paper resembles a diffusion model. The main contribution of the paper is to provide a computationally efficient heuristic for performing Thompson sampling with this prior. Experiments on synthetic data show that the proposed technique is much better at learning optimal policies than other popular baselines such as LinTS, LinUCB.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of handling large action spaces in contextual bandits seems interesting. The empirical evaluation shows promise in the proposed approach

Weaknesses:
- **Related Work:** There are several ways in which large action spaces are typically handled in contextual bandits. One popular approach that is used in practice is to associate a feature vector to each arm (this feature vector is known to the learner ahead of time), and the reward of pulling certain arm for a context is a function of both the context and arm features. In the absence of arm features, the other approach is to impose some structure on the unknown arm parameter vectors. There are several works which do this, and the current paper falls in this line of research. Some of these works assume the arms can be clustered into a small number of groups or can be embedded in a low-dimensional latent space and learn the low-dimensional features during the course of the online learning (https://arxiv.org/pdf/2010.12363, https://arxiv.org/abs/2209.03997, https://arxiv.org/pdf/1810.09401, https://proceedings.neurips.cc/paper_files/paper/2023/file/f334c3375bd3744e98a0ca8eaa2403b0-Paper-Conference.pdf). The diffusion prior used in the current work resembles the low-dimensional embedding assumption. In particular, it is assumed there is a latent vector (psi_1) from which all the arm parameter vectors are generated; this is a form of rank-1 assumption on the arm features. Unfortunately, none of these works were brought up in the paper. It would be great if the authors perform a thorough literature review and better position their work.

- **Linear Setting**: A lot of emphasis has been placed on the linear model in the paper. I understand it is used to derive the heuristic for the non-linear setting. Beyond that, I do not find the regret bounds derived in section 4 to be interesting. In the linear setting, there isn't a need to work with the complex hierarchical diffusion prior. It looks like one could totally remove the latent variables psi_{*, L}, .... psi_{*, 2} and simply place a Gaussian prior on psi_{*,1} and get an equally powerful model. This would also improve the regret bounds, by removing the L factor in the regret. Given this, I'm not sure about the utility of section 4.
  - Section 4.1 compares the regret bounds obtained in this work with other baselines. But this comparison is only meaningful under the assumption that the diffusion prior is properly specified. This raises the following question: why is this a reasonable prior to use in practice? How do various techniques compare if this prior is misspecified? (There are some experiments section 5.2 on prior misspecification, but the misspecifications considered there seem to be very minor)

- **Quality of Heuristics:** How good is the heuristic used for non-linear diffusion model? There is no discussion on this in the paper (In my opinion, this needs to be thoroughly discussed in the paper, as it is the primary novelty of the work). Some empirical evaluation comparing it with other standard estimation techniques (such as variational techniques, and other posterior estimation techniques) would have been helpful in understanding his question.

Limitations:
See my comments above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
mdK1vhgpa5;"REVIEW 
Summary:
The paper proposes a method to continually adapt a pre-trained classifier to an unlabeled stream of test data. They address the problem of continual test-time adaptation through the lens of Bayesian deep learning. Their method consists of three main components: (1) a variational warm-up strategy to turn any source model into a Bayesian Neural Network, (2) a mixing strategy between the source model and the last posterior to leverage the trade-off between adaptation and forgetting, and (3) a modified entropy term that is symmetric and incorporates data augmentations. The authors compare their method on standard CIFAR-C and Imagenet-C datasets to a set of TTA baselines. The paper also includes ablation studies on the components and an evaluation of uncertainty estimates.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper tackles one of the most relevant problems in continual domain adaptation, namely the trade-off between agile adaptation while preventing the forgetting of the source model. It does so by constituting a mixture of the source and last adapted model in a VI framework, which is novel to my knowledge. (originality)
- Further, in a setting where robustness is crucial and therefore uncertainty quantification can be helpful, the combination of Bayesian deep learning and continual test-time adaptation is interesting and insightful. (originality)
- The methodological backbone is accompanied by insightful ablation studies that highlight the significance of the different parts of the paper’s contribution. (quality)
- The paper is, in most parts, pleasant to read. The notation is clear and consistent, and the reader is well guided through the different sections. (clarity)
- The paper presents clear experimental evidence in support of the method. The experiments show an improvement in adaptation accuracy on already quite saturated datasets (up to 1.8% points on CIFAR-10-C). VCoTTA also seems to be advantageous on most corruption types. (significance)

Weaknesses:
The paper presents strong evidence in support of the proposed method. However, it is left unclear to me why the method performs so much better than previous approaches.

- The method consists of a range of specific components. However, in some cases, the specific design of the components is not clearly motivated. In particular, equations 10 and 13 lack supporting citations or explanations. Why have exactly these formulations been chosen?
- I’d like to get more clarity on the difference between this paper and the original CoTTA work, as it seems to me there are certain components in common (e.g., student-teacher approach, EMA). More precisely, could you please highlight the difference in the update equations between the two papers? My understanding is that adding the VI framework notably changes (i) the optimization objective by adding the KL term (instead of solely minimizing entropy) and (ii) the predictions by marginalizing out the model parameters. Where else does the VI framework contribute to differences?

Limitations:
The authors have listed limitations including computational efficiency and the need for access to the source data at adaptation time

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces VCoTTA, a novel variational Bayesian approach to address the Continual Test-Time Adaptation (CTTA) task, which focuses on effective domain adaptation during continuous domain shifts at test time. The authors' main contributions include a method to measure uncertainties in CTTA, addressing the issue of error accumulation due to the use of unlabeled samples. They propose transforming a pretrained deterministic model into a Bayesian Neural Network (BNN) using variational warm-up at the source stage, and employ a mean-teacher update strategy during test time. The approach updates the student model by combining priors from both source and teacher models, with the evidence lower bound formulated as the cross-entropy between student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating error accumulation within the CTTA framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper demonstrates originality through the novel Variational Continual Test-Time Adaptation (VCoTTA) approach, which creatively utilizes Bayesian Inference for Continual Test-Time Adaptation, and employs strategies like the variational warm-up and prior mixture techniques. The quality of the work is evident in its solid theoretical foundation, comprehensive methodology, and empirical validation on multiple datasets. 

The paper's clarity is apparent in its well-structured presentation, use of visual aids, and explicit statement of contributions. The significance of the research is underscored by its practical relevance to risk-sensitive applications, potential for broad applicability, and the reported improvements in predictive accuracy and uncertainty estimation under distribution shifts. By addressing critical challenges in CTTA, such as error accumulation and uncertainty estimation, and bridging Bayesian methods with test-time adaptation, the paper not only advances the current state of the art but also opens up promising avenues for future research. Overall, this work represents a valuable contribution to the field, offering both theoretical insights and practical advancements in continual learning and test-time adaptation.

Weaknesses:
Regarding the computational overhead discussed in the paper, while it is noted that online Variational Inference is employed to make the approach computationally feasible, a detailed analysis of the computational costs associated with VCoTTA is absent. Table 13 presents a comparison of time and memory costs, but the source of these values is unclear. Could you specify which dataset was used for these measurements? Also, is it possible to clarify whether the time and memory comparisons pertain to training or testing phases?

The manuscript contains several typographical and grammatical errors that need to be addressed. Specifically, brackets are missing in Equation (5) and in the sentence following Equation (13). Could these omissions be corrected to prevent misinterpretation of the mathematical expressions and enhance the clarity of the paper?

There are multiple grammatical issues that require rectification. The sentence ""MT is initially proposed in semi-supervised and unsupervised learning"" is somewhat unclear. Could this be rephrased for better coherence? Additionally, the sentence ""We use the mean entropy derived from a given *serious* data augmentation to represent the confidence of the two prior models, and mix up the two priors with a modulating factor"" appears to contain a typographical error and could be better structured. Could these issues be addressed to improve the readability and accuracy of the text?

The heading for Section 5.7 seems to not accurately reflect the content discussed within. Could this heading be revised to more accurately convey the main topics or findings of the section, thereby ensuring clarity and relevance for the reader?

The explanation of how MT operates in semi-supervised and unsupervised learning settings appears incomplete and potentially misleading. The current statement, ""where the teacher model guides the unlabeled data, helping the model generalize and improve performance with the utilization of large-scale unlabeled data,"" lacks specificity. Could you specify which model (teacher or student) benefits from this guidance and in what manner? Additionally, the phrase ""where the teacher model guides the unlabeled data"" seems incorrect. Could this be clarified or corrected to accurately reflect the operational dynamics of the MT framework?

Limitations:
The authors have adequately addressed the limitations in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a variational continual adaptation method. Where a sequence of test-time domain adaptation problems are shown to a model. More specifically a labeled dataset is given as an initial dataset to learn from, and then afterwards a sequence of unlabelled datasets with domain shifts are presented to the model.

Using traditional variational continual learning methods will result in error accumulations in the posterior over parameters. So the authors propose a scheme that regularizes against the posterior learned from an initial source dataset. 

The authors propose do away with using sequential Bayesian inference. Instead, the authors use a mix of a source prior (learned using labeled data) and a teacher prior. The teacher prior is an EMA of the previous task’s posterior learned with variational inference. The method the authors propose is named VCoTTA.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper is well written and the components of VCoTTA are clearly explained. 
* The authors provide an ablation to demonstrate which design choices worked well, for instance, to demonstrate that the mixture of priors worked well for test-time adaptation.

Weaknesses:
Novelty:
* In terms of novelty I’m not convinced that there are important applications of continual test-time adaptation in the form of classification tasks derived from CIFAR10 datasets. I could be wrong, but some justification in the paper is required and some more realistic benchmarks would be nice.
* Maybe I have misunderstood the variational warm-up procedure. But using the MLE estimates to initialize the BNN mean parameters was done in VCL https://arxiv.org/abs/1710.10628 . So this is not a novel idea. Furthermore, there are better ways to initialize a BNN such as using Bayesian linear regression: https://proceedings.mlr.press/v97/rossi19a/rossi19a.pdf.

Clarity
* Why does the teacher model use EMA updates instead of using the inference variational posterior?
* Why is data augmentation an important component in VCoTTA (Sec 4.2)? This is suddenly presented in the paper without justification.

Notation:
* In Section 4.2, the title is a “Mixture-of-Gaussian prior”, but Eq 11 is an addition of two priors which are Gaussians by design, so this is a “scale-mixture prior” https://arxiv.org/pdf/1505.05424, not a mixture of Gaussians (https://www.inf.ed.ac.uk/teaching/courses/mlpr/2016/notes/w9b_mixture_models.pdf)?
* Confusing notation of the source prior: it is denoted as $p_0$ (Fig 2) and $p_1$ (Eq 11), this needs to be consistent.


Empirical weaknesses:
* No standard errors in the experimental results. So difficult to see which method outperforms another.
* Uncertainty estimation is not performed with standard methods like ECE or OOD detection like https://arxiv.org/abs/2102.06571. It is unclear to me whether the Brier Score estimates uncertainties.

Limitations:
There is a good discussion on the limitations of VCoTTA.

One limitation that is not discussed is in the effectiveness of (variational) Bayesian sequential inference methods. Weight space variational inference has been shown to be very difficult to do in practice, https://arxiv.org/abs/2301.01828. So weight space variational inference (without tricks like multi-head networks and coresets) might not be the best choice when wanting to remember a source distribution.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a variational Bayesian approach to handle uncertainties in continual test-time adaptation (CTTA). The source pretrained model is made Bayesian by variational warm and a mean-teacher update strategy is used at test time. To avoid drift due to uncertainty of priors using only unlabeled data at test time, the paper proposed to update the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method’s effectiveness in mitigating error accumulation within the CTTA framework.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Novelty: Bayesian approach in Continual Learning is a principled and elegant approach to the problem which this paper is relying on. In CTTA, there are additional issues due to the uncertainty of the prior distributions using only the unlabeled data from unknown domains. This paper presents a novel solution by using adaptive mixture prior models and student-teacher update on top of an existing framework.

- Relevance: CTTA is a topic that can interest a general audience, and the  Bayesian and variational framekwork can also be of interest to many.

Weaknesses:
- While Bayesian approach is nice in principle, it can be computationally demanding and offer little benefit in practice. Most of the existing CTTA methods are computationally and memory efficient, whereas this method present an opposite end of the spectrum. While the reported results are impressive, it is unclear to me why the proposed method is superior to other SOTA methods.
- The hyperparameter selection process is not addressed in the paper, which is critical in TTA where all hyperparameters should be predetermined before data access. How are they chosen?

Limitations:
Limitations are mentioned in the paper, albeit very brief. Overall, the proposed method is more complex and demanding (such as requiring a pretrained probabilistic model or source data) for TTA applications. Perhaps the proposed approach may work even better with UDA or other CL scenarios with (partially available) target labels?

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
m772lelDe3;"REVIEW 
Summary:
The paper studies the conditions that lead to consensus in matrix-weighted consensus networks when constant time delays are present. The analysis considers both leaderless and leader-follower settings. The paper considers single integrators with uniform time delays, heterogeneous time delays, and double integrators with two constant time delays. The paper derives the conditions for asymptotic convergence to a consensus or clustering configuration. The mathematical techniques include direct eigenvalue evaluation and application of the Lyapunov-Krasovkii theorem. The paper explains how the derived results can be applied to the problem of bearing-based network localization.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1. 1The paper provides a novel theoretical characterization of the asymptotic consensus for matrix-weighted consensus networks in several settings. In multi-agent networks  

S2.	The analysis is clearly presented and the arguments are logical and well-structured. The employed techniques are innovative.

Weaknesses:
W1.	Although the scope of NeurIPS is broad, and papers are encouraged from diverse fields, this paper makes little attempt to connect to any learning-based problem or application. The paper seems to be a poor match for a machine learning conference. There is not a single citation of a paper from one of the leading machine learning conferences or journals. 24 of the 37 references are associated with papers in control journals and conferences. The paper would be of much more interest to the control community. If the authors consider this to be a significant research contribution that furthers the understanding of matrix-weighted consensus, then why not submit it to Automatica (this is the forum for several other cited matrix-weighted consensus papers), or IEEE Trans. Automatic Control, or IEEE Trans. Control of Networked Systems. If there is a belief that the paper exposes a problem or a technique to the machine learning community, then there must be a much more convincing effort to highlight the connections – where/how would the machine learning community find the presented results useful? 

W2.	The main paper shows how the theoretical results are applicable to the bearing-based network localization problem. There is very little explanation of whether the proposed approach to localization is advantageous, and how the theoretical results are useful – whether it is for analysis of a network, or for design of a network. The appendix provides the results of simulations. But even there, the discussion is limited to simple observations regarding the behaviour of the simulated network (consensus/instability). The simulation analysis needs to be more convincing and explain in detail how the theoretical results are useful for this problem. Alternatively, additional theory could be provided that pertains to the bearings-based network localization task. 

W3.	The presentation of the paper should be improved. In particular, the figures on pages 16-18 are far too small. The text in these figures is illegible. It becomes almost impossible to understand what information the figures are supposed to convey.

Limitations:
The paper includes one or two sentences in the conclusion to discuss the limitations. The only acknowledged limitation is the restriction to the constant time delay setting. The paper would be strengthened by a much more thorough discussion of the limitations. For example, it would be helpful to understand whether the authors consider the constant time restriction to simply lead to conservative bounds, or whether the results would be completely inapplicable in a variable time delay setting.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Paper under review analyzes the consensus of agents over a network. The agents have arbitrary but identical state space dimension, so not just scalar dynamics. The communication between agents is delayed and can be heterogeneous. Lyapunov–Razumikhin functionals with an LMI (that grows with the size of the network) are the main analysis tools. The literature in this area is vast. The results may already be contained as special cases of more general results from the control literature that was not referenced in the original submission

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- originality: very crowded area of research
- quality: high quality except for not being very precise about what is different from other work, and lacks a detailed discussion about what impacts the LMI bound
- clarity: paper is well written
- significance: more general results on the same topic appear to already exist

Weaknesses:
- Authors discuss some of the relevant literature but are never explicit about what actually is different. This makes it very challenging to understand what is different and new about this contribution.
- Along the same lines as the above statement. Results and analysis techniques may be already present in the papers referenced below.   
	- Jiang, W., Liu, K., & Charalambous, T. (2022) in particular solves the more general problem of consensus with heterogenous delays where each agent is an arbitrary linear dynamical system (A,B,C). The results in this paper appear to be a subset of that class of systems.
- LMI grows with the size of the network making it not scale well
- No interpretation of the LMI once it is derived (but they are huge matrices which kind of make them hard to interpret)

Limitations:
limitations discussion is not really sufficient, they only discuss future directions

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates consensus conditions for matrix-weighted consensus networks, both leaderless and leader-follower, in the presence of constant time-delays. It explores delayed consensus algorithms for networks of single- and double-integrators using relative positions. The study derives conditions for networks to achieve consensus or clustering using eigenvalue evaluation and the Lyapunov-Krasovkii theorem. It also discusses an application in bearing-based network localization. Some numerical simulations are also provided to demonstrate effectiveness of the results.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper studied an important problem in the control theory, proposing novel algorithms and extending existing work to matrix-weighted networks with time-delays. They provided structured and rigorous mathematical analysis/proof of their results and provide helpful numerical simulations to validate their theoretical results.

Weaknesses:
In my opinion, the major weakness (in terms of publication at NeurIPS) of this paper is below. This paper has strong focus on control theory and consensus algorithms, which makes it less relevant to the core interests of the NeurIPS audience, since NeurIPS emphasizes more on machine learning methodologies and applications. The paper does not clearly establish connections to machine learning problems or provide good experimental results involving machine learning tasks.

This paper contains quite some valuable novel research results, but it seems to me it is more appropriate for publication on a traditional control journal or conference, rather than such a top tier machine learning conference like NeurIPS. It is just not a good fit, and it might be better to reserve the space (which is quite limited) for some other good candidate paper submissions more relevant to Machine Learning, which are more aligned with the interests of core audiences of NeurIPS.

Limitations:
N/A.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
lizRmKCnBp;"REVIEW 
Summary:
The manuscript introduces a neural compression paradigm for effectively compressing diverse sets of 3D geometry models. The authors propose a two-stage framework that first converts irregular mesh models into a regular 4D TSDF-Def volume representation and then employs a quantization-aware auto-decoder network to achieve redundancy elimination and compact representation. The method claims to compress a large number of 3D mesh models with high accuracy and preservation of geometric details, outperforming state-of-the-art methods both quantitatively and qualitatively.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper presents a unique method for compressing 3D geometry sets by leveraging neural networks, which is a significant advancement in the field. NeCGS achieves an impressive compression ratio, which is a critical metric for 3D geometry data compression.
- The method maintains high accuracy and preserves detailed geometric structures even at high compression ratios. The authors have conducted comprehensive experiments and ablation studies across various datasets, demonstrating the effectiveness of their approach.
- The inclusion of source code in the supplemental material enhances the reproducibility and transparency of the research.
- The paper is well-organized, with clear explanations of the methodology and results.

Weaknesses:
- The manuscript mentions that the optimization process for TSDF-Def volumes is time-consuming (over 15 hours), which could be a limitation for practical applications. The manuscript should address the long optimization time required for the TSDF-Def volumes. Future work could focus on accelerating this process to make the method more practical.
- While the method performs well on tested datasets, it is unclear how well it generalizes to other, more complex, or varied 3D geometry sets, such as some geometry with thin structures or open boundaries (cloth). 
- The choice of an auto-decoder network is effective, but the paper could benefit from a more detailed explanation of why this architecture was chosen over others.
- While the method outperforms existing techniques, a more thorough comparison in terms of trade-offs, especially related to computational resources, would be insightful.
- The paper could provide more insights into how the method scales with the size and complexity of the 3D geometry sets. The paper should include scalability tests to understand how the method performs with larger and more complex datasets.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a neural compression algorithm, NeCGS to significantly compress geometry datasets. The algorithm mainly consists of 2 components, 1) regular geometry representation: This is an optimization algorithm to optimize the TSDF field such that the error between the original geometries and the geometries reconstructed by the deformable marching cube algorithm is minimized and 2) compact neural representation: regresses the optimized TSDF-def fields from compressed latent states, quantizes the latent states and compresses them further into bitstreams. The trained decoder can then be used to reconstruct the TSDF-def fields and the geometries can be reconstructed using the DMC algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The NeCGS algorithm can provide high compression ratios with impressive reconstruction capability of the geometries. Better geometry representations can be achieved using the proposed optimization algorithm. This is evident from the ability of the DMC method to accurately reconstruct surfaces. The DMC algorithm is also significant and seems to provide better reconstruction of detailed structure in the geometries. Overall, the developed compression method has high potential and the results presented in the paper are very impressive.

Weaknesses:
The biggest weakness of the proposed approach is the computational cost of the method. The exorbitantly large times required to compress the datasets reduce the value proposition. Additionally, it is not clear how much the computational cost scales with the size of the geometry dataset.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method to compress 3D geometry of diverse categories of objects. In the first step, the paper proposes a method to first convert an irregular mesh to a regular representation like a 4D TSDF-Def volume that implicitly describes the geometry. After this, an auto-decoder is trained that learns to reconstruct the 4D TSDF-Def volume from a compressed feature vector which is unique for each shape. Hence, with this design the model can summarize the similarity of local geometric structures within and across different 3D meshes resulting in a compact representation. Results on AMA, DT4D and Thingi10K datasets shows that the model can achieve compression of 3D models to a reasonable extent.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) **Clarity:** the paper is well written with each component of the method explained clearly which is easy to understand.
2) **Reproducibility:** All the details to replicate the results are provided along with the code and architecture details in the supplementary material.

Weaknesses:
1. The intuition behind preferring TSDF-Def 4D volume over TSDF 3D volume is unclear, even though an ablation study shows better reconstruction for thin structures. The quantitative results in Table 2 only show marginal improvements. An brief intuitive explanation of the design choice is helpful.
2. There are lot of methods which try to compress a neural field. For e.g. Triplanes[1], HashGrid [2], Vector Quantization [3], TensoRF [4], Dictionary Fields [5]. It is not very clear why this method does not compare with all these techniques which can be used for compression? 
3. Can this method generalize? Can I use the trained auto-decoder setting to compress a new 3D mesh on which the model is not trained on? How about other methods with which the method compares.
4. The paper does not do a relative comparison of the compression time with the baseline methods. Given the optimization time shown in Table 3, I have concerns about the practical usage of this method.

[1] Peng, Songyou, et al. ""Convolutional occupancy networks."" ECCV, 2020. \
[2] Müller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives with a multiresolution hash encoding. ACM TOG, 2022. \
[3] Takikawa, Towaki, et al. ""Variable bitrate neural fields."" ACM SIGGRAPH, 2022. \
[4] Chen, Anpei, et al. ""Tensorf: Tensorial radiance fields."" ECCV, 2022. \
[5] Chen, Anpei, et al. ""Dictionary fields: Learning a neural basis decomposition."" ACM TOG, 2023.

Limitations:
Limitations are adequately discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
this paper looks at the problem of compressing 3d shapes (esp geometry). this paper proposes a two stage approach. the first stage is regular geometry representation. the second stage is compact neural compression. results show some improvements.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. compressing 3d shapes is important to many applications

Weaknesses:
1. this paper over claims what it does. in L1-3, it says that they made the first attempt to tackle the problem of compressing 3D geometry sets containing diverse categories. this isn't true. there are at least two papers doing geometry compression of 3D geometry [a], [b].  

[a] On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes https://arxiv.org/abs/2009.09808
[b] Neural Progressive Meshes https://arxiv.org/abs/2308.05741

2. [a] and [b] are very important references but they are not cited nor discussed. it's not necessary to compare the proposed method with [a] and [b], but at least the authors should acknowledge the existence of these two papers.

3. optimization time is too long

4. it is unclear whether the proposed method is reproducible

5. typo L43: Matching cubes -> Marching cubes

Limitations:
yes

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
lTUXlmjcva;"REVIEW 
Summary:
This paper proposes the affinity score, which measures the non-linearity of an activation function $\sigma(X)$ given the distribution of $X$.
The affinity score is defined based on how well the 2-Wasserstein distance $W_2(X, Y)$, where $Y=\sigma(X)$, is approximated by $W_2(N_X, N_Y)$, where $N_X$ and $N_Y$ are Gaussian approximations of the distributions of $X$ and $Y$, respectively.
Note that $W_2(N_X, N_Y)$ has a closed-form solution, and it holds that $W_2(X, Y) = W_2(N_X, N_Y)$ if the relation between $X$ and $Y$ is locally affine on the support of the given $X$.
The authors then propose to characterize a DNN model by the set of affinity scores of activation functions in the model under a given input distribution.
Experimental results suggest that the affinity scores are relatively low in transformer-based vision models, meaning that the activation functions are used in a more non-linear region compared to CNN models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* The proposed score presents an interesting insight in comparing the series of CNN models and transformer-based models. Experiments suggest that transformer-based models utilize the non-linearity of activation functions more efficiently, leading to the higher prediction performance.

Weaknesses:
* It is empirically shown that the proposed score has a low correlation with existing non-linearity metrics such as R^2, but it is unclear whether the existing metrics are insufficient to analyze the DNN models in the way proposed in this paper. I would like to see how the distribution in Fig. 3(C) changes when other metrics such as R^2 are used instead of the proposed $\rho_{aff}$.
* In my opinion, one would expect the nonlinearity score to behave symmetrically at $x=0$ for activation functions like ReLU, but the proposed affinity score seems to have a lower score at negative $x$, as shown in Fig.2 or Fig.6. Is there any reasonable explanation for such a behavior of the proposed score?

Limitations:
See weakness above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes empirical statistics about different DNN architectures in the hope to shed some light into why some architectures are better than others for some computer vision tasks. To do so, the study leverages common optimal transport results on DNN's internal representations, under some strong assumption about the distribution of those representations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper proposes to consider an interesting and useful question of going to the bottom of why some architectures are better than others as measured by some restricted downstream task.

Weaknesses:
- I do not agree with the following statement `Without non-linear activation functions,
84 most of DNNs, no matter how deep, reduce to a linear function unable to learn complex patterns.` as to me, models such as transformers with linear attention and linear MLP blocks have no actual nonlinearity but are higher order polynomial of the input, i.e., are not linear. Could the authors provide clarifications on that statement or did I misunderstand something?

- I also disagree with the following `Activation functions were also early identified [29, 30, 31, 32] as a key to making even a shallow
86 network capable of approximating any function, however complex it may be, to arbitrary precision.` since again, Fourier series for example can approximate any function as well. Hence DNN nonlinearities are certainly not the key ingredient to function approximation in general

- Many formal results such as Theorem 3.3 are well known and have been established for years (even decades) but no reference is provided which is misleading to the reader.

- Fig 2. is also misleading since the ""nonlinearity"" of any activation function depends on the range of the inputs. The only case that wouldn't be true is e.g. for ones with constant second derivatives, i.e., a linear activation function.... hence again that statement is highly misleading in presenting ReLU as inherently benefiting form that property compared to others

- the statement `No other metric extracted from the activation functions of the
260 considered networks exhibits a strong consistent correlation with the non-linearity signature.` is again an overstatement as the authors only compare with a few alternatives and theorem is provided to support such a statement

- the statement `We proposed the first sound approach to measure non-linearity of activation functions in neural
270 networks` is also incorrect, see e.g. 
  - https://jmlr.org/papers/v20/18-418.html
  - https://arxiv.org/pdf/2301.09554
  - https://arxiv.org/abs/1810.09274
  all the above works have been published in peer reviewed journals/conferences

Limitations:
In addition to my concerns expressed above, the study does not provide any actionable insights or understanding on the ""why"" of different architectures performing differently beyond the proposed statistical numbers. How could one use the provided analysis to better design model architectures or for model selection? 

Also, the paper does not provide any novel theoretical results. All the major theorems and results are already widely known within the community, yet they are presented as part of the contributions. With that in mind, the paper solely leverages existing OT tools, with some underlying simplifications on the DNN's data distribution, and report computed metrics. Hence the study falls below acceptance level in my opinion and would need a major rewriting + additional novel contributions to be worth acceptance.

The writing style is also filled with unsupported claims and highly misleading statements (see the **Weaknesses** examples).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel method for quantifying the non-linearity of activation functions in neural networks, termed the ""non-linearity signature."" Using an affinity score derived from optimal transport theory, it measures the non-linearity of individual activation functions. It defines the non-linearity signature as a comprehensive set of these scores across all functions in a deep neural network (DNN). The study compares these signatures across a range of popular DNN architectures in computer vision, revealing clear patterns in their evolution over the past decade, notably showing a trend towards decreasing non-linearity until the disruptive impact of vision transformers. It emphasizes the uniqueness of their measure, as it does not strongly correlate with other metrics across different architectures. The approach could potentially be applied to analyze the non-linearity of newer large language models (LLMs) and identify innovative neural architectures that optimize internal non-linear characteristics for enhanced performance, crucial in the era of costly experiments with large-scale model optimizations.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Novelty and importance. The paper introduces a theoretically grounded measure, the affinity score, for quantifying the non-linearity of activation functions using optimal transport theory, providing a robust framework for analysis. This is the first approach to approximately measure the non-linearity of DNNs, which is crucial for understanding their inner workings.
2. Solid theoretical and experimental validation.  The method is grounded in optimal transport theory, providing a rigorous theoretical foundation for the proposed non-linearity signature. This enhances the credibility and robustness of the findings. The experimental results demonstrate the practical utility of the non-linearity signature. It can predict DNN performance and meaningfully identify the family of approaches to which a given DNN belongs, making it a valuable tool for researchers and practitioners.
3. Clear Writing. The structure of the paper is well-organized, with a clear presentation of background knowledge, theoretical properties, experimental evaluations, and conclusions. This clarity aids in understanding the contributions and implications of the research. The paper's figures and tables are comprehensive, providing clear and precise information, and the writing maintains a coherent logical sequence.

Weaknesses:
1. The authors should discuss more activation functions. Currently, only ReLU, Tanh, and Sigmoid are included.  While these are among the most commonly used activation functions in neural networks, many other activation functions have been introduced and shown to be effective in various contexts, like GELU. Including a more comprehensive analysis of a diverse set of activation functions would enhance the robustness and applicability of their proposed method. 
2. There is currently some research on the nonlinearity of deep neural networks that should be compared and discussed.
3.  It would be beneficial to showcase examples from domains beyond computer vision. While the paper focuses on computer vision tasks, it may not address the non-linearity signature's applicability to other domains such as NLP, speech recognition, or reinforcement learning. The findings might be less generalizable if the proposed measure does not perform equally well across diverse types of tasks and data.

Limitations:
Yes, they have discussed the assumption of Theorem 3.3.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
lEDuaGGiCV;"REVIEW 
Summary:
In this paper the authors introduce LUCY, a new LLM based framework for converting text-to-SQL to query databases. Primarily this framework focusses on addressing user queries to databases that contain a large number of tables with complex relations between them.
The core idea of this approach is to decompose the query generation process into distinct stages. LLMs (GPT-4) are utilized for generative tasks such as identifying relevant tables and attributes and generating the SQL query. Meanwhile a deterministic constraint solver (OR-Tools) is employed to map relationships between these elements. In essence LUCY processes a user query through 3 phases namely MatchTables, GenerativeView and QueryView phases. In the MatchTables phase the goal is to identify the relevant tables and attributes. This is accomplished by iteratively prompting a Large Language Model (such as GPT-4) to identify relevant tables and attributes based on the user query and the database model, which includes the schema and an optional list of high-level design patterns. The database model is presented in a hierarchical manner and explored using a breadth-first search approach. Once the relevant relations and attributes are identified a schema graph is constructed and solved using a constrainst solver (i.e to identify the optimal path to join the tables) to build a view in the GenerativeView phase. A LLM is then prompted to generate a SQL query given the summary view and the user query in the QueryView phase. The authors further conduct experiments that demonstrate that the proposed technique achieves a better execution accuracy as compared to the existing state-of-the-art techniques on standard datasets(ACME, BIRD). Furthermore, they also introduce a show large improvements on a new benchmark dataset (Cloud Resources).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The literature review is comprehensive and the paper does a good job at clearly defining the problem to solve. 
2. The novelty of the proposed approach lies in the decomposition of tasks involved in generating SQL queries. By employing LLMs to handle specific subtasks, it effectively circumvents the need for LLMs to perform complex reasoning. A core distinguishing factor from prior research is the use of constraint solvers to identify the relevant paths for joining the identified tables. 
3. The authors also demonstrate that the proposed approach achieves a better execution accuracy than the existing SOTA on several benchmarks.

Weaknesses:
1. The paper ends abruptly without a clear and comprehensive conclusion. The paper presentation needs improvement in this regard.
2. The authors introduce a new benchmark for evaluation but do not offer sufficient details regarding it. A detailed overview of the queries and an analysis of why the existing SOTA techniques do not perform well on the same could be provided which could greatly inform future work.
3. The practical utility of the proposed technique seems to be limited as each user query requires multiple calls to be made to LLMs thereby entailing both increased latency and cost.
4. The error analysis is not very comprehensive and could be improved. For instance how does this technique fare when the names of entities in database schemas are not semantically meaningful or if there are conflicts in descriptions etc (as is often the case in real-world industrial databases).

Limitations:
1. As the technique leverages LLMs it seems to be heavily reliant on having semantically meaningful entity names /descriptions 
2. The proposed technique seems sensitive to hallucinations as it involves processing a query through multiple LLM phases. The errors in any of the earlier phases would result in it propagating to the next stage. For instance as the authors pointed out if the MatchTables phase produces an extra table this could in turn effect the end output.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of developing effective LLM-based assistants for querying SQL databases. In this context, users pose questions to a relational database in natural language, and the goal is to generate a SQL query that correctly answers the user's question when executed. The authors focus on overcoming a limitation of current text-to-SQL approaches: the difficulty LLMs face in handling databases with numerous tables and complex relationships, making it hard to determine the necessary table joins for the query.

To tackle this issue, the authors propose a workflow that begins with using the LLM to identify relevant tables and their attributes. In the second step, a constraint satisfaction solver (CSP) is employed to determine the necessary joins while adhering to database constraints. In the third step, a materialized view is created by joining the relevant tables. Finally, this view, combined with the user's question, is used to prompt the LLM to generate the final SQL query.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper tackles a very relevant practical problem, which is attracting significant attention in both academia and industry. The proposed workflow may add practical value.

Weaknesses:
The paper lacks depth, and the writing does not, in my opinion, meet the quality standards required for a venue like NeurIPS. Additionally, several potential limitations of the proposed workflow are not discussed.
	•	Accuracy of the answers is not the only important requirement in generating SQL from text. Database users also expect query generation to be time-efficient. The proposed workflow includes several computationally intensive steps: first, solving an NP-complete problem (CSP), and second, creating a potentially enormous materialized view by joining many tables. I would have expected a discussion on the computational limitations of this approach.
	•	The workflow lacks sufficient precision and clarity. For example, it is unclear whether the final query is expressed with respect to the materialized view as the only table or with respect to the original schema. Additionally, how lookup tables and various schema design patterns are identified in the input database is not well-explained. The authors claim that their approach guarantees the generated query respects database constraints, but this guarantee is not clearly defined. Algorithm 1 is underspecified; at this high level of detail, the algorithm seems redundant and could be subsumed by the text description. The exact SQL fragment covered by this approach is also unclear. While the limitations section mentions that queries requiring the union operator are not supported, it is unclear if other standard SQL constructs are also unsupported.
	•	While relevant related work is cited, the main body of the paper lacks a detailed discussion on the contribution in relation to recent approaches.
	•	The evaluation section is somewhat lacking. The tables are confusing, and it is unclear what each of the rows actually represents.

Limitations:
The limitations section mentions some but not all of the relevant limitations of this approach.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author proposes a new method, Lucy, designed to handle large databases with complex relationships between objects. Lucy operates through three steps: MatchTables, GenerateView, and QueryView. It first identifies relevant tables and attributes using LLMs, constructs a combined view with an automated reasoner, and generates the final SQL query. Lucy shifts complex reasoning from LLMs to a CSP solver, supporting various database design patterns. Experiments on ACME insurance, Cloud Resources, and the two BIRD databases show that Lucy outperforms other zero-shot text-to-SQL models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The proposed method offers a fresh perspective on tackling text-to-SQL research with a logical workflow.
- The paper is well-written and easy to follow.

Weaknesses:
- I am not convinced by the motivation of zero-shot text-to-SQL with the example of industrial databases having complex relationships. Text-to-SQL systems deployment in real-industry requires high performance. I doubt that people won't be using zero-shot models for real use applications. In KaggleDBQA, it also states ""we believe the zero-shot setting is overly-restrictive compared to how text-to-SQL systems are likely to be actually used in practice."" I would like to hear the authors' thoughts on this.
- The paper does not appear to be well-grounded in text-to-SQL research. For example, one way to handle complex relationships in text-to-SQL using LLMs is through schema linking. However, the paper does not mention this area of research and instead proposes MatchTables, seemingly ignoring the rich literature of text-to-SQL works. Other approaches include least-to-most prompting attempts in text-to-SQL for task decomposition and Natural SQL for intermediate representation (although it does not handle query nesting). Properly discussing these relevant methods of the proposed method will better situate the work.

Limitations:
The limitations of the work are well-stated.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Lucy, a framework for solving Text2SQL by LLMs, particularly for complex enterprise databases. Lucy leverages LLMs' understanding and reasoning capabilities to handle intricate database relationships and constraints. The framework operates in three phases: identifying relevant tables and attributes (MatchTables), constructing a view through constraint reasoning (GenerateView), and generating the final SQL query (QueryView). The empirical studies show Lucy achieves performance improvements on several zero-shot Text2SQL tasks.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Text2SQL is an essential problem in commercial scenarios.

Weaknesses:
The draft seems far from complete, so leave some high-level suggestions.
1. Make the title, abstract, and introduction more concrete. It is hard to tell the contribution or uniqueness of this work among other papers about Text2SQL by LLMs.
2. Survey related works and clearly state the contribution/novelty of the proposed method against others.
3. Define the terminologies or abbreviations before their first appearance.
4. Make the draft concise by removing unnecessary content. For example, the first challenge introduced in Motivation section is not relevant to this work.
5. The empirical studies could be more convincing by following others' evaluation protocols, such as BIRD.
6. Lack of comparison to other competitors.
7. The figures, tables, and their captions should be self-explanatory.

Limitations:
There is a discussion about the limitation, though the first limitation seems too broad and unnecessary.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ks0FrTSCnK;"REVIEW 
Summary:
The paper extends the problem setting of learning with noisy labels (LNL) to include open-set noise, where noisy labels may come from unknown categories, in contrast to the traditional focus on closed-set noise. The authors theoretically compare the impacts of open-set and closed-set noise and analyze detection mechanisms based on prediction entropy. They construct two open-set noisy datasets, CIFAR100-O and ImageNet-O, and introduce an open-set test set for the WebVision benchmark to validate their findings. Their results show that open-set noise exhibits distinct characteristics from closed-set noise. The paper emphasizes the need for comprehensive evaluation methods for models in the presence of open-set noise, calling for further research in this area.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The research problem is interesting. Compared with learning with closed-set noise, learning with open-set noise is under-explored. 
- The theoretical analysis seems to be solid.

Weaknesses:
- Some technical details are hard to follow. Writing needs to be polished. 
- The contribution from the algorithm perspective is not enough.

Limitations:
N/A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an approach to address the challenge of open-set noise in the context of learning from noisy labels. The authors propose a method that differentiates between 'easy' and 'hard' types of open-set noise, which is critical for improving the robustness and performance of learning models faced with noisy data. By integrating existing Learning with Noisy Labels (LNL) techniques with novel entropy-based noise detection mechanisms, the paper presents both theoretical insights and empirical validations of the proposed methods. The contributions are significant as they offer a refined perspective on handling different noise complexities, which can enhance the utility of machine learning models in real-world applications dealing with noisy labels.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Originality: The paper addresses the issue of open-set noise in learning from noisy labels with a novel approach, differentiating between 'easy' and 'hard' noise types. This nuanced consideration is original as it pushes the boundaries of how noise is typically treated in noisy label learning.

Quality: The theoretical explanations are thorough and complemented by robust empirical evidence that strengthens the methodological claims.

Clarity: The paper is well-structured, offering clear explanations of complex concepts, which aids in understanding the proposed methods and their implications.

Significance: The significance of this work is evident as it tackles a critical issue that can potentially enhance model robustness and performance in real-world scenarios where label noise is common.

Weaknesses:
Dependency on Specific Methods: The reliance on entropy-based techniques for noise distinction may not generalize across all scenarios or noise types.

Experimental Scope: The experiments primarily utilize synthetic datasets, which might not fully capture the complexity of real-world data applications.

Limitations:
Limited Experimental Scope: The experimental validation focuses predominantly on synthetic datasets like CIFAR100-O and ImageNet-O. While these are commonly used in the research community for benchmarking, the real-world implications of the findings might be limited without additional testing on more varied and real-world datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on open-set label noise problem. Authors first formally extending closed-set transition matrix to open-set transition matrix and define two noise ratios for open-set and closed-set separately. 
Then authors define error inflation rate as a measurement for noisy label impact and measure for two conditions, classifier fitted noisy distribution or memorized (overfit) noisy label. Later, authors propose a new type of open-set noise by exclusively transitioning outlier classes to a specific inlier class, and consider this as a ""hard"" open-set noise and traditional open-set noise as ""easy"" case. Authors further analysis two noise types on two classifier conditions and claim traditional entropy based open-set detection might only works on ""easy"" case. Experiments are performed on CIFAR-100, ImageNet and Webvision datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Authors formally define open-set noise with a similar symmetric/asymmetric setup as closed-set noise, and find out that it shows opposite trend with different classifier cases.

Weaknesses:
- The experiment parts lack of baselines. With a new type of noise proposed, previous baselines on easy open-set noise should be run to assess the performance gap and set up the benchmark.
- Figure 4 (a) and (b) have similar distribution, it is hard to draw conclusions from entropy dynamics.
- Supp E.1 results are confusing. ""X+EntSel"" should be a better strategy since it selects inlier clean samples. However, why the closed-set classification accuracy is always the worst? Table 1 Webvision result is similar as well. Why is the claim ""EntSel + SSR improves open-set detection performance"" valid? The Acc and AUC are both dropping after adding EntSel. Why SSR/DivideMix + EntSel is always the worst performance? Considering it is a combination of inlier and clean, shouldn't it be the best performing one? I assume this is still the normal accuracy and AUC, which is the higher the better.

Limitations:
Authors do not address any limitations in conclusion. A possible limitation might be related to approximation of fitted and memorized classifiers.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper refines the problem of learning with noisy labels (LNL) by addressing the often overlooked issue of open-set noise. It provides a comprehensive theoretical analysis comparing the impacts of open-set and closed-set noise, introduces novel datasets for empirical validation, and explores the effectiveness of entropy-based noise detection mechanisms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper offers a thorough theoretical analysis of the differences between open-set and closed-set noise, extending the current understanding of LNL.
- The exploration of entropy-based mechanisms for detecting open-set noise adds a practical tool for improving LNL methods.
- This paper is well-written and easy to understand.

Weaknesses:
- The author summarizes two types of open-set noise, i.e., the easy and the hard noise, which is very similar to the symmetric and asymmetric label noise from the perspective of the transition matrix. So does there exist the instance-dependent open-set noise? What is its form if exists?

- In Section 3.5, the author conducts analyses regarding entropy dynamics-based open-set detection, which belongs to the **Fitted case**. If adopting the vision language model (such as CLIP) to fine-tune and detect the open-set noise, is it aligned with the **Memorized case**? It would be better for the author to provide a real-world application for the memorized case.

- The author should clearly illustrate the construct method of closed-set in the experiment (Figure 3) for reproducibility.

Limitations:
See weaknesses

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
kOp0kiXZ3a;"REVIEW 
Summary:
The paper addresses challenges in model quantization for deep neural networks (DNNs), focusing on optimizing quantization-aware training (QAT) across multiple bit-widths with weight-sharing. To this end, this paper introduces a novel quantization method that exploits the highest integer precision to achieve nearly lossless bit-switching, reducing storage without relying on full precision. Key contributions include: (1) Adaptive Learning Rate Scaling: A technique that dynamically adjusts learning rates for different precisions to address competitive interference and inconsistent gradient issues during one-shot joint training. (2) Double Rounding: An extension for one-step rounding quantizer in fixed-precision quantization to improve accuracy. Experimental results on the ImageNet-1K dataset show that the proposed methods surpass state-of-the-art approaches in both multi-precision and mixed-precision scenarios, achieving higher efficiency and accuracy.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This submission is well-written, as well as with good figures in Sec.4.
- The authors conduct extensive experiments on multiple datasets and multiple networks.

Weaknesses:
- Some analysis is missing. For example, I'm wondering whether the second rounding leads to more quantization errors, as the first rounding is used to produce INT8 weights and second rounding is then performed to quantize lower bit-width, the twice quantization is possible to cause more clipping errors and rounding errors, some analysis could enhance the strength of proposed methods. 
- Some designs should be further clarified, e.g., why ALRS is applied only for the scaling factors? Intuitively, weights of small bit-width is induced large gradient variance by STE, and thus the weights of small bit-width should also benefit from using smaller LR. 
- Fig. 1 is a bit confusing, some colored arrows are not well explained. 
- This works essentially lies in the research of mixed-precision quantization, so I think it is better to compare more MPQ (e.g., HAQ, DNAS, LIMPQ, etc) research in the Sec.4. Moreover, some recent papers on multi bit-width quantization are missed on the , e.g., [1] (PTQ-based) and [2][3] (QAT-based), which could be included into the Related Work. 

[1] Xu, Ke, et al. ""PTMQ: Post-training Multi-Bit Quantization of Neural Networks."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 14. 2024.

[2] Tang, Chen, et al. ""Retraining-free model quantization via one-shot weight-coupling learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. 

[3] Zhong, Yunshan, et al. ""MultiQuant: A Novel Multi-Branch Topology Method for Arbitrary Bit-width Network Quantization."" arXiv preprint arXiv:2305.08117 (2023).

Limitations:
Please refer to the weaknesses. Overall, this paper currently needs more experiments and analysis to reveal some designs are reasonable.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a QAT scheme to jointly optimize a single model with different precisions. The authors apply their scheme on various CNN-based models on CIFAR-10 and ImageNet datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written
2. The ablation study is strong in my opinion and they evaluate various aspects of their scheme

Weaknesses:
1. I think the main limitation of the paper is the models and datasets. I believe that the study should be done on larger models (LLMs for example) as a architecture goal. For example, the authors show that they do not save a FP32 master copy of the model in their scheme. However, ResNet style models (or MobileNet) are easy to fit in even moderate GPUs and I don't think FP32 master copy is a big problem in that case (please correct me if I'm wrong).

2. I couldn't find a source-code to reproduce the results of the paper in my side.

Limitations:
yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses advanced methods in multi-bit model quantization. Specifically, this paper proposes a method for one-shot joint training of multiple precisions. To this end, the authors introduce a double-rounding quantizer that leverages the highest integer precision to achieve nearly lossless bit-switching while reducing storage requirements. Moreover, they also propose an Adaptive Learning Rate Scaling technique that adjusts learning rates dynamically for different precisions. Two proposed techniques mitigate the competitive interference between bit-widths caused by inconsistent gradients of different precisions during biased gradient estimation. They also extend their Double Rounding method to support one-shot mixed precision training and develop a Hessian-aware Bit-witdh sampling strategy. Experimental results on the ImageNet-1K classification task show that their methods outperform state-of-the-art one-shot joint QAT in both multi-precision and mixed-precision scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Eliminating the costs of retraining for mixed-precision quantization is a meaningful and challenging topic.

- The end-to-end experiments are sufficient, and the presentation is good.

Weaknesses:
- More uniquness analysis needed. The using of Hessian information seems a bit trivial, each layer's Hessian is just used to compare with the averaged Hessian trace. Firstly, as shown in recent zero-cost NAS research [1], the architectural proxies will be less effective as the training goes on, I'm not sure the Hessian information obtained on the initial full-precision model will remain useful as the quantization-aware training continues. Moreover, the sampling probability is modified with a simple ascending heuristic, which is not Hessian-aware. 

- Also applies here: the design of the double-rounding quantizer is similar to Bit-Mixer, Adabits, and ABN. Specifically, ABN also uses 

- ALRS needs further ablations. In ALRS, the authors use a fixed scaling ratio to bit-widths, e.g., 8-bit is 1, 6-bit is 0.1, and 4-bit is 0.01, the choice of these scaling factors still requires more ablation studies and discussions. 

- More comparisons needed. Since this paper adopts an ILP-based search algorithm to find optimal subnets, it is better to compare with these ILP-based mixed-precision quantization papers, e.g., [2] and [3]. 



[1] A Deeper Look at Zero-Cost Proxies for Lightweight NAS 
[2] Mixed-precision neural network quantization via learned layer-wise importance, ECCV 2022. 
[3] Hawq-v2: Hessian aware trace-weighted quantization of neural networks, NIPS 2020.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a bit-switching quantization method using Double Rounding, which applies rounding twice to achieve nearly lossless switching without storing a full-precision model. They also introduce Adaptive Learning Rate Scaling (ALRS) to adjust learning rates dynamically across precisions, ensuring consistent quantization updates. Additionally, they develop Hessian-Aware Stochastic Bit-switching (HASB) for one-shot mixed-precision training, optimizing bit-width distribution based on layer sensitivity, thus eliminating retraining stages.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. ALRS heuristic can help practitioners who wish to train mutli-precision model

2. Authors made extensive experiments on vision models and compare to previse methos

3. Most sections are well written

4. Code is given

Weaknesses:
**Novelty** is limited and I am not highly motivated that the problem is important.

1.	The main contribution is to not same 32bit weight and different quantization parameters but only the high bidwith using a pretty straightforward idea of double rounding during training
2.	The ALRS is based on observation and heuristic to fix it. It is nice and helps for when trying to use 2 bits as well. Yet, I am not sure it is important for methods that don’t use the double rounding.

**Motivation**

3.	Since we usually don’t switch models based on data I am not sure why this is important. Do we really have edge device that switch on a daily base model precision and thus need to store in small local memory the 32bit model? Can you elaborate why and where multi precision is really important.

4. No results on more recent models (LLMs)

Limitations:
The authors partially discuss limitation

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
k0qTnbQxzR;"REVIEW 
Summary:
The paper presents CogCoM, a novel approach to training large Vision-Language Models (VLMs) using a mechanism called Chain of Manipulations (CoM). This mechanism enables the model to solve visual problems step-by-step with evidence, inspired by human cognitive processes like marking and zooming into images. CogCoM integrates manipulations such as grounding, zooming, and OCR into the VLM architecture, allowing it to handle various visual problems without external tools. The model is trained using a robust data generation pipeline and evaluated across multiple benchmarks, demonstrating state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Advantages of the Paper

1. **Explainable Reasoning and Manipulation Mechanism**: CogCoM generates intermediate steps with evidence, making the reasoning process transparent and explainable, which is crucial for complex visual tasks. The model incorporates a flexible set of manipulations that can be adapted to various visual problems, improving its versatility and problem-solving capabilities.

2. **Data Generation Pipeline**: The paper introduces an efficient pipeline for generating high-quality training data, which is essential for training VLMs to perform detailed visual reasoning.

3. **Superior Performance**: CogCoM achieves superior results across multiple benchmarks, including detailed visual question answering and visual grounding, showcasing its effectiveness and robustness.

These advantages highlight the paper's contributions to advancing the capabilities of VLMs in solving detailed and complex visual problems through a novel, human-inspired approach.

Weaknesses:
Weakenss in Points

This paper is generally good but I can still spot the following issues.

1. **Design of Figures and Tables**: The figures in the paper are not well-designed. The first and second figures are repetitive in meaning, and the colors in the first figure are too light (consider adding black outlines to the boxes). The font size in the second figure is too small to be legible on smaller screens. Additionally, the captions for Table 2 and Table 3 are too close to the tables, violating the submission guidelines.

2. **Lack of Discussion on Related Work**: The paper lacks a discussion of existing related work. It should consider citing and comparing with at least other agentic LMMs such as LLAVA-Plus[1] to provide a comprehensive comparison and context.

[1] https://arxiv.org/abs/2311.05437

Limitations:
See above Weakness part.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the Chain of Manipulations (CoM) mechanism for data generation to enhance visual reasoning in VLMs. The authors developed a data generation pipeline, producing 70K high-quality samples, and created the CogCoM model. CogCoM achieves state-of-the-art results across nine benchmarks, demonstrating significant improvements in various visual tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The CoM introduces a new data generation mechanism that enables VLMs to perform step-by-step visual problem solving with supporting evidence.
2.A data generation pipeline is proposed, producing a dataset of 70K high-quality samples.
3.The trained model, CogCoM, achieved SOTA in nine benchmarks.
4.This paper is well-written and easy to understand.

Weaknesses:
1.During data generation, the process relies entirely on GPT-4 for prompting and existing models (GroundingDino, PaddleOCR) for generation. As mentioned in the appendix, inaccuracies in these current visual models can affect the quality of generated data and the model's reasoning capabilities. However, the system lacks validation or filtering mechanisms to enhance data quality.
2.To highlight the specific improvements brought by CoM, it would be helpful to provide results both with and without the incorporation of CoM data. This would clarify the impact of CoM, especially since CogCoM integrates a significant amount of additional data such as MultiInstruct and LLaVAR during the instruction tuning stage as shown in Table 1.
3.The CoM dataset includes 6K high-quality manually annotated math samples, but no test results for math problems are provided. Clarification is needed on whether the purpose of this math data is solely to enhance the model's reasoning capabilities.
4.The paper emphasizes that CogCoM is a model capable of multi-image multi-turn understanding, but no corresponding test results (qualitative or quantitative) are provided.
5.In the model section, some parameters are not specifically explained, such as the maximum turns the model can accept and the predefined threshold.
6.Typos error: Line 288 CogOM->CogCoM

Limitations:
see weakness

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Drawing inspiration from human cognition to solve visual problems through localizing, zooming, etc., this paper introduces a new framework called CogCom, which solves visual problems by automatically combining six types of basic manipulations. When facing a visual problem, CogCom can use reasoning to solve each step and employ basic tools to aid in the problem-solving process. To achieve this goal, CogCom constructed a data generation pipeline that leverages GPT4 to build the training data for   CogCom. The CogCom leads to performance gains compared to its baseline CogVLM on several benchmarks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The CogVLM makes gains based on CogVLM on several benchmarks. 
2. The pipeline that leverages GPT4 to construct manipulation pipelines for problem-solving is reasonable.

Weaknesses:
1. The VQA benchmarks reported in Table 1 are not very convincing. It would be beneficial to consider more modern and challenging benchmarks such as MMBench, MathVista, and SeedBench.
2. The comparison of baseline methods seems to be based on relatively outdated approaches. It might be more informative to compare them with more recent LVLMs like LLaVA-1.5, Monkey, and ShareGPT4V.
3. It would be helpful to discuss a closely related work ViperGPT [3] and V* [4]. ViperGPT shares an idea for solving visual problems via planning tool pathways. V* shares the idea of searching and zoom-in progressively.
4. The differences with some other related works should be discussed [5][6]. 

[1] Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models

[2] ShareGPT4V: Improving Large Multi-Modal Models with Better Captions

[3] ViperGPT: Visual Inference via Python Execution for Reasoning

[4] V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs

[5] CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding

[6] DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models

Limitations:
The limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
jwE1dgOox1;"REVIEW 
Summary:
Large scale topological descriptors of data are leveraged to compute point/node-level descriptors, which encode to which large scale topological feature each point belongs to. For this, a combination of applied algebraic topology and applied harmonic analysis is used. More specifically, large scale homological features are computed using persistent homology, then represented with harmonic cocyles, and then averaged locally to obtain a point-level descriptors. The problem of topological clustering (already introduced in the literature) is addressed, whose objective is to determine to which large scale topological feature a certain data point belongs to. A set of benchmark datasets are introduced for topological clustering. The pipeline is applied to these datasets as well as real world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Large scale topology of data is leveraged to assign point/node-level features to data. This gives concrete meaning to what it means for a data point to belong to a certain large scale topological feature.
- The method is based on well-established mathematical concepts.
- The concept of topological clustering is interesting and has potential.
- A suite of synthetic datasets is introduced.

Weaknesses:
Regarding unjustified claims:

- Existing approaches are undersold. Specifically, in the introduction it is said that ""none of these approaches is able to represent higher-order topological information"" and that ""such higher-order topological information is however invisible to standard tools of data analysis like PCA of k-means clustering"". However, cluster structure is topological structure. Does ""higher order"" mean homology in dimensions 1 and above?
- Remark 4.2 says that ""datasets with topological structure consist in a majority of cases of points sampled with noise from deformed n-spheres"". This seems like a really strong claim. Is there any evidence of this?

Regarding theory:

- Theorem 4.1 applies in a very restricted scenario. Moreover, I do not understand why the harmonic representative takes values in {0, -1, 1}. This seems very surprising since harmonic cycles/cocycles almost always take fractional values (in order to minimize energy). I did not understand the proof of this fact; specifically, why $g$ being a harmonic generator for the entire filtration range of $(b,1)$ implies this claim.

Regarding the methodology:

- The method, specifically line 225, seems to assume that a cycle with coefficients in $Z/3Z$ will also be a cycle when interpreting those coefficients (0,-1, or 1) as real numbers. However, this need not be the case. To see this it suffices to consider a simplicial complex given by a triangle with no interior. Thus, step 3 of Algorithm 1 (and the method more generally) seems to be heuristic.
- The setup up Table 1 is unclear to me. How can one compare TOPF, which produces feature vectors, with, say, DBSCAN, which produces a clustering?
- Figure 4 is hard to interpret. For example, how should one assess the effectiveness of the algorithm in Fig 4(a)?
- The methodology has many hyperparameters. Some choices, like delta=0.07 in line 241, seem arbitrary.

Limitations:
- The experimental evaluation is limited.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces TOPF, a topological feature extraction mechanism on point cloud data. The authors consider Vietoris-Rips/$\alpha$ filtrations over point clouds and compute the persistent homology. They propose a heuristic to select the “top” features from the barcodes. They consider the corresponding representatives for these features and project them onto the harmonic space of the simplices. These projected vectors are then normalized and used to construct a point-level feature vector. The authors use this framework for clustering. Towards this end, the authors introduce a topological point cloud clustering benchmark and report the experimental results on this benchmark.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors propose to use Hodge Laplacian and Hodge decomposition to compute feature vectors over points in point-cloud data, which is a novel idea.

Weaknesses:
1. I do not fully understand the “learning” the representation here, because the representation is not particularly being learnt. It is being computed by using the persistent homology of the point cloud. 

2. Experimental evaluation is limited to clustering. And even in clustering, it is primarily limited to shapes which are partially/fully topologically spherical.  

3. The robustness of the approach is due to the robustness of harmonic persistent homology known in the literature. 

4. The paper uses well-known notions in the TDA literature in the context of point-clouds, which amounts to an incremental progress in this direction. 

5. It would strengthen the paper if the authors include a small paragraph explaining why projecting onto the harmonic subspace solves the problems that exist in using the homology representatives directly. 

Minor: 

Page 2, Line 81: ‘Spaces in topology are “continuous”’. Continuity is a notion defined for functions on topological spaces and not for topological spaces themselves. Spaces are connected.

Limitations:
Yes, the authors have discussed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an approach to select and compute some point-level topological features for point cloud or general data set analysis.
The main ideas is to define a multi-scale simplicial complex representation, thus we can track how the homology modules change along the filtration and then select the homologies that persist for a long range of scales.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Topological features are usually not localized, the idea of being able to bring back the topological descriptor to the relevant points is quite novel and impactful.
- The approach is theoretically sound and well analyzed.
-The experimental evaluation is limited but convincing.

Weaknesses:
- the feature selection is very heuristic.
- The evaluation is only on point cloud clustering. Since we are evaluating effectiveness and robustness of localized features, feature/point correspondence problems would have been interesting.

Limitations:
The main limitation, i.e. the selection of the features, has been briefly addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel method for extracting per point topological features - TOPF. The method builds on previous results in topological data analysis which described a shape or a point cloud with a single global feature, by generating per-point topologically-aware features. The paper presents a quantitative evaluation and comparison of the proposed method with prior art on a new benchmark consisting of several synthetic examples, evaluates the robustness of the proposed method under noise, as well as presents qualitative examples of its performance on synthetic and real work data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
* The paper is well written and easy to follow. Prior art and the proposed algorithm description is detailed and comprehensive.
* To my understanding, the paper describes a novel method for per-point feature extraction based on topological information contained in a point cloud, and describes theoretical guarantees for its correctness on point clouds sampled from multiple n-spheres.
* The paper describes a new topological point clustering benchmark dataset consisting of seven synthetic point clouds with up to 5 labels, and evaluate the proposed and existing methods on this dataset showing that the proposed method outperforms existing methods in most cases.

Weaknesses:
* The paper lists common machine learning applications requiring point level features as a motivation for the proposed method. However, only quantitative experiments for point cloud clustering on a set of synthetic examples, and anecdotal evidence of performance on real world data, were presented. In order to fully understand the potential of the proposed approach to be applied beyond synthetic data, it would be beneficial to include additional evaluation, qualitative and quantitative, on real-world data and additional applications, e.g. as described in lines 304-307.
* Specifically, it would be interesting to see experiments on non-synthetic datasets with topological structure mentioned in line 266.
* Additionally, comparison with other well performing modern machine learning methods, such as graph neural networks for point cloud clustering, needs to be discussed, for completeness.

Limitations:
The authors adequately addressed the limitations and impact of the proposed approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
jKzLukkKZO;"REVIEW 
Summary:
The paper ""Learning to Control the Smoothness of GCN Features"" investigates the impact of activation functions, specifically ReLU and leaky ReLU, on the smoothness of node features in Graph Convolutional Networks (GCNs). It provides a geometric characterization of these effects, showing how altering the input's projection onto eigenspace M can control the smoothness of output features. The study introduces a Smoothness Control Term (SCT) to modulate the smoothness of node features, aiming to improve node classification tasks in both homophilic and heterophilic graphs. Experimental results validate the efficacy of SCT, demonstrating significant improvements in node classification accuracy for several GCN-style models .

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
## Originality
- The paper introduces a novel approach to control the smoothness of Graph Convolutional Networks (GCNs) features, which is a significant departure from traditional methods. It builds upon and extends the work of Oono & Suzuki and Cai & Wang by integrating geometric insights with the message-passing process in GCNs.

## Quality
- The paper provides a robust theoretical framework, including geometric characterizations and proofs, that underpin the proposed methods.
- Extensive experiments validate the theoretical claims, showing significant improvements in node classification accuracy. Detailed descriptions of the experimental setup, including datasets and hyperparameter tuning, enhance the reproducibility of the results.

## Clarity
- The paper is well-structured, with clear sections that logically flow from introduction to theoretical analysis, experimental validation, and conclusions.
- The paper provides a comprehensive review of related work, situating its contributions within the broader context of graph neural networks research.

## Significance
- The ability to control the smoothness of GCN features addresses a to me interesting and important challenge in graph neural networks, with potential applications in various domains such as social network analysis, biological networks, and recommendation systems.
- The proposed SCT shows improvements in real-world datasets.
- The insights gained from this work could inform future research on activation functions and feature smoothness in other types of neural networks.

Weaknesses:
## Weaknesses
- The removal of white space in the paper makes it hard to read. The authors should really try and make the paper easier to read visually by not condensing as much math in the main text as possible. Not only is this arguably in violation of the guidelines, but it also illustrates that the authors need to distinguish clearer what the main contributions are and which parts in the main text can go to an appendix.
- While the geometric insights provided are valuable, the complexity of the mathematical formulations is challenging for readers not well-versed in advanced geometry and spectral graph theory. Simplifying explanations or providing more intuitive examples could enhance accessibility. Moreover, I feel like the math could be made more intuitive by giving verbal explanations before the theorems. The cramming of the paper and not highlighting enough what the main contributions should be improved.
- Although the paper compares SCT with a few baseline models, it would benefit from a broader comparison with additional state-of-the-art methods in GCNs and GNNs to provide a more comprehensive evaluation of its effectiveness.
- The experiments are primarily conducted on benchmark datasets. Incorporating more real-world applications and diverse datasets would demonstrate the practical relevance and versatility of the proposed method.
- The drop in accuracy for deeper models (16 or 32 layers) is noted but not deeply analyzed. A more thorough investigation into the causes of this performance degradation, beyond mentioning vanishing gradients, could offer insights into potential improvements. What happens if you use techniques that combat eg oversmoothing?
- While the paper mentions computational efficiency, a more detailed discussion on the computational overhead introduced by SCT, including potential trade-offs between accuracy and efficiency, would be beneficial.

Limitations:
## Limitations

The authors have acknowledged several limitations of their work, including the over-smoothing issue in deep GCNs and the dependence on specific activation functions, but they could improve by providing more detailed discussions on computational efficiency and potential negative societal impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the challenge of balancing smooth and non-smooth features in graph convolutional networks (GCNs) for node classification. Building on previous work that highlighted the correlation between feature smoothness and classification accuracy, the authors propose a novel method to control the smoothness of node features through a geometric approach and an augmented message-passing process. Their strategy involves establishing a geometric relationship between input and output vectors of activation functions like ReLU and Leaky-ReLU, and integrating a learnable term in the graph convolutional layers to modulate feature smoothness. The paper provides an empirical study to showcase the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper offers a novel geometric insight into the effects of different activation functions, specifically ReLU and Leaky-ReLU, on smoothing in GCNs. Furthermore, it introduces an innovative method to enhance the message-passing framework for GCNs (and similar networks) to better control smoothing.

Weaknesses:
* The empirical results section is very weak and a major revision is needed in order to be able to judge the merit of the proposed method. The most significant weaknesses are:
    * The paper does not compare to established baselines from other competing methods. For instance, one could take a look at the tables in https://arxiv.org/abs/2202.04579 or https://arxiv.org/abs/2210.00513 and add the results to the comparison in this paper (i.e., for cornell, texas, wisconsin, squirrel, and so on). 
    * The paper reports different (mostly weaker) performance for the baseline models such as GCN and GCNII. Again, this should be fixed and established results can be taken from the tables in the two papers mentioned above.
    * The improvement using the proposed SCT is very marginal in many experiments, and in particular the performance still drops (or improves very little) for higher number of layers. It is somewhat expected that increasing the number of layers and solving oversmoothing does not help much in homophilic tasks, i.e., the tasks considered in Table 1. However, if the method works it should lead to significant improvements on heterophilic tasks, even for increasing number of layers. Therefore, please show the results for the datasets in Table 2 in the same way as you show them in Table 1, i.e., for increasing number of layers.
    * It is claimed in Table 1 that the drop in performance of GCN for increasing number of layers is due to vanishing gradients and not due to oversmoothing. This claim has to be justified empirically. 
    * Figure 4 in the appendix showing gradient norms is not meaningful. The y-axis should be displayed on a logarithmic scale. Vanishing gradients occur for gradients approaching zero (exponentially fast). It is not clear if this is the case here, since it is plotted in linear scale.
    
* The structure and readability of this paper should be improved. For instance, it would be helpful for readers who are not very familiar with the graph-learning field, to introduce the concept of GNNs and GCNs in the introduction. It would be advisable to move the technical aspects from section 1 to section 2. In fact, none of the definitions presented at the beginning of the introduction are needed anywhere else in the remainder of the introduction section. Thus, this could be moved to section 2.

Limitations:
* The method necessitates a pre-processing step to compute the eigenbasis in equation (5). This may not scale efficiently for very large graphs.
* The method does not show significant improvements. Notably, there are more effective models and methods available for mitigating oversmoothing, which have not been cited or empirically compared. Examples include https://arxiv.org/abs/2202.04579, https://arxiv.org/abs/2210.00513, https://arxiv.org/abs/2206.05437, https://arxiv.org/abs/2110.14446, https://arxiv.org/abs/2206.10991,
https://arxiv.org/abs/2006.11468.
* While the theoretical justification of the approach is intriguing, the insights are somewhat limited. For example, there are no provided estimates for choosing the parameter alpha, which instead has to be learned through gradient descent.
* The empirical results are not convincing.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies how GCN smoothes node features in terms of unnormalized and normalized smoothness. The results show that adjusting projection can alter the normalized smoothness to any desired level. Based on this, the paper proposes a new method SCT to let GCN learn node features with a desired smoothness to enhance node classification and verifies it effectiveness in practice.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	Understanding the effect of nonlinearities in GNNs is an important yet underexplored problem due to the complexity of nonlinearities. The paper offers a new perspective on it.

2.	Oversmoothing is a known issue, while it is also known that some amount of smoothness is desired for graph learning. How to find the ideal amount of smoothness among node features is an important but nontrivial problem.

3.	The proposed method SCT is principled and seems effective.

Weaknesses:
1.	While I can see that normalized smoothness has its own merit, this notion could be better motivated and connected to the literature. 

   a.	Given analysis in [27, 4] is asymptotic, I would not say that “over-smoothing – characterized by the distance of features to eigenspace M or the Dirichlet energy – is a misnomer”, as it is too strong of a claim to make. Those results essentially say that “very deep GCNs are bad due to oversmoothing” which has their limitations but can be well justified by the distance of features to eigenspace M or the Dirichlet energy.

   b.	My understanding is that normalized smoothness could be more connected to the non-asymptotic notion of oversmoothing studied in [32], which is defined based on the Bayes error of classification (the distance to the decision boundary) and hence taken the magnitude of features into account.

   c.	Based on the above, the argument presented in the paper can be strengthened in the following way: the motivation of normalized smoothness should be based on a discussion on how the magnitude (and hence normalized smoothness) is more related to a non-asymptotic notion of oversmoothing, which is directly related to the classification performance of finite-depth/shallow GNNs. Based on this, the results present in this paper is more practically relevant than the previous asymptotic result.

I would suggest the authors modify the relevant text in the introduction and analysis accordingly.


2.	The analysis only applies for GCNs, while whether the analysis or the proposed method can be extended to more complexed GNNs such as GATs or graph transformers is unclear. 

3.	The analysis only applies for ReLU and LeakyReLU, which reads a bit specific. I wonder if the results can be generalized to a general family of nonlinearities.

4.  For the experiments, there is a lack of baseline comparisons except the basic backbone architecture. For example, I wonder how it would compare to APPNP, which is proposed to balance the need to explore larger neighborhood and locality and the implicit goal is also to produce node features with the ""right"" amount of smoothness.

4.	Another presentation suggestion I have for the authors is that one should minimize the use of in-text math and bold or italic fonts for highlighting (such as line 167-173).  Math is hard to read in-text and when too many texts are highlighted, the paper becomes ever harder to read because everything seems to be emphasized and it kind of messes up with its original purpose.

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper first shows that in GCN, the output of ReLU or LeakyReLU lies on a sphere whose input is characterized by components parallel and perpendicular to $\mathcal{M}$, the space spanned by eigenvectors for the maximum eigenvalue of a graph. As a corollary, this paper shows that these activation functions do not increase the component of the feature vector perpendicular to $\mathcal{M}$. Furthermore, this paper defines the normalized smoothness and evaluates how its range varies with the activation functions. Based on this discussion, this paper proposes an SCT that learns the parallel components of the feature vectors. The proposed method is applied to GCN, GCNII, and EGNN, and verifies its effectiveness by applying it to node prediction tasks with various heterophily.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The theorems presented in the theoretical analysis (Propositions 3.2 and 3.3) enable a unified treatment of ReLU and Leaky ReLU.
- Numerical experiments show that the proposed method is effective for various data sets and models, showing the versatility of the proposed method.

Weaknesses:
- I need help understanding the explanation in Section 3.3. More specifically, it is difficult to understand that the *independence* of the inequality means that the upper bound of the inequality does not depend on the value of $\boldsymbol{Z}_{\mathcal{M}}$. I suggest writing it explicitly. 
- P7, L.265: It seems strange that although SCT changes its architecture depending on whether the underlying GNN is GCT or GCNII, it has the same name. I suggest naming SCT for GCT and SCT for GCNII differently.

Limitations:
The authors discuss in the conclusion section that the proposed method's limitation is that it assumes the model's oversmoothing. However, I do not think this is a limitation because if the model does not cause oversmoothing, there is no need to use the proposed method. Rather, I recommend evaluating whether SCT has bad effects when the model does not cause oversmoothing.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper deals with (Over-)smoothing is Graph Neural Networks. While it was previously known that GCN-type GNNs oversmooth, this paper reexamines the case for GCN-type architectures with Relu-type activation functions in terms of *normalized* smoothness. The authors show that the convergence behaviour of GCNs can be split into two parts, a ""smooth"" part and a ""non-smooth"" part and that by manipulating the smooth part, one can influence the normalized smoothness of the signal. From these theoretical insights, the authors propose a new system that uses a learnable parameter that modulates the smooth part, making the model able to learn the most beneficial normalized smoothness for the problem at hand.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper furthers the understanding of oversmoothing in GNNs
- The proposed approach is well-founded in theory

Weaknesses:
- The experiments are unconvincing. 
    - There is a slight improvement to be found in models with SCT, but this is not too surprising, as these models also have more parameters, and mostly improvements are quite slim.
    - You choose GCN, GCNII and EGNN, two of which have built-in skip connections, that are known to help with oversmoothing. This also coincides with the models that cope quite well with a larger number of layers. The vanilla GCN takes a huge hit in all benchmarks apart from ogbn-arxiv. So learning the normalized smoothness of features does not actually seem to help in the case of GCN. 
    - The other two models don't suffer from oversmoothing to begin with

Limitations:
The limitations are not well-discussed. 
The only limitation the authors claim for this work is, that: ""without this condition [that oversmoothing happens], SCT cannot ensure performance guarantees."" There are no performance guarantees given for SCT. This is the only limitation discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies how ReLU and Leaky ReLU affect the smoothness of node features in graph convolution layers. The authors demonstrate that adjusting the input projection onto eigenspace $\mathcal{M}$ of the node feature matrix can achieve any desired normalized smoothness. Additionally, they propose a Smoothness Control Term (SCT) to enhance node classification in Graph Convolutional Networks, validated on both homophilic and heterophilic graphs.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. From a geometric perspective, the authors prove that how ReLU and Leaky ReLU affect the smoothness of node features in graph convolution layers.
2. The experimental results validate the theory proposed by the authors.

Weaknesses:
Equation 5 implies that using SCT requires performing eigendecomposition. This paper avoids the high time complexity associated with eigendecomposition, especially when the number of nodes in a graph is very large.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
iRHxp1ibFj;"REVIEW 
Summary:
The paper introduces a novel image-level supervision method for semantic segmentation, utilizing approximate targets for the relative sizes of segments in training images. These targets, represented as categorical distributions for the expected average prediction over pixels, are integrated using a zero-avoiding variant of KL divergence as the training loss. This approach achieves quality comparable to full pixel-level supervision but is significantly less costly, requiring only rough estimates of the areas occupied by each class.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Using object size as a form of supervision is both innovative and interesting.
2. The proposed method is straightforward and easy to understand.

Weaknesses:
1. The title of the paper is misleading. It claims that approximate size targets are sufficient, but the work also uses image labels for supervision.
2. The most important comparison in Figure 1 is between 'Tag' and 'Size target,' as this validates the significance of using target size supervision. To clearly demonstrate that 'Size target' is superior to 'Tag' under identical conditions, it would be better to use the same architecture for both comparisons.
3. Labeling the size of objects can be challenging for humans and may introduce significant noise, especially for tiny objects. Although the authors demonstrate impressive accuracy with up to 8% size target errors, this remains a stringent annotation standard, particularly for small objects. For instance, as seen in Table 1, the mean relative error (mRE) often exceeds 10% during human annotation in the Pascal VOC dataset. Moreover, estimating target sizes in Pascal VOC is relatively easy since objects are typically large and centered. However, labeling images in more complex datasets, such as COCO, might result in a higher mRE.
4. In Table 1, the authors should also report the speed of tag annotation to highlight the cost of estimating target sizes.
5. The proposed method is straightforward and impressive for its end-to-end training, especially considering that existing weakly supervised semantic segmentation (WSSS) methods typically use CAM and two-step training. However, as shown in Table 2, while the proposed method achieves comparable accuracy to state-of-the-art WSSS methods, it relies on additional supervision and a high annotation standard (8% mRE). Moreover, Table 2 indicates that the accuracy with only tag supervision is close to that of fully supervised methods, suggesting that tag supervision alone may be sufficient for segmentation.

Limitations:
The authors do not discuss the limitations and broader impact of their method, which necessitates a dedicated discussion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new weakly supervised semantic segmentation task. This task uses pixel-level categorical distribution as the label in the training stage. KL divergence is used as the training loss. Experiments on three public segmentation datasets show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The proposed task is interesting. It provides the community another choice for segmentation with less annotation effort.

2.The proposed KL divergence loss is effective, demonstrated by experiments on three public datasets. It achieves performance comparable to methods using more expensive labels, like the box supervised one.

3.The proposed method is robust to size target error, which makes it more practical.

4.The writing is fluent and easy to follow.

Weaknesses:
1.Labeling effort on complex images. Images from PASCAL VOC (like Figure 1) are easy to annotate. It contains few classes and the background is generally clean. The density of target objects is low, and hence it’s also suitable for the proposed grid-based size target annotation way.

However, in practice, scenes are much more complex, with more classes, more crowded objects, and complex backgrounds. The authors are recommended to show the annotation effort on those images, like images from Cityscapes and ADE20K. I think when the scenes become more complex, the labeling effort will increase significantly. The labeling effort of size target will be much more than the tag way, since tagging will be less influenced in such cases.

2.Model performance on complex images. Similarly, it’s recommended to evaluate the model’s performance with the proposed loss on these complex datasets. This will give a more comprehensive understanding of the proposed method.

Limitations:
No negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper titled ""Approximate Size Targets Are Sufficient for Accurate Semantic Segmentation"" proposes a novel method of semantic segmentation that leverages approximate size targets instead of full pixel-level supervision. The method involves using categorical distributions to represent the expected average prediction over image pixels, utilizing the zero-avoiding variant of KL divergence as a training loss. The approach aims to reduce annotation costs while maintaining segmentation accuracy comparable to full supervision.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Originality: The use of approximate size targets as a form of weak supervision for semantic segmentation is novel and creative.
2. Quality: The experimental results are comprehensive and demonstrate the effectiveness of the proposed method across different datasets and segmentation architectures.
3. Significance: The approach has significant implications for reducing annotation costs in semantic segmentation tasks, making it highly relevant to practical applications.

Weaknesses:
1. Simplicity of Method：While the proposed method is innovative, it seems relatively simple. There might be opportunities to enhance its contributions with further development or by integrating additional techniques.
2. Limited Scope of Evaluation: While the paper evaluates the method on several datasets, it would benefit from a broader range of scenarios, including more diverse and complex images.

Limitations:
The authors have addressed the limitations related to annotation errors and have demonstrated the robustness of their method to these errors. However, it would be beneficial to discuss potential limitations in more detail, such as the scalability of the method to larger and more diverse datasets, and any assumptions made about the nature of the size target annotations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel image-level supervision method for semantic segmentation using approximate segment size targets. It utilizes categorical distributions for expected average predictions, reducing annotation cost and complexity. The authors propose a zero-avoiding KL divergence as a training loss, compatible with any segmentation architecture, and demonstrate significant robustness to size target errors, improving generalization. The method achieves state-of-the-art performance on multiple datasets with standard segmentation models like ResNet101. Additionally, it requires minimal extra information and no architectural changes, making it a practical and effective solution for weakly-supervised semantic segmentation in real-world applications.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper introduces a novel form of image-level supervision for semantic segmentation using approximate segment size targets. This approach is original in its use of categorical distributions for expected average predictions, providing a fresh perspective on weakly-supervised segmentation methods.

2. The quality of the research is high, with comprehensive experiments conducted on multiple datasets. The use of a zero-avoiding variant of KL divergence as a training loss is well-justified and demonstrates robustness to size target errors. The empirical results show that the method achieves state-of-the-art performance using standard segmentation models.

Weaknesses:
1. The paper claims robustness to size target errors but provides limited detailed analysis on this aspect. Including more experiments to quantify and analyze how different levels of size target errors impact performance would provide a clearer understanding of the method's robustness.

2. Lack of related work. The paper’s logical flow and organization need improvement. 

3. The paper lacks comprehensive comparisons with the latest models, such as ""SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation (AAAI24)"".

Limitations:
1. Fig and Figure are inconsistent in Line 24

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
hiwHaqFXGi;"REVIEW 
Summary:
The paper introduces DiGGR (Disentangled Generative Graph Representation Learning), a self-supervised learning framework that aims to guide graph mask modeling through disentangled latent factors to enhance the disentanglement of learned representations. Extensive experiments across 11 public datasets for node and graph classification tasks demonstrate the framework's effectiveness, significantly outperforming many existing self-supervised methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Innovative Approach: The DiGGR framework innovatively utilizes disentangled latent factors to guide graph mask modeling, a novel contribution in generative graph representation learning that significantly enhances the model's explainability and robustness.
Comprehensive Experiments: The paper conducts extensive experiments on multiple datasets and tasks, showing significant performance improvements over existing methods, thus providing strong empirical support for the proposed approach.

Weaknesses:
Complexity and Scalability: The framework appears computationally complex, which might limit its scalability to very large graphs or real-time applications. Unfortunately, this aspect is not extensively discussed in the paper.
Lack of Theoretical Analysis: While the empirical results are strong, the paper lacks a detailed theoretical analysis of why the disentanglement process improves performance, which could provide deeper insights into the method’s efficacy and limitations.

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a framework called DiGGR, aimed at improving the robustness and explainability of generative graph models by addressing the issue of entangled graph representations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper tells the story in an easy-to-read way, and the whole paper is quite easy to follow.
2. The problem of disentangled learning is a very popular yet important task.
3. The paper conducts comprehensive experiments to evaluate their method.

Weaknesses:
1. Lack of novelty. Graph disentangled learning is not a new task. There are tons of existing methods for disentangled representation learning, such as those maximizing KL divergence or minimizing mutual information between two sets of representations. A lot of related works such as [1], [2], [3] and [4] are not discussed. Also node factorization is not a new idea, such as node clustering in [3].

[1] Disentangled graph collaborative filtering. SIGIR 2020.
[2] Disentangled Graph Convolutional Networks. ICML 2019.
[3] Deep Generative Model for Periodic Graphs. NeurIPS 2022.
[4] Disentangled contrastive learning on graphs. NeurIPS 2021.

2. The motivation of the proposed method is not clear to me. For example, why should we use mask? Also, why the proposed method sticks to GAE, not VGAE or other types of GNN, such as GCN, GIN or GAT?

Limitations:
Yes. Limitations have been discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work proposes a disentangled generative self-supervised learning method for graphs. The authors introduce a latent factor learning module to capture the heterogeneous factors in the nodes. The proposed method factorizes the graph into factor-specific subgraphs, and jointly trains a disentangled Graph MAE applying distinct masks for each subgraph. Experimental results demonstrate that DiGGR outperforms traditional methods that treat the graph holistically, without accounting for its latent structure.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method first explores a factorization method for generative graph SSL. 
2. The authors provide extensive experimental results and analysis on both node and graph-level tasks to show the improved effectiveness, interpretability, and generalization by using the proposed method.

Weaknesses:
- The computation complexity of the proposed method is quite high. Could the author pride training time comparison to the baseline methods to help us get a sense of the real complexity?
- Could the author provide more insights on how to find an optimal factor number K according to the statistics of diverse datasets? This might be useful for real-world applications.

Limitations:
Yes, the authors discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a self-supervised learning framework DiGGR, aimed at enhancing the disentanglement of learned graph representations. The authors argue that existing generative graph models tend to overlook the entanglement of learned representations, leading to non-robust and non-explainable models. DiGGR addresses this by introducing a latent factor learning module and a disentangled graph masked autoencoder, allowing for factor-wise graph representations. The framework is tested on various benchmarks, demonstrating its effectiveness in outperforming previous self-supervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies an interesting research problem that is disentangled graph representation learning. This research problem is very hot recently.

2. The model design is easy to understand. The paper provides a detailed explanation of the proposed model.

3. The experiments demonstrate the effectiveness of the model. The performance improvement on some comparisons seems to be significant.

Weaknesses:
1. One of my concerns is from the novelty. I think the model design is a little similar to the works [1-2]. The authors should make more comprehensive discussions to show the differences between them.

2. The experiments ignore some recent or related contrastive baselines [1-4] for comparisons.  The improvements on some datasets seem to be not significant. 

3.  More large-scale benchmarks should also be considered, e.g., OGB. The experimental settings are not very clear for reproducing the results.

[1] Disentangled contrastive learning on graphs. NeurIPS 2021.

[2] Disentangled Graph Contrastive Learning With Independence Promotion. TKDE 2022.

[3] Augmentation-Free Graph Contrastive Learning of Invariant-Discriminative Representations. TNNLS 2023.

[4] MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning. AAAI 2023.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
hc9GkYw9kR;"REVIEW 
Summary:
This paper proposes a novel surrogate-assisted evolutionary algorithm named LORA-MOO, the core contribution is the introduce of ordinal-regression-based model  spherical coordinates approximation to SAEA and LORA-MOO can find a good trade-off between optimization efficiency and optimization results.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
This paper provides a novel perspective for modeling surrogates with high efficiency.

The experiments results seem good.

Weaknesses:
Motivation and contributions are limited. To the best of my knowledge, the main contrition of LORA-MOO is introducing ordinal-regression-based model for convergence and spherical coordinates for diversity. However, why do it and what is the connections between them? Besides, the manuscript includes a lot of informal expression. The proposed method is complex and effectiveness is limited.

Limitations:
See questions.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed the LORA-MOO framework, a surrogate-assisted MOO algorithm that learns surrogates from spherical coordinates. This includes an ordinal-regression-based surrogate for 10 convergence and M −1 regression-based surrogates for diversity.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The considered problem is pretty important.

Weaknesses:
Major:
1. Line 113-116, a bit too repetitive. 

2. Line 121, an initial dataset size of 11D-1, too specific.

3. Lines 121-134: the algorithms are described too specifically. It is more suitable for EC journals rather than NeurIPS.

4. Selection Criteria, too simple. 

5. What does LORA-MOO means?

6. Some HV-based MOBO methods are not compared as they are failed to solve many objectives. This argument is not accurate, please consider to run ""https://github.com/xzhang2523/libmoon/blob/main/libmoon/solver/mobo/run_dirhvego.py"", which supports more than ten objectives problems. 


Minor:
There are too many grammar errors in this paper. 
(1) Line 249, they are failed -> they failed. . 

(2) Line 270, HV use -> HV uses . 

(3) Line 175, consider using \max.

(4) line 21, are widely exist -> widely exist. 

(5) line 64, an non-pa .. -> a non-pa..

Limitations:
See weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a surrogate-assisted evolutionary many-objective optimization algorithm, named LORA-MOO. LORA-MOO is composed of a surrogate for ordinal modeling, which focuses on convergence, and m-1 surrogates for distribution modeling, which focus on diversity. Empirical study demonstrates the effectiveness of the proposed algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well-written and easy to follow. Although the proposed algorithm seems somewhat complicated, all the technical details are clearly presented, and the motivations behind them are explained. The empirical study is generally solid, with all the major parameters included in the ablation study, and most of the commonly used test instances are covered. The results show that LORA-MOO outperforms the baselines.

Weaknesses:
Although LORA-MOO obtained better indicator values than the baselines in synthetic problem benchmarks, it is difficult for me to find some fundamental differences between LORA-MOO and previous MOAs. Nor could I see what new insights this paper can provide for solving expensive MOPs. LORA-MOO models the convergence of solutions by a surrogate problem of the domination level, which is a common idea in MOO, adopted by many past methods such as NSGA-II. Such a surrogate is intuitive, but it is unclear to me why it could work better than the existing surrogates such as pairwise relation or function values, and why such a surrogate can be successfully modeled by a Gauss process. LORA-MOO uses m-1 surrogates to predict the spherical coordinate, but it seems identical to predicting function values. LORA-MOO also contains many other components, such as EA, PSO, non-dominated sorting, various clustering methods, and some subset selection mechanisms. These components have long been widely adopted by many MOAs, and there are many alternatives available as well. I agree that these components could usually make an algorithm perform better, but this paper does not adequately demonstrate any necessity for such a combination or any connections between these components. The ablation study appears to be a parameter-tuning experiment, presenting some results under different parameters. However, there is no ablation for the many components in the algorithm, so it is unclear what contributions these components actually make.

Expensive optimization problems are closely connected to real-world applications, and many real-world MOPs are indeed expensive problems. Therefore, I believe this is a very valuable research direction. However, the empirical study in this paper is mainly conducted on synthetic problems. DTLZ and WFG have undoubtedly driven the development of the MOO field, but they have also caused a significant number of researchers to focus narrowly on these synthetic test sets. As a result, there are now many algorithms that perform excellently on synthetic test sets but struggle to adapt to real-world problems. The authors have also conducted tests on NASBench, which is crucial for comprehensively demonstrating the algorithm's capabilities, but the results do not seem to be sufficiently convincing.

Limitations:
This paper does not summarize its limitations. I suggest the authors reconsider the limitations of this work and fully present them in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a surrogate assisted method for multi-objective optimization. The approach learns a surrogate function with the ordinal values as the regression labels. The ordinal values are generated using a iterative algorithm with the most dominated solutions having the highest ordinal values. The ordinal values are used to train a Kriging model used to select a point for observation using the convergence criterion. Another point is selected using the Kriging model trained on spherical coordinates via a diversity criterion. The approach has several parameters which are tuned via experimentation on real and benchmark datasets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The approach provides an innovative way for optimizing multi-objective functions by separating out the two objectives thus simplyfying the problem.
  1) The convergence objective designed to select the best solution wrt the ordinal values
  2) The diversity objective designed to improve the diversity of the Pareto optimal solutions
- It is experimentally shown that the method improves the IGD metric compared to several benchmark and real MOO problems.
- The ideas presented in the paper are well motivated and well presented.

Weaknesses:
- The approach presented in the paper is not sufficiently novel. Ordinal regression for multi-objective optimization has been studied before [1]. The differences with related prior work have not been discussed in detail.
- The proposed algorithm has many tunable parameters, and it is unclear how the parameters affect performance on real world problems when they have only been tuned on benchmark problems.
- The real world experiment on NAS shows improved regret eventually, but converges slower than other existing approaches. It is difficult to judge on the effectiveness of this approach based on a single experiment. Experiments on more real world optimization problems are necessary to make a conclusion.
- The paper is missing several notable MOO approaches from the Bayesian optimization community [2,3,4,5].

[1] Yu, Xunzhao, et al. ""Domination-based ordinal regression for expensive multi-objective optimization."" 2019 IEEE symposium series on computational intelligence (SSCI). IEEE, 2019.

[2] Tu, Ben, et al. ""Joint entropy search for multi-objective bayesian optimization."" Advances in Neural Information Processing Systems 35 (2022): 9922-9938.

[3] Zhang, Richard, and Daniel Golovin. ""Random hypervolume scalarizations for provable multi-objective black box optimization."" International conference on machine learning. PMLR, 2020.

[4] Paria, Biswajit, Kirthevasan Kandasamy, and Barnabás Póczos. ""A flexible framework for multi-objective bayesian optimization using random scalarizations."" Uncertainty in Artificial Intelligence. PMLR, 2020.

[5] Abdolshah, Majid, et al. ""Multi-objective Bayesian optimisation with preferences over objectives."" Advances in neural information processing systems 32 (2019).

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
hGhLd2ByoR;"REVIEW 
Summary:
This paper reveals novel pathologies in existing unsupervised methods aimed at discovering latent knowledge from large language model (LLM) activations. Instead of extracting knowledge, these methods tend to identify the most prominent features of the activations.

The paper theoretically demonstrates that arbitrary features (not just knowledge) can satisfy the consistency structure of a popular unsupervised knowledge-elicitation method, namely contrast-consistent search. Additionally, the authors conducted a series of experiments showing that current unsupervised methods for discovering latent knowledge are insufficient. While the paper proposes potential future solutions, it does not provide a definitive solution to the problem with existing unsupervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the paper is well-written, and its theoretical and analytical contributions may be useful. I am impressed about the extensive experiments.

Weaknesses:
More experiments on other LLMs are needed to further validate the claim.

It would be better to offer possible solutions to address the problems in existing unsupervised methods.

Limitations:
No solutions to address the problems in existing unsupervised methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a careful study on existing methods for discovering the latent knowledge from large language models (LLMs), especially Contrastive-Consistent Search (CCS). The authors prove that CCS might not actually discover the knowledge of LLMs, instead, it could fit any features that satisfy certain conditions. Through a series of experiments, the authors further demonstrate that CCS could be distracted by random words, irrelavant texts like the character's opinion, and remain sensitive to the choice of prompt. Finally, the authors propose some general principles for the future works about unsupervised LLM task discovery.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, the paper is well written and eazy to follow. The authors made interesting obervations about existing methods on knowledge discovery of LLMs. The theoretical analysis is well supported by the experiments. Sevaral guiding principles are also proposed for the future works. I think this paper would provide good information to the research community about unsupervised knowledge discovery of LLMs.

Weaknesses:
From my experience on unsupervised learning, I'd argue that the content of this paper *would not be sufficient to refute existing methods about unsupervised knowledge discovery (CCS)*. First of all, CCS is a method built on top of features from pretrained models. It'd definitely be sensitive to the features and thus also sensitive to the prompts, because features changes from different prompts (this could also be seen from the PCA visialization). Furthermore, as an unsupervised method, it'd be expected that the method might find multiple valid solutions, where only one of the solutions corresponds to the knowledge we are looking for. Taking the experiments from Section 4.2 as an example. The constructed dataset actually has two valid labels: the sentiment of the text and the sentiment of Alice. Depending on the optimization and the implicit bias of the algorithm, it could totally happen that an unsupervised method could found both valid labeling, or could only find one of them. I believe this is a common phenomenon shared by exsiting off-the-shelf unsupervised methods (like K-Means) cause they're searching for labels without supervision. From this perspective, I'd regard that this paper provides a method to construct ""adversarial datasets"" for CCS. However, it would not be a problem for CCS in practice.

Furthermore, the authors don’t provide solutions to this issue.

Also, I believe the mathematical notation in Section 3 could be simplified.

Minor issues: typo $c(x_i^+=1), c(x_i^+)=0$ in line 102

Limitations:
The authors have mentioned that this paper is focused on current methods and might not be directly applied to future works.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the failure modes of the method called ""constraint-consistent search (CCS)"" in knowledge discovery for language models. In particular, they showed: there is no unique identification on the minimizer of CCS, as there are a class of features achieves the optimal loss; demonstrated experimentally classic unsupervised methods detect features other than knowledge; discovered features are sensitive to prompt formats.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper points out a popular method's overlooked short-comings and presents both theoretical and experimental results to support that CCS may not be able to discover the true knowledge feature: 1. the observation on CCS loss is driven by xor operator rather than the feature is clever; 2. given the vast space of feasible features, CCS method is very sensitive to prompts and thus deserves more careful examination if to use CCS in practice.

Weaknesses:
The main weakness of the paper is its lack of novelty and potential impact to the field. The paper is more an analysis work on the application of a single method [1] proposed in 2023, which given the speed of ML innovation, it is hard to see long-term benefits of this criticism.  The general principles proposed in the discussion section (Section 6) are interesting and fit more into the line of proposing desiderata for the field - though in their current status, require more rigorous work. 

[1] C. Burns, H. Ye, D. Klein, and J. Steinhardt. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, 357 2023.

Limitations:
Yes.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
h8goI8uPXM;"REVIEW 
Summary:
The paper introduces decoupleQ, a novel method that decouples model parameters into integer and floating-point parts. This approach transforms the quantization problem into a mathematical constrained optimization problem, avoiding the limitations of traditional heuristic quantization methods. DecoupleQ achieves a significant improvement over existing methods in LLM., especially at extreme low bits (2-bit) and also release the W2A16 CUDA kernel.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. DecoupleQ eliminates the need for ad-hoc techniques to handle outliers and sensitive channels, focusing solely on optimizing model accuracy under extreme low-bit quantization.
2. DecoupleQ achieves a notable advancement over existing methods in LLM, particularly at extremely low bit. And the W2A16 CUDA kernel has been released.
3. DecoupleQ approach can be readily extended to supervised fine-tuning (SFT) to enhance model accuracy, or adapted for downstream sub-tasks.

Weaknesses:
1. Please correct me if I am wrong. It seems that decoupleQ combines several existing approaches. Specifically, it uses Adaround to get the integer part in ResNets and GPTQ to get the integer part in LLMs. Additionally, it integrates PTQ and QAT by applying PTQ to the integer part while using supervised training for the floating-point part.
2. Regarding your point from lines 58-61, I believe GPTQ clearly outlines how to calculate scale and zero point in their code. Moreover, GPTQ can be seen as a constrained optimization problem, where the constraints align with yours: each integer weight is confined within [$\alpha$, $\beta$], which is a default constraint in GPTQ.
3. Further experiments on LLMs are essential. For example, evaluating decoupleQ's performance in multi-task settings and within the LlaMa 3 family would provide valuable insights.
4. Could you provide more ablation studies in the second stage, such as experiments without training norm layers?
5. There is a typo in line 125. The first letter of 'decoupleQ' should be capitalized.

Limitations:
Please refer to the weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a linear and uniform quantization method, decoupleQ, which abandons the traditional heuristic quantization paradigm and decouples the model parameters into integer and floating-point parts, then transforming the quantization problem into integer and floating-point part. Experiments show decoupleQ achieves comparable acc as fp16/bf16 on 2-bit weight quantization setting.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Experiments show decoupleQ achieves comparable acc as fp16/bf16 on 2-bit weight quantization setting.

Weaknesses:
1. Experiments are based on W2A16, lower activation bitwidth(<=8bit) should be experimented.
2. The novelty is limited. The core idea of decoupleQ is similar to Normalization(Batch-Norm or Layer-Norm). The learnable floating part of decoupleQ equals to a learnable Normalization parameters.
3. More existing Quantization methods should be compared, such as, NWQ[1], PD-Quant[2]

[1] Leveraging Inter-Layer Dependency for Post -Training Quantization 
[2] PD-Quant: Post-Training Quantization based on Prediction Difference Metric.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents decoupleQ, a post-training quantization method that improves the accuracy of quantized models, particularly at very low bit-widths (2-bit). It achieves this by separating model parameters into integer and floating-point components and formulating the quantization process as a constrained optimization problem. This approach eliminates the need for traditional quantization techniques like outlier handling and focuses on optimizing the core objective.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The paper introduces a fresh perspective on quantization by abandoning traditional methods and reframing it as a constrained optimization problem.
2. decoupleQ demonstrates impressive results in 2-bit quantization, achieving accuracy comparable to higher precision formats like fp16/bf16 in large speech models.
3. The quantization process is linear and uniform, making it easier to implement in hardware compared to non-uniform methods.

Weaknesses:
1. The paper's writing lacks cohesion and clarity regarding its ultimate goal. The paper also has several spelling mistakes.
2. The authors claim to separate the model parameters into integers and floating-point components. However, as far as I understand, this practice is not a novel contribution but rather a common approach in quantization.
3. They address a portion of the optimization problem using GPTQ and another portion similar to BRECQ.
4. The authors acknowledge that their solution may not be optimal. 
5. The quantization process in decoupleQ can be more time-consuming than other methods.

Limitations:
Yes, however, it will be beneficial to divide the current Discussion section into separate Conclusion and Limitations sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel post-training quantization method to achieve 2-bit uniform quantization on large language and speech models. The proposed method decouples the quantized values into integer and floating-point parts, which are then optimized via a constrained optimization problem that can be solved with off-the-shelf solutions. The proposed method allows uniform quantization down to extreme bits.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper proposes a novel optimization-based method to conduct PTQ on large models. The proposed method is solid and unique from previous methods.
2. The proposed method achieves good performance with only uniform quantization, without special procedure for outliers etc., providing direct benefit to the runtime of the quantized model on general hardware.
3. The limitations and future directions are clearly discussed in the paper.

Weaknesses:
1. The distinction between the proposed decoupleQ and the traditional quantization methods are not clearly derived in Sec. 3.2. The statement that ""(s,z) lost the traditional meaning"" on line 138 is not clear. My understanding is that W, s, and z are now totally independent of the original weight w0 in the optimization process, as long as the final output error is minimized? I think adding a comparison with the optimization objective/procedure of the traditional quantization here will help.
2. The proposed method appears to be sensitive to the size of the calibration set, so that the calibration size reported in the experiments are much larger than that of the previous baselines. As it is understandable that the optimization process may require more data to avoid overfitting, it would be more fair if the baseline methods are also calibrated with the same dataset/training cost.
3. For the LLM experiments, only ppl is used as metric. However, the ppl has been shown to be an inaccurate metric to reflect the utility of the LLM after compression. More evaluations such as zero-shot performance on downstream tasks and the instruction following ability etc., as in SqueezeLLM and OmniQuant papers, would be helpful to see if the quantized model still retains the ability as the FP one.

Limitations:
The limitations and potential social impacts are adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
gsott1UXlq;"REVIEW 
Summary:
GATSM effectively captures temporal patterns and handles dynamic-length time series while preserving transparency, outperforming existing GAMs and matching the performance of black-box models like RNNs and Transformers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* This paper is easy to understand.
* GATSM can be understood as a linear representation with good transparency.
* Surprisingly, this method improves performance while providing better interpretability compared with black-box.

Weaknesses:
* It is not clear how multi-head attention works in Definition 3.1 to learn temporal patterns. My understanding is that the global feature interacts with the current feature in Eq.3, so why is it that the input to the attention is not $x$ but the transformed $\tilde_x$. And then, are temporal patterns captured from attention? 

* The addition of attention to NBM is under-motivated, so why not just replace $w^{nbm}$ to attention weight, so that you can learn one less set of parameters $w^{nbm}$.

* The experiment lacks DLinear [1], a strong baseline and providing interpretability. I think it's highly relevant.
1. Are Transformers Effective for Time Series Forecasting? AAAI 2023.

* Given the emphasis on interpretation or white-box modeling, qualitative experiments of the contributions/explanations need to be compared rather than visualization. If there is no ground-truth of the contribution, occlusion experiments in post-hoc methods [2, 3] can also be designed to explore the trade-off between performance and additive features.
2. Encoding time-series explanations through self-supervised model behavior consistency, NeurIPS 2023.
3. TimeX++: learning time-series explanations with information bottleneck, ICML 2024.

* In Table 7, why is the throughput of GATSM so much lower than NBM? Does NBM add up as features at the time level without sharing?

Limitations:
It's hard to deal with the higher-order interactions, e.g. GA^2M/GA^NM in time series, since time series often have long time points and the high complexity of gam-based techniques. In addition, the temporal-level causal interrelationships can be further explored.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GATSM(Generalized Additive Time Series Model), designed for handling multivariate time series data with a focus on transparency and interpretability. Using independent networks to learn feature representations and transparent temporal modules to learn cross-time step dynamics, GATSM effectively learns temporal patterns and maintains interpretability.  It achieves comparable results achieves comparable performance to black-box time series models on various datasets, and proves the transparent predictions with cases.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. A key strength of GATSM is its focus on transparency, providing clear insights into the decision-making process, which is crucial for applications in high-stakes domains like healthcare.
2. The paper presents a thorough evaluation across multiple datasets, including Energy, Rainfall, AirQuality, and several healthcare datasets, showcasing the model's robustness and generalization capabilities.
3. The authors provide extensive details about the experimental setup, including data splits, hyperparameters, and computational resources, facilitating reproducibility.

Weaknesses:
1.  The need for a large number of feature functions can limit scalability (even if it is reduced from TxM to B), particularly with high-dimensional data.
2. The dataset tasks encompass both 1-step forecasting and classification, each requiring distinct evaluation metrics. Presenting all results in a single table without proper clarification leads to confusion. 
3. The selected baseline black-box models are relatively simple. Consider including one or two state-of-the-art methods for a more comprehensive comparison.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a Generalized Additive Model for time series, combining feature embedding and attention layer. The proposed solution is evaluated on forecasting, binary and multiclass classification, over 8 datasets, against black box and transparent models. Global, local, time-focused and feature focused interpretability methods are provided.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Experimental evaluation is convincing: the presented model has state of the art performance. On the transparency side, the comparison between models allows the author to postulate on the presence of absence of interactions of covariates, and time-specific patterns. Evaluations of weights at the final linear layer give varied visualizations.

The reasoning behind the model construction and component is clear, and ablation experiment for each component were provided.

Paper is clear with no major problem in writing.

Weaknesses:
I am not sure if the work is original. The idea of using DNN first on the time axis without covariate interaction is not new, but wether there is a model similar to the proposed solution, I do not know. The review of previous works focuses on Generalized Additive Models, but a similar neural structure may have been presented without being positioned as a GAM.

One improvement to be done would be to add more clarity on figure captions. In its present form, it requires back and forth to the text to understand both what is plotted and what conclusions to draw from it.

Limitations:
Several limitations are identified: possible overfitting of the model due to overparameterization in NBM part, slow attention mechanisms that do not benefit from state of the art methods, and the fact that it was not evaluated for long sequences (and might not be suited for them).

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This work aims to build transparent models for the time series domain for better interpretability. Specifically, they proposed a work called Generalized Additive Time Series Model (GATSM) that consists of independent feature networks as well as a temporal attention module to learn temporal patterns. The corresponding model can be written into a scalar form to ensure interpretability/transparency. The authors applied their model to several datasets, and showed that GATSM can outperform existing generalized additive models. The model can also be used to interpret the features in original dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Important motivation. Building transparent models for time series is a critical task.
- The scalar representation of features seems clean (eq 11)

Weaknesses:
While this work is not of my direct expertise, I think the following contents have room for improvement:

1. Experimental results are weak.
- Black-box Time Series Models seem out-of-dated. The authors should consider better models such as TimesNet, PatchTST, FreqTransformer, or Informer for commonly used black-box models.
- Forecasting tasks use R2 score for evaluation, but an R2 score of 0.07 (or in general, below 0.5) seems very low. The authors should show some visualization examples to ensure the model is functioning.
- Figure 4/5 are not self-explainable, the authors should try to explain what is happening in those figures, and how the interpretability is quantified.
- The work could benefit from synthetic dataset, where casual relationships are manually crafted and thus can be evaluated.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
gBOQ0ACqoO;"REVIEW 
Summary:
This study reveals that modalities have varying impacts depending on depth, leading to the proposal of DH-Fusion. This method
dynamically adjusts feature weights using depth encoding, improving multi-modal 3D object detection. Results on nuScenes show DHFusion outperforms prior methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-presented. The structure is clear and easy to follow.
2. Comprehensive experiments on the nuScenes dataset are conducted to validate the effectiveness of the proposed DH-Fusion.

Weaknesses:
1. Lake of Novelty: The Depth Encoder in DH-Fusion is similar to the 3D Position Encoders in PETR (PETR: Position embedding transformation for multi-view 3d object detection). The Depth-Aware Global Feature Fusion (DGF) module and Depth-Aware Local Feature Fusion (DLF) module in DH-Fusion are analog to the Hierarchical Scene Fusion (HSF) module and Instance-Guided Fusion (IGF) module in IS-Fusion (IS-Fusion: Instance-scene collaborative fusion for multimodal 3d object detection). In conclusion, the contribution of this work seems like ""A+B,"" which is limited.
2. For the nuScenes test leaderboard, DH-Fusion achieved a Top 10 ranking only with 384x1056 image size and SwinTiny backbone.
Please provide results when using larger 900x1600 image size and ConvNeXtS backbone.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed a LiDAR-camera modality feature fusion method based on depth encoding for robust 3D object detection. Based on the observation that the LiDAR and camera modality information should have dynamic relative importance depending on the distance of object to be detected, the paper proposed a Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy which consists of a Depth-Aware Global Feature Fusion (DGF) module and a Depth-Aware Local Feature Fusion (DLF) module. Experiment on the public nuScenes and nuScenes-C dataset demonstrates that the proposed method is robust to various kinds of corruptions and achieves SOTA performance on 3D object detction.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The idea of depth-aware multimodality feature fusion for 3D object detection is reasonable, especially for the detection of distant objects.
2. The ablation study clearly demonstrates the effectiveness of the proposed DGF&DLF module when using BEVFusion as baseline
3. The presentation is clear and the ability of the proposed method on the detection of distant object in Figure 6 is impressive

Weaknesses:
1. How about the algorithm's performance on small object detection? small object could be normal-sized object at far distance or small-sized object in near distance, is it possible that the proposed depth-aware module hurts the detection performance of small-sized object in near distance? since according to Figure 5, LiDAR modality will have relatively larger weights at near distance, but it is in low resolution, so not good for small object detection.
2. Compare with SOTA, the achieved performance improvement is not that significant. as shown in table 1, the performance gap between the proposed method and IS-Fusion is small and IS-Fusion even achieves slightly better mAP, it is not clear whether the proposed method can achieve similar performance improvement as indicated in ablation study when using IS-Fusion as baseline.
3. In Figure 5(b), it would be good to add a color bar to indicate the magnitude corresponding to each color

Limitations:
There is no paragraph explaining the weakness of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel strategy for LiDAR-camera 3D object detection that emphasizes the importance of depth information in feature fusion processes. The authors argue that different modalities, such as LiDAR point clouds and RGB images, contribute variably at different depths, and this variation has been overlooked in previous works. The key contribution is the Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy that dynamically adjusts the weights of point cloud and image features based on depth encoding at both global and local levels. The DH-Fusion method surpasses previous state-of-the-art methods in terms of NDS on the nuScenes dataset and demonstrates robustness to various data corruptions. In general, the design is reasonable and performance is impressive.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is well-structured, with a clear abstract, introduction, methodology, experiments, and conclusion sections that logically flow from one to the next.
2.	The authors effectively communicate complex ideas through clear language and comprehensive illustrations, aiding the reader's understanding of the proposed method.
3.	The motivation of design is clear and experiments are extensive.
4.	The idea of depth encoding for dynamical fusion is interesting and reasonable.
5.	The performance is very impressive and the robustness makes the method more applicable to challenging scene.

Weaknesses:
The paper has no obvious weakness except they didn't do experiments on other datasets.
But I think the nuScenes is already large enough to demonstrate the general effectiveness.

Limitations:
There is no discussion of limitation in main text, but a justification is given in Checklist: using an attention-based approach to interact with the two modalities makes the detection results sensitive to modality loss.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces DH-Fusion, a novel Depth-Aware Hybrid Feature Fusion strategy for multimodal 3D object detection that leverages LiDAR and camera data. The key innovation lies in dynamically adjusting the weights of point cloud and RGB image features based on depth encoding at both global and local levels. The authors propose two modules: Depth-Aware Global Feature Fusion (DGF) and Depth-Aware Local Feature Fusion (DLF), which enhance feature integration and compensate for information loss during the transformation to Bird's-Eye-View (BEV) space. Experiments on the nuScenes dataset demonstrate that DH-Fusion surpasses state-of-the-art methods in terms of Novelty Detection Score (NDS) and is more robust to data corruptions, as evidenced by superior performance on the nuScenes-C dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper proposes a novel feature fusion strategy that adaptively adjusts the weights of LiDAR point cloud and RGB image features based on depth
2. The introduction of depth encoding at both global and local levels allows for more nuanced and context-aware feature integration, enhancing the detector's ability to understand the scene's depth structure.

Weaknesses:
1. The authors only present results on nuScenes dataset. The alogrithms should be also evaluated on other prevailing public dataset like KITTI.
2. The depth-aware fusion might be tailored to the specific characteristics of the training dataset, potentially leading to overfitting and reduced performance on diverse or unseen data.
3. While the paper includes ablation studies, a more extensive set of experiments that isolate the impact of different components of the system could provide deeper insights.

Limitations:
1.  While the method shows strong performance on the nuScenes dataset, its generalizability to other datasets or varied real-world conditions might require further investigation.
2. The paper does not provide a detailed discussion on the computational efficiency, which is crucial for practical applications, especially in terms of processing time and resource usage.
3 .The method assumes high-quality, synchronized data from LiDAR and camera sensors, which might not always be guaranteed in real-world scenarios.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
fzdFPqkAHD;"REVIEW 
Summary:
The paper presents ATS (Agent-To-Sim), a framework to enable agent behavior modeling from multiple casual video captures in indoor scenarios captured during long spans of time. The proposed pipeline consists in (1) 4D reconstruction of the scene geometry and observer and agent motion, and (2) controllable agent behavior learning and generation.

For the first stage, multi-video registration uses coarse-to-fine registration to globally align the cameras to a shared canonical space derived from DINOv2 per-frame features (initialized with a walkthrough clip of the environment) and then jointly optimizes the 3D structures while adjusting the cameras locally with novel featuremetric losses (which makes the optimization robust to changes of lighting and appearance and improves alignment accuracy) and standard photometric and regularization losses. With the proposed (annealed) swapping of latent per-video codes during optimization, missing information is shared across videos, while video-specific details are kept.

For the controllable agent behavior modeling, in order to generate plausible interactive behaviors, the generated behavior conditions on an encoding of the scene, observer, and past from the agent's egocentric perspective, which avoids overfitting to specific locations in the scene. Then, the ego-perception-conditioned generation of full body motion proceeds hierarchically via diffusion: Generated goals Z condition generated paths P, which finally condition generated body motions G.

The included experiments reflect the quality of the 4D reconstructions achieved by the proposal, the improvements in displacement errors compared to two baselines (as well as ablations of the proposed method), and a qualitative analysis of the effects of the behavior conditioning signals.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Great technical achievement to reconstruct agent behavior in indoor settings, exploiting the shared information across different videos captured at different times via robust alignment based on semantic features from foundational image models (DINOv2) and diffusion-based short-term hierarchical motion generation.
- Plausible long-horizon generation of agent motion for different bodies, conditioned on the environment, observer, and past trajectory.
- Despite the complexity of the system, the description is relatively brief and complete, whig, along with the rest of the paper, is excellently written.

Weaknesses:
- The paper focuses on environment-aware motion of agents in the presence of a (human) observer. Even if out of scope for this paper, it would be interesting to discuss more complex agent-environment interactions (see my questions below).
- I believe the current experiments use a small number of environments/scenes, which makes it hard to justify considering the system for larger-scale deployment, but I'll be happy to update my score if the authors correct me.

Limitations:
The authors reasonable address limitations and social impact in the appendices.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses using an iPhone's RGBD camera to collect several hours of videos within a room over a time span of one month. Through these multi-view videos, a 4D reconstruction of the room is generated. A collection of rigid bodies is used to simulate agents (such as cats, dogs, etc.) in the room. Utilizing goal-conditional path generation technology, users can ultimately control the movement of these agents by setting goals.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The video presented in this paper is very effective; it reconstructs 4D video from a single view and reconstructs a complete room from multiple views.
2. In addition to reconstruction, the paper also discusses how to control the movement of the agent through goal-condition path generation.
3. Intuitively, I think this is a good paper and may inspire researchers in the field of 4D reconstruction.

Weaknesses:
1. While I am not an expert in 4D reconstruction, I find the presentation of this paper rather unclear, particularly the methodology section, which is extremely difficult to understand. My confusion began around lines 126-127. What are the color and feature descriptors of the video? I later noticed that ψ is described as the DINOv2 [40] feature of the input image. So, is ψ a feature of an image? How to obtain it? The paper should clarify this. Additionally, what is X, and is it a point cloud obtained from a mobile phone? If so, how does the point cloud acquire its color in Equation 2?

2. I suggest using a table to explain each symbol in detail. If the explanation of a symbol requires context from the paper, ensure it is as understandable as possible. For technical terms, provide detailed explanations within the paper. A comprehensive symbol table in the appendix would significantly enhance the paper's clarity.

3. The paper lacks detailed quantitative experiments to demonstrate the effectiveness of the method.

Limitations:
The authors adequately addressed the limitations and potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents Agent-to-Sim, an approach to learn a 3D agent in a 3D environment from casual videos of the same agent captured over a long horizon. ATS first conducts 4D spatio-temporal reconstruction from the set of videos, including a deformable agent, the background scene, and a moving observer. This is done with a coarse-to-fine video registration method. Then, given the 4D reconstruction, ATS learns a hierarchical diffusion model over the agent's goal, path, and pose trajectories.  The overall approach is tested on a dataset of iPhone videos for over several types of agents and motion patterns.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- I am not a subject matter expert in this field. However, the paper was clear and well-written such that even a non-expert like myself can understand the proposed high-level approach. The attached supplementary materials give a great visual overview of the paper.
- The paper outlines several limitations of the proposed approach and future directions to address them. The limitations are meaningful and help the reader better understand the problem setting, modelling assumptions, and future directions.
- The paper tackles a challenging problem on the path towards building scalable and realistic simulators.

Weaknesses:
- Certain technical details are not clear for readers unfamiliar with the related literature. This limits understanding and reproducibility. See questions.
- Evaluation of the method seems limited and is mostly limited to qualitative comparisons. I suppose this is inevitable given that ATS tackles a new problem setting than related work. However, it does limit the reader's ability to evaluate the significance of this methodology.
- For behavior generation evaluation, I don't understand why certain baselines were selected. In particular, FaF seems like a detection + multi-agent motion forecasting paper for self-driving, so it's not immediately clear how it can be adapted to this setting.

Limitations:
The authors have adequately addressed limitations and potential social impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a method for learning interactive behaviors of various agents, including humans, cats, dogs and a bunny, by leveraging unstructured videos captured casually. The various videos are registered together in a common frame, offering a 4D reconstruction of the agent and the environment. Based on this reconstruction, the multi-modal distribution describing different agent behaviors is learned by using diffusion models and Control UNets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper addresses the very challenging problems of learning agent behaviors from a collection of unstructured videos captured over different sessions. To learn interactive behaviors, both the trajectories of the agent and the surrounding environment need to be reconstructed, as to have relevant context of the behavior. Additionally, the motion of the camera/observer need to be reconstructed as well, to allow the registration of the videos in a common frame. As the videos are collected over a potentially large period of time, change in the environment can occur, complicating the tasks of registration and reconstruction.

The idea of using ego-perception encoding for the learning and generation of plausible interactive behaviors is another strong point. After the agent and the environment are reconstructed, ego-perception encoding is learning perception codes of the scene, the observer and past trajectory, factors that condition the generation of the agent's body motion.

Behavior generation considers the generation of the goal and the conditioned generation of the path, taking into account the goal.

Weaknesses:
There are numerous models employed in the proposed framework. Due to the limited space available, few details are provided about their motivation and their implementation. This makes both understanding of the work and its reproducibility very challenging. 

A particular aspect which is not addressed in detail is the modeling of the agents, especially of animals like cats that are quite challenging due to their non-rigid nature. In particular, it is not clear how eq.2 is combined with eq.3, and why the same number of ""bones"" (b=25, L.137) is used for all agents. Also, the nature of G^b is not discussed in detail. 

Additionally, details on how NeRF-type reconstructions are combined with feature descriptors, and how this helps in handling layout changes is not discussed in detail.

More examples like the previous can be given for different aspects covered in the paper, like camera localization (eq.6), scene alignment (eq.7) and behavior learning (eq.10 and 11). Each of these aspects would certainly require more space for describing in detail the corresponding models and support the relative claims in the experimental evaluation. 

Regarding experimental evaluation in particular, only high-level results regarding the agent behavior prediction are provided, while it would be crucial to quantitatively assess the quality of 4D reconstruction and, importantly, to include a detailed ablative study.

Overall, although some very interesting ideas are proposed in this work, both for 4D reconstruction of agent behaviors and behavior learning and generation, I think that the paper is too densely packed without having enough space to describe the paper contributions in sufficient detail. In my view, even describing in detail one of the 4D reconstruction or agent behavior modeling parts alone would be challenging in the space available. This affects also the experimental evaluation, as not all claims are supported by the results.

### Minor comments
- L.35: ""Such systems do not scale well""
- Figure 1, caption: incomplete sentence ""conditioned different observer trajectories""
- L.88: ""whiling accounts""
- L.113: what ""longitudinal videos"" are?
- Figure 3, caption: what does ""low latency"" means in this context?
- L.215: ""we collect the a""

Limitations:
Limitations of the work are discussed in the Appendix.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
fTOw3BzcWs;"REVIEW 
Summary:
The paper introduces ExID, an offline reinforcement learning algorithm that enhances learning performance in limited data scenarios by combining domain knowledge in the form of simple decision trees with agent experience replay data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Domain Knowledge Utilization: ExID incorporates domain knowledge to guide decision-making in data-limited scenarios
* Teacher-Student Architecture: A teacher network, informed by domain knowledge, regularizes a student critic network to improve generalization.
* Regularization with Domain Knowledge: The algorithm uses a regularization term to align the critic's decisions with the teacher's advice for states covered by domain knowledge.

Weaknesses:
* Discrete Action Space Limitation: The algorithm is currently limited to discrete action spaces, necessitating future extensions for continuous action domains.
* Hyperparameter Tuning Challenge: The need for precise hyperparameter tuning complicates the deployment of ExID in scenarios where extensive optimization is impractical.
* The paper does not have enough strong experiment comparisions. The methods of the paper is related with offline RL methods, such as SCQ[1], ReDS[2], A2PR[3], CPED[4]. But it lacks the experiments comparisions with offlien RL methods. I think adding some SOTA baseline methods will improve your paper. It is not required that experimental comparisons must be given, but at least add some discussion with these methods to the paper.  

References：

[1] Shimizu, Yutaka, et al. ""Strategically Conservative Q-Learning."" arXiv preprint arXiv:2406.04534 (2024).

[2] Singh, Anikait, et al. ""ReDS: offline reinforcement learning with heteroskedastic datasets via support constraints."" Proceedings of the 37th International Conference on Neural Information Processing Systems. 2023.

[3] Liu, Tenglong, et al. ""Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning."" In International Conference on Machine Learning (ICML). PMLR, 2024.

[4] Zhang, Jing, et al. ""Constrained policy optimization with explicit behavior density for offline reinforcement learning."" Advances in Neural Information Processing Systems. 2023

Limitations:
* The paper only conducts experiments in several simulated environments and a real-world sales promotion dataset, which may not fully verify the effectiveness and applicability of the algorithm in more diverse and complex real-world scenarios.
* The performance of the ExID algorithm heavily relies on the quality of the domain knowledge. If the domain knowledge is incomplete, inaccurate, or biased, it may mislead the learning process and result in suboptimal policies. Moreover, obtaining high-quality domain knowledge can be challenging and time-consuming in practice.
* The proposed method mainly concentrates on discrete action spaces, and its performance and applicability in continuous action spaces are not clear. This limits the algorithm's utility in many real-world control tasks that involve continuous action spaces.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies offline RL when data is limited. The authors propose a domain knowledge-based regularization technique to learn from an initial tracker network and limited data buffer. The experiments verified the effectiveness of the proposal, which outperforms the classic RL baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is simple and technically reasonable.
2. The experimental results on the real sales promotion dataset show the proposal is a promising solution in real-world applications.

Weaknesses:
1. The technical novelty is limited. Despite the claimed use of expert knowledge, the method adopted by the paper is to directly train a policy from the knowledge, which assumes that the information provided by the domain knowledge is at the state-action level (a decision tree in this paper), which limits the feasibility of this method. Compared to the use of knowledge between latent concepts discussed in neuro-symbolic learning, I think it's more like traditional model distillation. 
2. In practice, limited offline data may come from domain knowledge-based strategies, such as human-designed rules, thus I have great concerns about whether these two can promote each other. Empirical studies on more real-world datasets or rigorous theoretical analysis will provide support to this issue and further improve this work.  
3. The introduction uses the sales task as an example, but the visualization is based on the Mountain Car dataset.
4. Definition 4.1 seems strange, why not directly define the offline dataset as a subset of the complete state spaces?
5. The $\eta$ in Proposition 4.2 is not well defined.

Limitations:
The authors have provided a discussion about the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel technique ExID, a domain knowledge-based regularization method, that adaptively refines initial domain knowledge to boost performance of offline reinforcement learning (RL) in limited-data scenarios. The key insight is leveraging a teacher policy, trained with domain knowledge, to guide the learning process of the offline-optimized RL agent (student policy). This mitigates the issue of erroneous actions in sparse samples and unobserved states by having the domain knowledge-induced teacher network to cover them. And the initial domain knowledge would be improved when the student policy reaches a better perform than the teacher policy.  Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase compared to traditional offline RL algorithms operating on limited data

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Originality: The paper's originality lies in its integration of domain knowledge into offline RL through a teacher policy network. This approach addresses performance degradation in limited-data settings, which is a novel and underexplored area. The introduction of the domain knowledge-based regularization technique and adaptive refinement of initial domain knowledge are particularly innovative.

2. Quality: The quality of the work is evidenced by the solid theoretical analysis and the thorough empirical evaluations conducted on multiple standard datasets, including OpenAI Gym environments (Mountain Car, Cart-Pole, Lunar Lander) and MiniGrid environments, as well as a real-world sales promotion dataset. The results consistently show that ExID outperforms existing offline RL algorithms in these settings.

3. Clarity: The paper is well-structured, with clear explanations of the problem, methodology, and results. The use of diagrams and tables helps understand the motivation of the problem (figure 1), the proposed method (figure 2), illustrate the effectiveness of ExID (Table 1-2). Each section logically follows from the previous one, making the overall argument easy to follow.

4. Significance: By tackling the challenge of limited data in offline RL, the paper makes a significant contribution to the field. The proposed approach has practical implications for various real-world applications where data is scarce and expert knowledge is available, such as in business, healthcare, and robotics.

Weaknesses:
1. Generalization to Continuous Domains: The paper is limited to discrete action spaces, which restricts its applicability to a broader range of RL problems involving continuous action spaces. This limitation is acknowledged by the authors.


2. Scalability: The scalability of ExID to more complex environments that requires a complex representation (e.g., a significant large tree) of domain knowledge is not thoroughly explored.  It would be beneficial to understand how the method performs in such settings and what challenges might arise because the challenging of updating the domain knowledge represented in a complex representation could hinder the learning process of the student policy in the proposed method ExID.

Limitations:
The authors acknowledge several limitations of their work, including the reliance on the quality of domain knowledge and the focus on discrete action spaces. While these limitations are well-addressed in the paper, it may be worth to consider a broad evaluation:
   * Conducting experiments on a wider variety of environments that have larger state and action spaces, would provide a more comprehensive evaluation of the method's applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
egGFHCFPiU;"REVIEW 
Summary:
This work creates a hybrid LLM and classic planning algorithm, by integrating a LLM into the GraphPlan algorithm. The GraphPlan is an algorithm that solves a relaxed planning problem (forward expansion), and then traverses the created graph to find a valid plan (backtracking). Both steps are expensive. In the hybrid approach, a LLM is prompted in the forward expansion to limit the exploration of states deemed irrelevant. In the backtracking phase, the LLM is used to sort actions to explore first. Experiments with corrupted domain files show that LLMs can better handle corruption than the GraphPlan algorithm.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- A very interesting novel idea of a hybrid planning approach with a fundamental classic planning algorithm.
- The paper provides an introduction to an interesting research area of classical planning (e.g., Figure 4).

Weaknesses:
- Multiple missing experiments and discussions severely undermine the results of the paper.
    - It is not clearly motivated why experiments with corrupted pddl domain files are interesting. This was introduced quite suddenly in the *results section* (lines 261-262) without enough details and without providing motivation.
    - The paper is missing important discussion and experiments about the trade-off between the hybrid approach and the classic GP algorithm. Experiments with valid pddl domain files are not included, which could have alleviate it.  
    - The effect of hyperparameters on the results, such as the number of iterations (N) in Algorithm 1, is not discussed.
    - The failure of LLMs4PLAN-GPT3.5 compared to the phenomenal success of LLMs4Plan-GPT4 is somewhat unexpected and undermines the results of the paper.
- Multiple details are missing regarding the experimental setups. (see questions below)
- The paper's writing needs to be improved. (see suggestions below)

Limitations:
The authors did not discuss limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates how large language models (LLMs) can be integrated into established planning frameworks, specifically graph-based planning. The authors propose a novel framework called LLMs4Plan, which incorporates LLMs at two critical stages of the planning process: action selection during graph expansion and candidate action set generation during backtracking. The framework is tested across various planning domains, demonstrating improved efficiency and effectiveness in planning tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper's approach of embedding LLMs into graph-based planning is innovative and contributes to the field of automated planning.
2. The technical implementation of LLMs4Plan is well-detailed, with descriptions of how LLMs are utilized in action selection and candidate set generation.
3. The effectiveness of the proposed framework is empirically validated across ten planning domains, showcasing its practical applicability.

Weaknesses:
1. The proposed integration of LLMs into planning frameworks in LLMs4Plan may be complex and difficult to scale.
2. Comparisons with more recent LLM integrated planning baselines is limited.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
There have been debates about the fundamental planning abilities of LLMs in planning tasks. To achieve more reliable performance, several recent works have embedded an LLM into a search framework (e.g., MCTS, BFS) and viewed LLMs as heuristics. Along this line, this work take a closer look at the roles LLMs can play in Planning Graph. It considers two tasks for LLMs: pruning actions and sorting actions (as heuristics).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written, with precise language and formalism.
- The experiment is conducted on over 10 domains, making it quite comprehensive.

Weaknesses:
1. My biggest concern with this work is that it restricts the use of LLMs to specific roles within a classical planning algorithm. There are many other roles LLMs can play in planning. For instance, see the recent LLM-modulo framework below. Instead of just filtering and ranking actions, LLMs have also been used to evaluate state values or rank plans (i.e., action sequences rather than individual actions).

    - Kambhampati, Subbarao, et al. ""Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks."" ICML 2024

2. The evaluation based on the number of nodes explored is partial. We should not ignore the time cost (e.g., latency of calling LLMs) + financial cost of using commercial LLMs. It could be very likely that, although LLM+Graph Planning expands fewer nodes, it may take a longer wall-clock time to give the final outputs. I understand that the evaluation could be tricky and it remains an open question for a while. However, the authors should at least make an attempt to address this.

3. In the abstract, this statement is inaccurate: “works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs.” There have been quite some paper embedding LLMs in off-the-shelf planning algos

    - Zhao, Zirui, Wee Sun Lee, and David Hsu. ""Large language models as commonsense knowledge for large-scale task planning."" NeurIPS 2023.
    - Yao, Shunyu, et al. ""Tree of thoughts: Deliberate problem solving with large language models."" NeurIPS 2023.


4. While the corrupted domain model experiment looks interesting, it is unclear what messages it tries to convey. Specifically, why would one run the algo on top of a corrupted domain model when there exists approaches that can leverage LLMs to help complete the domain model before starting the search?

    - Guan, Lin, et al. ""Leveraging pre-trained large language models to construct and utilize world models for model-based task planning."" NeurIPS 2023
    - Wong, Lionel, et al. ""Learning adaptive planning representations with natural language guidance."" ICLR 2024.


5. The step of LLM-based action pruning can make the search incomplete, since an LLM may keep ignoring the required action(s) -- in other word, there is no guarantee that the LLM can produce a goal-reaching plan. I notice the authors mention this at a later section (which should be moved to earlier part) that including pruning probabilities could address the problem. I don’t fully agree with this. Can the authors give more detail on how pruning probabilities could guarantee completeness?

6. In the prompt (fig. 3), only the proposition set at the current state is provided. Did the authors consider including the running history of actions (i.e., the partial plan)? Would this affect the overall performance?

7. Line 109: typo in “Algorithm ??”

8. Several works (mentioned earlier) already show that LLMs can be useful heuristics. Can the authors elaborate on the new insights this work provides?

-----
Overall, this study provides a thorough evaluation of LLMs within the Planning Graph algorithm. I appreciate the comprehensiveness of the experiments. However, I also have concerns over the scope of this study (i.e., restricting itself to a limited set of roles). I need to discuss with other reviewers and the authors before finalizing my recommendation.

Limitations:
See the Weakness section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to investigate integrating large language models (LLMs) into classical planning frameworks to enhance the planning effectiveness. The authors proposed a novel method named LLMs4Plan which integrates LLMs into action selection and mutual constraints solving within the graph-based planning framework. Evaluated across ten classic planning problems, this approach demonstrates improved success rates and reduced computational complexity compared to traditional methods. The study concludes that while LLMs alone are insufficient for planning, their integration into classical frameworks significantly boosts performance,.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper investigates an intriguing topic: the performance of LLMs in classical planning problems. While the impressive performance of LLMs in natural language processing and coding tasks is well-investigated, their efficacy in planning tasks remains largely unexplored. Understanding whether LLMs can replace classical planning algorithms is a significant and meaningful research question.
2. The paper conducts extensive experiments on ten classical planning problems, which enhances the credibility of its findings and conclusions. This comprehensive evaluation demonstrates the robustness of the proposed approach.
3. The paper reveals that LLMs still cannot surpass classical planning algorithms, thereby highlighting a valuable direction for future research. This insight encourages further investigation into how LLMs can be effectively integrated with traditional planning methods.

Weaknesses:
1. Although the authors point out that LLMs cannot outperform classical planning algorithms on their own and need to be integrated with classical methods to perform well, the paper lacks detailed insights on this integration. For example, specific strategies for integrating LLMs with the classic planning algorithms and the roles where LLMs excel within planning problems are not thoroughly discussed. The designed ""expandGraph"" and ""sortActions"" may not be the best practice manner. Future research directions to enhance the planning capabilities of LLMs should be more explicitly outlined.
2. The experiments are conducted in simulated planning domains, and the paper does not provide real-world applications or case studies to validate the practical utility of the approach. Including experimental results from more realistic scenarios would strengthen the paper.
3. While the method is effective for graph-based planning, its applicability to other planning frameworks or domains is not thoroughly investigated. A broader analysis could reveal the versatility of the proposed approach.
4. Typos: Algorithm ?? in Line 109.

Limitations:
See the Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
eYUbBKxABP;"REVIEW 
Summary:
The paper presents a formalization of fairness metrics intended to ease analysis of discrimination by automated decision making systems in the UK. While there is a relatively applied angle, the bulk of the contribution is intended to be a generic and re-targetable mathematical formalism.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper shows significant strength in its understanding of nuance with the way law works–something that is sorely missing from the vast majority of CS papers that attempt to handle legal concepts. I was very pleased overall by the mapping the authors performed between relevant legal concepts in the UK and their formal model of fairness. The bulk of the contribution here is in the modelling–which while it results in a simple formulation, should not be taken to undercut the value of the contribution.

Non-US legal contexts often get left out of the literature, even common law jurisdictions–yet they impact a significant number of people, and this work takes formalising fairness across that rubicon.

Weaknesses:
I do not have any major scientific critiques, though there were areas where the clarity of the paper could improve.

Lines 240-274 were written in harder to parse prose than the bulk of the rest of the paper. I had to reread that area multiple times.

The case study in Appendix A was actually very useful for understanding the authors' formalism and it is a shape that some of that context was not woven into the paper as concrete examples of how to understand the math.

The discussion on proxy discrimination never seemed to finish? I wasn't able to understand its meaning under UK law.


Missing a ref to Homer on L299.

All these are very minor issues. I'm substantially in favour of accepting this paper.

Limitations:
Ultimately, adherence to a formalism is *not* what courts generally take into account. While statistical analyses may be used to advance a given line of argument, the standards used are open-textured–and this is an inherent limitation of this line of work.
It also would have been good to see where this formalism sits under EU law (or representative EU-member law) or perhaps a discussion of how civil law jurisdictions handle these sorts of issues.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper maps existing literature and law on algorithmic fairness onto a decision-theoretic framework. It describes various desiderata (e.g. statistical parity) and legal restrictions (e.g., legitimate aims) in terms of expectations, distributions, estimation error, etc.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written and survey a large literature. It appears to state legal tests (particularly under U.K.) with care, while being careful not to overclaim about what its definitions actually establish.

Weaknesses:
n/a

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
- There is a gap between the definitions of fairness studied in the computer science literature, and the definitions of fairness operationalized by courts adjudicating discrimination claims. This limits the usefulness of the CS definitions.
- Amongst work attempting to reconcile legal and computational definitions of fairness, little has focused on anti-discrimination law outside the US.
- This paper makes four contributions in this context:
    - (1) It formalizes elements of anti-discrimination law into a decision-theoretic formalism
    - (2) If analyzes the legal role of the data-generation process
    - (3) It proposes conditional estimation parity as a legally-informed target
    - (4) It provides recommendations on creating SML models that minimize the risk of unlawful discrimination in automated decision-making

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The paper’s focus is interesting–the fairness literature is biased towards the US, and I imagine most fairness researchers would be unaware of subtle differences between UK and US anti-discrimination law.
- Because UK law is influential around the world, understanding how it regulates fairness in algorithmic systems has global importance.

Weaknesses:
- Much of the paper reads like a review of anti-discrimination law. This makes it difficult to parse out (1) what the technical contributions are, (2) why they’re novel, and (3) why they matter. 
- It’s extremely unclear what the technical payoff of the paper’s modeling choices are. The fairness field is overwhelmed with different definitions/frameworks. Why is the one proposed by the author’s meaningful over others? 
- It seems like an essential point to the paper’s argument is that prior work hasn’t studied UK anti-discrimination law. But if the paper wants to successfully extend that into an argument about modeling choices, I think it needs to explain why the existing definitions of fairness do not work for UK law.
- The recommendations provided are extremely general. Are these new or different from the many recommendations that already exist in the fairness/responsible AI literature?

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issues around existing fairness metrics and bias detection/mitigation methods not corresponding with legal notions of fairness, specifically under UK anti-discrimination law. The authors propose a theoretical framework for a data-generating process that aims to formalise the legitimacy of decisions and features in the data. Further, they propose a new metric ""conditional estimation parity"" which compares estimation errors for different protected groups.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well written and coherent. It translates potentially inaccessible legal scholarship and discussions clearly for a technical audience.
2. There is interesting discussion and the paper combines existing literature well. Although these discussions are not particularly novel, UK Equality Law in particular is rarely discussed and the investigations done here are useful to extend the literature for this niche.
3. The work addresses some big limitations in existing literature such as existing fairness metrics not aligning with legal notions of discrimination, particularly under non-US regulations, not considering context of what features are legitimate for an application or considering the estimation errors of decisions.

Weaknesses:
1. A lot of the paper is background or a collation of existing literature. The main contribution is the new conditional estimation metric metric but this metric relies on the true DGP and evaluating the estimation error which, as stated, can be complex in practice. This could make it difficult to use the metric in practice.
2. I understand it would be hard to use the metric for evaluating discrimination in existing datasets for the reasons specified above and also due to the inherent context-dependency of the metric (which is a benefit) but it could be useful to include some experimentation or results in a hypothetical scenario to show how it might be used in practice. As there are no results as such to comment on, it is difficult to assess it's significance.
3. The conclusions drawn such as ""Assess data legitimacy"" or ""Build an accurate model"", although justified with evidence in the paper, are not novel and are pretty standard, common-sense recommendations. 
4. Overall, the main novel contribution is the new metric but this is a small part of the paper. The rest of the paper is a nice collation and narrative of existing literature but I am not sure it significantly advances the field.

Other comments:
1. I can't see where SML terminology is introduced - I assume this means supervised machine learning?
2. In Section 1.4, DGPs are mentioned for the first time. It would be useful to have some more background to them before this - what exactly is a DGP? I do not believe it is ever explained.

Limitations:
The authors are honest about the strengths and weaknesses of their work (although some are hidden away and not pointed towards in the checklist). It would be useful to improve the discussion of limitations in Section 1.4 as it only mentions the limitation of applicability only in the UK.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a UK-and-European-law-based view of anti-discrimination law as it relates to fair machine learning and automated decision systems. It does a good job laying out the doctrine, arguing correctly that work in this area to-date is very centered on US legal concepts such as disparate treatment vs. disparate impact. Although I am willing to believe that there are subtle differences that drive important aspects of fair ML analysis, as the paper claims, I think the specifics of these differences could be made much clearer and need to be for the paper to have the impact it should.

Of particular note, the paper is very well situated in the surrounding literature. Although this contextualization should make the contributions more clearly offset from prior work, as presented I find the opposite: it is difficult to tell what is new as a contribution here. For example, while the contributions are clearly identified in 1.4, I think it would aid the paper if they appeared higher in the intro and were clearer about what is new and why it matters. The example in Appendix A could be used as a running example to show where new concepts are needed and what about existing work does not capture this different legal regime. In particular, after claiming that disparate treatment/disparate impact are distinct to direct & indirect discrimination, the definitions given from 105-114 seem to align tightly to the former. And while I'm not a lawyer, I don't believe that disparate impact claims require a showing of intent under US law either, so I found that distinction somewhat confusing.

On the technical level, the discussion of the true data generating process should really be contextualized in the literature on measurement and construct validity, specifically with respect to work by Jacobs & Wallach, which in particular encompasses the material in 2.3 on estimation parity (at least in part). Also, the causal analysis components of the discussion of data generation could cite more of the work of Kohler-Hausmann and also Hu (one paper from these authors is cited, but others are also relevant and speak more directly to causality and counterfactual fairness claims).

As a final observation, although the ML community talks in terms of ""fair"" outcomes, it is often conceptually clearer (and more in line with legal analysis) to use the same techniques as tools for identifying ""unfair"" activities or outcomes. Phrasing some of the claims this way may condense some arguments and tighten the presentation overall. Related to this, the discussion of these tools as part of an overall practical strategy for risk management is important and should receive more attention. For example, it would be good to discuss how the measures proposed would be used in real legal analysis of an example, such as in litigation or a regulatory proceeding.

I was also a bit confused about the analysis of constructed proxies for protected variables in 2.7. I understand that it's necessary to look beyond a formalistic view of whether a specific attribute is considered, but what happens if the proxy for a protected attribute is (say) the sum of two legitimate attributes? Why is it good enough to use only legitimate features? Also, at 393-394 it might be valuable to look at the recent paper on ""Less Discriminatory Algorithms"" and compare the approaches and outlooks.

Incredibly minor: 
* There is a missing period at 81.
* At 284-288, there is a latent call to questions of ecological validity which could be made more explicit

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* Generalizing beyond the US legal context is important and valuable and this paper does a good job explaining the UK and related legal systems' approach to anti-discrimination law.
* The paper is well written and well situated in existing literature

Weaknesses:
* Novelty is at times hard to identify. I think it's there, but the claims on what it covers should be clearer. In particular, the discussion of the decision-theoretic framing seems a bit under-attended even though it's potentially very useful.
* Some important concepts are missed, notably theories of measurement and construct validity/reliability are at least partially re-invented when they should just be treated as background.

Limitations:
I believe the limitations are expressed well.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
eM3Wzs6Unt;"REVIEW 
Summary:
The paper presents an off-policy hierarchical RL method, based on the HiT-MDP formulation of a Semi-MDP. The HiT-MDP formulation treats the option $o$ as an extension of the original state $s$ (which can be chosen by an extended action), and combines initialization-, termination- and option-policy in a single Markovian master policy $p(o\_{t}|s\_t,o\_{t-1})$. The policy in the extended state-action space, thus, decomposes into the high-level and low-level policies, $p(o\_{t}, a\_t | s\_t, o\_{t-1}) = p(o\_{t} | s\_t, o\_{t-1}) p(a\_t | s\_t, o\_{t})$, which can be trained using standard RL algorihms. 
Compared to the prior work, the paper makes the following contributions:
- Whereas previously PPO was used for reinforcement learning, the paper proposes to use SAC, resulting in improved sample efficiency
- The paper motivates the algorithm from a control-as-inference perspective

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The proposed method seems to be technically sound, and using off-policy agents for HiT-MDPs seems sensible. (Quality)

The provided code clarifies the implementation which helps reproducibility. (Quality)

The presentation is mostly clear. (Clarity)

Applying an off-policy agents to HiT-MDPs seems to be novel and effective (Origingality, Significance)

Weaknesses:
Originality
-----------
One of the main weaknesses of the submission is the limited novelty. Replacing the PPO agent of MOPG by a SAC agent seems to be straightforward, so this contribution is quite incremental. Indeed, the authors of HiT-MDP stated, that their ELBO ""can easily be extended to a SAC-like algorithm"" [35]. Furthermore, given that MaxEnt-RL was already derived from a control-as-inference perspective, deriving the special case of an HiT-MDP using this technique does not seem to be significant contribution either. I also don't see the value of this derivation that would justify devoting so much space on it; couldn't we just argue that we apply SAC to such particular form of an MDP?


Quality
---------
The experimental evaluation seems to be another weakness of the submission. While the method is evaluated on a reasonable number of MuJoCo environments, where it outperforms a reasonable number of baselines, the choice of baselines is not convincing because it looks like the method is only compared to on-policy algorithm. The submission claims that there method ""significantly outperforms existing on-policy and off-policy option variants"", but it is not clear to me to which off-policy baselines this claim refers to. It would be important to focus to flat and hierarchical off-policy methods in the experiments, such as [19], [50], [33] and Hao et. al (2023).  Furthermore, the choice of environments is not convincing, because it does not include more challenging long-horizon tasks that are typically used for evaluating HRL methods, such as Ant-Maze. While the performance on the standard locomotion environments is reasonable, the reported numbers don't seem to improve on the SOTA of flat-RL methods.   

The paper does not discuss the hyperparameter search although it states in the questionnary  that these details are provided in the main content and the appendix.

The paper argues that it did not perform any ablations due to limited computational resources. However, I don't find this argument very convincing, since the experiments are performed on simple vision-free locomotion tasks, that can be run on standard workstation, not even requiring any GPU. Ablations on the number of options would be very useful.



Clarity
---------
I found the background material on control-as-inference a bit confusing. In particular, line 106 which states states policy improvement constitues an M-Step of an EM algorithm that *maximizes* the KL towards $P(\tau|\mathcal{E})$. I don't think any practical algorithm involves such maximization, since the optimum would correspond to a delta distribution on the least-likely trajectory. (

Visually, the presentation is rather bad. Figures are not on the top, and in particular Fig. 2 seems to hide some text, since the sentence in line 271 ends with "", which"". Fig. 2 itself could be improved by increasing the plot sizes (there are some unnecessary white spaces) and by making the legend more readable.  


Significance
-----------------
While I think that the proposed combination of the HiT-MDP formulation and SAC is somewhat interesting, the submission does not provide a convincing argument for the method. When should I use it, instead of existing (hierarchical or flat) methods?

References
----------
* Hao, C., Weaver, C., Tang, C., Kawamoto, K., Tomizuka, M., & Zhan, W. (2023). Skill-critic: Refining learned skills for reinforcement learning. arXiv preprint arXiv:2306.08388.

Limitations:
The limitations are adequately discussed and I don't have any concerns regarding negative societal impact of the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Variational Markovian Option Critic (VMOC), an off-policy algorithm for hierarchical reinforcement learning. VMOC aims to address exploration inefficiency and update instability in existing methods. Key contributions include: 1. Use of variational inference for update stabilization 2. Low-cost option embeddings for improved scalability. The authors evaluate VMOC on Mujoco environments, comparing it to other on-policy and off-policy methods. They report improved performance in learning option sets for complex tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The paper is well-written, and the proposed method is theoretically justified.
2. The empirical evaluations show favorable results compared with existing methods.

Weaknesses:
1. Very similar ideas of the variational option framework have been proposed in [33] (off-policy) and [35] (on-policy). While [35] proposes an on-policy version, its off-policy version is also straightforward to deduce following [ref1]. The use of option embeddings is following [35].
2. The empirical evaluations are very limited; there is no ablative evaluation reported, which makes it hard to determine the contribution of the proposed method to the overall performance gain over various baselines.

References: 
[ref1] Levine, Sergey. ""Reinforcement learning and control as probabilistic inference: Tutorial and review."" arXiv preprint arXiv:1805.00909 (2018).

Limitations:
The empirical evaluations, especially ablation studies, are somewhat limited in scope.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the Variational Markovian Option Critic (VMOC) which combines variation policy iteration and the option critic. VMOC also modifies HiT-MDPs, where options are represented as latent embeddings rather than triples of (init states, policy, termination condition), to the off-policy setting. The paper performs comparisons to option-based methods and PPO on 10 Mujoco environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy-to-read. The figures clearly highlight the performance of the method. The translation from theory to the practical algorithm is well detailed.

2. The advantage in sample-efficiency over other option methods and PPO is clearly seen in Figure 2 across Mujoco environments. In fact, this gain looks to be in atleast two orders-of-magnitude (of fewer steps required by VMOC) which is amazing. The underlying MaxEnt objective in VMOC appears to be very useful with exploration in the high-dim mujoco envs.

Weaknesses:
1. It is not clear if this gain in sample-efficiency will transfer to discrete environments or is somehow applicable only in continuous envs. Perhaps the authors can perform comparisons on Atari or Procgen to demonstrate the same? It would be great if the authors could also discuss the changes in the algorithm in the discrete and continuous settings (perhaps such as the sampling of a_t from the replay buffer?)

2. It is unclear if all methods use the same number of options (e.g. the value used in VMOC appears to be 4). A clear ablation of various design choices like number of options would help demonstrate that VMOC is thoroughly better than the other option methods and is not brittle to hyperparameter choice. 

The analysis of the actual options learnt is also missing (this is for example seen in the option critic paper). This, alongside an analysis of the number of options, is crucial to understand if the method is actual learning composed actions that are further composable and generalizable or degenerating to something simple like learning the action primitives (although the latter would apply more to a discrete rather than continuous env).

3. Minor comment: The location of Theorem 1 in the preliminaries makes it unclear if it is a contribution of the authors or well-known statement. Perhaps the authors can clarify?

4. Another minor comment: It would be great if the authors could discuss other ways of combining options in the related work such as in [1] and [2].

[1] The Option Keyboard: Combining Skills in Reinforcement Learning, Barreto et al, NeurIPS 2019

[2] Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates, Dutta et al, CoRL 2022

Limitations:
The authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces e Variational Markovian Option Critic (VMOC), which learns actions and options simultatenously. They build upon the Hidden Temporal Markovian Decision Process (HiT-MDP) [1] to build a novel off-policy algorithm that utilizes entropy augmented rewards. Their method learns options’ embedding vectors (rather than conventional option tuples utilized in Semi-MDP [2]). They benchmark the learning performance of their method against several competitors on many classic control benchmark environments. 

*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.
2. Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2), 181-211.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- Extensive comparison against 8 competitor algorithms on 10 benchmark tasks
- A novel Soft Option Policy Iteration Theorem

Weaknesses:
The paper does offer a potentially interesting contribution to the wider research community. But it is held back by the lack of clarity and polish in writing. For example, two glaring signs of a hasty submission:
1. Sec 4 and Sec 5 are both titled experiments. Sec 4 is only 1 paragraph, and it essentially repeats the same information from the introductory paragraphs of Sec 5
2. In Sec. 5, line 271 just trails off without completion. I believe the authors moved around the images to correct for vertical space and accidentally hid the text.

While the experimental results focus on learning curves, where VMOC does well, they fail to provide other relevant evaluation metrics:
1. What do the learned options look like? A good evaluation could follow Fig. 5 and Fig. 6 from [1]
2. How many options are learned? Digging through the appendix, it says that they learned 4 option vectors. This leads to another question: how do they choose the number of options to learn?
3. The VMOC algorithm listed in the appendix only describes the gradient update process. No details about action sampling or other hyper-parameter tuning are described here
4. The environments used are challenging for model-free RL algorithms. That said, they may not be satisfactory for showcasing the potential of learned options. 


*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.

Limitations:
Much like Soft Actor-Critic, this work develops a novel Soft Option Critic style algorithm. I believe this line of work is very interesting and potentially impactful in the near future. However, their current draft is not well-written and hard to follow. Their experimental evaluation is also insufficient.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
UqvAFl0lkT;"REVIEW 
Summary:
This paper investigates using an emergent communication protocol as a auxiliary reward in navigation reinforcement learning setting, particularly those where exploration is a difficult (i.e., sparse reward settings).  The experiments show that certain emergent communication games can be effective in solving the RL problem.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
In conjunction with standard criteria, there are three characteristics that are particularly important for emergent communication research: reusability (how easily can another researcher use the products of this research), generalizability (how much do the findings of this research apply broadly to our knowledge of emergent communication), and directedness (does this research contribute concretely to particular questions in emergent communication research).

### Quality
- (minor) The experiment demonstrate that some of proposed approaches beat the baseline.
### Clarity
- Nothing of note.
### Reusability
- Nothing of note.
### Generalizability
- Nothing of note.
### Directedness
- (minor) Comparing emergent and natural language's utility for reinforcement learning is an important problem in emergent communication research.
- (minor) If emergent communication protocols are effective abstract representations of environment states, this could be useful to RL more generally.

Weaknesses:
### Quality
- See Clarity.
- (major) Is there a ""competitive baseline"" tested in this experiments; that is, the state-of-the-art, no-frills method that the proposed solutions would be competing against in the real world?  If there is, the comparison needs to be a clearer as to what exactly the advantage of using EReLELA is.
- (minor) The natural language baseline does not seem to actually be ""natural""; synthetic seems more accurate.  If the language procedurally generated, I do not think it can be considered natural.
### Clarity
- (major) I found this paper (esp. Section 3) very difficult to understand, even after rereading certain sections.  Overall, I do not have a concrete idea of what EReLELA is or why it is important.  For example, I understand that emergent communication protocol is supposed to abstracting observations and that the referential game is used as an ""Intrinsic Reward Generator"", but I do not understand how this is incorporated into the RL algorithm.  Furthermore, how is the referential game distinguished from more straightforward ways of generating auxiliary rewards?
### Reusability
- See Clarity.
### Generalizability
- (minor) How would the core findings of EReLELA apply more generally to other RL or emergent communication settings?
### Directedness
- Nothing of note.

Limitations:
N/A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work presents the idea of employing emergent languages (EL) abstractions combined with count-based approaches for exploration, to improve exploration in sparse reward reinforcement learning (RL) settings.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Overall, the major strength of this paper comes from its **novelty**. 

* The idea of emergent languages in RL has been scarcely explored and the idea presented is sensible and interesting.
* The Compactness Ambiguity Metric (CAM) definition is useful to compare how emergent languages compare to natural languages.
* The results presented in simple Minigrid environments show that the method has the potential to improve exploration capabilities.

Weaknesses:
The paper presents some weaknesses that make my opinion lean towards rejection.
* **Presentation**: from an aesthetic point of view there are things make the paper hard to read, such as the use of bright colours for text on a white background (see Experiments section) or wrapped Figures and equations that are too close to the main text (see captions of Fig.1 and 2)
* **Clarity**: some of the explanations provided are not completely clear. For instance, I struggle to understand what is happening in Figure 1 and the caption of the Figure (which is a long description with no sentence breaks) does not clarify enough. Similarly, for Figure 2, it is not clear what is happening, e.g. why some events are above or below the black line, and the Figure is not clearly explained in the text or caption.
* **Insights on the learned representation**: given that the use of emergent language abstractions is the main contribution, it would have been useful to get more insights into the representation being learned by the agent. While the authors present quantitative results in terms of performance or distance from natural language, it is not clear what are the properties of the emerging language, e.g. sentence length, number of unique utterances, etc. Assuming the RL community is one of the targeted audiences for the paper, it would be crucial to present some insights into this to ensure the contribution is clear.
* **Limited evaluation**: the evaluation is extensive in terms of ablations (also to be found in Appendix) but is quite limited in terms of environments and baselines tested. Differences from other related works, e.g. reference [51], should be at least described more in details, if running additional baselines is not feasible

Limitations:
Some limitations are presented at the end of the experiments section

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to leverage the Emergence Communication paradigm via the use of referential games to learn state abstractions for a Reinforcement Learning domain. The authors claim that using this approach, their proposed method is able to learn abstractions that boost exploration for an RL agent, and leads to performance that is comparable to Natural Language-based state abstractions.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1) The paper strongly motivates the problem highlighting the need of state-based abstractions for RL agents, the advantages and limitations of Natural Language-based abstractions, and how Emergent Language-based abstractions can avoid those limitations while achieving comparable results.
2) In my honest opinion, this paper greatly stands out for explaining the relevant literature and how this paper situates itself within the existing works. I thoroughly enjoyed reading the Introduction, and Section 2.1even more so! 
3) The method is well-explained, and experimental setup flows very logically making it intuitive to the readers the insights presented by the paper and the questions that arise.

Weaknesses:
Please see the questions section for more details.

Limitations:
The authors have discussed important limitations of the work, and also a well thought-out section on the broader impact of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
EReLELA investigates whether by asking the agent to learn and describe the environment through emergent language (EL) can help with hard exploration tasks, compared to using natural language (NL) description alone.

I personally find this angle interesting and refreshing -- and the connection to count-based exploration bonus is novel.

In theory, EL should work better than NL because by definition of pragmatics, through reference game (RG), the description from EL should be more compact and discriminative than NL. However, I appreciate the honesty of the authors that they point out there was no significant difference between EL and NL.

I thought back and forth about whether to accept or reject this paper. My current stance is that -- if the authors cannot substantially rewrite the experiment section and update their figures to make their conclusions very easy to understand, I don't think this paper meets the bar of acceptance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The direction is novel. The idea and execution are both solid. Ablations are great.
2. The first 4 pages of writing (intro, related work, background) are clear.
3. The experimental hypotheses are very clear (H1/H2/H3) and reasonable
4. Evaluation environments (KeyCorridor of MiniGrid) make sense and is commonly used.

Weaknesses:
1. I can only vaguely understand CAM. The way it's being described is still very confusing to me, and I already have some background on speaker-listener models. I recommend the authors considering rewriting with general audience in mind -- maybe present an algorithm box that shows how it's computed? Currently this section is interleaved with intuitions and actual procedure. Maybe separate them to some extent? (I see Appendix F/G is about agent architecture and RG. Would you guys consider condense the paper and move these two sections into main text?)
2. Sec 3.2 is very brief.
3. Experiment figures are labeled in a way that is beyond confusing. I would urge the authors to not use `Agnostic STGS-Lazlmpa-5-1 ELA+AccThresh=90+Distr=256+UnifDSS` as labels in their figure. Such label is fine for internal presentations/reports, but it is difficult for reviewers to quickly understand what the figure is saying.

My main concern of this paper is not about the content nor the experiment, just about the presentation.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
dQmEIwRw16;"REVIEW 
Summary:
This paper studies the loss function for soft class labels and entropy-based clustering. In particular, it introduces a new loss function called 'collision cross-entropy' as an alternative to Shannon's cross-entropy when class labels are represented by soft categorical distributions. The motivation for this new loss function is to handle ambiguous targets/labels in classification. The authors provide an EM algorithm for pseudo-label estimation and conduct experiments to demonstrate that this approach leads to improvements in classification accuracy when models are trained with soft, uncertain targets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The proposed collision cross-entropy may have advantages over Shannon's cross-entropy when handling soft labels in certain scenarios.
- Through experiments, the authors demonstrate that the proposed method achieves better robustness to label uncertainty, which is important for self-labeled clustering methods.

Weaknesses:
- [**Theory-1**] The main contribution of this paper is proposing the new loss function 'collision cross-entropy'. However, there is not much theoretical analysis about this loss function. From the current paper presentation, the Eqn. (9) can be interpreted as a modified version (or inspired by) Eqn. (6). For example, by minimizing the new objective for learning linear models, could this new loss lead to the right linear classification model?

- [**Theory-2**] For the EM algorithm, is there any convergence analysis for the EM algorithm proposed in this paper? 

- [**Experiments**] State-of-the-art for comparison. The methods for comparison in Table 1/2/3 are not very recent. It is possible that the previous methods still work well and be the state-of-the-art. However, I found some recent papers could achieve much better results, for example, the ACC on CIFAR10 of [DTC+2023] is 92%+, however the result in this paper is <84%.


[DTC+2023] Unsupervised Manifold Linearizing and Clustering. Tianjiao Ding, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, Benjamin D. Haeffele. ICCV 2023.

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Soft labels are often used to represent ambiguous/noisy/uncertain targets in classification, particularly in self-labelled clustering, where pseudo-labels are estimated together with model parameters.
The authors propose an alternative to Shannon cross-entropy for a loss term, called the collision probability. 
This term arises as a limiting case of a Renyi entropy, or as a probability that two random variables are equal.
The collision cross entropy admits several advantageous properties: it is robust to large deviations in the target data, it agrees with Shannon cross-entropy for one-hot labels, it is symmetric, and points that are labelled as uniform distribution have no contribution to training.
The authors provide an EM algorithm for pseudo-label estimation and show state of the art results.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- The paper is **very well written**, providing strong intuition and flowing prose. The intuition in Figure 1 is helpful, especially in showing that the proposed measure is robust to large target errors. 
- The main technical element appears to be an EM algorithm for solving the clustering problem obtained by using a collision cross-entropy in place of the Shannon entropy (which a swap in the arguments between equations (10) and (11)). This algorithm appears to be **technically sound**, and guarantees convergence of the subproblem in the M step. 
- The proposed term has **several nice properties** (as I mentioned earlier). Tt is robust to large deviations in the target data, it agrees with Shannon cross-entropy for one-hot labels, it is symmetric, and points that are labelled as uniform distribution have no contribution to training.

Weaknesses:
- My main concern is that **conceptually, the contributions are rather limited**. Generalisations of entropy are well-known, and as far as I understand (correct me if I am wrong), the main contribution is that authors use a different measure of entropy to Shannon entropy inside existing formulations. This leads the authors to investigate EM-algorithm and empirical performances, but as the paper is currently written (see further comments below), I cannot see whether these EM-algorithm and empirical performance benefits are actually real and beneficial. I also do not understand why this particular notion of entropy was used, compared with the other spectra of entropies. 
- An incomplete review of relevant generalized formulations of entropy is provided. This is not a weakness per se, however **perhaps the title in section 2.2 could be changed to something like Renyi Entropy**. Similarly tone down the discussion of generalised entropy measures throughout the paper. Alternatively, the authors might consider expanding their discussion and including more well-known entropy measures. For example, see section 8 and 11 of [1].
- The bold numbers in table 2 require clarification. The caption doesn't mention the number of trials (however the text mentions 6 trials). Compared with MIGD, excluding MNIST, due to the high variance in the trials, the results do not appear to be **statistically significant**. Perhaps the authors could consider running more trials and performing a significance test, and/or also bold relevant entries in MIGD. 
- As above for Table 3, 4 and 5. 
- It is **not clear how long the method takes to run** compared with competitors. Does the EM algorithm outperform a naive marginalisation of the log likelihood (using e.g. MC), both in terms of time and in terms of predictive performance? 
- Related to the above, is the reason for specialising on $\alpha \to 1$ because it allows for the EM algorithm? If you consider other values of $\alpha$, how do the results compare in terms of time and performance. Or is this setting intractable?


[1] Generalized Thermostatistics, Jan Naudts, 2011.


Minor:
- The text in the tables is too small to read without zooming in a lot.
- Recommend less active tense in the abstract: ""In case of soft labels y, Shannon’s CE teaches the model predictions σ to reproduce the uncertainty in each training example"" could be ""In case of soft labels y, Shannon’s CE results in model predictions σ which reproduce the uncertainty in each training example"".

Limitations:
The author checklist appears to be incomplete. The authors answer NA to ""Does the paper discuss the limitations of the work performed by the authors?"", without a justification. I do see a small discussion around local minima and numerical instability towards the end of section 4, but I think these could be further elaborated on.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the concept of collision cross-entropy (CCE) as an alternative to Shannon's cross-entropy (SCE) for self-labeling in the context of unsupervised and semi-supervised learning. The primary motivation is to address the limitations of SCE, especially its sensitivity to label noise and uncertainty. CCE aims to enhance robustness to such uncertainties by defining a probabilistic interpretation that encourages collision events between predicted and true distributions. The paper provides theoretical foundations, describes an EM algorithm for efficient optimization, and presents experimental results demonstrating the superior performance of CCE over SCE on the task of deep clustering.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality
-The paper introduces a novel loss function, the collision cross-entropy, which is well-motivated by the need to handle soft and uncertain labels in classification tasks, particularly in self-labeled clustering. The idea of maximizing the collision probability is distinct from the traditional approach of minimizing the (implicit) KL divergence between distributions.

Quality
-The paper provides a solid theoretical foundation for the collision cross-entropy, including its properties and relationship to other entropy measures. The derivation of an efficient EM algorithm for pseudo-label estimation further strengthens the paper's technical contribution.

Clarity 
- The paper is generally well-written and organized. The motivation, theoretical analysis, and experimental results are presented clearly. The authors provide sufficient details for an expert reader to understand and potentially reproduce the work.

Significance
- The proposed collision cross-entropy has the potential to be a valuable tool for handling soft and uncertain labels in various machine learning tasks.

Weaknesses:
Quality
- The superiority of CCE seems to hinge on making the model capture the same ""decisions"" as the target distribution, without forcing the model to capture the entirety of the distribution, as well as de-weighting target distributions which are not spiky. While the properties of the loss are clear, it is not self-evident to me that the properties *of the loss function* translate into necessarily *better properties for models*, both as a function for training a classification model directly or for clustering. 
- In addition, the experiments were conducted on fairly old architectures (VGG, ResNet) and small datasets. Often improvements on small datasets do not translate into improvements on larger-scale models. I would encourage the authors to examine for full imagenet dataset at the very least. This also open up the capability to look at various robustness / calibration properties of the models on the various corrupted forms of ImageNet. 

Clarity
- Certain sections, the task to which this method is applied and the desired model properties for the task could be more clearly explained. It took me a while to get my head around the deep clustering task which the authors are solving. 

Significance
- The impact of CCE on real-world applications beyond the presented datasets and tasks could be further elaborated. This notion that CCE is better for noisy pseudo labels immediately suggests to me examining it as a loss function for doing distillation / noisy teacher-student training of a model on a pseudo-labelled corpus of data, however, I didn't see any links to the area of distillation / teacher-student training within this paper.
- The significance would be bolstered by demonstrating CCE's performance on larger scale, more diverse and challenging datasets.

Limitations:
**Strengths:**
- The paper acknowledges the need for robustness to label noise and addresses this effectively through CCE.
- The paper briefly mentions the potential increase in privacy disclosure risk with larger synthetic datasets but does not elaborate on this limitation or discuss potential mitigation strategies. It would be beneficial to include a more detailed discussion of the privacy implications of the proposed method and any potential negative societal impacts.

**Weaknesses:**
- The discussion on limitations could be more explicit, particularly regarding any assumptions made and potential edge cases where CCE may not perform optimally.
- The experimental evaluation uses very old model architectures (VGG, ResNet) and small datasets (CIFAR-10, CIFAR-100, MNIST, STL-10) which feature images only as large as 96x96 pixels. I would be curious whether the advantages of this method translate to high-dimensional image data with more classes, and on more modern, transformer-based  architectures (eg: ViT) .
- Similarly, could the author's consider extending this approach to deal with pseudo-labelled sequence data, for language models or for translation models, for example?

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on the choice of the loss function in problems with soft distributions of the labels, in particular in the context of pseudo-labeling for unsupervised or self-supervised problems such as clustering. In sections 1-2 the paper gives a thorough review of existing practices and relevant theoretical research. In section 3 the paper proposes a new collision cross-entropy loss as a replacement of the standard Shannon loss, and discusses various aspects of this new loss. In section 4 the paper proposes a new EM algorithm for pseudo-label estimation in connection with the new loss. Finally, in Section 5 the new algorithm is experimentally compared with existing ones and is shown to outperform them.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is generally well-written. Sections 1-3 contain a thorough discussion of entropies and losses suitable for self-labeled clustering, with abundant references. The main point of the paper, the new collision cross-entropy loss, is well explained and motivated. The paper provides an experimental comparison of the proposed algorithm with alternatives and shows its significant advantage.

Weaknesses:
I'm confused by mixing the discussion of losses and EM algorithms in section 4. The bulk of the paper is focused exclusively on the advantages of the proposed new collision cross-entropy loss. The main claim in the abstract and introduction is that the proposed loss is better than the standard Shannon loss. The EM algorithm is mentioned only in the last line of abstract, as if in passing. However, the experimental comparison in section 5 obviously crucially depends on the EM algorithm proposed in section 4. How can we tell if the experimentally demonstrated advantage is due to the new loss or the EM algorithm? Since the main claim is about the superiority of the loss, why not just take any existing soft-labeled clustering algorithms and replace the standard Shannon loss by the proposed new loss? In my opinion, the lack of such a direct comparison substantially weakens the main claim of the paper. The advantage shown experimentally is good, but the conceptual takeaway may be misleading.

I found section 4 on the EM algorithm harder to read relative to the other sections (in fact, I'm not familiar with such algorithms and not even sure what EM stands for - apparently Expectation-Maximization, but this acronym is not explained in the paper). In constrast to the other sections, this one seems to assume familiarity of the reader with related algorithms. I didn't understand, for example, how equation (14) (E-step) was derived.

Another weakness I see is that the strongest results of the paper are largely experimental (not counting general arguments and auxiliary theoretical constructions in Section 4), but, as far as I understand, they are not easily verifiable since the code is not open-sourced.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cgb0Tn4uHy;"REVIEW 
Summary:
This paper introduces a method that supplements the traditional estimate of a class-dependent transition matrix, which is popular in label-noise learning. Traditional transition matrix methods are less effective for instance-dependent noise. To overcome the limitation, the proposed method adds a residual term such that it can extend the projection of a class-dependent T on label predictions to fit the true one as if we have an instance-dependent T. Theoretical analyses of the algorithm confirm its convergence and generalization properties under specific assumptions. Experimental results on various synthetic and real-world noisy datasets such as CIFAR-N and Clothing1M show the performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The performance is eye-catching.
2. The method is proposed with both theoretical analyses and experimental results.

Weaknesses:
1. The intuition of the proposed residual is not clear. For example, why a sparse structure is preferable in this problem? Why do u and v enable a sparse structure? Why is a Hadamard product employed? Why not simply use a vector u?
2. The theoretical part of the main paper is heavy but the outcome is not convincing. Specifically, there is a huge gap between Eq. (17) and Theorem 3.1.
3. The assumption in Eq. (7) is too strong.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem of learning with noisy labels. To handle the instance-dependent noise, the authors propose an extended model for transition matrix-based methods. Specifically, their model combines a class-dependent transition matrix with a sparse implicit regularization term. The authors provide a theoretical analysis of the proposed method. Experiments conducted on both synthetic and real-world noisy label datasets verify the effectiveness of their method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Theoretical analysis of the convergence and generalization are provided.
2. Experiments are conducted thoroughly, including experiments on synthetic and real-world datasets. The ablation study is also conducted.

Weaknesses:
1. The method proposed in this paper appears to be a straightforward combination of VolMinNet and SOP.
2. The experimental results for TMR are missing for the CIFAR-N, Clothing1M, and WebVision datasets.
3. An important baseline, CCR [1], which is the state-of-the-art among transition matrix-based methods, is absent.
4. The paper lacks an analysis of the estimation error of the transition matrix. It would be beneficial to compare the estimation errors of the transition matrix for TMR against those of other baselines.

**Reference**

[1] Cheng, De, et al. ""Class-dependent label-noise learning with cycle-consistency regularization."" *Advances in Neural Information Processing Systems* 35 (2022): 11104-11116.

Limitations:
I did not find that the authors have discussed the limitations and potential negative societal impact of their work. To improve the paper, the authors can provide a thorough analysis of the limitations of their method in an independent section. For example, scenarios where the method might not perform well can be included.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In learning from noisy labels, existing methods generally focus on class-dependent (but instance-independent) noise that can be modeled by a transition matrix $\mathbf{T}$. Some methods have also been proposed for instance-dependent noise (modeled by $\mathbf{T}(x)$). This work belongs to the latter. In particular, it proposes to implicitly model $\mathbf{T}(x)$ using an extended model based on the transition matrix $\mathbf{T}$ and a residual term $\mathbf{r}(x)$. Some theoretical properties (e.g. convergence and generalization) of the proposed algorithm (TMR) are analyzed under certain conditions. Experiments show that the proposed algorithm outperforms baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
**Originality**

The paper studies the challenging problem of instance-dependent label noise, which is less addressed in the literature compared to class-dependent noise. The proposed extended model for transition matrix, which is a combination of a transition matrix with residual terms, seems novel and effective. Related work is adequately cited.

**Quality**

The experiments are quite comprehensive. The paper compares the proposed method with multiple methods (including some state-of-the-art ones) on various datasets. The experimental results show that the proposed method outperforms all those baselines. Some theoretical properties (e.g. convergence and generalization) of the proposed algorithm are also analyzed under certain conditions.

**Clarity**

The description of the proposed method is clear. The experiment section is generally clearly written and well-organized.

**Significance**

The proposed method shows significant improvements compared with various baselines. Therefore, it has the potential to be adopted by other researchers and practitioners, advancing the state of the art in learning from noisy labels.

Weaknesses:
**Originality**

- In Lines 120-125, the residual term $\mathbf{r}(x)$ is introduced. However, it is not clear to me how novel it is compared to the previous work [57,25,30,31]. The authors should elaborate on this point.
- I can see why residual term $\mathbf{r}(x)$ might be useful, but why is it modeled as in the form in Line 124? The motivation should be explained.

**Quality**

- The convergence analysis seems very restrictive to me because it requires too many assumptions (Lines 171-175, Lines 183-186, and Appendix B.2).
- The generalization analysis (Theorem 3.2) is w.r.t. the training loss (surrogate loss) under the noisy distribution $\tilde{\mathbb D}$, but the test accuracy under the clean distribution $\mathbb D$ is what people really care about. Is it possible to prove any consistency guarantees?
- Knowledge of the ground truth $R_*$ is required to derive Theorem 3.2, but we do not know $R_*$ in practice.
- Section 3 is not clearly written, and I found it hard to follow and assess its correctness (see below).

**Clarity**

Section 3 is not clearly written, and I found it hard to follow and assess its correctness. Specifically:

- In Lines 173-174, is $R_{\ast}$ assumed to be $U_{\ast} \odot U_{\ast} - V_{\ast} \odot V_{\ast}$?
- In Lines 203-205, $\mathcal F$ is a set of loss functions. What is the exact meaning of ""about the data""? Why is $R$ not considered in $\mathcal F$? Is a fixed $R$ being used here?
- In Lines 206-207, what is the definition of $\epsilon$-cover?
- In Lines 207-208, what are the mathematical definitions of the ""average losses""?
- In Lines 210-213, it seems that here $R_{\ast}$ is fixed. Yet, it does not make sense to me because $R$ should depend on the transition matrix $T$ and the distributions $\mathbb D$ and $\tilde{\mathbb D}$. What is ""ground truth"" w.r.t. here?

**Significance**

The significance of the proposed method could be further enhanced through a more rigorous theoretical analysis (see above).

Limitations:
I did not see where the authors discussed the limitations of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In noisy label learning problem, noise is often characterized by confusion matrix. In contrast to instance-independent noise, this work considers a setting where confusion matrices could be different for different samples. Under this setting, the authors proposed to use a global confusion matrix shared by all instances and a residual term for each instance to account for the different between instance-dependent confusion matrix and the global confusion matrix. For learning, an MLE loss combined with an implicit sparsity regularizer is optimized.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The work is tackling a challenging yet important setting in noisy label learning. The proposed model is a natural and intuitive extension to instance-independent confusion matrix as it allows for a wider range of noise. The proposed algorithm (TMR) is simple to implement, and demonstrated to be effective under synthetic and real-data experiments.

Weaknesses:
- Motivation for the use of sparsity regularizer is not clear. The authors does not discuss much on why the vector $\textbf{r}$, or matrix $\textbf{R}$ in their model should be sparse. They did point out in page 3, line 117 that the difference when using the global transition matrix and the instance-dependent transition matrix should be small. However, that is not sufficient to promote sparsity, as any other $l$-p ($l>1$) norm could have promoted that goal.
- The use of implicit regularizer is also not clear. And more importantly, since the output of $\textbf{T}^T P(\textbf{Y} | X) + \textbf{r}(X)$ is a probability vector, $\textbf{r}$(X) has to satisfy certain constraints. This is not discussed nor specified anywhere in the paper. And hence it is questionable how the parameterization of $\textbf{r}(X)$ could produce valid probability vector $\textbf{T}^T P(\textbf{Y} | X) + \textbf{r}(X)$.
- The analysis might contain flaw. Equation (14) is incorrect: $\widetilde{\textbf{Y}}$ is a matrix composing of one-hot vectors while the RHS is a matrix composing of probability vectors. The two are not equal in general. This equation seems to be the key step to motivate the objective to be analyzed in (17), and also the key step in the proof of Theorem 3.1 (page 15, line 534). 
- The analysis is based on linear model which is not very realistic.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cVbd7uTvD8;"REVIEW 
Summary:
This paper (SC3D) proposes a single image-to-3D reconstruction method. It combines multi-view diffusion model and a 3D reconstruction model, and uses the 3D reconstruction results as self condition to improve the multi-view generation process. The motivation of the proposed method is to improve the geometric consistency previous single image reconstruction pipeline, namely first generate multi-view images then perform sparse view reconstruction. The core idea proposed in the paper, 3D-aware feedback, is reasonable and also appear in concurrent works IM-3D and VideoMV. Several ablation studies need to be included to prove that the proposed 3D-feedback (including RGB and coordinate maps) are improving the reconstruction quality. Authors also need to justify more about the contribution w.r.t. related work VideoMV. Furthermore, there is still space to improve the readability in the submission.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Major:
- The idea of using 3D reconstruction rendering as condition to improve the geometry consistency of multi-view diffusion models is reasonable.
- The experiments are comprehensive. The results demonstrate that the proposed feedback mechanism is solid in the multi-view reconstruction approach.

Weaknesses:
- Claim about key contributions: the 3D-feedback idea appears already in VideoMV. Since the VideoMV is already available on Arxiv in March, authors need to justify more clearly about the difference and contribution w.r.t. VideoMV.
- lacks generalization results: the method is evaluated on google scan objects, which is standard. However, i am curious to see if the approach generalizes to real world images.
- lacks one ablation: SVD+RGBs feedback, which is missing in Tab. 2 and Tab. 3.
- the readability of the Alg.1 and Alg.2 can be improved. Currently it is too specific and looks like python program. A more abstract algorithm is expected in a scientific paper.
- A typo: in line 213, after comma, ""we"" instead of ""We"" (wrong capitalized ""W"")

Limitations:
- No obvious limitations are found in the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for 3D asset generation conditioned on a single image. The approach follows the recent trend of a two-stage feed-forward model – first generating multi-view images and then using a sparse-view reconstructor to reconstruct the 3D object (specifically, LGM in this paper). This two-stage model has a significant drawback: the inconsistency of the multi-view generation model may result in an imperfect input for the reconstructor, thus causing quality degradation of the final generated 3D assets.

To address this issue, the authors propose adding a 3D-aware feedback mechanism to improve multi-view consistency and enhance the final reconstructed results. Specifically, a self-conditioned mechanism is introduced, where the output of the reconstruction model is fed into the diffusion model. This output is involved in the diffusion process, leading to better 3D consistency.

Overall, the method seems sound to me.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The problem definition and the motivation for the project are very clear.

(2) The paper is well-written and easy to understand.

(3) The method seems sound. By adding the rendering results of a reconstructor as input, which present strong multi-view consistency, the diffusion model is also capable of generating multi-view-consistent images.

(4) Table 3 appears reasonable and as expected.

(5) The appendix provides helpful details on training and network architectures, aiding in the reproduction of the results.

Weaknesses:
(1) Some related works lack citation and discussion:
(a) In ""Dmv3d: Denoising Multi-View Diffusion Using 3D Large Reconstruction Model"" [ICLR 2024], the paper uses a similar mechanism (though not entirely the same) by employing a 3D reconstructor as a multi-view image denoiser.
(b) “Carve3D: Improving Multi-View Reconstruction Consistency for Diffusion Models with RL Finetuning” [CVPR 2024] enhances multi-view consistency through RL fine-tuning.

(2) I encourage the authors to provide more visual results to help readers understand and appreciate the diffusion/reconstruction process. For example, could the authors provide some visual results of $\tilde{x}_0$ at different denoising steps?

(3) In the comparisons, although quantitative results are provided, could the authors include some qualitative (visual) comparisons to the baseline methods?

Other minor issues:

(1)	Line 118, The Plucker coordinate should be (d, o x d)

(2)	Line 206, We -> we

(3)	Line 225, meshe -> meshes

Limitations:
(1)	The method is limited to object-level reconstruction with a clean background. Though this is a common limitation in recent related works, I encourage the authors to explore this issue in future work.

(2)	As discussed in the paper, extracting high-quality surface geometry from the Gaussian model remains an open problem. This is an interesting topic for future research.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper observes that the current state-of-the-art image-to-3D generation models consist of two separate parts: generate multi-view images from a single image and run on top the 3D reconstruction. This process has no feedback loop, i.e. the reconstruction does not inform the image generation which in turn leads to a worse quality of reconstruction. They propose a method that builds in a feedback to loop back the feedback of the reconstruction into the diffusion process. They report superior 3D reconstruction quality over the usual two separate step method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The overall idea that drives the paper to provide a link between 3D reconstruction and diffusion at training and inference time is very powerful and novel, and has not been explored in existing text-to-3D papers. I think this is a significant asset of the paper in a crowded area.
2. The results presented seem to improve quite a bit over the existing state-of-the-art for the results shown.

Weaknesses:
1. The presentation of the paper is not clear. 
    - In Fig. 1 the paper is describing an iterative process. Fig. 1 has also no output, but suggests the 3D representation is the output. However, Fig. 2 suggests the (multi-view) images are the final output? Are the two decoders the same? In the paper you are referring to different models G and F. They are not mapped to the figure to get a better picture.
    - Lines 169-172: this is describing the training strategy. That should be moved to the part starting from line 180 where you are actually describing the training strategy.
    - Equation 1: c_skip is not explained
    - The paper has many typos. Especially in part 4, they appear in almost every paragraph.
    - Lines 227-228: This statement seems contradicting: “Directly employing a NeRF-based feed-forward model during the training process significantly reduces training speed due to the computational demands of volumetric rendering.”
    - Replacing the algorithm code with more concise pseudo code may make it much easier for more readers to understand.
2. Comparisons are not very comprehensive
    - None of the methods in Figure 1 are qualitatively compared. 
    - In Figure 3, it seems different views are compared in the first and second column
    - Minor: It would be also really helpful to introduce some visual cues into figure 9 to easier grasp the results.
3. Some claims are not justified
    - Is the section on augmenting the diffusion model with camera control in 3.1 a claimed contribution of the paper? The statement that: “This approach allows for more detailed and accurate 3D rendering, as pixel-specific embedding enhances the model’s ability to handle complex variations in depth and perspective across the video frames.” Is not justified at all, as other types of embeddings are not ablated.

Limitations:
Overall the paper does describe some limitations of the method, but it’s not clear if they are relevant. For example, is using the gaussian splatting method really a limitation in this case? I’d be interested to know how long this method takes (is it much slower than LGM baseline), how computationally intensive it is, or how sensitive it is to the initial generation by the multi-view diffusion model.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes SC3D for the single-image-to-3D generation, which integrates the diffusion-based multi-view generation and Gaussians-based 3D reconstruction through a self-conditioning mechanism. Specifically, during each denoising step, SC3D injects the rendered image and geometric map from the reconstruction model into the denoising process to enhance the multi-view consistency of the multi-view generated images. Experiments on GSO dataset demonstrates its superiority over existing methods mentioned in this paper.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. SC3D integrates multi-view image generation and 3D reconstruction into a single framework, ensuring similar data distribution between the two modules and thereby improving reconstruction quality during the reference process.

2. SC3D proposes a self-conditiond 3D-award feedback mechanism to bridge the multi-view image generation and 3D reconstruction, in which the rendered images and geometric map are injected in the multi-view generation network. Such design makes sense and could improve the consistency of the generated results from the multi-view generation network.

Weaknesses:
1. Lack detailed visual comparisons with baseline methods. The authors only compare SC3D with LGM but do not show results generated from other baselines, making the visual comparison results less convincing.

2. The paper suffers from poor organization. For example, Figure 4 and Figure 5 are not referenced anywhere in the text. The purpose of Figure 6 is confusing, as its caption suggests it shows results from another work, and it is difficult to discern differences among the three rows. Additionally, the paper's typesetting is of poor quality. There are many blank spaces in the text.

Limitations:
Please refer the weaknesses and questions above.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
cdYIL6OQr6;"REVIEW 
Summary:
This paper introduces a novel mixture of experts model that applies local differential privacy to the gating mechanism. Their methods leverages the one-out-of-n gating mechanism and provides specific generalization bounds.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The overall insight of the paper is clear and strong.
2. Improve the tightness of bounds on the risk for mixtures of experts models.
3. Reduce complexity by relying on fewer parameters.

Weaknesses:
1. The section 3 talks about PAC-Bayesian bounds for mixtures of experts, but it lacks insight into why PAC-Bayesian bounds are applied instead of other bounds. More explanation is needed here, similar to the explanation needed in section 4 regarding Rademacher bounds.

2. In the experiment section, it is unclear why the chosen dataset is used for the experiments and why only 5 epsilon values were selected.

3. Even though there are very few existing guarantees, the experiment should include other methods as baselines and compare the results.
4. For the experiment section, only consider mixtures of n linear experts in binary classification tasks seems easy. Need to add other classification tasks.
5. There is no description about the datasets used in experiments.

Limitations:
1. Lack many reference as I motioned in questions part. 
2. The paper lacks a smooth flow, making it difficult to follow. Specifically, there is no clear insight or reasoning provided to explain why the existing mechanisms or bounds were chosen, as highlighted in weakness 1.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach to regularize mixtures of experts by imposing local differential privacy (LDP) on the gating mechanism. The authors provide theoretical justifications and derive PAC-Bayesian and Rademacher bounds tailored to this approach. Experiments conducted on various datasets demonstrate that using LDP as a regularizer improves the generalization ability of the models, especially in cases prone to overfitting. The method offers a balance between leveraging neural networks for gating and maintaining robust theoretical guarantees, making it a valuable contribution to the field of machine learning.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
This paper demonstrates originality by integrating local differential privacy (LDP) into the mixture of experts model, addressing privacy concerns while improving model generalization. The theoretical contributions, including PAC-Bayesian and Rademacher bounds, are rigorously derived and tailored to the new approach. The clarity of exposition makes complex concepts accessible, and the experiments validate the practical benefits of the method. The significance lies in enhancing the robustness and scalability of mixture of experts models, making them more applicable to real-world scenarios prone to overfitting.

Weaknesses:
The primary concern regarding this paper lies in its significance. There have been previous works that incorporated differential privacy (DP) into the construction of mixture of experts models with privacy considerations. This paper, however, utilizes local differential privacy (LDP) to analyze the theoretical aspects of mixture of experts models. The introduction of LDP significantly alters the generalization behavior of these models because both LDP and DP are methods that inherently enhance algorithm robustness, thereby affecting generalization. If the main goal of the paper is to enhance privacy, it is imperative to compare this approach with existing DP-based methods and highlight what specific aspects LDP protects that traditional DP cannot. Without this comparison, the added value of using LDP over existing DP methods remains unclear. On the other hand, if the focus is on analyzing the generalization of mixture of experts models, the paper must justify the rationale behind incorporating LDP for this analysis, as LDP is not inherently required for mixture of experts models. The paper needs to elaborate on why LDP is a suitable and necessary tool for this analysis and how it fundamentally impacts the generalization properties of the models in a meaningful way. Additionally, while the theoretical contributions are substantial, the practical implications need to be demonstrated more robustly through experiments. Comparing the results directly with models using traditional DP methods would strengthen the paper by showing the practical improvements and specific scenarios where LDP outperforms DP. Furthermore, using a broader range of datasets could better illustrate the claimed benefits in robustness and scalability. By addressing these concerns, the paper can more convincingly argue the necessity and advantages of using LDP in mixture of experts models, thereby enhancing its significance in the field.

Limitations:
No.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides generalization bounds for a particular type of mixture of experts (MoE) networks. They focus on MoE architectures where an input $x$ first goes through a gating function $g$, and then gets routed to a single (one out of n) expert $i \in [n]$ according to the $g(x) \in [0,1]^n$ distribution. The final output of the MoE network is the output of the expert $h_i(x)$.

The authors observe that when the gating function $g$ has certain regularization properties, which correspond to local differential privacy (LDP), then the resulting network has better generalization bounds than what appears in the existing MoE litterature. The authors provide such bounds. 

Finally, the authors evaluate LDP-regularized routing on binary classification tasks with mixtures of linear models. The results show that LDP regularization outperforms an un-regularized baseline.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Mixture of experts models are still understudied, especially from a theoretical standpoint, so I appreciate the new analysis provided by this paper. The connection with differential privacy is creative and seems fruitful, although I have some reservations about it (see below). 

The bounds do improve on existing generic bounds for this type of MoE models. The paper is well-written and easy to follow, and the experimental code is available.

Weaknesses:
First, it is worth emphasizing that the MoE networks in this paper do not satisfy local differential privacy themselves. The gating network is not even *trained* with differential privacy. This paper only uses local differential privacy as a regularization condition on the *gating network only* at *inference* time, which does not provide meaningful privacy guarantees. That is not immediately clear from the title of the paper. To be fair, this work does not try to achieve any privacy goals, and is entirely focused on generalization bounds. But if privacy is not needed, then it is not clear why DP is the right tool for the job. The paper directly uses LDP (it could have been called something like ""exponentially regularized routing"") but does not motivate this choice. Are there other forms of regularization that could achieve similar or better bounds? While there are some known connections between robustness, differential privacy, and generalization, they are not mentioned here. In the context of MoEs, some large models already regularize their gating functions (e.g., the Switch Transformer adds some ""jitter"" noise to the routing logits). 

Next, the paper motivates the study of MoE models by mentioning recent progress with LLMs such as the Switch Transformer, which uses multiple layers of experts (deep MoE) and combines the output of different experts. All the modern LLM MoE models I am aware of are such deep MoEs. Meanwhile, the paper focuses on simple shallow MoE models with a single gating network followed a single layer of experts, which limits the potential impact of the paper in my opinion. While theoretical bounds may be of interest even on shallow MoE models, I would appreciate at least some discussion about whether the authors' approach can generalize to deeper models. 

Finally, the experiments have some limitations, which mostly stem from the two previous concerns. 
* The authors only evaluate a single, rather simplistic (3-layer MLP gating network followed by linear experts), MoE architecture. More concerningly, they use a fixed number of experts (n = 100), thereby missing an opportunity to evaluate their claim that ""we can have many more experts with almost no penalty from the theoretical point of view"".
* The only baseline is ""No LDP"", which I think is a quite weak baseline. It is not entirely surprising that adding some regularization, in the form of LDP routing, improves generalization compared to a completely un-regularized baseline. How about other forms of regularization, such as dropout, clipping, or jitter noise (which already exists in the context of MoEs)?
* Another baseline would be a non-MoE model, with a comparable number of parameters, e.g., even a simple, dense, multi-layer perceptron. Showing that shallow MoEs outperform dense models would alleviate concerns about the practical relevance of this work.

Minor comments:
* Table 1 might be more readable as a graph.
* It is quite surprising to see MNIST being qualified as a ""large"" dataset, for which a 4-layer network takes 3 hours to train on a GPU, in 2024.
* Also, it is unclear why MNIST has to be broken down into 3 binary classification tasks.

Limitations:
The authors adequately addressed the limitation they identified (difficulty of tuning epsilon), even though this is not the main limitation of this work in my opinion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the mixtures of experts models, in particular the one-out-of-n gating mechanism for ease of theoretical analysis, and show that applying a soft-max, which is also the exponential mechanism, on the gating mechanism gives LDP and can improve generalization. The privacy techniques are largely the same as previous work, PATE, but specifically applied to mixtures of experts. The authors then provide theoretical analysis showing generalization bounds for this approach.


Unfortunately, I’m not familiar enough with the mixture of experts literature to evaluate the novelty of applying the soft-max and the corresponding theoretical guarantees. I am rather surprised though that the soft-max has never been applied and am still somewhat confused upon what the previous techniques were in the one-out-of-n mixtures of experts.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors show that choosing an expert through the soft-max provides better generalization. The privacy guarantee is then essentially proportional to the regularization factor (\beta) for the soft-max application. Further they give theoretical generalization bounds for this approach.

Weaknesses:
Unless I am mistaken (please correct me if I’m wrong) the authors are not providing privacy guarantees for the model itself, but only one inference call to the model. In particular, if feature vector x is input to the model, then the expert is chosen randomly according to the soft-max / exponential mechanism, which is \epsilon-LDP. If inference was then run again, suppose even on the same feature vector, then the random draw from experts would occur again. This is known as composition in the privacy literature, and the privacy guarantees	would now be 2*\epsilon. 

Providing privacy guarantees on only one inference call to a model is both not very useful nor interesting due to the composition properties.

Limitations:
See weaknesses section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors  take the first step (I though so at first) to MoE under LDP theoretically.  I read again and found that the author seems to have raised the utility lower bound of existing studies. Few experiments could be found. Perhaps, I am not an expert in MoE, but it really leave a  hard time.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Important poblem.

Weaknesses:
1. Perhaps I am not an expert in MoE, and I cannot tell from the author's introduction that there are any challenges. 
2. The experimental results and application scenarios are not clear.

Limitations:
see weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
cRs4jvF4mO;"REVIEW 
Summary:
This paper proposes new methods, Kernel Density Forest (KDF) and Kernel Density Network (KDN), to address issues in confidence calibration for traditional deep learning models and random forests. The motivation stems from the existing literature that deep neural networks using ReLU tend to exhibit high confidence on out-of-distribution (OOD) data due to affine transformations. The proposed methods improve confidence calibration for both in-distribution (ID) and OOD data by partitioning the feature space into polytopes and replacing affine functions within each polytope with Gaussian kernels. Experimental results demonstrate that the proposed methods outperform existing techniques in terms of calibration performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality:
The approach of replacing affine functions within polytopes with Gaussian kernels is novel. The proposed methods address the confidence calibration problem for both ID and OOD data simultaneously, providing an integrated solution to these calibration issues.

Quality:
The theoretical proofs are robust, and the effectiveness of the proposed methods is validated through both simulations and real-world datasets.

Clarity:
The paper is written clearly and concisely.

Weaknesses:
Validity of Metrics:
The paper evaluates calibration using Maximum Calibration Error (MCE) for ID data, but does not justify the use of MCE over Expected Calibration Error (ECE) or Adaptive Calibration Error (ACE)[1]. A more detailed explanation and comparison of these metrics would enhance the paper's credibility. Additionally, the definition and justification for OCE (Out-of-distribution Calibration Error) would benefit from a similar comparison with ACE.

[1] https://arxiv.org/abs/1904.01685

Experiments:
To emphasize the effectiveness of the proposed methods, a comparison of execution times would be beneficial, especially since practical applications like web Click-Through Rate (CTR) estimation place significant importance on runtime. The paper should clarify what the noise in Table 1 represents. It would also be advantageous to include experiments on larger and more varied datasets, as well as an evaluation of the methods' performance when combined with in-training calibration methods, which are commonly used alongside post-hoc calibration methods.

Limitations:
This paper mentions computational complexity and limitations in practical applications, but lacks detailed experimental results to support these claims. Including such data would provide valuable insights for future research and implementation.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel approach for OOD detection by learning a posterior distribution that is calibrated for both ID and OOD individuals. It models the class-wise conditional distribution of features by a gaussian kernel respectively for a set of polytopes that cover the feature space. The tail property of gaussian kernels contribute to both ID and OOD calibration. Empirical evidence shows the power of the proposed algorithm across tabular and vision dataset under both ID and OOD settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well motivated from the tradeoff of ID calibration and OOD calibration for current approaches for OOD detection methods. The technique of gaussian kernel has a clear geometric intuitive. Compared to affine functions, the tail property ensures that the posterior distribution converges to the prior of labels when a OOD sample deviates far enough from the training support, as proved in Proposition 2. On the other hand, the interpolation by gaussian kernels between neighboring polytopes contributes to ID calibration.

Weaknesses:
The major concern is insufficient discussion over the research context of the paper, which renders it hard to precisely evaluate the contribution. The related work section is short. Section 2 shows that ""OOD detection"" is the closest area to this paper, but this keyword is totally absent from the introduction, where the research area is named ""OOD confidence calibration"". What is the relation between OOD detection and OOD confidence calibration? 
The introduction also reveals two potential approaches for this area: discriminative and generative methods. There are also two settings: ID and OOD confidence calibration. The readers might expect to review current progress for all those categories in the related work section.

Limitations:
The author has addressed limitations of their work in terms of sample complexity.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a way to calibrate ReLU networks or random forests by breaking them down into piecewise linear functions on polytopes and replacing the linear parts with Gaussian kernels. This approximation allows to naturally calibrate the models for the ID domain, where confidence will be high due to the density of ID samples that translates into high kernel values, and for the OOD domain, where confidence will be low due to the large distance to ID samples.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The method is novel and mathematically grounded
- The presentation is clear
- The benchmarks are OK

Weaknesses:
The main weakness I find is about the computational time of the method. The number of polytopes scales exponentially with the number of neurons, so I am concerned with the applicability of the method to large (or even medium-scale) neural networks. What is the computational cost of the method for the considered benchmarks, in terms of runtime?

The toy simulations are unnecessarily tedious to grasp and take up a lot of space. I do not say that they are complex, but they hinder the reading flow and do not bring much to the presentation. I would advise putting some of them in the appendix to leave more space for other explanations. Indeed, Section 5 is difficult to read (many ""chunk"" paragraphs with mathematical notations) and would benefit from more structured writing and more flow.

Limitations:
The authors have adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cO1llRY2Br;"REVIEW 
Summary:
This paper focuses on model editing at a low cost. Evidence suggests that modules carrying knowledge in a Transformer module are primarily the MLP blocks. Therefore, the authors propose a method, namely iReVa, to initialize and retrofit key-value pairs into MLP blocks in a Transformer for explicitly inserting new knowledge. Specifically, they insert new neurons in the MLP blocks for each piece of knowledge. Each neuron is initialized with the embedded key and value derived from the input-output pair, respectively. To prevent dramatic change to the irrelevant knowledge, iReVa further retrofits the key and value by fine-tuning with multiple objectives. Compared to the existing methods such as MEND, ROME, MEMIT, and MELO, iReVa reveals better interpretability and stronger capacity for carrying traceable edits. The experiments on zsRE-10K and PARAREL-10K datasets reveal that iReVa has superior performance regarding edit success, generalization, and specificity. Further edit withdrawal test indicates that iReVa can explicitly manipulate the activation of neurons and easily withdraw the edits.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	This paper focuses on modeling editing, which has significant applications in the era of LLMs. It can be applied to alleviate the hallucination issue of LMs and resolve the out-of-date as well as missing knowledge in an LM.
2.	This paper introduces a novel editing method with key-value adaptors for traceable model editing. The proposed method makes sense to me. The initialization with embedded key and value derived from the input-output pair can easily make precise edits to the model. Further retrofitting refines the adaptors to satisfy the task.
3.	For experiments, the author has comprehensively shown the superiority of their method in the perspectives of edit success, generalization, and specificity. And more analyses reveal the generalization of iReVa. Particularly, the edit withdrawal test in Section 6.2 is well-designed, which shows the effect of traceable edits and could provide a potential solution for dynamic knowledge maintenance for LMs.
4.	Overall, this paper is well-written and easy to follow.

Weaknesses:
1.	The discussions on the limitations and broader societal impacts of iReVa are not included in the paper. I have some questions about the application scope of the proposed method. Please see the questions below.

Questions
1.	Could iReVa lead to a dramatically increasing number of parameters? Let’s see if there are millions of knowledge for editing, how can you potentially insert all the knowledge into LMs with iReVa? 
2.	After you change a piece of knowledge, can the reasoning still be conducted for the edited knowledge? For example, if we have edited the president of America, could some reasoning questions like ``Who is the wife of the president of America” also be resolved with the new knowledge?
3.	Typo: ``evident’’ in line 6 should be ``evidence’’. Please check.

Limitations:
No, the author should discuss the limitations of the proposed method such as the application scope, the potential risks, and future improvement to indicate how robust the results are to violate the
assumption. I would like the author to add such information during the rebuttal.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the tracable sequential model editing challenge by plugging in additional model components to a transformer MLP blocks. The proposed approach adds additional model components for each edit, allowing for traceability for each edit.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
•	The results indicate that it is a strong approach compared to relevant literature and its performance is relatively stable when scaling it to thousands of edits.
•	The approach allows for ""separability of each edit"" which in turn allows for additional operations such as edit updation or deletion as showcased in the Edit withdrawal experiment. 
•	The edit withdrawal experiment is both unique and intriguing, as the concept of removing edits appears to be a novel area of exploration.

Weaknesses:
•	The overall approach does not appear to be novel, as it closely resembles T-Patcher. Both iReVa and T-Patcher rely on inserting neurons for editing and using neuron activations to control when to run the patch/adopter. Furthermore, analysis of the editing approach across different layers reveals the same pattern as discussed in the T-Patcher paper which involves adding additional model components in the final layer for optimal results.

•	Experiments with T-Patcher are missing from the comparisons to the existing methods section. Given its similarity, T-Patcher should be included for comparison.

•	Although T-Patcher performs editing in batches, it still uses a single patch of neurons for each edit, making its editing similarly traceable. Thus, the paper's claim of a ""stronger capacity for carrying traceable edits"" seems unfounded.

•	The Edit Withdrawal Test section is hard to understand. How exactly was the experiment conducted? Were all edits removed or only a limited set? Detailed experimentations for this section are needed as it is the only use case of traceability explored in the paper.

•	Editing techniques that rely on code books with playback vectors e.g. GRACE would allow for edits to be removed. The authors should make it clear that the withdrawal test is not possible for the editing techniques that they have chosen for comparison.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces iReVa, a novel method for model editing that explicitly initializes and retrofits key-value pairs into MLP blocks of transformer models to perform CRUD (Create, Read, Update, Delete) operations on LMs. iReVa aims to update knowledge in LMs without damaging irrelevant knowledge, offering better interpretability and traceability compared to existing methods. The method is validated through experiments on GPT series models, showing significant improvements in edit success and generalization without affecting specificity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Provision of the first attempt at conducting knowledge withdrawal tests for model editing methods.

The paper includes a comprehensive analysis of iReVa's performance, including knowledge withdrawal tests and generalization tests.

iReVa's approach to model editing is innovative, focusing on retrofitting key-value adaptors into MLP blocks for traceable model editing

Weaknesses:
This paper could benefit from a more detailed comparison with other model editing methods, especially those focusing on lifelong learning and continual editing [1][2].

It does not discuss the computational efficiency of iReVa in terms of inference time or memory, which is crucial for real-world applications.

The reliance on the hypothesis that factual knowledge is stored in MLP blocks may be limiting [3], and the authors could explore the broader implications of this assumption.

The method's applicability to other types of tasks, such as erasing hallucinations, is not validated.

There is a noticeable absence of experimental validation on other recent and updated models such as GPT-J (used by ROME etc.), LLaMA.

The technical novelty of iReVa is somewhat limited, as it builds upon existing concepts like MEMIT [4] and key-value memory structures in MLPs [2].

The absence of a strategy for selecting the adaptor layer may hinder the method's rapid migration and application to various language models。

Equation 3 requires clarification, why 'i' and 'o' in Equation 3 are both passed through SELF_ATTEN again?

References

[1] Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors, Hartvigsen et al,
Neurips 2023.

[2] Transformer-Patcher: One Mistake worth One Neuron, Huang et al, ICLR 2023.

[3] What does the Knowledge Neuron Thesis Have to do with Knowledge? Niu et al, ICLR 2024

[4] Mass-Editing Memory in a Transformer, Meng et al, ICLR 2023.

Limitations:
No

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel method called iReVa for knowledge editing. iReVa initializes and retrofits key-value pairs into MLP blocks to create a new mapping of knowledge without affecting related information. Compared to existing methods, iReVa offers better interpretability and a stronger ability to make traceable edits.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed methods demonstrate great performance compared to other baselines under the batch editing scenarios.

Weaknesses:
1. The color in the figure is not obvious to discriminate between the original knowledge neurons and new knowledge neurons.
2. The computation of the proposed method is similar to T-Patcher, I'm curious about the difference between them. The proposed methods are designed to tackle the batch edit, but it seems it still needs to add one neuron for each example.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
cEb305kE1V;"REVIEW 
Summary:
The authors introduce the idea of implicit optimization, and coupled feature extraction for images, to achieve robust image registration. I liked the overall idea and was eager to gain insight into how implicit optimization

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I really liked the overall ideas here. 

An implicit optimization layer does indeed address a DL shortcoming that hs been pointed out before (several DL methods show that some fine-tuning of the output deformation field will improve registration for many domains/moels). 

I like the idea of using this in combination with a feature extractor and using those learned features to drive the optimization, with potential benefits down the line. 

The paper is also pretty clear and concise, which I like.

There are extensive experiments, with measures of variance, which is a great start.

Weaknesses:
Unfortunately, I found the rest of the paper (beyond the core idea) lacking and having several weaknesses. 

Importantly, the authors mischaracterize important relevant literature and conceptual ideas, that I think are incorrectly described in the motivation, and not compared to properly in the experiments. This makes it challenging to assess and gain insights from the contribution.

Incorrect characterization: the authors say a few statements that to me seem directly false (and are important to the paper). For example, on line 50 they say ""Moreover, design decisions like sparse keypoint learning for affine registration [103, …] do not facilitate dense deformable registration"" -- they repeat this in several parts of the paper (e.g. line 107). This is wrong -- their first citation, for example, 103, uses keypoints for *deformable* registration (along with affine). A few lines lower they say ""Current DLIR methods are not robust to minor domain shift like varying anisotropy and voxel resolutions, different image acquisition and preprocessing protocols [62, 53, 70, 43]."" -- which is incorrect and not supported by the citations. First, citations 62, 53, 70 are from before 2012 and do not discuss DLIR methods at all (but just general registration), whereas citation 43 *is explicitly tackling and achieves robustness to domain shift*. It may well be that the authors' method does better (I am not sure, see below), but the claim is incorrect. Many other papers tackle distribution shift in DL registration -- see Mok et al, 2023. Another crucial omission is related to the authors' claim that DL methods may not output local minima results -- which is true, but plenty of works propose to take the output of neural networks and perform a bit of instance-specific optimization of the resulting field to get to that local minima -- essentially 'fine-tuning' the field for a couple of seconds (e.g. VoxeMorph TMI 2019, but plenty of other methods as well after that for example from Matthias Heinrich's group). This is crucial to the current paper, since the proposed method essentially does the same thing at inference -- runs a forward neural network (albeit just as a feature extractor), and then performs an optimization for the image pair -- and while it's done differently (and more elegant in some sense) than the existing literature, these approaches are very related. Overall, I was excited about the method but overall found the motivation/related-works either misleading or lacking rigor -- perhaps the authors are simply not aware of the abilities of existing literature mentioned above, but this does  limit the novelty and insight substantially

In the experiments, it seems to me that some obvious results are missing:
. I am not sure why methods used in Figure 3 (e.g. [43]) are missing from Table 1. It seems like a crucial comparison. Deformable KeyMorph [103] is missing from the whole experiments section, and is close to the existing method in that it separates the feature extraction (there via keypoints, but using a parallel net) from the optimization. Training keymorph on oasis and testing it seems like an important comparison if we are to extract some insights into how to decisions in the proposed method (the feature extractor and the implicit optimization) improve our insights in the field.
. Overall the results in Table 1 do not seem impressive -- comparable at best with existing methods. This is totally okay in my book, if the authors are able to communicate other interesting insights. Unfortunately, I do not believe this is the case.
. In the domain-shift section the authors show that their method tens to outperform the DL methods. However, their method gets the benefit of doing instance-specific optimization (the proposed layer) after feature extraction, at the cost of some GPU work for each pair. This is what instance-specific optimization does at the end of DL methods (as discussed below), which was employed in several papers, but this is not included in the comparison! This is a peculiar omission to me -- it should be included for completeness, but importantly it is also crucial to understand whether the proposed method behaves differently -- perhaps there is some advantage, in several situations, to the proposed method, or perhaps it offers more guarantees, etc -- we simply don't know. Minor: it would also be interesting to understand what are the limits of this domain shift of this model -- does it generalize to more substantial variations in modality, or 7mm slice spacing found in clinical sequences?
. I also find the claim that DL methods do not work without crops peculiar - most DL methods are convolutional and hence not size specific, and some (e.g. SynthMorph, which the authors refer to here) does not even require both images be of the same size.
. Since one of the contributions of the paper is the parallel feature extraction (to be used with the optimizer), it seems to me that it would be an important ablation to take the features of some robust method (does not have to be registration, even a domain-shift-robust segmentation network will do, or a 'robust foundation model', etc) and see if that can be combined with an optimizer. This would help provide insights if the formulation of the proposed model and the end-to-end training is useful. 
. The claim in 4.4 is also missing some reaosnable comparison -- would it not be possible to take the displacement field of any other DL method, initialize your favorite parametrization (freeform, diffeomorphic, etc) with that, and run the (any) optimizer? It seems like this is easily doable and a reasonable comparison?

Limitations:
Yes the paper has a limitations section. It would be nice if the authors could comment on how this method can be used on CPUs -- by far the standard hardware available to non-ML users (neuroscientists, clinicians, etc).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel image registration framework that aims to bridge the gap between classical and learning-based approaches. It incorporates fidelity optimization directly into the neural network as a layer. The framework employs end-to-end implicit differentiation through an iterative optimization solver, ensuring that the features learned are both registration and label-aware. Additionally, the warp functions derived are guaranteed to represent local minima of the registration objective within the feature space. The authors report that this framework performs exceptionally well on in-domain datasets and remains robust against domain shifts, such as anisotropy and variations in intensity profiles. Furthermore, the framework is designed to allow seamless switching between different transformation representations at test time without the need for retraining.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper's motivation is both clear and innovative, effectively merging classical optimization techniques with neural networks by embedding an optimization layer within the network. This enhances data consistency and steers the optimization toward local minima.
2. A significant technical achievement of this paper is the backpropagation of gradients through the optimization layer using the implicit function theorem. This demonstrates the paper's technical depth and is the key contribution of the paper.
3. The analysis of loss landscapes provided in the paper is insightful. The flattening of the feature space by neural networks introduces a wider range of possible gradient directions, which, when combined with fidelity loss, enhances overall performance.

Weaknesses:
1. A major limitation is the framework's registration accuracy, as measured by the Dice score, which does not demonstrate clear advantages over neural-network-only methods. For instance, in Table 1 on the OASIS dataset, the proposed LKU-Net variant achieves a Dice score similar to that of TransMorph-Large. This calls into question the practical benefit of integrating classical optimization into the network.

2. The absence of specific smoothness measurements in comparison with other methods, such as the percentage of negative Jacobian determinants (||J||<0) or the standard deviation of the logarithm of the Jacobian determinant, is a significant oversight. Without these metrics, it's unclear whether the observed increase in Dice score represents a genuine improvement or merely a trade-off with the deformation field's smoothness.

3. The paper lacks clarity in the reproducibility of results compared to other methods. For example, the learn2reg OASIS leaderboard indicates that LKU-Net can achieve a Dice score of 88.5 on the OASIS dataset without explicitly optimizing for smoothness. Similarly, TransMorph scores 88.5. Additionally, in Table 4, the performance of LKU-Net with Dice supervision is paradoxically worse than without, which is counterintuitive and raises concerns about the implementation fidelity and methodological consistency.

4. While the introduction of an optimization layer with a fidelity function is a key contribution, the paper falls short in comparing its method to other registration methods that also combine learning with optimization. This lack of comparative analysis leaves unanswered questions regarding the true effectiveness and novelty of the proposed method compared to existing approaches.

Limitations:
The paper commendably integrates a fidelity function with an optimization layer into a neural network, but there are several limitations:
1. The optimization layer has not demonstrated significant performance improvements compared to traditional neural network methods on OASIS leaderboard, and the implementation issues in the other datasets/methods raises concerns about the completeness of the result. Clarifying scenarios where this integration is advantageous could enhance the paper's impact.
2. The absence of quantitative smoothness metrics for registration performance is a critical gap. Introducing these metrics would strengthen the comparative analysis with existing methods.
3. The paper lacks a comprehensive comparison with major competitors combines optimization with learning. Expanding this analysis would provide clearer insights into the framework's unique contributions.
4. Concerns about implementation and reproducibility persist.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduced DIO, a differentiable implicit optimization layer to a registration network that aimed to bridge the gap of classical-learning-based image registration, considering the incorporation of weak supervision like anatomical landmarks into the learned features. The authors decoupled feature learning and optimization and trained a deep network to predict multi-scale dense features registered through a black box iterative optimization solver.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**S1.** The proposed model significantly improves over SOTA models on domain shift experiments.

**S2.** The paper is well-written

**S3.** Multi-scale optimization seems an interesting approach that might be a possible module that can be integrated in deformable image registration.

Weaknesses:
**W1. Possibilities of getting artifacts from different voxel sizes.** How do different voxel sizes ensure that the velocity or transformation field is differentiable or invertible? I believe this approach might introduce artifacts and lose fine details when propagating the source image to match the target image. And how processing the image features independently will preserve the diffeomorphism property in generating the transformation field? Can the image matching term efficiently capture the intensity differences likewise treating the input images as pairwise? Overall, treating the input images separately from the feature extractor raises several questions regarding the credibility of the transformation field $\phi^*$. 

**W2. Motivation for using multi-scale optimization.** I found the motivation of using multi-scale is somewhat underdeveloped. What is the rationale behind using such kind of optimization considering different source/target image features? 


**W3. Applicabilities of learned multi-scale dense features from sparse images.** In Sec. 4 the authors tried to show that DIO learned interpretable dense features and compared to the classical methods DIO preserved the gradient in the loss function. On the other hand, the authors also discussed that deep networks recovered affine transform with $~90$% overlap. I wonder what is the advantage of capturing multi-scale dense features compared to existing DLIR methods such as VoxelMorph, TransMorph, etc. 

**W4. Experimental supports.** Though the authors are getting comparable performance in image registration (Tab. 1), they are achieving improved results in testing out of domain/distribution datasets. However, the authors might want to show their model's performance without adapting their proposed multi-scale optimization. Basically, is the optimization scheme or the multi-scale features helping the complete registration model in achieving those bits of improvements? and the important question is why? Two interesting sets of experiments that validate the domain shift hypothesis can be the following - 
*(i) train on some of the other datasets (excluding OASIS) and test on the rest,* and 
*(ii) train on multiple datasets, including OASIS, and test on the rest.* 


Overall, I appreciate the authors for working in the domain of image registration which is very relevant as well as important in the medical imaging domain. However, the current version of the manuscript lacks some important experimental justification and further experiments. With that being said, the current version of the manuscript is under the threshold of acceptance. However, I am open to reconsidering the initial rating if the above concerns are adequately justified.

Limitations:
Limitations have been discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
cCOpatbXFU;"REVIEW 
Summary:
This paper investigates a new definition for the stochastic gradient variance in mirror descent.
Most existing analyses for stochastic mirror descent require a strongly convex distance generating function to bound the gradient variance.
This limits the their applications especially when this assumption fails.
In particular, Le Priol et al. (2021) have shown that the none of the existing convergence rates applies to Gaussian maximum likelihood.

This paper aims to fix this issue by proposing a new definition of gradient variance.
They show that the new definition is strictly stronger (more likely to hold in practice) than existing definitions, and derive convergence rates in convex setting.
The authors demonstrate an application of the new variance definition bounding the estimation error of MAP for one-dimensional Gaussian distributions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Analyzing stochastic mirror descent is hard when the distance generating function is not strongly convex.
This paper is a step towards generalizing mirror descent analyses.
In particular, I like Section 2.2 where the authors show that the proposed definition is strictly better than existing ones.

- The mirror descent analysis in this paper yields a non-asymptotic bound for the estimation error of Gaussian MAP. This seems to be a fundamental problem lacking theoretical guarantees based on Le Priol et al. (2021).
However, I am now knowledgeable enough to confirm the significance or novelty of this result in statistics.

Weaknesses:
While developing the new gradient variance definition is certainly interesting, I have the following concerns.

- The authors have shown that their gradient variance \\(\sigma_{\star, \eta}^2\\) is finite for every fixed step size \\(\eta\\).
The convergence rates in the convex setting are proved using constant step sizes, and thus the optimality gap does not vanish.
To make the optimality gap vanish, diminishing step sizes are often required, which is not covered in this paper.
Proving convergence with diminishing step sizes probably requires characterizing the average variance \\(\frac1T \sum_{t=1}^{T} \sigma_{\star, \eta_t}^2\\), which I think can be done only on a case-by-case manner depending on the specific application.

- The only case so far where this new definition shines while all other definitions fail is maximum likelihood estimation for one-dimensional Gaussian distributions.
This is very restrictive.
Is it possible to generalize this result to multivariate Gaussian distributions?
In addition, it would be great if the authors could provide other applications to further justify the necessity of this new definition.

Minor:
- Line 192: Add a period.
- Bad notation in Section 4.2: It might be confusing to use $\Sigma$ to denote the standard deviation.
Consider using a different letter like $s$ or $\tau$.

Limitations:
NA.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Problem:
In proof of Proposition 2, by the definition of $\sigma_{*,\eta}^2$, we can obtain that
$ \sigma_{*,\eta}^2 = \frac{\min_x f(x) - \min_x f_\eta(x)}{\eta}  $. 
However,  $x_* =\argmin_x f(x)$ does not equal to $x_*' = \argmin_x f_{\eta}(x)$.
This will lead to $\sigma_{*,\eta}^2 \neq \frac{1}{\eta^2} D_h(x^*, x^+）$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Weaknesses:
No.

Limitations:
No

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new analysis of SMD using a newly introduced generalized variance notion. The benefit of the new analysis is demonstrated in the application to maximum a posteriori estimation of Gaussian parameters.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
After introducing a new variance notion, the paper delves into comparison with other existing notions and shows that the proposed one is the largest meaningful notion. After a careful comparison, analysis of SMD is presented using this mild assumption. This analysis substantially departs from the results known in the literature. The demonstration of the use case of this new theory in the context of statistical estimation is also clear and adds more significance to the new theory.

Weaknesses:
Major:

As explained after theorem 4.3, the guarantees are derived for a reverse KL and may not imply anything on the desired quantity $f(\theta) - f(\theta_*)$. This of course, limits the contribution in this application significantly as non-asymptotic rates were known before. 

Minor problems that I hope the authors can fix in the next revision. 

1. Is the set C compact? If not, why the minimum exists in Proposition 2.2?

2. Cannot find where $x_*$ is defined. Why does it exist? 

3. There is a small issue with indicies in equation (12) and in paragraph before. $\eta_{n} = \frac{1}{n_0+n+1}$, and the stochastic gradient should depend on the new sample $X_{n+1}$.

Update: meaningful results are obtained only for relatively strongly convex case (which is a stronger assumption than even strong convexity). In the convex case, a different (much stronger) definition is used. This becomes clear only after reading Appendix D. This limitation should be clarified in section 3.2, where convergence on some surrogate loss is shown. I will update my evalutation.

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new variance assumption for the analysis of stochastic mirror descent (SMD) to handle cases where standard bounded variance assumption does not hold. The authors show this new assumption can be shown to hold under some regularity assumptions. The authors use the new results to show some convergence guarantees for MLE and MAP of a Gaussian with unknown mean and variance using the connection between this problem and SMD convergence guarantees.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The topic is definitely interesting and timely. Results for stochastic optimization without bounded variance assumptions are quite interesting. As shown in the prior literature, this task is especially subtle in the Bregman case. As the authors argue in detail, this difficulty is acknowledged in previous works such as [7] and [17]. It is neat that the authors show the importance of the new results by deriving convergence bounds for MAP/MLE of a Gaussian with unknown mean and variance by using the connection between these bounds and SMD in [17] (which itself is a nice connection). This adds a nice and clear motivation. The work makes some progress towards solving open questions from [17], while as the authors clearly explain, the open questions are still not completely solved.

Weaknesses:
I find the motivation of the paper and its application to MAP bounds interesting, however I have some concerns about writing and the strength of the derived results in the context of the application in Section 4. It seems necessary for the latter point to be clarified.

- Authors write after Theorem 4.3 that the open problem from [17] is not completely resolved because  the convergence is not shown for the desired quantity. In particular, the authors describe that the guarantee is for $D_A(\theta_*, \theta^{(n)})$ instead of $D_A(\theta^{(n)}, \theta_*) = f(\theta) - f(\theta_*)$. The authors then write that two quantities can be related asymptotically but they state: ""but we might also be able to exploit this control over the course of the iterations"". Can you make this point more precise? It is not clear to me what this last part is trying to describe. Is it meant to be understood as an open question or is it possible for the authors to derive the stronger result? Since the paper mentions at many places that showing convergence guarantees for MAP is an important contribution of the paper, it is important to justify the convergence metric used in the results for justifying the contribution of the paper fully.

- It might be better to replace MLE in the abstract to MAP since Section 4 is mostly about MAP.

- Abstract states a couple of times ""strong convergence"", I suggest to remove this since ""strong convergence"" has a precise meaning in infinite-dimensional optimization and usage in the abstract is confusing because of this. Clearly this is not how the authors are using this term, but it seems authors are using this as a subjective adjective, which is not necessary. By subjective, I mean that: how can one decide what convergence result is strong and what is not?

- Assumption 1 requires all $f_\xi$ are convex. This is rather strong since the standard assumption is $\mathbb{E} f_\xi$ to be convex. Can you discuss this more? According to Prop 4.1, this holds for the main application of the paper, but it might be worth discussing why componentwise convexity is needed.

Limitations:
The limitations are discussed clearly. The authors provided explanations after Theorem 3.3 and Theorem 4.3 to describe the limitations of their result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This submission studies stochastic mirror descent (SMD) under quite mild conditions on the mirror map and objective function. More specifically, there are a variety of SMD analysis in the literature, but virtually all of them require strong conditions on the mirror map (such as strong convexity) that do not hold in cases where we only have relative smoothness (and/or relative strong convexity) of the objective function with respect to the mirror map. The authors propose a definition of variance of SMD that is better behaved under minimal assumptions. They show how this new variance can be used to obtain general convergence results for SMD. Finally, they show how the new variance definition for SMD can show some kind of non-asymptotic convergence rates for MLE and MAP of Gaussian parameter estimation with unknown mean and covariance, making partial progress on a conjecture posed by Le Priol et at.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
This is an interesting paper that tackles a hard theoretical problem. I think it is of interest for researchers interested in mirror descent. The new definition of variance of SMD has interesting properties even under very mild assumptions, as the authors show when comparing the new definition with other definitions of SMD variance in the literature. Moreover, the results on Sec 4 already show how this is an interesting way to analyze SMD, and is likely to lead to follow-up work on the area.

So the strengths summarized in bullet points:
- Thorough comparison of new variance definition with other definitions in the literature and proof of finiteness under assumption 1. 
- General convergence theorems of SMD under mild assumptions that recover known results in the deterministic case, showing this is may be a ""natural"" variance definition for SMD and useful for our understanding of SMD.
- Partial progress towards the conjecture of Le Priol et al.

Weaknesses:
In its current form, I have one main concern with the paper:
- Despite what is written at the beginning of the paper, **Assumption 1** is NOT a blanket assumption used throughout the paper. In fact, it appears only section 2 uses assumption 1. The rest of the paper uses a weaker assumption that is never clearly stated, which makes it hard to understand when the results hold or not. 
This is likely to be a problem with presentation, but in its current form it is often not clear what are the assumption required at each point. Since the main point of the paper is to use a minimal number of assumptions, it is very important for those to be clearly stated. 

A minor weakness is the lack of an example besides MAP/MLE. I could not easily think of a concrete example where I could apply the convergence results in sec 3 or 4. If the authors have an example besides MLE or MAP (even if a bit artificial), it would be great. For example, some example with a mirror map such as $- \log x$ would be interesting, but this is a minor suggestion, since it would be nice to see a concrete example of the use of the results in Sec 2 (the results in Sec 4 require a specialized bound on the variance) 

Summary of weaknesses:
- Unclear requires assumptions for many of the results
- (Minor weakness) Lack of a concrete (even artificial) example of application of any of the theorems in Sec 3 beyond MAP/MLE (and the latter require specialized bounds on the variance).

Limitations:
Although the authors are not explicit about some of the limitations of the results on sec 3, they do discuss how to interpret some of the results and limitations from their convergence rates.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
buSEDdP5YX;"REVIEW 
Summary:
The authors present two potential sources of error which can arise when composing sub-sampled DP mechanisms. On one hand, they discuss cases in which the composition of worst-case datasets does not yield the expected result, on the other hand, they disambiguate guarantees for mechanisms with Poisson sampling vs. sampling WOR.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
I appreciate that this paper points out some of the subtleties which arise regarding the distinctions between worst-case databases and dominating pairs and regarding the correct accounting of specific sampling schemes (Poisson/WOR). These subtleties can pass unnoticed, and lead to errors which compromise the privacy of individuals.

Weaknesses:
There is nothing particularly wrong with the paper. The facts stated are valid, and they are interesting, especially since they point out potential sources of confusion. However, none of this is surprising or even particularly novel. Most of this information is implied by earlier work (Zhu et al. in particular), and some of the facts stated here would be better suited as GitHub issues on the relevant accountants, followed by a technical report at a venue like TPDP or Journal of Privacy and Confidentiality. That is to say: I am not against this paper in general, but this is not a NEURIPS paper to me. It is a highly specialised piece of technical writing with a very narrow scope, and is likely to be of interest only to a very small community. I would recommend the authors to submit it to a venue which is better suited to its content.

Limitations:
The discussion on limitations is a bit lacking in my opinion. The authors state (in the checklist) that the ""main limitation is expressed in Conjecture 12"". Not being able to find a counterexample for a proposition is not really what one understands under the term ""limitation"" of a work. In particular, the supposed ""limitation"" is --by the authors' own admission-- easily resolved by just running the accountant on the two curves separately and taking the supremum. I would have much preferred an experimental section where the consequences of the pitfalls stated in the work are actually shown to affect the real-world use of DP-SGD or other mechanisms, and/or to see that specific privacy threats are practically enabled by overlooking these subtleties (e.g. through auditing).

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper examines the discrepancies between privacy accounting methods and their implementations, highlighting several cases where these mismatches lead to incorrect results. Specifically, it compares the noise requirements for achieving privacy guarantees under Poisson sampling versus sampling without replacement, and explores the limitations of worst-case dataset assumptions in subsampled mechanisms. Additionally, the authors address challenges in computing tight differential privacy (DP) bounds under the substitution relation of neighboring datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper addresses an important and timely topic in the field of differential privacy regarding privacy accounting.

- The findings have strong practical implications, potentially preventing unintended privacy breaches.

- The authors' message is well articulated, promoting better practices among DP practitioners.

- Despite critiquing existing methods, the authors maintain a respectful and constructive tone.

Weaknesses:
- The different messages of the paper may be convoluted sometimes, which makes the paper hard to follow.

- No viable technical solutions are provided for the identified issues, which might be a difficult research problem.

Limitations:
The authors discuss the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The two main contributions of the paper is as follows:

the privacy guarantee of composition of subsampled mechanism may not be defined by worst-case dataset(s) for the underlying mechanism
Poisson subsampling and sampling without replacement may not have similar privacy guarantee.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper studies a very important problem of composition of subsampled privacy mechanism. There has been a lot of work in the recent past that performs a tight privacy accounting. This work is in the line of these works. These accounting results are used in deployment as well to show how much privacy loss has happened during training when using a prescribed noise scale. Based on these bounds, the training is stopped once we have expired the privacy budget. In this regard, their second result is very important because we definitely use subsampling without replacement in DP-SGD.

Weaknesses:
There are some typos, and the result for the gap is shown empirically. I have to state that I have not seen the Appendix so if the authors have a provable guarantee for this gap in the Appendix, please point me that. To me, the selling point of the paper is this result and it should be placed front and center. Most of the results that are given in the form of propositions and lemma are from previous works.

Limitations:
Mostly seem like an empirical study of the composition result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the notion sampling with replacement for differential privacy. Most of the literature on machine learning with differential privacy benefits from privacy amplification by poisson sampling in the privacy analysis. However, when implementing the mechanisms, engineers ofter use the sub-sampling with replacement as a substitude for poisson sampling, mainly due to efficiency issues. This paper studies the gap between these two settings. Their main contributions are as follows: 

 - Identifying the Problem with DP-SGD Implementations: The authors highlight a critical issue with implementations of (DP-SGD). They argue that many implementations incorrectly assume that Poisson sampling and batch-sampling yield similar privacy guarantees, which is not necessarily true.

- Gap between Batch-Sampling and Poisson Sampling: The paper demonstrates a significant privacy gap between these two sampling methods. They provide an example showing that for certain hyperparameters, Poisson subsampling can result in an ϵ≈1, whereas batch-sampling without replacement can result in an ϵ>10. This discrepancy is critical for privacy accounting in DP-SGD.  Authors compare the privacy guarantees of Poisson subsampling and batch-sampling. They show that the privacy guarantees can differ significantly depending on the sampling technique used. Their analysis reveals that the method of sampling batches (Poisson vs. fixed-size) significantly impacts the resulting privacy guarantees, cautioning against the interchangeable use of different sampling techniques in privacy analysis.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Identifying an important problem with implementation of DP-SGD

Weaknesses:
- I have some concerns about the correctness of the results.

- There is not much technical novelty.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
bNVvcgfEwD;"REVIEW 
Summary:
The paper examines the convergence properties of Adafactor, an adaptive learning rate optimizer designed for deep learning tasks, particularly in memory-constrained environments. The study focuses on Adafactor’s performance in non-convex optimization scenarios and provides theoretical convergence proofs under smooth conditions. Despite its widespread practical use, especially for training large language models, Adafactor’s theoretical understanding has been limited. This research fills that gap by proving that Adafactor can reach a stationary point with a specific convergence rate, highlighting both its efficiency and the impact of different parameter settings on its performance. The paper also introduces modifications to the default hyper-parameter settings based on theoretical insights, which are validated through empirical tests, showing potential improvements over traditional setups.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The analysis of Adafactor’s convergence is crucial for its application in training extremely large models, such as large language models (LLMs). This study significantly contributes to the theoretical foundations of Adafactor, supporting its practical use with a solid mathematical framework.

Weaknesses:
The manuscript could benefit from a deeper discussion on the proving techniques and the tightness of the provided convergence bounds.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the convergence of Adafactor for non-convex smooth objectives. The paper looks at both full batch and stochastic cases and analyze the convergence rate. Experiments are provide to validate some of the findings about the hyperparameters. 

The main contributions of this paper are: (1) convergence rates for both full-batch and stochastic cases for Adafactor (2) Provide empirical evidence that the hyperparameters leading to optimal convergence rates yields better empirical performance. My main concern regarding this paper is about novelty and lack of comprehensive experimental evidence. First regarding novelty, convergence of Adaptive methods has been studied in several earlier papers for full-batch settings (e.g. De et.al., 2018). Similar, the issue of second moment decay parameter increasing at the rate of 1 - 1/k is well known (e.g. Reddi et al., 2018) and under this particular schedule, the algorithms roughly boils down to Adagrad like schedule (instead of exponential moving average). Both these contributions are quite well-known for adaptive methods. Thus, it is not entirely clear to me if these contributions for Adafactor are novel enough to warrant acceptance.

Furthermore, Shazeer et.al., in Section 7.2 of their paper already discuss about the aspect of second moments decay. The experiments in the current paper are neither comprehensive or convincing that this leads to he optimal convergence in practice. It is important to do a very thorough investigation if the authors have to demonstrate this phenomenon (e.g. try it in different NLP settings where adaptive methods are very effective). Overall, in my opinion, the main weaknesses of this paper are novelty and poor empirical study.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
See summary

Weaknesses:
See summary

Limitations:
See summary

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the convergence of a memory-efficient, adaptive algorithm, Adafactor, under non-convex smooth settings. First, the authors show that in the full-batch setting (with appropriate hyperparameters), Adafactor converges to a stationary point at an $\tilde{O}(1/\sqrt{T})$ rate. For the stochastic setting, they study two regimes: with and without clipping $\eta_k$, and show that under appropriate selection of hyperparameters, Adafactor attains an $\tilde{O}(1/\sqrt{T})$ rate of convergence to the stationary point, matching SGD up to logarithmic factors. The observations are complemented with empirical findings.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Considering large-scale language model training, the work focuses on an important memory-constrained practical optimizer for which we have limited theoretical understanding. The work is clearly motivated, the introduction to Adafactor and its connection to Adam is concisely discussed, and the paper is easy to follow in general. Even though it’s not exactly vanilla Adafactor, it’s exciting to see the authors have established bounds matching SGD in the stochastic case. I also appreciate the authors not shying away from discussing the impact of various hyperparameters (and potential negative points).

Weaknesses:
Even though there are space constraints, I would like to see some proof sketch (at least for the full-batch case) in the main body to provide a general sense for the reader. For example, it could be as simple as starting from smoothness in Taylor series (Eq. 14), telescoping over $k$, lower bounding (a), upper bounding (b) in Eq 20. The more critical step appears to be the lower bound, and a general flavour of how Lemmas $A.2, A.3$ are used to achieve that would be nice to see.

This is minor, but it would be helpful for the reader if figures are referenced whenever a discussion about some experiment is invoked. Two instances I noticed are line 222, the effect of $\epsilon_1$, and line 265, the effect of time-increasing $d_k$. Please look for other instances, if any.

Figure 1, experiment on the effect of different decay rate parameter $c$: Showing the test performance is nice, but as the discussion is about train-loss convergence, their time-evolution should be included. It’s fine if the convergence doesn’t speed up with increasing ($c$) and doesn’t match theory, but its important to have them as the entire work deals only with train-loss convergence.

Limitations:
Yes, the authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
azkuhJBZXi;"REVIEW 
Summary:
This paper develops a structured and generalized reasoning framework, CreDes, for long-range reasoning in LLMs. In the framework, the Causal Relationship Enhancement (CRE) is used to guarantee the solid causal rightness between each step of reasoning and state transition, and the Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree, to improve the efficiency.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper is well-structured and clearly states the problem they studied. It considers the long-range reasoning of LLMs from two aspects: the correctness from one-step reasoning (OSR) to state transition, and the efficiency of the solving process.
2. This paper transits the long-range reasoning problem of LLMs into the construction of causal probability trees from the initial and goal states and uses Dual-End Searching to improve efficiency. This is a reasonable and interesting thought.
3. The experimental results are SOTA in long-range reasoning tasks in terms of both accuracy and time efficiency.

Weaknesses:
1. The main concern is the understanding of ATE. This paper frequently uses ATE as part of the loss function and thinks the lower ATE can guarantee the solid causal rightness between each step of reasoning and state transition. However, ATE is used to measure the causal influence level between variables from the observational data, and causality does not mean rightness.
2. The DES section is not clear enough. It is suggested that more explanation be provided for the reason for the ATE as part of the loss. For example, if “B is the number of unfolded layers where the current leaf is located Ni”, what does E(A|do(B)) and E(A) mean in Formula (5)?
3. This paper needs to supplement the usage scenarios of methods, specifically in which scenario to use CreDes, in which scenario to use Cre alone, and whether Des is used separately.

Limitations:
None.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces CreDes, a framework to improve the long-range reasoning capabilities of LLMs, consisting of two main components: Causal Relationship Enhancement (CRE) and Dual-End Searching (DES). CRE is developed to reduce causal hallucinations in LLMs by strengthening the causal relationships between reasoning steps and state transitions; it uses structural causal modeling and optimizes the Average Treatment Effect (ATE) during training. DES breaks down long-range reasoning tasks into shorter segments by simultaneously searching from both the initial and goal states on a causal probability tree. The authors evaluate CreDes on Blocksworld, GSM8K, and Hanoi Tower puzzles, showing improvements in both accuracy and efficiency compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- CreDes demonstrates significant improvements over existing methods, especially for complex tasks requiring many reasoning steps.
- The use of causal modeling concepts like ATE provides a solid theoretical foundation for the proposed approach.
- The method shows effectiveness across different types of reasoning tasks (e.g., spatial reasoning, math problems).
- CreDes enables simultaneous multi-step reasoning, potentially reducing computation time compared to sequential methods.

Weaknesses:
Major concerns: 

- The generalizability and scalability need better justification. The paper primarily tests the CreDes framework on Blocksworld, Hanoi Tower, and some mathematical reasoning tasks (GSM8K). These are relatively structured, rule-based problems that may not represent the full spectrum of reasoning challenges. In addition, the proposed method cannot be well scaled to long-range reasoning; for example, in Table 1, performance drops significantly for Blocksworld tasks beyond 8 steps, with success rates falling from 0.68 to 0.34 for 12-step problems using Llama-2-7B + CreDes. Table 3 shows even steeper declines for Hanoi Tower, with success rates dropping from 0.27 at 9 steps to just 0.07 at 13 steps for Llama-2-7B + CreDes. Notably, the authors explicitly acknowledge this limitation in Section 4.6, stating: ""The DES approach, while effective for moderate-length tasks, struggles with very long reasoning steps, leading to a decline in performance.""

- The presentation of this paper could be improved.

  -- In the problem definition, there is no explanation of the difference between training without common instructions and with common instructions.

  -- There is no detailed discussion of the differences between correlation and causation in Sec 3.2. I am confused about whether the correlation of two variables has anything to do with their distributions.

  -- While efficiency gains are mentioned, the added complexity of CRE and DES likely introduces some computational overhead, which could be further discussed.

  -- There is no analysis of the impact of the choices of hyperparameters on the methods, particularly in the CRE component.

- The proposed method lacks comparison to more recent state-of-the-art methods. The paper compares CreDes mainly to older baselines: Reasoning via Planning (RAP), Chain of Thought (CoT), and Reflexion of Thoughts (RoT). However, it doesn't evaluate against more recent advances in LLM reasoning, such as Tree of Thoughts (ToT) extensions in line 42, and the paper doesn't mention or compare to other recent works such as [a] and [b], which also address multi-step reasoning challenges. As a result, the technical contribution is not entirely clear.

[a] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, & Xinyun Chen (2024). Large Language Models as Optimizers. In The Twelfth International Conference on Learning Representations.

[b] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, & Zhiting Hu (2023). Reasoning with Language Model is Planning with World Model. In The 2023 Conference on Empirical Methods in Natural Language Processing.

Minor concerns: 

- Experiments are primarily conducted with 7B parameter models, leaving questions about scalability to larger models. How does the performance of CreDes scale with increasing model size (e.g., to 10B+ parameters)? The computational overhead may limit the framework’s scalability and applicability in real-world scenarios with limited resources.

- The approach achieves significantly lower accuracy in tasks with very strict ordering constraints, such as the Hanoi Tower problem.

- Since Blocksworld involves random steps, an analysis of the robustness of the performance may be needed.

 - More analysis/discussion on the sequential ordering of steps may be helpful. Notably, the ATE cannot recognize casual logic.

 - Some editorial issues, e.g., Line 110

Limitations:
The authors have discussed the limitations, and it is adequate to me. I do not see any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The integration of Causal Relationship Enhancement (CRE) and Dual-End Searching (DES) mechanisms presents a novel solution to addressing causal hallucinations and large search spaces in long-range reasoning tasks. The CRE mechanism’s use of Structural Causal Modeling (SCM) and Average Treatment Effect (ATE) is  ensure causality between reasoning steps. Extensive testing on datasets such as Blocksworld, GSM8K, and Hanoi Tower demonstrates the effectiveness of the CreDes framework.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea seems novel and it test on well-known reasoning datasets.

Weaknesses:
The method presented in this paper evaluates ATE on LLMs, but this approach's validity hinges on the assumption that LLMs can perfectly represent the real-world environment. The very reason we criticize LLMs for their reasoning issues is because their inferences are not accurate. Estimating ATE might only bring the prediction results closer to Y while maximizing the influence of the intervention factor on Y. However, it does not necessarily mean that the intervention factor is the true cause of Y. In other words, since there is no alignment with the causal relationships in real-world scenarios, the implementation of this method does not prove that the reasoning is causally sound.

The method lacks deeper thinking. The authors just apply the concept of ATE to the Chain-of-Thought (CoT) without thorough analysis. This oversight leads to a misalignment between the experimental results and the motivation of the paper. Suppose LLMs are not a good s simulations of the real world. In that case,  performing interventions on LLMs (whether they align with the real world or their identifiability) requires sound theoretical analysis and experimental validation. The current paper lacks a deep discussion on this matter.

Limitations:
See weakness.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to improve LLMs in dealing with long-reason reasoning problems, especially the challenges of causal hallucination (inconsistency between one-step reasoning and corresponding state transition) and large search space. To tackle the first challenge, average causal effect of the one-step reasoning (treatment) on the state transition (outcome) is added to the loss function of the LLM; and for the second challenge, a dual-end (i.e. bi-directional) search approach is taken to improve efficiency. Experiments are conducted to demonstrate the effectiveness of the proposed method and its superiority over the compared existing methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. An interesting idea of formalizing the problem from the perspective of causal effect and incorporating causal effect into the loss function.
2. The adoption of a dual-end search approach for improving efficiency.
3. The motivation of the paper is well presented in general.

Weaknesses:
1. The soundness of the proposed CRE method (for dealing with the challenge of causal hallucination) is in doubt.

(a) It's not clear why the method aims to $\textbf{minimize} the absolute value of the average treatment effect (ATE) of the one-step reasoning on state transition. Assuming that the ATE can be accurately estimated, what we want here would be to maximize the ATE that can be achieved by the LLM, i.e. when the one-step reasoning is correct done will likely lead to a correct state transition.

(b) It's not clear how an unbiased estimation of the ATE can be obtained, and what assumptions are made in terms of ATE estimation.

(c) The definition or understanding of ATE is incorrect. In particular, formula (2) is wrong, and formula (5) is incorrect too. 

2. The presentation/technical quality requires improvement, including the presentation of related work. Please find below some examples:

(a) In Lines 42 to 44, it is said that the existing methods such as CoT are limited in task decomposition, but Lines 78-80 state that they can breakdown queries into manageable steps.

(b) Section 3.1 is titled as ""Problem Definition"", but it rather looks like a section on experiment setting.

(c) Lines 145-146 state that Fig. 1 shows ""we leave the reasoning path selection to be controlled by the cross-entropy loss"", but I cannot see this indicated in Fig. 2.

(d) Line 159: do(.) is an operator, specifically the do operator, rather do-calculus, although do-calculus uses this operator.

(e) Lines 159-160: the statement on the do(.) operator or do-calculus is incorrect, since an do operation on the treatment X would lead to the change of the outcome Y, especially if X is a cause of Y.

Limitations:
The authors have presented some discussions on the limitations of the proposed method. It would be better if the assumptions made could be presented more clearly and what the practical implications would be if the assumptions are violated.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new framework, CreDes, designed to enhance causal reasoning in large language models (LLMs) and solve complex, long-range reasoning problems. The framework integrates two main innovations: the Causal Relationship Enhancement (CRE) mechanism, which applies cause-effect interventions to maintain causal accuracy across reasoning steps, and the Dual-End Searching (DES) method, which approaches problem-solving by initiating searches from both the initial and goal states to efficiently navigate large search spaces. The efficacy of CreDes is demonstrated through rigorous testing on challenging datasets like Blocksworld and Hanoi Tower, where it outperforms existing state-of-the-art models in both accuracy and efficiency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Novel approach: The paper addresses essential limitations in LLMs' reasoning capabilities for long-range tasks in a causal perspective.
2. Comprehensive evaluation: The authors test their method on multiple datasets and compare against several baselines and shows improvements in both accuracy and time efficiency.

Weaknesses:
1. Limited model sizes: The experiments are primarily conducted on 7B parameter models, which may not reflect performance on larger state-of-the-art LLMs.
2. Lack of error analysis: The paper doesn't provide a detailed analysis of the types of errors made by the model or how they differ from baseline methods.
3. Dataset validity and construction: More details is needed for the use of a custom-made Hanoi Tower dataset which potentially limiting the reproducibility and generalizability of the results.
4. Computational efficiency and scalability: As mentioned in the Limitation, the paper lacks a detailed discussion of the computational requirements and scalability of the CreDes framework.
5. Generalization to less structured tasks: The framework's effectiveness is primarily demonstrated on highly structured tasks but it's unclear about its applicability to more dynamic or open-ended reasoning scenarios.
6. Lack of statistical significance: The paper doesn't report error bars or statistical significance for its experimental results.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
apI1GltwSx;"REVIEW 
Summary:
This work studies batch-to-global dataset distillation, optimizing the synthetic dataset by matching the statistical information of the synthetic batches to that of the full real dataset. Previous batch-to-global methods lacked diversity because each batch had the same optimization objective, leading to redundant information being learned across different batches. Based on this, the paper proposes an early-late training method. First, the real data is divided into lowest, medium, or highest probability patches based on a pretrained model, and these patches are sampled to initialize the synthetic dataset. During training, within-class samples are divided into smaller sub-batches, which are gradually concatenated for batch-to-global training.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. Previous batch-to-global methods indeed faced the problem of synthetic datasets receiving the same supervision signal, leading to redundant information being learned. This paper attempts to propose a new solution to this issue.

2. Extensive experiments demonstrate the good efficacy of the proposed method.

Weaknesses:
##  The main problem of this work is the writing.

>*It dedicates too much space to introducing previous work.*

The introduction describes previous methods in too much detail, leading to redundancy with the content in the related work section. The related work section also spends too much space summarizing and describing previous methods.

>*The technical part is confused*

The proposed method appears straightforward, but the authors describe the entire process almost entirely in text, lacking mathematical descriptions and definitions, which makes it somewhat difficult to understand. I suggest the authors dedicate more space to explaining the Concatenation Training and Training Procedure, incorporating some formulas to clearly demonstrate how the training is conducted.

>*It is doubtful whether the proposed method can effectively solve the issues present in previous approaches.*

Although the synthetic dataset is further divided within classes and different initializations are used, the supervision signal for each sub-batch seems to still be the same global signal as in other batch-to-global methods. This means that each sub-batch is still optimized in the same direction, potentially resulting in redundant information being learned. I suggest that the authors try to provide a more sound explanation.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Recent advancements in dataset distillation have led to two main approaches: batch-to-batch and batch-to-global matching. While the former excels in small datasets, the latter, though popular for large datasets, faces a diversity challenge due to independent optimization. Authers propose an EarlyLate training scheme that enhances diversity in batch-to-global matching by partitioning IPC samples into subsets and optimizing them locally. Experiments show significant improvements over previous methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1) The technical approach is solid and robust, demonstrating a high level of technical competence.
2) The performance metrics presented are highly competitive, showcasing the method's effectiveness in comparison to existing benchmarks.

Weaknesses:
1) The motivation for the research is unclear, lacking an explicit articulation of the unifying challenges faced by current state-of-the-art works.
2) The resolution of the figures is inadequate, impeding clear interpretation of the results.
3) There is inconsistency in the styling of table borders and captions, with captions for Table 1 and 2 placed in different positions compared to subsequent tables, some above and some below the table.
4) The experimental settings are not uniformly aligned, and efforts should be made to cover all datasets and settings consistently across all experiments to ensure comparability and rigor.

Limitations:
see weaknesses

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a simple but novel approach to enhance image diversity in dataset distillation. previous methods face challenges in balancing computational efficiency and diversity in synthetic images. The proposed EarlyLate training scheme addresses these issues by partitioning predefined IPC samples into smaller subtasks and using local optimizations for each subset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The EarlyLate training scheme effectively enhances the diversity of synthetic images. This is a very simple but novel approach to increase the diversity of distilled datasets. I believe it can provide some inspiration for future work.
2. The method, or the training scheme reduces the computational load compared to batch-to-global matching methods.
3. The experiments are comprehensive, including performance, cross-architecture generalization, ablation, and application. These experiments verify the method's superiority.

Weaknesses:
1. Compared to previous methods, the work in this paper is incremental.
2. The motivation and the advantages of the ""Selection Criteria"" in the initialization approach are not clear. And I am confused about how to rank, which is presented in Fig 5, could the authors explain it here?
3. There are a lot of hyperparameters involved. How should these hyperparameters be tuned? Are there any principled approaches?
4. I want to know the impact of the initialization method. In the ablation study, only CDA+init is shown. More advanced methods with init and whether EarlyLate uses init are not presented.
5. The performance of other sota methods on MobileNet-v2 is not presented in Table 1. Is the proposed method still better than other sota methods on MobileNet-v2?
6. Training tricks like random crop play a significant role in methods such as SRe2L. I would like to know to what extent the method proposed in this paper relies on such tricks.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes an EarlyLate curriculum learner, which distills the easiest samples first and gradually add harder samples. Based on batch-to-global distillation algorithms, the proposed method consistently enhances the distillation performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The writing is clear.
- The proposed curriculum learning scheduler seems effective, which is also an interesting point to analyze.
- Good distillation performance.

Weaknesses:
Limited contribution and potential overclaiming: 

1. In section 3, the initialization with real samples is common in DD, the data selection is proposed by RDED, and only the training scheduler is proposed in this paper. I suggest that, at least, add some diversity analysis and comparison of the distilled data.
2. Though the paper is titled with ""diversity-driven"", the method part lacks justification of the relation between ""diversity"" and the proposed scheduler. It seems that only the real initialization contributes to the diversity.

Limitations:
The authors have adequately discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
anxYEohntP;"REVIEW 
Summary:
The paper investigates the potential for Large Language Model (LLM) agents to exhibit prosocial behavior through irrational decision-making, paralleling human cognitive biases. It introduces the CogMir framework, which leverages the hallucination properties of LLMs to simulate and assess social intelligence through various cognitive biases. Experimental results demonstrate that LLM agents and humans show high consistency in irrational and prosocial decision-making under uncertain conditions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Innovative Framework: The introduction of the CogMir framework is a novel approach to studying social intelligence in LLMs by mirroring human cognitive biases.
2. Comprehensive Evaluation: The paper provides a detailed evaluation of multiple cognitive biases, such as Herd Effect, Authority Effect, and Confirmation Bias, among others.
3. Interdisciplinary Approach: Combining insights from social sciences and evolutionary psychology enriches the study and provides a broader context for understanding LLM behavior.

Weaknesses:
1. Why use hallucinations to mirror human cognitive biases? I think more explanation is required.
2. How to manipulate hallucination?
3. Why use all new datasets in experiments? Do existing datasets all don't have the data you want?
4. I don't think the conclusion is interesting.

Limitations:
Please compare with existing multi-agent social system and point out your advantages.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper implements a framework for evaluating LLMs’ social cognitive biases. The social science experiments are automatically collected by LLMs and then verified by humans. The framework includes two communication mode for interaction between multiple humans and multiple LLMs. The experiments include seven LLMs, across seven social science experiments. Results show that most LLMs show cognitive biases in the designed scenarios.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	The paper investigates a relatively underexplored area, which is the social cognitive biases in LLMs. This evaluation plays an important role in assessing the human-like ability of LLMs.
2.	The experiments include seven different models, providing insights into the comparison of their difference in abilities.
3.	The paper develops a framework and collects corresponding data for future use by more LLMs.

Weaknesses:
1.	The paper treats the scenario where LLMs have wrong beliefs (for example, apple is blue) as where LLMs have cognitive bias because of some external influence (observing many others’ choices, authority, etc.) However, there are no experiments showing that the wrong beliefs are caused by the external influence. I mean (though with a pretty low probability), what if without the external influence, LLMs themselves hold the belief that apple is blue? I think a better way is to measure the change of LLMs’ belief towards a concept without and with the external influence.
2.	The paraphrasing may not be a good choice to evaluate rumor chain effect, since continuing paraphrasing a sentence will indeed lower the similarity with the original one, and this is nothing to do with how message spreads in LLM agents. I believe the authors should design a scenario closer to daily communication.
3.	Since the constructed dataset is an essential part in the framework, how do you construct the dataset becomes important. Further explanations are needed about: How do LLMs automatically do the literature search? What is your manual selection criteria about the social science experiments?
4.	Presentation of the paper needs further improvement. There are too many module names in section 3 and readers can easily get confused with these messy concepts. Also, how the modules are organized is not clearly illustrated. A big problem is that some names in Figure 2 cannot match those in texts. For example, is “Mirror Settings” the same as “Environmental Settings?”

Minor suggestions to presentation:

1.	Please be consistent in the terminology. Currently some terms are “presocial” while others are “pre-social.”
2.	It will be better to make the four titles in Fig. 2 the same as introduced in section 3.

Limitations:
I think the author can mention that the current method does not verify LLMs’ original beliefs towards the knowledge in the proposed datasets.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces **CogMir**, a novel framework designed to assess the social intelligence of LLM agents to mirror human cognitive biases. Through an evolutionary sociology perspective, the authors systematically evaluate the social intelligence of LLM agents, revealing their tendencies towards prosocial behaviors and irrational decision-making. The CogMir framework is applied to various cognitive bias scenarios, demonstrating high consistency between LLM agents and human behavior under uncertain conditions. The paper contributes to the understanding of LLM agents' social intelligence and provides a platform for further research.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Important research question: With the rapid development and application of LLM agents, the behavior studies of LLM agents especially under uncertain situations are getting more and more important.
2. Innovative framework: The introduction of CogMir is a significant contribution, offering a new way to evaluate and understand the social intelligence of LLM agents.
3. Open-ended design: CogMir's modular and dynamic design allows for continuous interpretative study and adaptation to future research needs.

Weaknesses:
1. The paper tries to demonstrate *LLM Agents can leverage hallucinations to mirror human cognitive biases*, while the experiments do not show how to measure hallucinations and what role hallucinations play here.
2. Human subjects are included in the experiments, while the recruitment and the details of them are missing. The paper claims LLM agents' behaviors are similar to humans, but there is no quantitative comparison between these two.

Limitations:
The paper includes the study of human-AI interaction but the ethical risks are not clearly stated. Such risks may also limit the research and application of this field.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores the potential of LLM agents to exhibit irrational social intelligence by mirroring human cognitive biases through their hallucination properties. The authors propose CogMir, a modular and dynamic multi-LLM agent framework that utilizes hallucination to assess and enhance social intelligence through cognitive biases.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1: The experiments explicitly compare LLM agent responses with known human cognitive biases, providing valuable insights into the similarities and differences between human and LLM decision-making processes.

S2: CogMir’s modular structure allows for flexibility in configuring experiments and exploring different social scenarios, making it adaptable for various research needs.

S3: CogMir’s open-ended nature encourages collaboration and further research, promoting the development and refinement of LLM agent social intelligence evaluation methodologies.

Weaknesses:
W1: The framework primarily focuses on language-based interactions, neglecting the simulation of non-verbal behaviors and their impact on social intelligence, limiting the scope of the analysis.

W2: The framework primarily focuses on language-based interactions, neglecting the simulation of non-verbal behaviors and their impact on social intelligence, limiting the scope of the analysis.

Limitations:
See limitation

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
aYJ2T5TXoX;"REVIEW 
Summary:
The paper tries to formalize the concept of generalizability in experimental studies in machine learning research. It relies on three different types of kernels in order to quantify difference between the rankings in an experiment output. A core contribution is the development of an algorithm for estimating the number of experimental studies needed in order to generalize the results at a desired level.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is interesting and worthwhile.
- The paper is clearly written.
- The formalization of generalizability is well-defined and nicely parameterized through the use of kernels.
- The practical usefulness of the algorithm is somewhat unclear to me.

Weaknesses:
- There is no discussion on the computational costs of the algorithm (except for a vague statement that it is very fast in the checklist).
- The empirical evidence for the algorithm's effectiveness appears somewhat weak to me (see respective item in _Questions_).
- The python package is not properly configured I think. I see this after having installed the package into a virtual environment with the correct python version and
  using `pip install . -r requirements.txt`:

  ```python
  In [1]: import genexpy
  ---------------------------------------------------------------------------
  ImportError                               Traceback (most recent call last)
  Cell In[1], line 1
  ----> 1 import genexpy

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/__init__.py:4
        2 from .src import lower_bounds
        3 from .src import mmd
  ----> 4 from .src import probability_distributions
        5 from .src import rankings_utils
        6 from .src import relation_utils

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/src/probability_distributions.py
  :11
        8 from typing import Literal
      10 from genexpy import kernels as ku
  ---> 11 from genexpy import rankings_utils as ru
      12 from genexpy import relation_utils as rlu
      15 def sample_from_sphere(na: int, n: int, rng: np.random.Generator) -> np.ndarray[float]:

  ImportError: cannot import name 'rankings_utils' from partially initialized module 'genexpy' (most l
  ikely due to a circular import) (/home/<anonymous_reviewer>/.pyenv/versions/genexpy/lib/python3.11/site-packages
  /genexpy/__init__.py)

Limitations:
Some of the limitations are discussed but I still think the paper could be more self-critical of for instance $n^*$. Possible computational costs are also not discussed.

There are no potential negative societal impacts of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper deals with experimental studies. After providing a mathematical formalization, it focuses on the generalizability of these studies. The main contribution is a quantitative estimate of the the size of the study to obtain generalizable results. Experiments on LLMs are conducted.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- [mathematical formulation] It is nice to have a solid formulation of experimental studies, this is quite relevant to the community.

Weaknesses:
- [train / test split] A concrete problem in machine learning practical experimentation is that of train / test split, and more particularly its absence (that is, training on the test). I do not see this issue discussed in the paper. Can it be incorporated in the setting? Is it possible to clarify whether the paper assumes that the training is done on a training set without calibration on a validation set or is this hidden somewhere? What would then be the influence on the number of experiments?
- [testing between rankings] If I understand correctly, the paper proposes (in Section 4.1) to check whether rankings are consistent by performing kernel two-sample test, with adapted kernels. This does not seem standard to me: there exists some ad-hoc statistical tests (e.g., Kendall's \tau, Spearman's \rho, etc.). Why not use them directly? Is there an advantage to using MMD?  

- [minor comments]:
  - missing ref line 111
  - repeated word ('of') line 300

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to formalise the notion of an experimental study by considering the sampling process of acquiring a dataset.  It then uses this notion to argue about generalisability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem of understanding the performance of machine learning when tested on new data is a very important problem.  The authors use some technically sophisticate methods to tackle this problem.

Weaknesses:
For me the authors model of an experiment is too simplistic and does not capture the problems faced by machine learning.  If we collect medical data then that data is likely to vary depending on the equipment used, the clinicians running the equipment and population where the data comes from.  These kinds of variations are the bugbear for machine learning, but not captured at all by the model.  Another issue is that a lot of data is non-stationary.  Even in the much used example of checkmate in one.  If a machine learns this very well, then players against the machine are likely to learn their mistake and alter their play.  Thus, I am not convinced that the model being proposed is particularly interesting.

Limitations:
This is fine.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a formalism for the generalizability of experimental studies in ML.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Anything pushing to get better practices in evaluation of ML is very important.

Weaknesses:
I could quibble with some of the setup, which is a bit confusing to me: design factors being properties of the context rather than of the alternative, for example, is kind of odd, but I don't think this is very important.

The big problem is that there is a huge literature on a very similar problem and I have very little sense of how this work connects to it: starting with the Neyman-Pearson lemma and going down through the standard corpus of decision theory, we have a lot of statistical tools for thinking about this problem in very broad strokes. After reading this paper, I have a sense that you're trying to solve a very similar problem (given a sample from some population, what can I say about the reliability of my estimate? How many samples would I need to be sure that it's reliable?)

Section 4.3 seems to be rederiving some form of power analysis.

Looking at A.3.3, it appears that the procedure is essentially the following:
(1) [the inner loop from 1...n_rep] construct a null hypothesis at a given sample size, find the upper alpha quantile of that null hypothesis
(2) Repeat this at a variety of sample sizes
(3) Estimate a power-law relationship between sample size and the upper-alpha quantiles
(4) Predict the sample size which would have such an upper-alpha quantile

This procedure is an empirical version of power analysis where the null distribution is not known but simulated and extrapolated. If I know the type-I error rate, type-II error rate and a distribution under the null and under the alternative, deriving the required sample size is straightforward. Indeed, we have a CLT for MMD (at least under some kernels) [1], so these distributions are known asymptotically, which is likely plenty for the purposes of sample size determination. Do \alpha^* and \delta^* map onto concepts from Neyman-Pearson? It's entirely possible.

This is an important question because decision theory has very well established results on things like uniformly most powerful tests. When we just invent a new framework rather than relying on well-trod ones, we are likely to derive suboptimal procedures unless we compare very carefully to these existing procedures. There's no similarly sophisticated discussion of error properties in this paper, which would be reasonable if this were truly the first paper in its vein, but I don't think that's the case.

Further, it's not clear to me why these similarities between rankings should be the target of inference. Rather, shouldn't I care about whether, based on the sample of allowed-to-vary factors I've used, alternative A is preferred to alternative B? This is an extremely standard matter of decision theory as far as I can tell. By moving to these more complicated research questions about rankings it clouds this fact, but I'm not sure it needs to. If the target of inference were instead to be a rank of K alternatives, I believe a decision theorist would take a somewhat similar approach to what you've done here: define a similarity metric based on the research question. An example solution to a problem like this would be [2], [3]. I just don't see why we need this new framework to accomplish a task I think we already have the tools for.

It's entirely possible that there's a contribution here, but it can't just be ""this is a new task"". We have methods from decision theory that have been designed for a wide range of decision tasks, and its incumbent upon the authors to demonstrate why those existing tools do not fit the task in front of them.

[1] https://www.jmlr.org/papers/volume24/22-1136/22-1136.pdf
[2] https://onlinelibrary.wiley.com/doi/abs/10.1002/mcda.313
[3] https://www.sciencedirect.com/science/article/abs/pii/S0377221715008048

Limitations:
see above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a new mathematical framework and a corresponding new algorithm to evaluate the generalizability of published experimental studies, by adapting Montgomery's classification of experimental factors [44].
They demonstrate the efficacy of this framework in evaluating the generalizability of two popular published experimental studies.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper appears to be theoretical strong. 
It goes beyond the standard notion of reproducibility of an experimental study and comes up a definition of generalizability of an experimental study.
Morever, Proposition 4.2 provides a theoretical result on the sample size $n$ necessary to obtained a desired generalizability $\alpha^*$ for a desired similarity $\epsilon^*$, and the authors also provide an algorithm (A.3.3) to compute this sample size.
Since the similarity $\epsilon^*$ is hard to specify, they make it a function of the kernelized distance between rankings $\delta^*$.

2.
The empirical evaluation in Fig. 2 and Fig. 3, on the categorical encoder comparison from [41] or the BIG-bench framework for LLM comparison from [55], respectively, demonstrates the practical utility of the proposed approach in determining sample sizes to guarantee generalizability.

Weaknesses:
1. The clarity in the writing can be significantly improved:

1.A. Symbols are used before defining them, typos exist, and symbols are not used consistently:

1.A.a. On line 118, the symbol $\mathcal{R}_{n_a}$ is mentioned, but the relation of this symbol to the ranking on alternatives only becomes clear later in Definition 3.1.

1.A.b. The Section number is missing on line 111.

1.A.c. The symbol $m$, is defined as the number of shots, on line 115, whereas line 114 uses the symbol $n$ rather than $m$. Moreover, on line 88, $n$ is defined as the number of shots.

1.A.d. In contrast to 1.A.c, in eq. (1), after line 170, the symbol $n$ is now used without providing a definition. It now appears to be the size of any study, in a general definition, rather than the number of shots, as defined on line 88. 

1.A.e. On Sec. 5.3, line 315, $N$ is defined as the number of preliminary experiments, whereas on line 154, it is defined as the size of the sample of valid experimental conditions. Do these mean the same thing ?

1.B. Sec. 3.1 defines a ranking of alternatives as the primary result of an experimental study.
However, the effect size, i.e., the magnitude and sign of the difference between two alternatives, can be important in certain experiments.
The MMD kernel, used in Sec. 4.2, actually allows measuring this effect size, as discussed in [27], but the limitations imposed by the usage of this MMD kernel within the author's generalizability framework, are not clear despite the somewhat cryptic discussion in Sec. 6.

2. 
The experimental evaluation is limited to a comparison of ranking differences between alternatives, and does not include a measurement of the practical differences between alternatives, or the significance of these differences.

Limitations:
Please refer to the potential limitation underlying weakness #2. Is it possible to quantify the magnitude of differences between alternatives using the generalizability framework provided by the authors ? The authors mention this limitation in Sec. 6, but it is not clear why the MMD kernel cannot quantify magnitude of differences.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ZJZqO4grws;"REVIEW 
Summary:
This study presents a contrastive regularizer to improve meta-learning. Specifically, the authors propose to incorporate a contrastive meta-objective that improves the alignment and the discrimination abilities of meta-learners, leading to better task adaptation and generalization. The authors demonstrate empirical effectiveness of the proposed ConML across several meta-learning and in-context learning scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The introduction of contrastive regularization sounds intuitively straightforward and motivating. 
2. While equipping meta-learning with contrastive objective is not a new concept (e.g., [1], [2]) the implementation covers major meta-learning methods, including optimization- metric- and amortization-based methods. This means that the study is more comprehensive than previous studies.
3. The numerical results are promising. Code is provided - reproducibility is commendable.


[1] Gondal et al. Function Contrastive Learning of Transferable Meta-Representations. In ICML 2021.
[2] Mathieu et al. On Contrastive Representations of Stochastic Processes. In NeurIPS 2021.

Weaknesses:
My primary concern lies in the specific contrastive strategy employed.

1. The contrastive objective aims to minimize intra-task distances while maximizing inter-task distances. However, the absence of appropriate regularization or constraints raises concerns about potential model representation collapse. This collapse could manifest as representations converging to trivial solutions, such as constant vectors or confinement to low-dimensional subspaces. The authors should address whether they have considered these risks. 
2. In addition, I wonder why the contrastive objective does not follow commonly studied ones, e.g., InfoNCE, in contrastive learning.
3. Moreover, the meta-objective necessitates computations involving representations from different tasks within a batch during each episode. Since the paper lacks a discussion on training and inference efficiency, the impact of this strategy on scalability is unclear.
4. The effectiveness of this method is likely dependent on hyperparameters tuning and the sampling strategy for creating subsets of tasks. As aforementioned, incorporating contrastive learning into meta-learning is not something completely new, even though, the authors do not include detailed discussion on how different strategies would affect the performance/efficiency, which is something to be expected on my end.

Limitations:
The authors do have discussed the limitation of this study.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to enhance the meta-learning process by implementing more robust supervision within the model space. Specifically, the authors seek to augment learning capabilities through model alignment and discrimination, aiming to approximate human-like rapid learning abilities. They propose that models trained on tasks within the same super-task exhibit similarity, while those trained on different task sets generalize effectively across diverse tasks. To achieve this, the authors devise a contrastive framework that remains independent of the specific meta-learning algorithms employed. This framework encourages closer alignment of representations for models adapted to similar tasks while pushing representations apart for models derived from dissimilar tasks. Moreover, the framework seamlessly integrates with optimization-based, metric-based, and amortization-based meta-learning methods. The approach demonstrates improvements across standard benchmarks in all evaluated scenarios. Furthermore, the proposed method shows promise for integration into the in-context learning of large language models, resulting in observed enhancements.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
•  The paper is well-written and easy to understand. It's reasonable to enforce the model to emulate human learning capabilities through alignment and discrimination.

•  The proposed contrastive learning framework is versatile and applicable to most meta-learning methods.

•  The proposed method consistently improves upon existing meta-learning methods across standard benchmarks.

Weaknesses:
•  The paper lacks validation on MetaDataset[A], which is a common large-scale dataset for few-shot learning tasks.

•  There is a need for sensitivity analysis on certain hyperparameters, such as λ and the choice of similarity function for contrastive learning.

[A] Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples, ICLR2020.

Limitations:
Please see weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals learning to learn (meta-learning) problem, from the perspective of exploring inner-task and intra-task relationship. Specifically, this paper proposed a Contrastive meta-objective by exploring intra- and inter-task distances and severed as an additional term for training objective (in addtion to classification loss). 

Experiments are conducted on both conventional few-shot image classification and in-context learning settings. Common benchmarks are used to compare with simple few-shot methods such as MAML, ProtoNet, SCNAPs. For In-context learning, simple synthetic functions are used for comparison.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The overall method makes sense. Designing intra- and inter-distance to explore the task-level contrastive information and further introduce this into to the meta-learning objective is reasonable. The diverse tasks naturally compose the contrastive pairs, useful for training. 
- The proposed method is general and can be applied on top of different few-shot classification/regression methods, such as metric-based, optimisation-based, simpleCNAPs, and in-context learning. 
- The performance gains over these simple baselines are significant, showing the effectiveness of the proposed method.

Weaknesses:
- Technically, the proposed contrastive meta-objective is similar to the idea of supervised contrastive learning, which already provides good insights to the representation learning and deep learning community. Therefore, the proposed method is kind of incremental and provides less new knowledge to the field. 
- The method is only verified on top of simplest baseline methods (MAML, ProtoNet, etc). In meta-learning, various works have been proposed to investigate the possible exploration of the task-level information for improved meta-learning, such as [R1-R4], to name a few. 
However, none of those previous efforts were discussed or compared. Only beating the naive baseline cannot comprehensively demonstrate the advantages of this paper. 
- Experimentally, the proposed method tries to show its superior performance over simple baselines rather than SOTA. This is less convincing. 
- The in-context learning experiments are only on simple synthetic data, lack of significance. 
- The tile and scope: Learning-to-learn is very general, but in fact only classification related experiments are conducted. By convention, the learning-to-learn approaches will also verify on reinforcement learning. 

[R1] Fei, N., Lu, Z., Xiang, T., & Huang, S. (2021). MELR: Meta-learning via modeling episode-level relationships for few-shot learning. In International Conference on Learning Representations.   
[R2] Agarwal, P., & Singh, S. (2023). Exploring intra-task relations to improve meta-learning algorithms. arXiv preprint arXiv:2312.16612.   
[R3] Han, J., Cheng, B., & Lu, W. (2021). Exploring task difficulty for few-shot relation extraction. arXiv preprint arXiv:2109.05473.   
[R4] Zhang, Tao. ""Episodic-free Task Selection for Few-shot Learning."" arXiv preprint arXiv:2402.00092 (2024).

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a contrastive meta-objective that can be applied to various meta-learning methods. Also, interpreting in-context learning as a meta-learning formulation, extended the proposed method to in-context learning. Specifically, the objective is to contrast task identity obtained after episode optimization. The task identity is defined as the model weight or the feature obtained by feed-forwarding, in the case of in-context learning. Finally, the authors demonstrated the superiority of the proposed method by applying it to several meta-learning methods to improve their performance.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The proposed method can improve diverse meta-learning methods.
- Contrasting task identity sounds intuitive.

Weaknesses:
In general, the text is not easy to understand.

- Not self-stained figures.
  - In Figure 1, it's hard to understand what $h_{w_i}$ and $w_i$ are since the caption has no explanation.
  - In Table 1, the caption could have included the definition of $g$ or $\psi$.
  - Figure 2 is hard to read; (b) and (e) missed the x, y-axis meaning and are too small to see something.
- Experiment details are missing.
  - Hard to understand Section 5.1. Though the sine wave regression problem is well-known in this domain, it's hard to interpret results without task definitions.
  - The tasks in Section 5.3 are not clearly defined.
- An ablation study would be helpful.
  - How to decide distance function $\phi$?
  - What if increasing the number of task-sampling $K$?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Z2f4Laqi8U;"REVIEW 
Summary:
Authors propose a greedy structure learning algorithm in the context of ancestral graphs. The score function is based on the normalized likelihood. The score is decomposable using the concept of ac-components. Limited experimental results are presented as experimental evidence.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The multivariate cross-entropy decomposition is explained in details, especially the link between its definition and its application in the context of ancestral graphs. Leveraging this framework allows a flexible and effective definition of scoring criteria for the class of graphical models.
- The usage of ac-connecting paths and ac-connected subsets for the likelihood decomposition (theorem 1) is the main result of the paper and allows to map the structure of an ancestral graph to a specific estimand.

Weaknesses:
- In the case of ancestral graphs, it's crucial to understand the differences between edge patterns. I would suggest to refactor Figure 1 to improve readability, e.g. isolating different edge patterns combinations with horizontal lines.
- The performance of the proposed solution is poor. For instance, Figure 2 most of the metrics overlap with existing solutions.
- The experiments keep the number of samples fixed even is the number of the parameters of the model increase, reducing the information available for the biggest models. Moreover, a sample size of 100 for BARLEY (114,005 parameters) is rather unrealistic.

Limitations:
- The experimental evidence is poor: few reference models with rather sparse hyperparameters space exploration.
- The search and score algorithm limits the number of possible exploration paths to the barely minimum.
- It is unclear what is the actual computational burden of the proposed score.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors present a novel likelihood decomposition of ancestral graphs, which is based on a novel concept ac-connected subset. With this decomposition, the authors present an efficient hybrid causal discovery method. But it is worthy to note that in the implementation of the causal discovery method, they use the approximate local scores limited to the closed surrounding vertices of each node and edge, but not the exact likelihood score.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The likelihood decomposition of ancestral graphs is very novel and could be useful in the future.
2. For me, I encourage the studies on the MAG/PAG learning methods. It is a hard and fundamental problem.
3. The paper is clearly written.

Weaknesses:
See questions.

Limitations:
No.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper defines a score for ancestral graphs through an inclusion-exclusion expansion and implements it into a hybrid search approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The likelihood decomposition (Theorem 1) is nice and there seems to be a slight improvement in the empirical testing.

Weaknesses:
There are close overlaps to published and preprint works by Hu and Evans, so it is hard to assess the novelty here. For example Theorem 1 (in another form) is from reference 14. 

Then in the simulation studies, relevant methods from the references are excluded, even though the greedy search approaches (like references 13 and 32 for example) are not exact, but also heuristic. These (and similar) would need to be included in the benchmarking to judge the importance of the work.

Limitations:
The limitation on the depth of the inclusion/exclusion is discussed in the limitation section, but maybe this wasn't so clear throughout the paper.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a greedy search-and-score algorithm for causal structure discovery in ancestral graphs which allow for both directed and bidirected edges.The paper provides an explicit decomposition of the likelihood function of ancestral graphs
in terms of multivariate cross-information over relevant ‘ac-connected’ subsets of variables which is related to head-and-tail factorization developed in prior work. In the main theoretical result authors show that cross-entropy and the likelihood function of an ancestral graph can be decomposed in terms of multivariate cross-information for ac-connected components. As a corollary this implies that two ancestral graphs are Markov equivalent if and only if they have the same ac-connected subsets of vertices.
Authors use the likelihood function decomposition in terms of cross-information of ac-connected componenetsto propose a search-and score algorithm that computes scores for nodes and edges and probes orientations of edges to minimize the score.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper proposes a new empirical algorithm for causal structure discovery in ancestral graphs. The practical algorithm is motivated by a theoretical decomposition of likelihood function in terms of multivariate cross-information summed over ac-connected components. The developed algorithm is scalable to graphs with several dozens of vertices and links (thousands of unknown parameters)

The authors provide experimental results on both synthetic datasets and challenging benchmarks from the bnlearn repository, showing that developed algorithm typically has higher precision and similar recall to prior MIIC algorithm.

Weaknesses:
The proposed algorithm seems to be only empirical and lacks theoretical guarantees and relies on MIIC as a base of the algorithm. To my understanding, the paper does not provide a guarantee that the algorithm always converges in a reasonable number of steps (even if not to a global minimum). Multivariate cross-information over a large set of variables may be very sensitive to noise, which I think is a weakness compared to algorithms that use cross-information only over small number of (e.g. triples) of variables.

Also as the authors point out Theorem 1 is equivalent to the decomposition established in prior work, which makes novelty of the main theorem somewhat questionable.

I also think that it will be beneficial to the reader if the connection between the theoretical results in Theorem 1 and the proposed algorithm are explained a bit more deeply with a more clear connection of how that theorem is being used.

Limitations:
na

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Yq2dYPkfRU;"REVIEW 
Summary:
This paper studies the generalization measured by gradients via a uniform gradient stability. For $\beta$-uniformly stable algorithms, the paper gives generalization bounds of order $O(1/n+\beta+\sqrt{E_Z[\|\nabla f(A(S);Z)\|_2^2]/n})$, which yields fast rates if $E_Z[\|\nabla f(A(S);Z)\|_2^2$ is small. The paper then uses this generalization measured by gradients to derive generalization error bounds under a PL condition. Applications to empirical risk minimization, gradient descent and stochastic gradient descent are given for strongly convex, smooth and Lipschitz problems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper gives high-probability bounds for generalization gap on gradients, which include the gradient norm at the minimizer. This can imply fast rates in an interpolation setting. Under the PL condition, this gives fast rates of order $O(1/n^2)$.

The paper provides comprehensive applications to several algorithms such as empirical risk minimization, gradient descent and stochastic gradient.

Weaknesses:
The high-probability analysis based on uniform stability follows largely from existing work. I do not see enough novelty in the analysis. It would be helpful if the authors can summarize the challenges in the analysis and their novelty. 

As stated in the paper, Theorem 1 and Theorem 2 only improve the existing results by a constant factor. This improvement is not significant. 

As stated in the paper, the generalization by gradients is mostly interesting for nonconvex problems. However, for the applications in Section 4, the paper considers strongly convex problems. Also the results require smoothness and Lipschitz continuity. These assumptions seem to be a bit strong.

The paper gives fast rates under the case $F(w^*)=O(1/n)$ in Section 4. Note that Section 4 considers strongly convex problems. Then, the objective function should be of the form $F(w)=G(w)+\mu\|w\|^2$, where $G$ is related to loss. Then, if we require $F(w^*)=O(1/n)$, one needs $\mu\|w^*\|^2=O(1/n)$. Suppose we assume $\|w^*\|=O(1)$. Then, this requires $\mu=O(1/n)$. In this case, the generalization bound would be vacuous since $n\mu=O(1)$.

For SGD, the computation cost seems to be high. For example, in Theorem 6, the paper requires $T=n^4$ while in Theorem 13 the paper requires $T=n^2$. This high computational cost may not be appealing for large-scale problems.

In the proof of Lemma 1, the paper uses $\|\nabla F(A(S))\|_2\geq \mu\|A(S)-w^*\|$. This inequality does not generally hold under a PL condition. Indeed, Theorem 2 in Karimi et al 2016 require $\|A(S)-w^*\|$ to be replaced by the distance between $A(S)$ and the set of minimizers.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work shows high probability excess risk bounds of $O(1/n^2)$ for several algorithms under strong convexity, smoothness, Lipschitz continuity and low noise assumptions using algorithmic stability.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The results of the paper are interesting, showing a risk bound of $O(\frac{1}{n^2})$ using algorithmic stability.

2. The paper uses a novel technique, involving the stability of gradients to demonstrate excess risk bounds and presents applications of this technique in convex optimization.

Weaknesses:
1. The problem setup of the paper is not clearly detailed before the technical section, including the assumptions used for proving the results.

2. The presentation of results and related works is somewhat lacking. It would be beneficial if the authors summarized the results from previous work and compared them to the results in the current paper, including the set of assumptions made in each work.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper achieves the high probability excess risk bounds $\mathcal{O}(1/n^2)$ for empirical risk minimization, projected gradient descent and stochastic gradient descent under strong convexity, smoothness and Lipschitz continuity assumptions.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Please see Summary.

Weaknesses:
1.The paragraph from line 52 to line 65 first analyzes the non-convex problems. Then, it suddenly mentions that this paper explores the stability of stochastic convex optimization algorithms with strongly convex losses in line 61. And, it doesn’t mention its non-convex analysis. It is so confusing. So, I suggest authors rewrite this paragraph to benefit readers’ understanding.

2.The paragraph from line 70 to line 73 is unnecessary since we can not obtain any useful information.

3.In Related Work, it is unnecessary to list the literature related to uniform convergence since it is not helpful for readers’ understanding of the contributions of this paper. It may be better that authors list the detailed literature related to high probability bound.

4.Theorem 1 in this paper is not the sharpest p-moment bound for sums of vector-valued functions. Authors demonstrate their bound is indeed tighter than Theorem 1 of [1]. However, [2] also provided a bound (Theorem 1) that is likely tighter than Theorem 1 in this paper. Note that, [2] also used Marcinkiewicz-Zygmund’s inequality to prove their bound. Besides, the paragraph from line 131 to line 136 states “On the other hand, in Section 3.2, we will carefully construct vector-valued functions which satisfies all the assumptions in Theorem 1 and ensures M = 0 at the same time. Under this condition, we can eliminate the first term.”. This point is also considered in Theorem 1 of [2]. I think that authors just consider the improvement to [1], but omit other related work.

[1]J. Fan and Y. Lei. High-probability generalization bounds for pointwise uniformly stable algorithms. Applied and Computational Harmonic Analysis, 70:101632, 2024.

[2]X. Yuan, P. Li. Exponential generalization bounds with near-optimal rates for $L_q$-stable algorithms. ICLR, 2023.

5.In Section 3.2, authors build some relationships between generalization error and stability parameter $\beta$. Authors think these relationships are under non-convex, non-smooth, non-PL conditions. These bounds are not the final generalization bounds but the relationships. After giving the stability bounds, the generalization bounds are finally determined. However, in Section 4, authors provide the stability bounds under (strongly) convex and smooth conditions. Therefore, authors didn’t remove (strongly) convex and smooth conditions for generalization analysis.

6.The symbol $M$ is repeatedly used in Theorem 1 and Theorem 2. Therefore, I suggest authors should carefully check their symbol settings.

7.In Remark 4, authors compare their Theorem 3 with the bound in [3]. It is unfair since, as mentioned in the above 5., the bound in [3] is a final result but Theorem 3 is not.

[3]Y. Xu and A. Zeevi. Towards optimal problem dependent generalization error bounds in statistical learning theory. Mathematics of Operations Research, 2024.

8.In line 633, $F(A(S)) - F(A(S))$ is wrong.

9.In line 633, Equation (31) should be an inequality.

10.In line 152, authors state “In nonconvex problems, we can only find a local minimizer by optimization algorithms which may be far away from the global minimizer. Thus the convergence does not make much sense in function values.”. So, they use uniform stability in gradients. However, in Section 4, they provide some uniform stability bounds in gradients for strongly convex problems. It is a paradox.

11.The form of the relationship in Theorem 3 is very normal. The method to obtain an excess risk bound $\mathcal{O}(1/n^2)$ is very simple. I think other normal generalization results (like [1]) in gradients can derive the excess risk bound $\mathcal{O}(1/n^2)$. The main contribution of this paper may be the simple method combining PL condition with some usual decompositions as shown in Proof of Remark 5. However, as mentioned in the above 10., the uniform stability in function values is more unreliable than the one in gradients under strongly convex condition.

Limitations:
Considering the 4. in Weaknesses, I suggest the author reconsider whether their result is the sharpest.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the standard statistical learning setting and derives $\mathcal{O}(\frac{1}{n ^ {2}})$ ($n$ denotes the number of samples) high-probability bounds for the excess risk $F(A(S)) - \inf_{w} F(w)$ ($A$ denotes the algorithm and $S$ denotes the training set) of ERM, PGD, and SGD. The best-known bounds prior to this work were $\mathcal{O}(\frac{\log n}{n})$ for ERM, PGD, that was derived using algorithmic stability, and $\mathcal{O}(\frac{1}{n ^ {2}})$  or ERM, SGD that was derived using uniform convergence. However, the latter demanded $n = \Omega(d)$ samples, thereby introducing the an undesirable dependence on $d$. The current paper shows that it is possible to obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds for the algorithms (without any dependence on $d$) under the lens of stability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The considered problem is interesting and the contributions of the paper show that sharper bounds (at par with uniform convergence, albeit without any dependence on $d$) are possible for ERM, SGD, and PGD for strongly convex and smooth stochastic convex optimization. Below I explain the roadmap taken by the authors in doing so, which also sheds light on some of the other aspects of the paper.

1) The authors first study the generalization gap via gradients, i.e. the quantity $\||\nabla F(A(S)) - \nabla F_S(A(S)\||$ for the statistical learning setting under the assumption that the function is Lipschitz and the algorithm is uniformly stable in gradients (Theorem 1 and 2). Under the nonconvex setting, the authors obtain dimension-independent bound for the generalization gap via gradients. This bound is subsequently studied under the assumption that the function is smooth and satisfies the Polyak-Lojaseiwicz (PL) condition. The obtained bound is a function of the gradient norm obtained at the end of optimization, i.e. $\|| \nabla F_S(A(S) \||$. 

2) To obtain excess risk bound for the algorithms, the (i) authors show that the algorithms are uniformly stable in gradients; (ii) translate the excess risk bound to a bound on the gradient via the PL inequality (recall from the premise that these algorithms are analyzed in the strongly convex and smooth setting of stochastic convex optimization, therefore PL holds vacuously); (iii) use triangle inequality to relate the bound on the gradient norm to the generalization gap, and use the bound on the generalization gap via gradients (see 1 above).

Weaknesses:
I found several typos in the main theorems in section 4. For example, the stability equation in Lemma 4 should be written with respect to the output at the iteration instead of the output of the ERM.  Also, why is the reference in Theorems in section 4 to Theorem 3, instead of Lemma 1? From my understanding (explained in the Strengths above), Lemma 1 is a specific instantiation of Theorem 3 to smooth + PL functions, which is exactly the premise in Section 4. What exactly is $w$ in the bound $F(w) - F(w^\star)$ in Theorem 5? I expect it to $w_{T + 1}$ (or $w_T$).

I don't see the point of Marcinkiewicz-Zygmund’s inequality with improved constants. The whole paper is about the improved dependence with respect to $n$, so I don't see a point in improving the specific constants in the inequality. 

I need some more clarification in lines 204--207. The authors state that Klochkov and Zhivotovskiy obtained $\mathcal{O}(\frac{1}{n})$ style bounds for the excess risk. What's the assumption on $f$ considered by them? The authors mention that they can obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds with an extra PL and smoothness assumption. Is this something for which Klochkov and Zhivotovskiy, 2021 could only obtain a suboptimal $\mathcal{O}(\frac{1}{n})$ bound? Earlier, my interpretation was that this work obtained $\frac{1}{n}$ bounds for ERM, and PGD for smooth and strongly convex stochastic convex optimization, but lines 204--207 made my understanding unclear. I want to make sure the authors are not invoking extra assumptions to get improved dependence.

Along similar lines as above, Lines 259--261 seem to be saying inconsistent things (is the assumption just smoothness, or strong convexity and smoothness). I would appreciate clarifications from the authors.

Minor Typos: 1) In definition 1, $\gamma$ and $\mu$ should be strictly positive; (2) Line 182: $\gamma$-smooth instead of $\gamma$-smoothness.

Based on the authors' responses, I would be happy to revise my assessment of the paper.

Limitations:
Yes, the authors have adequately addressed this.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
YZoGmJrOS9;"REVIEW 
Summary:
This work presents an analysis of in-context learning (ICL) for a variety of hybrid architectures (composed of different blocks from preexisting large language model architectures) on different regression tasks. The experiments are built on top of a couple of prior works [1, 2] that also explored ICL in similar contexts. This paper highlights that several prior results can be reproduced, and for novel hybrid architectures which are the main focus of this work – most of them converge to optimal solutions while some others can escape suboptimal solutions or even fail to converge in the first place. The authors also propose a new metric “ICL regression score” to evaluate ICL performance in comparison to a known baseline. The modularized code for this work is publicly available for the broader scientific community.


[1] Garg S, Tsipras D, Liang PS, Valiant G. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems. 2022.
[2] Park J, Park J, Xiong Z, Lee N, Cho J, Oymak S, Lee K, Papailiopoulos D. Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248. 2024.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The open source codebase with simple abstractions and interfaces to facilitate reproducibility, extensions, and modifications are a welcome contribution.
- The intuitive explanation behind the ICL regression score values in Figure 2(a) are well-appreciated and helpful to follow along the results.
- The authors evaluate multiple architectures and tasks and clearly outline what components they are using from prior works to build on top of.

Weaknesses:
- The paper is not very well-motivated. Why are hybrid architectures (especially the two that are focused on) important to study? What intuitions or profound reasons drive the authors to make the experimental design choices that they did? 
- Additionally, use cases for ICL itself are not well-motivated. Are there any practical use cases that warrant such extensive evaluation? The writing is not easy to understand for a reader not very up-to-date with the ICL literature. 
- The technical novelty of the work is limited.
- The results are presented in a manner where the performance metrics are reported for the 12 architectures and 5 tasks but it is not very clear what the reader or the scientific community working on ICL should take away from the results. Are there patterns regarding why certain hybrid architecture + task combinations make ICL shine compared to the baselines and why some others do not? A lot of the interpretation of such results is left to the reader to figure out. A lack of a deeper understanding and intuition about the reasons behind the results makes it hard to see solid/impactful takeaways that others could build on top of. 
- The authors mention and describe a 6th task Vector MQAR but do not report or discuss any of its results in detail in the main text of the paper. One figure is present in the Appendix but it is not explained and it is too hard to read the text in the figure.
- Some typos:
     - Line 161: Mention the word “**Figure**” before 2a.
     - Line 164: “**Figure 2b**” instead of “Table 2b”
     - Line 185: Park et al. [14] **show** that ..
     - Line 202: “Sparse Linear ~~on~~ adopts a suboptimal..”
- References to result tables (Table 3 for lines 213, 230)  and model descriptions (Figure 1a for lines 194, 204, 201, 16 etc.) could further enhance readability and the user’s understanding.

Limitations:
In terms of negative societal impacts, could the potential misuse of natural language tasks via the hybrid architectures presented be a legitimate concern? It is certainly out of scope of this work for evaluation but could be listed as part of the broader impacts section of the checklist.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors explore a number of different attention-based architectures along with Mamba on a series of in-context regression tasks. The architectures vary in their choice of normalization, positional encodings, activations, and hybridization with Mamba. They discover some varying capacities for the different ICL tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper appears to be closely related to [Park et al.](https://arxiv.org/abs/2402.04248), and I'm uncertain what additional scientific value it adds. As such, it's originality and significance appear to be limited.

Weaknesses:
As mentioned above, the present work seems to be closely related to [Park et al.](https://arxiv.org/abs/2402.04248), with little added insight. While an attempt has been made to explore minor architectural aspects like the choice of normalization, positional encodings, and activations, for the most part the changes seem to matter little (according to Table 3). I'm unsure what the key take-away is. Is there a particular architectural configuration that works best? What concrete practices can a user apply to improve their models' performance? The authors appear to have an ambition towards answering questions like these, but do not ultimately resolve them.

The results are sometimes difficult to interpret. At times, this is simply because the plots are unreadable, with text that is too small. I'm unsure how to interpret the in-context regression score. It looks like it's often <1 across models. Does this mean they all fail to outperform the baseline? Is this score comparable across different tasks?

A main objective of the paper is to compute numerics comparing models, but only one training run was executed for each experiment. If the compute budget is very limited, a more valuable approach may be to consider a smaller subset of models (e.g. keeping either GPT-2 or Llama, but not both) and simpler task parameterizations. Doing so will enable you to sweep across many more settings and increase your experiment replications, generating more convincing numerics. 

Additional minor formatting comments:
- Line 25: consider compressing citations (e.g. with sort&compress)
- Table 1: third hline from the top intersects with text
- Consider plotting figures with point-markers for each data point, to clarify where exactly your data points fall
- For related models that vary by parameter (e.g. training iteraitons), consider using the same color but with different shadings / style
- Figure 15: text is unreadable
- Figure text overall is small and hard to read

Limitations:
The authors adequately state the limitations of their approach.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors build on the ICL work of [1] and [2], wherein networks are ""trained to in-context learn"" several tasks of varying complexity (e.g., Linear Regression, Sparse Linear Regression, Vector MQAR, etc.).  Carrying on from [2], the main contribution of the presented work is the combination of various permutations of the attention/SSM blocks from GPT-2, LLama-2, and Mamba models.  Such permutations include swapping the Layer Norm in GPT-2 attention blocks with RMSNorm, the LLama-2 SwigGLU with a Mamba Mixer, and so on; in total, 9 different hybrid combinations are considered.  For each task and specific hybrid model, the model is trained over task samples for 500k steps, then evaluated on ICL performance on that task.  In addition to reporting squared error per task, the authors also propose a new metric, called the ""ICL regression score.""  The results of specific model-task pairs are plotted and several trends are discussed.

[1] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583–30598, 2022.
[2] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
**Originality** - while the authors largely adapt the ""trained to in-context learn"" framework from other works, they consider new model configurations.  This can be useful to determine what permutations of model components leads to failure modes in ICL.

**Quality** - In considering 9 different model configurations for 8 different tasks, the author thoroughly consider a large number (72) of different LLM building blocks and their ICL capabilities across tasks of varying complexities.

**Significance** - The framework of [1] has grown as an interesting alternative to standard ICL, as a quick means to assess the ICL success and failure modes of different models.  Thus, the presented work can aid in flushing out this information over new LLMs and previously proposed tasks.

[1] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583–30598, 2022.

Weaknesses:
### Clarity
There is significant room to improve the clarity of the presented work.  Currently, several important details are missing (discussed below), key concepts are not fully explained, and the current paper could benefit from an editorial pass (it is currently difficult to read).  It is also important to note that, while the authors presented the results of model + task pairs and detailed where models failed, no possible explanations and follow up ablation studies were conducted; the question of ""why"" remains for all the presented results.  E.g.,
> Specific hybrid architectures can hesitate to learn/converge for certain function classes.

Do the authors have intuition or an explanation which can be explored to explain this?  Arguably, this is one of the most important contributions to be made in a large empirical review like the presented paper, i.e., to make sense of the experiments to gain a greater intuition for why an LLM behaves in a certain way.

Wrt important missing information:
- ""We replicate the function classes Linear Regression, Sparse Linear Regression, 2-Layer MLP Regression, and Decision Tree Regression from Garg et al. [6] as they present a wide range of ""difficulty"" for sequence models. In addition, to capture the existence of some ICL ability, we also regress onto the two function classes examined in Park et al. [14]: parity function with induced sparsity (Sparse Parity) and parallel associative recall (Vector MQAR)."" <- - How training instances are produced per task? How many test samples are produced per task?  If this follows Garg et al., then each model is trained *from scratch* on 40 samples per task. Can you please clarify and state these in the main text?
- It is not clear	what the author's mean by ""zero estimator.""  Is this the zero shot prediction? Correspondingly, it is not clear exactly what the presented ICL regression score represents.
- ""To determine task-specific ICL ability, our sequence models regress onto the functions shown above [14].""  <- It would help to clearly state the paper trains the models ""from scratch"" to in-context learn, as in previous works.

Limitations:
The did well to state limitations of the presented evaluation.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents a codebase for benchmarking the in-context learning ability of language models, especially for hybrid models. In addition, several empirical results are presented to show that some model architectures fail entirely or have suboptimal performance on specific in-context learning tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The codebase for studying in-context learning ability could be useful to understand capabilities and limitations of hybrid models, which will accelerate research in this area.
* It is interesting to find that even a small change in architecture (e.g., adding RMS to GPT-2) will lead to noticeable differences on some tasks (e.g., sparse linear). It would be interesting to investigate the root reason behind that.

Weaknesses:
* I feel that it is hard to assess the contribution of this work. It seems that this work's main contribution is the implementation of the in-context learning ability benchmark codebase. While such a codebase is important and useful, I did not find what technical challenges the codebase is trying to address and the effectiveness of the codebase.
* Another contribution is the empirical findings of the relationship between architectures and per-task performance on in-context learning. However, I found the empirical results are not systematic and hard to interpret. I am unsure how these findings motivate future architecture design.

Limitations:
see above

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
YZWvf58dBS;"REVIEW 
Summary:
This paper introduces the TransLLM framework to transform English-centric chat LLMs to non-English languages, addressing the challenges of transferring advanced abilities without supervised data and preventing catastrophic forgetting of original knowledge. Key contributions include using the Translation Chain-of-Thought (TCOT) to divide the transfer process into sub-tasks, employing Low-Rank Adaptation (LoRA) and Recovery Knowledge Distillation (KD) to maintain original LLM parameters and recover knowledge. TransLLM's effectiveness is demonstrated through experiments transforming LLaMA-2-chat-7B to Thai, where it outperformed strong baselines and ChatGPT in multi-turn conversations and safety benchmarks, highlighting significant improvements in helpfulness and safety without extensive supervised data. This framework offers a solid foundation for developing safe and useful non-English LLMs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The introduction of TransLLM, which combines Translation Chain-of-Thought (TCOT) and recovery knowledge distillation, provides an effective method for transforming English-centric LLMs to non-English languages. This approach addresses both the transfer of advanced abilities and the prevention of catastrophic forgetting.
2. The method shows notable improvements in rejecting harmful queries and maintaining human preference alignment, as evidenced by outperforming GPT-4 and ChatGPT on the safety benchmark AdvBench, highlighting the robustness of the model in safety-critical applications.

Weaknesses:
1. The experiments are primarily conducted on transforming LLaMA-2-chat-7B to Thai, which may limit the generalizability of the findings to other non-English languages and other models. Further validations on other models (not necessarily bigger than the current one) or other size of Llama-2 would have strengthened the paper.
2. While the proposed method exhibits promising results in MT-bench and AlpacaEval, it is not tested on other traditional NLU benchmarks, just like MMLU in English. Incorporating more diverse evaluation benchmarks would provide a more comprehensive assessment of the model's performance across various language tasks.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a transfer pipeline, TransLLM, which uses a translation chain-of-thought (TCOT) to adapt English-centric large language models (LLMs) to low-resource languages. This pipeline consists of pre-training and supervised fine-tuning (SFT) phases. During the pre-training stage, the authors select monolingual data in the target language and translation parallel data for language modeling. This pipeline also uses an external translation model to construct TCOT dialogue data, which is used in the SFT phase. In the SFT phase, to avoid the catastrophic forgetting problem caused by continual learning, the authors adopted the LoRA PEFT method and further proposed the recovery Knowledge Distillation (KD) method. Specfically, the recovery KD method mixes responses generated by the original model during the SFT-based transfer phase. The effectiveness of this pipeline was demonstrated by successfully transferring the Llama-2-7B model from English to Thai. Compared to other baseline models based on open-source LLMs, the transferred model exhibited great multi-turn dialogue capabilities and safety on the Thai MT-bench and AdvBench benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. **Originality**: Compared to other machine translation-based transfer methods in the community, the proposed pipeline introduces the concept of Translation chain-of-thought and specifically addresses the catastrophic forgetting problem that might arise from SFT-based transfer methods, proposing effective solutions.
2. **Evaluation Protocol**: I appreciate the efforts made by the authors in the experimental section to ensure the effectiveness of evaluations for low-resource language (Thai). The authors employed professionals for evaluation and also validated the agreement between GPT-4's automatic evaluation and human evaluation on the MT-bench. Additionally, they used human translation in constructing some of the test data.
3. **Significance**: Currently, training data and resources for LLMs are predominantly English-centric. The proposed method helps build strong chat LLMs for low-resource languages and minority groups.

Weaknesses:
1. **Flexibility of Methodology**: Although the proposed method reduces the dependency on instruction-following data in the target language, it still relies on parallel corpora and external translation models for data construction (Translation pre-training data, TCOT data). If high-quality parallel corpora or models are not available, the proposed method might be infeasible. For example, even commercial translation systems cannot support translations for some endangered languages or dialects.
2. **Scope**: While the method proposed in this paper might be extendable to other low-resource languages, the authors only validated it on Thai, which lacks empirical evidence for the generality of proposed pipeline. I also noticed that the authors emphasized ""non-English"" in the title, but a broader range of non-English languages still requires exploration.
3. **Effectiveness**: The authors tested the method on Llama-2-7B. Experiments on other model series and larger models could further support the effectiveness of proposed pipeline.
4. **Quality of MT data**: The authors used Google Translate for translating TCOT and AdvBench data. Although this is a commonly used commercial system, it would be better to supplement the evaluation and report on the quality of these data translations to explore their impact. In the absence of reference translations, quality estimation methods like CometKiwi [1] and TransQuest [2] could be used.
5. **Typos**:
    - Line 45: instruct tuning -> instruction tuning

References

[1] Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022. CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634–645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

[2] Tharindu Ranasinghe, Constantin Orasan, and Ruslan Mitkov. 2020. TransQuest: Translation Quality Estimation with Cross-lingual Transformers. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5070–5081, Barcelona, Spain (Online). International Committee on Computational Linguistics.

Limitations:
The authors discussed some limitations after the conclusion.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors present a method for transforming a chat-based English LLM to Non-English (Thai is the only language experimented with) based on a series of steps that teach the LLM to take in a non-English query and respond in non-English for that query. The methods presented is referred to as TransLLM pipeline which comprises of extending the based model vocab and finetuning with LoRA in multiple stages -- comprising of target language pretraining (on monolingual target language data), translation pretraining and transfer finetuning. The experiments are done on LLaMA2-Chat-7B with Thai as the target language and show the model performs well on both translation into and out of Thai and obtains good performance wrt baselines on MT-Bench & Alpaca-Eval.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The paper presents a simple and scalable solution pipeline to adapt chat LLMs to new languages.

Weaknesses:
1. The experiments are done on only one LLM and on only one language (Thai). This severely constrains the extend to which the results could be verified.
2. The novelty of the proposed method is very thin and very limited analysis is done to motivate that novelty.

Limitations:
Limitations have been adequately addressed.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on a scenario as transforming a English-centric **chat** large language model to a non-English chat large language model (or not just en-centric). The authors want to address the catastrophic forgetting problem where further tuning on the original En-chat LLM without reusing their original SFT data will hurt their original chat abilities mainly in English. In terms of this, they introduce several techniques, including translation chain-of-thought, low-rank adaptation, as well as recovery KD, and they claim that with only single-turn Thai data, they can successfully transform English LLama2-chat-7B to Thai, while the performance in MTBench and some others remain competitive.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors propose a pipeline to tune the chat English language model to a non-english language model. The pipeline looks interesting and the performance looks good in MTBench, Th-translated MTBench, etc.

Weaknesses:
1. To be honest, I am not sure whether this paper has good starting point. They focus on a scenario where you want to further fine-tune the chat model in English to a chat model in other languages. Could you provide more concrete application scenarios in your introduction? I am not convinced because typically, in practice, people will use all the data available to tune a base model, and usually, this renders the best downstream task performance, or at least, the robustness defined as the average downstream task performance of a particular tuned chat LLM. Why cannot we follow this paradigm?
2. I suggest the authors to further polish the paper abstract. I am getting confused several times, for example, in your statement ``Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method'', why can you simply say this is a resource-efficient method? As far as I know, if you just use knowledge distillation by distilling the data synthesized from a stronger model in your target languages, you might still need to use stronger model, usually those closed-source models, e.g., ChatGPT, this is still expensive, right? Besides, as even the knowledge distillation, in the SFT stage, you have to mix all the data to do the training as well. So it is still expensive.

Limitations:
See the weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Y9aRCjuhnV;"REVIEW 
Summary:
This paper proposes a variant of Multi-Armed Bandits (MAB) named EdNetRMABs that model the interdependencies between learning contents to meet the real-world education scenarios. Subsequently, the authors introduce EduQate, an interdependency-aware Q-learning algorithm to optimize content recommendation given the EdNetRMABs. The paper demonstrates the theoretical and empirical effectiveness of EduQate with the experimental results of synthetic and real-world data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work uses RMABs in education to model the learning process with interdependent educational content as the different knowledge concepts have relevance.
2. The paper introduces EduQate employs Q-learning to make decisions on arm selection that do not require knowledge of the transition matrix to compute an optimal policy and provides related theoretical analysis.

Weaknesses:
1. As this work considers interdependency awareness in content recommendation in educational scenarios, the generated group in the experiment is not clear. The authors need to clarify how to capture relevance between the selected exercises of each topic in different datasets. Furthermore, does the different number of topics influence the experimental results?
2. The definition of state space representing a student's knowledge states a binary value. However, in real-world scenarios, the knowledge states are multi-level. A fine-grained knowledge state estimation result is essential for the content recommendation in adaptive learning.
3. The paper lacks the details of EdNetRMABs. Some illustrations would help to understand.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces **EduQate**, a system that generates adaptive educational curricula using restless multi-armed bandits (RMABs). This method aims to efficiently achieve mastery across multiple interdependent educational contents. Unlike traditional methods that assume learning contents are independent, EduQate acknowledges and leverages the interdependencies between different educational concepts.

The paper addresses key challenges in modeling and optimizing the learning process in educational environments where the learning of different topics is interconnected. By considering these interdependencies, the proposed EduQate system ensures more accurate and effective personalized learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
**Originality** The paper introduces the concept of EdNetRMABs, a model that considers the interdependencies among learning contents in educational settings. This approach contrasts with existing methods that typically assume independence among different educational topics. By employing the Whittle index and Q-learning to develop the EduQate algorithm, the paper further demonstrates originality in adapting and integrating these techniques to address the specific challenges of personalized education.

**Quality** The authors provide clear definitions and a well-structured EdNetRMABs model. The optimality guarantees of the EduQate algorithm are well-explained and supported through rigorous proofs. Additionally, the empirical results are compelling, demonstrating the effectiveness of the proposed method over benchmark strategies using both synthetic and real data.

**Clarity** The paper is clearly written and well-organized, making complex concepts easier to understand. The authors systematically introduce the problem, the proposed solution, and both theoretical and empirical validations. The use of figures helps to elucidate the model and results. However, the clarity could be further enhanced by including more detailed explanations of the algorithm's implementation and practical applications.

**Significance** The significance of the paper lies in its potential impact on the field of educational technology. By addressing the interdependencies among learning contents, EduQate offers a more accurate and efficient approach to personalized education.

Weaknesses:
1. **Clarity of Explanations**: Due to the limited length, many explanations in the main text are not very clear. For instance, the survival and design of the student model and the content and information of the datasets are not thoroughly explained. For example, there is a dataset mentioned later that lacks similarity content, which should have been noted when introducing the dataset.

2. **Inconsistency in Notation**: In Section 4.1, ""Analysis of EduQate,"" the variable k in the second line has a different font from the k mentioned later. Additionally, the content related to k mentioned earlier is too far from this section, making it difficult for readers to understand and potentially leading to misunderstandings.

3. **Simplified Modeling**: The model uses only one pseudo-state, where if any content within a group is learned, all unlearned content in the group is marked as pseudo-state. This approach seems overly simplified. For example, in Case 1, if only one piece of content in a group is learned, all other content is marked as pseudo-state. In Case 2, if many pieces of content in a group are learned, the remaining content is also marked as pseudo-state. Although both cases result in a pseudo-state, their actual significance differs, and it seems the authors did not consider this distinction.

4. **Bidirectional Relationships**: The relationships between knowledge points in the paper are bidirectional, with only grouping relationships. However, in practice, many relationships between knowledge points within the same group are unidirectional. Recommending subsequent courses without recommending prerequisite courses first can lead to students needing to self-study prerequisite courses when learning subsequent courses, increasing their learning burden. The modeling in the paper does not seem to account for such unidirectional structures, instead using groups to link knowledge points, which could lead to this reverse learning issue.

5. **User Experience**: The discussion on user experience for educators and students is insufficient. Including qualitative and quantitative feedback from pilot implementations would be useful. The authors should conduct user research to gather insights on the intuitiveness and user-friendliness of the system for the target users. This could include surveys, interviews, and usability testing, focusing on ease of use, effectiveness, and areas needing improvement.

6. **Scalability of the EduQate Algorithm**: Although the paper mentions the efficiency of the EduQate algorithm, it lacks an in-depth exploration of its scalability in large-scale educational environments. Detailed performance benchmarking of the algorithm, including analysis of computation time and resource utilization, would be beneficial. The authors should provide a comprehensive performance analysis of the algorithm as data scale and complexity increase, including potential optimization techniques for handling large datasets.

Limitations:
The authors have explicitly acknowledged the limitations of their work, providing explanations for these constraints and discussing their impact on the research. The paper does not pose any negative social impact, as it aims to improve educational practices through a more personalized approach to learning.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes EduQate, an innovative framework that leverages EdNetRMABs to achieve interdependence among knowledge points. By using Q-learning, EduQate implements optimal strategies for personalized learning, offering optimality guarantees without needing explicit knowledge of transition functions governing student learning states. This approach dynamically adapts educational content to individual progress, optimizing learning experiences and outcomes. The effectiveness of EduQate is validated using three real-world datasets, demonstrating its capability to identify and implement the most effective teaching strategies.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Overall, this article provides valuable insights and has a good overall quality. It has a certain impact on the AI education domain. The proposed approach, EduQate, adapts the Q-learning method of the multi-armed bandit model and introduces active and passive arms to establish correlations between knowledge points, which demonstrates a level of innovation. The article ensures the validity of the approach at the theoretical level, making it more persuasive. In terms of narrative, the article maintains logical coherence and is comprehensible. The discussed topics are indeed important in the field of education.

Weaknesses:
The formatting of the article is messy, and the images are not very clear. The layout on the sixth page is problematic, with formulas and text mixed together, making it confusing. The images on the seventh page and some of the appendix images are blurred. I suggest using a different format to redraw them.

There are too few baselines in the experimental section, and there are very few validation experiments. Merely presenting outstanding performance in one experimental metric is insufficient to demonstrate the superiority of your method. It would be beneficial to include some reinforcement learning baselines for comparison. Other reinforcement learning methods, such as the Bayesian network approach mentioned in the citations, can also accomplish the task. Therefore, the necessity of your method is unclear in the paper.

Your method does not consider the specific circumstances of practical problems. It raises fairness concerns. Ensuring that all knowledge points are fairly selected in a specific educational context is a crucial issue. Although the article presents a good framework, I believe it is not feasible to use it.

Limitations:
As mentioned above, there are many factors to consider in the specific field of education. These factors include fairness, mapping each question's knowledge points to the ARMs mentioned above, and the presence of cold-start problems, among others. Time is also a crucial factor to consider during real-time usage, although the paper does not mention specific algorithmic time information in practice.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a solution to generate personalized learning curricula in educational settings, focusing on the challenge of accounting for interdependencies between learning topics. It argues that existing approaches, often based on the Restless Multi-Armed Bandit (RMAB) framework, fall short by assuming independence of learning content which is unrealistic in educational settings. To address this the paper introduces a new model Restless Multi-armed Bandits for Education (EdNetRMABS) enabling the capture of relationships between independent learning items. Building upon this model, the authors propose a new algorithm called EduQate, which leverages Q-learning and the Whittle index to compute an interdependency-aware teacher policy for recommending educational content. Notably, EduQate doesn't require prior knowledge of the transition matrix, unlike traditional Whittle index methods.

The authors provide a theoretical analysis demonstrating the optimality of EduQate for the case of recommending a single item at each time step (k=1). While finding the optimal solution for recommending multiple items (k>1) is proven to be NP-hard, a heuristic greedy algorithm is proposed to find solutions.

Through experiments on synthetic and real-world datasets (Junyi and OLI), the paper demonstrates the superiority of EduQate over baseline policies, including Threshold Whittle (TW), WIQL, Myopic, and Random. EduQate consistently achieves higher intervention benefits and average rewards across all datasets. Further analysis reveals the effectiveness of the replay buffer in EduQate, mitigating the ""cold-start problem"" common in reinforcement learning applications.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well organized and very well written with comprehensive explanations of the model, algorithm and the experimental setup.
2. Addressing the crucial issue of interdependency in learning content is a significant contribution. EdNetRMABs offer a realistic model for educational settings, moving beyond the simplifying assumption of independence prevalent in traditional RMAB approaches.
3. The paper presents a rigorous theoretical analysis of EduQate, proving its optimality for the k=1 case and providing complexity bounds for the k>1 case.
4. The authors address the cold-start problem by incorporating experience replay and perform ablations to show its effectiveness. This is a practical enhancement relevant for real-world applications.

Overall, this paper presents a strong contribution to the field of adaptive learning by introducing a novel and effective approach for generating personalized curricula that account for interdependencies in learning content.  The theoretical analysis, empirical results, and focus on practical considerations make this work both insightful and impactful.

Weaknesses:
The assumption of fully observable knowledge states is a significant limitation. Future work should explore extending the model and algorithm to handle partial observability, a more realistic scenario in education.
While the complexity analysis is provided, further investigation on the scalability of EduQate to larger datasets and more complex networks would strengthen its practical applicability.
The current work primarily focuses on maximizing long-term rewards. Further analysis on balancing exploration and exploitation in EdNetRMABs could offer valuable insights for curriculum design.

Limitations:
1. The experiments are based on simulated students and existing datasets. Real-world classrooms involve numerous factors not accounted for in the model, such as student motivation, engagement or diverse learning styles. This may limit the practical utility of this method.
2. The paper does not address the interpretability of EduQate's recommendations.
3. The assumption that the student states are fully observable is a major limitation of this work. This can result in overconfident recommendations.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a way to extend RMAB with Q-learning to accommodate the fact that some items/arms belong to a group. RMAB can be considered as a weakened version of contextual bandit CB but also a strong version of CB since it considers state transitions (explicitly defined on arms).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This is a very solid paper with quite a few reasons to accept it:
1. The contribution of education simulators. Can the authors comment on whether they will release their simulators publicly for other education or bandit communities to use in the future? Currently, there is a lack of high-quality education simulators that are based on real-world data. This could be a great contribution. I hope the authors make an effort to make such resources public.
2. The technical contribution of introducing a pseudo-action that extends RMAB and WIQL.
3. Clean writing and presentation.

MAB has been used in many different real-world settings and is the main algorithm behind many learning platforms. Any innovation in this space will have a huge impact on students around the world. Unfortunately, this area is very niche and requires high technical sophistication. Unless someone can convince me that this paper's algorithm has already been published elsewhere or is fully derivative of some other work, I think it's a great paper to present at NeurIPS.

I might also be a bit concerned if the authors decide not to share their code/algorithm/simulators to encourage more future research in this direction.

Weaknesses:
1. Can add some more ablation studies.
2. Some clarifications (see question section).

Limitations:
The authors discussed limitations in a section.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
XkMCKoHNCD;"REVIEW 
Summary:
The authors present an analysis of logistic regression on sentence embeddings as a way to predict the speaker of a particular line of dialogue in the Big Bang Theory. Specifically, they fit a PCA model to embeddings obtained from a sentence transformer and then use each PCA dimension as a linear feature. The authors present some qualitative analysis of the most predictive PCA dimensions and some quantitative analysis of the classification accuracy. In addition, they present a brief analysis of the ability of GPT-4 to directly classify lines of dialogue and compare to a limited user study.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The authors have identified an interesting and important debate in the AI community — more tests which help researchers discriminate between mere stochastic parroting and true generalization are certainly needed! In addition, the authors are very thorough in their description of the methods involved and their qualitative analysis of the PCA features is extensive. I also appreciate the thought the authors have given to the limitations of their study and the need for further work.

Weaknesses:
First and foremost, I feel that this paper needs to be much clearer and more focused in its research question. The introduction indicates that the objective of the study is to determine the extent to which the apparent ability of large language models like GPT-4 to generalize to novel tasks is actually attributable to their ability to parrot data from their training. However, a good part of the analysis appears dedicated to the specifics of The Big Bang Theory and the features of its dialogue. Section 3.1, for instance, extensively interrogates the PCA dimensions obtained from the sentence embeddings in a way that feels very specific to the particular dataset. Similarly, the conclusion raises claims that the ability for logistic regression to predict the speaker with reasonable accuracy is due to stereotyping in the characterization of the show. These claims are potentially warranted given the experimental evidence (though a more detailed and statistically-motivated analysis would be necessary to make such claims with certainty), but feel as though they belong in a different paper (a potentially quite interesting paper for a different venue, I should add). The connection between these results and the initial framing of LLM evaluation remain, unfortunately, somewhat murky. This is not to say there is no possible link between dialogue speaker prediction and LLM abilities! I encourage the authors to think about this problem more and articulate the specific claim they hope to interrogate.

On that note, and assuming that the main motivation is indeed to study large language models, I feel that the analysis could be strengthened. First, it would be helpful to justify some of the specific decisions made as part of evaluation. For instance, why were the Big Bang Theory and Friends selected over other possible dialogue datasets? Why was dialogue speaker prediction always studied between exactly two characters? Why were these specific characters selected? Do the characters have a similar amount of lines, or are there other statistical biases in the dataset that might affect the results?  When proposing a novel task, it’s important to make the assumptions and decisions that went into the task selection clear.

With regards to human evaluation, I encourage the authors to widen their study. That is to say, a user study which consists of only two participants (both of whom are related to one of the authors) makes it difficult to ascertain the reliability of the results. Indeed, I would suggest a study consisting of a larger number participants (ideally participants who do not have any externally motivating factors like relationships to the authors) so that a more general measure of human ability can be obtained. Further, I think it could actually be preferable for the participants to not have prior experience with the television show. This would make the test more an examination of the ability for participants to generalize their knowledge of personality traits to a novel situation instead of their ability to recall information (which is, ostensibly, closer to the desired research question in LLMs).

Despite these critiques, I hope that the authors continue to refine their research question, justification, and methodology. There are interesting questions to study here!

Limitations:
I feel that the authors have been very up front with the limitations of their work and have situated it in the context of broader impacts.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors are focused on whether or not LLMs can be thought of as stochastic parrots or contain ""Sparks of AGI"". They look into what kind of data is recoverable from internal LLM representations. Specifically, the authors investigate to what extent the task of identifying TV personalities (e.g. Penny vs Sheldon) based on their dialogue lines is solvable using various methods. The authors compare a classifier based on PCA components extracted from existing LLM embeddings, GPT-4 zero shot performance, and human expert judgments. They find that all methods show fairly good performance, with human experts showing best results, followed by GPT-4, followed by the classifier. The authors also present a brief qualitative analysis, interpreting the more prevalent axes of variation in the embeddings identified using PCA.

Soundness:
1: poor

Presentation:
3: good

Contribution:
1: poor

Strengths:
The authors tackle a very ambitious and important problem. The writing is clear throughout, and the authors provide extensive background for the methods they use.

Weaknesses:
I need to preface this by saying that I hope that my negative review does not discourage the authors from further pursuing the topic. I feel bad for having to reject this paper as it has some good ideas behind it and has an intention of researching a highly important problem. I hope that in next iterations, their work can be improved and expanded. At present, unfortunately, it does not match publication standards. I will try to explain why, and give pointers on how to potentially fix it in the future.

The biggest flaw of the paper is the experiment design. The authors never clearly define what exactly it means to be a ""stochastic parrot"" as opposed to ""general intelligence"". The authors also don't explain how their experiments would help to decide one way or another. So the results we have are impossible to interpret. It would help to go back to the original question and work through the argumentation more clearly. If the internal LLM representations have information about TV personalities, does it make them more or less of a stochastic parrot and why.

Otherwise, the experiments give a very exploratory impression. For example the authors run PCA on sentence embeddings computed on their dataset and interpret the components. But it's unclear why and how this would help to answer the main question the paper attempts to answer.

Additionally, the paper's methods are extremely well-known, but unfortunately, the authors don't refer to relevant literature. The work highly overlaps with the topic of linear and nonlinear probes, as well as with the general theme of transfer learning. In essence, what the authors did can be described as adding a ""classification head"" to a pre-existing LLM. This is a very well-known technique.

If we want to gain new insights into what the models are doing, it is usually more interesting to look into the computations in intermediate layers of the model, rather than the last embedding layer. It is also often desirable to look at causal probes (rather than just a classifier).

Lastly, there are certain writing choices that deviate from common ""conventions"" in academic publishing. For example, oftentimes the authors go into excessive detail on well-known methods (explaining how PCA works and what a covariance matrix is). I highly suggest that the authors look at existing successful papers that use similar methods and copy their approach when it comes to decisions on what to explain in the main text, what to put into the appendix, and what to omit. The general rule of thumb is that newly introduced and important ideas should be at least briefly given in the main text, with extra details given in the appendix. Extremely well-known and established methods such as Principal Component Analysis don't need a full explanation, and a simple reference to the original source is enough.

I really hope that the authors don't get discouraged and try to refine and improve their research in the future. The first starting point would be to more clearly define the problem, and to study in depth the existing literature on linear probes and probing in general, and on investigating what the internal LLM representations contain. One potential starting point is the paper ""Evaluating the World Model Implicit in a Generative Model"", Vafa et al. 2024 and related works.

Limitations:
The authors acknowledge some of the limitations of their study.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper’s main contribution is to apply a logistic regression on the principal components of the LLM embeddings for classifying TV series characters based on their dialog lines. The main finding is the logistic regression approach does worse than GPT-4 in predicting TA characters, but is comparable to human evaluations with two annotators.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The paper focus on an interesting angle of using language model features for predicting the belongings of dialog lines of characters of TV shows.

Weaknesses:
The methodology of using logistic regression over PCA of language model embeddings is not novel, and there's no rigorous quantitative evaluations of the method beyond qualitative examples. The connection of the method and task to the broad discussion around ""spark of AGI"" and ""Stochastic Parrots"" is farfetched.

Limitations:
The paper claims that ""the contribution of the paper is primarily methodological, and their study is limited to a qualitative study of two very specific datasets."" However, the method they adopt is a fundamental ML technique, which lacks novelty.

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper aims to prove LLMs work as ""stochastic parrots"" (Bender et al) rather than ""sparks of agi"" (Bubeck et al). To prove this claim, the paper presents an experiment where a task can be solved by training a linear model (logistic regression) on top of PCA of the LLM output. The authors then claim, based on the linear model experiments, that the LLM doesn't exhibit any sparks of agi due to the ability of (nearly) solving the task using linear models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The authors show a simple linear model trained on the output of an LLM for a given task is good enough to solve it, compared to using a GPT4 model, raising questions on the supposed intelligence often ascribed to the model.

Weaknesses:
While I generally agree that LLM's are closer to ""stochastic parrots"" than ""sparks of agi"", the claim that it can be proved using the proposed PCA experiments is weak to me. 

- Firstly, the embeddings are essentially the output of the LLM in question (all-MiniLM-L6-v2) - I would call it outputs rather than embeddings, as embeddings just indicate input word embeddings to the model, which clearly here isn't the case. 
- Secondly, the outputs itself being feature rich to be used for classification is unsurprising. It is expected the principal components of this embedding would be useful in predicting the properties of the task (as shown in the projection of PCA plots). This just shows the underlying model (SentenceBERT here) is good at extracting rich semantic and syntactic features from the input sentence (probing literature essentially proves that [1]).
- Lastly, the experiment also shows the representations extracted from the sentence embedding model is sufficient for the task. For a harder task, if the linear probe on all-MiniLM-L6-v2 was not good with respect to GPT4, that would also not conclusively prove the ability of GPT4 is due to any sparks, rather it can be explained that GPT4's own embedding features are richer. That is, a linear probe trained on GPT4 embeddings for a harder task would also likely mimic its own performance. (this is theoretical, as neither the author or anyone other than OpenAI have access to their embeddings)

[1] https://aclanthology.org/D19-1250/

Limitations:
There are no explicit limitation section, however the last paragraph of conclusion discusses it.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
XgkrXpHl5j;"REVIEW 
Summary:
The paper introduces a Generalized Multimodal Fusion (GMF) method using the Poisson-Nernst-Planck (PNP) equation to address challenges in multimodal fusion, such as feature extraction efficacy, data integrity, feature dimension consistency, and adaptability across various downstream tasks. The GMF method leverages theoretical insights from information entropy and gradient flow to optimize multimodal tasks, treating features as charged particles and managing their movement through dissociation, concentration, and reconstruction. 

Key contributions of the paper include:
1. A theoretical framework combining PNP and information entropy to analyze multimodal fusion.
2. A novel GMF method that dissociates features into modality-specific and modality-invariant subspaces.
3. Experimental results showing GMF achieves competitive performance with fewer parameters on multimodal tasks like image-video retrieval and audio-video classification.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The application of the PNP equation from physics to multimodal feature fusion is novel and creative. The theoretical framework combining PNP and information entropy provides an original perspective on analyzing multimodal learning.

Quality: The paper provides a solid theoretical foundation with detailed proofs and derivations. The experimental evaluation is comprehensive, covering multiple datasets and task types (NMT, EMT, GMT).

Clarity: The paper is well-structured and clearly written. The methodology is explained step-by-step with helpful visualizations.

Significance: The proposed GMF method shows promising results in terms of performance, parameter efficiency, and robustness to missing modalities. It has potential for broad applicability as a frontend for other fusion methods.

Weaknesses:
1.The theoretical analysis, while extensive, could benefit from more intuitive explanations to improve accessibility.
2. The experimental section lacks ablation studies to isolate the impact of different components of the GMF method.
3. While the method shows good results, the performance improvements over some baselines are relatively small in certain experiments (e.g. Table 2).
4. The paper does not thoroughly discuss potential limitations of the approach or scenarios where it may not be suitable.

Limitations:
The authors briefly mention some limitations of linear operations for high-dimensional inputs in the conclusion. However, a more thorough discussion of potential limitations and failure cases would strengthen the paper. Additionally, while not highly relevant for this theoretical/methodological work, some discussion of potential negative societal impacts of improved multimodal fusion techniques (e.g. privacy concerns) could be included.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces CrossCheckGPT, a novel method for assessing hallucination robustness in multimodal foundation models without requiring reference standards. Utilizing cross-system consistency, the proposed method aims to provide a universal evaluation framework capable of being applied across various domains and tasks. This approach contrasts significantly with traditional hallucination assessments, which rely on comparison with gold-standard references and are limited to specific domains.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The introduction of a reference-free universal hallucination ranking method addresses a significant gap in the evaluation of foundation models, particularly in new and emerging areas.
2. The paper effectively demonstrates the method's versatility across different modalities (text, image, and audio-visual), enhancing its relevance to a wide range of applications.
3. The development of the AVHalluBench, the first audio-visual hallucination benchmark, is a noteworthy contribution that sets a new standard for evaluating models in this complex domain.

Weaknesses:
1. The analysis on how different models' outputs are compared and the implications of these comparisons could be more detailed. Specifically, the paper lacks a deeper exploration into the sensitivity of CrossCheckGPT to variations in model architecture or training data.
2. Lack of additional visual representations of the data flow or examples of the hallucination checks.
3. Lack of a more comprehensive set of benchmarks, including more direct comparisons with state-of-the-art methods.

Limitations:
The effectiveness of the method hinges on the diversity and independence of the models used as evidence sources.  This dependence could pose challenges in scenarios where similar or homogenous models are prevalent.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel multimodal fusion method named Generalized Multimodal Fusion (GMF), which leverages the Poisson-Nernst-Planck (PNP) equation from physics to manage the feature fusion process in multimodal learning tasks. By treating features as charged particles, the method allows for a dynamic separation and recombination of modality-specific and modality-invariant features, thereby enhancing the fusion process and reducing the entropy in downstream tasks. This approach addresses common challenges in multimodal learning, such as feature dimension consistency, data integrity, and adaptability across various tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The application of the PNP equation, traditionally used in physics to describe the dynamics of charged particles, to multimodal feature fusion is highly original.
2. The paper is grounded in a solid theoretical framework that is well-articulated and robust.

Weaknesses:
1. The method, while innovative, appears to be complex in terms of implementation, particularly in how features are treated as charged particles. This complexity might limit its accessibility or usability for practitioners not familiar with the underlying physical equations.
2. The paper could benefit from more rigorous quantitative analysis, including statistical significance tests and error analysis. Such analyses would provide a clearer picture of the method's performance relative to benchmarks.
3. Can the authors provide the results on multimodal datasets with text-image modalities [1] ?

[1] Provable Dynamic Fusion for Low-Quality Multimodal Data.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors combined the Poisson-Nernst-Planck (PNP) equation with information entropy theory and proposed a generalized multimodal fusion approach, which disassociates modality-specific and modality-invariant features, thereby reducing the join entropy of input features and meanwhile decreasing the downstream task-related information entropy. The experimental results demonstrate that the proposed approach can improve the generalization and robustness of multimodal tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- It is innovative to employ PNP for solving the multimodal fusion issue, by treating features as charged particles to disassociate them. 
- A generalized multimodal fusion approach was designed, which can overcome the strong assumptions made by existing methods. 
- Experiments showed the effectiveness of the proposed GMF approach in efficiency and flexibility.

Weaknesses:
- Significance test (e.g., Wilcoxon signed-rank test) would be helpful to better illustrate the significance of the proposed method compared against baselines. 
- Social impacts are not explicitly discussed in the paper/appendix.

Limitations:
Please refer to Weakness and Question, which are the aspects suggested to be further improved.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
XYDMAckWMa;"REVIEW 
Summary:
The paper proposes a loss for training flow matching (rectified flows, stochastic interoplants) that is based on integrating the target velocity over the available data as the regression target. This reduces the variance of the gradient estimator and can also be applied in the stochastic variant of flow matching.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
To the best of my knowledge, the idea is novel to use a larger batch of data for the velocity target in the flow matching objective than as the input for the network. This reduces the variance in the loss estimate.

The additional compute over the FM loss seems negligible, since only the target velocity field is adapted, but the network still receives batches. This makes it an attractive improvement to flow matching training.

Weaknesses:
I genuinely like the idea, but cannot recommend acceptance in the current presentation. I am happy to be corrected on any point and adjust my score.

Below I grouped the weaknesses by category.

## Theoretical contributions can be simplified and potentially reveal existing result (update: largely addressed, but presentation to be improved)

I think the theoretical derivation can be greatly simplified:

1. The notation is overly complicated. Why not use the standard notation for joint $p(a, b)$, conditional $p(a|b)$ and marginal $p(a)$ probabilities instead of index notation, where the index sometimes means ""joint"", ""conditional"" or ""marginal"", and sometimes indexes time $t$ or a condition such as $x_1$? (or $\\rho$ instead of $p$).
2. I think this reveals that the new loss is simply obtained by writing the target velocity in the ExFM loss as the expectation over the training data $\\rho_1$: Eqs (8, 10) say that the target velocity is given by $\\int w(t, x_1, x) \\rho(x|x_1, t) dx $, that is just average the velocities over the entire training data, weighted by the probability that the path actually  (where $x$ is sampled from the linear conditional paths). Given this observation, it seems to me that the actual contribution of the paper does not lie in this new loss, but how to efficiently estimate this integral, which is currently hidden in Appendix B.

In fact, I think that sections 2.1 and 2.2 can be merged to a simple importance sampling argument in the original flow matching argument.

I also think that the authors are missing that their third contribution has already been derived in the same form by their reference [10] in Eq. 4 and I think the second contribution is a simple extension to different conditional flow fields resulting from different inversions $\\varphi^{-1}$.

## Evaluation on tabular data is wrong (update: fixed)

The NLL defined in Appendix H.5.3 does not contain the volume change, which is an integral part of the negative log-likelihood. Did you use this equation for evaluation? My current score reflects the belief that volume change was accounted for.

Also, in Table 3, sometimes the highest values and sometimes the lowest values in each row are marked bold. Which model is better and does this use the incorrect formula? Please update without e-notation, adapting -1.29E+02 to -129, this is hard to read.

## Toy data evaluation (update: fixed)

It is easy to construct a very good approximation for the moons distribution by taking a Gaussian mixture of values for Table 4.

## Typos (has no influence on my recommendation)

Here is a list of what I found:

- l. 30: introduced -> introduce
- l. 33: base -> based
- l. 65: $rho$ -> $\\rho$
- l. 70: need -> needed
- l. 92/93: using map -> using the map
- l. 100: we return to end of the standard CFM loss representation -> ?
- l. 105: just (unknown) -> just the (unknown)
- eq. 7: the integral shares variables with the outside expression, e.g. add tilde on the integration variables
- l. 124: inevitable -?> invertible
- l. 126: have -> has
- eq. 16: consider moving numerical tricks like using softmax from the theory section of the paper, page 6 already introduces a lot of notation.
- throughout: dispersion -> variance

Limitations:
I do not think that the limitations of the work are properly addressed in the theoretic part, in contrary to the statement of the authors in the paper checklist. In particular, I did not fully understand how accurately Eq. (10) (which is part of the loss) can be estimated. One question that can be a way towards addressing this is by expanding on when the assumption in line 172/173 is valid (and why this Jacobian is even a problem).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes an analytic formula for the vector field satisfying the continuity equation for the given change of density which interpolates between two distributions in the sample space. This is a common setting in the Flow Matching model [6] (Rectified Flows [1], Stochastic Interpolants). 

The authors apply the formula for several special cases like linear interpolation between samples and propose to use this formula for the training. They study the proposed training procedure empirically for synthetic examples and CIFAR-10 images.

[1] Liu, Xingchao, Chengyue Gong, and Qiang Liu. ""Flow straight and fast: Learning to generate and transfer data with rectified flow."" *arXiv preprint arXiv:2209.03003* (2022).

[6] Lipman, Yaron, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. ""Flow matching for generative modeling."" *arXiv preprint arXiv:2210.02747* (2022).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The presentation of the proposed method is clear.

Weaknesses:
The proposed formula is already well-known in the community. It was proposed in [1] (see eq. 4, Def 3.1, Sec 5.1 for demonstration) and [2] (see Appendix D). In [3] (see eq. 4), the authors propose to use an analogous formula for training a generative model and conduct a much more exhaustive empirical study. Moreover, [4,5] develop different applied methods building upon this formula.

The downside of this formula is also very well-known in the community, i.e. one has to use large batch sizes to accurately estimate the vector field, which comes with a significant computational cost. Moreover, for any finite size, the estimation of the vector field is biased and the estimation of the loss is biased which would deteriorate generation quality when used to train large-scale models. This limitation is not adequately studied in the paper, e.g. the models are not compared in terms of the training time and memory used.

Given that the main contribution of the paper has already been proposed and the empirical study of this formula is unsatisfactory, I cannot recommend this paper for acceptance.

Minor comments:

- The authors refer to the original FM framework [6] as Conditional Flow Matching, which was proposed in [7].
- The objective proposed in Flow Matching is already tractable, so it is not clear what “a tractable form of the Flow Matching objective” means.
- There is a typo in line 65.
- The derivative notation used is inconsistent with its description in the text.
- From eq. 19, I assume that “dispersion” means “variance”.

[1] Liu, Xingchao, Chengyue Gong, and Qiang Liu. ""Flow straight and fast: Learning to generate and transfer data with rectified flow."" *arXiv preprint arXiv:2209.03003* (2022).

[2] Neklyudov, Kirill, Rob Brekelmans, Daniel Severo, and Alireza Makhzani. ""Action matching: Learning stochastic dynamics from samples."" In *International conference on machine learning*, pp. 25858-25889. PMLR, 2023.

[3] Xu, Yilun, Ziming Liu, Max Tegmark, and Tommi Jaakkola. ""Poisson flow generative models."" *Advances in Neural Information Processing Systems* 35 (2022): 16782-16795.

[4] Scarvelis, Christopher, Haitz Sáez de Ocáriz Borde, and Justin Solomon. ""Closed-form diffusion models."" *arXiv preprint arXiv:2310.12395* (2023).

[5] Xie, Tianyu, Yu Zhu, Longlin Yu, Tong Yang, Ziheng Cheng, Shiyue Zhang, Xiangyu Zhang, and Cheng Zhang. ""Reflected Flow Matching."" *arXiv preprint arXiv:2405.16577* (2024).

[6] Lipman, Yaron, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. ""Flow matching for generative modeling."" *arXiv preprint arXiv:2210.02747* (2022).

[7] Tong, Alexander, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. ""Improving and generalizing flow-based generative models with minibatch optimal transport."" *arXiv preprint arXiv:2302.00482* (2023).

Limitations:
The authors do not provide a necessary literature review nor study the limitations of the proposed approach (see Weaknesses section above).

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a novel approach to training flow-based generative model by deriving the conditional flow matching objective function with respect to the flow function. The author argues that this new method of training will reduce variance, add stability, and ultimately lead to faster convergence. Additionally, the reformulation allow derivation of the exact vector field expression, and in some simple cases, enables the computation of the oracle trajectory solution.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- If the derivations are correct, this methodology could potentially add some innovations in the field of flow matching.

Weaknesses:
- The paper is difficult to follow and lacks clear writing and organization. Specifically, the authors' use of notations is very confusing.
- The mathematical computations do not appear to be very rigorous and some assumptions seem very incorrect. I may have misunderstood some derivations, so please correct me if I'm wrong (see Questions section).
- The experimental results are not very robust or convincing. For instance, while the paper proposes that one of their contributions is the reduction of variance during training, many of the results demonstrate larger variance across numerous, different metrics.
- Overall, the paper feels like it requires substantial revisions and is far from being polished.
Typos:
- Line 65, rho_1$\rightarrow$ $\rho_1$.
- Line 130, practical $\rightarrow$ practical form.

Limitations:
The author has adequately addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This works aims at producing a lower variance loss for flow matching. This is done by using the formula for the ground truth marginal velocity field, estimating it using self-normalized importance sampling, then regressing onto this estimated marginal velocity field.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Variance reduction for CFM is a useful avenue of research

Weaknesses:
- This paper lacks polish. I felt this paper is cumbersome to read, with a lot of heavy notation that can be drastically simplified, whereas the proposed algorithm is quite simple.
- The proposed approach is not exactly novel. The ""Explicit Flow Matching"" objective is just the original ""Flow Matching"" objective where we regress onto the optimal velocity field. The practical implementation being proposed is a simple application of self-normalized importance sampling.
- Importantly, the proposed approach leads to a biased objective (when estimated through a minibatch) where the optimum is not guaranteed to be the correct velocity field. This is not a problem for the low-dimensional experiments but importance sampling becomes more problematic in high dimensions.

Limitations:
The proposed ExFM objective is equivalent to the original FM objective, and the paper should not over-claim this contribution. This objective is intractable, so the use of smart estimation methods is interesting in its own right.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
WxW4nZMD3D;"REVIEW 
Summary:
The authors propose to use network Lasso to learn a multi-task bandit problem with given network structure. More specifically, the network structure has pre-defined unknown clustering structure, where within each cluster all the bandit tasks share the same model. The authors propose a bandit algorithm that can learn and provide a sublinear guarantee. The key difference of this paper with GOBLin in Cesa-Bianchi et al., 2013 is that this paper uses a network Lasso (or something like a group Lasso) penalty while GOBLin use a ridge penalty.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Even though I think the authors overclaimed their contributions, which I will state below, I still feel it's meaningful to discuss network Lasso and design a bandit algorithm based on certain network structure, given the limited literature on multitask bandit. Compared to previous network bandit literature such as Cesa-Bianchi et al., 2013, this paper characterizes the network structure in more detail.

Weaknesses:
1. I think the authors need to provide more real-world examples to show why their network structure (and the correpondingly induced network Lasso bandit algorithm) is pratical, instead of stating their algorithm is good because it provides piecewise constant property in constrast to smoothness of GOBLin in Cesa-Bianchi et al., 2013. The key assumption in this paper is that the network structure is given, and the network can be splitted into connected clusters, within which the task parameters are the same. Can authors find or describe a couple of pratical examples/datasets where such a network exists, given that bandit is a very pratical problem?

2. The literature review that compare with the previous literature are not very accurate and sufficient IMO. For example, the authors mention Gentile et al., 2014 and Li et al., 2019 can cause overconfidence in constructing clusters. However, these algorithms do not have prior information about clusters such as a given network and thus they have to learn the clusters conditioned on the task similarities. In that sense, these algorithms are more pratical because oftentimes in practice network information is lacking. Here one should also add a related reference Context-Based Dynamic Pricing with Online Clustering by Miao et al., 2022. There are also robust multitask bandit algorithms (e.g., Multitask Learning and Bandits via Robust Statistics by Xu and Bastani, 2024) that can also solve the network bandit problem if the network structure follows certain assumptions; Multi-Task Learning for Contextual Bandits by Deshmukh et al., 2017 discuss a multitask bandit problem but use kernal based method. I suggest the authors add a more detailed literature review to discuss their paper's connection with these current multitask bandit algorithms. 

3. Typically, greedy algorithm (the method proposed in this paper is greedy too due to Assumption 2) has better performance in bandit simulations compared to UCB-based algorithm (see Bastani and Bayati, 2020). Therefore, it's not a fair comparison in Figure 1, where all benchmarks are UCB algorithms. I think it's necessary to add the following benchmarks: OLS-Bandit or Lasso-Bandit (Bastani and Bayati, 2020) without task sharing, and Cella et al., 2023 which use low-rank structure to do multitask bandit, and also a few others mentioned above as a benchmark to show that the network structure indeed helps, fixing the difference due to UCB and greedy algorithms.

4. I feel it's a false claim that the regret bound in Theorem 3 ""doesn't depend on the dimension"" and it's due to the concentration inequality from Hsu et al., 2012. Intuitively, the regret bound should depend on the dimension unless one assume that the number of tasks in a cluster is d-dependent so the regret bound is smaller compared to a typical single bandit regret bound. I think the reason why here the bound seems to be d-independent is because the dimension $d$ is hiden in the problem-dependent parameter $\phi$. Since the authors assume the context $x$ has norm 1, the minimum eigenvalue of $E[xx^\top]$ should scale as $1/d$, and hence $\phi^2$. I don't think a typical tail inequality in Hsu et al., 2012 can improve the bound regarding the context dimension. 

5. I think the asymptotic assumptions in Theorem 3 is incompatible with the finite-sample analysis in a typical bandit analysis and looks weird. I suggest the authors keep the isoperimetric ratio and centrality index as part of the regret bound (instead of forcing them out using the asymptotic assumptions), even though it might add an additional T-linear term due to the misspecification error caused by the inter-cluster edge connections. I feel that's the case because in the extreme case when each cluster has size 1, there will be misspecification error penalizing tasks connected the inter-cluster edges towards each other. I think it's totally fine to have such non-sublinear terms to provide a more comprehensive understanding of the limit of such network structures. 

I am willing to raise my rating if the authors can solve my questions and concerns.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, authors work under the multi-task contextual bandit settings by representing the task correlations through the graph structure. To solve this problem, authors propose an algorithm that utilizes a linear regression formulation with Lasso constraint in terms of the node connectivity. Theoretical analysis as well as experiments against several baselines are presented to demonstrate the effectiveness of proposed method.

-	The paper is generally well-written, with crisply clear descriptions of required assumptions, and the proposed solution is intuitive and well-motivated.

-	Good empirical performances. Authors compare proposed algorithm with several clustering of bandits baselines, showing the effectiveness of the proposed method. The performance gain over existing methods is impressive. 

-	Novel theoretical analysis roadmap. Overall, the theoretical analysis pipeline is novel and the looks promising to me. With the additional introduced RE assumption, authors are able to improve the regret bound to $\tilde{O}(\sqrt{\bar{T}})$ instead of the vanilla time horizon.

-	My major question is regarding the numerous assumptions required for the theoretical analysis. For instance, in Assumption 1, authors assume the candidate arms across different arounds are generated i.i.d. from a fixed distribution. This is different from existing clustering of bandits works, where the candidate arm contexts in each round is conditioned on previous observed arms. In this case, assumption 1 is somehow deviates from the actual applications of recommender systems where the candidate arms of each round are refined along with more information collected from the environment.

-	For the experiments, authors have compared against multiple clustering of bandit works. In this case, it would be good if authors can include additional discussions comparing your theoretical outcomes with those of existing clustering of bandits works, which can offer more intuitive comparison with existing approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Please see my comments above.

Weaknesses:
Please see my comments above.

Limitations:
Please see my comments above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the multi-task bandit problem using graph information. The given graph represents the relationships between tasks. Assuming that the preference vector of clustered tasks is constant, the problem is formulated as a network lasso problem to estimate the lasso estimator. 
A modified restricted eigenvalue condition, commonly used in high-dimensional statistics, is defined to derive the oracle inequality for the network lasso estimator on non-i.i.d. data. The oracle inequality of the proposed network lasso estimator is derived under the assumption that the true multi-task Gram matrix satisfies the adapted RE condition.
Based on the derived oracle inequality, a greedy-type algorithm is presented, achieving $\sqrt{T}$ regret. Numerical experiments support the theoretical performance of the proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The proposed algorithm efficiently learns task preference vectors by using graph information that encodes relationships between tasks. Specifically, it employs a network lasso estimator under the assumption that preferences within clustered tasks are constant, demonstrating its effectiveness in high-dimensional contexts.
- The paper adapts the restricted eigenvalue condition from high-dimensional statistics to the graph-based multi-task bandit setting. Based on the adapted RE condition, they established oracle inequality for network lasso estimator and showed that the proposed algorithm achieved $\sqrt{T}$ regert even though I haven't verified every proof in detail.
- The algorithm's performance seems robust even as the number of tasks and dimensions increase.

Weaknesses:
- Since I'm not very familiar with the graph-based multi-task bandit setting, it may be that the concepts explaining the restricted eigenvalue condition (Def 2) are too heavy. It would be helpful to include comparisons or examples from existing RE conditions in high-dimensional statistics or high-dimensional contextual bandits to improve understanding.

Limitations:
The authors have well-addressed the limitations in Appendix D.4 and further research directions in Section 7.

The content discussed in this paper appears to have little to no negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a multi-task contextual bandit algorithm that leverages a graph structure to model relationships between tasks. The algorithm assumes that the preference vectors of the tasks are piecewise constant over the graph, forming clusters. By solving an online network lasso problem with a time-dependent regularization parameter, the algorithm estimates the preference vectors, achieving a sublinear regret bound lower than independent task learning. Theoretical findings are supported by experimental evaluations against other graph bandit and online clustering algorithms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1) The paper introduces a approach by incorporating graph structures to model relationships between tasks.

(2) The algorithm is supported by comprehensive theoretical analysis, including a oracle inequality and a regret bound.

(3) Extensive experiments validate the proposed method, showing that it outperforms existing baselines in terms of cumulative regret, highlighting its practical applicability and effectiveness.

Weaknesses:
(1) The problem setting and algorithm presented are primarily adaptations of existing works, such as Oh et al. [2021]. The main difference is the inclusion of a graph matrix in the user preference vector, but this is not the first algorithm to incorporate a graph in contextual bandits, limiting the overall novelty.

(2)  The i.i.d. assumption in contextual bandits is quite strong. Even in clustering approaches like CLUB, a conditional i.i.d. assumption is used. The current regret upper bound complexity is \(\sqrt{VT}\). There should be special cases where the algorithm can improve over \(V\) to demonstrate a more significant advantage.

(3)  Since 2019, there have been many more works on clustering in bandits. The authors should conduct a broader survey to include these more recent works and relevant baselines. Using SCLUB, which is considered outdated, as a baseline, limits the comprehensiveness and relevance of the comparative analysis.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
WpEaUIBIWH;"REVIEW 
Summary:
The paper addresses unsupervised anomaly detection by proposing a method named UniCAD. The authors aim to enhance anomaly detection performance by establishing a theoretical connection between representation learning, clustering, and anomaly detection. They introduce a unified framework that jointly optimizes these three components, using a probabilistic mixture model and a Student's-t distribution for robust representation learning and clustering. The framework also includes an anomaly-aware data likelihood objective, which reduces the impact of anomalous data on the learning process. Additionally, the authors propose a gravity-inspired anomaly scoring method that leverages relationships between samples and clusters.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Modeling the connection between representation learning, clustering, and anomaly detection is highly relevant. This paper effectively demonstrates how these three tasks are interrelated and can be jointly optimized to improve anomaly detection performance.

2. The paper is well-written, presenting its hypothesis and method clearly. 

3. The results are impressive and demonstrate the effectiveness of UniCAD.

Weaknesses:
1. The ablation study on the hyperparameters $k$ and $l$ is insufficient. The authors only present results from a single dataset, satimage-2, where their method achieves an almost perfect score. It would be more informative to perform ablation studies across all 30 datasets or at least a subset where the model also shows lower performance. This broader analysis would demonstrate how these hyperparameters affect the average ranking of the method, similar to the results reported in the paper's table.

2. The authors introduce a $g(\Theta)$ term to prevent shortcut solutions, mentioning it in Equation 15. However, they do not discuss its importance or impact on performance after its introduction. Key questions remain unanswered, such as how the $g(\Theta)$ term affects the model's performance, what happens if it is removed, and how the autoencoder is implemented. These details are crucial, as the regularization term may significantly influence the results.

Limitations:
The authors mention some of the limitations, but they do not address the potential negative impact of the work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes UniCAD, a theoretically unified framework for representation learning, clustering, and anomaly detection. This paper first introduces the mixture of Student-t distribution $p(x|\Theta, \Phi)$ with degree of freedom $\nu=1$ based on a representation learner $f_\Theta$ using NN. Then, this paper combines with an anomaly indicator $\delta$ for maximum likelihood estimation. Parameters $(\Theta, \Phi)$ are optimized by EM algorithm and SGD. In addition, when detecting anomalies, an improved score is used with reference to gravity. The UniCAD achieved good performance on experiments with various datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well written and easy to follow.
- Good experimental results.

Weaknesses:
- We have several questions about the proposed method and experiments. Please see Qustions.
- The comparison with DeepSVDD and DIF is excellent, but I think the paper also needs to be compared with other Deep anomaly detection methods. For example, DROCC [1].

[1] Goyal, Sachin, et al. ""DROCC: Deep robust one-class classification. ""International conference on machine learning. PMLR, 2020.

Limitations:
- Hyper-parameter sensitivity seems to be one limitation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel probabilistic mixture model for unsupervised anomaly detection (UAD) that unifies representation learning, clustering, and anomaly detection into a single theoretical framework. The proposed UniCAD model addresses the lack of a unified approach in existing methods, which often consider these components separately or in pairs. The experimental results show that UniCAD consistently outperformed other methods in terms of AUC-ROC and AUC-PR. The model’s iterative optimization process using EM was also highlighted as effective and convergent.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- This paper introduces a novel integration of a probabilistic mixture model that unifies representation learning, clustering, and anomaly detection into a single theoretical framework.
- The proposed approach is well-motivated (Fig. 1) and supported by a robust theoretical foundation that maximizes anomaly-aware data likelihood, ensuring the model effectively leverages the interplay between representation learning, clustering, and anomaly detection.
- The paper is well-written, offering clear and comprehensive explanations of the proposed method, including detailed theoretical derivations and intuitive motivations for the design choices. The methodology section is particularly well-structured, logically outlining the steps and equations involved in the proposed model.
- The comprehensive evaluation design underscores the robustness of the proposed method.

Weaknesses:
- The connection between force analysis and anomaly detection, particularly between Equations 7 and 8 in Section 3.2.1, could benefit from further justification. While the analogy is interesting, it may not be immediately intuitive for all readers.
- The iterative optimization process may pose scalability issues for large datasets. An in-depth analysis and discussion of this would further strengthen the quality of this research.
- Although the model maps data to a low-dimensional representation space, the effectiveness of this mapping for very high-dimensional datasets could be explored further.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose UniCAD to jointly model representation learning,
clustering and anomaly detection.  The main objective is maximizing
the product of anomaly indicator (1 is normal, 0 is anomaly) and the
joint probability of instance x_i and cluster c_k given parameters for
representation learning theta and clustering phi.  The joint
probability is decomposed into the prior of c_k and likelihood of
p(x_i|c_k), which is modeled by a Student's-t distribution on the
distance between representation z_i and mean mu_k with covariance
Sigma_k.  p(x_i) is the marginal over c_k.  Anomaly indicator delta is
zero for p(x_i) in the lowest l percentage.  The anomaly score is
1/p(x_i).

Compared to Newton's law of Universal Gravitation, the anomaly score
function has similar components, except for the unit vector r_ik
(which indicates the directions of forces, beyond the
magnitudes). Hence, they incorporated r_ik into their anomaly score
function.

For updating the clustering parameter phi (mixture weights, means,
covariance), they use EM.  In the E-step, they estimate the posterior
p(c_k|x_i).  In the M-step, they estimate phi.  For updating
representation parameters theta, they use gradient descent to minimize
negative log likelihood of instances, together with a reconstruction
loss via an autoencoder to prevent shortcut solutions.

For empirical comparisons, they use 30 tabular data sets and 17
existing algorithms.  The proposed approach generally outperforms the
others in terms of average rank in AUCROC.  The vector version of
anomaly score function is ranked higher than the scalar version.  On
computation time, UniCAD is in the middle among 5 algorithms.  Ablation
studies indicate the contributions of the different components.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution is combining representation learning,
clustering, and anomaly detection in a unified single probabilistic
formulation, which is interesting.

The empirical results indicate that UniCAD compares favorably against
17 existing techniques on 30 tabular datasets.  Compared to four
existing algorithms, computation is not the most intensive.

The paper is generally well written.

Weaknesses:
The clustering part is similar to a typical Gaussian mixture model for clustering via EM, except for t-distribution instead of Gaussian and the scaling factor.

Two neural-network-based approaches were compared.  As UniCAD utilizes
representation learning, comparing with more approaches that utilize
representation learning would be significant.  Approaches without
representation learning have an inherent disadvantage.

Some parts could be clarified--see Questions.

Limitations:
Limitations of the proposed approach do not seem to be mentioned.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Wmodg5UiEd;"REVIEW 
Summary:
The work studies linear contextual dueling bandits with adversarial feedback. In each round $t$ the agent observes a context $x_t$ and chooses two actions $(a_t,b_t)$. The environment generates a binary preference label $\ell_t = \mathbb{I}(a_t > b_t)$. The underlying assumption is that there exists a linear reward function $r(x,a) = \theta_{\star}^{\top}\phi(x,a)$, where $\theta_*$ is a latent $d$-dimensional vector and $\phi$ is a known feature map such that $\|\|\theta_*\|\|_2 \le B$ and $\|\|\phi\|\|_2 \le 1$.

Based on this, the preference $\ell_t$ is a random variable such that
$$
\mathbb{P}\big(a > b \mid x\big) = \sigma\big(r(x,a)-r(x,b)\big)
$$
The link function $\sigma$ is antisymmetric, $\sigma(-z) = 1-\sigma(z)$ and such that $\sigma' \ge \kappa > 0$.

It is further assumed that a nonoblivious adversary may occasionally flip the preference label with the knowlege of $(a_t,b_t)$, where $C_T$ indicates the number of flips in the first $T$ rounds.

The regret is measured according to the following formula
$$
\max_a \sum_t 2r(x_t,a) - \sum_t \Big( r(x_t,a_t) + r(x_t,a_t) \Big)
$$
The main result is a regret bound of the order $d\sqrt{T} + dC$, ignoring logarithmic factors. This bound is tight because $\Omega(d\sqrt{T})$ is the known lower bound without adversarial corruption and $\Omega(dC)$ is shown to be the regret due to the adversarial corruption.

Experiments complete the contribution.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The topic is of adversarial feedback in dueling bandits is interesting and was only studied in a $K$-armed setting.

The regret bound is tight up to log factors

The related work section is complete and accurate.

The experimental section is significant.

Weaknesses:
The main results (upper and lower bounds) appear to be mostly based on combinations of known techniques from previous papers. The originality of the technical contributions is unclear.

The analysis of the $C$ unknown case (Section 5.2) is trivial.

Assumption 3.2 could create bad dependencies in the bounds on $B$ (and $\|\|\theta_{*}\|\|_2$).

The conditions $\sigma' \le 1$ and $\phi_t^{\top}\theta_{*} \le 1$ in the paragraph before (4.3) are not explicitely stated in the assumptions. Moreover, the second condition seem to imply that $B \le 1$.

There is no discussion on the hardness of computing $(a_t,b_t)$.

Limitations:
There is an explicit limiation paragraph in the conclusions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigated the contextual dueling bandits with adversarial feedback, where the adversary can corrupt the binary feedback of the agent to a certain level. A new algorithm named RCDB has been proposed. The key idea lies in the utilization of uncertainty-weight MLE. Regret analysis of RCDB was provided along with some experimental evaluations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper studies a known problem but with new angle, i.e., the adversary can corrupt the binary feedback of the agent to a certain level. The problem is well motivated. 
- A novel algorithm named RCDB was designed and incorporated uncertainty-dependent weighting into the MLE.
- The theoretical performance of RCDB in terms of regret is provided. 
- Experimental results to validate the performance of RCDB is presented and compared to existing methods.

Weaknesses:
- Assumption 3.1 assumes a linear reward. The reviewer agrees that this is a ""widely-used"" assumption in the recent RLHF literature, but was curious if your framework and regret analysis can be extended without such an assumption? If not, what are the new challenges? Can you comment on this? 
- The construction of the parameter estimator of $\theta$ requires the Taylor expansion. How the $\approx$s in Section 4 impact the regret analysis? 
- The experiments were run for multiple times, however, the variance is not shown in Figure 1. 
- The paper is very dense, and the authors have changed the template a bit, e.g., the space has been largely squeezed throughout the paper.

Limitations:
N/A.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The author proposed an algorithm, coined robust contextual dueling bandits (RCDB) for advarial feedback, using uncertainty-weighted maximum likelihood estimation. The algorithm guarantees $\widetilde{O}(d\sqrt{T}+CT)$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Their algorithm is not limited to a finite number of arms.

2. Their algorithm considers adversarial attacks based on selected actions (although the maximum number of adversarial attacks is restricted).

Weaknesses:
1. Hard to follow the paper

Limitations:
1. The authors precisely pointed out the limitations of their results, such as the reward function being linear with a known feature map.

2. Additionally, choosing $a_t, b_t$ (by computing argmax) might be infeasible when interaction with the environment needs to be fast.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the Contextual Dueling Bandits from Adversarial Feedback problem, in a linear reward setting. The authors propose an algorithm named robust contextual dueling bandits (RCDB), which is designed based on uncertainty-weighted regression and MLE. The authors prove that the proposed algorithm achieves a nearly optimal regret upper bound that matches the lower bound both in scenarios with and without (C=0) adversarial feedback. Experimental evaluations are provided to validate the theoretical results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem is well-motivated and important.
2. The paper is well-written.
3. The authors prove a nearly optimal regret upper bound for the proposed algorithm that matches the lower bound with and without (C=0) adversarial feedback. 
4. The authors also conduct some experiments to validate the theoretical results.

Weaknesses:
1. The title may be a little bit misleading, I think the setting of this paper is the adversarial corruption setting, not the setting with completely adversarial feedback. And the setting is the linear reward model, which is not specified in the title.

2. I have not checked the details, but I feel the uncertainty-weighted technique (which is the key to dealing with the corruption in the linear bandits) is mostly based on the previous works, could the authors highlight the technical difficulties in the dueling bandit setting?

Limitations:
No negative societal impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
VajjTXRj6J;"REVIEW 
Summary:
Based on the analysis of the shortcomings of DPO (Section 2.3), the authors proposed a simple reward distillation approach (Section 3.1) to align language models and a pessimistic variant (Section 3.2). These approaches outperform the vanilla DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The analysis in Section 2.3 extends the result in the IPO paper [23].
- The reward distillation approach is simple and natural, giving good empirical results.

Weaknesses:
I think that this paper is technically well done. However, it has a limited application scope.

The authors aim to address the shortcomings of offline alignment methods like DPO, while maintaining an offline approach. However, online alignment methods such as PPO or Reinforce-style algorithms naturally alleviate these offline learning issues. This raises a question: Given the authors' findings on the limitations of offline alignment, why not adopt an online alignment method like PPO directly?

Admittedly, the distilled DPO method and its pessimistic variants are less complex than PPO. However, these proposed offline variants introduce additional complexities to vanilla DPO by requiring separate reward models and separate training phases. This makes them more akin to online methods. Furthermore, the experiment section lacks a comparison against online method baselines, leaving it unclear if the distilled-DPO variants offer a better performance--compute tradeoff compared to vanilla online methods like PPO.

To be clear, the paper offers intriguing insights. However, these techniques seem niche. They benefit those who prefer robust generalization in preference optimization without using online data, an assumption not very well communicated in the paper.

In my opinion, there are ways to increase the method's applicability. While the proposed distillation approaches are positioned as offline methods, to my understanding, they are also applicable to an online regime. For example, in Equation (7), instead of sampling $(x, y_1, y_2)$ from an offline dataset, as the authors currently do, we can sample $x$ from an offline dataset and then sample $(y_1, y_2)$ from the model in training. The learned models, in this way, are directly comparable to those learned via online alignment methods like PPO, but in a simpler approach than these online baselines. If the authors could demonstrate that these models outperform PPO or require less complexity in training, they could expand the method's application scope and impact.

Limitations:
See ""Weaknesses"" section above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the limitations of DPO in LM alignment by proposing a reward model distillation approach. DPO, while efficient, often leads to overconfident and degenerate policies due to limited preference annotations. The authors introduce a method that trains LMs to match the distribution from a reward model, improving robustness and performance, particularly in biased preference datasets. Their approach also includes a pessimistic extension to handle uncertainty in reward models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper makes a considerable theoretical contribution by addressing the critical issue of degeneration in DPO, a problem of significant concern in the community. The authors present a well-structured and clearly written analysis that helps in understanding the overfitting commonly observed with the DPO. The proposed method of reward model distillation is both theoretically sound and intuitive, offering a robust solution that potentially improves upon traditional DPO methods. I appreciate this approach and its theoretical support.

Weaknesses:
The main concern about this paper is the evaluation. As discussed in Section 2.3, DPO can shift nearly all probability mass to responses that never appear in the training set, also called OOD extrapolation in other papers. This issue arises when there are few annotations per instance (x, y1, y2). Therefore, it is expected that the authors should demonstrate how their proposed approach mitigates this problem, maybe by presenting the log-probabilities of the winning and losing responses. Specifically, the log-probability margin between winning and losing responses should not keep increasing (I assume this is true?).

Additionally, I can see the paper presents results showing robustness against distribution shifts in preference data. But what is the factor that makes the proposed algorithm learn the policy whose underlying preference distribution closer to the true preference distribution in biased setting? I would be willing to increase my score if these concerns are solved.

Limitations:
I have no concerns about the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors discuss and give formal results on the limitations of DPO that have been observed in practice, and investigate reward model objectives for 1) distilling reward differences into the generator (eq. 7), and 2) pessimistic ""minimax"" distillation over a family of reward models, to mitigate these limitations. The theoretical equivalance of the ""forward"" and ""reverse"" pessimistic formulations (eq. 8 and 9) of the standard RLHF objective (eq. 1) is shown, and an approximate pessimistic objective (eq. 10) with a minimum over distillation losses for the reward model family considered in a Langrangian term. Results on TL;DR preference data that is biased to varying degrees to prefer long responses (Figure 2) shows good results for distilled models generally, and that pessimistic optimization over a family of reward models  optimized for varying response lengths improves results in irregular settings (i.e. when shorter responses are preferred).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A well written, insightful paper, with well formulated, novel objectives for preference optimization.

Weaknesses:
- The results, as the authors acknowledge, are currently quite limited. These methods should really be tested on at least one additional preference task, and for results utilizing multiple models, pessimism should be compared with other basic ensembling strategies. 
- The gamma parameter is annealed during training, suggesting that the setting is important and perhaps sensitive. Some ablations and discussion around this seems prudent.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
VTcGX5HO19;"REVIEW 
Summary:
This paper proposes using Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate model for Bayesian optimization (BO). BKTF approximates the objective function using a low-rank tensor factorization, with Gaussian process priors placed on the latent factors to capture dependencies and enable uncertainty quantification. The key advantages are the ability to handle complex functions that are non-stationary and non-separable, and the sharing of information across dimensions to enable a more global search compared to standard GP models with local kernels. Inference is performed via MCMC sampling. Experiments on benchmark optimization functions and hyperparameter tuning tasks demonstrate improved performance over GP-based BO, especially when the initial sample size and evaluation budget are limited.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The BKTF surrogate is a novel and creative approach to extend BO to handle more complex objective functions. Modeling the objective as a tensor factorization with GP priors on the factors is an elegant way to introduce non-stationarity and multi-scale correlations in a principled Bayesian framework.

The method is grounded in a clear mathematical framework, with full details of the model specification and MCMC-based inference procedure provided. Positioning BKTF as a type of deep GP offers useful insight into its expressive power.

The paper includes extensive experiments on a range of synthetic functions and real-world hyperparameter tuning tasks. The results convincingly demonstrate the advantages of BKTF over standard GP-BO in terms of optimization performance and sample efficiency, especially in the realistic setting of a very limited evaluation budget.

The paper is clearly written, with the methodology explained in detail and the experimental setup and results presented thoroughly. The authors also discuss the limitations of their work, including the scalability challenges and the restriction to a grid-based search space.

Weaknesses:
The main weakness is that the proposed BKTF method is only compared against basic GP-based BO with standard kernels. To fully demonstrate the advantage of the BKTF surrogate, comparisons should be made to more advanced GP models such as deep kernel learning, deep GPs, and other scalable GP variants. Without these comparisons, it's difficult to assess how much of the performance gain is due to the specific BKTF approach vs. simply being a more flexible GP.

The BKTF model bears significant similarities to existing works on scalable GPs, such as ""Gaussian Processes for Big Data"" and ""Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)"", which also use inducing points on grids to obtain tractable approximations. The relationship of BKTF to these methods is not discussed, and it's unclear whether BKTF provides any substantial advantages over these existing scalable GP approaches.

The experiments on the synthetic test functions are somewhat limited in their dimensionality (only up to 10d). Given that BO is most useful for optimizing high-dimensional black-box functions, testing on some higher-dimensional benchmarks would be valuable. The scalability of BKTF as the dimensionality and grid size increase is not fully investigated.

The MCMC inference procedure may become prohibitively slow for high-dimensional spaces or large evaluation budgets. The paper does not report the wall-clock time of the experiments, which makes it hard to assess the computational feasibility of BKTF in practice, especially compared to alternative approaches.

For the hyperparameter tuning experiments in Section 5.2, the strong performance of BKTF with very few iterations seems counterintuitive and is not fully explained, since the BKTF model has many parameters and would be expected to require a substantial amount of data to train effectively. This seems to contradict the results on the synthetic test functions, where GP-EI performs equally well in the first few iterations.

Limitations:
The relationship of BKTF to existing scalable GP methods is not thoroughly discussed. The paper does not make clear how BKTF differs from or improves upon approaches like ""Gaussian Processes for Big Data"" and ""Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)"", which also exploit grid structure. A more thorough comparison is needed to justify BKTF as a novel contribution.

The cubic scaling of the covariance matrix computations in the number of grid points, which could make BKTF infeasible for high-dimensional or very fine-grained grids. Some discussion of potential ways to scale up BKTF, e.g., by exploiting grid structure or using sparse approximations, would be valuable.

The fact that BKTF relies heavily on a sensible grid specification, and may fail badly if the grid is poorly chosen. Some experiments showing the sensitivity of the results to the grid choice would help assess this risk.

The lack of comparison to a wider range of flexible surrogate models beyond standard GPs, including more sophisticated GP models as well as other probabilistic regression approaches. The current experiments are not sufficient to establish BKTF as the best choice for BO in practice.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Bayesian Kernelized Tensor Factorization (BKTF) as a surrogate for Bayesian optimization (BO). This model uses a CP decomposition to define a set of random basis functions drawn from a GP prior. These latent functions are then weighted by another set of random variables. This defines a probabilistic model that can perform uncertainty quantification, which can then be trained by performing MCMC sampling over the hyperparameters. The acquisition function is calculated by computing the first and second moments of the samples, and calculating the upper confidence bound (UCB).

This procedure results in a non-stationary, non-separable model that can capture complex functions. This model is tested on a range of synthetic and ML hyperparameter BO benchmarks, each of which are non-separable functions that exhibit high degrees of interaction between variables.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality:**
This paper is the first to use kernelized tensors in the Bayesian optimization setting. Gaussian processes are a common surrogate in this setting, and this proposes an alternate surrogate with good arguments for its adoption.

**Quality:**
The paper demonstrates the performance of the BKTF surrogate on a wide range of benchmarks. The performance is strong, justifying the claims in the paper. Further, many supplementary results are provided to further investigate the modelling decisions made.

**Clarity:**
The explanation of the BKTF model and fitting process is explained well, providing a clear description of the model. Specifically, the 2D example in Figure 1 provides a clear motivation for modelling functions that have a high degree of interaction.

**Significance:**
The proposed method is a strong surrogate for Bayesian optimization, one that can model functions with mixed input spaces and high degrees of interaction between variables. This presents a good addition to the range of available surrogates in the field.

Weaknesses:
**Comparison to other methods:**
The authors claim that a strength of their method is that their method is non-stationary and non-separable. However, I do not feel that the paper sufficiently justifies this explanation for the model's success, for the following reasons:

I do not believe that the GP ARD is separable. The authors present that the ARD kernel is the product of $D$ independent kernels in 3.2. However, this is not how these kernels are implemented. Instead, (specifically for stationary kernels) they are expressed as functions of weighted distance [5], where $d=\sqrt{\sum_{d=1}^D (x_d - x'_d)^2/l_d}$. These kernels cannot be written as products of 1-dimensional kernels, and are non-separable. The authors also suggest that these experience the curse of dimensionality as the dimension increases, but this effect is not severe in the low-dimension regime studied in the paper.

The authors claim that the flaw of using additive GP kernels is that they:
> [require] strong prior knowledge to determine the proper interactions and [involve] a large number of kernel hyperparameters to learn

I do not find this argument convincing. For low dimensional problems, additive kernels can include all interactions up to order D, and learn the weighting of each order of interaction [1]. In fact, I believe that the number of kernel hyperparameters is of a similar order to the BKTF method. Additive kernels also work well with MAP estimation of the hyperparameters (especially for the low-dimensional problems investigated here), although I do not see why MCMC could not also be used for additive kernels if the number of kernel hyperparameters is considered too large. The additive GP baseline should therefore be order D, not order 1, to provide a fair comparison against existing non-separable modelling methods.

It is unclear why the authors compare to SaaSBO, a technique designed for high dimensional (D>100) spaces that places a strong prior on the lengthscales of the inputs (with the prior belief that few of the inputs are important, which is not the case for the test functions used).

Moreover, the authors do not provide comparisons against methods that are designed for non-stationary settings e.g. [1, 2]. 

(Minor comment) I would like to see MCMC over the GP kernel hyperparameters to obtain a fully Bayesian acquisition function, as in [6].

**Hyperparameter choices with BKTF:**
I disagree with the authors that BKTF is robust to rank misspecification. Figure 13 shows that the CRPS doesn't converge until 30 observations for the rank 2 model. Including the initial 30 datapoints, this model is fit on 60 datapoints, for a 2D problem - the GP model provides a much better fit to the data for <60 datapoints.  Since these models are used in a BO setting where few datapoints is common, this behaviour suggests that rank *is* an important parameter, and further that the model does not fit well with few datapoints. 

I would also want to see this experiment repeated on higher dimension test problems, to see if the problem is exacerbated in high dimensions. Moreover, the CRPS of the rank 2 model seems to converge only to that of the GP models, suggesting the performance over GP may not be solely due to modeling quality: this should be further investigated.

**Experiments:**
It is unclear how the authors handle categorical inputs for the baselines. The authors should be using methods designed for mixed input spaces, such as [4].

Following from the discussion on CRPS, this paper would benefit from some experiments on the quality of the fit of the model.

(Minor comment) It would be interesting to see the (arguably more popular) Matern 5/2 kernel compared to the 3/2 kernel, especially for the GP baselines.


[1] Snoek et al. ""Input Warping for Bayesian Optimization of Non-stationary Functions""    
[2] Eriksson et al. ""Scalable Global Optimization via Local Bayesian Optimization""    
[3] Duvenaud et al. ""Additive Gaussian Processes""    
[4] Ru et al. ""Bayesian optimisation over multiple continuous and categorical inputs""    
[5] Williams and Rasmussen. ""Gaussian Processes for Machine Learning""    
[6] Snoek et al. ""Practical Bayesian Optimization of Machine Learning Algorithms""

Limitations:
The authors provide good discussion on the limitations of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new surrogate model for Bayesian optimization, based on a functional tensor factorization. The approach discretizes the model to a pre-specified grid and uses MCMC sampling for inference. Bayesian optimization is carried out by selecting promising points from the pre-specified grid, as quantified by an acquisition function. The paper includes experimental results on synthetic functions as well as ML hyper-parameter tuning tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Nice experimental results on the ML hyper-parameter tuning task.
- Generally well written paper.

Weaknesses:
- Optimization method appears to be restricted to an a-priori defined grid of possible candidate points.

- I am concerned about reproducibility of the benchmarks, as the code submission not complete, e.g. appears to be missing implementations of basic functions like baseline GP fitting (`fitrgp`) and predicting (`predict`), the additive GP model mentioned in line 274 of the paper, the continuous optimization of the acquisition function. 

- Grid-based GP-UCB, GP-EI baselines are missing for ML tuning tasks (Figure 3). This is notable, because these experiments contain discrete variables, which requires a rounding operation if they are relaxed to a continuous space, as is done by the paper. This rounding operation is likely to degrade the performance of ""continuous"" GP-UCB and GP-EI, as it will can be prone to sampling ""between"" integers it has already seen, reducing its sample efficiency.

Limitations:
> A limitation of BKTF is that we restrict BO to a grid search space in order to leverage tensor factorization; however, we believe that designing a compatible grid space based on prior knowledge is not a challenging task.

An important limitation to highlight here once more that it goes from ""not challenging"" to prohibitively expensive as the dimension increases.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for Bayesian optimization where the prior is a low-rank sum of tensor products of GPs. An MCMC scheme is developed for approximate updating and UCB sampling. Several experiments show strong performance relative to baselines on artificial function optimization and ML hyperparam tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This is a seemingly new approach to BO with a more flexible type of prior, which shows good empirical performance.

Weaknesses:
It's not clear what is new relative to previous BKTF papers [10,11], other than the scheme for using UQ in UCB sampling.

The clearest potential drawbacks to the approach are the memory and compute costs. These should be reported for the experiments.

Limitations:
The paper motivates the approach in part because standard methods assume the generating process is stationary, but the BKTF is also stationary. It’s nonstationary only after conditioning on values of $g$.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
